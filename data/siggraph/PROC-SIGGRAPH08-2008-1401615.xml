<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/11/2008</start_date>
		<end_date>08/15/2008</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1401615</proc_id>
	<acronym>SIGGRAPH '08</acronym>
	<proc_desc>ACM SIGGRAPH 2008 new tech demos</proc_desc>
	<conference_number>2008</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2008</copyright_year>
	<publication_date>08-11-2008</publication_date>
	<pages>45</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1401616</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Airborne ultrasound tactile display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401616</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401616</url>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099802</person_id>
				<author_profile_id><![CDATA[81100309182]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099803</person_id>
				<author_profile_id><![CDATA[81384612893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tatezono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099804</person_id>
				<author_profile_id><![CDATA[81365594770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099805</person_id>
				<author_profile_id><![CDATA[81100555829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278290</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Allard, J., Menier, C., Raffin, B., Boyer, E., and Faure, F. 2007. Grimage: markerless 3d interactions. In <i>International Conference on Computer Graphics and Interactive Techniques ACM SIGGRAPH 2007 emerging technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Immersion, 2007. Cybertouch. http://www.immersion.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Airborne Ultrasound Tactile Display Takayuki Iwamoto* Mari Tatezono Takayuki Hoshi Hiroyuki Shinoda§ 
The University of Tokyo 1 Introduction Currently, the importance of haptic interaction techniques gather 
much more attention with the progress of the computer graphics, the physical simulation and the visual 
display technologies. There have been a lot of interactive systems which aim to enable the users to handle 
3D graphic objects with their hands (for example, see [Allard et al. 2007]). If tactile feedback is provided 
to the user s hands in 3D free space, the usability of those systems will be con­siderably improved. 
One strategy to provide tactile feedback in 3D free space is to attach tactile displays on the user s 
hands. For example, Immersion Cor­poration developed CyberTouch [Immersion 2007] which features small 
vibrotactile stimulators on each .nger and the palm to inter­act with objects in a virtual world with 
tactile feedback. However, this strategy inherently degrades tactile feelings due to the contact between 
the skin and the device occurring even when there is no need to provide tactile sensation. Air-jet is 
another possible candidate. However, air-jet can not pro­duce localized force due to diffusion. It also 
suffers from limited bandwidth. In addition, even if multiple air-jet nozzles are used, the variation 
of the spatial distribution of the pressure is quite lim­ited. The airborne ultrasound tactile display 
is designed to provide tac­tile feedback in 3D free space. The display radiates airborne ul­trasound, 
and produces high-.delity pressure .elds onto the user s hands. without the use of gloves or mechanical 
attachments. 2 Principle The method is based on a nonlinear phenomenon of ultrasound; acoustic radiation 
pressure. When an object interrupts the propa­gation of ultrasound, a pressure .eld is exerted on the 
surface of the object. This pressure is called acoustic radiation pressure. The acoustic radiation pressure 
P [Pa] is simply described as P = aE (1) where E [J/m 3] is the energy density of the ultrasound, a is 
a constant ranging from 1 to 2 depending on the re.ection properties of the surface of the object. Eq. 
(1) means that the acoustic radia­tion pressure is proportional to the energy density of the ultrasound. 
The spatial distribution of the energy density of the ultrasound can be controlled by using the wave 
.eld synthesis techniques. With an ultrasound transducer array, various patterns of pressure .eld are 
produced in 3D free space. Unlike air-jets, the spatial and temporal resolutions are quite .ne. The spatial 
resolution is comparable to the wavelength of the ultrasound. The frequency characteristics are suf.ciently 
.ne up to 1 kHz. The airborne ultrasound can be applied directly onto the skin with­out the risk of the 
penetration. When the airborne ultrasound is *e-mail: iwa@alab.t.u-tokyo.ac.jp e-mail: tatezono@alab.t.u-tokyo.ac.jp 
e-mail: star@alab.t.u-tokyo.ac.jp §e-mail: shino@alab.t.u-tokyo.ac.jp Figure 1: The .rst prototype of 
the airborne ultrasound tactile dis­play. Due to the ultrasound radiated from the transducer array, acoustic 
radiation pressure is exerted on the user s skin. Each transducer on the array is driven so that the 
emitted ultrasound produces a single focal point. The camera measures the position of the hand and the 
tactile feedback is provided when the hand is in contact with the virtual object. applied on the surface 
of the skin, due to the large difference be­tween the characteristic acoustic impedance of the air and 
that of the skin, about 99.9% of the incident acoustic energy is re.ected on the surface of the skin. 
Hence, this tactile feedback system does not require the users to wear any clumsy gloves or mechanical 
attach­ments. 3 Application Our airborne ultrasound tactile display is designed to provide tactile feedback 
for the users of 3D modeling software, video games and so on. Fig. 1 shows the .rst prototype system. 
The system is comprised of the airborne ultrasound tactile display and a vision­based hand tracking system. 
The tactile display exerts the radiation pressure on the user s hands when they touch 3D virtual objects. 
The hand tracking system used in the prototype is a simple system comprised of a single camera. However, 
if the airborne ultrasound tactile display is combined with more sophisticated hand tracking systems 
like Grimage [Allard et al. 2007], it would be more prac­tical haptic interaction system. It is also 
expected that by superim­posing the acoustic radiation pressure onto the 3D graphic objects presented 
with stereoscopic displays, it effectively enhances the re­ality of the 3D virtual objects. References 
ALLARD, J., MENIER, C., RAFFIN, B., BOYER, E., AND FAURE, F. 2007. Grimage: markerless 3d interactions. 
In International Conference on Computer Graphics and Interactive Techniques ACM SIGGRAPH 2007 emerging 
technologies. IMMERSION, 2007. Cybertouch. http://www.immersion.com/. Copyright is held by the author 
/ owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401617</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Animatronics for control of countenance muscles in face using moving-units]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401617</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401617</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Medical information systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010447</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health care information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099806</person_id>
				<author_profile_id><![CDATA[81421595861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Taisuck]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kwon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099807</person_id>
				<author_profile_id><![CDATA[81320489695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Etsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Genda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099808</person_id>
				<author_profile_id><![CDATA[81421596777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kuriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsunaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 1 Introduction ....tif ....tif If we can see the muscles under the human skin, it will be an interesting 
and wonderful experience. For, they are inside our body and invisible to our eyes. Up to now, the muscles 
could be seen only in books or models. Moreover, for the moving muscles, we had to be satisfied with 
seeing them in 3D computer graphics. For this reason, this work was made to express the movement of countenance 
muscles in the face over the neck and other subsidiary muscles with a robot for their observation in 
actual 3D. 2 Exposition In a precedent study (which is in the examination process), the Moving-Unit 
(hereafter, MU) was researched and proposed as a basic unit by multiple actuators to control the mechanical 
muscle structure. There are 26 MUs in total, of which 20 MUs are used to express the 58 AUs(Action-Units) 
of FACS**. Moreover, the animatronics of face model has been actually produced to verify the applicability 
of MU. On the basis of precedent study, the skull was supposed as of Koreans, and the skin thickness 
was also referred to the average tissue thickness of Koreans. The modeling was made with clay in the 
method of the facial reconstruction, which is used in the forensic medicine. After then, it was produced 
with the silicon rubber to express the shape and texture of actual muscles as close as possible. To control 
11 MUs, 15 actuators(servo motors) were employed. With this, more than 36 of 58 AUs in FACS could be 
expressed. Let alone 6 basic countenances like Happiness, Sadness, Anger, Disgust, Surprise, and Fear, 
other innumerable countenances could be expressed by mutual combining of 15 actuators. CPU was ATMEGA128 
of ATMEL, and the memory was Prochips 24LC32 eeprom. As the programming language, RoboBasic was used 
for the production. Animatronics for control of countenance muscles in face using Moving-Units Taisuck, 
Kwon* Graduate School of Kyushu University Etsuo, Genda Kyushu University Kuriko, Matsunaga§ Graduate 
School of Kyushu University   Figure 1: Facial Muscle Model Animatronics Table 1: A Summary of Moving-Unit 
&#38; Relation of Action-Units *e-mail: bamsemy@hotmail.com e-mail: genda@design.kyushu-u.ac.jp §e-mail: 
kuricom@gsd.design.kyushu-u.ac.jp MU-ID Description AU in Charge A Action of Lid 5, 7, 41, 42, 43, 
44, 45, 46 BI, BO Action of Brow 1, 2, 4 C Action of Upper Lip 10 D Action of Chin 16, 17 E Eye turn 
left &#38; right 61, 62, 65, 66 F Eyes up &#38; down 63, 64 G Lip Corner Puller 6, 11, 12 H Lip Corner 
Depressor 15 I Nose Wrinkler 9 J Action of Jaw 25, 26, 27, 31 K Action of Cheek 13, 33, 34, 35 L 
Dimpler 14 M Action of Nostril 38, 39 N Jaw Thrust &#38; sideways 29, 30 O Lip Bite 32 P Action of 
Ear  Q Neck Tightener 21 R Action of Tongue 19, 36, 37 S Action of Trunk  T Action of Arm or Forefoot 
 U Leg or Hind Leg  V Option  W Option  X Option  Y Head turn left &#38; right 51, 52 Z Head tilt 
left &#38; right, up &#38; down 53, 54, 55, 56, 57, 58 Example of mixing MU D+G Lip stretcher 20 C+D+G+H 
Action of Lip 8, 18, 22, 23, 24, 28  3 Conclusion This study is expected to help the production of 
robots not only for entertainment but for education, medicine, sports, and other purposes. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401618</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Ants in the Pants]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401618</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401618</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099809</person_id>
				<author_profile_id><![CDATA[81537733856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Keiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099813</person_id>
				<author_profile_id><![CDATA[81421596791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshimi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099814</person_id>
				<author_profile_id><![CDATA[81421593157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099815</person_id>
				<author_profile_id><![CDATA[81350575122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shogo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099816</person_id>
				<author_profile_id><![CDATA[81421592605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099817</person_id>
				<author_profile_id><![CDATA[81545957556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kanako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099818</person_id>
				<author_profile_id><![CDATA[81365598027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Sayaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ooshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099819</person_id>
				<author_profile_id><![CDATA[81421594132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yuichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099820</person_id>
				<author_profile_id><![CDATA[81421597054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Rika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099810</person_id>
				<author_profile_id><![CDATA[81421592727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Satsuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099811</person_id>
				<author_profile_id><![CDATA[81311484645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099812</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Weinstein. S., "Intensive and extensive aspects of tactile sensitivity as a function of body part, sex and laterality", in D. R. Kenshalo (Ed.), "The skin senses", Springfield, Ill.: C. C. Thomas, Pub. pp. 195--222, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401619</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[ARScope]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401619</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401619</url>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099821</person_id>
				<author_profile_id><![CDATA[81421594598]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099822</person_id>
				<author_profile_id><![CDATA[81331496816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuroki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099823</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099824</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099825</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>946825</ref_obj_id>
				<ref_obj_pid>946248</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Inami, M., Kawakami, N., and Tachi, S. 2003. Optical camouflage using retro-reflective projection technology. In <i>Proc. of the 2nd ISMAR '03</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858134</ref_obj_id>
				<ref_obj_pid>857202</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kato, H., and Billinghurst, M. 1999. Marker tracking and hmd calibration for a video-based augmented reality conferencing system. In <i>Proc. of the 2nd IWAR 99</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2154290</ref_obj_id>
				<ref_obj_pid>2154273</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wagner, D., Pintaric, T., Ledermann, F., and Schmalstieg, D. 2005. Towards massively multi-user augmented reality on handheld devices. In <i>Proc. of the 3rd Pervasive 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ARScope Takumi Yoshida * Shinobu Kuroki Hideaki Nii Naoki Kawakami § Susumu Tachi ¶ The University 
of Tokyo Figure 1: Application Examples of ARScope. 1 Introduction ARScope is a novel visual interface 
for various applications of aug­mented reality. A user views the world through a handheld device that 
has a shape similar to that of a magnifying glass or a crystal ball. When the user holds the device over 
an object, the image of the occluded part is displayed on the surface of the device seamlessly as if 
the surface of the device were transparent glass. The displayed image can be edited freely by using some 
of the techniques that are used for augmented reality. For example, a red apple can be colored blue, 
or animated CG characters can be made to appear suddenly. 2 Features In order to desplay the image of 
augmented reality applications, various techniques have been researched [Kato and Billinghurst 1999][Wagner 
et al. 2005]. The two typical devices used in these techniques are a head-mounted display and a handheld 
display such as a PDA. There are some dif.culties associated with the use of these devices. One of them 
is the problem of tracking the user s viewpoint. In order to create an appropriate image for the user 
s arbitrary viewpoint, the application needs a head-tracking sensor or some markers. The other problem 
is the seamless connection be­tween the displayed image and the real environment. Therefore, we propose 
ARScope, a device that aims at solving these problems. Our proposed system has the following three features 
that have not been achieved by the conventional method. Mobility: Users can look anywhere from an arbitrary 
viewpoint without the use of any head-tracking device or markers. Seamless connection: The image displayed 
on the device and the real environment are connected seamlessly. Multiple users: Two or more user s can 
use the same device at the same time since different pictures are presented in different directions. 
*E-mail:takumi yoshida@ipc.i.u-tokyo.ac.jp E-mail:shinobu kuroki@ipc.i.u-tokyo.ac.jp E-mail:hideaki nii@ipc.i.u-tokyo.ac.jp 
§E-mail:kawakami@star.t.u-tokyo.ac.jp ¶E-mail:tachi@star.t.u-tokyo.ac.jp Figure 2: System Con.guration. 
 3 Principle ARScope employs a method for optical camou.age using a retro­re.ective projection technology 
[Inami et al. 2003]. Figure 2 shows the system con.guration. The proposed system comprises a hand­held 
device and a head-mounted projector. The handheld device is covered with a retro-re.ective material, 
and a superimposed image is projected onto it from the projector. Both devices have built-in cameras. 
Cameras placed on the inner side or the reverse side of the handheld device capture the back­ground image. 
Meanwhile, a camera built inside the projector cap­tures the image of the user s view. By matching the 
feature points between these two images, a homography matrix between the two images is computed. Then, 
a projective transformation is applied to the background image in order to generate image that is suitable 
for the user s viewpoint. This technique enables the ARScope to dis­play an image of the area occluded 
by the handheld device without the use of any head-tracking device or markers. 4 Conclusion We have 
proposed and implemented the ARScope. In the future, we will improve the processing speed and the performance 
of this device. References INAMI, M., KAWAKAMI, N., AND TACHI, S. 2003. Optical cam­ou.age using retro-re.ective 
projection technology. In Proc. of the 2nd ISMAR 03. KATO, H., AND BILLINGHURST, M. 1999. Marker tracking 
and hmd calibration for a video-based augmented reality conferenc­ing system. In Proc. of the 2nd IWAR 
99. WAGNER, D., PINTARIC, T., LEDERMANN, F., AND SCHMAL-STIEG, D. 2005. Towards massively multi-user 
augmented real­ity on handheld devices. In Proc. of the 3rd Pervasive 2005. Copyright is held by the 
author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401620</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Artifacts of research]]></title>
		<subtitle><![CDATA[on singularities]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401620</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401620</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099826</person_id>
				<author_profile_id><![CDATA[81421595506]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Chertok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Artifacts of Research: On Singularities Jonathan D. Chertok* (Universal Joint) Figure 1: Recreated 
series of Rodenberg models with lines of ruling c. 2008. &#38;#169;Chertok 2008. 1. Introduction Relatively 
recent developments in computer visualization have, among other things, focused increasing interest on 
a classical mathematical model collection originally made by hand in plaster in the 1860 s. Much like 
the effort to classify the animal kingdom this was an effort to catalog the mathematical universe. I 
welcome the opportunity to exhibit a full-size recreation of the largest series in this collection the 
series designed to represent types of singularities possible on a cubic surface. These models were created 
by Carl Rodenberg for his thesis and produced under the direction of Felix Klein (1849 1925) the founder 
of modern topology. They are recreated here as plaster-based Rapid Prototyping (RP) models. Additionally 
presented are unique new models of the Clebsch Diagonal Cubic, a so-called "smooth" surface and the "god-head" 
of the series from which the series of singularities originate. I would also like to direct your attention 
to a video of the Clebsch Diagonal Cubic which simultaneously demonstrates the complex geometric configuration 
of this famous surface and gives some insight into the Computer-Aided Design (CAD) origins of the work. 
I am currently at work on a comprehensive, illustrated summary of the typology and mathematics behind 
these forms to accompany the exhibition. 2. Exposition Beyond their sublime beauty, the new models address 
a surprisingly large number of current digital technology issues involving computational geometry, discrete 
differential geometry, algebraic geometry as well as their relation to nurbs, mesh, and reverse engineering. 
In this vein, three observations regarding technique are germane. First, the fact that the surfaces had 
to be modeled as mesh in mathematical modeling software (the only way to generate these zero sum implicit 
equations of the surfaces) while the straight lines were generated as nurbs lines in CAD software (from 
their parametric equations) resulted in the lines proving inherently more malleable and easy to manipulate 
as a consequence of their nurbs nature (i.e. mesh are sometimes hard to control). Second, the relative 
precision of the parametrically derived lines - when literally placed against the complicated curving 
* chertok@universaljointdesign.com http://www.universaljointdesign.com mesh surfaces generated by the 
Marching Cube Algorithm of the software - allowed for a check of the algorithm, which would have been 
difficult to discern otherwise. Third, the ability to "fly-through" using a 3D mouse provided unequalled 
power for querying both the model and the relationships inherent in it. Thus, the use of CAD to work 
with these models provided obvious visualization and interactive benefits as compared to conventional 
mathematical modeling software. Similarly, the ability to work with actual models provides benefits. 
Even with a solid plaster-based RP model of the Clebsch Diagonal Cubic in your hand, it is still quite 
tricky to see, for instance, that the 27 lines on its surface are in fact straight, let alone to see 
their relationship amongst each other (the hyperboloid at the waist of this model creates an appearance 
of curvature). Utilizing newer RP model technologies can provide unique feedback in these and other respects. 
On a computational note, I am currently working on a "script" that would generate all possible 36 combinations 
for the numbering of the lines on this surface based upon the actual CAD configuration and in conjunction 
with the historical documentation of the geometric relationships of these lines. I believe this could 
be used to check the numbering of the original Clebsch models' lines which were numbered in various ways 
in the 1800's. 3. Conclusion The research represents an empirical exercise in design and critical thought 
and I am conducting parallel research on both the large scale fabrication of so-called "free-form" structures 
and in the emerging field of Architectural Geometry. I detail directions for future research in the more 
technically oriented Supporting Document. Inevitably returning to my black and white photographs of the 
original models cause me to reflect upon the relationship of nature, beauty and making things. Perhaps 
as Robin Evans put it in another context, by using more geometry they appear to have less . Jonathan 
Chertok May 2008 Austin, Texas Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401621</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Atta texana leafcutting ant colony]]></title>
		<subtitle><![CDATA[a view underground]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401621</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401621</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099827</person_id>
				<author_profile_id><![CDATA[81361605368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[LaFayette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099828</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099829</person_id>
				<author_profile_id><![CDATA[81421595816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Pierce]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St. Lawrence University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099830</person_id>
				<author_profile_id><![CDATA[81421601035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tatsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Starz Animation, Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099831</person_id>
				<author_profile_id><![CDATA[81421599116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lauren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simpson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Leckebusch, J., 2001. Investigating the True Resolution and Three-Dimensional Capabilities of Ground-penetrating Radar Data in Archaeological Surveys: Measurements in a Sand Box, <i>Arch. Prospection 8</i>, 29--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Moser, J. C., 1963. Contents and Structure of Atta texana Nest in Summer, <i>Annals of the Entomological Society of America 56</i>, 3, 286--291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nakamura, T., 2006. <i>The Soprants: Conceptual and Technical Framework for a 3D Interactive Video Game</i>. Masters Thesis, Texas A&M University, 23--27.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Parke, F. I., 2005. Lower Cost Spatially Immersive Visualization for Human Environments, <i>Landscape and Urban Planning 73</i>, 2--3, 234--243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Atta texana leafcutting ant colony: a view underground Carol LaFayette, MFA* Frederic I. Parke, Ph.D. 
 Carl J. Pierce Tatsuya Nakamura§ Lauren Simpsonß Associate Professor Professor M.S., P.G., Geophysics 
Software Developer Graduate Candidate Visualization Department Visualization Department St. Lawrence 
University Starz Animation Visualization Department Texas A&#38;M University Texas A&#38;M University 
Toronto Texas A&#38;M University  Figure 1. Atta Texana colony on an immersive visualization system 
1. Introduction The Atta project maps tunnels and chambers of a vast leafcutting ant colony. A Ground 
Penetrating Radar scan was translated into a 3D model that can be viewed on an immersive visualization 
system, scaling the viewer to ant size. The scanning is nondestructive and is the . rst time GPR has 
been used to map a living ant colony. To achieve this goal, the project combines the site-speci.c nature 
of an indexical system, GPR, with the ability of an algorithm to parse the data. The model retains a 
formal connection with its subject and can be distributed and viewed in many different ways. 2. Exposition 
One of Texas s smallest natives is also one of its largest: myrmecologists refer to ant colonies as superorganisms. 
Atta texana harvests tree leaves to farm a fungus in underground cavities that can spread over more than 
an acre of land and reach to great depths, with over a million ants in residence. Excavated leafcutting 
ant nests have proven large enough to contain a 3-story house. [Moser 1963] Previous attempts to model 
ant colonies have been undertaken by myrmecologist Walter Tschinkel, whose technique involves pouring 
casting material into the nest, digging it up and piecing it back together. Another method involves using 
a bulldozer to scrape away successive layers of soil and measuring the diameter of the holes. Tunnels 
collapse with this method and cannot be tracked. If measurement is the aim, either approach could be 
used. But our goal was to gain a unique view of this subterranean architecture using a method that would 
not disturb the colony. Ground Penetrating Radar (GPR) sends high frequency radar pulses from a surface 
antenna into the ground. Elapsed time between when the pulse is transmitted re.ected from buried materials 
or sediment and soil changes and when it is received, is measured. The sender and receiver are moved 
along the surface, following transects of a grid. GPR scans contain noise (soil composition, radio interference, 
and magnetic properties of substances) which can interfere with the results. [Leckebusch 2001] The data 
was . rst .ltered to reduce noise. We then differentiated voids from soil and other materials by targeting 
the velocity of radar waves in air (the speed of light 3 x 108 meters per second) in relation to other 
geologic materials (with an average velocity * e-mail: lurleen@viz.tamu.edu e-mail: parke@viz.tamu.edu 
e-mail: cpierce@stlawu.edu § e-mail: tatsuyan@acm.org ß e-mail: lrsimpson@gmail.com of 1 x 108 meters 
per second). An algorithm processed isosurfaces from density values distributed within the overall volume. 
The result formed polygons nested in layers corresponding to different densities in the GPR scans. [Nakamura 
2006] Thus, one can enable successive layers surrounding voids to hint at tunnel structure and fungus 
caches. Scale changes provided by an immersive visualization system transformed the viewer into the size 
of an ant surrounded by the tunnel architecture. [Figure 1] [Parke 2005] 3. Conclusion GPR delivers 
an indexical signal, formed by the action of radar pulses passing through substances over time and distance. 
In this way it can be loosely compared to a photograph, formed by the pattern of light striking a photosensitive 
surface. Photographer Henri Cartier-Bresson described taking a photograph as .xing a decisive moment, 
a con. uence of the artist s position in relation to the geometry of an unfolding event. Indexical signs 
offer what Roland Barthes termed the punctum, a speci.c feature of the subject that cannot be separated 
from time or place. Conversely, digital representations often rely on generalization of physical phenomena. 
Gravity, water, or terrain are simulated with algorithms, freed from substance and geographic locale. 
Due to its mathematical structure, an algorithm enables .uid recomposition of one form into another. 
In mapping an Atta nest, a connection is maintained with this particular subject deep in the soil of 
a Texas .eld, while the colony architecture is simulated in a medium distributable across time and space. 
References LECKEBUSCH, J., 2001. Investigating the True Resolution and Three-Dimensional Capabilities 
of Ground-penetrating Radar Data in Archaeological Surveys: Measurements in a Sand Box, Arch. Prospection 
8, 29-40. MOSER, J. C., 1963. Contents and Structure of Atta texana Nest in Summer, Annals of the Entomological 
Society of America 56, 3, 286-291. NAKAMURA, T., 2006. The Soprants: Conceptual and Technical Framework 
for a 3D Interactive Video Game. Masters Thesis, Texas A&#38;M University, 23-27. PARKE, F. I., 2005. 
Lower Cost Spatially Immersive Visualization for Human Environments, Landscape and Urban Planning 73, 
2-3, 234-243. &#38;#169;2007 Texas A&#38;M University. Data and documentation is freely available for 
noncommercial purposes. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401622</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[<i>Bellows</i>]]></title>
		<subtitle><![CDATA[bringing digital animation into the physical world]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401622</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401622</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099832</person_id>
				<author_profile_id><![CDATA[81421598165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland Baltimore County]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401623</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Bird Watching]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401623</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401623</url>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099833</person_id>
				<author_profile_id><![CDATA[81421600082]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kathy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marmor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Vermont]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bird Watching Kathy Marmor University of Vermont  Bird Watching is an interactive audio and video installation 
created specifically to comment on the invisible presence of space satellites. I focus on satellites 
to highlight the politics of remote sensing, amateurism, and identity. Bird Watching asks in a personal 
and intimate way if satellites, as tools of globalization, are transforming our conceptions of identity. 
My playful cardboard birds challenge the perception of satellites as remote objective eyes and my distinctive 
low-tech approach makes remote sensing satellites accessible and their surveillance palpable. The DIY 
creativity of amateur inventors inspired the unusual combination of cardboard and sensors. Bird Watching 
consists of six folded cardboard boxes that are suspended from the ceiling. Two boxes open and close 
each time a sensor is triggered. The other four boxes are embedded with proximity sensors and audio speakers. 
Each box s sensors, speakers, and wiring are clearly apparent and available. The audio speaker in one 
box is connected to a VHF/UHF radio scanner that listens for transmissions being sent to ground stations 
from low earth orbiting satellites. The radio receiver sits on a small worktable and is connected to 
a satellite antenna. Next to the radio is a computer monitor that displays the internet satellite tracking 
application Predict. Predict visually represents the satellite s orbit over the installation space and 
uses the computer s voice to announce the satellite s arrival when it is within radio range. The apparatus 
that my satellites use for remote sensing are simple proximity sensors. These sensors trigger a sound 
to play through the satellites speakers and the sound s duration and pitch is controlled by the participant 
s proximity to the sensor. The sound s pitch gets higher the closer one is to a sensor. Only when one 
maintains a specific distance from the sensor does the sound become comprehensible. I use sounds, such 
as a woman crying or children laughing, to remind us of the specificity of satellite data. The obvious 
correlation between sound and physical presence fosters an awareness of the ongoing interchange between 
people and technology. e-mail: kmarmor@uvm.edu e-mail: j6@umbc.edu. By emphasizing this exchange as well 
as initiating anticipation for the next orbiting satellite, I encourage participants to be satellite 
watchers . This notion of watching and being part of an active exchange inverts the usual power dynamic 
in surveillance by suggesting that the observed (or the watched) is an active accomplice who always has 
agency. My installation further accentuates our complicated relationship with technology by using the 
proximity sensors to track the participant s route in the installation as well. I worked with Jonathan 
Decker, a computer science student from the University of Maryland, Baltimore County to create a visualization 
that maps participants paths through the installation. This map is analogous to the satellite tracking 
on the computer monitor and these two adjacent tracking systems articulate the dialectic between the 
local and the global. The local and global are also linked through the viewer s participation in the 
installation. Moving one s body to activate the sound is an immediately local experience. However, over 
time participants realize that their interactions with the installation are being displayed in real time 
as an orbit. Thus a local action has global implications. Bird Watching creates an environment that emphasizes 
what I refer to as the ecology of perception. I define the ecology of perception as a reciprocal relationship 
between self and environment. It is my belief that our experience of our environment informs our perceptivity 
of self. Bird Watching suggests that space satellites provide a unique perspective of the earth and its 
peoples. This new perspective has come to define us but my installation questions how we identify ourselves 
as global citizens with new responsibilities.  Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401624</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA["Calakmul: the adventure"]]></title>
		<subtitle><![CDATA[background]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401624</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401624</url>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Calakmul: The Adventure Background Our software kiosk application brings the Mayan archaeological site 
of Calakmul to life and celebrates the ingenuity of the ancient Mayan people. Calakmul is a UNESCO World 
Heritage site. Our interactive software integrates rich 3D renderings of historical architecture and 
museum artifacts with 2D illustrations and animations. The presentation format speaks in a contemporary 
visual language to young people. The main goal for the user is to retrieve a personal necklace from a 
jaguar by exploring the complex and ultimately by mending a broken funerary mask and restoring harmony. 
Along the way, facets of the Mayan culture are presented and explained in text, audio and graphical terms. 
Thatching of a hut roof, preparation of a cacao drink, practicing the ball game, designing a stela, producing 
a fresco wall painting, these are some of the daily activities of the Mayan complex shared with the visitor. 
Monumental architecture such as the main temple and the palace, and detailed interior tomb settings, 
immerse the visitor in a pristine past reality that contrasts with the decayed contemporary remains that 
are transformed into their original states during the journey. The software works on two levels, a tour 
for those children who want a casual experience, and an in-depth educational resource for those who want 
to study the subject matter. Text and audio in Spanish and English are used throughout, for glyph and 
artifact description, and for characters. Detailed modeling in Autodesk 3D Maya Design software and 2D 
illustrations and animations are brought together in Flash software and augmented by XML coding. Our 
small group includes consultants in 3D CG and interactive Flash coding and we employ undergraduates from 
the Carnegie Mellon Dept. of Architecture for modeling, animation, interface graphics and design. The 
project idea emerged several years years ago during a conversation at Siggraph San Antonio where we presented 
the Israel Museum project The Virtual Dig at the San Antonio Children s Museum. A past curator from the 
Papalote Children s Museum (Mexico City) suggested that we develop software about the ancient Maya for 
their Museum. Calakmul, whose importance was just emerging in the eyes of scholars, presented a unique 
opportunity. After making visits and documentation of the remote Mayan archaeological site in the rain 
forest of the Calkmul Biosphere, and viewing tomb artifacts at a distant museum in Campeche, it was clear 
that an educational software could virtually unite all of these elements together to present a kind of 
living active cultural space. Background research, scholarly articles, archaeological references and 
publications served as the basis for reconstructions, elucidations and character activities. The influence 
of the environment can be seen in the materials and processes used in habitats, temples, food, clothing, 
art making and in the presence in art and myth of the creatures of the rain forest. Attention is brought 
to the need to preserve the legacy of the ancient Maya and to problems of looting of stelae in the complex. 
Maribel Ibarra, current Papalote Museum Exhibitions Director and Curator, continues to support the implementation 
of this project in the gallery spaces.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401625</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Confucius computer]]></title>
		<subtitle><![CDATA[transforming the future through ancient philosophy]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401625</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401625</url>
		<keywords>
			<kw><![CDATA[Confucianism]]></kw>
			<kw><![CDATA[culture technology]]></kw>
			<kw><![CDATA[illogical computing]]></kw>
			<kw><![CDATA[inter-generational cultural communication]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099834</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099835</person_id>
				<author_profile_id><![CDATA[81311485601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eng]]></first_name>
				<middle_name><![CDATA[Tat]]></middle_name>
				<last_name><![CDATA[Khoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099836</person_id>
				<author_profile_id><![CDATA[81325489384]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099837</person_id>
				<author_profile_id><![CDATA[81421602043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xiao]]></first_name>
				<middle_name><![CDATA[Ming]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099838</person_id>
				<author_profile_id><![CDATA[81421600108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099839</person_id>
				<author_profile_id><![CDATA[81421600434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Xiao]]></first_name>
				<middle_name><![CDATA[Yuan]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Feng, Y. L., and Bodde, D. 1952. <i>A History of Chinese Philosophy, Vol. 2: The Period of Classical Learning</i>. Princeton University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hao, H. F. 2007. Confucius teaching methods in the analects. <i>Chuanshan Journal 3</i> (March), 113--115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Huang, S. C. 1963. Musical art in early confucian philosophy. <i>Philosophy East and West 13</i>, 1 (April), 49--60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rong, J. Y., Huang, G. H., Zhang, H., and Wu, M. M. 2007. Exploration and analysis on harmony thought of inner canon of huangdi. <i>Chinese Archives of Traditional Chinese Medicine 12</i>, 2620--2621.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Zeng, Z. 2004. On the 'linear' phenomenon in traditional chinese music. <i>Journal of Southwest China Normal University (Humanities and Social Sciences Edition) 30</i>, 2 (March), 167--169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Confucius Computer: Transforming the Future through Ancient Philosophy Adrian David Cheok*, Wei Liu 
, Xiao Yuan ZhangI , Eng Tat Khoo , Xiao Ming Hu§, Peter Marini¶Mixed Reality Lab National University 
of Singapore Keywords: Culture technology, Confucianism, illogical comput­ing, inter-generational cultural 
communication 1 Introduction Confucius Computer is a new form of illogical computing based on the Eastern 
paradigms of balance and harmony, which are rad­ically different from the ancient Greek logic normally 
experienced in computing. It aims to facilitate inter-generational cultural com­munication by enabling 
young and old to interact and explore an­cient Asian cultural heritage. The system uses new media to 
revive and model these historical philosophies and teachings, presenting them in new contexts, such as 
online social chat, music and food. This enables people to experience and explore ancient culture using 
the literacy of digital interactivity. The system core is a virtual Confucius which can appear as a friend 
in online social groups such as Facebook. Based on our artistic interpretation of the model of Eastern 
mind and teaching, the sys­tem enables users to have meaningful social network communica­tion with virtual 
Confucius. But not only chat, our system allows many every day activities to be formed using history 
with the future. Confucius Computer offers a new artistic playground for interactive music-painting creation 
based on our Confucius music .lter and the ancient model of Cycles of Balance. Confucius Computer also 
al­lows users to explore the traditional Chinese medicine concept of Yin-Yang through recipe mixing. 
Confucianism is an Eastern ethical and philosophical system that has had great impact, especially in 
Asian countries, for more than 2500 years, with more and more interest being generated in west­ern society. 
However Confucian philosophies are extremely com­plex, and thus may not be easily accessible to people 
especially given the cultural and language barrier. Also, traditional passive media provides only a linear 
understanding of his profound teach­ings which restricts the learning process. In addition, young peo­ple 
are more inclined to use modern networked and social digital media. Hence our vision is to enable users 
to experience this rich ancient culture using computing technology in an interactive and dynamic way. 
Confucius computer provides a personalized experi­ence, in which users can explore and understand Confucianism 
and Eastern culture. 2 System Description The core of the system is a virtual Confucius thought engine 
that models Confucius knowledge from the Analects and his teaching method Yin Cai Shi Jiao (teaching 
student according to his/her ap­titude) [Hao 2007]. Based on the question asked, Virtual Confu­cius identi.es 
the Yu (subdomain of knowledge) in the Analects. The system then further queries the user to determine 
his Hui (ap­titude) on that topic. According to Yu and Hui, Virtual Confucius *e-mail: adriancheok@mixedrealitylab.org 
e-mail: khooet@mixedrealitylab.org e-mail: wei.liu@mixedrealitylab.org §e-mail: hu@nus.edu.sg ¶e-mail: 
u0506752@nus.edu.sg Ie-mail: u0607212@nus.edu.sg replies to the user. Users from our modern society 
could signif­icantly bene.t from this interactive and personalized advice from ancient virtual Confucius, 
which is not possible in passive media, such as the printed text. The core virtual model of Confucius 
also allows customized mod­ules or even widgets. For example, Confucius Computer introduces algorithms 
to .lter and transform any type of music into positive music that could promote personal character development 
[Zeng 2004]. The system .lters the timbre, rhythms and scale of the music and outputs the positive music. 
The output music is in the Chi­nese pentatonic scale which corresponds to the cosmological theory of 
the .ve elements [Huang 1963]. At the same time, based on the ancient cycles of balance [Feng and Bodde 
1952] and the .ve ele­ments, the music is then visualized in the form of a dynamic Chi­nese painting. 
The system also allows bidirectional music-painting interaction that enables users to manipulate the 
Chinese painting to generate music output. Another example is the Confucian cooking module. Confucius 
em­phasized that being physically healthy is an act of .lial piety. Ac­cording to traditional chinese 
medicine, the human body is a minia­ture universe [Rong et al. 2007]. To maintain a healthy body is to 
maintain a balance of Yin-Yang. One way to achieve this is by choosing the correct food which is divided 
into hot, cold and neu­tral. Factors that in.uence the choice include the current body state and the 
external environment (e.g. season). Using deep modeling of such philosophy, the system allows users to 
gain insights into the complex concept of Yin-Yang in a unique context of food through recipe mixing 
game. 3 Conclusion Confucius Computer is a new form of culture technology which models Eastern teachings 
and philosophies. It aims to mediate the transmission of culture through the use of new media that enable 
intergenerational users to experience and explore ancient culture using the literacy of digital interactivity. 
 References FENG, Y. L., AND BODDE, D. 1952. A History of Chinese Phi­losophy, Vol. 2: The Period of 
Classical Learning. Princeton University Press. HAO, H. F. 2007. Confucius teaching methods in the analects. 
Chuanshan Journal 3 (March), 113 115. HUANG, S. C. 1963. Musical art in early confucian philosophy. Philosophy 
East and West 13, 1 (April), 49 60. RONG, J. Y., HUANG, G. H., ZHANG, H., AND WU, M. M. 2007. Exploration 
and analysis on harmony thought of inner canon of huangdi. Chinese Archives of Traditional Chinese Medicine 
12, 2620 2621. ZENG, Z. 2004. On the linear phenomenon in traditional chinese music. Journal of Southwest 
China Normal University (Human­ities and Social Sciences Edition) 30, 2 (March), 167 169. Copyright is 
held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401626</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Copycat Arm]]></title>
		<subtitle><![CDATA[an aping arm mimicking human motions without delay]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401626</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401626</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099840</person_id>
				<author_profile_id><![CDATA[81319493088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099841</person_id>
				<author_profile_id><![CDATA[81421597791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Motomasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099842</person_id>
				<author_profile_id><![CDATA[81421596844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Emi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copycat Arm - An aping arm mimicking human motions without delay - Kiyoshi Hoshino Motomasa Tomida 
Emi Tamaki (University of Tsukuba) 1 Introduction The proposed system is capable of estimating the shape 
or posture of the human hand with a high speed and accuracy by means of a single high-speed camera, regardless 
of the differences among individuals, such as finger size, length of the bones, skin color, and state 
of creases, as well as differences in the lighting environments. In addition, this system can also estimate 
the accurate posture of a hand performing three-dimensional motions such as rotations of the wrist and 
shoulder by using a single camera, without having to install multiple cameras surrounding the hand, i.e., 
the object to be photographed. The error in the estimation is approximately 5 to 6 degrees in the finger 
joint angle, thus achieving an accuracy level almost identical to that of the conventional technology. 
This system realizes a processing speed of more than 100 fps, even when a single notebook personal computer 
with ordinary specifications is used. Our research group made an exhibit of a Copycat Hand [1] [2] at 
Siggraph Emerging Technologies in 2006. Although this system achieved a high processing speed of 150-160 
fps, it could not necessarily make an accurate estimation of the motions of the fingers during three-dimensional 
motions such as rotations of the wrist and shoulder. In other words, a very low accuracy was achieved 
in the estimation of the hand posture when the palm of the user was aligned sideways with respect to 
the camera or when the user performed waving motions, for example, saying good-bye. On the other hand, 
the Copycat Arm introduced this year can estimate the posture of a hand with high accuracy, even if the 
user moves his hand or arm freely before a high-speed camera. The secret lies in saving the contour information 
of a large number of hand shapes in advance in a database; this enables us to identify similar images 
among unknown images (this function was also adopted for Copycat Hand ). Further, it is also advantageous 
to save the slenderness ratio of the hand images in the database and to hierarchically rearrange the 
data set of the database in advance, depending on the degree of similitude. If one practices gymnastics 
or dances in front of a high-speed camera, the Copycat Arm imitates the motions of the person s upper 
limbs without delay so that he/she can enjoy interactive communications with it. Therefore, this system 
is also known as the Aping Arm. 2 System configuration Human hand images and joint angle data were acquired 
as a set for preparing the database. The characteristics were calculated with respect to a reference 
point and its vicinity using a higher order local autocorrelation function Clustering was conducted by 
means of a self-organizing map in order to collect data with mutually similar slenderness ratios of the 
hand images and joint angles. The large-scale database was divided into an approximately uniform number 
of classes and data in order to achieve uniformity in the estimation time. 3 Hand posture estimation 
A subject held up a hand roughly 1 m away from the high-speed camera and moved his hand and fingers freely. 
The hand was allowed to move in all directions. The estimated results showed that the finger angles have 
possibly been estimated with a high precision during continuous hand and fingers movements (Fig.1). It 
is also apparent that this system can estimate the accurate posture of a hand performing wrist rotation. 
The system operates at a rate faster than 100 fps. It also realizes a processing speed of more than 100 
fps, even when a single notebook personal computer with ordinary specifications is used. When the estimation 
results are transmitted to a humanoid robot arm with a hand (designed by our research group), which can 
pinch a small, thin, or fragile object using its fingertips, the dexterous hand can mimic the motions 
of the human hand with high accuracy at a processing speed of 100 fps or more, as shown in Fig. 2.  
hoshino@esys.tsukuba.ac.jp http://www.kz.tsukuba.ac.jp/~hoshino/ Copyright is held by the author / owner(s). 
SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401627</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Diamond road online]]></title>
		<subtitle><![CDATA[a user-guided documentary experience]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401627</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401627</url>
		<categories>
			<primary_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Information browsers</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010515</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Multi / mixed media creation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10003254</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Hypertext / hypermedia</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003300.10003302</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web interfaces->Browsers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099843</person_id>
				<author_profile_id><![CDATA[81421597063]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lachman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ryerson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>215302</ref_obj_id>
				<ref_obj_pid>217279</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Davenport, Gloriana and Michael Murtaugh," Con Text: Towards the Evolving Documentary", in <i>Proceeding of the Third ACM Iinternational Conference on Multimedia</i>, ACM Press, San Francisco, 1995, 377--378]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 1. Introduction Diamond Road Online (DRO) is an experimental interactive documentary system that uses 
a unique interface and a software recommendation system to tell stories of the global diamond trade. 
It is associated with Kensington Communication s 3-part television documentary Diamond Road , airing 
internationally. DRO uses a spreading-activation net and a collection of keyword-tagged media content 
to let viewers browse through a documentary database in a connected fashion. The DRO system uses software 
intelligence and user-recommendations to develop a custom documentary movie on the fly , as users move 
from video-clip to semantically relevant video-clip. Diamond Road Online reworks, re-imagines and significantly 
extends Davenport and Murtaugh [1995] in its inclusion of story-telling­model meta-tags, the incorporation 
of user-feedback/content, the concept of story-sequences, the user­interface/representation-of-state, 
and the development of a robust/reusable and publicly accessible architecture. 2. Exposition A corpus 
of over 8 hours of video, text, photo or audio vignettes (1-3 minutes in length each) was fed into a 
database. Each piece of content was be tagged with descriptive meta-information using a combination of 
top-down ontologies and bottom-up folksonomies. Users are able to browse and view any of these micro-stories 
through a Flash interface. Our recommendation algorithm tracks the key-words for recently-viewed content 
using the notion of activation energy in a spreading-activation network. It then searches the database 
for content linked to similar keywords, in order to find contextually relevant media the user hasn t 
yet seen. The top recommendations are presented to the user to preview or select from, providing a passive 
guided search through a rich-media database.  * e-mail: richlach@ryerson.ca Keywords can exert more 
or less influence on database-searches for related keyword-linked content, based on time-decay, popularity, 
availability and uniqueness. The recommendation engine can use simple models for how good stories evolve 
[eg: introduction before details for a given topic] to further refine its recommendations. In addition 
to keywords, the algorithm incorporates user feedback aggregated over time. Users can vote for and rate 
content as they view it, which will influence recommendations for later viewers. They can also collect 
media-clips together into story-sequences, allowing a type of editing to tell their own stories and generate 
their own content. These user-generated stories can then be shared with other users through the same 
software-recommendation algorithms, creating mini-documentaries that offer a particular point of view. 
 DRO represents an innovative approach to storytelling, in which users and content-creators collaborate 
in the viewing experience. The project can be expanded/improved using the existing content, and the engine 
can also be customized for other stories or modes of interaction. The content corpus can evolve and be 
edited as content is changed (eg: updates from correspondents, latest news on an issue) on-the-fly, without 
any re-processing of existing content. The Diamond Road Online documentary experience is online and available 
to the general public at http://www.diamondroad.tv , and the concept/software is being explored for future 
documentary subjects and for domains such as news and media-archiving.  References Davenport, Gloriana 
and Michael Murtaugh, Con Text: Towards the Evolving Documentary , in Proceeding of theThird ACM Iinternational 
Conference on Multimedia, ACM Press, San Francisco, 1995, 377-378 Copyright is held by the author / 
owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 Martin Rappaport 81 Weight Martin Rappaport 90 Weight Conflict Diamonds Weight55 Conflict Diamonds 
Weight38  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401628</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Digital sports using the "Bouncing Star" rubber ball comprising IR and full-color LEDs and an acceleration sensor]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401628</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401628</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099844</person_id>
				<author_profile_id><![CDATA[81421602087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Izuta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099845</person_id>
				<author_profile_id><![CDATA[81537528856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099846</person_id>
				<author_profile_id><![CDATA[81421592614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099847</person_id>
				<author_profile_id><![CDATA[81351603011]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sachiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kodama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099848</person_id>
				<author_profile_id><![CDATA[81100297951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099849</person_id>
				<author_profile_id><![CDATA[81365592103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099850</person_id>
				<author_profile_id><![CDATA[81421595558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kaoru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099851</person_id>
				<author_profile_id><![CDATA[81435604554]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Haruko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mamiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>303115</ref_obj_id>
				<ref_obj_pid>302979</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., Wisneski, C., Orbanes, J., Chuu, B., Paradiso, J., "PingPongPlus: Design of an Athletic-Tangible Interface for Computer-Supported Cooperative Play", Proc of CHI'99, pp. 394--401, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1178862</ref_obj_id>
				<ref_obj_pid>1178823</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sugano, Y., Ohtsuji, J., Usui, T., Mochizuki, Y., Okude, N., "SHOOTBALL: The tangible ball sport in ubiquitous computing", ACM ACE2006 Demonstration Session, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>877443</ref_obj_id>
				<ref_obj_pid>876866</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ohno, Y., Miura, J., Shirai, Y., "Tracking Players and Estimation of the 3D Position of a Ball in Soccer Games," 15th International Conference on Pattern Recognitio (ICPR'00)-vol. 1, p. 1145, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Sports Using the Bouncing Star Rubber Ball Comprising IR and Full-color LEDs and an Acceleration 
Sensor Osamu Izuta, Jun Nakamura, Toshiki Sato, Sachiko Kodama, Hideki Koike, Kentaro Fukuchi, Kaoru 
Shibasaki, Haruko Mamiya The University of Electro-Communications 1. Background and Purpose of this 
project Since ancient times, many people in the world loves a ball as an entertainment equipment. Design 
of the ball has been established and sophisticated as sport equipment used in soccer and baseball, in 
the sports that many people enjoy enthusiastically. Recently, a technical breakthrough has transformed 
the ball. Electronic devices have been started to be installed inside the balls. In our project, we focus 
on the creation of a new ball and entertainments for human beings using a ball comprising electronic 
devices. The previous studies carried out on our ball are as follows: .Lumica Glow Football (Lumica Corporation) 
.A ball containing infrared (IR) LEDs: IR Roboball MK2 .A ball containing IR LEDs: RCJ-04 (EK Japan Co., 
Ltd.) .A ball containing a three-axis acceleration sensor in Kyukon We propose a ball that contains the 
abovementioned elements in one ball in order to realize a quality entertainment environment for human 
beings. The appearance of our ball (surface color, flashing speed of light and graphical effect around 
the ball) changes dynamically according to the acceleration data, and the position of the ball. To change 
the appearance, the ball is composed of full-color LEDs, whose color is controlled by a microprocessor 
connected to an acceleration sensor inside the ball. The ball is designed to emit IR light so that it 
can be detected. In order to obtain high elasticity, silicone was used as material to manufacture the 
ball Bouncing Star (Hane-Boshi in Japanese). Furthermore, we created effective computer graphics for 
determining performance using this ball. We also created new digital sports content Space Ball using 
Bouncing Star. 2. Formation and Mechanics of <Bouncing Star> The diameter of Bouncing Star is 98 mm, 
its weight is 525 g. The electronic devices are placed inside the central core. The middle and outer 
layers are made of silicone materials. The central core of Bouncing Star is composed of a 3D wiring circuit. 
Six circular printed wiring boards (diameter 60 mm) are used to construct a grid sphere. At the center 
of the sphere, a chargeable battery is located. We also arranged a PIC, an acceleration sensor, a terminal 
for battery charging, switches for the selection of program, six full-color LEDs, and 12 IR LEDs inside 
the grid. 3. Illumination of Full-color LEDs Depending on Acceleration The color of the LEDs is controlled 
by the microprocessor (PIC16F88) that receives acceleration data from the sensor. The data reflect the 
player s actions such as throwing, kicking, bouncing, catching, rolling, and rotating the ball such that 
the light emitted from the LEDs changes in response to the actions. Nine different light-emission modes 
are programmed in the microprocessor used for Bouncing Star. 4. Image Recognition Using IR LEDs: Recognition 
of Position and Bounce of Ball Inside Bouncing Star, six IR LEDs are placed so that the ball can be easily 
image recognized from any angle. An IR ray camera can recognize the ball. Position of the ball can be 
gained in the area of the camera can capture the image. Under the present conditions, we can recognize 
the position of the ball, status of light emission, ball bounce when people bounce the ball on the floor 
or play the game catch by the image recognition technique (OpenCV) using two high-speed cameras and PCs 
which detect the IR rays emitted from the ball. 5. Interaction Between the Ball and Computer Graphics: 
Creation of Space Ball Content The computer graphics projected on the floor or ground is dependent on 
the conditions of Bouncing Star. After we examined various graphic effects, we created a sports content 
titled Space Ball. In Space Ball, two players compete with each other. They can stand anywhere outside 
of a 4 × 4 (m) square, in which 10 × 10 small grids are projected. When a player throws the ball in the 
square or kicks it inside, the color of the grids, through which the ball passes, changes in real time. 
If the player can bounce the ball on the remaining grids, he/she can gain more points. Fig.1 Image captured 
from a game of Space Ball Image recognizing camera and LCD projector are placed on the balcony (height 
10 m) of the third floor of the closest building. [1]Ishii, H., Wisneski, C., Orbanes, J., Chuu, B., 
Paradiso, J., PingPongPlus: Design of an Athletic-Tangible Interface for Computer-Supported Cooperative 
Play , Proc of CHI 99, pp.394 401, 1999. [2]Sugano, Y., Ohtsuji, J., Usui, T., Mochizuki, Y., Okude, 
N., SHOOTBALL: The tangible ball sport in ubiquitous computing , ACM ACE2006 Demonstration Session, 2006. 
[3] Ohno, Y., Miura, J., Shirai, Y., "Tracking Players and Estimation of the 3D Position of a Ball in 
Soccer Games," 15th International Conference on Pattern Recognitio (ICPR'00)-vol.1, p.1145, 2000. Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401629</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[/ed (slashed)]]></title>
		<subtitle><![CDATA[gut feelings when being cut and pierced]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401629</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401629</url>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099852</person_id>
				<author_profile_id><![CDATA[81365598027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sayaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ooshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099853</person_id>
				<author_profile_id><![CDATA[81421601405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukuzawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Polytechnic University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099854</person_id>
				<author_profile_id><![CDATA[81311484645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099855</person_id>
				<author_profile_id><![CDATA[81100486051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hideyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099856</person_id>
				<author_profile_id><![CDATA[81100363092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Junji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Communication Science Laboratories and Japan Science and Technology Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099857</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C. C. Collins: Tactile television - mechanical and electrical image projection, IEEE Man-Machine Systems, 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1050020</ref_obj_id>
				<ref_obj_pid>1048934</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. B. F. van Erp: Vibrotactile spatial acuity on the torso: effects of location and timing parameters, WorldHaptics 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 - Gut Feelings when Being Cut and Pierced - Sayaka OOSHIMA1, Yasushi FUKUZAWA2, Yuki HASHIMOTO1 Hideyuki 
ANDO3, Junji WATANABE3, 4, and Hiroyuki KAJIMOTO1 1: The University of Electro-Communications, 2: Tokyo 
Polytechnic University 3: NTT Communication Science Laboratories, 4: Japan Science and Technology Agency 
Interface for internal bodily sensation Vibration displays have been rapidly brought intoevery day life 
in the fields of communication and entertainment [1]. Notably, vibro-tactile feedback provided by game 
controllers has been intensively pursued to enrich haptic experiences in gaming.Haptic feedback in games 
is presented mainly to thehands with the game pad. However, though the game characters have a wide range 
of haptic experience inthe virtual world, the feedback consists of simpleshocks or vibration patterns. 
For example, though thecharacters in "Sword and Magic" are slashed by theirenemies in swordfights, previous 
devices are not able to present realistic haptic sensations of such situations. Here, we propose a novel 
vibro-tactile interface, called /ed (slashed) , which can present haptic sensation of movement on the 
body s surface, such as that of being cut with a sword (Fig. 1A), and movement passing through the body, 
such as that ofbeing pierced with a sharp object (Fig. 1B). Althoughbody-worn haptic feedback devices 
(haptic vests) havebeen proposed [2], they cannot provide the new dimensions of haptic experience offered 
by our interface. With our interface, not only vibrationpatterns at a location, but also haptic movements 
on the body s surface (frontal parallel plane) and internal movements passing through the body(sagittal 
dimension) can be accomplished. Fig. 1. Feelings of being slahsed (A), and piereced (B). Sensations 
of being cut and pierced The interface is composed of several vibration speakers arranged in a line, 
and a visual presentationdevice. The speakers are attached to a belt with rings as shown in Fig. 2. The 
rings are used to confine the vibrations within each area. For demonstration; the user wears a head mounted 
display and wraps the belt around the waist as shown in Figs. 3(A) and 3(B). When a swordfight is presented 
with the display, thespeakers are sequentially vibrated in tandem to thevisual information. In addition, 
sound effects arepresented in synch using the same speakers. Then, the user perceives the series of vibrations 
and soundsas haptic apparent movement on or through the body.When the speakers on the front of the body 
arevibrated as in Fig. 3(C), the user can perceive the haptic movement on the abdomen surface as if he/sheis 
being cut. When the speakers located on the front and back are vibrated as in Fig. 3(D), the user canperceive 
the haptic movement passing through theabdomen as if he/she is being pierced. To determine the design 
principle of the interface, we performed psychophysical experiments. We specified the temporal interval 
between the vibrationonsets and created the sound effects for realistic presentation of the sensations. 
Novel aspects of our interface are haptic movement presentation on and through the body and its design 
principle based on psychophysical experiments.  Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401630</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Emotional touch]]></title>
		<subtitle><![CDATA[a novel interface to display "emotional" tactile information to a palm]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401630</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401630</url>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099858</person_id>
				<author_profile_id><![CDATA[81311484645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099859</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. Sekiguchi, K. Hirota and M. Hirose: Haptic Interface Using Estimation of Box Contents Metaphor, ICAT2003 (2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>506525</ref_obj_id>
				<ref_obj_pid>506443</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[I. Poupyrev, S. Maruyama and J. Rekimoto: TouchEngine: A Tactile Display for Handheld Devices, Proc. CHI 2002 (2002), 644--645.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>797589</ref_obj_id>
				<ref_obj_pid>795683</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[H. Yano, M. Yoshie and H. Iwata: Development of a Non-Grounded Haptic Interface Using the Gyro Effect, Proc. Haptic Symposium 2003 (2003), 32--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Emotional Touch - A novel interface to display "Emotional" tactile information to a palm - Yuki Hashimoto 
 Hiroyuki Kajimoto The University of Electro-Communications {hashimoto, kajimoto}@kaji-lab.jp Introduction 
Recently, specifications for handheld devices have been dramatically improved. As the information has 
become richer, we need to consider a new type of user interface that will co-exist with existing interfaces. 
These might be audio and visual interfaces that are sufficiently intuitive to reduce a user s load. A 
tactile interface is one such possibility. Many projects have attempted to develop haptic/tactile sensations 
in handheld devices [1][2][3]. Although these approaches succeeded in presenting literal information, 
the quality of the sensation is not high and users can feel just a single or only a few kinds of sensations. 
There are two types of resolution in a tactile display; spatial and temporal. Conventional tactile displays, 
such as Braille, seek to achieve higher spatial resolution to display rich literal information. We call 
these, temporally low, spatially high resolution tactile displays. On the other hand, as with conventional 
wearable tactile displays, because of limitations of mounting space and power source, one or just a few 
vibration motors or solenoids are used. Thus, temporal bandwidth is notably narrow. We call these temporally 
low, spatially low resolution tactile displays. In a different approach to these tactile displays, our 
goal is to display emotional , not literal information. Because handheld devices already can present 
figures, photographs, and video through visual displays, they do not need to present shape by tactile 
sensation. For our goal, temporal bandwidth is much more important than spatial resolution. Our strategy 
is thus to present richer expressions of tactile sensations by improving the temporal bandwidth while 
keeping the spatial resolution low (Figure 1 right). Method and System Configuration Our basic strategy 
is for a user to hold one or two speakers with their hands while an rubberized band around the circumference 
of the speaker cone seals the air between the palm and the cone (Figure 1 left). If the cone of the speaker 
is pulled, the user feels suction, or negative pressure. If the cone is pushed, the user feels positive 
pressure. By controlling frequency and amplitude, our system can present temporally rich tactile sensations. 
Suctioning forceCone of speaker is pulledCone of speaker is pushedPushing forceSuctioning force Temporal 
ResolutionSpatial ResolutionConventional Tactile DisplayProposed InterfaceConventional Wearable Tactile 
Display Figure 1. Left: Method of our system         Right: Feature of conventional tactile 
display and the proposed interface We used a speaker in our tactile device because it has great potential 
for actuating from very low (about 1Hz) frequencies to very high (about 20kHz) ones. In our method, the 
speaker presents very soft and comfortable tactile feelings at 1-30Hz. Normal vibrations are felt at 
30-1kHz. As a speaker can obviously present sound, it is a simple and natural multimodal interface device. 
Using air pressure also has another merit. For a mechanical tactile display, the spatial distribution 
of any distortion is not uniform. Users then experience some sort of shape , which can be unnecessary 
and cumbersome information. Conversely, when we use air pressure, we can present purely uniform pressure 
onto the palm, and the user feels only the pressure without any feeling of edges. Our prototype is composed 
of one or two speakers, a force sensor, an acceleration sensor, a microprocessor board, and a stereo 
amplifier (figure 2). In this system, the microprocessor generates sinusoidal waves. A sound play unit 
also generates sound waveforms. The two waveforms are synthesized and transmitted to the speakers via 
a stereo amplifier. The values of the force sensor and the acceleration sensor are monitored by the microprocessor. 
Thus we can change frequency and amplitude of the wave through these values in real time. Force sensorinsideAcceleration 
sensor insideSpeakerHandheld PartMicro Computer(H8 3048F)Stereo Amplifer(RSDA202)Control PartValue ofaccelerationValue 
of forceSignalSinusoidal waveformSound PlayUnitSound waveformControl signal+ Figure 2. Overview of our 
system We have already demonstrated our system, with about a hundred people experiencing it to date. 
From the 1 to 30Hz frequency vibration, most people felt a soft tactile sensation to the palms; rather 
like living matter. They described it as a marvelous sensation that something living seemed to be in 
between their palms. As a result of other application using two sensors people felt as if some liquid 
or small object was moving inside the device. This type of feeling of physical dynamics has previously 
been presented by haptic (force) displays; our system, however, just stimulated tactile or skin sensations. 
  Figure 3. Scene of experience Clearly, our system is able to present very rich tactile sensations 
so that users can imagine some sort of a living matter is present. We achieved this by using a temporally 
high, spatially low resolution tactile presentation. References [1] Y.Sekiguchi, K.Hirota and M.Hirose: 
Haptic Interface Using Estimation of Box Contents Metaphor, ICAT2003 (2003). [2] I. Poupyrev, S. Maruyama 
and J. Rekimoto: TouchEngine: A Tactile Display for Handheld Devices, Proc. CHI 2002 (2002), 644-645. 
 [3] H.Yano, M.Yoshie and H.Iwata: Development of a Non-Grounded Haptic Interface Using the Gyro Effect, 
Proc. Haptic Symposium 2003 (2003), 32-39.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401631</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[FingerSight#8482;]]></title>
		<subtitle><![CDATA[fingertip control and haptic sensing of the visual environment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401631</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401631</url>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099860</person_id>
				<author_profile_id><![CDATA[81365596255]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Galeotti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099861</person_id>
				<author_profile_id><![CDATA[81421596376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Samantha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Horvath]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099862</person_id>
				<author_profile_id><![CDATA[81100209584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Roberta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klatzky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099863</person_id>
				<author_profile_id><![CDATA[81421596295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brock]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nichol]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099864</person_id>
				<author_profile_id><![CDATA[81100574658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Siegel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099865</person_id>
				<author_profile_id><![CDATA[81339530484]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stetten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University and University of Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FingerSightTM: Fingertip Control and Haptic Sensing of the Visual Environment John Galeotti1,a, Samantha 
Horvath2,b, Roberta Klatzky3,c, Brock Nichol2,d, Mel Siegel1,e, George Stetten1,2,f 1 Robotics Institute, 
Carnegie Mellon University 2 Department of Bioengineering, University of Pittsburgh 3 Department of Psychology, 
Carnegie Mellon University 1 Introduction Many devices that transfer input from the visual environment 
to another sense have been developed. The primary assistive technologies in current use are white canes, 
guide dogs, and GPS­based technologies. All of these facilitate safe travel in a wide variety of environments, 
but none of them are useful for straightening a picture frame on the wall or finding a cup of coffee 
on a counter-top. Tactile display screens and direct nerve stimulation are two existing camera-based 
technologies that seek to replace the more general capabilities of vision. Notably, these methods preserve 
a predetermined map between the image captured by a camera and a spatially fixed grid of sensory stimulators. 
Other technologies use fixed cameras and tracking devices to record and interpret movements and gestures. 
These, however, operate in a limited space and focus on the subject, rather than the subject s interrogation 
of his environment. With regard to haptic feedback devices for the hand, most existing devices aim to 
simulate tactile exploration of virtual objects. FingerSightTM is the underlying concept for a visual 
sensing device with haptic feedback that allows users to both actively interrogate and sense the 3D environment, 
and to manipulate specific aspects of the environment by gesture. FingerSightTM implementations are not 
necessarily limited to a specific, predetermined environment. The original goal of FingerSightTM was 
to aide the visually impaired. Visual sensing combined with haptic feedback allows users to receive additional 
information about their surroundings without interfering with auditory cues. The introduction of control 
into FingerSightTM has expanded the potential target population to the general public, who could make 
use of it as an intuitive and possibly even enjoyable new form of remote control. The current model, 
shown in Figure 1, utilizes a small finger-mounted video camera to track graphical controls on a computer 
screen and provides vibrotactile feedback to alert the user when they have locked onto a control.  2 
Exposition With FingerSight,TM each finger individually senses and analyzes visual information. Finger, 
wrist, and arm motions replace eye and head movements, controlling the field of view. The camera is mounted 
to the top (dorsal) side of the finger to avoid any interference with the normal operation of the hand. 
The camera image is analyzed in real-time to identify edges, corners, and/or entire objects. The camera, 
coupled with computer vision, allows a e-mail: galeotti+sg@cs.cmu.edu b e-mail: sjh26+sg@pitt.edu e-mail: 
klatzky@cmu.edu d e-mail: ban19+sg@pitt.edu e e-mail: mws@cmu.edu f e-mail: george@stetten.com  for 
flexible identification of objects that would not be possible with a more specific sensing device. Detection 
of an object is relayed to the finger through a haptic feedback device mounted behind the camera. Haptic 
feedback, such as may be provided by a cell phone vibrator, is used to stimulate the finger when key 
objects have been identified. The feedback allows the user to associate the visual object with the finger 
that sees it. In addition to visual sensing, FingerSightTM allows users to remotely control targets with 
finger motions and hand gestures. This could empower the low-vision population to interact with the distal 
environment. Sighted users could manipulate objects beyond their reach, enabling them, e.g., to safely 
interact with remote targets in sterile or hazardous environments, and educators and gamers could use 
the technology in training simulations and virtual reality systems. Real controls might include a light 
switch or doorknob (provided a separate control channel is available), and virtual controls can be directly 
displayed and manipulated on a computer screen. In the presented demo, movement sensed by the camera 
in relation to virtual controls is used to adjust them. Haptic feedback informs the user when they have 
locked on to a control (or set thereof). The user can rotate knobs and move sliders, which could be implemented 
to control arbitrary parameters, such as volume and song selection. 3 Conclusion We are just beginning 
to realize FingerSight sTM potential as a vision substitution and remote manipulation device. As opposed 
to previous visual-tactile methods, FingerSightTM does not depend upon a fixed spatial map between the 
image and the sensory stimulators. Rather, each individual finger explores what amounts to its own receptive 
field in the visual environment. We are working to incorporate multiple cameras into our implementation 
of FingerSightTM to allow for stereo depth perception, more intricate control pattern recognition, and 
enhanced navigational cues. We are also working to improve the haptic feedback. Copyright is held by 
the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401632</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[ForceTile]]></title>
		<subtitle><![CDATA[tabletop tangible interface with vision-based force distribution sensing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401632</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401632</url>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099866</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099867</person_id>
				<author_profile_id><![CDATA[81335492333]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kensei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099868</person_id>
				<author_profile_id><![CDATA[81421594194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Katsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099869</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099870</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099871</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099872</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099873</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1186161</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kamiyama, K., Kajimoto, H., Kawakami, N., Mizota, T., Tachi, S., and Vlack, K. 2004. GelForce. In <i>SIGGRAPH Emerging Technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>263551</ref_obj_id>
				<ref_obj_pid>263407</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ullmer, B., and Ishii, H. 1997. The metadesk: Models and Prototypes for Tangible User Interfaces. In <i>Proceedings of UIST'97</i>, ACM, 223--232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ForceTile: Tabletop Tangible Interface with Vision-based Force Distribution Sensing Yasuaki Kakehi* 
Kensei Jo Katsunori Sato Kouta Minamizawa Hideaki Nii Naoki Kawakami Takeshi Naemura Susumu Tachi * Presto, 
Japan Science and Technology Agency The University of Tokyo Figure 1: Photo Viewer with ForceTiles 
Figure 2: Pinching/stretching interaction Figure 3: Interaction with force vectors 1 Introduction Today, 
placing physical objects on a tabletop display is common for intuitive tangible input [Ullmer and Ishii 
1997]. The overall goal of our project is to increase the interactivity of tabletop tangible inter­faces. 
To achieve this goal, we propose a novel tabletop tangible interface named ForceTile. This interface 
can detect the force dis­tribution on its surface as well as its position, rotation and ID by us­ing 
a vision-based approach. In our previous optical force sensors GelForce [Kamiyama et al. 2004], an elastic 
body and cameras are .xed together. Contrarily, on this system, users can place and move multiple tile-shaped 
interfaces on the tabletop display freely. Furthermore, users can interact with projected images on the 
table­top screen by moving, pushing or pinching the ForceTiles. 2 Technical Innovations of ForceTile 
In our ForceTile, we offer core technical innovations as following. One is the design of the interface 
and the table-based camera system to detect the force distributions of each tiles on the tabletop. The 
tile interface consists of a transparent acrylic case .lled with an elastic body, two layers of markers 
attached within the body for the force distribution sensing and a marker attached underneath the case 
for the position sensing. Inside the table, cameras and IR light emitters are installed underneath. When 
users put ForceTiles on the tabletop, the system detects their locations and ID by the shape of position 
marker. Furthermore, to calculate the force vectors on the surface, we adopt the method of GelForce. 
When forces are applied on the surface of the ForceTile, this system derives the force vectors by detecting 
the internal strain of the body through the movement of the force markers. Note that no electric devices 
are attached on the tabletop interfaces and users need not wear any special equipment for interaction. 
Secondly, this system can show images on the tabletop screen and ForceTile surfaces. By making a force 
marker with a transparent heat insulating material that passes the visible incoming light and blocks 
the infrared incoming light, this system can adopt a back *e-mail: tabletop@hc.ic.i.u-tokyo.ac.jp projection 
so that the interface bodies and user s hands don t disturb the projection light. Third innovation is 
a software architecture design for recognizing users actions and generating projection images. According 
to var­ious input information such as position, rotation, ID of tiles, in­tensity and direction of forces 
applied on them, users can control projected images in real-time. 3 Applications While ForceTile offers 
brand new interactions on tabletop, it can in­tegrate several functions of previous interfaces simultaneously 
such as touch panel, pointing device (i.e. mouse), tangible physical in­terface and small-sized screen. 
We have already implemented some interactive applications. One is photo viewer application (see Figure 
1). Initially photo im­ages are displayed on the tabletop. By pushing the ForceTiles, you can change 
the displayed photo or the image size. In the second ap­plication, you can control the tabletop image 
with multiple .ngers. When you pinch or stretch the tile s surface, the scale of the table­top image 
changes according to the intensity and the direction of the force (see Figure 2). The third application 
is for entertainment. On the tabletop screen, an image of a lady beetle is walking around. When you cover 
the ForceTile on the lady beetle and apply a force on it, it .ys away according to the magnitude and 
direction of the applied force as Figure 3. In the future, we plan to develop much more applications 
by using the ForceTiles in various situations. References KAMIYAMA, K., KAJIMOTO, H., KAWAKAMI, N., 
MIZOTA,T., TACHI, S., AND VLACK, K. 2004. GelForce. In SIGGRAPH Emerging Technologies, ACM. ULLMER, B., 
AND ISHII, H. 1997. The metadesk: Models and Prototypes for Tangible User Interfaces. In Proceedings 
of UIST 97, ACM, 223 232. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401633</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[GhostGlove]]></title>
		<subtitle><![CDATA[haptic existence of the virtual world]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401633</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401633</url>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099874</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099875</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099876</person_id>
				<author_profile_id><![CDATA[81335490472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Souichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukamachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099877</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099878</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lederman, S. J. and Klatzky, R. L. 1987. Hand-movements: A window into haptic object recognition. <i>Cognitive Psychology</i>, 22, 421--459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278289</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, S. Fukamachi, K., Kajimoto, H., Kawakami, N. and Tachi, S. 2007. Gravity Grabber: Wearable Haptic Display to Present Virtual Mass Sensation, In <i>Proceedings of ACM SIGGRAPH 2007</i>, Emerging Technologies.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GhostGlove: Haptic Existence of the Virtual World Kouta Minamizawa* Sho Kamuro Souichiro Fukamachi Naoki 
Kawakami Susumu Tachi The University of Tokyo  Encounter Pressure Contour following Stroke Grasping 
Bimanual enclosure Unsupported holding (Position) (Hardness) (Surface shape) (Texture) (Global shape) 
(Volume) (Weight) Figure 1: The categorized behaviors and perceptions that are represented by our proposed 
method. 1. Introduction In the real world, we usually do not perceive haptic sensations consciously. 
However, if the haptic sensations are absent or if they are not of the desired quality, it would result 
in discomfort, and the existence of the world would be degraded. In order to eliminate the feelings of 
discomfort and represent genuine feelings of the existence of objects in virtual reality interactions, 
as shown in figure 2, we show that haptic sensations play a decisive role in inciting cognitive functions 
in a virtual environment. There are many researches on haptic displays for virtual reality interactions 
with the hand, such as CyberTouch and CyberGrasp [Immersion Corp.]. However, the represented sensation 
is still inadequate to provide us with the experience of feeling the definite existence of virtual objects 
or virtual creatures. Generally, it is considered that a complex and expensive device is required to 
provide realistic haptic sensations; many attempts to simplify the device would result in poor sensations, 
for example, simple vibrations. To merge the simplicity of the device and realistic sensation of existence, 
we propose a novel wearable haptic interface named GhostGlove. This device generates natural and realistic 
haptic sensations over the entire hand on each finger and the palm and integrates the perceptions on 
the entire hand along with the visual sensations to enables us to recognize the existence of the virtual 
world. 2. Method Usually, the palm is used for a rough and quick recognition of the size and shape of 
an object, while the fingers are used for dexterous manipulations. We found that the interactive combination 
of the cutaneous perceptions on the palm and fingers is effective in the haptic recognition of the objects. 
Based on the study on exploratory procedures [Lederman and Klatzky 1987], we categorized the behavior 
in human-object interactions by focusing on the relative motion of the palm and the object as shown in 
figure 1. The direction of the motions represented by Encounter and Pressure are perpendicular to the 
object surface, while those represented by Contour following and Stroke are lateral to the surface. Bimanual 
enclosure, grasping, and unsupported holding are examples of static contact. We then implemented the 
prototype device shown in figure 3 to reproduce the corresponding cutaneous perceptions in each relative 
motion. The mechanism for the reproduction of the sensation on each finger and the palm is based on our 
unique technology to provide significantly realistic sensations of touch and the dynamics of * e-mail: 
kouta_minamizawa@ipc.i.u-tokyo.ac.jp Figure 2: Conceptual representation of our proposed system. The 
user with HMD and the devices on both hands can see and touch the VR object. The haptic sensation is 
represented interactively corresponding to the user's motion. Figure 3: Prototype device and closeup 
of index finger virtual objects [Minamizawa et al. 2007]. By reproducing the vertical and shearing forces 
on the fingerpad by this simple mechanism, the device could generate high-responsive and precise haptic 
interactions of the dynamic motion of the virtual objects. We extended this method to the entire hand 
including the palm, and also confirmed the effectiveness of bimanual coordination particularly in the 
perception of volume and surface. 3. Summary We proposed a novel haptic interface to be worn over the 
entire hand. This interface allows realistic and responsive haptic interactions in virtual reality environments 
via a simple method that considers the importance of the unconscious perception of haptic sensations 
for recognizing the existence of the external world. GhostGlove provides an unprecedented feeling of 
the dynamics of virtual objects and the vitality of virtual creatures. References LEDERMAN, S. J. AND 
KLATZKY, R. L. 1987. Hand-movements: A window into haptic object recognition. Cognitive Psychology, 22, 
421-459. MINAMIZAWA, S. FUKAMACHI, K., KAJIMOTO, H., KAWAKAMI, N. AND TACHI, S. 2007. Gravity Grabber: 
Wearable Haptic Display to Present Virtual Mass Sensation, In Proceedings of ACM SIGGRAPH 2007, Emerging 
Technologies. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 
11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401634</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[IncreTable, bridging the gap between real and virtual worlds]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401634</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401634</url>
		<abstract>
			<par><![CDATA[<p>IncreTable is a tabletop game inspired by the Incredible Machine. It provides a multi-modal interaction based on a bi-directional projection tabletop, digital pens, a depth camera, and custom-made physical objects, while attracting users for an active participation in creating content which diffuses the boundary between the real and virtual world.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Computer-supported cooperative work</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.3</cat_node>
				<descriptor>Collaborative computing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003567.10003570</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing and business->Computer supported cooperative work</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003130.10003131.10003570</concept_id>
				<concept_desc>CCS->Human-centered computing->Collaborative and social computing->Collaborative and social computing theory, concepts and paradigms->Computer supported cooperative work</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1099879</person_id>
				<author_profile_id><![CDATA[81319495417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jakob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leitner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria Univ. of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099880</person_id>
				<author_profile_id><![CDATA[81361594945]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brandl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria Univ. of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099881</person_id>
				<author_profile_id><![CDATA[81319500586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seifried]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria Univ. of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099882</person_id>
				<author_profile_id><![CDATA[81100048383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria Univ. of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099883</person_id>
				<author_profile_id><![CDATA[81413601479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kyungdahm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099884</person_id>
				<author_profile_id><![CDATA[81100535951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Woontack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099885</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099886</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1006068</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Magerkurth, C., Memisoglu, M., Engelke, T., and Streitz, N., Towards the next generation of tabletop gaming experiences. In GI '04: Proceedings of the 2004 conference on Graphics interface, pages 73--80, Ontario, Canada, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wilson, A. Depth-Sensing Video Cameras for 3D Tangible Tabletop Interaction. Tabletop 2007: The 2nd IEEE International Workshop on Horizontal Interactive Human-Computer Systems, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095047</ref_obj_id>
				<ref_obj_pid>1095034</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wilson, A. PlayAnywhere: A Compact Tabletop Computer Vision System, Symposium on User Interface Software and Technology (UIST), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 IncrreTablee, Bridging thhe gap betweeen reaal and vvirtual worldds Jakob LLeitner, Petter Brandl, 
KKyungdahmm Yun, Maki Sugimoto , Masahikoo Thomas SSeifried, Miichael Halleer WWoontack Woo Inammi Upper 
Austriia Univ. of Appplied Sciencces GIST Universityy of Electro -CCommunicattions mi-labb@fh-hageenberg.at 
kkyun@gist..ac.kr inami@ina mi.info  ABSTRACCT IncreTable is aa tabletop gamee inspired by thee Incredible 
Maachine. It provides a multi-modal innteraction base d on a bi-direectional projection tablletop, digital 
ppens, a depth camera, and c ustom­made physica l objects, whiile attracting users for an active participation 
inn creating conntent which diiffuses the bo undary between the reaal and virtual wworld. Categoriess 
and Subjeect Descrip tors H.5.1 [Informmation Interfaces and Presenntation]: Multtimedia Information 
SSystems [HH.5.3]: Collaboorative compuuting Computer-suppported coopeerative work K.8.0 [Peersonal Computing]: 
GGeneral gamess. Keywords Mixed Reality,, Interaction dessign, tabletop ggaming, tangiblee user interfaces, 
pervvasive games. 1. INTROODUCTIONN In order to impprove the sociaal gaming experrience, Magerkkurth 
et al. proposed a tabletop setup which combinees the advantagges of a digital environnment with the social 
impact of board gammes [1]. Wilson demon strated PlayAnnywhere, a flexiible and transpportable tabletop 
projecction setup [2]. Wilson also prresented the paiiring of a depth-sensingg camera with aan interactive 
taabletop to creatte a car racing game inn which virtual cars raced realiistically over phhysical objects 
placedd on the table s surface [3]. Motivated byy these tabletop gamess, we developedd IncreTable. 2. 
INCREETABLE Inspired by thhe Incredible Machine, the general objecttive of IncreTable is to arrange a 
ggiven collectionn of virtual annd real objects in a coomplex fashion sufficient to pperform a simplle 
task. Each level preesents a puzzlee requiring muulti-modal inteeraction provoking userr creativity. 
Diggital pens can b e used to place virtual domino blockss. A depth ZSennse camera fromm 3DV Systemms, 
Ltd. is used to reecognize physi cal objects (oobstacles) and users gestures. The rresult, an 8 bit 320x240 
depthh image influennces the digital terrain. Players can uuse any kind of physical obbjects (ffolded 
paper, rramps etc.) to modify the terrrain. A projecctor is used to display tthe additional ccontent 
(e.g. a jjumping ball) oon the suurface. When tthe digital balll, for example,, moves over aa real raamp, 
it jumps appropriately. In addition, small robots move arround the tabl e interacting aagain with virttual 
and real iitems. TThese robots hhave embeddedd brightness seensors, which track prrojected markeers. 
Finally, sppecial physicall objects, so ccalled pportals , are uused to connecct the virtual wworld with 
thee real wworld. Using thhese portals, thee real domino bbricks, for exaample, caan be knockeed over 
by thhe virtual onees and vice vversa. Summarizing, InncreTable has thhe following noovel features: 
Demonsttration of multti-modal interacction based onn new technologgies  The provvision of new exxperiences 
disssolving the bounndary between virtual and reall worlds  User-gennerated contennt through muulti-user, 
interaactive interface s  Bi-directtional projectioon setup that aallows content to be displayedd in 
multiple levvels. TThe amalgamation of the reaal and virtual world throughh our teechnological deevelopments 
alllow for a new, unparalleled gaaming exxperience. Morre details, illusttrating our systeem in action, 
ccan be foound online at: ­ http://www.mi-lab.org/.  RREFERENCCES [11] Magerkurth Towards th In GI 
04: interface, p h, C., Memisog he next generati Proceedings of ages 73 80, Onlu, M., Engelke ion of 
tabletop f the 2004 con ntario, Canada, e, T., and Streit gaming experie ference on Gra 2004. tz, N., 
ences. aphics [22] Wilson, A. Tabletop Internationa Computer S Depth-Sensing Interaction. T al Workshop 
o Systems, 2007.g Video Camer Tabletop 2007 on Horizontal ras for 3D Tan : The 2nd Interactive Hu ngible 
IEEE uman­ [33] Wilson, A. Vision Syst Technology PlayAnywher tem, Symposiu y (UIST), 2005 re: A Compact 
um on User Int . Tabletop Com terface Softwar mputer e and Copyright is held by the author / owner(s). 
SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401635</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Infinite 4D fish]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401635</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401635</url>
		<abstract>
			<par><![CDATA[<p>This work originated a unique idea of imaginary multidimensional fish by integrating CG and real sculpture. Making an CG animation which includes a tentacle-like fish, and a Lenticular of it, then transform it in a real sculpture with vivid color. There happen an intersection between each dimension as hyper, 4D sculpture.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[lenticular]]></kw>
			<kw><![CDATA[neural-sculpture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099887</person_id>
				<author_profile_id><![CDATA[81100605028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Infinite 4D Fish Yoichiro Kawaguchi The University of Tokyo yoichiro@iii.u-tokyo.ac.jp   ABSTRACT 
This work originated a unique idea of imaginary multidimensional fish by integrating CG and real sculpture. 
Making an CG animation which includes a tentacle-like fish, and a Lenticular of it, then transform it 
in a real sculpture with vivid color. There happen an intersection between each dimension as hyper, 4D 
sculpture. Keywords Lenticular, neural-sculpture. DESCRIPTION Infinite 4D fish is a unique, new mixture 
of sculpture, CG and Lenticular 3D picture. Making an CG animation which includes a tentacle-like fish, 
and a Lenticular of it, then transform it in a real neural sculpture with vivid color. There happenan 
intersection between each dimension as hyper, 4D sculpture. People can see and even walk around inbetween 
those elements like Lenticular 3D panel and real sculpture itself and enjoy the gradually changing images 
of imaginary fish. Also the lenticular has some massive real structures on its surface, and it shows 
a kind of transformation from 2D animation to lenticular 3D picture, and more hyper dimensional expression 
of 4D world.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401636</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Landscape bartender]]></title>
		<subtitle><![CDATA[landscape generation using a cocktail analogy]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401636</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401636</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099888</person_id>
				<author_profile_id><![CDATA[81421593485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Noda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099889</person_id>
				<author_profile_id><![CDATA[81421595104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kentarou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nomura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099890</person_id>
				<author_profile_id><![CDATA[81421594183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Komuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099891</person_id>
				<author_profile_id><![CDATA[81414592984]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099892</person_id>
				<author_profile_id><![CDATA[81421595106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099893</person_id>
				<author_profile_id><![CDATA[81100338478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kazunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Landscape Bartender: Landscape Generation Using a Cocktail Analogy Takahiko Noda Kentarou Nomura Naoyuki 
Komuro Tao Zheng Chen Yang Kazunori Miyata Japan Advanced Institute of Science and Technology   (a) 
Daytime     (b) Evening      (c) Night     (d) System Overview Figure 1 Examples and system 
overview: A change of the sky due to the ratio of the moon and the sun. 1 Introduction  There are 
some cocktails that are likened to landscapes, e.g. tequila sunrise. It likens orange juice and grenadine 
to the morning sky with glow of the sun. These two elements generate a landscape of sunrise. We present 
a system that generates landscapes using a cocktail analogy. 2 System Overview  Figure 2 shows the flow 
of player s experience. The player generates a landscape in the same manner as making a cocktail. Each 
ingredient of the landscape cocktail, each landscape element, is water in a separate bottle. The player 
picks a bottle containing the intended landscape element, and pours a suitable amount of water into a 
shaker. The system has eight elements; sand, rock, water, plants, sun, moon, star, and cloud. The elements 
of landscape are categorized into two attributes; a) soil: sand, rock, water, and plant, b) sky: sun, 
moon, star, and cloud. The mixture ratio of these two categorized elements determines the ratio of ground 
and sky in the image. The amount of water used from each bottle determines the ratio of landscape elements. 
And then, the relief of ground and the position of each element is changed by shaking the shaker. To 
give the player the feeling of generating his/her own landscape, the system displays a vague in-progress 
image while he/she is shaking. The in-progress image is unclear, and the contents of the shaker can t 
be seen. After shaking the shaker,the player pours water into the cocktail glass. The landscape result 
can only be seen clearly when the cocktail glass is placed on the coaster.  Figure 2 Flow of player 
s experience 3 Implements  The system consists of four modules as shown in Figure 2; 1) shaker-type 
controller with wireless three-axis accelerometer, 2) measuring module for sensing the volume of water, 
3) counter-type image display unit, and 4) PC. Figure 1 (d) shows the system overview. The stainless 
shaker is used as the body of the controller. The wireless three-axis accelerometer (Hitachi Metals: 
H48C) is hidden inside the cap of the shaker. The acceleration data is used for changing the relief of 
ground and the position of each element. The system consists of eight digital scales (DRETEC: R-209) 
for measuring the volume of water. Each scale is used for sensing the amount of each landscape element, 
and the data is used for the compounding process. The sensing module for detecting the placement of glass 
on the coaster is installed in the counter-type image display unit. A magnetic chip is put on the base 
of the glass. The digital compass is used as the sensing module, and the module detects the approach 
of the glass. The data from each module is transmitted to the PC via serial connection and wireless signal. 
 Figure 3 System configuration 4 Generation of the Landscape  4.1 Landscape recipe The elements of landscape 
are categorized into two attributes. We recommend using at least 1 element from each attribute. The relief 
of ground depends on the mixture ratio of sand and rock. The relief becomes smooth when the ratio of 
sand element is high, otherwise it becomes rough. The plants are grown only if there are water and sand/rock. 
The ratio of sun and moon changes over time with the altitude of sun/moon. If the ratio of sun element 
is high, the scene is generated in daytime as shown in Figure 1(a), otherwise the scene is a night scene, 
as shown in Figure 1(c). If the ratio of sun and moon is even, the scene is generated as evening, as 
shown in Figure 1(b). The volume of stars and clouds relates to the volume of each element. A recipe 
book is available for users to generate their preferred landscapes. 4.2 Shaker control The relief of 
ground becomes rough if the controller is shaken vertically, and becomes smooth if it is shaken horizontally. 
The sun shifts horizontally if the controller is shaken horizontally. This system provides the enjoyment 
of creating one s own favorite scenery using a cocktail analogy. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401637</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Latte art machine]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401637</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401637</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099894</person_id>
				<author_profile_id><![CDATA[81421595197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oleksiy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pikalo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OnLatte]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schomer, D. 1994. Latte art 101. <i>CoffeeTalk</i> (Dec.), 13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wikipedia, 2008. Latte art --- Wikipedia, the free encyclopedia. {Online; accessed 27-February-2008}.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Latte Art Machine OleksiyPikalo* OnLatte  Figure 1: Example of latte art graphics produced by our 
machine. 1 Introduction Latteartrefers tographicdesignson thesurfaceof espressobased drinks. By manipulating 
the .ow of milk from a jug into the espresso, barista may form several different patterns, including 
rosettas, leaves and hearts [Schomer 1994]. It is also possible to further enhance the pattern using 
stencils, powder and milk foam[Wikipedia2008]. Unsatis.ed with the limited choiceofpatterns,wehavedecided 
to investigatealternativemeansof reproducingdesignson thesurface of variety espresso-based drinks. After 
experimenting with differ­ent automated etching and pouring techniques with some success, wehavebuiltaprototype 
that usesamodi.ed inkjetprint cartridge toinfuse tinydropletsof colorantintotheupperlayerofthebever­age.Asit 
turnsout,themicro-bubbled textureofthebeveragepro­vides suf.cient isolation and stability for small droplets 
deposited using traditional inkjet technologies. The resulting images are sus­tainableforperiods overhalf 
anhour. 2 Exposition Atpresent time,latteart machineconsistsofthecartridgemounted on computer controlled 
X-Y translation stage. The stage responds directly tocommandssentthroughGPIBinterface,withnozzle.r­ing 
control being synchronized to the motion. Essentially it is a latter artplotter, capable of reproducing 
vectorgraphics,but we are activelyworking onbuilding asmallerandfasterversionof theap­paratuswithastandardprinting 
interface. After experimenting with different colorants, we discovered that although itispossible todisplayfaintimagesusing 
coffeedirectly, the contrast is much better using edible inks readily available from severalgourmet companies. 
The objective of our demo is to engage conference attendees into latte art printing process directly. 
Using our latest prototype con­nected directly to the graphics software makes drawing on top of lattepossiblewithinminutes,andtherearenospecialbarista 
tricks tolearn.Aspart ofthedemo,wemayinstallWACOM ctabletto &#38;#169; renderquick sketchesinreal time.Bydefault,itwillbepossibleto 
display current conference schedule highlights on latte, the context being updated everyhour. Strategically 
placed right next to the coffee vendor at the confer­ence, we welcome experimental designs on other beverages. 
And our own automatic coffee center will be continuously self-serving plain lattes throughout the conference, 
providing limitless supply ofblank media. 3 Conclusion To our best knowledge no other latte art printing 
machines have beenbuilt todate.Itisourhopeisthatwiththeriseinpopularityof latteart and artisticpresentation 
in thecup, this technology will .nd its way to the commercial world in one or two years. Meanwhile, our 
apparatus could also inspire artists and engineers to develop innovative artworks and application. References 
SCHOMER, D. 1994. Latte art101. CoffeeTalk (Dec.),13. WIKIPEDIA, 2008. Latte art Wikipedia, the free 
encyclopedia. [Online;accessed27-February-2008]. * e-mail: opikalo@onlatte.com Copyright is held by the 
author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401638</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Maglev haptics]]></title>
		<subtitle><![CDATA[butterfly haptic's new user interface technology]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401638</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401638</url>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099895</person_id>
				<author_profile_id><![CDATA[81100381625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ralph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hollis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Maglev Haptics: Butterfly Haptic's New User Interface Technology Ralph Hollis Carnegie Mellon University 
Butterfly Haptics, LLC  1 Introduction There is an increasing interest in haptic (sense of touch) 
interaction with computers. A new kind of haptic system has recently been developed at Carnegie Mellon 
University and commercialized by Butterfly Haptics. The system eliminates the bulky links, cables and 
general mechanical complexity of other haptic devices on the market today in favor of a single lightweight 
moving part that floats on magnetic fields. At the heart of the maglev haptic interface is a bowl-shaped 
assembly called a flotor that has six embedded coils of wire. Electric current flowing through the coils 
interacts with powerful permanent magnets, causing the flotor to levitate. A handle is attached to the 
flotor. A user moves the handle much like a computer mouse, but in three dimensions with six degrees 
of freedom up/down, side to side, back/forth, yaw, pitch, and roll. Optical sensors measure the position 
and orientation of the flotor, and this information is used to control the position and orientation of 
a 3D virtual object. As this virtual object encounters other virtual surfaces and objects, currents are 
sent to the flotor s coils, resulting in haptic feedback to the user. The new system provides extremely 
high performance levels resulting in a very high fidelity user experience. 2 Exposition Several of 
the new maglev haptic systems at SIGGRAPH 2008 New Tech Demos feature demonstrations of one- and two-handed 
haptic interaction with simple 3D visual/haptic virtual environments. These include moving a box within 
a box, moving a probe over sinewave and dithered textures, touching and navigating around a large (70,000 
triangle) object, and the enjoyment of a haptic playground populated with balls, cubes, and a teeter-totter. 
3 Conclusion The new systems were developed under NSF Major Research Instrumentation grant EIA-0321057, 
and represent dramatic improvements and lower costs compared to an earlier prototype system, also developed 
with NSF funding. Ten of the new systems were produced, with six systems going to leading haptics researchers 
at Purdue, Harvard, Stanford, Cornell and the universities of Utah and British Columbia. The Magnetic 
Levitation Haptic Consortium was formed to help foster research and collaboration with this new form 
of interaction. Current magnetic levitation haptic research at Carnegie Mellon includes psychophysical 
studies of hard contact, texture and deformable object perception; teleoperation, micromanipulation of 
biological materials, and control of a six-legged running robot. Potential future applications of maglev 
haptics include character animation, visualization of complex multi-dimensional data sets, virtual surgery 
and virtual dentistry, computer augmented design, micro- and nano-manipulation, and teleoperation of 
remote robot arms and vehicles. * * ralph@butterflyhaptics.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401639</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Matsumoto-jo]]></title>
		<subtitle><![CDATA[a virtual 16th century Japanese castle]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401639</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401639</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099896</person_id>
				<author_profile_id><![CDATA[81365598392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amakawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[STUDIO Amakawa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401640</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[MeisterGRIP]]></title>
		<subtitle><![CDATA[cylindrical interface for intuitional robot manipulation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401640</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401640</url>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor>Operator interfaces</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor>Manipulators</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Command and control</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Control theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010553.10010554.10010555</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics->Robotic components</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010481.10010482.10010486</concept_id>
				<concept_desc>CCS->Applied computing->Operations research->Industry and manufacturing->Command and control</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010214</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Computational control theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554.10010558</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics->External interfaces for robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099897</person_id>
				<author_profile_id><![CDATA[81421598078]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Komeiji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099898</person_id>
				<author_profile_id><![CDATA[81331503619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099899</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099900</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099901</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099902</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bar-Cohen, Y. 1999. Haptic Interfaces. In <i>Automation, Miniature Robotics and Sensors for Nondestructive Evaluation and Testing</i>, Volume 4 of the Topics on NDE Series, ASNT, Columbus.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kamiyama, K., Kajimoto, H., Kawakami, N., and Tachi, S. 2004. Evaluation of a Vision-based Tactile Sensor, In <i>Proceedings of International Conference on Robotics and Automation (ICRA2004)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MeisterGRIP: Cylindrical Interface for Intuitional Robot Manipulation Shuji Komeiji*1 Katsunari Sato*2 
Kouta Minamizawa*3 Hideaki Nii*4 Naoki Kawakami*5 Susumu Tachi*6 The University of Tokyo 1 Introduction 
In recent times, robot manipulations have been used to perform operations in extreme environments, communicate 
with people in remote locations, and realize entertainment systems that control virtual robots. In all 
these activities, an interface for robotic hand and arm manipulations is important to interact with the 
external environment [Bar-Cohen, 2000]. Conventional interfaces consist of a few levers and switches; 
unfortunately, such interfaces are difficult to control, and the users must have prior experience as 
far as the operation of these interfaces is concerned. The other type of interfaces reflects the postures 
of the user s hand and arm directly in the robotic hand and arm so that the robots are capable of intuitive 
manipulation. However, these devices have a trouble of wearing and restrict the hand size of the user. 
We believe that an interface for robotic hand manipulations should be developed in such a manner that 
anyone can use it intuitively and easily. Therefore, we developed a novel interface called MeisterGRIP. 
This device measures the user s grasping conditions and reflects this information in the robotic hand 
and arm control. The user is simply required to grasp the device; therefore, the complexity of the setup 
is reduced and there are fewer restrictions on the user. Furthermore, by reflecting the grasping force 
of the user directly in the robotic hand and arm, intuitive manipulation is possible. MeisterGRIP allows 
robot manipulations to be easily conducted, and it can be widely used in general households. In the future, 
it will be possible to travel and interact with objects at remote locations by using our proposed device, 
even if the user is sitting on a sofa in his/her living room. 2 Principle In order to measure the user 
s grasping conditions, the force vector distribution caused by the user s grasping is used. When a user 
grasps our proposed device, the device recognizes the five fingers and the palm of the user as six input 
positions from the pattern of the force vector distribution. Then, the three-axis force vectors are measured 
at each input point and relayed to each finger and wrist of the robotic hand . The input points could 
be set at any position on the device surface; this allows the user to grasp the device at any position 
in any posture. The individual differences among the hand sizes are also cleared by this feature. In 
order to measure the force vector distribution, we applied our vision-based tactile-sensing technology 
[Kamiyama, et al., 2004]. The force vector distribution is calculated by capturing the movement of colored 
markers on the surface of an elastic body with a camera. MeisterGRIP has a cylindrical shape to support 
any grasping posture, as shown in Figure 1. Two CCD cameras and two spherical mirrors are attached at 
the end and the center of the cylindrical body, respectively. The mirrors enable us to capture *1E-mail: 
shuji_komeiji@ipc.i.u-tokyo.ac.jp *2E-mail: katsunari_sato@ipc.i.u-tokyo.ac.jp *3E-mail: kouta_minamizawa@ipc.i.u-tokyo.ac.jp*4E-mail: 
hideaki_nii@ipc.i.u-tokyo.ac.jp *5E-mail: kawakami@star.t.u-tokyo.ac.jp *6E-mail: tachi@star.t.u-tokyo.ac.jp 
all the markers on the inner surface of the cylindrical body using a few cameras.  Figure 2: Manipulation 
of a robot system using MeisterGRIP. 3 Summary and Application We propose a novel cylindrical interface 
called MeisterGRIP that treats the force vector distribution. This elastic device allows institutional 
and dexterous robot manipulation based on vision-based tactile-sensing technology. Furthermore, it provides 
universal manipulation that can tolerate the personal differences in hand sizes and grasping postures 
of the users. This device is proposed for use in a cockpit to manipulate a robot in both real and virtual 
environments (Figure 2). We attached a stick to the device and fixed it to the ground. By using the force 
information measured from MeisterGRIP, the user can manipulate not just the robotic hand, but the robotic 
arm. To manipulate the robotic arm, we use the six-axis force information caluculated from force vector 
distribution. Using this device, the users would feel as if they become a robot in remote environment 
by only the information of user s grasping. References BAR-COHEN, Y. 1999. Haptic Interfaces, In Automation, 
Miniature Robotics and Sensors for Nondestructive Evaluation and Testing, Volume 4 of the Topics on NDE 
Series, ASNT, Columbus. KAMIYAMA, K., KAJIMOTO, H., KAWAKAMI, N., AND TACHI, S. 2004. Evaluation of a 
Vision-based Tactile Sensor, In Proceedings of International Conference on Robotics and Automation (ICRA2004). 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401641</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[mHashup]]></title>
		<subtitle><![CDATA[fast visual music discovery via locality sensitive hashing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401641</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401641</url>
		<categories>
			<primary_category>
				<cat_node>I.5.3</cat_node>
				<descriptor>Similarity measures</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Pattern analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor>Signal analysis, synthesis, and processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor>Systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010750.10010762.10010767</concept_id>
				<concept_desc>CCS->Hardware->Robustness->Hardware reliability->Signal integrity and noise analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010258.10010260.10003697</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Learning paradigms->Unsupervised learning->Cluster analysis</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099903</person_id>
				<author_profile_id><![CDATA[81421596557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths Digital Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099904</person_id>
				<author_profile_id><![CDATA[81100086657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Casey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths Digital Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099905</person_id>
				<author_profile_id><![CDATA[81320494057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rhodes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths Digital Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2210157</ref_obj_id>
				<ref_obj_pid>2209799</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Casey, M., Rhodes, C. and Slaney, M. Analysis of Minimum Distances in High Dimensional Musical Spaces, <i>IEEE Transactions on Speech, Audio and Language Processing</i>, accepted for publication.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Slaney, M. and Casey, M. 2008. Locality Sensitive Hashing for Music Audio, <i>IEEE Signal Processing Magazine</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 mHashup: Fast Visual Music Discovery via Locality Sensitive Hashing Michela Magas . Michael Casey Christophe 
Rhodes Stromatolite Design Lab Goldsmiths Digital Studios Goldsmiths Digital StudiosGoldsmiths Digital 
Studios Figure 1: mHashup Introduction Millions of tracks in music download services pose a problem: 
how to find dark media those items for which little is known but that users may want to find. The problem 
is similar for music lawyers and musicologists: how to identify closely matching music fragments in large 
recorded music collections. mHashup is a novel visual interface to large music collections, such as today 
s million-song download services, for discovering musical relationships among tracks. Users engage in 
direct on-screen query and retrieval of music fragments in an instantaneous feedback flow performed by 
a locality-sensitive hash table in secondary storage.  mHashup The mHashup interface was constructed 
by analysing user interac­tions with complex audio search processes and selectively identify­ing the 
salient elements of the query formation, search and retrieval processes. A system of visual symbols, 
representing the audio query and results, with corresponding auditory displays was designed to facilitate 
enhanced interaction with the search engine. The visual interaction design of mHashup makes content-based 
search intuitive. The mHashup visual language consists of track bars with relative lengths corresponding 
to the temporal extent of each track. Within each mHash bar, the start point and end point of a selected 
music fragment, called an audio shingle, are highlighted and afford direct manipulation to change the 
location and extent of the selection. The shingles are short-windowed segments of a track, usually extend­ing 
over 1s-10s of audio. The user triggers a search via a search button; the results consist of a display 
of similar tracks to the user's query with matching shingle locations within the tracks highlighted as 
new query handles. The track displays are aligned according to the best matching time-point within each 
track. This affords ease of visual inspection of the time positions of the retrieved tracks with respect 
to the .email: michela@stromatolite.com email: m.casey@gold.ac.uk email: c.rhodes@gold.ac.uk query track. 
The user selects a shingle handle within any one of the result tracks to hear the match and by activating 
the search button the user iterates the search process; this can be performed in a feedback cycle affording 
rapid traversals of musical geodesics in the audio similarity space. Matching specific audio content 
requires temporal features. We use a combination of cepstral coefficients and chroma features extracted 
from the audio in short time intervals of 100ms. An audio shingle consists of a concatenation of audio 
features into high-dimensional feature vectors. The system uses Euclidean distance in the normed audio 
shingle space to retrieve similar tracks; a locality sensitive hashing algorithm (LSH) performs the retrieval 
with sublinear time complexity in the total number of shingles; 100ms per retrieved track for a database 
of 1.4 million shingles compared with 2 hours per track using linear distance computation. The degree 
of closeness of similar tracks is visually represented by the use of greater or lesser transparency in 
the track display. A memory-store is generated for each query enabling a return to a previously chosen 
query pathway and a jumping-off point for new searches. Conclusion mHashup facilitates both professional 
music use cases, such as musicologists and copyright lawyers seeking the origins of sampled music with 
location markers precisely given for each returned track, and end-user music applications, such as discovery 
of dark media by its relationship to known hot items. Applica­tions include locating sampled audio; un-mixing 
remixes; finding cover songs; and spotting musical influences. mHashup's visual interface reflects the 
core functionality of a content-based search engine as a visual grammar to be explored by direct manipulation. 
 References CASEY, M., RHODES, C. and SLANEY, M. Analysis of Minimum Distances in High Dimensional Musical 
Spaces, IEEE Transactions on Speech, Audio and Language Processing, accepted for publication. SLANEY, 
M. and CASEY, M. 2008. Locality Sensitive Hashing forMusic Audio, IEEE Signal Processing Magazine. Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401642</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Mobile, dexterous, social robots for mobile manipulation and human-robot interaction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401642</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401642</url>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003241.10003243</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Decision support systems->Expert systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099906</person_id>
				<author_profile_id><![CDATA[81100258451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cynthia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breazeal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099907</person_id>
				<author_profile_id><![CDATA[81100574653]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Siegel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099908</person_id>
				<author_profile_id><![CDATA[81100380885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099909</person_id>
				<author_profile_id><![CDATA[81100403038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jesse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gray]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099910</person_id>
				<author_profile_id><![CDATA[81100500609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Rod]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grupen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UMass Amherst]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099911</person_id>
				<author_profile_id><![CDATA[81343491507]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deegan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DigitROBOTICS, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099912</person_id>
				<author_profile_id><![CDATA[81545263256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Meka Robotics, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099913</person_id>
				<author_profile_id><![CDATA[81421596469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Kailas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narendran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xitome Design, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099914</person_id>
				<author_profile_id><![CDATA[81100572473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McBean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xitome Design, LLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mobile, Dexterous, Social Robots for Mobile Manipulation and Human-Robot Interaction Cynthia Breazeal* 
Rod Grupen Patrick Deegan JeffWeber§ Kailas Narendran¶ Michael Siegel UMass Amherst DigitROBOTICS, 
LLC Meka Robotics, LLC John McBean Matt Berlin Xitome Design, LLC Jesse Gray MIT Media Lab 1 Introduction 
The Personal Robots Group at the MIT Media Lab (URL: robotic.media.mit.edu) is leading the development 
of a new class of robots that feature a novel combination of mobility, dexterity, and human-centric communication 
and interaction skills that sets a new standard for research into personal robots. We refer to this class 
of robots as MDS for Mobile-Dexterous-Social. The purpose of this platform is to support research and 
education goals in human-robotinteraction and mobile manipulation with ap­plications that require the 
integration of these abilities. In partic­ular, our research aims to develop personal robots that work 
with people as capable teammates to assist in eldercare, healthcare, do­mestic chores, and other physical 
tasks that require robots to serve as competent members of human-robot teams. The robot s small, agile 
design is particularly well suited to human-robot interaction and coordination in human living spaces. 
Our collaborators include the Laboratory for Perceptual Robotics at the University of Mas­sachusetts 
at Amherst, Xitome Design, MekaRobotics, and digitROBOTICS. Figure 1: The MDS robot (left). The MDS 
robot coordinating joint action with a person in a VICON space (right). 2 Design Highlights The total 
height of the MDS robot is approximately 48 inches, and thetotalweightisroughly65lbs(in tetheredpowermode).Apho­tograph 
of the MDS robot withface, neck, and shoulder shells is shown in Figure 1. The MDS mobile basebuilds 
upon the uBot-5platform developed at the University of Massachusetts at Amherst. The uBot-5 system features 
two wheels and a dynamically balancing control system, as well as two arms that employ force feedback. 
The arm design supports a large diameter workspace, including a sizeable reach for objects on the .oor 
and two-handedmanipulation of objects. The *e-mail: cynthiab@media.mit.edu e-mail:grupen@cs.umass.edu 
e-mail:pdeegan@digitrobotics.com §e-mail:jaweber@mekabot.com ¶e-mail:kailas@xitome.com mobile base allows 
the robot to turn in place and move forward and reverse at human walking speeds in indoor environments. 
The bal­ancing system can overcome signi.cant impulse forces to support stability critical behaviors 
such as pushing and throwing a ball. A statically stable version of the base is showcased on the MDS 
robot at the SIGGRAPHNewTech Demos. In addition, the MDS robot currently has two2degree of freedom hands 
to support pointing gestures and simple object manipulation. The .ngers are designed using a cast rubber 
technique to be re­silient tofalls and collisions with objects. The .ngers can compli­antly close around 
target objects using a power grasp, simplifying the control required for interaction with objects. In 
the next genera­tionhand(on displayattheNewTech Demo),theindex .ngerand thumb are independently controlled 
to enable the robot to perform a wider range of gestures and object manipulations. Aslip clutch in the 
wrist of the2 degree of freedom forearm makes the wrists robust to impact forces. A particularly distinguishing 
aspect of the MDS design is the robot s sociallyexpressive4degreeof freedom neck anda17 de­greeof freedomface 
 includinggaze,eyelids,eyebrows,andajaw. Theface and neck design supportsa wide rangeof emotional and 
dialog-based expressions. The robot has multiple on-board sensing technologies for visual, auditory, 
and kinesthetic feedbackto allow the robot to interact in real-time with the environment and with people. 
A four micro­phone array in the head supports sound localization. Alaser range .nder mounted in the torso 
supports navigation and environment mapping. Each eye has a color .rewire camera with a 6mm mi­crolens.Areal-time3DIR 
depth-sensing camerais mountedin the forehead. High resolution encoders and current sensing exist on 
all joints. All sensors use a uniform xmlrpc basednetwork interface. An on-board network supports both 
on-board and off-board com­putation. Three 1GHz PC-104+ on-board computers support low level motor control, 
sensor management, and wireless communi­cation. Off-board computation allows for higher-level perception, 
cognition, learning, and behavior. Much of the robots cognitive abilities are supported by the C5M cognitive 
architecture codebase developed by the Personal Robots Group. This repertoire is con­tinuously being 
extended through collaboration with other research groups as part of joint research projects. The robot 
can operate in either tethered power mode or on battery power. Estimated run time using battery power 
in the current design is1hour. Acknowledgments: The development of the MDS robot was funded by an ONR 
DURIP Award N00014-06-1-0516 and a Mi­crosoft Research Grant. Cosmetic designbyFardadFaridi. Please contact 
Xitome Design for a commercial version of the MDS, dig­itROBOTICS for a commercial version of the uBot-5 
platform, and Meka Robotics for a commercial version ofthe hands. Copyright is held by the author / owner(s). 
SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>ONR DURIP</funding_agency>
			<grant_numbers>
				<grant_number>N00014-06-1-0516</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401643</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Multi-focal compound eye]]></title>
		<subtitle><![CDATA[liquid lens array for computational photography]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401643</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401643</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099915</person_id>
				<author_profile_id><![CDATA[81421601688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kensuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099916</person_id>
				<author_profile_id><![CDATA[81421592751]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dongha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099917</person_id>
				<author_profile_id><![CDATA[81331496069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo and Systems Development Laboratory, Hitachi, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099918</person_id>
				<author_profile_id><![CDATA[81309485710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Keita]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099919</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berge, B., and Peseux, J. 2000. Variable Focal Lens Controlled by an External Voltage: an Application of Electrowetting. <i>Eur. Phys. J. E 3</i>, 2, 159--163.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383927</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Georgiev, T., Zheng, K. C., Curless, B., Salesin, D., Nayar, S., and Intwala, C. 2006. Spatio-Angular Resolution Tradeoff in Integral Photography. In <i>Proceedings of Eurographics Symposium on Rendering 2006</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1158914</ref_obj_id>
				<ref_obj_pid>1158814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nayar, S. K. 2006. Computational Cameras: Redefining the Image. <i>IEEE Computer Magazine, Special Issue on Computational Photography</i> (Aug), 30--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Br&#233;dif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light Field Photography with a Hand-held Plenoptic Camera. Tech. Rep. CTSR 2005-02, Stanford Tech Report.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ueda, K., Koike, T., Takahashi, K., and Naemura, T. 2008. Adaptive IP Imaging with Variable-Focus Lens Array. In <i>Stereoscopic Displays and Applications XIX</i>, 6803--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186172</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Yamamoto, T., Kojima, M., and Naemura, T. 2004. LIFLET: Light Field Live with Thousands of Lenslets. In <i>ACM SIGGRAPH 2004 Emerging technologies</i>, 16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-Focal Compound Eye: Liquid Lens Array for Computational Photography KensukeUeda * DonghaLee TakafumiKoike 
 KeitaTakahashi TakeshiNaemura GraduateSchool ofInformationScience andTechnology,TheUniversity ofTokyo 
SystemsDevelopmentLaboratory,Hitachi,Ltd. Figure 1: Experimental results. Constant focal lengths are 
set to all lenses in the left and center images. Auto-focusing is applied to each lens in the right image, 
where both the nearer and the farther objects are clearly in focus. Wedevelopedanovelimaging system that 
captures images through anarray of variable-focus lenses.Since thefocallengthof eachlens canbecontrolledindependently,thesystemiscalled 
a Multi-focal compound eye . Background Several compound-eye cameras were developed for various appli­cationssuch 
as image-based rendering anddigital refocusing[Ya­mamoto et al.2004;Ng et al.2005;Georgiev et al.2006]. 
However, those systems were implemented by using .xed-focus lenses. Fur­thermore,by usingdynamicopticaldevices,oursystem 
introduces theconcept ofProgrammableImaging[Nayar2006] tocompound­eye cameras. Implementation As shown 
in Fig. 2, the system consists of an array of 64 liquid lenses and a high-resolution video camera behind 
it. Each lens is 7.75 mm in diameter, and its focal length can be quickly changed by using electric signals[Berge 
andPeseux2000]. We compactly arranged them on a planer board in an 8 by 8 matrix, which is 66 mm inwidthandheight.Toprovidean 
independentfocuscontrol, we wired various electric lines. The video camera takes images through thearray 
with2048by2048pixelsat15fps. Experiment We implemented an auto-focusing method for each of the lenses: 
the better focal length for its viewing scope is selected from two candidate values. The experimental 
results are shown in Figure 1. The left and center imagesarecaptured withconstantfocal lengths set to 
all lenses. Here, only the nearer/farther objects(.ower and tree/background logos) are clearly in focus, 
respectively. In con­trast, the right image is captured from the same scene with auto­focusing.In this 
case, both objects are captured in focus. Another possibleapplicationisimage-basedrendering,anditsbasicconcept 
isdescribed in[Uedaet al.2008]. We believe our new imaging system will greatly extend the poten­tial 
of compound-eye camerasfor various applications. Finally, we expressourspecialthanks toProf.HiroshiHarashimaforthevalu­ablediscussions. 
* Project e-mail: comp-photo@hc.ic.i.u-tokyo.ac.jp  Figure 2: Appearance of system.  References BERGE, 
B., AND PESEUX, J. 2000. Variable Focal Lens Controlled by an External Voltage: anApplicationofElectrowetting. 
Eur. Phys. J. E 3,2,159 163. GEORGIEV, T., ZHENG, K. C., CURLESS, B., SALESIN, D., NAYAR, S., AND INT-WALA, 
C. 2006. Spatio-Angular Resolution Tradeoff in Integral Photography. In Proceedings of Eurographics Symposium 
on Rendering 2006. NAYAR,S.K.2006.ComputationalCameras:Rede.ningthe Image. IEEE Computer Magazine, Special 
Issue on Computational Photography (Aug),30 38. ´ NG, R., LEVOY, M., BREDIF, M., DUVAL, G., HOROWITZ, 
M., AND HANRAHAN, P. 2005. LightFieldPhotography with aHand-heldPlenopticCamera. Tech.Rep. CTSR2005-02,StanfordTechReport. 
 UEDA, K., KOIKE, T., TAKAHASHI, K., AND NAEMURA, T. 2008. Adaptive IP Imaging with Variable-Focus Lens 
Array. In Stereoscopic Displays and Applica­tions XIX,6803 45. YAMAMOTO, T., KOJIMA, M., AND NAEMURA, 
T. 2004. LIFLET:LightFieldLive withThousandsofLenslets. In ACM SIGGRAPH 2004 Emerging technologies,16. 
Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401644</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Navigator]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401644</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401644</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010515</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Multi / mixed media creation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10003254</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Hypertext / hypermedia</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099920</person_id>
				<author_profile_id><![CDATA[81320489191]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jorn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ebner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Jorn Ebner Navigator abstract Navigator is a circuitous Flash-based CD-Rom with Navigation of environments 
-of the one on CD and of the real ones depicted in the work - as its theme. At the end of each animation 
sequence - figurative, abstract and photographic -, choices need to be made yet the user will not know 
where these are going to lead. synopsis Navigator is a circuitous Flash-based CD-Rom that contains simple 
Flash Projector files for Mac and Windows. The Flash animations contain abstract and figurative imagery 
with digital photography. Upon start up, an introductory animation leads to 4 buttons, each of which 
access another set of 11 animations. At the end of each of these animations, buttons give a new choice 
to play a new animation. These animation sequences are called up randomly so that the viewing sequence 
is not fixed. To exit, any key can be pressed. When Navigator says "good-bye", a mouse click will lead 
to the credits sequence. At the very end, a start again button invites for a new viewing. Each of these 
animations is related to the navigation of an environment. The imagery is based on children's drawings 
and digital photographs which have been redrawn and animated by the artist. At the end of each animation, 
the buttons - which oscillate - need to be found. The navigation of the CD-Rom parallels the navigation 
of real spaces in the sense that choices need to be made when moving through existence. technical requirements 
Navigator requires audio speakers, keyboard, monitor / projector, computer. Navigator contains both a 
Flash Projector for Mac, and an .exe file with autostart for Windows. Both have been tested on recent 
versions of Mac OSX and Windows XP but should also work on older versions. Copyright is held by the author 
/ owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401645</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Optical tone]]></title>
		<subtitle><![CDATA[dynamic color composition]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401645</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401645</url>
		<abstract>
			<par><![CDATA[<p>Navigator is a circuitous Flash-based CD-Rom with Navigation of environments --- of the one on CD and of the real ones depicted in the work --- as its theme. At the end of each animation sequence --- figurative, abstract and photographic ---, choices need to be made yet the user will not know where these are going to lead.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.4</cat_node>
				<descriptor>Navigation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010515</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Multi / mixed media creation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10003254</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Hypertext / hypermedia</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099921</person_id>
				<author_profile_id><![CDATA[81421600348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tsutomu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mutoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[International Media Research Foundation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Optical Tone - dynamic color composition Tsutomu Mutoh International Media Research Foundation  1 
Introduction To explore and to know the real world we live, how we perceive it is to be the basic start 
point. The principle should be true in the world of color in visual expression, here in terms of our 
perception and recognition of dynamic characteristics of the light. The impres­sionist Claude Monet often 
painted the same motif under several natural conditions, and fixed the dynamic effect of light on the 
renowned 33 canvasses of "Cathedrale de Rouen". The artist explored in essence how human being perceives 
change of natural scene in time and its color dynamism beyond simple problems of expression. But few 
people know that painting and coloring under the sky for the impressionists' exploration of natural light 
has made partly possible by the advanced technology at that time of pressing zinc metal to manufacture 
paint tubes. The advance of technology should be unvaryingly one of the motivation for re-examine the 
color in expression and communication on products or media in natural or artificial light environment. 
 human color perception in a straight way. To solve this problem, I have developed an original algorithm 
based on color theory and color engineering technique to control RGB output device along with human psychological 
measurements of lightness, hue and chroma (Figure 2). Then I manufactured a tumbler shape product and 
implemented full color LED device, whose color should be changed by the position in accordance with the 
human psychologi­cal color measurements using the algorithm. And I designed a color composition to be 
painted on the wall for the following experiment. On this situation, we can interactively examine the 
problem of human perception and recognition of dynamic characteristics of the light that can not be experienced 
in daily life. The interaction between the swinging device as dynamic light-source color and the wall 
as object color allows us to perceive the pure color world freeing from our control mechanism of color 
constancy. 3 Conclusion The experiment will lead further explorations for improvement of light environment 
in digital age and researches on understanding of human color perception, which will open a new history 
of visual expression and communication on electronic media. HSB(RGB) hue color circle Figure 1: Installation 
image. Optical Tone Algorithm hue color circle  2 Exposition Following the impressionists' approach, 
I have started an experi­ment on interaction of color with recent LED technology to control dynamic light 
from the view point of color composition method (Figure 1). Today the light source such as RGB monitors 
or full-color LED devices controlled by the additive mixture color method is being common in daily life. 
But the RGB (Red, Green and Blue) light emission is hard to adjust in accordance with the Itten's 12-hue 
color circle Figure 2: Hue color circle illustration. Copyright is held by the author / owner(s). SIGGRAPH 
2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401646</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Origami optics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401646</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401646</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099922</person_id>
				<author_profile_id><![CDATA[81421600585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Tremblay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099923</person_id>
				<author_profile_id><![CDATA[81548029064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Ford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099924</person_id>
				<author_profile_id><![CDATA[81421595464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Stack]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Distant Focus Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099925</person_id>
				<author_profile_id><![CDATA[81421595345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rick]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Morrison]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Distant Focus Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Origami Optics Eric J. Tremblay 1 Ronald A. Stack 3 Joseph E. Ford 2 Rick L. Morrison 4 University of 
California, San Diego Distant Focus Corporation 1. Introduction With smaller, slimmer and lighter cameras 
in high demand for consumer portable devices and military applications, optics is challenged with the 
question- how can we make optics smaller without giving up the functionality associated with larger cameras? 
A big part of the problem is simply scalability. As conventional imaging systems are scaled down, the 
focal length (i.e. magnification) scales down with the allowed optical thickness. Being additionally 
limited by the size of the smallest available image sensors, we find that these cameras are usually limited 
to short focal length, small aperture lenses- what you find in the majority of cell phones. While fine 
for many applications, there are others, such as surveillance and high-end portable device cameras that 
could benefit dramatically from larger magnification and better light collection with minimal added bulk 
or thickness. With up to eight reflections, large ray angles, a thin aspect ratio and a large field of 
view (for a reflective design); origami optic designs can be seen as a somewhat extreme extension of 
traditional reflective telescope design. Light enters the optic through an annular aperture and is reflected 
or "folded" multiple times within the optical thickness. Compared to a refractive lens, where light propagates 
in one direction as it is focused to the image plane, the multiple-reflection geometry reflects or "folds" 
the optical path multiple times back onto itself and allows for greatly reduced overall thickness for 
a given focal length. The enabling technology for this type of optical design is single-point diamond 
turning (SPDT). The recent maturity of SPDT for visible light optics in addition to its ability to maintain 
tight tolerances and fabricate complex surface shapes makes it a nearly ideal fabrication method for 
our designs. 2. Demonstration For our tech demo at SIGGRAPH 2008, we will demonstrate our four-reflection 
or "fourfold" camera prototype shown in Figure 1. This prototype design has an 18 mm focal length, field 
of view of 16.6º, effective F/# of 1.1, diameter of 28 mm and an optical thickness of just 5.5 mm. The 
optical element is fabricated as two Calcium Fluoride parts pressed together with index-matching gel 
and a nominal separation of 300 µm. This gap is made adjustable by a 120 pitch, 128 tooth focus adjusting 
gear and a 12 tooth pinion to provide fine linear translation of the gap. 1e-mail: etremblay@ucsd.edu 
2e-mail: jeford@ucsd.edu 3e-mail: rstack@distantfocus.com 4e-mail: morrison@distantfocus.com This adjustment 
allows for an extremely sensitive method of refocus where less than 40 µm of travel is required to refocus 
the optic from close range to infinity. The small travel of this focus adjustment may be especially suitable 
for use with a voice coil or a piezoelectric actuator in future prototypes. The sensor used in the four-reflection 
prototype is a .4-inch Forza/Sunplus 1.92 Mpixel CMOS color sensor interconnected via ribbon cable to 
a USB interface PCB. MDOSim- a custom software interface created by Distant Focus Corporation is used 
to control the sensor and capture image/video data with a laptop computer. Figure 1. A four-reflection 
origami optic prototype  3. Conclusion Origami Optics may play an effective role in achieving larger 
magnification, optical zoom functionality and better light collection in space-constrained applications 
such as tomorrow's high-end cell phones and micro-UAV imagers. The compact, thin form and long focal 
length of Origami Optics makes it an attractive approach for applications where the additional bulk and 
thickness of refractive optics is problematic. This research was supported by DARPA via the MONTAGE program, 
grant no HR0011-04-I-0045; and by the Natural Sciences and Engineering Research Council of Canada (NSERC) 
through a graduate student scholarship. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los 
Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401647</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Perception-based high definition haptic rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401647</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401647</url>
		<keywords>
			<kw><![CDATA[force feedback]]></kw>
			<kw><![CDATA[haptic redering]]></kw>
			<kw><![CDATA[rigid-body simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Perceptual reasoning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010393</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Perception</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099926</person_id>
				<author_profile_id><![CDATA[81421595917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ikumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Susa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099927</person_id>
				<author_profile_id><![CDATA[81421596345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ikeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099928</person_id>
				<author_profile_id><![CDATA[81421598717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokizaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099929</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099930</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099931</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Okamura, A. M., Dennerlein, J. T., and Cutkosky, M. R. 2001. Reality-based models for vibration feedback in virtual environments. <i>ASME/IEEE Transactions on Mechatronics 6</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258878</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ruspini, D. C., Kolarov, K., and Khatib, O. 1997. The haptic display of complex graphical environments. In <i>Proc. of SIGGRAPH 97 Computer Graphics</i>, ACM SIGGRAPH, 345--352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perception-based High De.nition Haptic Rendering Ikumi Susa* Yuto Ikeda * Takashi Tokizaki * University 
of Electro-Communications * Keywords: haptic redering, force feedback, rigid-body simulation 1 Perception-based 
Haptic Rendering Haptic perception is essential for dexterous object manipulation in both real and virtual 
world. Recently, haptic interaction is being researched for manipulation in virtual environment. Conventional 
haptic rendering employs penalty method which cal­culates feedback forces using only spring-damper model 
[Ruspini et al. 1997]. High frequency (1 kHz-) simulation is required for stable control over haptic 
interfaces for meaningful haptic interac­tions. However, high frequency simulation tends to cause over.ow 
of computational quantity and make simulation for virtual worlds in large scale dif.cult. Perception-based 
High De.nition Haptic Rendering(Perception­based HDHR) calculates feedback forces regarding not only 
phys­ical law but also nature of human perception. This rendering is make possible using haptic interfaces 
whose frequencies are around 1 kHz and physical simulators based on analytical methods. Con­sequently, 
Perception-based HDHR presents rich haptic perception for large scale virtual worlds (Fig.1). Figure 
1: Perception-based HDHR  2 Innovation Perception-based HDHR displays properties of objects such as 
ma­terial, viscosity and elasticity respecting the nature of human per­ception and cognition under the 
constraints of controls and compu­tational quantity. Humans perceive objects shape via normal forces, 
dynamics (inertia and visco-elasticy) from the relations of applied forces and motions of objects, and 
material properties from vibra­tion forces caused by contact between objects. First, let us consider 
perception of material. Vibrations are one of the major cues for perception of material and simulation 
of such kind requires high resolution deformable models and high fre­quency update. However, humans seem 
to perceive only spectrum and envelope of vibration or precise waveform of vibration seem not to be very 
important. Therefore, waveform of the vibration *e-mail:{ susa,ikeda,tokizaki,mitake,hase} @hi.mce.uec.ac.jp 
e-mail: msato@pi.titech.ac.jp Hironori Mitake * Makoto Sato Shoichi Hasegawa * P &#38; I lab. Tokyo 
Institute of Technology could be generated from spectrum and envelope model from ex­perimental data aside 
from feedback force which represents object shapes[Okamura et al. 2001]. Another example is perception 
of inertial and visco-elastic proper­ties. Humans perceive dynamics of objects from the relation be­tween 
applied forces and motion of objects which are touched by hands. Simulations of this relation require 
high frequency update and consideration to effects of all objects that are in contact with the users 
hands. Therefore, we propose a linear model for the rela­tion as an approximation and calculate the parameter 
of the model in the low frequency simulation for the whole virtual world. Then, the high frequency simulator 
calculates only the linear model whose computational cost is far small than the original simulation. 
The lin­ear model cannot account for collision impulses given from objects not belonging to the contact 
group. Impulsive forces should be ap­plied to the user s hand for consistency involving visual and haptic 
cues. Because visual cues have low time precision, these forces are easily achieved by simple transmission 
of impulsive forces. Figure 2: The System Overview  3 Vision Perception-based HDHR is an application 
of psychophysical knowledge of haptics. It brings high .delity haptic sensation to low cost virtual environments 
with ordinary haptic devices. Thanks to psychophysical knowledge of haptics, Perception-based HDHR en­ables 
rich haptic interactions along with human perception without special devices. Recent remarkable progress 
on haptics have been opened by insep­arable developments of psychophysics and interface devices. We hope 
to involve haptic rendering into this revolution. References OKAMURA, A. M., DENNERLEIN, J. T., AND 
CUTKOSKY, M. R. 2001. Reality-based models for vibration feedback in virtual en­vironments. ASME/IEEE 
Transactions on Mechatronics 6, 3. RUSPINI, D. C., KOLAROV, K., AND KHATIB, O. 1997. The haptic display 
of complex graphical environments. In Proc. of SIGGRAPH 97 Computer Graphics, ACM SIGGRAPH, 345 352. 
 Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. 
ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401648</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Personal robot with a sense of taste]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401648</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401648</url>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor>Medicine and science</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099932</person_id>
				<author_profile_id><![CDATA[81100135508]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mie University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099933</person_id>
				<author_profile_id><![CDATA[81100379256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimazu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NEC System Technologies, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099934</person_id>
				<author_profile_id><![CDATA[81421600568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kaori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NEC System Technologies, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099935</person_id>
				<author_profile_id><![CDATA[81330493047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takaharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kameoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mie University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Personal Robot with a Sense of Taste Atsushi Hashimoto* Hideo Shimazu Takaharu Kameoka§ Mie University 
Kaori Kobayashi Mie University NEC System Technologies, Ltd. 1 Introduction This paper describes an 
artificial sense of taste and the world s first Tasting Robot , which was developed under support by 
the New Energy and Industrial Technology Development Organization (NEDO) for the Robot Project: Prototype 
Robot Exhibition at EXPO 2005 Aichi, Japan. The robot has a sense of taste based on an optical tongue 
concept. The optical tongue is the integration of infrared (IR) spectroscopy with pattern recognition 
technologies. It acquires an IR spectrum of a food sample, spectroscopically and quantitatively analyzes 
the food components, and identifies the name of the food. As these results are transmitted to the body 
of the Tasting Robot, it evaluates them and promptly gives various advices on health and meal to a user 
in natural language with graphical images. It sees if foods are best to eat, the taste has deteriorated, 
or if they are good for health from the point of the content of sugar and fat and gave advices to users 
by referring to food and health knowledge bases. Especially in case of wine, the robot behaves as the 
Sommelier Robot (Wine-Bot). The Wine-Bot is a functionally enhanced version of the Tasting Robot. 2 
Artificial Sense of Taste The robot could be expected to have at least three major potential uses when 
it has a sense of taste. The first potential is to qualitatively and quantitatively analyze the major 
components in a food sample. The second potential 1.0×10-4 is to express a similar taste feelings of 
food sample to a human's feelings. The last potential is to identify the names of food products. It requires 
a human with special skills and training. 3 IR Spectral Characteristics of Foods It is known that each 
food has different IR spectral characteristics. Figure 1 shows the absorption spectra characterizing 
the wine components other than ethanol and water. The spectral differences among the wine brands are 
then clearly observed and the fingerprint information is able to be confirmed. Based on the above improvements, 
the robot could identify several tens of wine brands.  4 Conclusions Aimed for the realization of a 
robot with close-to-human efficiency, the five senses of human nature are drawing attention. The goal 
of this project is to build a robot sommelier. Wine is one of the most sophisticated and hard-to-identify 
food. The achievement is made possible by combining robot technologies and pattern recognition technologies 
with the analytical techniques in the infrared spectroscopic technology for food. We will continue to 
lead the industry in robotic development that supports the prevention of lifestyle-related diseases and 
help in the treatments for those diseases through dietary therapy and elemental technology. d2Abs/d. 
2 0.5 0.0 -0.5 -1.0 -1.5  1500 1400 1300 1200 1100 Wavenumber [cm-1] * hasimoto@bio.mie-u.ac.jp shimazu-hxa@necst.nec.co.jp 
Figure 1: IR Spectra of Wine Components. kobayashi-kxd@necst.nec.co.jp§ kameoka@mie-u.ac.jp Copyright 
is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401649</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Rome Reborn]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401649</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401649</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099936</person_id>
				<author_profile_id><![CDATA[81319491821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bernard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099937</person_id>
				<author_profile_id><![CDATA[81547368756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abernathy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099938</person_id>
				<author_profile_id><![CDATA[81100369656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gabriele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guidi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Politecnico di Milano]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099939</person_id>
				<author_profile_id><![CDATA[81421600059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Myers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Past Perfect]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099940</person_id>
				<author_profile_id><![CDATA[81421595811]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cassie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thibodeau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[mental images GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099941</person_id>
				<author_profile_id><![CDATA[81421598421]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salvemini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[mental images GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099942</person_id>
				<author_profile_id><![CDATA[81502733879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Procedural Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099943</person_id>
				<author_profile_id><![CDATA[81100639071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hofstee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099944</person_id>
				<author_profile_id><![CDATA[81421600987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Barry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rome Reborn Bernard Frischer Gabriele Guidi Joel Myers Dean Abernathy Politecnico di Past Perfect University 
of Virginia Milano Productions Rome Reborn (www.romereborn.virginia.edu) is an international initiative, 
started in 1996 and based at the Institute for Advanced Technology in the Humanities (IATH; see www.iath.virginia.edu), 
to create 3D urban models illustrating the development of ancient Rome from the first settlement in the 
late Bronze Age (ca. 1,000 B.C.) to the depopulation of the city in the early Middle Ages (ca. A.D. 550). 
Other institutional partners have included the Politecnico di Milano, UCLA, the Université de Caen, and 
the Ausonius Institute at the Université de Bordeaux­III. Commercial rights to Rome Reborn have been 
exclusively licensed to Past Perfect Productions s.r.l., a corporation based in Rome, Italy (http://www.pastperfectproductions.com/). 
Rome Reborn is based on the key ideas of collaborative research and scholarly communication. Each element 
of the city model is created by a team of subject experts working closely with experts in 3D modeling 
and other pertinent technologies. Thus far, over a dozen archaeologists from Italy, the United States, 
France, Germany, and the United Kingdom have participated in the project. When a team s contribution 
to the overall urban model is completed, the subject experts sign a form declaring the model complete 
and ready for public dissemination. The resulting model is scholarly not only in the sense that it has 
qualified authors but also because it offers the user transparency of evidence and argument: along with 
the 3D model, the project always publishes related metadata and archaeological documentation. Once the 
model and its documentation are georeferenced, they can be seamlessly linked in the user interface. As 
you explore the model, you can stop and open a window that explains the evidence and hypotheses behind 
the reconstruction you are seeing on the screen. Rome Reborn 1.0. The first result of the project, finished 
in 2007, is called Rome Reborn 1.0, a digital model of the city as it might have appeared at the height 
of its urban development in the time of Constantine the Great in A.D. 320. The model includes a digital 
terrain map with the hills, valleys, and water features of the city. It is composed of over 7,000 buildings 
within the late­antique Aurelian Walls, home to a multicultural population of over 1 million people. 
Of the 7,000 buildings, ca. 250 are known with great specificity as to identification, location, and 
design. These are known as the Class I monuments. 31 of these were made at a scale of 1:1 at UCLA. The 
Class II monuments are the other 6,750 buildings of the ancient city that are known from ancient sources 
including, notably, two late­antique catalogues of the building stock of the city. The Class II buildings 
are very schematic and rely heavily on textures instead of geometry for architectural details. They derive 
from 3D scan data collected from the Plastico di Roma Antica, a 1:250 plaster­of­Paris physical model 
of the city created from the 1930s to the 1970s and housed in a museum in Rome. Creation of the Class 
II models was the responsibility of the Department of Design of the Politecnico di Milano. Rome Reborn 
1.0 was created with a variety of software, all ultimately imported into MultiGen Creator and displayed 
on PCs as a real­time, interactive urban model using Open Scene Graph. Google Earth is used to georeference 
the archaeological documentation. Originally conceived for use in an immersive theater at UCLA, it is 
not possible to run the model on the Internet. Cassie Thibodeau Pascal Müller Peter Hofstee Antonio 
Salvemini Procedural Inc. Barry Minor mental images GmbH IBM  Rome Reborn 1.1 was jointly created by 
IBM and IATH in 2008. It represents a conversion of version 1.0 into BVH format and runs on the IBM Cell 
platform. As compared to version 1.0, 1.1 brings improvements in illumination, frame­rate, and resolution. 
It also includes the Circus Maximus, a new major Class I monument created by the Ausonius Institute at 
the Université de Bordeaux­III (http://www­ausonius.u­bordeaux3.fr/). Rome Reborn 2.0 was jointly created 
by IATH, Procedural, and Mental Images in 2008. It runs on a 64­core server generously donated by Sun. 
Like versions 1.0 and 1.1, Rome Reborn 2.0 represents the city as it might have appeared in A.D. 320. 
Version 2.0 uses the 32 hand­made Class I models created at UCLA and Bordeaux and converted by IBM and 
IATH to 3D Studio Max format. It completely replaces the Class II models derived from the physical model 
with procedural models created with the CityEngine software of Procedural (www.procedural.com/) using 
archaeological research undertaken by the Université de Caen (www.unicaen.fr/rome/index.php) and by IATH. 
Thus, version 2.0 is greatly improved with respect to geometric detail. In contrast to versions 1.0 and 
1.1, Rome Reborn 2.0 runs not only on a workstation but also­­thanks to the RealityServer software of 
mental images (www.mentalimages.com/2_3_realityserver/index.html) ­­on the Internet. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401650</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Stop motion goggle]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401650</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401650</url>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Perceptual reasoning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010393</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Perception</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099945</person_id>
				<author_profile_id><![CDATA[81421595909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Naohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ngaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099946</person_id>
				<author_profile_id><![CDATA[81421598895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabian]]></first_name>
				<middle_name><![CDATA[Foo Chuan]]></middle_name>
				<last_name><![CDATA[Siang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099947</person_id>
				<author_profile_id><![CDATA[81335490502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Furukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099948</person_id>
				<author_profile_id><![CDATA[81365592229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokiwa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099949</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099950</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stop Motion Goggle Naohisa Ngaya*, Fabian Foo Chuan Siang*, Masahiro Furukawa* Takuji Tokiwa*, Maki 
Sugimoto*, Masahiko Inami* The University of Electro-Communications 1 Introduction Stop Motion Glasses 
(SMG) expands your visual perception of moving objects by allowing you to perceive visual information 
selectively through a high speed shutter. In this system, you can easily observe not only periodic rotational 
motion such as rotating fans or wheels, but also random motion like a jumping ball. SMG can be considered 
as a time domain sunglasses. Sunglasses enables easy viewing of surroundings by cutting off bright light, 
on the other hand, SMG makes it easy to see moving object by temporally controlling optical density. 
In addition, SMG deblurs artifacts due to fast moving object over a short exposure time. This system 
induces similar physiologic effects of Stroboscope in dark environment that augments human visual perception 
of moving objects anywhere as a simple wearable device. 2 Technology SMG used a DisplayTech ferro-electric 
shutter as a high speed shutter (Figure 1). The ferro-electric shutter s on/off contrast is high (500 
: 1) and switching time is less than 100 microseconds. Figure 1: Stop Motion Goggle With our technology, 
we control an exposure time which is determined from the shutter s on/off ratio and frequency. It can 
be said that this way is a same as adjusting shutter speed and a frame rate when we use a video camera. 
Most suitable value of these to easily observe moving objects depends on a movement velocity of objects 
and characteristics of human visual perception. From our experimental results, suitable value of on/off 
ratio is 2% and frequency is 25 Hz at the object which does horizontal uniform motion with 140 degree 
per second. 3 Application SMG enables users to easily observe moving objects. For example, the users 
can observe a shape of a rotating fan and perceive character on the fan blade (Figure 2). SMG can be 
used for other motion such as uniform motion (Figure 3) and random motion (Figure 4).  Figure 2: Rotating 
motion  Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 
15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401651</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Superimposing dynamic range]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401651</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401651</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>E.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002978.10002979.10002985</concept_id>
				<concept_desc>CCS->Security and privacy->Cryptography->Mathematical foundations of cryptography</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712.10003713</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory->Coding theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099951</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099952</person_id>
				<author_profile_id><![CDATA[81313481091]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mantiuk, R., Daly, S. J., Myszkowski, K., and Seidel, H.-P. 2005. Predicting visible differences in high dynamic range images - model and its calibration. In <i>Proc. IS&T/SPIE's Annual Symposium on Electronic Imaging</i>, vol. 5666, 204--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., and Vorozcovs, A. 2004. High dynamic range display systems. In <i>Proc. ACM Siggraph</i>, 760--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Superimposing Dynamic Range Oliver Bimber*, Osaka University , Bauhaus-University Weimar and Daisuke 
Iwai Figure 1: Registering a projector precisely to a hardcopy allows extending contrast, perceivable 
tonal resolution and color space beyond the capabilities of either hardcopy or projector. From left to 
right: experimental setup and example for achieved registration precision (projected checker on printed 
checker with a .eld size of 0.62 mm or 7 cycles per degree (cpd) at 50 cm viewing distance), low and 
high exposure photographs of different hardcopies (ePaper display, photographic print, laser print and 
X-ray print) ampli.ed with LED and DLP projectors. 1 Introduction It was only recently that high dynamic 
range (HDR) displays were introduced which could present content over several orders of mag­nitude between 
minimum and maximum luminance (e.g., [Seetzen et al. 2004]). All of the approaches share three common 
properties: These are .rstly that they apply a transmissive image modulation (ei­ther through transparencies 
or LCD/LCoS panels) and consequently suffer from a relatively low light-throughput (e.g., regular color 
/ monochrome LCD panels transmit less than 3-6% / 15-30% of light) and therefore require exceptionally 
bright backlights. Secondly, one of the two modulation images is of low-resolution and blurred in order 
to avoid artifacts such as moir´ e patterns due to the misalign­ment of two modulators, as well as to 
realize acceptable frame-rates. Thus, high contrast values can only be achieved in a resolution of the 
low-frequency image. Thirdly, since one of the two images is monochrome (mainly to reach a high peak 
luminance), only lumi­nance is modulated, while chrominance modulation for extending the color space 
is in some cases considered future work. We present a simple and low-cost method of viewing static HDR 
content based on re.ective image modulation. 2 Superimposing Dynamic Range We project images onto hardcopies, 
such as photographs, X-ray prints, or electronic paper (ePaper) so as to boost contrast, perceiv­able 
tonal resolution, and color space values beyond the potential of either hardcopies (when viewed under 
environment light) or pro­jectors (when projecting onto regular screens) alone. A calibrated projector-camera 
system is applied for automatic registration, scan­ning and superimposition of hardcopies. Figure 1 illustrates 
exam­ples thereof. We do not intend to compete with interactive HDR displays, but rather offer an everybody 
can do alternative for do­mains that operate with static image content, such as radiology and other medical 
.elds, or astronomy. Yet, electronic paper allows for interactive visualizations. In our experiments, 
we achieved contrast ratios of over 45,000:1 *bimber@uni-weimar.de daisuke.iwai@sys.es.osaka-u.ac.jp 
 with a peak luminance of more than 2,750 cd/m2, could techni­cally re-produce more than 620 perceptually 
distinguishable tonal values (approximately 85% of all theoretically possible JND steps). Furthermore, 
we attained color space extensions of up to factor 1.4 (compared to a regular projections) or factor 
3.3 (compared to regular hardcopy prints). Thereby, the hardcopy resolution can be several thousand dots 
per inch, while luminance and chrominance are modulated with a registration error of less than 0.3 mm. 
 3 Discussion Compared with most existing interactive HDR displays, we support near distance viewing 
at a contrast resolution of up to 7 cpd (given our current registration precision and assuming a viewing 
distance of 50 cm). Due to scattering of light in the eye, the perceived local contrast is reduced, depending 
mainly on the adaptation luminance and on the spatial frequency of the observed content. Referring to 
the optical transfer function of the eye described in [Mantiuk et al. 2005], we can still achieve a perceived 
local contrast of 40%-69% at a spatial resolution of 7 cpd and an adaptation luminance of 0.06 cd/m2 
-2,750 cd/m2. It is also reported in [Mantiuk et al. 2005] that the contrast sensitivity (CS) for an 
adaptation luminance above 1,000 cd/m2 is maximal at this resolution, and that the CS-peak only shifts 
to lower frequencies with a decreasing adaptation lumi­nance. We believe that our approach has the potential 
to complement hardcopy-based technologies, such as X-ray prints for .lmless imag­ing, in domains that 
operate with high quality static image content, like radiology and other medical .elds, or astronomy. 
 References MANTIUK, R., DALY, S. J., MYSZKOWSKI, K., AND SEIDEL, H.-P. 2005. Predicting visible differences 
in high dynamic range images -model and its calibration. In Proc. IS&#38;T/SPIE s Annual Symposium on 
Electronic Imaging, vol. 5666, 204 214. SEETZEN, H., HEIDRICH, W., STUERZLINGER, W., WARD, G., WHITEHEAD, 
L., TRENTACOSTE, M., GHOSH, A., AND VOROZCOVS, A. 2004. High dynamic range display systems. In Proc. 
ACM Siggraph, 760 768. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401652</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[tangible workbench "TW"]]></title>
		<subtitle><![CDATA[with changeable markers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401652</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401652</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099953</person_id>
				<author_profile_id><![CDATA[81421592417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kienzl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099954</person_id>
				<author_profile_id><![CDATA[81421593041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ulf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marsche]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099955</person_id>
				<author_profile_id><![CDATA[81421599315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nadja]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kapeller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099956</person_id>
				<author_profile_id><![CDATA[81421597934]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gokcezade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 tangible workbench "TW" with changeable markers DI. Thomas Kienzl DI. Ulf Marsche MSc Nadja Kapeller 
MSc Adam Gokcezade inbox@kommerz.at Introduction Changeable markers afford completely new possibilities 
of interaction on a tangible user interface. The heart of the solution are changeable markers, which 
enable new, complex possibilities of interaction design, which upgrades a TUI from a classic presentation 
tool to a work tool. Applications on the TW become of interest to non-users and professionals due to 
the mix of intuitive and exact work. Thus, an interface is developed between designers/customers, managers 
and experts. This tool can be used in virtual factory planning, the planning of infrastructure projects 
and as a product configurator. A conventional TUI allows for 3 degrees of freedom (x/y, rotation and 
z). The appearance of the marker is changed by manipulating (pushing, rotating etc.) a controller object, 
and this creates additional levels of interaction. The design of the controller objects is based on normal 
everyday tools, such as an eraser, a button, pliers, a tool bar etc. The set up consists of a table (frosted 
glass tabletop with a projector and camera underneath), a separate monitor with realtime 3D application 
and the controller objects. A demo application awaits visitors, which allows them to construct a completely 
furnished flat. Exposition The system provides the following advantages: 1. Intuitive and exact work, 
using normal tool analogies. 2. Simple presentation and complex constructing. 3. Suited for everyday 
use, sturdy, no electronics in the controller objects 4. Involvement of managers and customers in the 
planning work. 5. Possibility to collaborate simultaneous work on a project from separate locations. 
 The MRI (mixed reality interface) serves as the basis of the system, which is augmented by changeable 
markers.  Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 
11 15, 2008. ISBN 978-1-60558-466-9/08/0008 3 Build a fully furnished flat only using tangible controller 
objects. 1. Construct: toolbar selection of mode: new object, position object or change attributes. 
Change the dimensions: x,y,z and the height as well as other parameters. 2. Controller object with button 
or in pliers design. 3. view -Walk through: (my point of view), UD camera (to look up and down) 4. 
view - Walk through: camera with capture und pull function (my point of view - pull the floor plan) 
5. Zoom/pan: Scroll wheel to zoom and pan the floor plan 6. Delete: eraser  Realtime 3D Application 
1. First, the user selects a wall from the menu, puts this on the navigation surface (as a projected 
object) and looks at the wall as a 3D object on the monitor. Then the user changes the dimensions and 
position, and chooses materials until the construction of the flat is finished. 2. The next step is 
to select and position the furniture, lamps and pictures on the wall. 3. All of the elements put down 
can be activated afterwards and adapted or changed according to dimensions, material etc. (change attributes) 
 4. The eraser makes completely deleting objects as easy as possible. 5. The user can use the scroll 
wheel to zoom or pan. If the user places it on the tabletop, then the user pans the floor plan, and if 
the user rotates the scroll wheel, the user zooms the floor plan. 6. view -Walk through: The user uses 
a camera or the UD camera to walk through the flat.   4 Conclusion The TW is a novel input device 
for providing control and constructing complex 3D visualization applications for a wide variety of applications 
and user groups, in particular untrained people as well as professionals. The system does not require 
any training or experience, and you can learn to operate it by watching other people use it.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401653</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[The programming of robots by haptic means]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401653</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401653</url>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099957</person_id>
				<author_profile_id><![CDATA[81317500954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[Keng Soon]]></middle_name>
				<last_name><![CDATA[Teh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099958</person_id>
				<author_profile_id><![CDATA[81100330650]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daishi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NEC Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099959</person_id>
				<author_profile_id><![CDATA[81100301137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kazuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunieda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NEC Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099960</person_id>
				<author_profile_id><![CDATA[81100354539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Keiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NEC Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Duffy, T. M., and Jonassen, D. H. 1992. <i>Constructivism and the Technology of Instruction: A Conversation</i>. Lawrence Erlbaum.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Grunwald, G., Schreiber, G., Albu-Schaffer, A., and Hirzinger, G. 2003. Programming by touch: the different way of human-robot interaction. <i>IEEE Transactions on Industrial Electronics 50</i>, 4 (Aug.), 659--666.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Piaget, J. 1955. <i>The Construction of Reality in the Child</i>. Routledge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Programming of Robots by Haptic Means James Keng Soon Teh* Daishi Kato Kazuo Kunieda Keiji Yamada§ 
National University of Singapore NEC Corporation NEC Corporation NEC Corporation 1 Introduction According 
to the theory of constructivism, children can learn a lot not just from being taught but also through 
their play experiences, especially when they are designing and creating things. By playing with blocks, 
for example, children learn by exploration and exper­imentation how the physical world works [Piaget 
1955]. It is also important for learners with different knowledge to collaborate in or­der to arrive 
at a shared understanding [Duffy and Jonassen 1992]. Based on these ideas, our objective has been to 
develop a system to program robots using the sense of touch. We believe that en­abling robot programming 
through physical touch encourages play, learning, sharing and collaboration. To the best of our knowledge, 
there is no previous example which applies robot programming through touch for children s learning. Grunwald 
developed a system for programming a mobile service robot by touch, which is more about demonstrating 
to a robot arm the intended actions for it to repeat [Grunwald et al. 2003]. Also, the system is not 
focused on learning and playing for children, but more for an industrial environment. 2 System overview 
We present a system for the programming of robots by haptic means, which is part of the WaitLess project. 
WaitLess is a collab­orative software programming system with the aim of simplifying programming by mashing 
up pieces of software submitted to the web server as shown in Fig. 1a. It allows the program that controls 
robot actions to be stored on the server. Using one robot each, chil­dren can program the robot actions 
by touching sensors, and these actions are stored on the server. In this way, a single program can be 
made up of different parts created by different children. We now describe the three major functionalities 
of our system. The robot and its on-body sensors (Fig. 1b) connect to the server program which controls 
the input and output functions of the robot. Based on the input data on the sensors, the server sends 
output com­mands to control the movements of the robot. This connection is done to make it simple for 
children to learn by themselves, without needing to learn the overall hardware architecture. Each sensor 
cor­responds to a single action while a program consists of one or more actions. For example, a touch 
on the right side of the head causes the robot to rotate its head to the right. An example of a program 
is a sequence of actions that lead to the robot performing a dance. This system also allows two or more 
single actions to be combined and executed in parallel. The allowed combined actions are de­signed to 
be as intuitive as possible. For example, by touching the left side of the head and the right side of 
the body at the same time, the robot rotates its head to the left while turning its body to the right. 
Furthermore, actions can be connected together in a sequence. At the start of the system, the memory 
state is empty. As users program the robot by touch, each action is kept in memory as an ordered list. 
*e-mail: james.tks@gmail.com e-mail:daishi@cb.jp.nec.com email:k-kunieda@ak.jp.nec.com §e-mail:kg-yamada@cp.jp.nec.com 
 Figure 1: a) System Overview; b) Sensors for programming of ac­tions The actions are automatically Recorded 
. The Play sensor, when activated, commands the robot to perform the sequence of actions in memory. The 
Reset sensor clears the memory. Using these sensors, children get real, physical feedback and learn how 
to create the ideal robot model they have in mind. 3 Advantages Key advantages of this system include: 
a) Intuitive physical interac­tion through programming robot by touch is ideal for self learning while 
playing. b) Children can perform parallel programming, a concept which is dif.cult to implement in traditional 
programming. c) Remote collaboration and sharing of knowledge help children learn independently as a 
child s program and ideas are stored and shared on the server with other children. 4 Conclusion Our 
next step would be to implement a more complex programming structure which incorporates conditional actions 
instead of just se­quential actions, and iterative constructs, whereby children would be able to program 
the robot to perform a sequence of actions re­peatedly. In the future, we envision children becoming 
engaged in creative programming using this system, in which they will be able to pro­gram complex robot 
actions by playing with the robot. It will also be possible to conduct remote collaboration, with each 
child con­tributing his or her own ideas to the program controlling the robot. References DUFFY, T. 
M., AND JONASSEN, D. H. 1992. Constructivism and the Technology of Instruction: A Conversation. Lawrence 
Erlbaum. GRUNWALD, G., SCHREIBER, G., ALBU-SCHAFFER, A., AND HIRZINGER, G. 2003. Programming by touch: 
the different way of human-robot interaction. IEEE Transactions on Indus­trial Electronics 50, 4 (Aug.), 
659 666. PIAGET, J. 1955. The Construction of Reality in the Child. Rout­ledge. Copyright is held by 
the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401654</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Traversing complex environments using time-indexed high dynamic range panoramas]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401654</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401654</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099961</person_id>
				<author_profile_id><![CDATA[81421599618]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSIGHT, Ramesseum.com]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099962</person_id>
				<author_profile_id><![CDATA[81100018898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MAFTO, INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099963</person_id>
				<author_profile_id><![CDATA[81100506574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1018698</ref_obj_id>
				<ref_obj_pid>1018408</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Allen, P., Feiner, S., Troccoli, A., Benko, H., Ishak, E. and Smith, B. 2004. Seeing into the Past: Creating a 3D Modeling Pipeline for Archaeological Visualization. In <i>Proceedings of the 3D Data Processing, Visualization, and Transmission, 2nd international Symposium on (3dpvt'04)</i>, IEEE Computer Society, 751--758.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Traversing Complex Environments Using Time­Indexed High Dynamic Range Panoramas * Antonio Hui Philippe 
MartinezKevin CainINSIGHT, Ramesseum.com MAFTO, INSIGHT INSIGHT 1 Introduction Static panoramic photography 
has been shown to contribute to context-rich descriptions of regions under archaeological study [Allen 
et al. 2004]. We show that fast traversal through a matrix of dynamic panoramas can allow users to quickly 
locate specific target features within a complex scene. Results are presented using two large archaeological 
monuments as a test subjects. 2 Capture and Processing We collect source images using a camera with a 
circular fisheye lens, articulated by a custom rotation controller. Capture is automated. The camera 
is positioned by unidirectional, monotonic rotation around the lens nodal point. Exposures are taken 
at ten discrete rotation intervals. In order to enable high dynamic range output, three bracketed exposures 
are acquired for each camera position. Figure 1. Initial image assembly from fisheye images. As the 
capture process iterates, a time-lapse sequence is built frame by frame with each successive revolution 
of the camera. The rotation period is approximately 1.8 minutes. In the processing steps, source images 
are geometrically calibrated to account for lens distortion and an HDR representation is computed. A 
rendered view for one frame of a panoramic sequence is shown in Figure 1. In our viewer application, 
we first index the recorded panoramas in a spatial network, using global coordinates recorded during 
image capture. After initialization, users interactively navigate through the image data using a novel 
multi-node viewer in which each node represents a different location on the time axis. The user s current 
position is drawn in the viewer s main window. The 3D location for each image capture location in the 
dataset is projected in this view as navigation links (Figure 2). By hovering over a given link, the 
user obtains an interactive preview of the linked panorama. In a separate window, a ground plan of the 
site enables spatial domain traversal of the scene. Indexed metadata is presented in a third window. 
View information between windows is updated in real time via an XML data stream (Figure 3). * e-mail: 
antonioh@mac.com e-mail: pmartine@ens.fr e-mail: kevin@insightdigital.org Figure 2. User interface for 
the panoramic viewer. 3 Results and Validation To test our system, we acquired 30 time-lapse image nodes 
in situ at Chichén-Itzá, Mexico. Separately, we acquired 60 image nodes at the Temple of Ramses II, Egypt. 
Archaeological researchers were asked to use our viewer system to identify 20 specific, non­repeating 
visual features present in each actual scene. They were then asked to identify the same features by viewing 
the set of rectified input photos gathered during our capture step. We found the seek time for a given 
target feature when using our system was faster by a factor of 3.1 for the smaller data set, and a factor 
of 7.4 for the larger data set. Our approach was shown to be an efficient, low cost technique for interrogating 
complex real-world scenes. We also found that the time advantage scales with database size. Figure 3. 
Components of the interactive viewer References ALLEN, P., FEINER, S., TROCCOLI, A., BENKO, H., ISHAK, 
E. AND SMITH, B. 2004. Seeing into the Past: Creating a 3D Modeling Pipeline for Archaeological Visualization. 
In Proceedings of the 3D Data Processing, Visualization, and Transmission, 2nd international Symposium 
on (3dpvt'04), IEEE Computer Society, 751-758. Copyright is held by the author / owner(s). SIGGRAPH 2008, 
Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401655</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Two-dimensional communication]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401655</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401655</url>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Channels and controllers</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.3</cat_node>
				<descriptor>Interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>B.4.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010602</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Interconnect</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010590</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Buses and high-speed links</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099964</person_id>
				<author_profile_id><![CDATA[81421601844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099965</person_id>
				<author_profile_id><![CDATA[81100425693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasutoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099966</person_id>
				<author_profile_id><![CDATA[81100555829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099967</person_id>
				<author_profile_id><![CDATA[81421598502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Itai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cellcross Co. Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2292228</ref_obj_id>
				<ref_obj_pid>2288578</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Laerhoven, K. V., Villar, N., Schmidt, A., Gellersen, H. W., Hakansson, M., and Holmquist, L. E. 2003. Pin&play: the surface as network medium. <i>Communications Magazine, IEEE 41</i>, 4, 90--95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>706692</ref_obj_id>
				<ref_obj_pid>646867</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lifton, J., Seetharam, D., Broxton, M., and Paradiso, J. A. 2002. Pushpin computing system overview: A platform for distributed, embedded, ubiquitous sensor networks. In <i>Pervasive '02: Proceedings of the First International Conference on Pervasive Computing</i>, 139--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>582469</ref_obj_id>
				<ref_obj_pid>582466</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Scott, J., Hoffmann, F., Addlesee, M., Mapp, G., and Hopper, A. 2002. Networked surfaces: a new concept in mobile networking. <i>Mob. Netw. Appl. 7</i>, 5, 353--364.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Two Dimensional Communication Kei Nakatsuma:, Yasutoshi Makino Hiroto Itai§ , Hiroyuki Shinoda The 
UniversityofTokyo Cellcross Co. Ltd. 1 Introduction Two Dimensional Communication is an alternative technology 
for conventional wired or wireless networks especially for ubiquitous computing and sensor networks. 
This novel communication tech­nologyprovides both54Mbps communicationanda10Wpower supply via a thin, 
soft and .exible sheet without wiring. For high-density sensor networks, it is quite dif.cult to connect 
all the nodes with wires, and the networks can become highly complex. Wireless connectionhasbeentheonlysimplewayto 
distribute sen­sor nodes to a large area. Supplying power to each node, however, is a critical problem. 
We have proposed an innovative communication device, theTwo Dimensional Communication sheet (2DC sheet). 
The nodes touch­ing to the 2DC sheet can communicate with each other with 54 Mbps of bandwidth and can 
acquire 10Wof electricity. The 2DC sheet requires no direct electrical contact between the nodes and 
the sheet. Therefore nodes can be moved freely on the sheet. Wireless and batteryless ubiquitous infrastructure 
is possible through these featuresof theTwo Dimensional Communication sheet. Theideaof communicationviaatwodimensional 
mediumwasalso proposed by several other groups ([Lifton et al. 2002] [Laerhoven et al. 2003] [Scott et 
al. 2002]). These projects, however, did not achieve both high speed communication and power supply simulta­neously 
with a simple sheet. 2 Principle The2DC sheet has three layers.Two conductive layers sandwich a dielectric 
layer. When the high frequency voltage between two conductive layers is supplied, microwaves propagate 
two dimen­sionally in the dielectric layer. The upwards-facing side of the con­ductive layer has a meshed 
structure. When microwaves propagate in the 2DC sheet, the meshed pattern creates an evanescent wave 
immediately above the surface of the sheet. The evanescent waves make it possible for the nodes to communicate 
with each other and to receive electricity. Since the coupling to the evanescent .eld re­quires no electrical 
contact, the nodes can be connected anywhere on the sheet.This makes the technology both safer and rustproof. 
The 2DC sheet corresponds to the physical layer of the OSI refer­ence model. Any existing protocols including 
the wireless LAN protocols, Bluetooth, ZigBee, etc. are available. The Two Dimensional Communication 
networks can be simply applied by replacing the existing antennas of wireless devices with a special 
surface connector for the 2DC sheet. 3 Applications The 2DC technology can be applied to a room. Based 
on our 2DC scheme, walls, .oors and ceilings can become both wireless and *e-mail: tsuma@alab.t.u-tokyo.ac.jp 
e-mail: yasutoc@alab.t.u-tokyo.ac.jp e-mail: shino@alab.t.u-tokyo.ac.jp §e-mail: itai@cellcross.co.jp 
 Figure 1: The demonstration of both data transmission and power supply through theTwo Dimensional Communication 
sheet. The right PC displays the movie data sent by the left one. The LEDs and the smallfans receive 
electricity from thepower source shown in the upper left of the picture via the 2DC sheet. batteryless 
communication infrastructures. As no wires and no bat­teries are required for the nodes, installation 
and maintenance be­comes easier. When the 2DC sheet .xed to a desktop surface, the desk itself pro­vides 
the ubiquitous environment. Wireless devices can be con­nected to networks via the desk as well as recharged. 
Since the microwaves propagate only within the sheet, the communication becomes safer than a conventional 
wireless scheme. Soft materials such asfabrics and rubbers can also be used for the conductive layers. 
This makes the sheet both .exible and stretch­able. Such soft 2DC sheets can be used for arti.cial skins 
or wear­able computing devices. Acknowledgements This researchwas partially supportedbthe Ministryof 
Education, Science, Sports and Culture, Grant in Aid for JSPS Fellows, 18 11193, 2006 2008. Thetextile2DC 
sheetwasprovidedbyTeijin corporation.  References LAERHOVEN, K. V., VILLAR, N., SCHMIDT, A., GELLERSEN, 
H. W., HAKANSSON, M., AND HOLMQUIST, L. E. 2003. Pin&#38;play: the surface as network medium. Communications 
Magazine, IEEE 41, 4, 90 95. LIFTON, J., SEETHARAM, D., BROXTON, M., AND PARADISO, J. A. 2002. Pushpin 
computing system overview: Aplatform for distributed, embedded, ubiquitous sensor networks. In Per­vasive 
02: Proceedings of the First International Conference on Pervasive Computing, 139 151. SCOTT, J., HOFFMANN, 
F., ADDLESEE, M., MAPP, G., AND HOPPER,A. 2002. Networked surfaces: a new concept in mo­bile networking. 
Mob. Netw. Appl. 7, 5, 353 364. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, 
California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Ministry of Education, Science, Sports and Culture</funding_agency>
			<grant_numbers>
				<grant_number>18-11193, 2006--2008</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401656</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[UlteriorScape]]></title>
		<subtitle><![CDATA[optical superimposing on view-dependent tabletop display and its applications]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1401615.1401656</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401656</url>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1099968</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099969</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1186174</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kakehi, Y., Iida, M., Naemura, T., Shirai, Y., Matsushita, M., and Ohguro, T. 2004. Lumisight Table: Interactive View-Dependent Display-Table Surrounded by Multiple Users. In <i>SIGGRAPH 2004 Emerging Technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179165</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kakehi, Y., Iida, M., Naemura, T., and Matsushita, M. 2006. Tablescape Plus: Upstanding Tiny Displays on Tabletop Display. In <i>SIGGRAPH 2006 Emerging Technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732300</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., Welch, G., Low, K., and Bandyopadhyay, D. 2001. Shader Lamps: Animating Real Objects with Image-Based Illumination. In <i>Eurographics Workshop on Rendering</i>, 89--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 UlteriorScape: Optical Superimposing on View-Dependent Tabletop Display and Its Applications Yasuaki 
Kakehi* Takeshi Naemura* Presto, Japan Science and Technology Agency The University of Tokyo Figure 
1: Superimposed Map Figure 2: Pick up and Move Figure 3: Tabletop Theater for Multiple Users 1 Introduction 
When we use a table, we often put various objects on the table­top. Likewise, for intuitive interactions 
on a tabletop display, the placed objects on the tabletop should be enhanced more as well as the tabletop 
surface [Raskar et al. 2001]. To achieve this goal, we propose a novel tabletop display system UlteriorScape 
. Ulteri­orScape takes two major functions from our previous systems. One is an interaction with small-sized 
screen objects. Like Tablescape Plus [Kakehi et al. 2006], we can utilize tabletop objects as projec­tion 
screens and input interfaces. As for the other function, this sys­tem also works as a view-dependent 
image projection. Though the Tablescape Plus was designed for a particular direction, the tabletop screen 
on UlteriorScape is physically single, but visually multiple like Lumisight Table [Kakehi et al. 2004] 
and it can show appropri­ate images to each user surrounding the table. 2 UlteriorScape In the UlteriorScape, 
we offer following core technical innovations. One is the novel optical design of display. This system 
is based on a screen system that is composed of Lumisty Film and Fresnel lens. It has high transparency 
for the perpendicular direction while it is diffusive from other directions. By projecting images from 
transparent direction, it allows us to reveal additional images just by holding a simple screen over 
the tabletop. On the other hand, when an image is projected onto the screen from the opaque direction, 
users can see the image from the projector in front of them on the tabletop screen. Secondly, this system 
has also the function of capturing the ap­pearance of the tabletop from inside the system by using the 
trans­parency of the screen.By attaching a marker of a known shape un­derneath the screen object, their 
ID, position and rotation can be recognized in real-time. Note that all devices are installed inside 
the table and users need not wear any special equipment for table­top interaction. Third innovation is 
the calibration of projected images so that the system can project images appropriately according to 
the object *e-mail: tabletop@hc.ic.i.u-tokyo.ac.jp data. By using the software-based calibration, we 
can harmonize images from each projector and also use various shape of objects which have not only horizontal 
screens but also inclined screens. We have developed three types of interactive applications. One is 
for a single user without sensors. In this application (Figure 1), when users hold paper screens over 
the aerial photo displayed on the tabletop screen, they can see the map information of the covered area. 
Besides, this sytem can change projected images interactively according to the position or ID of the 
object. In Pick up and Move application (Figure 2), by putting the screen object on the tabletop, the 
tabletop image of the covered area will be copied onto the screen object and the user can move it and 
tilt it up to easily viewable direction. We also have developed application aimed for multiple users. 
By utilizing a triangular shaped object, UlteriorScape can display dif­ferent images on each side of 
the triangle independently. In addi­tion, the tabletop also works as a view-dependent display which can 
show different images to each user around the table. As an exam­ple of multiple user applications, Figure 
3 shows a tabletop theater. Character s images from opposite angle are projected on each side of the 
triangular shaped objects and users can control them with intuitive manipulations. In the future, we 
plan to implement more applications such as new type of tabletop games and educational tools.  References 
KAKEHI,Y., IIDA, M., NAEMURA,T., SHIRAI,Y., MAT-SUSHITA, M., AND OHGURO, T. 2004. Lumisight Table: Inter­active 
View-Dependent Display-Table Surrounded by Multiple Users. In SIGGRAPH 2004 Emerging Technologies, ACM. 
KAKEHI,Y., IIDA, M., NAEMURA,T., AND MATSUSHITA,M. 2006. Tablescape Plus: Upstanding Tiny Displays on 
Tabletop Display. In SIGGRAPH 2006 Emerging Technologies, ACM. RASKAR, R., WELCH, G., LOW, K., AND BANDYOPADHYAY, 
D. 2001. Shader Lamps: Animating Real Objects with Image-Based Illumination. In Eurographics Workshop 
on Rendering, 89 102. Copyright is held by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, 
August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401657</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>4</pages>
		<display_no>42</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[wUbi-Pen]]></title>
		<subtitle><![CDATA[windows graphical user interface interacting with haptic feedback stylus]]></subtitle>
		<page_from>1</page_from>
		<page_to>4</page_to>
		<doi_number>10.1145/1401615.1401657</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401657</url>
		<abstract>
			<par><![CDATA[<p>In this work, we designed stylus type haptic interfaces interacting with a touch screen. There are two kinds of haptic styli termed wUbi-Pen. The type I has functions of providing vibration, impact, texture and sound and it is a stand alone system including own battery and wireless communication module. The type II does not include the texture display module and it is a miniaturized version. We present a new interaction scheme on the Windows graphical user interface based on pointer movement haptic feedback events such as clicking, drag, drop, moving and etc. In addition, a simple interactive digital sketchbook has been implemented, which is providing haptic and auditory feedback while drawing and touching objects. We also introduce a tactile image display method on a touch screen with the wUbi-Pen and a simple fitting puzzle utilizing haptic feedback events.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[GUI]]></kw>
			<kw><![CDATA[haptic]]></kw>
			<kw><![CDATA[image]]></kw>
			<kw><![CDATA[pen]]></kw>
			<kw><![CDATA[stylus]]></kw>
			<kw><![CDATA[tactile]]></kw>
			<kw><![CDATA[windows]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1099970</person_id>
				<author_profile_id><![CDATA[81100508504]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ki-Uk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kyung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronics and Telecommunications Research Institute (ETRI), Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1099971</person_id>
				<author_profile_id><![CDATA[81365593804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun-Young]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronics and Telecommunications Research Institute (ETRI), Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1372639</ref_obj_id>
				<ref_obj_pid>1371606</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kyung, K. U. and Lee, J. Y. 2008, Design and Applications of a Pen-Like Haptic Interface with Texture and Vibrotactile Display, <i>IEEE Computer Graphics and Applications</i>, IEEE Press, In Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Immersion Corporation, The TouchSense System, http://www.immersion.com/industrial/docs/touchscreen_may06_V2-LR.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1029682</ref_obj_id>
				<ref_obj_pid>1029632</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lee, J. C., Dietz, P. H., Leigh, D., Yerazunis, W. S., and Hudson, S. E. 2004, Haptic Pen: A Tactile Feedback Stylus for Touch Screens. <i>In Proceedings of the SIGCHI conference on Human Factors in computing systems (CHI 2004)</i>, ACM Press, 291--294.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278304</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lee, W., Pak, J., Kim, H., and Lee, G. 2007, TransPen &amp; MimeoPad: A Playful Interface for Transferring a Graphic Image to Paper by Digital Rubbing. <i>International Conference on Computer Graphics and Interactive Techniques archive: ACM SIGGRAPH 2007 emerging technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124800</ref_obj_id>
				<ref_obj_pid>1124772</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Luk, J., Pasquero, J., Little, S., Maclean, K., Levesque, V., and Hayward, V. 2006, A Role for Haptics in Mobile Interaction: Initial Design Using a Handheld Tactile Display Prototype. <i>In Proceedings of the SIGCHI conference on Human Factors in computing systems (CHI 2006)</i>, ACM Press, 171--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>506525</ref_obj_id>
				<ref_obj_pid>506443</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Poupyrev, I., Shigeaki, M., and Rekimoto, J. 2002. TouchEngine: A Tactile Display for Handheld Devices. <i>In Proceedings of the SIGCHI conference on Human Factors in computing systems (CHI 2002)</i>, ACM Press, 644--645.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 wUbi-Pen : Windows Graphical User Interface Interacting with Haptic Feedback Stylus Ki-Uk Kyung* Jun-Young 
Lee POST-PC Research Group, Electronics and Telecommunications Research Institute (ETRI), Korea Abstract 
In this work, we designed stylus type haptic interfaces interacting with a touch screen. There are two 
kinds of haptic styli termed wUbi-Pen. The type I has functions of providing vibration, impact, texture 
and sound and it is a stand alone system including own battery and wireless communication module. The 
type II does not include the texture display module and it is a miniaturized version. We present a new 
interaction scheme on the Windows graphical user interface based on pointer movement haptic feedback 
events such as clicking, drag, drop, moving and etc. In addition, a simple interactive digital sketchbook 
has been implemented, which is providing haptic and auditory feedback while drawing and touching objects. 
We also introduce a tactile image display method on a touch screen with the wUbi-Pen and a simple fitting 
puzzle utilizing haptic feedback events. CR Categories: H.5.1 [Information Interface and Presentation]: 
User Interfaces; H.1.2 [Models and Principles]: User/Machine Systems Keywords: haptic, pen, stylus, GUI, 
windows, image, tactile 1. Introduction &#38; Research Objective Touch screen has been widely adopted 
for the purpose of visual display and input device of mobile devices, kiosks and etc. Application of 
touch screen reduces additional input devices or interfaces of a system. However, the most common complaint 
of users when they manipulate user interface on a touch screen is uncertainty. Fore example, they are 
not sure whether they pressed a button or not although there was a visual feedback. In order to make 
up this uncertainty, there have been efforts providing haptic feedback in the use of touch screen. A 
tactile display for handheld devices TouchEngine was suggested [Poupyrev et al. 2002]. It creates instantaneous 
force when a user operates button. However, it could be applied to only small screen device since magnitude 
of the force was not enough. There was another trial to provide tactile feedback for a mobile device 
[Luk et al. 2006]. Array of shear stimulator conveys pattern information to users. However, it requires 
considerable amount efforts to electronics to be applied to a mobile device. More practically, tactile 
feedback actuators were installed at the corners between touch panel and display screen [Immersion Corporation]. 
When we install actuators inside the device, actuator usually requires strong output power to create 
*e-mail: kyungku@etri.re.kr e-mail: dozob@etri.re.kr recognizable haptic response. If we increase power 
of actuators, electronics problem should be solved for mobile devices. If we decrease power of actuators, 
a system does not provide available haptic feedback. The styli are most commonly adopted input interface 
of a touch screen in these days. There have been researches for separated haptic input interfaces in 
the form of stylus. Lee et al. [2004] achieved initial work for a haptic stylus. They installed a pressure 
sensor at the tip and a solenoid inside the body of the pen. It provides button clicking and buzzing 
sense. Recently, another approach adopted a similar device for a different application [Lee et al. 2007]. 
More recently, a pen-like haptic interface providing vibrotactile feedback and texture feedback was suggested 
[Kyung &#38; Lee. 2008]. All researches shows haptic styli are useful for manipulation of graphical user 
interface. However, miniaturization of their mechanism and electronics is required for realizing an interface 
of mobile devices. Our work has been focused on a mobile and wireless stylus termed wUbi-Pen providing 
various combinations of haptic stimuli. It provides vibration, bilateral impacts and sound feedback in 
accordance with manipulation events. We introduce a windows graphical user interface interacting with 
haptic feedback stylus. The interaction deals with buttons, pick up, drop, drag, scroll, size control 
and menu movement/selection. We suggest a simple interactive digital sketchbook has been implemented, 
which is providing haptic and auditory feedback while drawing and touching objects. We also introduce 
a tactile image display method on a touch screen with the wUbi-Pen. In addition, a simple fitting puzzle 
utilizing haptic feedback events will be introduced for the performance evaluation of proposed scheme. 
2. wUbi-Pen 2.1 Type I Fig. 1 shows an implemented version of the wUbi-Pen type I. As shown in Fig. 2, 
we installed a vibrator, an impact generator, speaker and a texture display module in the stylus. When 
we use the stylus as a pointing device on a touch screen, events occurred in computing device are transmitted 
to the stylus through Bluetooth wireless communication. Figure 1. wUbi-Pen Type I Copyright is held 
by the author / owner(s). SIGGRAPH 2008, Los Angeles, California, August 11 15, 2008. ISBN 978-1-60558-466-9/08/0008 
 Figure 2. Internal Structure of the wUbi-Pen Type I The vibrator generates vibration. The impact generator 
is a linear vibrator which produces a click-like sensation with bilateral collision of a mass. The generator 
is arranged along a longitudinal axis of the stylus housing. The texture display unit is composed of 
3x3 pin array. Each pin creates indentation on the skin and combination of pins movements represents 
tactile patterns. Battery and Bluetooh module enable the stylus to operate as a stand-alone system. Two 
profiles of serial protocol and headset have been installed in a Bluetooth module. 2.2 Type II As shown 
in Fig. 3 and Fig. 4 wUbi-Pen type II is a miniaturized version of type I. The texture display unit has 
been removed and the space has been replaced by a microphone. Since power consumption of the type II 
is less than 20% of type I, its battery is smaller version. The size of the type II will be a scale of 
pencil if it is customized. Our target is to insert the haptic stylus in the computing device such as 
PDA, mobile phone and etc.  Figure 4. Internal Structure of the wUbi-Pen Type II 3. Windows Interfaced 
based on Haptic Feedback We designed a new windows interface interacting with haptic stylus. When we 
use a touch screen device, precise manipulations are very strictly limited. Although touch screen decreases 
complexity of input interfaces, it also decreases usability. The loss of tactile feedback disturbs user 
s confidence in manipulations such as button pressing, menu selection, object/window selection/movement, 
scrolling and size control. As shown in Fig. 5, we implemented a Windows graphical user interface interacting 
with haptic feedback stylus. Manipulation events such as button pressing/releasing, pop-up or menu change 
are monitored and each event is matched with one or combination of varying vibration, short term vibration, 
falling down impact, rising up impact and continuous or gradual vibration. Figure 5. Windows GUI Interacting 
with the wUbi-Pen Button Clicking: The button clicking stimulation is composed of falling-down impact 
and rising-up impact for button pressing and releasing respectively. Menu Selection: When a menu is changed 
a short term vibration or impact is created. When the pointer moves on inactivated menu, tactile feedback 
is not provided. Object Selection/Movement: When we select an object or a window, clicking-like stimulation 
is generated. When we move the selected object, instantaneous tactile bit is generated according to the 
movement. Scroll &#38; Window Size Control: In order to guide a precise control of scrolling and enlarging 
window, tactile bit is generated in accordance with every event. Close, Maximize &#38; Minimize: There 
is short term vibration or varying vibration to indicate execution of command. 4. Interactive Drawings 
4.1 Interactive Digital Sketchbook Fig. 6 shows an example of digital interactive sketchbook. In these 
days, digital book has become new education or entertainment system for children. We add interactive 
functions to the digital book. The developed system provides following functions. Fig. 7 shows a young 
boy drawing a frog using the wUbi-Pen. Free Drawing: There is vibration feedback and sound feedback when 
a user draws on a touch screen. Vibration and sound intensities varies in accordance with drawing velocity. 
Contour Drawing: When a user draw contour exactly, there is a soft beating feedback. However, when the 
pointer deviates from the line, there is an instant vibration to guide the contour. Touching Object: 
There are objects, materials and animals in the screen. A user touches them with haptic feedback. Figure 
7. Scene of the User Test 4.2 Tactile Image Display The Ubi-pen mouse enables tactile pattern display 
on a touch screen. In a previous work, we suggest a methodology to display a texture with the Ubi-Pen 
[Kyung &#38; Lee, 2008].This program provides a symbolic pointer in the shape of a square, with a size 
of 15x15 pixels. A user can load any grayscale image. As shown in Fig. 8, when the user touches an image 
on the touch screen with the wUbi-Pen type II, the area of the cursor is divided into 9(=3x3) sub-cells 
and the average gray value of each cell is calculated. Then, this averaged gray value is converted to 
the intensity of the stimuli displayed on each pin of the tactile display. 5. Fitting Puzzle One of 
examples to verify performance of haptic styli and touch based graphical user interface, we designed 
a puzzle game. If we control objects precisely on a touch screen with a stylus, it will spread practical 
application area of touch screen devices. As shown Fig. 9, we specially designed a puzzle game. When 
we select, move, rotate and resize a polygon, tactile cues help to control precisely. In order to measure 
performance of touch based user interface, we measure task completion time and duration of each task. 
 Figure 9. Procedure of Fitting Puzzle 6. Conclusion In this work, we introduce stylus type haptic interfaces 
interacting with a touch screen and their applications. The styli provide tactile cues including vibration, 
impact, distributed pressure and sound. The haptic styli improve usability of touch screen devices. They 
are applied to touch based Windows user interface and interactive drawings on touch screen devices. The 
styli will be miniaturized to be inserted into mobile devices. Acknowledgment This work has been supported 
by the IT R&#38;D Program of MIC/IITA[2007-S032-01, Development of an Intelligent Service technology 
based on the Personal Life Log]. References KYUNG, K. U. AND LEE, J. Y. 2008, Design and Applications 
of a Pen-Like Haptic Interface with Texture and Vibrotactile Display, IEEE Computer Graphics and Applications, 
IEEE Press, In Press. IMMERSION CORPORATION, THE TOUCHSENSE SYSTEM, http://www.immersion.com/industrial/docs/touchscreen_may 
06_V2-LR.pdf LEE, J. C., DIETZ, P. H., LEIGH, D., YERAZUNIS, W. S., AND HUDSON, S. E. 2004, Haptic Pen: 
A Tactile Feedback Stylus for Touch Screens. In Proceedings of the SIGCHI conference on Human Factors 
in computing systems (CHI 2004), ACM Press, 291-294. LEE, W., PAK, J., KIM, H., AND LEE, G. 2007, TransPen 
&#38; MimeoPad: A Playful Interface for Transferring a Graphic Image to Paper by Digital Rubbing. International 
Conference on Computer Graphics and Interactive Techniques archive: ACM SIGGRAPH 2007 emerging technologies. 
LUK, J., PASQUERO, J., LITTLE, S., MACLEAN, K., LEVESQUE, V., AND HAYWARD, V. 2006, A Role for Haptics 
in Mobile Interaction: Initial Design Using a Handheld Tactile Display Prototype. In Proceedings of the 
SIGCHI conference on Human Factors in computing systems (CHI 2006), ACM Press, 171-180. POUPYREV, I., 
SHIGEAKI, M., AND REKIMOTO, J. 2002. TouchEngine: A Tactile Display for Handheld Devices. In Proceedings 
of the SIGCHI conference on Human Factors in computing systems (CHI 2002), ACM Press, 644-645.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>IT R&#38;#38;D Program of MIC/IITA</funding_agency>
			<grant_numbers>
				<grant_number>2007-S032-01</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
</content>
</proceeding>
