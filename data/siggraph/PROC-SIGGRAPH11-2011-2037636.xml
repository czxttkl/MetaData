<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/07/2011</start_date>
		<end_date>08/11/2011</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Vancouver]]></city>
		<state>British Columbia</state>
		<country>Canada</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2037636</proc_id>
	<acronym>SIGGRAPH '11</acronym>
	<proc_desc>ACM SIGGRAPH 2011 Courses</proc_desc>
	<conference_number>2011</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0967-7</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2011</copyright_year>
	<publication_date>08-07-2011</publication_date>
	<pages>1251</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>In SIGGRAPH 2011 Courses, attendees learn from the experts in the field and gain inside knowledge that is critical to career advancement. Courses are short (1.5 hours) or half-day (3.25 hours) structured sessions that often include elements of interactive demonstration, performance, or other imaginative approaches to teaching.</p> <p>The spectrum of Courses ranges from an introduction to the foundations of computer graphics and interactive techniques for those new to the field to advanced instruction on the most current techniques and topics. Courses include core curricula taught by invited instructors as well as Courses selected from juried proposals.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2808966</person_id>
			<author_profile_id><![CDATA[81100069081]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Ann]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[McNamara]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Texas A&M University]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>2037637</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>75</pages>
		<display_no>1</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D spatial interaction]]></title>
		<subtitle><![CDATA[applications for art, design, and science]]></subtitle>
		<page_from>1</page_from>
		<page_to>75</page_to>
		<doi_number>10.1145/2037636.2037637</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037637</url>
		<abstract>
			<par><![CDATA[<p>3D interfaces use motion sensing, physical input, and spatial interaction techniques to effectively control highly dynamic virtual content. Now, with the advent of the Nintendo Wii, Sony Move, and Microsoft Kinect, game developers and researchers must create compelling interface techniques and game-play mechanics that make use of these technologies. At the same time, it is becoming increasingly clear that emerging game technologies are not just going to change the way we play games, they are also going to change the way we make and view art, design new products, analyze scientific datasets, and more.</p> <p>This introduction to 3D spatial interfaces demystifies the workings of modern videogame motion controllers and provides an overview of how it is used to create 3D interfaces for tasks such as 2D and 3D navigation, object selection and manipulation, and gesture-based application control. Topics include the strengths and limitations of various motion-controller sensing technologies in today's peripherals, useful techniques for working with these devices, and current and future applications of these technologies to areas beyond games. The course presents valuable information on how to utilize existing 3D user-interface techniques with emerging technologies, how to develop interface techniques, and how to learn from the successes and failures of spatial interfaces created for a variety of application domains.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Documentation</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808967</person_id>
				<author_profile_id><![CDATA[81100283513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[LaViola]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808968</person_id>
				<author_profile_id><![CDATA[81100317632]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Keefe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1166260</ref_obj_id>
				<ref_obj_pid>1166253</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akers, D. 2006. CINCH: A cooperatively designed marking interface for 3D pathway selection. In <i>Symposium on User Interface Software and Technology</i>, 33--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>769960</ref_obj_id>
				<ref_obj_pid>769953</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Anderson, L., Esser, J., and Interrante, V. 2003. A virtual environment for conceptual design in architecture. In <i>Proceedings of the workshop on Virtual environments 2003</i>, ACM, New York, NY, USA, EGVE '03, 57--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Azuma, R., and Bishop, G. 1994. Improving static and dynamic registration in an optical see-through hmd. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1536527</ref_obj_id>
				<ref_obj_pid>1536513</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bott, J., Crowley, J., and LaViola, J. 2009. Exploring 3d gestural interfaces for music creation in video games. In <i>Proceedings of The Fourth International Conference on the Foundations of Digital Games 2009</i>, 18--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253301</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., and Hodges, L. F. 1997. An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 35--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836072</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Koller, D., and Hodges, L. F. 1997. Travel in immersive virtual environments: An evaluation of viewpoint motion control techniques. In <i>Proceedings of the Virtual Reality Annual International Symposium</i>, 45--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618556</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Wineman, J., Hodges, L. F., and Allison, D. 1998. Designing animal habitats within an immersive ve. <i>IEEE Comput. Graph. Appl. 18</i>, 5, 9--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993837</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Kruijff, E., LaViola, J. J., and Poupyrev, I. 2004. <i>3D User Interfaces: Theory and Practice</i>. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Brody, B., and Hartman, C. 2000. Painting space with BLUI. In <i>SIGGRAPH '00 Conference Abstracts and Applications</i>, 242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>57168</ref_obj_id>
				<ref_obj_pid>57167</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Brooks, F. P. 1988. Grasping reality through illusion: interactive graphics serving science. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM, New York, NY, USA, CHI '88, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Brown University Literary Arts Program, 2011. http://www.brown.edu/departments/literary-arts.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949612</ref_obj_id>
				<ref_obj_pid>949607</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Bryson, S., and Levit, C. 1991. The virtual windtunnel: an environment for the exploration of three-dimensional unsteady flows. In <i>Proceedings of the 2nd conference on Visualization '91</i>, IEEE Computer Society Press, Los Alamitos, CA, USA, VIS '91, 17--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147182</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Butterworth, J., Davidson, A., Hench, S., and Olano, M. T. 1992. 3DM: A three dimensional modeler using a head-mounted display. In <i>Symposium on Interactive 3D Graphics</i>, 135--138.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1581092</ref_obj_id>
				<ref_obj_pid>1581073</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Charbonneau, E., Miller, A., Wingrave, C., and LaViola, JR., J. J. 2009. Understanding visual interfaces for the next generation of dance-based rhythm video games. In <i>Sandbox '09: Proceedings of the 2009 ACM SIGGRAPH Symposium on Video Games</i>, ACM Press, 119--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360329</ref_obj_id>
				<ref_obj_pid>360303</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Clark, J. H. 1976. Designing surfaces in 3-D. <i>Communications of the ACM 19</i>, 8, 454--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1940048</ref_obj_id>
				<ref_obj_pid>1940006</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Coffey, D., Korsakov, F., and Keefe, D. F. 2010. Low cost VR meets low cost multi-touch. In <i>Proceedings of International Symposium on Visual Computing</i>, 351--360.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944777</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Coffey, D., Malbraaten, N., Le, T., Borazjani, I., Sotiropoulos, F., and Keefe, D. F. 2011. Slice WIM: a Multi-Surface, Multi-Touch interface for Overview+Detail exploration of volume datasets in virtual reality. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>, 191--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147199</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Conner, B. D., Snibbe, S. S., Herndon, K. P., Robbins, D. C., Zeleznik, R. C., and van Dam, A. 1992. Three-dimensional widgets. In <i>SI3D '92: Proceedings of the 1992 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 183--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Crassidis, J. L., and Markley, F. L. 2003. Unscented filtering for spacecraft attitude estimation. <i>Journal of Guidance, Control, and Dynamics 26</i>, 4, 536--542.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134039</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Deering, M. 1992. High resolution virtual reality. In <i>SIGGRAPH '92 Conference on Computer Graphics and Interactive Techniques</i>, 195--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>210087</ref_obj_id>
				<ref_obj_pid>210079</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Deering, M. F. 1995. Holosketch: A virtual reality sketching/animation tool. <i>ACM Transactions of Computer-Human Interaction 2</i>, 3, 220--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>954544</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Duda, R. O., Hart, P. E., and Stork, D. G. 2001. <i>Pattern Classification</i>. John Wiley and Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>168657</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., MacIntyre, B., Haupt, M., and Solomon, E. 1993. Windows on the world: 2d windows for 3d augmented reality. In <i>UIST '93: Proceedings of the 6th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 145--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ferguson, M., 2003. June. Film created with the SANDDE animation system. National Film Board of Canada http://www.nfb.ca.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Ferguson, M., 2006. Falling in love again. Film created with the SANDDE animation system. National Film Board of Canada http://www.nfb.ca.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Friedman, J. H. 1996. Another approach to polychotomous classification. Tech. rep., Department of Statistics, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>332491</ref_obj_id>
				<ref_obj_pid>332040</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Fr&#246;hlich, B., and Plate, J. 2000. The cubic mouse: a new device for three-dimensional input. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM, New York, NY, USA, CHI '00, 526--531.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122747</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Galyean, T. A., and Hughes, J. F. 1991. Sculpting: An interactive volumetric modeling technique. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH 91</i>), vol. 25, ACM, 267--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Giansanti, D., Macellari, V., and Maccioni, G. 2003. Is it feasible to reconstruct body segment 3-d position and orientation using accelerometric data. <i>IEEE Trans. Biomed. Eng 50</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835797</ref_obj_id>
				<ref_obj_pid>832288</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Gregory, A. D., Ehmann, S. A., and Lin, M. C. 2000. inTouch: Interactive multiresolution modeling and 3D painting with a haptic interface. In <i>Proceedings of IEEE VR 2000</i>, 45--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Grey, J. 2002. Human-computer interaction in life drawing, a fine artist's perspective. In <i>Proceedings of the Sixth International Conference on Information Visualisation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1029644</ref_obj_id>
				<ref_obj_pid>1029632</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Grossman, T., Wigdor, D., and Balakrishnan, R. 2004. Multi-finger gestural interaction with 3d volumetric displays. In <i>Proceedings of the 17th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, UIST '04, 61--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Gunturk, B. K., Glotzbach, J., Altunbasak, Y., and Schafer, R. W. 2005. Demosaicking: color filter array interpolation. <i>IEEE Signal processing magazine 22</i>, 44--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477456</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Hentschel, B., Tedjo, I., Probst, M., Wolter, M., Behr, M., Bischof, C., and Kuhlen, T. 2008. Interactive blood damage analysis for ventricular assist devices. <i>IEEE Transactions on Visualization and Computer Graphics 14</i>, 6, 1515--1522.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Hinckley, K., Pausch, R., Downs, J. H., Proffitt, D., and Kassell, N. F. 1997. The props-based interface for neurosurgical visualization. <i>Studies in Health Technology and Informatics 39</i>, 552--562. PMID: 10168950.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2196253</ref_obj_id>
				<ref_obj_pid>2195920</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Hoffman, M., Varcholik, P., and LaViola, J. 2010. Breaking the status quo: Improving 3d gesture recognition with spatially convenient input devices. In <i>IEEE Virtual Reality 2010</i>, IEEE Press, 59--66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009310</ref_obj_id>
				<ref_obj_pid>1009231</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Hua, J., and Qin, H. 2004. Haptics-based dynamic implicit solid modeling. <i>IEEE Transactions on Visualization and Computer Graphics 10</i>, 5, 574--586.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Jackson, B., and Keefe, D. F. 2011. Sketching over props: Understanding and interpreting 3d sketch input relative to rapid prototype props. In <i>Proceedings of IUI 2011 Sketch Recognition Workshop</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Julier, S., and Uhlmann, J. 1997. A new extension of the kalman filter to nonlinear systems. In <i>Int. Symp. Aerospace/Defense Sensing, Simul. and Controls</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364370</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F., Feliz, D. A., Moscovich, T., Laidlaw, D. H., and LaViola Jr., J. J. 2001. CavePainting: A fully immersive 3D artistic medium and interactive experience. In <i>Proceedings of I3D 2001</i>, 85--93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231912</ref_obj_id>
				<ref_obj_pid>2231878</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F., Zeleznik, R. C., and Laidlaw, D. H. 2007. Drawing on air: Input techniques for controlled 3D line illustration. <i>IEEE Transactions on Visualization and Computer Graphics 13</i>, 5, 1067--1081.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1373254</ref_obj_id>
				<ref_obj_pid>1373109</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F., Acevedo, D., Miles, J., Drury, F., Swartz, S. M., and Laidlaw, D. H. 2008. Scientific sketching for collaborative VR visualization design. <i>IEEE Transactions on Visualization and Computer Graphics 14</i>, 4, 835--847.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544220</ref_obj_id>
				<ref_obj_pid>1544196</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F., Zeleznik, R. C., and Laidlaw, D. H. 2008. Tech-note: Dynamic dragging for input of 3D trajectories. In <i>Proceedings of IEEE Symposium on 3D User Interfaces 2008</i>, 51--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1354476</ref_obj_id>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F. 2007. <i>Interactive 3D Drawing for Free-Form Modeling in Scientific Visualization and Art: Tools, Methodologies, and Theoretical Foundations</i>. PhD thesis, Brown University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F. 2008. Free-form vr interactions in scientific visualization.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F., 2009. Creative 3d form-making in visual art and visual design for science. CHI 2009 Workshop on Computational Creativity Support: Using Algorithms and Machine Learning to Help People Be More Creative, April.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1749422</ref_obj_id>
				<ref_obj_pid>1749397</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F. 2010. Integrating visualization and interaction research to improve scientific workflows. <i>IEEE Computer Graphics and Applications 30</i>, 2, 8--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F. 2011, In press. From gesture to form: The evolution of expressive freehand spatial interfaces. <i>Leonardo</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Kitware, Inc. Paraview: Open source scientific visualization. http://www.paraview.org/. Accessed May, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944794</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Konchada, V., Jackson, B., Le, T., Borazjani, I., Sotiropoulos, F., and Keefe, D. F. 2011. Supporting internal visualization of biomedical datasets via 3d rapid prototypes and sketch-based gestures. In <i>Poster Proceedings of ACM Symposium on Interactive 3D Graphics and Games</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1450659</ref_obj_id>
				<ref_obj_pid>1450579</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Konieczny, J., Meyer, G., Shimizu, C., Heckman, J., Manyen, M., and Rabens, M. 2008. Vr spray painting for training and design. In <i>Proceedings of the 2008 ACM symposium on Virtual reality software and technology</i>, ACM, New York, NY, USA, VRST '08, 293--294.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1328241</ref_obj_id>
				<ref_obj_pid>1328202</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Kratz, L., Smith, M., and Lee, F. J. 2007. Wiizards: 3d gesture recognition for game play input. In <i>Future Play '07: Proceedings of the 2007 conference on Future Play</i>, ACM, New York, NY, USA, 209--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667118</ref_obj_id>
				<ref_obj_pid>1667117</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Kruszynski, K., and van Liere, R. 2009. Tangible props for scientific visualization: concept, requirements, application. <i>Virtual Reality 13</i>, 235--244. 10.1007/s10055-009-0126-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313289</ref_obj_id>
				<ref_obj_pid>1313055</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. J., and Zeleznik, R. C. 2007. A practical approach for writer-dependent symbol recognition using a writer-independent symbol recognizer. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 29</i>, 11, 1917--1926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. 2000. Msvt: A virtual reality-based multimodal scientific visualization tool. In <i>Proceedings of the Third IASTED International Conference on Computer Graphics and Imaging</i>, 1--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>769976</ref_obj_id>
				<ref_obj_pid>769953</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. J. 2003. Double exponential smoothing: an alternative to kalman filter-based predictive tracking. In <i>EGVE '03: Proceedings of the workshop on Virtual environments 2003</i>, ACM, New York, NY, USA, 199--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344341</ref_obj_id>
				<ref_obj_pid>344326</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Luinge, H., Veltink, P., and Baten, C. 1999. Estimating orientation with gyroscopes and ac-celerometers. <i>Technology and Health Care 7</i>, 6, 455--459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[M&#228;kel&#228;, W. 2005. Working 3D meshes and particles with finger tips, towards an immersive artists' interface. In <i>IEEE VR 2005 Workshop Proceedings</i>, 77--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Mapes, D., and Moshell, M. 1995. A two-handed interface for object manipulation in virtual environments. <i>Presence: Teleoper. Virtual Environ. 4</i>, 4, 403--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258747</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Mine, M. R., Brooks, Jr., F. P., and Sequin, C. H. 1997. Moving objects in space: exploiting proprioception in virtual-environment interaction. In <i>SIGGRAPH '97: Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 19--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897820</ref_obj_id>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Mine, M. 1995. Virtual environment interaction techniques. Tech. rep., UNC Chapel Hill CS Dept.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Morstad, P., 2004. Moon man. Film created with the SANDDE animation system. National Film Board of Canada http://www.nfb.ca.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218495</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Pausch, R., Burnette, T., Brockway, D., and Weiblen, M. E. 1995. Navigation and locomotion in virtual worlds via flight into hand-held miniatures. In <i>SIGGRAPH '95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 399--400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218495</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Pausch, R., Burnette, T., Brockway, D., and Weiblen, M. E. 1995. Navigation and locomotion in virtual worlds via flight into hand-held miniatures. In <i>Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, ACM, 399--400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253303</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Pierce, J. S., Forsberg, A. S., Conway, M. J., Hong, S., Zeleznik, R. C., and Mine, M. R. 1997. Image plane interaction techniques in 3d immersive environments. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 39--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237102</ref_obj_id>
				<ref_obj_pid>237091</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Poupyrev, I., Billinghurst, M., Weghorst, S., and Ichikawa, T. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in vr. In <i>UIST '96: Proceedings of the 9th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 79--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124097</ref_obj_id>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Razzaque, S. 2005. <i>Redirected walking</i>. PhD thesis, Chapel Hill, NC, USA. AAI3190299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531517</ref_obj_id>
				<ref_obj_pid>1531514</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Rehm, M., Bee, N., and Andr&#233;, E. 2008. Wave like an egyptian: accelerometer based gesture recognition for culture specific interactions. In <i>BCS-HCI '08: Proceedings of the 22nd British HCI Group Annual Conference on HCI 2008</i>, British Computer Society, Swinton, UK, UK, 13--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1242205</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Rubin, C. B., and Keefe, D. F., 2002. Hiding spaces: A cave of elusive immateriality. ACM SIGGRAPH 2002 Conference Abstracts and Applications, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122753</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Rubine, D. 1991. Specifying gestures by example. In <i>SIGGRAPH '91: Proceedings of the 18th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 329--337.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617702</ref_obj_id>
				<ref_obj_pid>616020</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Sachs, E., Roberts, A., and Stoops, D. 1991. 3-draw: A tool for designing 3D shapes. <i>IEEE Computer Graphics and Applications 11</i>, 6, 18--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1624417</ref_obj_id>
				<ref_obj_pid>1624312</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Schapire, R. E. 1999. A brief introduction to boosting. In <i>IJCAI '99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</i>, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1401--1406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>365114</ref_obj_id>
				<ref_obj_pid>365024</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Schkolne, S., Pruett, M., and Schr&#246;der, P. 2001. Surface drawing: creating organic 3D shapes with the hand and tangible tools. In <i>Proceedings of CHI '01</i>, 261--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1034535</ref_obj_id>
				<ref_obj_pid>1032664</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Schkolne, S., Ishii, H., and Schr&#246;der, P. 2004. Immersive design of DNA molecules with a tangible interface. In <i>Proceedings of IEEE Visualization</i>, 227--234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Schkolne, S. 2003. <i>3D Interfaces for Spatial Construction</i>. PhD thesis, California Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1347395</ref_obj_id>
				<ref_obj_pid>1347390</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Schl&#246;mer, T., Poppinga, B., Henze, N., and Boll, S. 2008. Gesture recognition with a wii controller. In <i>TEI '08: Proceedings of the 2nd international conference on Tangible and embedded interaction</i>, ACM, New York, NY, USA, 11--14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801156</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Schmandt, C. 1983. Spatial input/display correspondence in a stereoscopic computer graphic work station. In <i>SIGGRAPH '83: Proceedings of the 10th Annual Conference on Computer Graphics and Interactive Techniques</i>, 253--261.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Schroering, M., Grimm, C., and Pless, R. 2003. A new input device for 3D sketching. In <i>Vision Interface</i>, 311--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[SensAble Technologies, Inc. 2005. <i>FreeForm Concept Product Brochure</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Sheppard, R., and Nahrstedt, K. 2009. Merging research modalities: TED (tele-immersive dance) collaboration offers a model for performance-based research and creative development. In <i>Proceedings of the Computational Creativity Support Workshop at ACM CHI 2009</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1274966</ref_obj_id>
				<ref_obj_pid>1274940</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Shirai, A., Geslin, E., and Richir, S. 2007. Wiimedia: motion analysis methods and applications using a consumer video game controller. In <i>Sandbox '07: Proceedings of the 2007 ACM SIGGRAPH symposium on Video games</i>, ACM, New York, NY, USA, 133--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409076</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Shiratori, T., and Hodgins, J. K. 2008. Accelerometer-based user interfaces for the control of a physically simulated character. <i>ACM Trans. Graph. 27</i>, 5, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Shivaram, G., and Seetharaman, G. 1998. A new technique for finding the optical center of cameras. In <i>ICIP 98: Proceedings of 1998 International Conference on Image Processing</i>, vol. 2, 167--171.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2192047</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Shotton, J., Fizgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A., and Blake, A. 2011. Real-time human pose recognition in parts from single depth images. In <i>IEEE Computer Vision and Pattern Recognition (CVPR) 2011</i>, IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Snibbe, S., Anderson, S., and Verplank, B. 1998. Springs and constraints for 3D drawing. In <i>Proceedings of the Third Phantom Users Group</i>. M. I. T. Artificial Intelligence Laboratory Technical Report AITR-1643.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Snibbe, S., 1998. Bondary functions, installation by scott snibbe. http://www.snibbe.com/. Accessed May, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1032321</ref_obj_id>
				<ref_obj_pid>1032273</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Sobel, J. S., Forsberg, A. S., Laidlaw, D. H., Zeleznik, R. C., Keefe, D. F., Pivkin, I., Karniadakis, G. E., Richardson, P., and Swartz, S. 2004. Particle flurries: Synoptic 3D pulsatile flow visualization. <i>IEEE Computer Graphics and Applications 24</i>, 2 (March/April), 76--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1166258</ref_obj_id>
				<ref_obj_pid>1166253</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Song, H., Guimbreti&#232;re, F., HU, C., and Lipson, H. 2006. Modelcraft: capturing freehand annotations and edits on physical 3d models. In <i>UIST '06: Proceedings of the 19th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 13--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1549869</ref_obj_id>
				<ref_obj_pid>1549820</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Steinicke, F., Bruder, G., Hinrichs, K., Steed, A., and Gerlach, A. L. 2009. Does a gradual transition to the virtual world increase presence? In <i>Proceedings of the 2009 IEEE Virtual Reality Conference</i>, IEEE Computer Society, Washington, DC, USA, 203--210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>223938</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Stoakley, R., Conway, M. J., and Pausch, R. 1995. Virtual reality on a wim: interactive worlds in miniature. In <i>CHI '95: Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>223938</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Stoakley, R., Conway, M. J., and Pausch, R. 1995. Virtual reality on a WIM: interactive worlds in miniature. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM Press/Addison-Wesley Publishing Co., Denver, Colorado, United States, 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1476686</ref_obj_id>
				<ref_obj_pid>1476589</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. 1968. A head-mounted three dimensional display. In <i>Proceedings of the AFIPS Fall Joint Computer Conference</i>, vol. 33, 757--764.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Tinkerman, 2008. Tinkermans method - casting textured silcone, http://nuigroup.com/forums/viewthread/2383/, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1567671</ref_obj_id>
				<ref_obj_pid>1567653</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Trueba, R., Andujar, C., and Argelaguet, F. 2009. Complexity and occlusion management for the World-in-Miniature metaphor. In <i>Smart Graphics</i>. 155--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1979142</ref_obj_id>
				<ref_obj_pid>1978942</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Valkov, D., Steinicke, F., Bruder, G., and Hinrichs, K. 2011. 2d touching of 3d stereoscopic objects. In <i>Proceedings of the 2011 annual conference on Human factors in computing systems</i>, ACM, New York, NY, USA, CHI '11, 1353--1362.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1581082</ref_obj_id>
				<ref_obj_pid>1581073</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Varcholik, P. D., LaViola, Jr., J. J., and Hughes, C. 2009. The bespoke 3dui xna framework: a low-cost platform for prototyping 3d spatial interfaces in video games. In <i>Proceedings of the 2009 ACM SIGGRAPH Symposium on Video Games</i>, ACM, New York, NY, USA, Sandbox '09, 55--61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Wan, E. A., and Van Der Merwe, R. 2002. The unscented kalman filter for nonlinear estimation. 153--158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>505041</ref_obj_id>
				<ref_obj_pid>505008</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Wesche, G., and Seidel, H.-P. 2001. Freedrawer: a free-form sketching system on the responsive workbench. In <i>Proceedings of VRST 2001</i>, 167--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Williamson, R., and Andrews, B. 2001. Detecting absolute human knee angle and angular velocity using accelerometers and rate gyroscopes. <i>Medical and Biological Engineering and Computing 39</i>, 3, 294--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2013903</ref_obj_id>
				<ref_obj_pid>2013876</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Williamson, B., Wingrave, C., and LaViola, J. 2010. Realnav: Exploring natural user interfaces for locomotion in video games. In <i>Proceedings of IEEE Symposium on 3D User Interfaces 2010</i>, IEEE Computer Society, 3--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1130524</ref_obj_id>
				<ref_obj_pid>1134820</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Wingrave, C. A., Haciahmetoglu, Y., and Bowman, D. A. 2006. Overcoming world in miniature limitations by a scaled and scrolling WIM. In <i>Proceedings of the IEEE conference on Virtual Reality</i>, IEEE Computer Society, 11--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1749428</ref_obj_id>
				<ref_obj_pid>1749397</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Wingrave, C., Williamson, B., Varcholik, P., Rose, J., Miller, A., Charbonneau, E., Bott, J., and LaViola, J. 2010. Wii remote and beyond: Using spatially convenient devices for 3duis. <i>IEEE Computer Graphics and Applications 30</i>, 2, 71--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>215647</ref_obj_id>
				<ref_obj_pid>215585</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Wloka, M. M., and Greenfield, E. 1995. The virtual tricorder: a uniform interface for virtual reality. In <i>UIST '95: Proceedings of the 8th annual ACM symposium on User interface and software technology</i>, ACM, New York, NY, USA, 39--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1907937</ref_obj_id>
				<ref_obj_pid>1907651</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Zhang, N., Zhou, X., Shen, Y., and Sweet, R. 2010. Volumetric modeling in laser bph therapy simulation. <i>IEEE Transactions on Visualization and Computer Graphics 16</i> (November), 1405--1412.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Zhou, W., Correia, S., and Laidlaw, D. H. 2008. Haptics-assisted 3D lasso drawing for tracts-of-interest selection in DTI visualization.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Spatial Interaction: Applicationsfor Art, Design, and Science ASIGGRAPH 2011 Course Sunday,August,7, 
2011 2:00PM -5:15 PM Joseph J. LaViola Jr. Daniel F. Keefe Dept. of EECS Dept. of Computer Science and 
Engineering University of Central Florida University of Minnesota -Twin Cities jjl@eecs.ucf.edu keefe@cs.umn.edu 
 Contents 1 Course Introduction 1 1.1 CourseTopics ......................................... 1 1.2 
Course Schedule ....................................... 2 1.3 Speaker Biographies ..................................... 
4 2 CommonTasksin3D User Interfaces 6 2.1 Navigation........................................... 7 2.1.1 
Gaze-DirectedSteering ................................ 7 2.1.2 Pointing........................................ 
8 2.1.3 Map-basedTravel................................... 8 2.1.4 GrabbingtheAir ................................... 
9 2.2 Selection ........................................... 9 2.2.1 TheVirtualHand ................................... 
10 2.2.2 Ray-Casting ..................................... 10 2.2.3 OcclusionTechniques ................................ 
11 2.2.4 Arm-Extension.................................... 11 2.3 Manipulation ......................................... 
12 2.3.1 HOMER ....................................... 12 2.3.2 Scaled-WorldGrab .................................. 
13 2.3.3World-in-Miniature .................................. 14 2.4 System Control ........................................ 
15 2.4.1 GraphicalMenus ................................... 15 2.4.2Voice Commands ................................... 
16 2.4.3 Gesturesand Postures ................................ 17 2.4.4Tools ......................................... 
18 3 Applications 1: Spatial Interfaces in Art and Design 20 3.1 Backgroundin3DUI sforArtandDesign .......................... 
20 3.1.1 Early Research in 3D User Interfaces for Art and Design . . . . . . . . . . . . . . 20 3.1.2 
Recent Research in 3D User Interfaces for Art and Design . . . . . . . . . . . . . 21 3.2 Example3DUser 
InterfacesforArt Applications ...................... 22 3.3 Installationand InteractiveArt ................................ 
23 3.4 Applicationsin Creative3DDesign ............................. 24 4 Working withVideo Game Motion 
Controllers 25 4.1 Motion ControllerDevice Characteristics .......................... 25 4.1.1 NintendoWii 
Remote ................................ 25 4.1.2 MicrosoftKinect ................................... 
29 4.1.3 PlaystationMove................................... 31 4.2 Getting StartedandSoftwareTools 
.............................. 34 4.2.1 NintendoWii Remote ................................ 34 4.2.2 
MicrosoftKinect ................................... 35 4.2.3 PlaystationMove................................... 
36 5 Applications2: Spatial Interfacesin Scienti.cVisualization 37 5.1 3D InteractiveTechniques for 
Scienti.cVisualization . . . . . . . . . . . . . . . . . . . . 37 5.2 SelectionRevisited ...................................... 
37 5.3 NavigationRevisited ..................................... 39 5.4 Does spatial computing for 
scienti.c applications require a $1,000,000 lab? . . . . . . . . 40 6 3D Gesture RecognitionTechniques 
41 6.1 InterpretingWiimoteData .................................. 41 6.1.1 Integration ...................................... 
41 6.1.2 Recognitionthrough Heuristics ........................... 42 6.2 Linear Classi.er ........................................ 
45 6.3 AdaBoost Classi.er ...................................... 46 6.3.1Weak LearnerFormulation .............................. 
46 6.3.2 AdaBoost Algorithm ................................. 47 6.4 FeatureSet .......................................... 
48 6.5 GestureSet .......................................... 48 6.6 3D GestureData Collection .................................. 
49 6.7 3D Gesture Recognition Experiments ............................ 50 6.7.1 User Dependent Recognition 
Results ........................ 51 6.7.2 User Independent Recognition Results ........................ 
54 6.8 AccuracyDiscussion ..................................... 55 6.9 Using3D Gesturesina PracticalSetting 
........................... 55 6.9.1 UsabilityStudy .................................... 56 6.9.2 Results 
........................................ 57 6.9.3 Discussion ...................................... 
58 7 Future 2: Mix and Match Spatial Input 60 7.1 Spatialinput meets multi-touch ................................ 
60 7.1.1 Building the Minnesota InteractiveVisualization Lab3D Multi-touchWorkbench . 61 7.1.2 Future 
Outlook .................................... 63 7.2Tangiblespatialinput ..................................... 
63 7.3 Cognitiveand perceptual issues ................................ 64   1 CourseIntroduction 
3D interfaces use motion sensing, physical input, and spatial interaction techniques to effectively control 
highly dynamic virtual content[Bowmanetal. 2004].Withtheadventofthe NintendoWii,SonyMove, and Microsoft 
Kinect, game developers and researchers are now faced with the challenge of creating compelling interface 
techniquesandgameplay mechanics thatmake useof these technologies[Wingrave et al. 2010]. At the same 
time, it is becoming increasingly clear that emerginggame technologies are not just going to change theway 
we playgames, they are also going to change theway we make and view art, design new products, analyze 
scienti.c datasets, and more. Researchers in the .elds of virtual and augmented reality as well as 3D 
user interfaces have been working on 3D interaction for nearly two decades,but today, thanks to emerging 
technologies, there is great potential for 3D spatial interfaces to radically change the way that we 
approach art, design, and science. 1.1 CourseTopics This course provides an introduction to the exciting 
and rapidly changing world of 3D spatial interfaces. Presenters will demystify the workings of modern 
day video game motion controllers and provide an overview of the techniques, strategies, and algorithms 
used in creating 3D interfaces for tasks such as 2D and3D navigation, object selection and manipulation, 
and gesture-based application control.We will discuss the strengths and limitations of various motion 
controller sensing technologies found in today s peripherals and describe useful techniques for working 
with these devices, including gesture recognition and non-isomorphic control-to-display mappings. Current 
and future applications of these technologies to areasoutsideofgameswillbe discussedthrough illustrativerecentexamplesof3Dinterfacesforart[Keefe 
et al. 2007], design [Keefe 2009], and science [Coffey et al. 2011] . Presenters will identify common 
themes and also critical differences across these applications and discuss how these insights might help 
shape the future of 3D spatial interaction. Attendees will receive valuable information on how to utilize 
existing 3D user interface techniques with emerging technologies, how to develop their own interface 
techniques,andhowto learn fromthe successesandfailuresof spatial interfaces createdforavarietyof application 
domains. 1.2 Course Schedule 1.Welcome, Introduction,&#38;Roadmap 15 minutes -LaViola (a) Motivate the 
importance of the topic (b) Introduce presenters (c) Applications of 3D spatial interaction  2. CommonTasksin3D 
User Interfaces 30 minutes -LaViola (a) Selection and manipulation techniques (b) Travel techniques 
 (c) System control techniques  3. Applications1: Spatial Interfacesin Art and Design 30 minutes -Keefe 
(a) 3D sketching and sculpting (b) Installation and experiential art (c) Design, from early concepts 
to precision engineering  4.Working withVideo Game Motion Controllers 30 minutes -LaViola (a) Device 
characteristics i. NintendoWii ii. Microsoft Kinect iii. SonyMove (b) Getting Started and Software 
tools  5. Applications2: Spatial Interfacesin Scienti.cVisualization 30 minutes -Keefe (a) Selection 
revisited (b) Navigation revisited (c) Does spatial computing in science require a $1,000,000 lab? 
 6. Future1:3D Gesture RecognitionTechniques 30 minutes -LaViola (a) 3D Gestures (b) Heuristic Recognition 
 (c) Linear and AdaBoost classi.ers (d) A25 gesture open source dataset (e) Using 3D gestures in a 
practical setting  7. Future2: Mix and Match Spatial Input 30 minutes -Keefe (a) Spatial input meets 
multi-touch (b) Tangible spatial input (c) Cognitive and perceptual issues  8. Q&#38;A 10 minutes 
-LaViola andKeefe 1.3 Speaker Biographies JosephJ. LaViolaJr. SAICFaculty Fellow and Assistant Professor, 
Universityof Central Florida University of Central Florida Dept. of EECS 4000 Central Florida Blvd. Orlando, 
FL 32816 (407)882-2285 (voice) (401)823-5419 (fax) jjl@eecs.ucf.edu (email) Joseph J. LaViola Jr. is 
an assistant professor in the School of Electrical Engineering and Computer Science and directs the 
Interactive Systems and User Experience Lab at the University of Central Florida. He is also an adjunct 
assistant research professor in the Computer Science Department at Brown University. His primary research 
interests include pen-based interactive computing, 3D spatial interfaces for videogames, predictive motion 
tracking, multimodal interaction in virtual environments, and user interfaceevaluation. Hiswork has 
appearedin journals such asACMTOCHI, IEEEPAMI, Presence, and IEEE Computer Graphics&#38;Applications, 
and he has presented research at conferences includingACM SIGGRAPH,ACM CHI, theACM Symposium on Interactive 
3D Graphics, IEEEVirtual Reality, and EurographicsVirtual Environments. He has also co-authored 3D User 
Interfaces: Theory and Practice, the.rst comprehensivebookon3D user interfaces. In2009,hewonanNSF CareerAwardto 
conduct research on mathematical sketching. Joseph received a Sc.M. in Computer Science in 2000, a Sc.M. 
in Applied Mathematics in 2001, and a Ph.D. in Computer Science in 2005 from Brown University. DanielF.Keefe 
Assistant Professor, Universityof Minnesota -Twin Cities University of Minnesota Department of Computer 
Science and Engineering 4-192Keller Hall 200 Union Street SE Minneapolis, MN 55455 (612) 626-7508(voice) 
 (612) 626-7508(fax) keefe@cs.umn.edu (email)  DanKeefe is an Assistant Professor in the Department 
of Computer Science and Engineering at the University of Minnesota. His research centers on scienti.c 
data visualization (visualization of time-varying data, visualization at scale, perceptually optimized 
visualization) and interactive computer graphics (3D interfaces, haptics,penand multi-touch input).Keefe 
receivedtheNSF CAREERAwardin 2011.Hehas received best paper and best panel awards at international conferences 
for his research and teaching. In addition to his research in computer science,Keefe is also an accomplished 
artist and has published and exhibited his artwork in national and international venues. Before joining 
the University of Minnesota, Keefe did post-doctoral work at Brown University jointly with the departments 
of Computer Science and Ecology and Evolutionary Biology and with the Rhode Island School of Design. 
He received the Ph.D. in 2007 from Brown Universitys Department of Computer Science, which nominated 
his dissertation for the ACM Dissertation Prize, and the B.S. in Computer Engineering summa cum laude 
fromTufts University in 1999. Figure 1: Two examples of a 3D UI. The image on the left shows a user 
interacting with a world-inminiature and the image on the right shows a user navigating using body-based 
controls. There are essentially four basic 3D interaction tasks that are found in most complex3D applications 
[Bowman et al. 2004]. Actually, there is a .fth task called symbolic input, the ability to enter alphanumeric 
charactersina3D environment,but we will not discussit here. Obviously, there are other tasks which are 
speci.c to an application domain,but these basicbuilding blocks can often be combined to let users perform 
more complex tasks. These tasks include navigation, selection, manipulation, and system control. Navigation 
is the most common VE task, and is consists of two components. Travel is the motor component of navigation, 
and just refers to physical movement from place to place. Way.nding is the cognitive or decision-making 
component of navigation, and it asks the questions, where am I? , where doI wanttogo? , howdoIget there? 
,andsoon. Selection is simply the speci.cation of an object or a set of objects for some purpose. Manipulation 
refers to the speci.cation of object properties (most often position and orientation, but also other 
attributes). Selection and manipulation are often used together, but selection may be a stand-alone task. 
For example, the user may select an object in order to apply a command such as delete to that object. 
System control is the task of changing the system state or the mode of interaction. This is usually done 
with some type of command to the system (either explicit or implicit). Examples in 2D systems include 
menus and command-line interfaces. It is often the case that a system control technique is composed of 
the other three tasks (e.g. a menu command involves selection), but it s also useful to consider it separately 
since special techniques have been developed for it and it is quite common. There are two contrasting 
themes that are common when thinking about 3D spatial interfaces; the real and the magical. The real 
theme or style tries to bring real world interaction into the 3D environment. Thus, the goal is to try 
to mimic physical world interactions in the virtual world. Examples include direct manipulation interfaces, 
such as swinging a golf club or baseball bat or using the hand to pick up virtual objects. The magical 
theme or style goesbeyond the realworld into the realmoffantasy and science .ction. Magical techniques 
are only limitedby the imagination andexamples include spell casting, .ying, and moving virtual objects 
with levitation. Two technical approaches used in the creation of both real and magical 3D spatial interaction 
techniques are referred to as isomorphism and non-isomorphism. Isomorphism refers to a one to one mapping 
between the motion controller and the corresponding object in the virtual word. For example, if the 
motion controller moves 1.5 feet along the x axis, a virtual object moves the same distance in the virtual 
world. On the other hand, non-isomorphism refers to ability to scale the input so that the control-to-display 
ratio isnotequalto one.Forexample,ifthe motion controlleris rotated30degreesabouttheyaxis,the virtual 
objectmay rotate60degrees abouttheyaxis. Non-isomorphismisaverypowerful approachto3D spatial interaction 
because it lends itself to magical interfaces and can potentially give the user more control in the virtual 
world. In this section, we will review several common techniques used to perform these basic tasks. Note 
that it is beyond the scope of this lecture to go into great detail on these techniques or on the concepts 
of 3D UIs in general. The reader shouldexamine 3D User Interfaces: Theory and Practice fora rigorous 
treatment of the subject [Bowman et al. 2004]. Also note for this section, we assume (for the techniques 
where it makes a difference) that y is the vertical axis in the world coordinate system. 2.1 Navigation 
The motor component of navigation is known as travel (e.g., viewpoint movement). There are several issues 
to consider when dealing with travel in 3D UIs. One such issue is the control of velocity and/or acceleration. 
There are many methods for doing this, including gesture, speech controls, sliders, etc. Another issue 
is that of world rotation. In systems that are only partially spatially surrounding (e.g. a 4walledCAVE,orasinglescreen),theusermustbeableto 
rotatetheworldorhisviewoftheworldinorder to navigate. In fully surrounding systems (e.g. with an HMD 
or 6-sided CAVE) this is not necessary since the visuals completely surround the user. Next, one must 
consider whether motion should be constrained inanyway,forexamplebymaintainingaconstant heightorbyfollowingthe 
terrain. Finally,atthelowestlevel, the conditions of input must be considered -that is, when and how 
does motion begin and end (click to start/stop, press to start, release to stop, stop automatically at 
target location, etc.)? Four of the more common 3D travel techniques aregaze-directed steering, pointing, 
map-based travel, and grabbing the air . 2.1.1 Gaze-Directed Steering Gaze-directed steering is probably 
the most common 3D travel technique, although the term gaze is really misleading. Usually noeye trackingis 
being performed, so the directionofgazeis inferred from theheadtracker orientation.Thisisasimpletechnique,bothtoimplementandtouse,butitissomewhat 
limitedinthatyou cannotlook aroundwhilemoving[Mine1995]. Potentialexamplesofgaze-directed steeringin 
videogameswouldbe controllingvehiclesortraveling aroundtheworldin real-time strategy games. Toimplementgaze-directed 
steering, typicallyacallback functionis setup thatexecutes before each frame is rendered. Within this 
callback, .rst obtain the head tracker information (usually in the form of a 4x4 matrix). This matrix 
gives you a transformation between the base tracker coordinate system and the head tracker coordinate 
system. By also considering the transformation between the world coordinate system and the base tracker 
coordinates (if any), you can get the total composite transformation. Now, consider the vector (0, 0, 
-1) in head tracker space (the negative z-axis, which usually points out the front of the tracker). This 
vector, expressed in world coordinates, is the direction you want to move. Normalize this vector, multiply 
it by the speed, and then translate the viewpoint by this amount in world coordinates. Note: current 
velocity isin units/frame. Ifyouwant truevelocity (units/second),you mustkeep track of the time between 
frames and then translate the viewpoint by an amount proportional to that time. 2.1.2 Pointing Pointing 
is also a steering technique (where the user continuously speci.es the direction of motion). In this 
case, the hand s orientation is used to determine direction. This technique is somewhat harder to learn 
for some users,but is more .exible thangaze-directed steering [Bowman et al. 1997]. Pointing is implementedinexactlythe 
samewayasgaze-directed steering,excepta hand trackeris used insteadof the head tracker. Pointing could 
be used to decouple line of sight and direction of motion in .rst and third person shootergames. 2.1.3 
Map-basedTravel Figure 2: Dragging a user icon to move to a new location in the world. The map-based 
travel technique is a target-based technique. The user is represented as an icon on a 2D map of the environment. 
To travel, the user drags this icon to a new position on the map (see Figure 2). When the icon is dropped, 
the system smoothly animates the user from the current location to the new location indicatedbytheicon[Bowmanetal.1998]. 
Map-basedtravelcouldbeusedto augmentmanyof the2Dgame maps currently foundin manygame genres. To implement 
this technique, two things must be known about the way the map relates to the world. First, we need to 
know the scalefactor, the ratio between the map and the virtualworld. Second, we need to know which point 
on the map represents the origin of the world coordinate system. We assume here that the map model is 
originally aligned with the world (i.e. the x direction on the map, in its local coordinate system, representsthex 
directionintheworld coordinate system). Whenthe user pressesthebuttonand is intersecting the user icon 
on the map, then the icon needs to be moved with the stylus each frame. One cannot simply attach the 
icon to the stylus, because we want the icon to remain on the map even if the stylus does not.Todo this, 
we .rst .nd the positionof the stylusin the map coordinate system. This may require a transformation 
between coordinate systems, since the stylus is not a child of the map. The x and z coordinatesofthestyluspositionarethepointtowhichtheiconshouldbemoved.Wedonotcoverhere 
what happens if the stylus is dragged offthe map,but the user icon should stick to the side of the map 
untilthe stylusismoved back insidethemap boundaries, sincewe don twantthe usertomove outsidethe world. 
Whenthebuttonis released,weneedto calculatethe desired positionoftheviewpointintheworld. This position 
is calculated using a transformation from the map coordinate system to the world coordinate system, which 
is detailed here. First, .nd the offset in the map coordinate system from the point correspondingtotheworld 
origin. Then,dividebythemap scale(ifthemapis1/100thesizeoftheworld,this corresponds to multiplying by 
100). This gives us the x and z coordinates of the desired viewpoint position. Sincethemapis2D,wecan 
tgetaycoordinatefromit. Therefore,the technique shouldhave some way of calculating the desired height 
at the new viewpoint. In the simplest case, this might be constant. In othercases,itmightbebasedontheterrainheightatthat 
locationorsomeotherfactors. Once we know the desired viewpoint, we have to set up the animation of the 
viewpoint. The move vector m.represents the amount of translation to do each frame (we are assuming a 
linear path). To .nd . m, we subtract the desired position from the current position (the total movement 
required), divide this by the distance between the two points (calculated using the distance formula), 
and multiplied by the desired velocity, so that . m gives us the amount to move in each dimension each 
frame. The only remaining calculation is the number of frames this movement will take: distance/velocity 
frames. Note that again velocity is measured here in units/frame, not units/second, for simplicity. 
2.1.4 Grabbing the Air The grabbingtheair technique usesthe metaphorof literally grabbingtheworld aroundyou 
(usuallyempty space), and pulling yourself through it using hand gestures [Mapes and Moshell 1995]. This 
is similar to pulling yourself along a rope, except that the rope exists everywhere, and can take you 
in anydirection. The grabbing the air technique has manypotential uses in videogames including climbingbuildings 
or mountains, swimming, and .ying. Toimplementthe one-handedversionofthistechnique(thetwo-handedversioncangetcomplexif 
rotation andworldscalingisalso supported),whentheinitialbuttonpressis detected,wesimplyobtaintheposition 
ofthehandintheworld coordinate system.Then,everyframeuntilthebuttonis released,getanewhand position, 
subtract it from the old one, and move the objects in the world by this amount. Alternately, you can 
leave the world .xed, and translate the viewpoint by the opposite vector. Before exiting the callback, 
be sure to update the old hand position for use on the next frame. Note it is tempting to implement this 
technique simply by attaching the world to the hand, but this will have the undesirable effect of also 
rotating the world when the hand rotates, which can be quite disorienting. You can also do simple constrained 
motion simply by ignoring one or more of the components of the hand position (e.g. only consider x and 
z to move at a constant height).  2.2 Selection 3D selection is the process of accessing one or more 
objects in a 3D virtual world. Note that selection and manipulation are intimately related, and that 
several of the techniques described here can also be used for manipulation. There are several common 
issues for the implementation of selection techniques. One of the most basic is how to indicate that 
the selection event should take place (e.g. you are touching the desired object,nowyouwanttopickitup). 
Thisis usuallydoneviaabutton press, gesture, orvoice command,but it might also be done automatically 
if the system can infer the users intent. One also has to have ef.cient algorithms for object intersections 
for manyof these techniques. Well discuss a couple of possibilities. The feedback given to the user regarding 
which object is about to be selected is also very important. Manyof the techniques require an avatar 
(virtual representation) for the users hand. Finally, considerkeepingalistofobjectsthatare selectable, 
sothata selection techniquedoesnothavetotest every object in the world, increasing ef.ciency. Four common 
selection techniques include the virtual hand, ray-casting, occlusion, and arm extension. 2.2.1 The Virtual 
Hand The most common selection technique is the simple virtual hand, which does real-world selection 
via direct touchingof virtual objects.Inthe absenceofhaptic feedback,thisisdoneby intersectingthe virtual 
hand (which is at the same location as the physical hand) with a virtual object. The virtual hand has 
great potential in manydifferent videogame genres. Examples include selection of sports equipment, direct 
selectionofguns,ammo,andhealthpacksin.rstpersonshootergames,asahandof God in real-time strategygames, 
and interfacing with puzzlesin action/adventuregames. Implementing this technique is simple, provided 
you have a good intersection/collision algorithm. Often, intersections are only performed with axis-aligned 
bounding boxes or bounding spheres rather than with the actual geometry of the objects. 2.2.2 Ray-Casting 
Another common selection technique is ray-casting. This technique uses the metaphor of a laser pointer 
an in.nite ray extending from the virtual hand [Mine 1995]. The .rst object intersected along the ray 
is eligible for selection. This technique is ef.cient, based on experimental results, and only requires 
the user tovary2degreesof freedom (pitchandyawofthe wrist) rather thanthe3DOFs requiredbythe simple virtual 
hand and other location-based techniques. Ideally, ray-casting could be used whenever special powersarerequiredinthegameorwhentheplayerhastheabilitytoselectobjectsata 
distance. There are many ways to implement ray-casting. Abrute-force approach would calculate the parametric 
equation of the ray, based on the hands position and orientation. First, as in the pointing technique 
for travel, .nd the world coordinate system equivalent of the vector (0, 0, -1). This is the direction 
of the ray. If the hands position is represented by (xh,yh,zh), and the direction vector is (xd,yd,zd), 
then the parametric equations are given by x(t)= xh + xdt (1) y(t)= yh + ydt (2) z(t)= zh + zdt. (3) 
Only intersections with t> 0 should be considered, since we do not want to count intersections behind 
the hand. It is important to determine whether the actual geometry has been intersected, so .rst testing 
the intersection with the bounding box will result in manycases being trivially rejected. Another method 
might be more ef.cient. In this method, instead of looking at the hand orientation in the world coordinate 
system, we consider the selectable objects to be in the hands coordinate system, by transforming their 
vertices or their bounding boxes. This might seem quite inef.cient, because there is only one hand, while 
there are many polygons in the world. However, we assume we have limited the objects by using a selectable 
objects list. Thus, the intersection test we will describe is much more ef.cient. Once we have transformed 
the vertices or bounding boxes, we drop the z coordinate of each vertex. This maps the 3D polygon onto 
a 2D plane (the xy plane in the hand coordinate system). Since the ray is (0, 0, -1) in this coordinate 
system, we can see that in this 2D plane, the ray will intersect the polygon if and only if the point 
(0, 0) is in the polygon. We can easily determine this with an algorithm that counts the number of times 
the edges of the 2D polygon cross the positive x-axis. If there are an odd number of crossings, the origin 
is inside, if even, the origin is outside. 2.2.3 OcclusionTechniques Occlusion techniques (also called 
image plane techniques) work in the plane of the image; object are selected by covering it with the virtual 
hand so that it is occluded from your point of view [Pierce et al. 1997]. Geometrically, this means that 
a ray is emanating from your eye, going through your .nger, and then intersecting an object. Occlusion 
techniques could be used for object selection at a distance but instead of using a laser pointer metaphor 
that ray casting affords, players could simply touch distant objects to select them. These techniques 
can be implemented in the same ways as the ray-casting technique, since it is also using a ray. If you 
are doing the brute-force ray intersection algorithm, you can simply de.ne the rays direction by subtracting 
the .nger position from the eye position. However, if you are using the 2nd algorithm, you require an 
object to de.ne the rays coordinate system. This can be done in two steps. First, create an empty object, 
and place it at the hand position, aligned with the world coordinate system. Next, determine how to rotate 
this object/coordinate system so that it is aligned with the ray direction. The angle can be determined 
using the positions of the eye and hand, and some simple trigonometry. In 3D, two rotations must be done 
in general to align the new objects coordinate system with the ray. 2.2.4 Arm-Extension The arm-extension 
(e.g., Go-Go) techniqueis based on the simple virtual hand,butit introducesa nonlinear mapping between 
the physical hand and the virtual hand, so that the user s reach is greatly extended [Poupyrev et al. 
1996]. Not only useful for object selection at a distance, Go-Go could also be useful traveling through 
an environment with Batman s grappling hook or Spiderman s web. The graph in Figure3shows the mapping 
between thephysical hand distance from the body on the x-axis and the virtual hand distance from the 
body on the y-axis. There are two regions. When the physical hand is at a depthlessthanathresholdD,the 
one-to-one mapping applies. OutsideD,anon-linear mappingis applied, so that thefarther the user stretches, 
thefaster the virtual hand movesaway. To implement Go-Go, we .rst need the concept of the position of 
the users body. This is needed because we stretch our hands out from the center of our body, not from 
our head (which is usually the position that is tracked). We can implement this using an inferred torso 
position, which is de.ned as a constant offset in the negativeydirection from the head.Atracker could 
alsobe placed on the users torso. Before rendering each frame, we get the physical hand position in the 
world coordinate system, and then calculate its distance from the torso object using the distance formula. 
The virtual hand distance can Figure 3: The nonlinear mapping function used in the Go-Go selection technique. 
then be obtained by applying the function shown in the graph in Figure 3. d2.3 (starting at D) is a useful 
function in many environments, but the exponent used depends on the size of the environment and the desired 
accuracyof selection at a distance. Once the distance at which to place the virtual hand is known, we 
need to determine its position. The most common implementation is tokeep the virtual hand on the ray 
extending from the torso and going through the physical hand. Therefore, if we get a vector between thesetwo 
points, normalizeit, multiplyitbythe distance, thenadd thisvectortothe torso point,we obtain the position 
of the virtual hand. Finally, we can use the virtual hand technique for object selection.  2.3 Manipulation 
As we noted earlier, manipulation is connected with selection, because an object must be selected before 
it can be manipulated. Thus, one important issue for anymanipulation technique is how well it integrates 
with the chosen selection technique. Manytechniques, as we have said, do both: e.g. simple virtual hand, 
ray-casting, and go-go. Another issue is that when an object is being manipulated, you should take care 
to disable the selection technique and the feedback you give the user for selection. If this is not done, 
then serious problems can occur if, forexample, the user tries to release the currently selected objectbut 
the system also interprets this as trying to select a new object. Finally, thinking about what happens 
when the object is released is important. Does it remain at its last position, possibly .oating in space? 
Does it snaptoagrid?Doesitfallviagravityuntilit contactssomethingsolid?The application requirementswill 
determine this choice. Three common manipulation techniques include HOMER, Scaled-World Grab, and World-in-Miniature. 
2.3.1 HOMER The Hand-Centered Object Manipulation Extending Ray-Casting (HOMER) technique uses ray-casting 
for selection and then moves the virtual hand to the object for hand-centered manipulation [Bowman and 
Hodges 1997]. The depth of the object is based on a linear mapping. The initial torso-physical hand distanceis 
mapped onto the initial torso-object distance, so that moving thephysical hand twice asfar away also 
moves the object twice asfaraway. Also, moving thephysical hand all theway back to the torso moves the 
object all the way to the users torso as well. Like Go-Go, HOMER requires a torso position, because you 
want to keep the virtual hand on the ray betweenthe usersbody(torso)andthephysicalhand.TheproblemhereisthatHOMERmovesthe 
virtual hand from the physical hand position to the object upon selection, and it is not guaranteed that 
the torso, physicalhand,andobjectwillalllineupatthistime. Therefore,we calculatewherethe virtualhandwould 
be if it were on this ray initially, then calculate the offset to the position of the virtual object, 
and maintain this offset throughout manipulation. When an object is selected via ray-casting, .rst detach 
the virtual hand from the hand tracker. This is due tothefact thatifit remained attachedbutthe virtual 
hand modelismovedaway fromthephysical hand location, a rotation of the physical hand will cause a rotation 
and translation of the virtual hand. Next, move the virtual hand in the world coordinate system to the 
position of the selected object, and attach the object to the virtual hand in the scene graph (again, 
without moving the object in the world coordinate system). To implement the linear depth mapping, we 
need to know the initial distance between the torso and the physical hand dh, and between the torso and 
the selected object do. The ratio do/dh will be the scaling factor. For each frame, we need to set the 
position and orientation of the virtual hand. The selected object is attached to the virtual hand, so 
it will follow along. Setting the orientation is relatively easy. Simply copy the transformation matrix 
for the hand tracker to the virtual hand, so that their orientation matches.To set the position, we need 
to know the correct depth and the correct direction. The depth is found by applying the linear mapping 
to the current physical hand depth. The physical hand distance is simply the distance betweenit and the 
torso, and we multiply thisby the scalefactor do/dh to get the virtual hand distance. We then obtain 
a normalized vector between the physical hand and the torso, multiply this vector by the virtual hand 
distance, and add the result to the torso position to obtain the virtual hand position. 2.3.2 Scaled-World 
Grab The scaled-world grab technique (see Figure 4) is often used with occlusion selection. The idea 
is that since you are selecting the object in the image plane, you can use the ambiguity of that single 
image to do some magic. When the selection is made, the user is scaled up (or the world is scaled down) 
so that the virtual hand is actually touching the object that it is occluding. If the user does not move 
(and the graphics are not stereo), there is no perceptual difference between the images before and after 
the scaling [Mine et al. 1997]. However, when the user starts to move the object and/or his head, he 
realizes that he is now a giant (or that the world is tiny) and he can manipulate the object directly, 
just like the simple virtual hand. Figure 4: An illustration of the scaled-world grab technique. To implement 
scaled-world grab, correct actions must be performed at the time of selection and release. Nothing special 
needs to be done in between, because the object is simply attached to the virtual hand, as in the simple 
virtual hand technique. At the time of selection, scale the user by the ratio (distance from eye to object/distance 
from eye to hand). This scaling needs to take place with the eye as the .xed point, Figure 5: An example 
of a WIM. The world-in-miniature (WIM) technique uses a small dollhouse version of the world to allow 
the user to do indirect manipulation of the objects in the environment (see Figure 5). Each of the objects 
in the WIM are selectable using the simple virtual hand technique, and moving these objects causes the 
full-scale objects in the world to move in a corresponding way [Stoakleyet al. 1995a]. The WIM can also 
be used fornavigationbyincludingarepresentationofthe user,inaway similartothe map-basedtravel technique, 
but including the 3rd dimension [Pausch et al. 1995a]. To implement the WIM technique, .rst create the 
WIM. Consider this a room with a table object in it. The WIM is represented as a scaled down version 
of the room, and is attached to the virtual hand. The table object does not need to be scaled, because 
it will inherit the scaling from its parent (the WIM room). Thus, the table object can simply be copied 
within the scene graph. When an object in the WIM is selected using the simple virtual hand technique, 
.rst match this object to the corresponding full-scale object. Keeping a list of pointers to these objects 
is an ef.cient way to do this step. The miniature object is attached to the virtual hand, just as in 
the simple virtual hand technique. While the miniature object is being manipulated, simply copyits position 
matrix (in its local coordinate system, relative to its parent, the WIM) to the position matrix of the 
full-scale object. Since we want the full-scale objecttohavethe same positioninthe full-scaleworld coordinate 
systemasthe miniature object does in the scaled-down WIM coordinate system, this is all that is necessary 
to move the full-scale object correctly.  2.4 System Control System control provides a mechanism for 
users to issue a command to either change the mode of interaction or the system state. In order to issue 
the command, the user has to select an item from a set. System control is a wide-ranging topic, and there 
are manydifferent techniques to choose from such as the use of graphical menus,voice commands, gestures, 
and tool selectors.For the most part, these techniques are notdif.cultto implement,sincetheymostlyinvolve 
selection.Forexample, virtualmenuitemsmightbe selectedusing ray-casting.Forallofthe techniques,good visual 
feedbackis required, sincethe user needs toknownotonlywhatheis selecting,butwhatwillhappenwhenhe selectsit.Inthis 
section,webrie.y highlight some of the more common system control techniques. 2.4.1 Graphical Menus Figure 
6: TheVirtualTricorder: anexampleofagraphical menu withdevice-centered placement. Graphical menus can 
be seen as the 3D equivalent of 2D menus. Placement in.uences the access of the menu (correct placement 
cangiveastrong spatial referencefor retrieval),andtheeffectsof possible occlusionofthe .eldof attention. 
The paperby Feineretal.isan important sourcefor placement issues [Feiner et al. 1993]. The authors divided 
placement into surround-.xed, world-.xed and display-.xed windows. The subdivision of placement can, 
however, be made more subtle. World-.xed and surround-.xed windows, the term Feiner et al. use to describe 
menus, can be subdivided into menus which are either freely placed into the world, or connected to an 
object. Display-.xed windows can be renamed, and made more precise, by referring to their actual reference 
frame: the body. Body-centered menus, either head referenced or body-referenced, can supply a strong 
spatial reference frame. One particularly interesting possibleeffectof body-centered menusiseyes-offusage,in 
which users can perform system control without having to look at the menu itself. The last reference 
frame is the group of device-centered menus. Device-centered placement provides the user with a physical 
reference frame (see Figure 6) [Wloka and Green.eld 1995]. Agood example is the placement of menus on 
a responsive workbench, where menus are often placed at the border of the display device. We can subdivide 
graphical menus into hand-oriented menus, converted 2D menus, and 3D widgets. One can identify two major 
groups of hand-oriented menus. 1DOF menus are menus which use a circular object on which several items 
are placed. After initialization, the user can rotate his/her hand along one axis until the desired item 
on the circular object falls within a selection basket. User performance is highly dependent on hand 
and wrist physical movement and the primary rotation axis should be carefully chosen. 1DOF menus have 
been made in several forms, including the ring menu, sundials, spiral menus (a spiral formed ring menu), 
and a rotary tool chooser. The second group of hand-oriented menus are hand-held-widgets, in which menus 
are stored at a body-relative position. The second group is the most often applied group of system control 
interfaces: converted 2D widgets. These widgets basically function the same as in desktop environments, 
although one often has to deal with more DOFs when selecting an item in a 2D widget. Popular examples 
are pull-down menus, pop-up menus, .ying widgets, toolbars and sliders. Figure 7: Anexampleofa3Dwidgetusedto 
scaleageometric object. The .nal groupof graphical menusisthe groupknownas3D widgets [Conneretal. 1992].Ina3Dworld, 
widgetsoftenmeanmovingsystemcontrol functionalityintotheworldorontoobjects(seeFigure7).This matchescloselywiththe 
de.nitionofwidgetsgivenby Conneretal., widgetsarethe combinationofgeometryandbehavior .Thiscanalsobethoughtofas 
movingthe functionalityofamenuontoanobject. Avery important issue when using widgets is placement. 3D 
widgets differ from the previously discussed menu techniques (1DOF and converted 2D menus) in the way 
the available functions are mapped: most often, the functions are co-located near an object, thereby 
forming a highly context-sensitive menu. 2.4.2 Voice Commands Voiceinputallowsthe initialization, selectionandissuingofacommand. 
Sometimes, anotherinput stream (likeabuttonpress)ora speci.cvoice commandisusedtoallowthe actualactivationofvoiceinputfor 
system control.Theuseofvoiceinputasasystem control techniquecanbeverypowerful:itis hands-free and natural. 
Still, continuousvoiceinputistiring,andcannotbeusedineveryenvironment. Furthermore, the voice recognition 
engine often has a limited vocabulary. In addition, the user .rst needs to learn the voice commands before 
they can be applied. Problems often occur when applications are more complex, and the complete set of 
voice commands can not be remembered. The structural organization of voice commands is invisible to the 
user: often no visual representation is coupled to the voice command in order to see the available commands. 
In order to prevent mode errors, it is often very important to supply the user with some kind of feedback 
after she has issued a command. This can be achieved by voice output, or by the generation of certain 
sounds. A Figure 8: Auser interacting with a dataset for visualizing a .ow .eld around a space shuttle. 
The user simultaneously manipulates the streamlines with his left hand and the shuttle with his right 
hand while viewing the data in stereo. The user asked for these tools using speechinput. When using gestural 
interaction, we apply a hand-as-tool metaphor: the hand literally becomes a tool. When applying gestural 
interaction, the gesture is both the initialization and the issuing of a command, just as in voice input. 
When talking about gestural interaction, we refer, in this case, to gestures and postures, not to gestural 
input with pen-and-tablet or similar metaphors. There is a signi.cant difference between gestures and 
postures: postures are static movements (like pinching), whereas gestures include a change of position 
and/or orientation of the hand. A good example of gestures is the usage of sign language. Gestural interaction 
can be a very powerful system control technique. In fact, gestures are relatively limitless when it comes 
to their potential uses in videogames. Gestures can be use to communicate with otherplayers,tocastspellsinaroleplayinggame,callpitchesorgivesignsina 
baseballgame,andissues combination attacksinactiongames.However,oneproblemwithgestural interactionisthattheuserneeds 
to learn all the gestures. Since the user can normally not remember more than about7gestures (due to 
the limited capacityoftheworking memory),inexperienced users canhave signi.cant problems with gestural 
interaction, especially when the application is more complex and requires a larger amount of gestures. 
Users often do not have the luxury of referring to a graphical menu when using gestural interaction -the 
structure underneath the available gestures is completely invisible. In order to make gestural interaction 
easiertouseforalessadvanceduser,strongfeedback,likevisualcuesafter initiationofacommand,might be needed. 
An example of application that used a gestural system control technique is MSVT [LaViola 2000]. This 
application combined gesturesandvoiceinputto createamultimodal interfaceforexploratory scienti.c visualization 
(see Figure 8). Figure 9: An example of a virtual toolbelt. The user looks down to invoke the belt and 
can grab tools and use them in the virtual environment. Virtual tools are tools which can be best exempli.ed 
with a toolbelt (see Figure 9). Users wear a virtual toolbeltaroundthewaist,fromwhichtheusercan accessspeci.c 
functionsbygrabbingat particularplaces on belt, as in the real world. Sometimes, functions on a toolbelt 
are accessed via the same principles as used with graphical menus, where one should look at the menu 
itself. The structure of tools is often not complex: as stated before, physical tools are either dedicated 
devices for one function, or one can access several (but not many) functions with one tool. Sometimes, 
a physical tool is the display medium for a graphical menu. In this case, it has to be developed in the 
same way as graphical menus. Virtual tools often use proprioceptive cues for structuring. It is still 
unclear how to best design tools for system control. Still, some general design issues can be stated.Inthecaseofphysicaltools,theformofthetooloftenstrongly 
communicatesthe functiononecan perform with the device, so take care with the form when developing new 
props. The form of a tool can highly in.uencethe directnessandfamiliaritywiththedeviceas well.With respectto 
virtual tools which can not be used without looking at them, their representation can be very similar 
to graphical menus. One example of a commonly used physically-based tool is the pen and tablet. Users 
hold a large plastic tableton whicha (traditional)2D interfaceis displayedinthe virtualworld(see Figure10). 
Users are able to use graphical menu techniques and can move objects with a stylus within a window on 
the tablet. The tablet supplies the user with strong physical cues with respect to the placement of the 
menu, and allows increased performanceduetofaster selectionof menu items[Bowmanetal. 1998]. For the implementation 
of this technique, the most crucial thing is the registration (correspondence) between the physical 
and virtual pens and tablets. The tablets, especially, must be the same size and shape sothattheedgeofthephysicaltablet,whichtheusercanfeel, 
correspondstotheedgeofthevirtualtablet as well. In order to make tracking easy, the origin of the tablet 
model should be located at the point where thetrackeris attachedtothephysicaltablet,sothat rotationsworkproperly.Evenwithcare,itsdif.cultto 
do these things exactly right, so a .nal tip is to include controls within the program to tweak the positions 
and/or orientations of the virtual pen and tablet, so that they can be into registration if there s a 
problem. Another useful function to implement is the ability for the system to report (for example when 
a callback function is called) the position of the stylus tip in the tablet coordinate system, rather 
than in the world or user coordinate systems. This can be used for things likethe map-based travel technique 
described earlier.   3 Applications 1: Spatial Interfaces in Artand Design In this course, we aim to 
give a broad overview of current and potential applications of spatial interfaces. Applications to art 
and design can teach us a great deal about how spatial interfaces can be employed creatively.Thischapterbeginswithanoverviewofearlyworkinthe 
computergraphicsanduserinterface research communities. This research paved the way for a variety of freeform 
3D modeling techniques, many of which utilize spatial interfaces to create art and design results that 
are dramatically different from what we have come to expect from more traditional desktop-based design 
tools (e.g., CAD tools). After this research context, a variety of speci.c contemporary applications 
are discussed. Much of the discussion in this chapter is adapted from Keefe s Ph.D. dissertation [Keefe 
2007], a good source for additional information on the topic of spatial interfaces in art and design. 
3.1 Backgroundin 3DUI sforArtand Design This section provides a computer graphics and user interface 
research context for applications of 3D user interfaces to art and design. The discussion places a special 
focus on the development of freeform 3D modeling tools, as these interfaces (typically based on sweeping 
movement of tracked hands or props in 3D space) have played an important role in innovative art and design 
applications. 3.1.1 EarlyResearchin3D User InterfacesforArt and Design Sutherland introduced the .rst 
head-mounted, tracked, stereo display in 1968 [Sutherland 1968]. Eight years later, Clark combined this 
hardware with a tracked-wand device to make the .rst virtual reality system for direct manipulation of 
3D surfaces [Clark 1976]. Clark s work introduced the idea that the intuitive control afforded by direct 
3D interaction coupled with a 3D display could be a better paradigm for 3D modeling tasks than more indirect 
methods involving 2D projections of 3D objects or mapping 2D input to a 3D space. He also discussed the 
implications of this approach for the design process. Freeing designers from needing to know particular 
numerical coordinate values describing a surface or orientations of coordinate system axes allows them 
to concentrate fully on the design task. These concepts have inspired manyresearchers, artists, and designers 
who have followed. Schmandt [Schmandt 1983] was the .rst we know to implement a modeling system that 
could generate form from sweeping movements of a 3D input device. Schmandt s system used an early Polhemus 
6D device to create a magic wand that could emit 3D paint in space. He used half-silvered mirrors, conventional 
video monitors, and shutter glasses to produce a stereo view. Schmandt s work was one of the .rst experiments 
on the interactive capabilities of stereo displays. His results indicated a good natural correspondence 
betweenthewandandthe3D paint,butlagand distortioninthe tracking .eld detracted signi.cantly from the 
interactive feel of the tool. Galyean and Hughes [Galyean and Hughes 1991] used direct 3D input with 
a monoscopic display to produce a voxel-based modeling technique. They created a poor man s 3D force-feedback 
device to assist in controlling the input tool by suspending a Polhemus tracker in a cube with eight 
elastic cords attachedtothecube corners.Thisworkwasthe.rstofthe3Dsculptingsystemstouseaclaymetaphorand 
oneofthe.rsttoexaminecreating artistic,blobbymodelsthatwereadrasticdeparturefromthemorerigid geometricshapesproducedwithCAD 
programs.Inthissystem, materialcouldbecutawayfromoradded to a block of clay. Alarge body of work based 
on this metaphor has followed, and virtual clay remains one of the most successful approaches to obtaining 
output in the form of voxel-based descriptions or enclosed triangle meshes, which are useful for applications 
in rapid prototyping and animation. Advances in computer and haptic hardware continue to make possible 
a wide variety of extensions to this work that provide more control of the interface and more sophisticated 
forms. At roughly the same time as the work by Galyean and Hughes, Sachs et al. presented the 3-Draw 
system [Sachs et al. 1991], which used direct 3D input with both hands to create an intuitive CAD modeling 
system. This work was ground breaking in establishing computers as a viable tool for the initial phases 
of industrial design. 3-Draw models consistedoflinesonly,butthe lines can easilybe thoughtofas de.ning 
surfaces. The tool allowed both unconstrained and various types of semi-constrained sweeping input to 
specify complex 3D curves and provided interaction techniques for specifying start and end points for 
curves and several curve-editing operations. This work was important in establishing free-form 3D input 
asa viabletoolfor seriousdesign problemsandin presenting interaction techniquesfor enhancing control 
over free-form input. The3DMsystem [Butterworthetal.1992] presentedby Butterworthetal.builton Clark s 
conceptionofa head-mounted display modeling environment with the additional goal of providing the user 
with an interfacethatwasaseasytolearnanduseasthatcommonlyavailablein2Ddrawing programsofthetime,such 
as MacPaint. In addition to performing geometrically constrained CAD-style modeling operations, 3DM incorporateda 
modelingmodebasedon sweeping3Dinputfor creatingsurfacesbyextruding curves.One of the important contributions 
of 3DM with respect to future art and design applications is its discussion of user experiences with 
this extrusion tool. Users were reported to have dif.culty aligning 3D objects, keeping two triangles 
parallel, and doing other geometrically constrained operations in this VR environment,but for the .rst 
time theycould easily perform highly complex free-formextrusions. Extrusions had already been proven 
to be extremely useful modeling tools in desktop-based programs. The difference in 3DM was the user s 
ability to perform the extrusion directly in 3D by dragging and twisting an extrusion curve along a free-form 
path. In this way, users could naturally control more than one parameter (position and twist) of the 
extrusion while creating it. Deering s Holosketch [Deering 1995] was the .rst system to combine a head-tracked 
stereo VR environment with a modeling system geared towards artistic creation. Holosketch was a strikingly 
complete modelingand animation packagewithafullydeveloped menuof modeling modesand operations.Several 
of Holosketch s drawing modes were based on continuous sweeping input, including a toothpaste mode reminiscent 
of Gaylean and Hughes additive sculpting [Galyean and Hughes 1991], a wire-frame-lines mode reminiscent 
of the 3D line drawings of 3-Draw [Sachs et al. 1991], and a mode in which clouds of random small triangle 
particles were left behind the wand as it was swept through the air. Holosketch worked in a .shtank VR 
[Deering 1992] setup using a 20-inch CRTand was also explored in alternative VR formfactors. 3.1.2 Recent 
Researchin3DUser InterfacesforArtandDesign In more recent work, two classes of free-form modeling techniques 
that utilize direct, sweeping 3D input have emerged: those based on large-scale movements of a tracked 
device in the air and those based on haptic feedback, usually utilizing the clay sculpting metaphor .rst 
introduced by Gaylean and Hughes [Galyean and Hughes 1991]. Schkolne et al. s Surface Drawing [Schkolne 
et al. 2001] is an example of free-form modeling utilizing a large-screen table display device. This 
display lends itself to the use of physical props that can be placed on the table when not in use. Natural 
use and selection of appropriate props and metaphors is the main thrust of Schkolne s work [Schkolne 
2003; Schkolne et al. 2004]. To create form in the system, the artist uses his hand augmented with a 
bend sensing glove as a device for sweeping out bits of surface in space. These surface fragments are 
then stitched together to create a bigger triangle-mesh model. Several other large-scale, open-air input 
systems have been created for free-form modeling. Keefe et al. s CavePainting system, described in more 
detail in the following sections, uses similar input metaphors (sweepingmovementsofthehandsinspace)to 
create3Dform[Keefeetal.2001].The FreeDrawer[Wesche and Seidel 2001] system runs on a responsive workbench 
and is a good example of a modern approach to a spline-based modeling system that utilizes sweeping 3D 
input. In the area of haptics-based free-form modeling, SensAbleTechnologies, makers of the Phantom force 
feedbackdevice, introducedwhatwasprobablythe.rsthapticsmodelingsystemthatallowedartiststofeel the 3D 
shapes theymodeled while pushing, pulling, and deforming them. The tool, which has undergone considerable 
re.nements andis stillavailable todayis called FreeForm [SensAbleTechnologies, Inc. 2005]. The current 
incarnation targets product designers working anywhere from the early conceptual stages of product design 
to .nal steps where output from the modeling tool can be sent to rapid prototyping machines for production 
of physical models. The models that skilled artists can create with this system are impressive in their 
precision and re.ned aesthetic. Similar approaches have also been presented within the research community 
(e.g., [Gregory et al. 2000], [Hua and Qin 2004]). One haptics-based modeling system that takes a different 
approach from the rest is the springs-andconstraints 3D drawing system presented by Snibbe et al. [Snibbe 
et al. 1998]. Snibbe et al. s approach is differentinthatitusesdynamichapticmodelstohelp artistsexplorevariousmodesof 
creation,butthese models are not based on interacting with a static geometry or properly simulating contact 
forces. Rather, theyhelpto controlandguide free-forminput,buttendtoallow artiststoremaingesturalintheir 
interactions. The artistic potential of the new medium described by Snibbe et al. is left relatively 
unexplored. Of .nal note is Schroering et al. s work [Schroering et al. 2003] in which a light pen is 
used to draw on a tablet that is tracked as it is moved through the air. The drawing/modeling application 
presented in this work is quite limited in its scope, but this input style could have important implications 
for free-form modeling in 3D.  3.2 Example3D User InterfacesforArt Applications Figure 11 shows a number 
of 3D models created with the CavePainting system mentioned earlier [Keefe et al. 2001], and Figure 12 
shows the haptic Drawing on Air interface that evolved from the original CavePainting tool. Both of these 
systems were designed to explore the potential of 3D user interfaces in art applications.Keefe describeshisart 
practicewiththetwo systemsoveraperiodofnearlya decadein an article to appear shortly in the journal, 
Leonardo [Keefe 2011, In press]. One of the most interesting points of feedback from the manyartists 
who have used the systems and critiqued work generated with the tools is that the style of form generated 
using these 3D user interfaces is quite different from what is available via other tools. For example, 
in the works shown in Figure 11, each 3D model includes some visual evidence that a human hand was involved 
in creating the model this is rarely seen with models created using traditional desktop modeling tools. 
Similarly, there is a painting-like aesthetic in the models produced with these tools that is quite distinct 
from the watertight meshes produced with more traditional modeling systems. While these unique aesthetic 
qualities are not desirable for all applications, they are particularly exciting for artists interested 
in exploring the connection between human movement and computer generated 3D form. Several other artists 
have worked with 3D user interfaces, especially in the area of 3D modeling and sketching.OfnoteistheworkoftheartistMa,whohasteamedwith 
researchersatHelsinkiUniversity akelofTechnologyto createa systemin which form canbe generatedby tracking 
.ngertips[Makela 2005]. The .ngertip control, achieved through a custom ultrasonic input device, adds 
the ability to control the thickness of swaths of form as they are swept out in space, a feature similar 
to that provided by the haptic Drawing on Air interface[Keefeet al. 2007]. Makela swork also illustrates 
compelling visual effects that are possible when combining point-and surface-based representations. Artist 
Jen Zen has worked extensively with 3D user interfaces as a mode of tracing form in space [Grey2002]. 
The BLUI system[Brodyand Hartman2000]hasbeenthetopicofseveralsketchesat SIGGRAPHinwhichphysical printouts, 
both 2D and 3D, of free-form objects created with the system have been presented. The IMAX SANDDE digital 
system, used in the .lms June [Ferguson 2003], Falling in Love Again [Ferguson 2006], and Moon Man [Morstad 
2004], is also based on freehand 3D drawing with a tracked wand in a stereoscopic environment. 3.3 Installation 
and Interactive Art 3D user interfaces have also been explored in installation and interactive art environments. 
New media writing projects have been developed and shown extensively within CAVE environments [Brown 
University Literary Arts Program 2011]. 3D input has manytimes been coupled with live dance performances, 
and new research continues to advance the possibilities of linking physical and virtual performers [Sheppard 
and Nahrstedt 2009]. Other artists have explored mappings between spatial input provided by audience 
memberswalkingthroughanartworkandtheform displayedinthework. RubinandKeefeexplored this conceptinaCaveof 
Elusive Immateriality, presentedat SIGGRAPH 2002 [RubinandKeefe 2002]. Snibbe extends similar 3D input 
concepts to installations that involve multiple viewers [Snibbe 1998].  3.4 Applications in Creative 
3D Design Many applications of 3D user interfaces to creative design problems are closely related to 
applications in art. Forexample,building directly on the freeform modeling applications described earlier,Keefe 
et al. have applied 3D user interfaces to the problem of developing early stage 3D prototyping tools 
for designing complex scienti.c visualizations [Keefe et al. 2008a]. While current commercial computeraided 
design tools allow designers to create amazingly intricate 3D models, what designers often miss in thesetoolsisanabilitytoworkas 
naturallyandexpressivelyaswithapencilandpaper.3Duserinterfaces can provide new ways to work with 3D design 
problems using tools that are immediate and gestural, similar to traditional sketching. The ModelCraft 
interface developed by Song et al. [Song et al. 2006] is another particularly interesting design-oriented 
3D user interface. The system uses an Anoto pen and paper. The paper can be folded into various 3D shapes, 
and then pen markings made directly on the shapes are used to indicate 3D design actions, such as cutting 
away or extruding material. In still other design-oriented applications of 3D user interfaces, several 
researchers and practitioners have explored the use of 3D input for architectural design. For example, 
Anderson et al. describe a virtual reality design system and the bene.ts it has provided, including a 
new ability to simultaneously work at multiple scales [Anderson et al. 2003]. A.nal example is motivated 
by automotive design and creative uses of spray paint in art. Koniecznyet al., present a virtual spray 
paint simulator [Konieczny et al. 2008]. Manyother applications of 3D user interfaces to creative design 
tasks exist; 3D user interfaces show great potential for creative 3D art and design work, especially 
in the early stages of conceptual design. 4.1 4.1.1 Nintendo Wii Remote Figure 13: TheWiimote, with 
labels indicating theWiimote s coordinate system. Multiple coordinate systems and partial spatial data 
make theWiimote dif.cult to design for. Frames of Reference. Figure13 shows theWiimote s FOR. Thex,yandz 
axes are labeled, along with the rotation about each axis: pitch, roll, and yaw, respectively. A second 
FOR is the Earth s, important becausetheWiimote saccelerometers detectthe Earth sgravity.AthirdFORistheWiimote 
srelationship tothe sensorbar. Threeexamples clarifyhow theseFOR interact.Tobegin,a useris considered 
holdinga Wiimote naturallywhen+zisupinboththeWiimote sandthe Earth sFORandtheWiimote sfrontpoints away 
from the user and toward a sensor bar, which is usually on top of the display. In the .rst example, the 
user moves theWiimote toward the sensor bar. This results in acceleration reported in the y-axis of WhentheWiimoteis 
pointedatthe sensorbar,itpicksuptwo points PL =(xL,yL) and PR =(xR,yR) from the LED arrays. The midpoint 
between these PL and PR can easily be calculated and used as a 2D cursor or pointer on the display. In 
addition, if theWiimote is rotated about theY-axis, we can calculate the roll of the device with respect 
to the X-axis using () .v roll = arccos .x  (4) ..v. where .x = (1, 0) and .v = PL - PR. Combining 
information from the sensor bar and the Wiimote s optical sensor also make it possible to determine howfar 
theWiimote is from the sensor bar using triangulation. The distance d between the sensor bar andWiimoteis 
calculated using w/2 d = (5) tan(./2) m  wimg w = (6) mimg v mimg =(xL - xR)2 +(yL - yR)2 (7) where 
. is the optical sensor s viewing angle, m is the distance between the sensor bar s left and right LEDs, 
wimg is the width of the image taken from the optical sensor, and mimg is the distance between PL and 
PR taken from the optical sensor s image. Note that ., wimg, and m are all constants. This calculation 
only works when the Wiimote device is pointing directly at the sensor bar (orthogonally). When the Wiimote 
is off-axis from the sensor bar, more information is needed to .nd depth. We can utilize the relative 
sizes of the points on the optical sensor s image to calculate depth in this case. To .ndd in this case, 
we compute the distances corresponding to the the left and right points on the optical sensor s image 
wL/2 dL = (8) tan(./2) wR/2 dR = (9) tan(./2) using wimg  diamLED wL = (10) diamL wimg  diamLED wR 
= (11) diamR where diamLED is the diameter of the actual LED marker from the sensor bar and diamL and 
diamR are the diameters of the points found on the optical sensor s image. With dL and dR, we can then 
calculate Wiimote s distance to the sensor bar as v d = dL 2 +(m/2)2 - 2dL(m/2)2 cos(.) (12) where d22 
- d2 cos(.)= LmR . (13) 2mdL Note that with d, dL, andm wecanalso.ndtheangularpositionoftheWiimotewithrespecttothe 
sensor bar, calculated as () d2m2 - d2 a = arccos L . (14) mdL 3-Axis Accelerometer. The secondWiimote 
inputis thedevice s 3-axis accelerometer. The accelerometer reports acceleration data in the device s 
x, y and z directions, expressed conveniently in g s. This is common of manydevices employing 3-axis 
accelerometers such as the cell phones likethe iPhone, laptops and camcorders. Withthis information,theWiimoteisableto 
sense motion, reportingvaluesthatarea blendof accelerationsexertedbythe userandgravity.Asthegravityvectoris 
constantly orientedtowards the Earth (or (0, 0, 1) in Earth sFOR),thegravityvectorcanbeusedtodiscoverpartoftheWiimote 
s orientation in terms of earth s frame of reference using () pitch = arctan az (15) ay () roll = arctan 
az . (16) ax Unfortunately, determiningyawinthe Earth sFORisn tpossible becausethe Earth sgravityvectoraligns 
with its z-axis. Another unfortunate issue is that determining the actual acceleration of theWiimote 
is problematic owing to the confounding gravity vector. To determine the actual acceleration, one of 
the following must take place: theWiimote must be under no acceleration other than gravity so that you 
can accurately measure the gravity vector (in which case, you already know the actual acceleration is 
zero),  you must make assumptions about theWiimote s orientation, thus allowing room for errors, or 
 you must determine the orientation by other means, such as by the SBC or a gyroscope (we discuss this 
in more detail later).  The implicationsfor orientation trackingbythe accelerometers are thattheWiimote 
s orientationisonly certain whenitis under no acceleration. For this reason, manyWiigames require that 
users either hold theWiimotesteadyforashortperiodoftime beforeusingitinagametrialorhaveitpointedatthe 
screen and orient by the SBC. Wii MotionPlus. This attachment uses two gyroscopes to report angular velocity 
along all three axes (one dual-axis gyro forx andyanda single-axis gyro forz). Mechanical gyroscopeswould 
typicallybe too largeandexpensiveforaWiimote.So,itusesMEMS (microelectromechanicalsystem) gyroscopes,which 
operateusingavibrating structure,areinexpensive,uselittlepower,andarefairly accurate.AMotionPlusaugmentedWiimote 
provides information on changes to theWiimote s orientation, alleviating manyof thedevice sdata limitations.Withthis, 
Nintendois attemptingtoimprovethe orientation accuracyofthe device. The MotionPlusisn tyetfullyreverse 
engineered.It reports orientation changesintwo granularities,fast and slow, withfast being roughly four 
times the rate per bit. The gyroscope manufacturer reports that the two gyroscopeshavea lineargainbut 
thatthedifferent gyroscopes reportvaluesintwodifferent scales, so there s no single scalingfactor. Additionally, 
temperature and pressure changes can impact this scale factor and change the value associated with zero 
orientation change. The hardware for the Kinect (see Figure 15) is comprised of a color camera, a depth 
sensor, a multiarray microphone, and a motorized tilt system. The camera is used to determine different 
features of the user and spaceby detectingRGB colorsandis mainly usedforfacial recognitionofthe user. 
The multi-array microphone is a set of four microphones that are able to isolate the voices of multiple 
users from the ambient noises in the room. It makes use of acoustic source localization and ambient noise 
suppression allowing userstobeafew feetaway fromthedevicebut stillbe ableto usethevoice controlsina headsetfree 
manner. The third component of the hardware, the depth sensor (generally referred to as the 3D camera), 
combines an infrared laser projector and a CMOS (complimentary metal-oxide semiconductor) sensor. The 
infrared projector casts out a myriad of infrared dots (see Figure 16) that the CMOS sensor is able to 
see regardless of the lighting in the room. This is, therefore, the most important portion of the Kinect 
which allows it to function. To acquire 3D depth information, software component of the Kinect interprets 
the data from the CMOS sensor. Rays are cast out via the infrared projector in a pseudo-random array 
across a large area. The CMOS sensor is able to then read the depth of all of the pixels at 30 frames 
per second. It is able to do this because it is an active pixel sensor (APS), which is comprised of a 
two-dimensional array of pixel sensors. Each pixel sensor has a photo detector and an active ampli.er. 
This camera is used to detect the  The Kinect software is able to track users skeletons by combining 
the depth information and knowledge about human body kinematics. This knowledgewas obtainedbygathering 
and labeling data from special rigs that captured user motions in everyday life. The images and labels 
were used for training a machine learning algorithm to create probabilities and statistics about the 
human form and movement. The software goes through a series of steps to make sense of the input from 
the camera and have the users body be the controller. Beginning with the user stepping in front of the 
Kinect, a 3D surface is generated using the depth information, creating a point cloud of the user. The 
Kinect then creates a guess at the users skeleton. Next, using using the kinematic data,the Kinect makes 
an educated guess at determining thedifferentpartsofthebody.Alevelof con.denceisalsoassignedtoeachguessbasedonhow 
con.dent the algorithm is about guessing the correct parts. Once this is done, the Kinect .nds the most 
probable skeleton (see Figure 18 that would .t these body parts and their con.dence levels assigned to 
them. This is performedrealtineat30 framesper second.For more detailonthe Kinectskeleton tracking algorithm, 
see Shotton et al. [Shotton et al. 2011]. 4.1.3 Playstation Move The PlayStation Move system consists 
of a PlayStation Eye and one to four PlayStation Move motion controllers (see Figure 19). It combines 
the advantages of camera tracking and motion sensing with the reliabilityandversatilityofbuttons.The 
wireless controllerisusedwithonehand,andithasseveraldigital buttonsonthefrontanda long-throwanalogTbuttonontheback. 
Internally,ithasseveralMEMS inertial sensors, including a three-axis gyroscope and three-axis accelerometer. 
But the most distinctive feature of the controller is the 44mm-diameter sphere on the top which houses 
a RGB LED that applications can set toanycolor.Thespherecolorcanbevariedto enhance interaction,buttheprimary 
purposeofthesphere is to enable reliable recovery of the controller 3D position using color tracking 
with the PlayStation Eye. The illuminated sphere design solves manyof the color tracking issues experienced 
with original EyeToy. Because the sphere generates its own light for the camera to see, scene lighting 
is much less of an issue. Trackingworks perfectlyina completely dark room,andfor scenes with highlyvaried 
lighting,the generatedlightmitigatesthevariability.Also,thelightcolorcanbeadjustedto ensureitisvisuallyuniquewith 
respect to the rest of the scene. Finally, the choice of the sphere shape makes the color tracking invariant 
to rotation; this simpli.es position recovery and improves position precision by allowing a strong model 
to be .t to the projection. Deriving the Playstation Move state involves two major steps: image analysis 
and sensor fusion. Though the exact details of these steps are beyond the scope of these notes, the following 
overview provides a qualitative understanding of each step. Image Analysis. Conceptually, the image analysis 
can be broken up into two stages: .nding the sphere in the image and then .tting a shape model to the 
sphere projection. Color segmentation is used to .nd the sphere and includes two steps; segmentation 
and pose recovery. Segmentation consists of labeling every video pixel that corresponds to the object 
being tracked. General shape segmentation for arbitrary objects is a dif.cult and computationally expensive 
problem. However, by designing the tracking object to have distinctive, solid, saturated colors, the 
object segmentation process can be simpli.ed to basic color segmentation. By choosing extremely saturated 
colors, the likelihood of these colors occurring in the environment is reduced. Color segmentation can 
be accomplished in a single pass through the image using chrominance banding/thresholding. Figure20showsthe 
resultsofsegmentationfora mace object consisting of a large orange ball mounted on a blue stick. By using 
only chrominance and not luminance, the segmentation process can be more robust to variation caused by 
scene lighting. Pose recovery consists of converting 2D image data into 3D object pose (position and/or 
orientation). Pose recovery canbe accomplishedrobustlyfor certain shapesofknownphysical dimensionsby 
measuringthe statistical properties of the shape s 2D projection. In this manner, for a sphere the 3D 
position can be recovered (but no orientation), and for a cylinder, the 3D position and a portion of 
the orientation can be recovered. Multiple objects can be also be combined for complete 3D pose recovery, 
though occlusion issues arise. Theexact color thresholds usedforsegmentation are basedon results from 
initial calibrationofthe sphere LED brightness and room lighting which is performed at the start of play 
(by simply pointing the controller This size and locationin the image are used asa starting point for 
.tting the shape model.To achieve the sub-pixel precision necessary for acceptable 3D position recovery, 
the shape model is .t to the original image RGB data rather than only the segmented pixels and accounts 
for camera lens blur. It is well-known that the 2D perspective projection of a sphere is an ellipse [Shivaram 
and Seetharaman 1998], though manytracking systems introduce signi.cant error by approximating the projection 
as a circle. In theory, .ttingsuchamodeltotheimagedatais straightforward,butinpracticemanyissuesarise. 
Motionofthe sphere causes motion blur in the image proportional to the exposure time. Motion also causes 
the sphere to appear warped due to rolling shutter effects; like most low-cost CMOS webcams, PlayStation 
Eye uses a rolling electronic shutter. This meansevery lineinthe videois imagedata slightlydifferent 
time, which leads to stretching or shortening for vertical motions and shear for horizontal motions. 
Subtle artifacts can also be introduced depending on the method used to convert the raw sensor data to 
RGB data. Like most low-cost cameras,the PlayStationEye sensor usesaBayer patternsuchthatonlyR,G,orBis 
actually measured at every pixel, and RGB must be inferred from surrounding pixels (a process known as 
Bayerpattern demosaicking, for which manyapproaches exist [Gunturk et al. 2005]). Another major concern 
is partial occlusion, which generally causes large errors in a global .t. To address this, a 2D ray-casting 
approach is used to identify areas of partial occlusion and remove them from the .t. Because the size 
of the sphere and the focal length of the PlayStation Eye are known, the 3D position of the sphere relative 
to the camera can be computed directly from the 2D ellipse parameters. The output of the image analysis 
stage is a timestamp and a measurement of the 3D camera-relative position for each sphere, updated at 
the framerate of the camera (typically 60Hz). Sensor Fusion. The results of the image analysis are combined 
with the inertial sensor data using a modi.ed unscented Kalman .lter. The details of this powerful state 
estimation technique are beyond the scope of these notes,but there are many excellentexplanationsavailable 
[Crassidis and Markley2003][Julier and Uhlmann 1997][Wan andVan Der Merwe 2002]. Though the sensors all 
contribute to the .nal state in a complex manner, each has a fundamental contribution that is necessary 
for the complete state computation. For example, the camera tracking provides an absolute measure of 
the 3D position. When the controllerisnotmoving,the accelerometerprovidesthe directionofgravity,whichgivesan 
absolute measure of the pitch and roll angles. In addition, when the orientation is known, gravity can 
be subtracted from the accelerometer data to recover the controller acceleration. The acceleration is 
part of the state, and it can also be used to reduce noise in the 3D position and to derive the 3D velocity. 
The gyroscope data is also crucial because it directly provides angular velocity. When integrated, this 
provides a responsive measure of 3D rotation (relativeorientation) and can be used to deriveangular acceleration. 
The remaining unknown, absoluteyaw,is the most tricky,butit canbe inferredby comparing the motion direction 
computed from body-relative inertial measurements to the motion direction computed from camera-relative 
image measurements. Again, though theoretically straightforward, in practice there are manyissues for 
sensor fusion. Because each controller, the PlayStation Eye, and the PS3 are driven by independent clocks, 
each piece of sensor data must be carefully timestamped so that it can be combined with other sensor 
data properly. Inertial sensor biases must be computed dynamically as they vary, most often due to changes 
in temperature. When the sphere is temporarily occluded or beyond the camera view, the state must be 
updated despite the lack of direct position measurements. And wireless data dropouts, though infrequent, 
must be hidden by extrapolating the state based on previous inertial data.  4.2 Getting Started and 
SoftwareTools 4.2.1 Nintendo Wii Remote TheWiimote connectstoaWiigame consoleor computer wirelessly through 
Bluetooth. Whenyou press both the1and2buttons simultaneously or press the red Syncbutton in the battery 
case, theWiimote s LEDs blink, indicating it s in discovery mode. Make sure your computer has a Bluetooth 
adapter and proper drivers. Currently, each OS and library handles the connection process differently. 
PCs. Here are the basic steps (for extra assistance, seek online help such as at www.brianpeek.com): 
1. Open BluetoothDevicesin the ControlPanel. 2. Under theDevices tab, click the Addbutton. 3. PuttheWiimote 
into discovery mode (continuously reenter this mode during this processasit times out). 4. SelectNext, 
selecttheWiimoteto connectto,and selectNextagain. 5. Don tuse a passkey, and select Next, then Finish. 
 Ifafailure occurs,repeatthis processuntilthePC connectstotheWiimote. Asimple startforaPCto retrieve 
data fromaWiimoteistheWiimoteLib(www.wiimotelib.org). Macs. The setup needs to run only once, and future 
Wiimote connections are simpler. To set up the Wiimote, run the Bluetooth Setup Assistant utility and 
follow these steps: 1. Select the AnyDevice option. 2. Put theWiimote into discovery mode. 3. Selectthedevice 
(NintendoRVL-CNT-01)andsetthepasskeyoptionsto Donotuseapasskey. 4. Press Continue until the Mac is connected; 
then quit the Setup Assistant.  To connectin the future,in the Bluetooth sectionof System Preferences, 
select theWiimote entry and set it to connect. Then, put theWiimote into discovery mode, and theWiimote 
will connect. The OS can keep some applications from connecting to theWiimote. In those cases, disconnect 
theWiimote and let the application connect to theWiimote. There are a variety software APIs and SDKs 
for using the Wiimote to develop 3D spatial interfaces. For C# programmers, Varcholiks Bespoke XNA 3DUI 
Framework [Varcholik et al. 2009] has a lot of functionalityandmakesitfairlyeasytoget started(www.bespokesoftware.org).Ithasmanyusefultools, 
and incorporates Brian PeeksWiimote library (www.brianpeek.com). Note with Bespoke, you can use as manyWiimotesasyouwant. 
Bespoke alsohasa gesture recognition engine whichmakeit easyto train and use gestures. OtherWiimote libraries 
includeWiiCadefor Flashdevelopers,GlovePie,anda hostof others (simplydoa google search forWiimote and 
API). 4.2.2 Microsoft Kinect At the time these notes were written, Microsoft had announcedbut not yet 
released their Kinect SDK. By the time of SIGGRAPH 2011, the SDK will be available. Microsoft s SDK is 
an ideal platform for using the Kinect to created 3D interfaces because it will be fully supported and 
will take advantage of the software usedin XBox 360gamedevelopment. The SDK provides Access to the four-element 
microphone array with acoustic noise and echo cancellation for crystal clear audio.  Sound source localization 
for beamforming, which enables the determination of a sounds spatial location, enhancing reliability 
when integrated with the Microsoft speech recognition API.  The Kinect depth data, which provides the 
distance of an object from the Kinect camera, as well as the raw audio and image data.  Robust skeletal 
tracking capabilities for determining the body positions of one or two persons moving within the Kinect 
.eld of view.  Documentation for the APIs and a description of the SDK architecture.  Sample code that 
demonstrates how to use the functionality in the SDK.  As an alternative to the Kinect SDK from Microsoft, 
you can use PrimeSense s NITE middleware drivers along with the OpenNI framework. This approach allows 
developer access to the RGB and depth cameras and provides simple gesture recognition and skeleton tracking. 
To use the Kinect with this approach perform the followthe steps found on http://www.codeproject.com/Articles/148251/How-to-SuccessfullyInstall-Kinect-on-Windows-Open.aspx 
 4.2.3 Playstation Move Sonyhas released the Move.Me application for the Playstation 3. This application 
lets the PS3 act as a device server for the Playstation Move system (see Figure 22) . Other applications 
on the PC can then connect to the server and get Move data tobuild 3D user interfaces and applications. 
The API for the Move.Me application is modular, so in can be incorporated into a variety of different 
graphics packages andgame libraries such asXNAand Unity3D. Figure 22: The Move.Me application architecture. 
The Move.Me SDK provides a simple interface for developers. The high-level state for each controller 
state can be queried at anytime, and consists of the following information: 3D position  3D orientation 
(as a quaternion)  3D velocity  3D angular velocity  3D acceleration  3D angular acceleration  Buttons 
status  Tracking status (e.g. visible) In addition, it allows developers to set the color of the sphere 
and initiate rumble feedback.  5 Applications 2: Spatial Interfaces in Scienti.c Visualization This 
chapter of the course notes returns to manyof the themes and tasks introduced initially in Chapter 2, 
CommonTasks in 3D User Interfaces. In this chapter, we consider selection and travel (navigation) techniques 
from the standpoint of scienti.c visualization applications. In addition to providing some concrete 
examples of the use of these techniques, scienti.c visualization applications are unique in that they 
demandalevelof precisionand accuracyof3D interfacesthatisnotalways requiredof other applications. Forexample, 
researchershaveextensively studied selectiontasks,andthe interaction literature documents a variety of 
selection techniques. However, selecting 3D regions of interest in medical brain-imaging data provides 
challengesthat requireustorevisit selection techniquesinanew context.Keefe s recent visualization viewpoints 
article presents additional detail on this and other 3D user interface challenges speci.c to scienti.c 
visualization applications[Keefe 2010]. 5.1 3D InteractiveTechniquesfor Scienti.c Visualization Thereisalong 
historyof applicationsof3D interactivetechniquesto scienti.c visualization. Early success stories include 
the virtual wind tunnel project [Bryson and Levit 1991] and pioneering work in virtual reality and haptics 
systems at UNC [Brooks 1988]. More recently, several applications have focused on interacting with anatomical 
models and .uid .ows (e.g., [Sobel et al. 2004; Hentschel et al. 2008] and even in ground-breakingly 
realistic surgical simulation [Zhang et al. 2010]. Although these and other applications have demonstrated 
what we believe is a great potential for 3D interaction in science, it seems that3D user interfaces remain 
underutilizedin scienceand medicine. Thismay soon change.Infact, one of our motivations in preparing 
this course is in recognizing that the emerging hardware and techniques beingdevelopedtoday, primarilyforgameand 
entertainment applications,couldhaveamajorimpacton scienti.c applications. 5.2 Selection Revisited When 
working with complex 3D data, developing accurate visual representations of spatial relationships is 
critical; intuitively, it makes sense that our input to the computer should match the visual output, 
also taking advantage of our abilities to process and describe information spatially in three dimensions. 
The taskof3D selectionis one area where utilizing reliable3D inputto complement3D visualization seemsas 
thoughitcouldhaveamajorpositiveimpacton scienti.cwork.ows. Earlier,we discussed selectioninthe context 
of generic virtual environments, consider now the challenges of selection in 3D or 4D scienti.c datasets,suchas 
medicalimagingdataor.uid.owdata.Oneexampleofthesedata comesfromdiffusion tensor MRI (DT-MRI), which can 
be used to derive dense 3D pathway datasets that correlate with neural .ber tracts in the brain. One 
important task that is often required when analyzing these data is partitioning 3Dpathwaysintobundlesthat 
represent coherent structuresinthe brain.Ascientistmightbe interested in the bundle of pathways that 
pass through a particular 3D location in the brain, the set of pathways that connect one region to another, 
or the set of pathways that belong to a known brain structure. From an interface standpoint, the task 
is selecting a subset of the visual elements displayed in a 3D scene. The challengeisthatthesedatatypicallyresultinsomeverycomplex3D 
scenes.Inparticular,thepathwaysare densely packed within space and manyof the pathways follow complex 
trajectories that vary in curvature and directionastheymovethrough space. These propertiesofthedatamakethisaparticularly 
challenging case for traditional 3D selection techniques. Figure 24: Three different views of the same 
simulated 3D .uid .ow volume demonstrate the spatial complexity that is typical of scienti.c datasets, 
motivating the need for improved spatial interfaces for selection, navigation, and other data analysis 
tasks (Flow data: Paraview of.ce demonstration dataset, http://www.paraview.org). Several researchers 
have addressed the challenge of working with complex scienti.c data through approaches that utilize 
3D user interfaces. Building on the brain tract selection problem described above and the3Ddrawing tools 
describedinthe .rst chapteron applications,Keefeetal.[Keefeetal. 2008b]and Zhou et al. [Zhou et al. 2008] 
developed techniques wherein 3D lassos are sketched around data features to select them. Figure 23 shows 
several examples of the types of 3D curves drawn with these interfaces, notice how theytwist and turn 
in space to encompass the features of interest specifying these 3D curves with a 2D interface would 
be very dif.cult to do. An exciting future direction for these 3D user interfaces would be to extend 
them to include additional data-driven interpretations of user input, such as the queryby-example strategies 
utilized in the CINCH system [Akers 2006] and other 2D visualization interfaces. The anticipated advantage 
of 3D user interfaces as demonstrated by the preliminary work described here is that the input from the 
user is particularly rich complex 3D curve inputs can be speci.ed directly whereas 2D interfaces would 
require, for example, multiple 2D inputs from the user to conveysimilar 3D information. Analyzing .uid 
.ow data is another extremely important application of visualization that poses similar challenges for 
3D selection interfaces, and when a time dimension is added, 4D .uid .ow data introduce even more challenges. 
Figure 24 illustrates the complexity of data that is often encountered. Several viewpoints of a line-based 
visualization of the same .ow vector .eld are shown in the .gure. These data comefromtheof.ceair.owsimulation 
demonstration dataset distributedwiththeParaviewtool[Kitware, Inc. ]. A variety of different .ow structures 
are seen within the data, even within this small set of views. Analyses of data like these often require 
researchers to identify features or volumes of interest within the .ow and then query underlying data 
values, for example, pressure, shear stress, and the like. Lasso selection techniques enabled by drawing 
curves in space have also been applied in this domain [Sobel et al. 2004], but from visualizing .uids 
to medical imaging data, volumetric selection remains a major challenge in interactivescienti.c visualization 
applications. Beyond the lasso-oriented strategies discussed in detail here, we believe that manyadditional 
3D selection strategies designed speci.cally for scienti.c visualization applications will be made possible 
and practical in the near future, given the proliferation of new commodity technologies for 3D input. 
 5.3 Navigation Revisited Chapter 2 of the course notes introduced the frequently used virtual reality 
interaction metaphor of a Worldin Miniature (WIM).Asanexampleofhow3D user interfaces canbe utilizedto 
supportnavigation in scienti.c applications,thissection describesanextensionofthecoreWIMframeworkforuseinvolume 
visualizations,forexample,navigatingthrough3D medicalimagingdata. Throughthis discussion,weaim to illustrate 
both the potential of applying 3D user interfaces to problems in scienti.c visualization and also describehow 
scienti.c applications often demanda rethinkingand re.nementof interfacesdeveloped in more general contexts. 
Figure 25 shows a recently developed WIM interface called Slice WIM [Coffeyet al. 2011]. As the name 
implies, one of the distinguishing characteristics of this interface relative to previous WIM methods 
is that it centerson slicing throughvolume data. Introducedby Stoakleyet al.[1995b]andPauschetal. [1995b], 
the core WIM concept is to provide a small (e.g. handheld) model of the virtual environment that can 
act as both a map and an interaction space as the user explores the large-scale environment essentially 
a world within a world. Operations that are dif.cult to perform in the large-scale world (navigating 
a great distance, selecting/manipulating an object thatisfaraway) canbe done easilyinthe WIM.To this 
core concept and related interaction techniques, researchers have more recently added features to support 
scaling and scrolling the WIM for use in very large environments [Wingrave et al. 2006] and automatic 
selection of optimal WIM views, including handling issues of occlusion, for use in complex architectural 
models where the 3D structure of the model can be analyzed algorithmically [Trueba et al. 2009]. For 
volumetric scienti.c data, issues of occlusion and scale are particularly important, and similarly important 
is the need to relate 3D representations of the data back to the raw imaging data from which they are 
derived. For these reasons, the SliceWIM interface pictured in Figure 25 makes heavy use of interactive 
slicing planes that clip the 3D geometry being visualized so as to provide useful internal views of the 
dataset. The WIM itself can be rotated to be viewed from anyorientation, and the user positions multiple 
slicing planes relative to the WIM in order to navigate through the dataset. Thus, to support scienti.c 
visualization of medical imaging data, this WIM-based interface introduces a reorient-able WIM and a 
metaphor for working with slicing planes for navigating inside the volume data. Aseries of new multitouch 
interaction techniques also accompanythese visuals, and these are described in detail in Chapter 6, which 
discusses mix and match spatial input, in this case, mixing 2D input from a multi-touch table with 3D 
interactions in the WIM space. Figure 25: Ascreen capture from the Slice WIM tool being used to explore 
a heart model extracted from imaging data. The pink miniature model in the foreground is an isosurface 
of the volume data, displayed at a small scale so that the user can understand his current placement 
within the dataset. Behind the miniature is a detailed (zoomed-in) display of the data that includes 
texture-mapped tissue information, etc. The WIM serves to facilitate navigation tasks and maintaining 
a global context while immersed withina small portionofthe volume. (Imaging data: National Libraryof 
MedicineVisible HumanProject http://www.nlm.nih.gov/research/visible/visible human.html) 5.4 Does spatial 
computingfor scienti.c applications requirea $1,000,000 lab? Until recently, each of the applications 
described earlier would have certainly required a major institutionalinvestmentina virtual reality researchlab, 
completewith3D trackingand display technology.Now, with the advent of new technologies primarily targeted 
atgame and movie applications, this is changing very rapidly. The scienti.c community is recognizing 
this and has held several workshops and special sessions at conferences on the topic of supporting new 
scienti.c work.ows via emerging, low-cost technologies [Keefe 2008; Coffeyet al. 2010]. It is quickly 
becoming possible for scientists to have in their labs and of.ces the same type of 3D interface technology 
that would have required major investments only a few years ago. This makes research in 3D user interfaces 
for science even more important, and it highlights the importance of research in areas such as accurate, 
controllable 3D selection and navigation. Although manygame technologies also support selection and 
navigation, thefast-as-you-can style of interaction typicallyinvolvedingamesisvery different than the 
typeof interaction needed for science. Ultimately, we believe that one of the exciting aspects of work 
in this emerging area of applying 3D user interface technologyto scienti.cproblemsisthatthevarious communitiesinvolvedcanalllearnfromeach 
other. Scientists clearlywanttobe abletoleveragegame technologiesin theirwork,but smart redesign and 
algorithm development will be needed. Game developers and researchers will undoubtably bene.t from learning 
about scienti.c applications and their requirements as these can open up new creative uses of emerging 
technologies and lead to the development of novel, robust interface techniques and algorithms that could 
be widely applicable.  6 3D Gesture RecognitionTechniques Recognizing 3D gestures has had a long history 
in the virtual reality and 3D user interface communities. The motion controllers we have discussed thusfar 
can all be used to detect 3D gestures for different applications. We divide gesture recognition into 
heuristic-based and machine-learning approaches. With heuristic-based approaches, the designer manually 
identi.es the heuristics that classify a particular gesture and differentiate it from other gestures 
that the system recognizes. Although this manual approach can be time consuming and tedious, especially 
as the set of recognized gestures grows, it s also highly understandable. Understandability is useful 
when new gestures are added to the gesture set and must be differentiated from past gestures.With machine 
learning approaches, gestures canbe recognizedby .nding useful features in the input data stream. These 
features de.ne a feature vector that is used as part of a machine learning algorithm. Typically, a set 
of gestures are collected to train the machine learning algorithm,whichisusedtomatchthe featurevectortothesetofpossible 
gestures,differentiating between them. In this section, we .rst examine some heuristic-based approaches 
to gesture recognition. Then, we examine two commonly used machine learning algorithms,a linear classi.er 
and an algorithm using the AdaBoost framework, and conduct experiments to examine their accuracy[Hoffman 
et al. 2010]. There are, of course, a variety of different machine learning algorithms that could be 
used to recognize 3D gesture and the reader is encouraged to examine [Duda et al. 2001]. Finally, we 
discuss using these gestures ina practical settingbyexploring user reactions andexperiences while playinga 
videogame. We make useofWiimotedatatoexplore these approaches,butdatafromanyofthedeviceswehave discussedin 
this course could be used. The only difference would be in how the features are calculated. 6.1 Interpreting 
Wiimote Data HerewereviewtwobasictypesofWiimotedata interpretation,integrationand recognition through 
heuristics. 6.1.1 Integration Because acceleration is the rate of velocity change, which is the rate 
of positional change, you can theoretically integratetheWiimote acceleration datato .ndvelocityandreintegrateitto 
.ndtheWiimote s actual position change. You should also be able to integrate the gyroscope s angular 
velocity to achieve absolute orientation. However,this approach has three signi.cant limitations [Giansanti 
et al. 2003]. First, acceleration jitter can lead to signi.cant positional deviations during a calculation. 
So, you can ignore accelerations below a threshold because they re most likely the result of jitter. 
Alternatively, a smoothing function can help reduce jitter. Second, you must remove the gravityvector 
from the reported acceleration before computingvelocity. If the sensor bar ora gyroscopeisavailable, 
you can possibly infer theWiimote s orientationand realizethegravityvectorasa1-gdownward force; otherwise,you 
must determine the gravity vector on the .y. One way to compute the gravity vector is by watching the 
derivative of the acceleration (or jerk data). When this is close to zero (that is, no acceleration change 
is computed) and the reported acceleration magnitudeis closeto1g, thereby eliminating casesof user-induced 
constant acceleration,youcan assumetheWiimoteis reportingonlythegravityvector. Subtractingthisvectorfrom 
future accelerations results in the acceleration being directly attributable to user action, assuming 
theWiimote maintains its orientation. Third, orientation must be accurate; even slight errors produce 
large positional errorsaftermovement.Forexample,onemeteroftravelaftera.ve-degree orientationchange(wellwithin 
thevarianceofthehand) resultsinan errorofnearly9 cm.Alarger30-degree orientationchange results in an 
error of 50 cm. Additionally, unaccounted-for orientation changes can give very wrong results. For example, 
raisingtheWiimoteafoot,invertingit,andloweringittoits original locationwill resultinno actual positional 
change,butas computed will resultinatwo-footupwardmovementintheWiimote s FOR.You can address this onlyby 
using the SBC ora gyroscope, because theWiimote s accelerometers can teasily providea gravityvector while 
theWiimoteis moving, as we discussed earlier. Inwork witha locomotion interfacefor American football[Williamsonetal. 
2010], double integrationlet users controlthe applicationusingaWiimoteandhave theirbodymovements maneuverthe 
quarterback. To achievethis,aWiimotewas attachedtothe centerofthe user s chest, whichwas closetothebody 
s center of mass. Although this improved the reported acceleration data, it broke the SBC. TheWiimote 
acceleration data was then passed through an exponential smoothing .lter: .acurrent = a.ai + (1 - a).ai-1 
(17) where a =0.9.An alternate approachisto usea Kalman .lter,but this requires more computation[LaViola 
2003]. Finally the double-integration step was performed on the smoothed acceleration data. Owing to 
these three steps, theWiimotewas responsive, and the user seemed to move relatively accurately for that 
application. To further evaluate the control s accuracy, tests were performed in which the user moved 
from a starting locationandthenback. Thesetestsshowedlittle errorinpositionovershorttimeperiods(5to10seconds), 
but only when users maneuveredin an unnaturally upright and stifffashion. 6.1.2 Recognition through 
Heuristics TheWiimote providesa raw data stream, and you can use heuristics to interpret and classify 
the data. Whether you use heuristics on their own or as features for gesture recognition, their higher-level 
meaning is more useful for 3DUI design and implementation. In this section, we discuss several heuristic-based 
approaches for interpretingWiimote data used in two prototypegame applications. One Man Band used aWiimote 
to simulate the movements necessary to control the rhythm and pitch of several musical instruments [Bott 
et al. 2009]. Careful attention to theWiimote data enabled seamless transitions between all musical instruments. 
RealDanceexplored spatial3D interactionfor dance-basedgamingand instruction [Charbonneau et al. 2009]. 
By wearing Wiimotes on the wrists and ankles, players followed an on-screen avatar s choreographyand 
had their movements evaluated on the basis of correctness and timing. Poses and underway intervals. 
A pose is a length of time during which the Wiimote isn t changing position. Poses canbe useful for identifying 
held positionsin dance, duringgames, or possiblyevenin yoga.AnunderwayintervalisalengthoftimeduringwhichtheWiimoteismovingbutnot 
accelerating. Underway intervals can help identify smooth movements and differentiate between, say, strumming 
on a guitar and beating on a drum. Because neither poses nor underway intervals have an acceleration 
component, you can t differentiate them by accelerometer data alone. To differentiate the two, an SBC 
can provide an FOR to identify whether theWiimote hasvelocity. Alternatively,you can use context, trackingWiimote 
accelerationsover time togauge whether the device is moving or stopped. This approach can be error prone,but 
you can successfully use it until you reestablish the SBC. Poses and underway intervals have three components. 
First, the time span is the duration in which the user maintains a pose or an underway interval. Second, 
the gravity vector s orientation helps verify that the userisholdingtheWiimoteatthe intended orientation.Of 
course, unlessyouuseanSBCora gyroscope, theWiimote syawwon tbe reliably comparable. Third,theallowedvarianceisthe 
thresholdvalueforthe amountofWiimote acceleration allowedin the heuristic before rejecting the pose or 
underway interval. In RealDance, poses were important for recognizing certain dance movements. For a 
pose, the user was supposed to stand still in a speci.c posture beginning at time t0 and lasting until 
t0 + N, where N is a speci.ed number of beats. So, a player s score could be represented as the percentage 
of the time interval during which the user successfully maintained the correct posture. Impulse motions. 
An impulse motionis characterizedbya rapid changein acceleration, easily measured bytheWiimote s accelerometers. 
Agoodexampleisa tennisorgolfclubswingin whichtheWiimote motion accelerates through an arc or a punching 
motion, which contains a unidirectional acceleration. An impulse motion has two components, which designers 
can tune for their use. First, the time span of the impulse motion speci.esthe windowover whichthe impulseis 
occurring. Shorter time spans increase the interaction speed,but larger time spans are more easily separable 
from background jitter. The second component is the maximum magnitude reached. This is the acceleration 
bound that must be reached during thetimespanin orderfortheWiimoteto recognizethe impulse motion. You 
can also characterize impulse motions by their direction. The acceleration into a punch is basically 
a straight impulse motion, a tennis swing has an angular acceleration component, and a golf swing has 
both angular acceleration and even increasing acceleration during the follow-through when the elbow bends. 
All three of these impulse motions, however, are indistinguishable to theWiimote, which doesn t easily 
sense these orientation changes.Forexample, the punch has an accelerationvector alonga single axis, as 
does the tennis swing as it roughly changes its orientation as the swing progresses. You can differentiate 
the motions onlyby using an SBC ora gyroscope orby assuming that theWiimote orientation doesn t change. 
RealDance used impulse motionsto identify punches.Apunchwas characterizedbya rapid deceleration occurringwhenthearmwasfullyextended.Inarhythmgame,this 
instantshouldlineupwithastrongbeat in the music. An impulse motion was scored by considering a one-beat 
interval centered on the expected beat.FortheWiimote correspondingtotherelevantlimb,thetimesampleinthetimespan 
corresponding tothe maximal accelerationintheWiimote s longitudinalaxiswas selected.Ifthis maximal acceleration 
was below a threshold, no punch occurred, and the score was zero. Otherwise, the score was computed from 
the distance to the expected beat. If the gesture involved multiple limbs, the maximal acceleration valuehadtobe 
greaterthanthe thresholdforallWiimotesinvolved. Theaverageofallindividuallimb scores served as the gesture 
s overall score. Impact events. An impacteventisan immediate halttotheWiimoteduetoa collision, characterized 
by an easily identi.able accelerationbursting across all three dimensions. Examplesof thisevent include 
the user tapping theWiimote ona table ora droppedWiimote hitting the .oor. To identify an impact event, 
compute the change in acceleration (jerk) vectors for each pair of adjacent time samples. Here, tk corresponds 
to the largest magnitude of jerk: tk = argmax ..at - .at-1.. (18) T Modal differentiation. You can use 
easily recognized modes in an interface to differentiate between functionality. Forexample,abutton s 
semantics can change as theWiimote changes orientation, or the Wiimote s pitch might differentiate between 
a system s states. Although modes can lead to errors when users are unawareof them, constant user action 
suchas holdingabuttonor pose can leadto quasimodal states that are easily understood and useful. This 
is important because theWiimote has several attachments, such as the Nunchuk, and multiplebuttons that 
you can use to create quasimodes. In One Man Band, the multi-instrument musical interface (MIMI) differentiated 
between .ve different instruments by implementingmodaldifferencesbasedontheWiimote s orientation.Figure26showsfourofthese.Ifthe 
userheldtheWiimoteonitssideandtotheleft,asifplayingaguitar,the application interpretedimpulse motionsas 
strumming motions.Ifthe userheldtheWiimotetotheleft,asif playinga violin,the application interpreted 
the impulse motions as violin sounds. To achieve this, the MIMI s modal-differentiation approach used 
a normalization step on the accelerometer data to identify the most prominent orientation: .a .anorm 
= (19) ..a. followed by two exponential smoothing functions (see Equation 17). The .rst function, with 
an a =0.1, removed jitter and identi.ed drumming and strumming motions. The second function, with an 
a = 0.5, removed jitter and identi.ed short, sharp gestures such as violin strokes. The MIMI also used 
the Nunchuk s thumbstick to differentiate between the bass and guitar.  6.2 Linear Classi.er The linear 
classi.er we discuss in this section is based on Rubine s gesture recognition algorithm [Rubine 1991]. 
Given a feature vector f., associated with a given 3D gesture g in a gesture alphabet C, a linear evaluation 
function is derived over the features. The linear evaluation function is given as F . gc = wc0 + wcifi 
(20) i=1 where 0 = c<C, F is the number of features, and wci are the weights for each feature associated 
with each gesture in C. The classi.cation of the correct gesture g is the c that maximizes gc. Training 
of this classi.er is done by .nding the weights wci from the gesture samples. First, a feature vector 
mean f.c is calculated using Ec-1 . 1 fci = fcei (21)Ec e=0 where fcei is the ith feature of the eth 
example of gesture c and 0 = e<Ec where Ec is the number of training samples for gesture c. The sample 
covariance matrix for gesture c is Ec-1 .  1cij =(fcei - fci)(fcej - f cj). (22) e=0 The 1cij are averaged 
to create a common covariance matrix C.-1 1cij 1ij = c=0 . (23) C.-1 -C + Ec c=0 The inversion of 1ij 
then lets us calculate the appropriate weights for the linear evaluation functions, F .  wcj = (1-1)ijfci 
(24) i=1 and F . 1  wc0 = - wcifci (25) 2 i=1 where 1 = j = F . 6.3 AdaBoost Classi.er The AdaBoost 
algorithm is based on LaViola spairwise AdaBoost classi.er [LaViola and Zeleznik 2007]. AdaBoost [Schapire 
1999] takes a series of weak or base classi.ers and calls them repeatedly in a series ofroundsontrainingdatatogeneratea 
sequenceofweakhypotheses.Eachweakhypothesishasaweight associated withit thatis updated after each round, 
basedonits performanceonthe training set.Aseparate setofweightsareusedtobiasthetrainingsetsothatthe importanceof 
incorrectly classi.edexamplesare increased. Thus, the weak learners can focus on them in successive rounds. 
Alinear combination of the weakhypothesesandtheirweightsareusedtomakeastronghypothesisfor classi.cation. 
More formally,for each unique 3D gesture pair,our algorithm takes as input training set (.x1,y1), ..., 
(.xm,ym), where each .xi, represents a feature vector containing J features. Each yi labels .xi using 
label set Y = {-1, 1}, and m is the total number of training samples. Since we are using a pairwise approach, 
our algorithmneedstotrainalluniquepairsof gestures.Foreachuniquepair,the AdaBoostalgorithmiscalled on 
a set of weak learners, one for each feature discussed in Section 6.4. We chose this approach because 
we found, based on empirical observation, that our features can discriminate between different 3D gesture 
pairs effectively. We wanted the features to be the weak learners rather than having the weak learners 
act on the features themselves. Thus, each weak learner Cj uses the jth element in the .xi training samples, 
which is noted by .xi(j) for 1 = j = J. 6.3.1 Weak Learner Formulation We use weak learners that employa 
simple weighted distance metric, breaking (.x1,y1), ..., (.xm,ym) into two parts corresponding to the 
training samples for each gesture in the gesture pair. Assuming the training samples are consecutive 
for each gesture, we separate (.x1,y1), ..., (.xm,ym) into (.x1,y1), ..., (.xn,yn) and (.xn+1,yn+1), 
..., (.xm,ym) and de.ne D1(i) for i =1, ..., n and D2(i) for i = n +1, ..., m to be training weights 
for each gesture. Note that in our formulation, D1 and D2 are the training weights calculated in the 
AdaBoost algorithm (see Section 6.3.2). For each weak learnerCj in each feature vector .xi(j) in the 
training set, the weighted averages are then calculated as . n k=1 xk(j)D1(k) j1 = . n (26) l=1 D1(l) 
and . m xk(j)D2(k) k=n+1 j2 = . m D2(l) . (27) l=n+1 Theseaveragesareusedto generatetheweakhypothesesusedinthe 
AdaBoost training algorithm. Ifa given feature value for a candidate gesture is closer to j1, the candidate 
is labeled as a 1, otherwise the candidate is labeled as a -1. If the feature value is an equal distance 
away from j1 and j2, we simply choose to label the gesture as a 1. Note that it is possible for the 
results of a particular weak classi.er to obtainlessthan50% accuracy.Ifthis occurstheweaklearnerisreversedsothatthe.rstgesturereceivesa 
-1 and second gesture receivesa 1. This reversal lets us use the weak learner s output to the fullestextent. 
 6.3.2 AdaBoost Algorithm For each roundt =1, ..., T *J where T isthe numberof iterationsoverthe J weak 
learners, the algorithm generatesa weakhypothesis ht : X . {-1, 1} from weak learner Cj and the training 
weights Dt(i) where j = mod(t - 1,J)+1 and i =1, ..., m. This formulation lets us iterate over the J 
weak learners and still conform to the AdaBoost framework [Schapire 1999]. Indeed, the AdaBoost formulation 
allows us to select weak classi.ers from differentfamilies at different iterations. 1 Initially, Dt(i) 
are set equally to m , where m is the number of training examples for the gesture pair. However, with 
each iteration the training weights of incorrectly classi.ed examples are increased so the weak learners 
can focus on them. The strengthofa weakhypothesisis measuredby its error . .t = Pri~Dt [ht(.xi(j)) =.yi]= 
Dt(i). (28) i:ht(.xi(j)). =yi Givena weakhypothesis, the algorithm measures its importance using the 
parameter () 11 - .t at = ln . (29) 2 .t Withat, the distribution Dt is updated using the rule Dt(i) 
exp(-atyiht(.xi(j))) Dt+1(i)= (30) Zt where Zt isa normalizationfactor ensuring that Dt+1 is a probability 
distribution. This rule increases the weight of samples misclassi.ed by ht so that subsequent weak learners 
will focus on more dif.cult samples. Once the algorithm has gone through T * J rounds,a .nalhypothesis 
() T *J . H(x)= sgn atht(x)(31) t=1 is used to classify gestures where at is the weight of the weak learner 
from round t, and ht is the weak hypothesis from roundt. If H(x) is positive, the new gesture is labeled 
with the .rst gesture in the pair and if H(x) is negative it is labeled with the second gesture in the 
pair. These stronghypotheses are computed for each pairwise recognizer with the labels and stronghypothesis 
scores tabulated.To combine the results from each stronghypothesis we use the approach suggestedby Friedman 
[Friedman 1996]; the correct classi.cation for the new gesture is simply the one that wins the most pairwise 
comparisons.If thereisatie, thentheraw scores fromthe stronghypotheses are usedand the one of greatest 
absolute value breaks the tie.  6.4 Feature Set The features used in the linear and AdaBoost classi.er 
on Rubine s feature set [Rubine 1991]. Since Rubine sfeatures were designedfor2Dgesturesusingthe mouseor 
stylus,theywereextendedtoworkfor 3D gestures. TheWiimote andWii MotionPlus sense linear acceleration 
and angularvelocity respectively. Although we could have derived acceleration and angular velocity-speci.c 
features for this study, we chose to make an underlying assumption to treat the acceleration and angular 
velocity data as position information in 3D space. This assumption made it easy to adapt Rubine s feature 
set to the 3D domain and the derivative information from theWiimote andWii MotionPlus. The .rst featureintheset, 
quanti.esthe total durationofthe gesturein milliseconds, followedby features for the maximum, minimum, 
mean and median values of x,y and z. Next we analyzed the coordinates in 2D space by using the sine and 
cosine of the starting angle in the XY and the sine of the starting angle in the XZ plane as features. 
Then the feature set included features with the sine and cosine of the angle from the .rst to last points 
in the XY and the sine of the angle from the .rst to last points in the XZ plane. After that, features 
for the total angle traversed in the XY and XZ planes, plus the absolute value and squared value of that 
angle, completed the features set which analyzes a planar surface. Finally, the length of the diagonalofthe 
boundingvolume,the Euclidian distance betweenthe .rstandlast points,the total distance traveledby the 
gesture and the maximum acceleration squared ful.ll Rubine s list. Initially,this featuresetwasusedforboththeWiimoteandtheWii 
MotionPlus1, totaling58 featuresused in the two classi.ers. However, after some initial pilot runs, a 
singular common covariance matrices was formingwiththe linear classi.er.Asingular commoncovariance matrix 
causethe matrixinverse neededto .nd the weights for the linearevaluation functions impossible. These 
singular matrices were formed when tryingtousethe featuresetwiththeWii MotionPlus attachment. Becauseofthis 
problem,the MotionPlus featuresetwas culledtouseonlythe minimumand maximumx,y,andzvalues,the meanx,y,andz, 
values,andthemedianx,y,andzvalues.Thus,29 featureswereusedwhenrunningtheexperimentswith theWiimote and41 
features were used when runningexperiments with theWiimote coupled with theWii MotionPlus. 6.5 Gesture 
Set The majorityoftheworkon3D gesture recognitionwith accelerometerand gyroscope-basedinputdevices containexperimentsusingonlyfourto10unique 
gestures[Kratzetal.2007;Rehmetal.2008;Schl  omer et al. 2008]. In order to better understand how manygestures 
these types of devices can accurately recognize and how manytraining samples would be needed to do so, 
we chose to more than double that set and use25 gestures. The gestures, depicted graphicallyin Figure27, 
are performedby holdingtheWiimote in various orientations. For a majority of the gestures, the orientation 
and rotation are the same for both left and right handed users. The onlyexceptions are the gesturesTennis 
Swing, Golf Swing,Parry, and Lasso. The gesturesTennis Swing and Golf Swing are performed on either the 
left or right side of the user, corresponding to the hand that holds theWiimote.For theParry gesture,a 
left-handed person will movetheWiimotetowardstheright,whilea right-handedpersonwillmovetheWiimotetowardstheleft. 
Finally, the Lasso gesture requires a left-and right-handed user to rotate in opposite directions, clockwise 
versus counterclockwise, respectively. Thisgesturesetwasdevelopedbyexaminingexistingvideogamesthatmakeuseof3Dspatial 
interaction, 1TheWii MotionPlus used maximum angularvelocity squared insteadof maximum acceleration squared. 
 speci.cally fromthe NintendoWiigaming console.Weexaminedavarietyofdifferentgames from different genres 
including sportsgames, .rst person shooters, .ghtinggames, and cookinggames. Although thegesturesetisgamespeci.c,itisgeneralenoughtoworkinawidevarietyof 
virtualandaugmented reality applications. Initially, we focused on simple movements such as line to the 
right and line to the left which could be applied to various actions. From there, slightly more complex 
gestures were added which involved closed .gures such as the square, triangle and circle. With this group, 
we add user variations in velocity, duration and distance traversed. Finally the last set of maneuvers 
allowed for more freedom in body movement in an effort to help disambiguate gestures during feature analysis. 
Examples of these gestures include golf swing, whip and lasso. 6.6 3D Gesture Data Collection Machine 
learning algorithms such as the linear and AdaBoost classi.ers need data to train on. In order for the 
algorithms to learn and then recognize gestures, a gesture database was created to use in both training 
and testing.We recruited17 participants(4 femaleand13male)fromtheUniversityof Central Florida,of which 
four men were left handed, to provide the gesture samples. Each participant had experience using the 
NintendoWiigaming console,withtwo participantsdoingsoonaweekly basis. Several steps were taken to ensure 
that participants provided good gestures samples. First, they were givena briefoverviewoftheWiimote gesture 
data collection interfaceanda demonstrationofhowto interact with the application. After the demonstration, 
participants were presented with an introduction screen asking them to choose what hand they would hold 
the Wiimote with to perform the gestures. This decision also told the application to present either left 
or right-handed gesture demonstration videos. Next, an orientation window (see Figure 28) was displayed 
to help participants learn how to perform the gestures before data collection began. Each of the 25 gestures 
are randomly listed on the left hand side of thewindowto reducetheordereffectduetofatigue.Participantsmovedthroughthelistof 
gesturesusing theupanddownbuttonsontheWiimote.Atanytime, participantscouldviewavideo demonstratinghow 
toperforma gestureby pressingtheWiimote srightbutton. These videos demonstratedhowtoholdthe Wiimote in 
the proper orientation in addition to showing howto actually perform the motion of the gesture. Before 
the gesture collection experiment began, an orientation phase required participants to perform and commit 
one samplefor each gesture. This reducedthe ordereffectduetothe learning curve.To performa gesture, participants 
pressedandhelddowntheWiimote s B buttontomakeagesture. After participants released the B button, theycould 
either commit the gesturebypressing theWiimote s A button or redo the gestureby pressingand holdingthe 
B buttonand performingthe gestureagain. This feature enabled participants to redo gesture samples they 
were not happywith. Once a gesture sample is committed to the database, the system pauses for two seconds, 
preventing the start of an additional sample. This delay between samplesallows participantsto resettheWiimote 
positionforthenext gesture sample.In addition, thedelaypreventstheuserfromtryingtogamethesystembyquickly 
performingthesamegesturewith little to no regard of matching the previous samples of that gesture. After 
participants created one sample for each of the 25 gestures, they entered data collection mode. The only 
difference between orientation mode and data collection mode is the amount of samples which need to be 
committed. When participants commit a training sample for a particular gesture, a counter is decremented 
and displayed. Amessage saying the collection process for a given gesture is complete is shown after20 
sampleshavebeen enteredfora gesture. This message indicatestothe participantthatthey can move on to providing 
data for a remaining gesture. A total of 20 samples for each gesture, or 500 samplesinall,isrequiredforafullusertrainingset.Intotal,8,500gesturesampleswere 
collected2.Each data collection session lasted from 30-45 minutes. 6.7 3D Gesture Recognition Experiments 
With the 3D gesture database, we were able to run a series of experiments to examine both learning algorithms 
in terms of user dependent and user independent recognition. The primary metric analyzed in our experiments 
is gesture classi.cation accuracy, while having the over-arching goal of maximizing the number of gestures 
correctly recognized at varying degrees of training the machine learning algorithms. For both the dependent 
and independentexperiments, the linear and AdaBoost classi.ers were tested using theWiimote data only 
and using theWiimote in conjunction with theWii MotionPlus attachment. Thus, the experiments attempted 
to answer the following questions for both the user dependent and independent cases: How manyof the 25 
gestures can each classi.er recognize with accuracyover 90%? 2The gesture database can be downloaded 
at http://www.eecs.ucf.edu/isuelab/downloads.php.  How manytraining samples are needed per gesture to 
achieve over 90% recognition accuracy?  How much accuracyimprovementisgained from using theWii MotionPlusdevice? 
 Which classi.er performs better?  Which gestures in our gesture set cause recognition accuracydegradation? 
 6.7.1 User Dependent Recognition Results For the user dependent recognition experiments, we used a 
subset of samples for each gesture from a single user to train the machine learning algorithms and the 
remaining samples to test the recognizers. This approach is equivalent to having a user provide samples 
to the recognizer up front, so the algorithm can be tailored to that particular user. For each user dependent 
test, two categories of experiments were created: classi.cation over all 25 gestures and .nding the maximum 
recognition accuracy rate over as manygesturesas possible. Both categories were then broken into threeexperimentsproviding5,10,or15 
training samples per gesture with the remaining samples used for testing accuracy. Each experiment was 
executed on all four of the classi.ers mentioned earlier. The experiment set was conducted on each of 
the 17 participant s data. Comparison Test Statistic PValue Linear5 -Ada5 t16 = 7.17 p < 0.01 LinearMP5 
-AdaMP5 t16 = 8.36 p < 0.01 Linear10 -Ada10 t16 = 6.48 p < 0.01 LinearMP10 -AdaMP10 t16 = 6.54 p < 0.01 
Linear15 -Ada15 t16 = 7.69 p < 0.01 LinearMP15 -AdaMP15 t16 = 7.16 p < 0.01 Table 1: Results from a 
set of t-tests showing signi.cance differences between the linear classi.er and AdaBoost indicating the 
linear classi.er outperforms AdaBoost in our test cases. Note that under the comparison column,MP stands 
for whether theWii MotionPlus was used and the numberrepresents how many samples were used for training 
the algorithms. The resultsoftryingto recognizeall25 gesturesineachexperimentareshowninFigure29.We analyzed 
this data using a 3 way repeated measures ANOVA and found signi.cance for the number of training samples(F2,15 
= 17.47,p < 0.01), the classi.cation algorithm(F1,16 = 119.42,p < 0.01), and the use of theWii MotionPlus 
data(F1,16 =8.23,p < 0.05). To further analyze the data, pairwise t tests were run. The most notable 
observation is the high level of accuracy across all experiments. In particular, the linear classi.ergaveamean 
accuracy valueof93.6%usingonly5trainingsamplesforeachgestureand 98.5% using 15 training samples per gesture(t16 
= -5.78,p < 0.01). Furthermore, the linear classi.er outperformed AdaBoostinevery situationbyat least3%(seeTable1forthe 
statistical results). Another important result shownin Figure29is the roleof theWii MotionPlusin both 
recognition algorithms. The Wii MotionPlus signi.cantly increased the recognition accuracyfor the linear 
classi.er to 95.5% using5 training samples(t16 = -2.81,p < 0.05)per gesture and 99.2% using 15 training 
samples per gesture (t16 = -2.54,p < 0.05).For AdaBoost, the changein accuracy was negligible. While 
these accuracylevels are good for some applications, it is important to know which gestures would need 
to be removed from the classi.cation set in order to improve recognition results. To begin this Figure 
30: The average recognition results for the user dependent experiments over a varying number of gestures.Thegoalofthisexperimentwastorecognize,withhigh 
accuracy,asmanygesturesas possible with different levels of training. The results are grouped by the 
number of training samples (5, 10, or 15) provided for each gesture within an experiment. A single user 
dependent experiment utilized two classi.ers (linear and AdaBoost), each executing with either theWiimote 
orWiimote+MotionPlus input device producing four accuracy values. examination, the set of gestures frequently 
recognized incorrectly from the previous three experiment categories were removed.Within this set we 
noticedafew gestures with similar attributes such asTennis Swing and Golf Swing or Square and Triangle. 
These gesture pairs involve similar movement which caused the classi.ers to misidentify each type as 
the other. Once the poorly classi.ed gestures were extractedthe accuracyresultseasily increasedtoabove98%.Wesystematicallyaddedeachoftheremoved 
gestures back into the gesture set ona casebycase basis, leaving one gesture from each similar pairing 
out in order to improve the recognition on the other included gesture from each pair. The results are 
shown in Figure30.Aswiththepreviousexperiment,werana3 way repeated measuresANOVA aswellasttests and found 
signi.cant results for the training sample/number of gestures pairs(F2,15 =5.01,p < 0.05), the classi.cation 
algorithm(F1,16 = 48.55,p < 0.01), and the use of theWii MotionPlus data(F1,16 = 9.69,p < 0.01). In theexperiment 
using5training samples, the gesturesForward, Golf Swing, Spike,Triangle and Line to Left were removed, 
thereby producing over 93.5% accuracy for AdaBoost and over 96.3% for the linear classi.er. Once the 
number of training samples was increased to 10, the set of removed gestures included only Spike and Triangle. 
This experiment yielded higher accuracy with the linear classi.er (t16 = -1.90,p =0.075)and AdaBoost(t16 
= -1.07,p =0.3),but these results werenot signi.cant. Finally, since the recognition rates for 15 training 
samples were already greater than 95%, no gestures were removed during this last test. These results 
show that recognition accuracy rates as high as 97% can be achieved for 20 gestures using only 5 samples 
per gesture and 98% for 23 gestures using 10 samples per gesture with theWiimote coupled with theWii 
MotionPlus attachment. Infact, for the linear classi.er, the recognizer obtained signi.cantly higher 
accuracyusing theWii MotionPlus attachment in the5training sample/20 gesture case(t16 = -2.32,p < 0.05)and 
the 10 training sample/23 gesture case (t16 = -2.18,p < 0.05). Figure 31: The average recognition results 
for the user independent experiments over a varying number ofgestures.Thegoalofthisexperimentwastorecognize,withhigh 
accuracy,asmanygesturesaspossible when given different levels of training. The results are grouped by 
the number of user training samples (100,200, or 300) pergesture used for training within anexperiment. 
These numbers are analogous to using5,10,and15 users datafortraining.Asingle user independentexperiment 
utilizedtwo classi.ers (linear and AdaBoost), each executing with either the Wiimote or Wiimote + MotionPlus 
input device producing four accuracy values. 6.7.2 User Independent Recognition Results For the user 
independent recognition experiments, a subset of the 17 user study gesture data .les was used for training. 
From the remaining .les, recognition is performed and reported on a per user basis. This approach is 
equivalent to having the recognizers trained on a gesture database and having new users simply walk up 
and use the system. The bene.t of this approach is there is no pre-de.ned training needed for each user 
at a potential cost in recognition accuracy. We ran three tests in this experiment, using data from 5, 
10, and 15 users from our gesture database for training with the remaining user data for testing. As 
in the user dependent study, each scenario was executed on all four of the classi.ers mentioned earlier. 
Note that for the independent recognition results, we did not perform anystatistical analysis on the 
data because we did not have an equal number of samples for each condition and the sample size for the 
15 user case as two small (only two test samples). An approach to dealing with this issue is to perform 
cross validation on the data so proper statistical comparisons can be made. The results, shown in Figure 
31, shows that the linear classi.er outperforms AdaBoost by at least 3% and using theWii MotionPlus attachment 
improved recognition for only the linear classi.er. However, unlike in the user dependent tests, theWii 
MotionPlus slightly hindered AdaBoost accuracy. After removing the poorly classi.ed gestures, we followed 
the reintroduction method we used for the user dependent tests. The linear classi.er was able to recognize 
a total of 9, 10, and 13 gestures with mean accuracy values of 95.6%, 97.6%, and 98.3% respectively using 
theWiimote coupled with theWii MotionPlus attachment. The gestures enabled for the5 user training set 
includedForward, Stop, Open Door,Parry, Chop, Circle, LinetoRight,LineUpandStab. Whenusingthe10 user 
trainingset,theTwisterandSquare gestures were added while maintaining slightly higher accuracylevels. 
On the other hand, the gesture Open Door was removed because the newly introduced training data increased 
confusion among samples of that type. Finally, for the 15 user training set, the gestures Open Door (again), 
In.nity, Zorro and Line Down were addedbut theTwister gesturewas removed.  6.8 AccuracyDiscussion From 
theseexperiments, we can see that253D gestures canbe recognized using theWiimote coupled withtheWii MotionPlus 
attachmentatover99% accuracyinthe user dependent caseusing15 training samples per gesture. This result 
signi.cantly improves upon the results in the existing literature in terms ofthetotalnumberofgesturesthatcanbe 
accuratelyrecognizedusingaspatiallyconvenientinputdevice. There is, of course, a tradeoffbetween accuracyand 
the amount of time needed to enter training samples in the user dependent case. 15 training samples per 
gesture might be too time consuming for a particular application. As an alternative, the results for5training 
samples per gesture only showsa small accuracy degradation.For the user independent case, we can see 
the accuracyimproves and the numberof gestures that can reliably be recognized also increases as the 
number of training samples increases. Although the overall recognition accuracyand the number of gestures 
was higher in the dependent case (as expected), the independent recognizer still provides strong accuracyresults. 
Comparing the two classi.ers in the experiments shows the linear classi.er consistently outperforms the 
AdaBoost classi.er. This is somewhat counterintuitive given the AdaBoost classi.er is a more sophisticated 
technique. However, as we discussed in Section 6.4, the linear classi.er can suffer from the singular 
matrix problem which can limit its utility when new features that could improve accuracy are added. With 
theWii MotionPlus, we also tried to adjust for gyroscopic drift in data. Our attempt at calibration involved 
setting theWiimote coupled with the MotionPlusdevice ona table with thebuttonsdown for approximately10 
seconds, followedby storingasnapshotoftheroll,pitchandyawvalues. That snapshot was then used as a point 
of reference for future collection points. However, using those offsets caused decreased accuracy in 
AdaBoost and singular matrices in the linear classi.er leading to the use of raw gyroscope data instead. 
 6.9 Using 3D Gestures in a Practical Setting Testing heuristic-based approaches and gesture recognition 
algorithms on a restricted set of data will providesomeinsightintohoweffectivethe accuracycanbe.However,theseexperimentsareoften 
considered toprovidean upper boundon performance,given ideal conditions. Thusitis importantto understandperformanceduringreal 
situations.Inthiscase, performanceisnotjust accuracy,butalsoplayerexperience. Wehave designedanddevelopeda 
videogame prototype,WizardofWii(See Figure32),a linear, .rst personadventuregame withitsgameplay based 
entirelyon3D gestures performed witha NintendoWiimote. Players are placed in the role of a wizard on 
an adventure and are required to complete a series of four quests in order to win. The player interacts 
with the world using 3D gestures to cast spells, wield a sword, and teleport between quests. The player 
moves automatically from the start to .nish in a linear fashion. Gameplay tasks are presented to players 
at prede.ned points, with visual cues to alert players that they are required to perform a gesture. Each 
visual cue, in the form of an image, also tells players which gesture to perform and how to perform it. 
Limiting players freedom to move and perform gestures enabledusto predict whena gesturewastobe performedand 
subsequently determineifitwas recognized correctly. For thegame, we use the gestures shownin Figure 27. 
In-game gestures are performedby holdinga NintendoWiimotein different starting orientations and makinga 
motionin the air. Some gestureshavea clear mapping to sword manipulation. These include Parry , Slash 
, Slice , Stab , Zorro , and Chop . Additionally, gestures with simple .icking motions were adapted to 
vertical and horizontal sword cuts ( Left , Right , Forward , and Stop ). Gestures depicting lines (Line 
to Left , Right, Up, Down ) are very easy to perform and were adapted to allowthe player to teleport 
between quests. The Square gesture was mapped into drawing a virtual window in the world, where players 
could see the quest information. All remaining gestures were mapped to spells. At each point where players 
were expected to perform a gesture, we limited the player to a maximum of 5mistakes. This feature was 
introduced after a pilot run of the experiment, during which one person was unable to proceed in thegame 
due to poor recognition accuracy. Also, before each quest in thegame, players were requiredto summona 
quest descriptionby useofthe Square gesture. This screen displayed the storylineforthe questand describedthe 
quest tasksin termsof gesturestobe performedbythe player. The quest description screen also served as 
a practice area where players could recall and practice the appropriategesturesbeforestartingthequest.Tomakeiteasierto 
remember gestures,nomorethanseven gestures were required for each quest. 6.9.1 Usability Study Subjects 
and Apparatus. Atotal of 25 participants (23 male and2female) were recruited from the University of 
Central Florida for participation in the investigation. Participants ages were between 18 and 28 years. 
22 participants were right-handed while three were left-handed. Each participant took 60-90 minutes to 
complete the entire experiment and was paid $10 for his/her time. Theexperimentwas conductedonacomputerequippedwithanIntel 
Core-i7-920 processor,andannVidia GeForce 460 graphics adapter. For display, we used a 50in Samsung DLP 
3D HDTV with the resolution set to 1680x1050 pixels and a refresh rate of 60Hz. The participants stood 
approximately 4-6 ft from the display and performed gestures while they were standing. Experiment Procedure. 
The experiment was split up into two parts. Participants .rst performed each gesture 25 times, a random 
15 of which were used for training the in-game recognizer in each session. Topreventfatigue duringthe 
data collection phase, participants wereaskedtotake frequent breaks.Participants could delete gesture 
data if theydiscovered theyhad done gestures incorrectly. Additionally, a proctor monitored participants 
at all times and pointed out mistakes, if any. The data collection session lasted 30-40 minutes. Duringthe 
secondpartoftheexperiment, participants were requiredtoplayWizardofWii.Twogameplay session were required 
from each participant. Each session lasted approximately 10-15 minutes with a few minutes break between 
sessions. After each session, participants were asked to .ll out a questionnaire. After the .rstgameplay 
session, the questionnaire includedafewextra questions becauseit asked participants to compare their 
performance between sessions and also included a few free-response questions seeking suggestions from 
participants. Gameplay Metrics. Fivemetrics were analyzedineachgameplaysession. Actual recognition accuracyand 
the number of recognition errors were automatically logged during each player s session. Arecognition 
error denotes an instance where the expected gesture does not match the recognized gesture. This occurs 
in two ways. Either a player performed a gesture incorrectly or the recognizer failed to recognize a 
correctly performed gesture. Given our experimental setup, it is impossible to distinguish the two cases. 
We attempted to minimize the .rst possibility by using visual cues to aid gesture recall and by limiting 
the number of gestures in each quest, but it may not have eliminated all cases of player error due to 
incorrect recall. Actual recognition accuracyineach roundwas computedasthe ratioof gestures correctly 
recognized in the .rst attempt to total gestures performed. Three other metrics were collected via player 
responses ingameplay questionnaires: perceived recognition accuracy, perceivedgameplay performance andabilitytorecall 
gestures.A7-pointLikertscalewasusedforthese metrics. Figure 33: FrequencyTable showinghowthe players 
perceptionofrecognition accuracychangedbetween the two gameplay sessions. 6.9.2 Results We performedpairwiseWilcoxonSignedRanktestsongameplay 
metrics,inordertoseeifthechangein each metric sdistributionwas signi.cant betweengameplay sessions. In 
the .rst session, mean recognition accuracyacross all playerswas 69.33%,which improved to79.2% in the 
second session(z = -3.664,p < 0.01). The mean number of recognition errors per player decreased from 
19.8 to 14.24 between the two gameplay sessions(z = -2.649,p < 0.01). Perception of Recognition Accuracy.Figure 
33 shows the change in players perception of gesture recognition accuracybetweengameplay sessions. The 
mean perceived recognition accuracychanged only slightly from 5.64 to 5.88. Although the increase is 
not statistically signi.cant(z = -1.255,p =0.210), Figure 33 indicates an increase in the number of players 
reporting higher perceived accuracyin the second session. It is interesting to note that although actual 
recognition accuracyincreased signi.cantly between sessions, the increase in mean perceived accuracy 
was not signi.cant, indicating that most players perception of recognition accuracy varied little from 
their initial impressions formulated during the .rst session. Perception of GameplayPerformance. The 
mean perceivedgameplay performance increased from 5.20 to 5.92 between sessions(z = -2.578,p < 0.05). 
A deeper look at the decrease in recognition errors also revealed that players needed fewer attempts 
in the second session before a gesture was recognized correctly.Thismayexplainwhyplayersperceivedbettergameplay 
performanceduringthesecondsession, even though they were not able to perceive a signi.cant increase in 
recognition accuracy. Amajority of players reported that theirgameplay performance had improved between 
sessions. When asked aboutthe impactof recognition accuracy ongameplay performance, responsesvaried from 
player to player. A common complaintwasthe frustration players felt whenthe recognizerfailedto correctly 
recognize gestures, resulting in distraction and decreased immersion for a small number of players. Some 
reported that good recognition accuracyincreased their immersionin thegame and theyactually felt asif 
theywerewieldingaswordorawand. Interestingly,afewplayersreportedthatthe accuracyof recognition did not 
impact theirgameplay performance at all. Perception of Gesture Recall. There was a signi.cant increase 
in player s perception of their ability to recall gestures in the second session. The mean response changed 
from 5.52 to 6 between sessions(z = -2.178,p < 0.05). This .nding is in agreement with earlier results 
that indicate that gesture recognition accuracyimproved between sessions while both the number of recognition 
errors and attempts for correct recognition decreased. GesturePreferences. After each session, players 
were asked which gestures were easy or dif.cult to recall, and which gestures were suitably adaptedtothegameenvironment. 
Analysisofthe responses indicatesa fewcommon themes. Most participants felt comfortable using simple 
and smooth motions, e.g. wrist .icks ( Left , Right , Down , Stop ) and lines ( LineTo Left, Right, Up, 
Down ). Some participants disliked gestures describing geometric patterns or shapes ( Circle , Open Door 
, Lasso , Twister , Triangle , Square and Figure 8 ). Several players reported confusion and frustration 
with gestures involving similar motions. Examples of such gestures include Lasso , Circle , Open Door 
, Twister , all involving one or more circles. Other gestures such as Triangle , Square or Figure 8 were 
disliked by some participants on the grounds that geometric patterns didn t suit the fantasy setting 
of Wizard of Wii and therefore detracted from the experience. Themajorityof participantsreportedtheywouldliketosee3Dgesturesusedinvideogamesif 
recognition errors couldbe eliminated. When asked whichgames could bene.t fromhaving3D gestures, mostof 
the participants responded with similar answers: First Person Games, Adventure Games, Role Playing Games, 
Fighting Games, and Sports Games. 6.9.3 Discussion We were able to achieve a maximum recognition accuracyof79.2%, 
whichisfar belowtheexpected accuracyof over 95% shownin Section 6.7, whichwas achievedover gestures 
collectedina datagathering environment. The decreasein recognition accuracy canbe attributedto stress 
inducedbythegameenvironment, which may impair gesture recall and also introduce more variation in the 
gestures themselves. Our results indicate that players were able to better recall gestures with repeated 
play and also perceived an improvementin theirgameplay performance.With repeated play, players gestures 
were correctly recognizedinfewer attempts. Thismayhave contributedtoa perceptionof highergameplay performance 
in the second session. Gesture recognition accuracyimproved signi.cantly betweengameplay sessions, possibly 
due to improved gesture recall. Interestingly, Players were unable to perceive the increase in recognition 
accuracy,indicating that .rst impressions were fundamental to their perception of recognition accuracy.Thisisanimportant.ndingfromagamedesigner 
sperspective becauseit seemstoimplythat games using 3D spatial input should attempt to make a good initial 
impression in terms of recognition accuracy.  7 Future 2: Mix and MatchSpatial Input Oneofthekeythemes 
thatwe believe will emergeinthe futureof spatial interfacesisa mixingof many of the techniques, technologies, 
and applications that we have discussed in this course. Haptics might be combined with low-cost depth 
cameras, tangible props might be combined with multi-touch input, scienti.c applications might be combined 
with art andgames. In this chapter of the course notes, we highlight some recent research activities 
that suggest these trends and discuss some of the open research questions in supporting mix and match 
spatial input. 7.1 Spatial input meets multi-touch Figure 34: The Slice WIM interface mixes 2D and 3D 
hardware and interaction techniques. The vertical screen displays a stereoscopic view, and using the 
stereo projection, the small world-in-miniature (WIM) model is positioned so that it appears to .oat 
in the air above the table (digitally superimposed on the photograph here to demonstrate the effect). 
The table is a multi-touchdisplay surface. The user performs actionsto manipulatethe3D scenebytouchingtheshadowoftheWIMonthetable.Inthisway,multiple 
simultaneous 2D inputs on the table surface are used to control a variety of 3D visualization techniques, 
including navigation and selection within the volume data. Earlierinthe noteswe discussed SliceWIM,anextensiontoWIM 
interfacesto supportnavigating through volumetric data. Figure 34 illustrates the interface for this 
WIM technique, which combines multi-touch input with head-tracked stereoscopic projection. The interface 
is based on the metaphor of interacting with the shadow of a 3D object that .oats in the air above a 
table. Multi-touch gestures made on the table are used to manipulate the shadow, which in turn causes 
the 3D object to update (translate, rotate, etc.) to maintain a consistent connection with the shadow. 
A series of widgets on the table are used to control slicing through the data (e.g., setting vertical 
cutting planes), which is an important technique for visualizing dense volume data. All of the interactions 
are 3D actions in the sense that the user s view is stereoscopicandtheentireenvironmentis3D,butinthiscase,theinputall 
occursonaplane,thus,itcan be viewed asa2D input albeitavery rich2D input since multiple simultaneous 
touches are recordedby the table device. Figure 35: Currently, custom hardware con.gurations are often 
required to perform the type of mix and match spatial input that we believe will become commonplace in 
the future. This left side of the .gure describes the hardware con.guration utilized for the Slice WIM 
interface shown in the previous .gure: The rear-projected image on the vertical screen originates from 
a 3D projector (1) mounted behind the screen. The horizontal table s image originates from a projector 
(2) mounted under the table and must be bounced off a front surface mirror (3) to reach the surface. 
An IR camera (4) is mounted on top of thetable sprojectorandisusedtotrackpointsof contactonthetable.AsetofNaturalPointOptiTrack 
cameras (5) are used to provide headtracking data to update the stereoscopic display. The right side 
of the .gure provides some additional system information, including the use of VRPN and TUIO protocols 
for handling data from the various devices. In mixing 2D and 3D and multi-touch on a surface with stereoscopic 
imagery, this system is an example of the type of mixing of interface and graphics technologies that 
we believe will become increasingly common in the future. Today, these systems must typically be constructed 
via custom hardware con.gurations,suchastheone diagrammedinFigure35.Infactthisdisplaycanbebuiltfor relativelylowcost, 
at least given what we have become used to in the virtual reality community. The following describes 
some noteson materialsand constructiontobuilda similar FTIR multi-touch tableand stereoscopicwall display 
(adapted from [Coffeyet al. 2010]). 7.1.1 Building the Minnesota Interactive Visualization Lab3D Multi-touchWorkbench 
Parts Listing: Table hardware, including projector  Viewsonic PJD5351 projector  Unibrain Fire-i BW 
board camera with 107. horizontal FOVlens  Acrylic sheet (4 x 3 x 3/8 )  Reel with5 m adhesive-backed 
ribbonof surface-mounted 850 nmIR LEDs  Power supply for LEDs  Rubber spacers for LEDs (4 setting block, 
cut as necessary)  Aluminum channel (2 pieces 3/8 wide, 3/4 deep, 98 long each)  Drafting .lm (two 
3 x 4 sheets)  Clear silicone sealant and Xylene or comparable solvent  First surface mirror (408 x 
608 mm, 4-6 wave)  Schott RG830 .lter (1 diameter)  Adjustable table legs   Stereoscopic wall hardware 
 DepthQ DQ-WXGA stereoscopic projector  Synchronization device and shutter glasses  Rosco Grey(9 x 
4.5 sheet)  Wood beams for screen frame  Bungee cord (1/8 diameter) and grommets (1/2 inner diameter) 
  Computer workstations/servers  Community CoreVision and OptiTrack servers (Anydesktop machine will 
suf.ce)  3D application workstation (with stereo capable graphics card)   Head-tracking system  Optitrack 
camera server (anydesktop machine will suf.ce)  Optitrack NaturalPoint6 camera system   Design and 
construction notes: The NUI Group forums (e.g., [Tinkerman 2008]) are an excellent source of information 
for developing these types of systems, and provided much guidance in designing the multi-touch system 
documented here. Some speci.c construction notes adapted from the forum, include: (1) It is strongly 
advised to prototype the placement of the interactive surface projector, the camera, and the mirror using 
paper triangles (cut out to scale) that closely estimate beam shapes and focal ranges. The paper triangles 
can be folded to the required mirror size and angle for the projector beam. (2) Acrylic sheet edges may 
need to be polished. (3)AmetalframecanbeusedtohousetheIRLEDsusedfortheFTIR multi-touchtable.TheLED smust 
beplacedinsidethe channelformedbythisframe;usingastripof pre-mountedLEDsgreatlyfacilitates assembly.(4)Toprovidean 
opaque projectionsurfaceontopofthetable,apieceof laminated drafting .lm is placed on top of the table 
surface. To improve the capture of touch events through this medium, a medium thickness mixture of clear 
silicone sealant and xylene is painted onto the bottom side of the laminated drafting .lm in two coats. 
The NUI group forum refers to this as the compliant surface, and provides several reviews of different 
options for creating an effective touch and display surface. Today, we believe the design discussed here 
is one of the best options for mixing touch input with 3D stereoscopic displays; however, we expect this 
will continue to change in the future. As 3D TV displays continue to improve, it s likely that one or 
both of the projectors used in the system described here may be abletobe replacedbyacommodityTV.The currentdesigndoesnot 
utilizea3DTV becausewefoundthat when stereoscopic images were presented so that the virtual objects appearedfar 
in front of the display, the ghosting on the TV displays was too distracting, ruining the stereo effect. 
 7.1.2 Future Outlook As 3D input and graphics technologies become increasingly available to the public 
at reasonable cost, we believe con.gurations such as this one, which combine touch with 3D or other technologies 
will become increasingly common,and perhaps holdakey to enabling emerging technologiestobe usedeffectively 
in applications to science, art, design, medicine, and other areas to complement increasingly compelling 
games and entertainment.  7.2 Tangible spatial input Today, 3D interfaces that include a sense of touch 
tend to be limited to full active haptic simulations (e.g., utilizing devices similar to the SensAble 
PHANTOM) or passive interfaces, such as interacting with a tablet prop within a virtual environment. 
In the future, we believe a number of other hybrid styles of tangible spatial interaction will emerge. 
Already today, we see multi-touch interfaces being implemented on cubes and curved 3D surfaces, and there 
appear to be manynewopportunities on the horizon for mixing spatial input with touch and other tangible 
styles of interaction. One area where we see some early work in this direction is in interacting with 
physical rapid prototype models. For example, Kruszyski et al. [Kruszynski and van Liere 2009] developed 
a smartly calibrated pen interface to interact with high-resolution 3D rapid prototypes of sea corals. 
This work extended early work in prop-based interaction (e.g., [Hinckleyet al. 1997]) to demonstrate 
the current potential impact on scienti.c work.ows of designing 3D interfaces for interacting with very 
accurate rapid prototype 3D printouts. Other researchers have continued in this direction, investigating 
how 3D gestures made relative to accurate, scienti.cally relevant3D printouts canbe usedaseffective visualization 
interfaces[Konchada et al. 2011; Jackson andKeefe 2011]. Building on the idea of interacting in the 3D 
space surrounding an object or a display, the style of tangible 3D interactions that we might come to 
expect in the future may be closely related to research results, such as the work by Grossman et al., 
a 3D gestural interface for bimanual interaction with emerging true 3D display technologies that render 
a true 3D light model inside a hemispheric dome [Grossman et al. 2004]. Emerging opticaldevices,suchasthe 
kinect, enableavarietyof freehand3D interactions,butweknow from years on research in the 3D user interaction 
community that this type of freehand unconstrained interaction can often be very dif.cult to control, 
much more so than techniques guided by even passive haptic feedback.Weexpectthatacritical challengeforthe 
futurewill thereforebeto determinehowbestto leverage the dramatic newcapabilities for 3D input that we 
havewith the advent of commercially-available devices, such as the kinect, while combining these capabilities 
with the advantages of tangible spatial input. In the research community, we have seen manysuccessful 
examples of physical input devices; the cubic mouse[Frohlichand Plate2000]forexample.How canwe combinethe 
successof these tangible spatial input devices with the exciting emerging trends in commercially available 
hardware, and how will this impact the many new applications that are becoming possible? We believe these 
arekeyquestions that will de.ne the future of spatial interaction. 7.3 Cognitive and perceptual issues 
The .nal emerging theme for future investigations of spatial interfaces that we will highlight here is 
the importance of taking cognitive and perceptual issues into account during interface design. In emerging 
3D environments, there is great potential for illusion, and manyresearchers have found that this can 
be both good and bad. Some of the most compelling examples of how perceptual illusion can be used to 
advantage include work in using redirected walking [Razzaque 2005] and portals [Steinicke et al. 2009] 
to move through virtual environments. Studiesof touch-based stereoscopic3Denvironmentshaverecentlyexploredthe 
perceptual issueof touching objects that exist the the space in front of or behind a surface display 
[Valkov et al. 2011]. Similarly, the impact of the realism of the shadow widgets used in the interface 
in Figure 34 has been explored and unrealistic shadows have been shown to often be advantageous [Coffeyet 
al. 2011]. These are just a few examples of how human perception can impact user interface design. As 
interfaces move increasingly in the direction of mixing 2D and 3D inputs, working in tandem with 3D displays, 
and mixing tangible and freehand modes of action, perceptual and cognitive issues are sure to become 
increasingly importantfactorsto considerin interface design. Theexciting possibilityis that, as some 
preliminary work has already demonstrated, it is often the case that perceptual illusion can actually 
be used to advantage in designing effective spatial user interfaces.  Acknowledgements AspecialthankstoDougBowman,ErnstKruijff,IvanPoupyrev,ChadWingrave, 
RichardMarks,Salman Cheema, Kris Rivera, and members of the InteractiveSystems and User Experience Lab 
for assisting in the development of the material for this course. Also thanks to co-authors, collaborators, 
and students whose work in scienti.c visualization, art, and 3D user interfaces features prominently 
in many examples within the course notes: Dane Coffey, Nick Malbraaten, FedorKorsakov, David Laidlaw, 
Fritz Drury, Robert Zeleznik, AndyForsberg, Jason Sobel, and the research groupsin the Universityof Minnesota 
Interactive Visualization Lab and the Brown UniversityVisualization Research Lab. The preparation of 
this course was supported in part by the NSF (CAREER award, IIS-1054783). References AKERS,D. 2006. 
CINCH:A cooperatively designed marking interface for3D pathway selection. In Symposium on User Interface 
Software andTechnology, 33 42. ANDERSON,L.,ESSER,J., AND INTERRANTE,V. 2003.Avirtual environment for 
conceptual design in architecture. In Proceedings of the workshop onVirtual environments 2003,ACM, NewYork, 
NY, USA, EGVE 03, 57 63. AZUMA,R., AND BISHOP,G. 1994. Improving static and dynamic registrationin an 
optical see-through hmd. In SIGGRAPH 94: Proceedings of the 21st annual conference on Computer graphics 
and interactive techniques,ACM,NewYork,NY, USA, 197 204. BOTT, J., CROWLEY, J., AND LAVIOLA, J. 2009. 
Exploring 3d gestural interfaces for music creation in videogames. In Proceedings of TheFourth International 
Conference on theFoundations of Digital Games 2009, 18 25. BOWMAN,D.A., AND HODGES,L.F. 1997.Anevaluationof 
techniquesfor grabbingand manipulating remote objects in immersive virtual environments. In SI3D 97: 
Proceedings of the 1997 symposium on Interactive 3D graphics,ACM,NewYork,NY, USA, 35 ff. BOWMAN,D.A.,KOLLER,D., 
AND HODGES,L.F. 1997.Travelin immersive virtualenvironments: An evaluation of viewpoint motion control 
techniques. In Proceedings of theVirtual Reality Annual International Symposium, 45 52. BOWMAN,D.A.,WINEMAN,J.,HODGES,L.F., 
AND ALLISON,D. 1998. Designing animal habitats within an immersive ve. IEEE Comput. Graph. Appl. 18, 
5, 9 13. BOWMAN,D.A.,KRUIJFF,E.,LAVIOLA,J.J., AND POUPYREV,I. 2004. 3D User Interfaces: Theory and Practice. 
AddisonWesleyLongman Publishing Co., Inc., Redwood City, CA, USA. BRODY, B., AND HARTMAN, C. 2000. Painting 
space with BLUI. In SIGGRAPH 00 Conference Abstracts and Applications, 242. BROOKS,F.P. 1988. Grasping 
reality through illusion: interactive graphics serving science. In Proceedings of the SIGCHI conference 
on Human factors in computing systems,ACM, NewYork, NY, USA, CHI 88, 1 11. BROWNUNIVERSITYLITERARYARTSPROGRAM,2011. 
http://www.brown.edu/departments/literary arts. BRYSON,S., AND LEVIT,C. 1991. The virtual windtunnel: 
an environment for theexplorationof threedimensional unsteady .ows. In Proceedingsofthe2nd conferenceonVisualization 
91,IEEE Computer Society Press, Los Alamitos, CA, USA, VIS 91, 17 24. BUTTERWORTH,J.,DAVIDSON,A.,HENCH,S., 
AND OLANO,M.T. 1992. 3DM:Athree dimensional modeler using a head-mounted display. In Symposium on Interactive 
3D Graphics, 135 138. CHARBONNEAU, E., MILLER, A., WINGRAVE, C., AND LAVIOLA, JR., J. J. 2009. Understanding 
visual interfacesforthenext generationof dance-basedrhythm videogames.In Sandbox 09: Proceedingsof the 
2009ACM SIGGRAPH Symposium onVideo Games,ACM Press, 119 126. CLARK,J.H. 1976. Designing surfacesin 3-D. 
Communicationsof theACM19, 8, 454 460. COFFEY, D., KORSAKOV, F., AND KEEFE, D. F. 2010. Low cost VR meets 
low cost multi-touch. In Proceedingsof International Symposium onVisual Computing, 351 360. COFFEY, D., 
MALBRAATEN, N., LE, T., BORAZJANI, I., SOTIROPOULOS, F., AND KEEFE, D. F. 2011. Slice WIM: a Multi-Surface, 
Multi-Touch interface for Overview+Detail exploration of volume datasets in virtual reality. In ProceedingsofACM 
SIGGRAPH Symposiumon Interactive3DGraphics and Games, 191 198. CONNER, B. D., SNIBBE, S. S., HERNDON, 
K. P., ROBBINS, D. C., ZELEZNIK, R. C., AND VAN DAM, A. 1992. Three-dimensional widgets. In SI3D 92: 
Proceedings of the 1992 symposium on Interactive 3D graphics,ACM,NewYork,NY, USA, 183 188. CRASSIDIS, 
J. L., AND MARKLEY, F. L. 2003. Unscented .ltering for spacecraft attitude estimation. Journal of Guidance, 
Control, and Dynamics 26, 4, 536 542. DEERING,M. 1992.High resolution virtual reality.InSIGGRAPH 92 Conference 
on Computer Graphics and InteractiveTechniques, 195 202. DEERING, M. F. 1995. Holosketch: A virtual reality 
sketching/animation tool. ACMTransactions of Computer-Human Interaction2, 3, 220 238. DUDA,R.O.,HART,P.E., 
AND STORK,D.G. 2001. Pattern Classi.cation. JohnWileyand Sons. FEINER, S., MACINTYRE, B., HAUPT, M., 
AND SOLOMON, E. 1993. Windows on the world: 2d windows for 3d augmented reality. In UIST 93: Proceedings 
of the 6th annualACM symposium on User interface software and technology,ACM,NewYork,NY, USA, 145 155. 
FERGUSON,M., 2003. June. Film created with the SANDDE animation system. National Film Boardof Canada 
http://www.nfb.ca. FERGUSON,M., 2006.Fallinginloveagain. Film created withthe SANDDE animation system. 
National Film Board of Canada http://www.nfb.ca. FRIEDMAN,J.H. 1996. Another approachto polychotomous 
classi.cation. Tech. rep., Departmentof Statistics, Stanford University. FRThe cubic mouse: a new device 
for three-dimensional input. In OHLICH, B., AND PLATE, J. 2000. Proceedings of the SIGCHI conference 
on Human factors in computing systems,ACM, NewYork, NY, USA, CHI 00, 526 531. GALYEAN,T.A., AND HUGHES,J.F. 
1991. Sculpting:An interactivevolumetric modeling technique. In ComputerGraphics(ProceedingsofACM SIGGRAPH91),vol. 
25,ACM, 267 274. GIANSANTI,D.,MACELLARI,V., AND MACCIONI,G. 2003.Isit feasibleto reconstructbodysegment 
3-d position and orientation using accelerometric data. IEEETrans. Biomed.Eng50, 2003. GREGORY,A.D.,EHMANN,S.A., 
AND LIN,M.C. 2000. inTouch: Interactive multiresolution modeling and 3D painting with a haptic interface. 
In Proceedings of IEEE VR 2000, 45 52. GREY,J. 2002. Human-computer interactioninlifedrawing,a.ne artist 
s perspective.In Proceedings of the Sixth International Conference on InformationVisualisation. GROSSMAN, 
T., WIGDOR, D., AND BALAKRISHNAN, R. 2004. Multi-.nger gestural interaction with 3d volumetric displays. 
In Proceedingsof the 17th annualACM symposium on User interface software and technology,ACM,NewYork,NY, 
USA, UIST 04, 61 70. GUNTURK,B.K.,GLOTZBACH,J.,ALTUNBASAK,Y., AND SCHAFER,R.W. 2005. Demosaicking: color 
.lter array interpolation. IEEE Signal processing magazine 22, 44 54. HENTSCHEL, B., TEDJO, I., PROBST, 
M., WOLTER, M., BEHR, M., BISCHOF, C., AND KUHLEN, T. 2008. Interactive blood damage analysis for ventricular 
assist devices. IEEE Transactions on Visualization and Computer Graphics 14, 6, 1515 1522. HINCKLEY,K.,PAUSCH,R.,DOWNS,J.H.,PROFFITT,D., 
AND KASSELL,N.F. 1997. The propsbased interface for neurosurgical visualization. Studiesin HealthTechnology 
and Informatics39, 552 562. PMID: 10168950. HOFFMAN,M.,VARCHOLIK,P., AND LAVIOLA,J. 2010. Breakingthe 
statusquo: Improving3d gesture recognition with spatially convenient input devices. In IEEEVirtual Reality 
2010, IEEE Press, 59 66. HUA, J., AND QIN, H. 2004. Haptics-based dynamic implicit solid modeling. IEEETransactions 
on Visualization and Computer Graphics 10, 5, 574 586. JACKSON,B., AND KEEFE,D.F. 2011.Sketchingover 
props: Understandingand interpreting3dsketch input relative to rapid prototype props. In Proceedingsof 
IUI 2011SketchRecognitionWorkshop. JULIER,S., AND UHLMANN,J. 1997.A newextensionofthe kalman .lterto 
nonlinear systems.In Int. Symp. Aerospace/Defense Sensing, Simul. and Controls. KEEFE, D. F., FELIZ, 
D. A., MOSCOVICH, T., LAIDLAW, D. H., AND LAVIOLA JR., J. J. 2001. CavePainting:Afully immersive3D artistic 
mediumand interactiveexperience.In Proceedings of I3D 2001, 85 93. KEEFE, D. F., ZELEZNIK, R. C., AND 
LAIDLAW, D. H. 2007. Drawing on air: Input techniques for controlled 3D line illustration. IEEETransactions 
onVisualization and Computer Graphics 13, 5, 1067 1081. KEEFE, D. F., ACEVEDO, D., MILES, J., DRURY, 
F., SWARTZ, S. M., AND LAIDLAW, D. H. 2008. Scienti.c sketching for collaborative VR visualization design. 
IEEETransactions onVisualization and Computer Graphics 14, 4, 835 847. KEEFE,D.F.,ZELEZNIK,R.C., AND 
LAIDLAW,D.H. 2008.Tech-note: Dynamic dragging for input of 3D trajectories. In Proceedings of IEEE Symposium 
on 3D User Interfaces 2008, 51 54. KEEFE,D.F. 2007. Interactive3DDrawing forFree-Form Modelingin Scienti.cVisualization 
and Art: Tools, Methodologies, and TheoreticalFoundations. PhD thesis, Brown University. KEEFE,D.F. 2008. 
Free-form vr interactionsin scienti.c visualization. KEEFE, D. F., 2009. Creative 3d form-making in visual 
art and visual design for science. CHI 2009 Workshop on Computational Creativity Support: Using Algorithms 
and Machine Learning to Help People Be More Creative, April. KEEFE, D. F. 2010. Integrating visualization 
and interaction research to improve scienti.c work.ows. IEEE Computer Graphics and Applications 30, 2, 
8 13. KEEFE,D.F. 2011,In press. From gestureto form: Theevolutionofexpressive freehand spatial interfaces. 
Leonardo. KITWARE, INC. Paraview: Open source scienti.c visualization. http://www.paraview.org/. Accessed 
May, 2011. KONCHADA,V.,JACKSON,B.,LE,T.,BORAZJANI,I.,SOTIROPOULOS,F., AND KEEFE,D.F. 2011. Supporting 
internal visualization of biomedical datasets via 3d rapid prototypes and sketch-based gestures. In 
PosterProceedingsofACM Symposium on Interactive3DGraphics and Games. KONIECZNY,J.,MEYER,G.,SHIMIZU,C.,HECKMAN,J.,MANYEN,M., 
AND RABENS,M. 2008. Vr spray painting for training and design. In Proceedingsofthe 2008ACM symposiumonVirtualreality 
software and technology,ACM,NewYork,NY, USA, VRST 08, 293 294. KRATZ,L.,SMITH,M., AND LEE,F.J. 2007.Wiizards:3d 
gesture recognitionforgameplay input. In Future Play 07: Proceedings of the 2007 conference on Future 
Play,ACM, NewYork, NY, USA, 209 212. KRUSZYNSKI, K., AND VAN LIERE, R. 2009. Tangible props for scienti.c 
visualization: concept, requirements, application. Virtual Reality 13, 235 244. 10.1007/s10055-009-0126-1. 
LAVIOLA,J.J., AND ZELEZNIK,R.C. 2007.Apractical approach for writer-dependent symbol recognition usingawriter-independent 
symbol recognizer. IEEETransactions onPattern Analysis and Machine Intelligence 29, 11, 1917 1926. LAVIOLA,J. 
2000. Msvt:Avirtual reality-based multimodal scienti.c visualization tool. InProceedings of the ThirdIASTED 
International Conference on Computer Graphics and Imaging, 1 7. LAVIOLA, J. J. 2003. Double exponential 
smoothing: an alternative to kalman .lter-based predictive tracking. In EGVE 03:Proceedingsofthe workshoponVirtualenvironments2003,ACM, 
NewYork, NY, USA, 199 206. LUINGE, H., VELTINK, P., AND BATEN, C. 1999. Estimating orientation with gyroscopes 
and accelerometers. Technology and Health Care7, 6, 455 459. MWorking 3D meshes and particles with 
.nger tips, towards an immersive artists AKELA, W. 2005. interface. In IEEEVR 2005WorkshopProceedings, 
77 80. MAPES,D., AND MOSHELL,M. 1995.Atwo-handed interfacefor object manipulationin virtualenvironments. 
Presence:Teleoper.Virtual Environ.4, 4, 403 416. MINE, M. R., BROOKS, JR., F. P., AND SEQUIN, C. H. 1997. 
Moving objects in space: exploiting proprioception in virtual-environment interaction. In SIGGRAPH 97: 
Proceedings of the 24th annual conference on Computer graphics and interactive techniques,ACM Press/Addison-WesleyPublishing 
Co.,NewYork,NY, USA, 19 26. MINE,M. 1995.Virtual environment interaction techniques.Tech. rep., UNC Chapel 
HillCS Dept. MORSTAD,P.,2004. Moon man. Film created with the SANDDE animation system. National Film 
Board of Canada http://www.nfb.ca. PAUSCH,R.,BURNETTE,T.,BROCKWAY,D., AND WEIBLEN,M.E. 1995.Navigationand 
locomotion in virtual worlds via .ight into hand-held miniatures. In SIGGRAPH 95: Proceedings of the 
22nd annual conference on Computer graphics and interactive techniques,ACM,NewYork,NY,USA, 399 400. PAUSCH,R.,BURNETTE,T.,BROCKWAY,D., 
AND WEIBLEN,M.E. 1995.Navigationand locomotion in virtual worlds via .ight into hand-held miniatures. 
In Proceedings of the 22nd annual conference on Computer graphics and interactive techniques,ACM, 399 
400. PIERCE, J. S., FORSBERG, A. S., CONWAY, M. J., HONG, S., ZELEZNIK, R. C., AND MINE, M.R. 1997. Image 
plane interaction techniques in 3d immersive environments. In SI3D 97: Proceedings of the 1997 symposium 
on Interactive 3D graphics,ACM,NewYork,NY, USA, 39 ff. POUPYREV,I.,BILLINGHURST,M.,WEGHORST,S., AND ICHIKAWA,T. 
1996. The go-go interaction technique: non-linear mapping for direct manipulation in vr. In UIST 96: 
Proceedings of the 9th annual ACM symposium on User interface software and technology,ACM,NewYork,NY, 
USA, 79 80. RAZZAQUE,S. 2005. Redirected walking. PhD thesis, Chapel Hill, NC, USA. AAI3190299. REHM,M.,BEE,N., 
AND ANDRE,E. 2008.Wave like anegyptian: accelerometer based gesture recognition for culture speci.c 
interactions. In BCS-HCI 08: Proceedings of the 22nd British HCI Group Annual Conference on HCI 2008, 
British Computer Society, Swinton, UK, UK, 13 22. RUBIN, C. B., AND KEEFE, D. F., 2002. Hiding spaces: 
A cave of elusive immateriality. ACM SIG-GRAPH 2002 Conference Abstracts and Applications, July. RUBINE,D. 
1991. Specifying gesturesbyexample. In SIGGRAPH 91: Proceedings of the 18th annual conference on Computer 
graphics and interactive techniques,ACM,NewYork,NY, USA, 329 337. SACHS, E., ROBERTS, A., AND STOOPS, 
D. 1991. 3-draw: A tool for designing 3D shapes. IEEE Computer Graphics and Applications 11, 6, 18 26. 
SCHAPIRE,R.E. 1999.Abrief introductionto boosting.InIJCAI 99: Proceedings of the Sixteenth InternationalJoint 
Conference on Arti.cial Intelligence, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1401 1406. 
SCHKOLNE, S., PRUETT, M., AND SCHR ODER,P. 2001. Surface drawing: creating organic 3D shapes with the 
hand and tangible tools. In Proceedings of CHI 01, 261 268. SCHKOLNE, S., ISHII, H., AND SCHRImmersive 
design of DNA molecules with a ODER, P. 2004. tangible interface. In Proceedingsof IEEEVisualization, 
227 234. SCHKOLNE, S. 2003. 3D Interfaces for Spatial Construction. PhD thesis, California Institute 
ofTechnology. controller. In TEI 08: Proceedings of the 2nd international conference onTangible and 
embedded interaction,ACM,NewYork,NY, USA, 11 14. SCHLOMER, T., POPPINGA, B., HENZE, N., AND BOLL, S. 
2008. Gesture recognition with a wii SCHMANDT, C. 1983. Spatial input/display correspondence in a stereoscopic 
computer graphic work station. In SIGGRAPH 83: Proceedings of the 10th Annual Conference on Computer 
Graphics and InteractiveTechniques, 253 261. SCHROERING,M.,GRIMM,C., AND PLESS,R. 2003.A newinputdevicefor3Dsketching.In 
Vision Interface, 311 318. SENSABLE TECHNOLOGIES, INC. 2005. FreeForm Concept Product Brochure. SHEPPARD,R., 
AND NAHRSTEDT,K. 2009. Merging research modalities: TED (tele-immersive dance) collaboration offers a 
model for performance-based research and creative development. In Proceedings ofthe ComputationalCreativity 
SupportWorkshopatACMCHI2009. SHIRAI,A.,GESLIN,E., AND RICHIR,S. 2007.Wiimedia: motion analysis methods 
and applications usinga consumer videogame controller. In Sandbox 07: Proceedings of the 2007ACM SIGGRAPH 
symposium onVideo games,ACM,NewYork,NY, USA, 133 140. SHIRATORI, T., AND HODGINS, J. K. 2008. Accelerometer-based 
user interfaces for the control of a physically simulated character. ACMTrans. Graph. 27, 5, 1 9. SHIVARAM, 
G., AND SEETHARAMAN, G. 1998. A new technique for .nding the optical center of cameras. In ICIP 98: Proceedings 
of 1998 International Conference on Image Processing, vol. 2, 167 171. SHOTTON, J., FIZGIBBON, A., COOK, 
M., SHARP, T., FINOCCHIO, M., MOORE, R., KIPMAN, A., AND BLAKE,A. 2011. Real-time human pose recognitionin 
parts from single depth images. In IEEE ComputerVision andPattern Recognition (CVPR) 2011, IEEE. SNIBBE, 
S., ANDERSON, S., AND VERPLANK, B. 1998. Springs and constraints for 3D drawing. In Proceedings of the 
Third Phantom Users Group. M.I.T. Arti.cial Intelligence LaboratoryTechnical Report AITR-1643. SNIBBE, 
S., 1998. Bondary functions, installation by scott snibbe. http://www.snibbe.com/. Accessed May, 2011. 
SOBEL, J. S., FORSBERG, A. S., LAIDLAW, D. H., ZELEZNIK, R. C., KEEFE, D. F., PIVKIN, I., KARNIADAKIS, 
G. E., RICHARDSON, P., AND SWARTZ, S. 2004. Particle .urries: Synoptic 3D pulsatile .ow visualization. 
IEEE Computer Graphics and Applications 24,2(March/April), 76 85. SONG, H., GUIMBRETI`2006. Modelcraft: 
capturing freehand ERE, F., HU, C., AND LIPSON, H. annotations and edits on physical 3d models. In UIST 
06: Proceedings of the 19th annual ACM symposium on User interface software and technology,ACM,NewYork,NY, 
USA, 13 22. STEINICKE,F.,BRUDER,G.,HINRICHS,K.,STEED,A., AND GERLACH,A.L. 2009. Doesa gradual transition 
to the virtual world increase presence? In Proceedings of the 2009 IEEE Virtual Reality Conference, IEEE 
Computer Society,Washington, DC, USA, 203 210. STOAKLEY,R.,CONWAY,M.J., AND PAUSCH,R. 1995.Virtual reality 
ona wim: interactiveworldsin miniature. In CHI 95: Proceedings of the SIGCHI conference on Human factors 
in computing systems, ACM Press/Addison-WesleyPublishing Co.,NewYork,NY, USA, 265 272. STOAKLEY,R.,CONWAY,M.J., 
AND PAUSCH,R. 1995.Virtual reality ona WIM: interactiveworlds in miniature. In Proceedings of the SIGCHI 
conference on Human factors in computing systems,ACM Press/Addison-WesleyPublishing Co., Denver, Colorado, 
United States, 265 272. SUTHERLAND, I. 1968. Ahead-mounted three dimensional display. In Proceedings 
of the AFIPSFall Joint Computer Conference, vol. 33, 757 764. TINKERMAN, 2008. Tinkermans method -casting 
textured silcone, http://nuigroup.com/forums/viewthread/2383/, July. TRUEBA,R.,ANDUJAR,C., AND ARGELAGUET,F. 
2009. Complexity and occlusion management for theWorld-in-Miniature metaphor. In Smart Graphics. 155 
166. VALKOV,D.,STEINICKE,F.,BRUDER,G., AND HINRICHS,K. 2011. 2d touchingof3d stereoscopic objects. In 
Proceedings of the 2011 annual conference on Human factors in computing systems,ACM, NewYork,NY, USA, 
CHI 11, 1353 1362. VARCHOLIK, P. D., LAVIOLA, JR., J. J., AND HUGHES, C. 2009. The bespoke 3dui xna framework: 
a low-cost platform for prototyping 3d spatial interfaces in videogames. In Proceedings of the 2009 ACM 
SIGGRAPH Symposium onVideo Games,ACM,NewYork,NY, USA, Sandbox 09, 55 61. WAN, E. A., AND VAN DER MERWE, 
R. 2002. The unscented kalman .lter for nonlinear estimation. 153 158. WESCHE,G., AND SEIDEL,H.-P. 2001. 
Freedrawer: a free-formsketching system on the responsive workbench. In Proceedings of VRST 2001, 167 
174. WILLIAMSON,R., AND ANDREWS,B. 2001. Detecting absolutehumankneeangleandangularvelocity using accelerometers 
and rate gyroscopes. Medical and Biological Engineering and Computing 39, 3, 294 302. WILLIAMSON,B.,WINGRAVE,C., 
AND LAVIOLA,J. 2010. Realnav: Exploring natural user interfaces for locomotionin videogames. In Proceedings 
of IEEE Symposium on 3D User Interfaces 2010, IEEE Computer Society,3 10. WINGRAVE,C.A.,HACIAHMETOGLU,Y., 
ANDBOWMAN,D.A. 2006.Overcomingworldin miniature limitations by a scaled and scrolling WIM. In Proceedings 
of the IEEE conference onVirtual Reality, IEEE Computer Society, 11 16. WINGRAVE, C., WILLIAMSON, B., 
VARCHOLIK, P., ROSE, J., MILLER, A., CHARBONNEAU, E., BOTT, J., AND LAVIOLA, J. 2010. Wii remote and 
beyond: Using spatially convenient devices for 3duis. IEEE Computer Graphics and Applications 30, 2, 
71 85. WLOKA, M. M., AND GREENFIELD, E. 1995. The virtual tricorder: a uniform interface for virtual 
reality. In UIST 95: Proceedings of the 8th annualACM symposium on User interface and software technology,ACM,NewYork,NY, 
USA, 39 40. ZHANG, N., ZHOU, X., SHEN, Y., AND SWEET, R. 2010. Volumetric modeling in laser bph therapy 
simulation. IEEETransactions onVisualization and ComputerGraphics16 (November), 1405 1412. ZHOU,W.,CORREIA,S., 
AND LAIDLAW,D.H. 2008. Haptics-assisted3D lasso drawing for tracts-ofinterest selection in DTI visualization. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037638</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>169</pages>
		<display_no>2</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Advances in new interfaces for musical expression]]></title>
		<page_from>1</page_from>
		<page_to>169</page_to>
		<doi_number>10.1145/2037636.2037638</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037638</url>
		<abstract>
			<par><![CDATA[<p>Advances in digital audio technologies have led to computers playing a role in most music production and performance. Digital technologies offer unprecedented opportunities for creation and manipulation of sound, but the flexibilty of these new technologies provides an often confusing array of choices for composers and performers. A decade ago, the presenters of this course organized the first workshop on New Interfaces for Musical Expression (NIME), to clarify this situation by exploring connections with the better-established field of human-computer interaction.</p> <p>The course summarizes what has been learned at NIME. Topics include the theory and practice of new musicalinterface design, mapping from human action to musical output, control intimacy, tools for creating musical interfaces, sensors and microcontrollers, audio synthesis techniques, and communication protocols such as Open Sound Control (and MIDI). The course presents several case studies focused on the major broad themes of the NIME conference, including augmented and sensor-based instruments, mobile and networked music, and NIME pedagogy.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808969</person_id>
				<author_profile_id><![CDATA[81327488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808970</person_id>
				<author_profile_id><![CDATA[81100493577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lyons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>634348</ref_obj_id>
				<ref_obj_pid>634067</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Poupyrev, Lyons, Fels, & Blaine 2001} Poupyrev, I., Lyons, M., Fels, S., & Blaine, T., New interfaces for musical expression. Extended Abstracts, CHI-2001, ACM Conference on Human Factors in Computing Systems, pp. 309--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667250</ref_obj_id>
				<ref_obj_pid>1667239</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Fels & Lyons, 2009} Fels, S. and Lyons, M. 2009. Creating new interfaces for musical expression: introduction to NIME. In: ACM SIGGRAPH 2009 Courses (SIGGRAPH '09). ACM, New York, NY, USA, Article 11, 158 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Wanderley & Battier, 2000} Wanderley, M. and Battier, M. 2000. Trends in gestural control of music, IRCAM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1201683</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Miranda & Wanderley, 2006} Miranda, E. R. and Wanderley, M. M. 2006. New digital musical instruments: control and interaction beyond the keyboard. AR Editions.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1245198</ref_obj_id>
				<ref_obj_pid>1245194</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Wessel & Wright, 2002} Problems and prospects for intimate musical control of computers, Computer Music Journal 26(3): 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085156</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Wessel & Wright, 2001} Problems and prospects for intimate musical control of computers, In Proceedings of the 2001 Conference on New interfaces For Musical Expression (NIME-01) (Seattle, Washington, April 01 - 02, 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Vertegaal et al. 1996} Vertegaal, R. and Ungvary, T. and Kieslinger, M.1996. Towards a musician's cockpit: Transducers, feedback and musical function, Proceedings of the International Computer Music Conference, pp. 308--311.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Mulder, 2000} Mulder, A. 2000. Towards a choice of gestural constraints for instrumental performers. In: Trends in gestural control of music, Wanderley, M. and Battier, M., eds., IRCAM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1116035</ref_obj_id>
				<ref_obj_pid>1116026</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Camurri, 2000} Camurri, A. and Hashimoto, S. and Ricchetti, M. and Ricci, A. and Suzuki, K. and Trocca, R. and Volpe, G. 2000. EyesWeb: Toward gesture and affect recognition in interactive dance and music systems, Computer Music Journal 24(1): 57--69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Waiswicz, 1985} Waiswicz, M. 1985. THE HANDS: A Set of Remote MIDI Controllers. Proceedings of the 1985 International Computer Music Conference (ICMC'85) Vancouver Canada, pp 313--318, San Francisco: ICMA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Marshall, 2009} Marshall, M. T. 2009. Physical Interface Design for Digital Musical Instruments, Ph.D. thesis, McGill University, March. http://www.idmil.org/_media/publications/marshall_phdthesis_final.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>47678</ref_obj_id>
				<ref_obj_pid>47676</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Moore, 1988} Moore, F. R. 1988. The dysfunctions of MIDI, Computer Music Journal 12(1): 19--28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Burzik, 2002} Burzik, A. 2002. Practising in Flow---The Secret of the Masters, Stringendo 24(2): 18--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972351</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Hunt & Wanderley, 2002} Hunt, A. and Wanderley, M. M. 2002. Mapping performer parameters to synthesis engines. Organised Sound 7: 97--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Hunt, Wanderley, & Paradis, 2003} Hunt, A. and Wanderley, M. M. and Paradis, M. 2003. The importance of parameter mapping in electronic instrument design, Journal of New Music Research 32(4): 429--440.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Rovan et al., 1997} Rovan, J. B., Wanderley, M. M, Dubnov, S. and Depalle, P. 1997. Instrumental Gestural Mapping Strategies as Expressivity Determinants in Computer Music Performance. In: Kansei, The Technology of Emotion. Proceedings of the AIMI International Workshop, A. Camurri, (ed.), Genoa: Associazione di Informatica Musicale Italiana, October 3-4, pp. 68--73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972353</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Arfib et al., 2002} D. Arfib, J. M. Couturier, L. Kessous and V. Verfaille 2002. Strategies of mapping between gesture data and synthesis model parameters using perceptual spaces. Organised Sound 7: 127--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972350</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Goudeseune, 2002} Goudeseune, C. 2002. Interpolated mappings for musical instruments. Organised Sound 7: 85--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Wessel, 1979} Wessel, D. 1979. Timbre space as a musical control structure, Computer music journal 3(2): 45--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Vertegaal & Eaglestone, 1996} R. Vertegaal and B. Eaglestone 1996. Comparison of input devices in an ISEE direct timbre manipulation task, Interacting with Computers, 8(1): 13--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085742</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Lyons et al., 2003 Lyons, M. J. and Haehnel, M. and Tetsutani, N. 2003. Designing, playing, and performing with a vision-based mouth interface, Proceedings of the 2003 conference on New interfaces for musical expression pp. 116--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142241</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Steiner, 2006} H.-C. Steiner. Towards a catalog and software library of mapping methods. Proceedings of the 2006 conference on New interfaces for musical expression, pp. 106--109, Paris, France, 2006. IRCAM Centre Pompidou.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Cook, 2004} Cook, P. R. 2004. Remutualizing the musical instrument: Co-design of synthesis algorithms and controllers. Journal of New Music Research, 33(3):315--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1772494</ref_obj_id>
				<ref_obj_pid>1772490</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Bartneck & Lyons, 2007} HCI and the face: towards an art of the soluble. In J. Jacko (Ed.), <i>Human-Computer Interaction, Part 1, HCII2007, LNCS 4550</i> (pp. 20--29). Berlin: Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085885</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Fels et al. 2004} Sidney S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to Instrument. 4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1--6. May. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Wright and Freed, 1997} Wright, M. and A. Freed 1997. Open Sound Control: A New Protocol for Communicating with Sound Synthesizers. Proceedings of the International Computer Music Conference, Thessaloniki, Hellas, pp. 101--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>76932</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Mathews & Pierce, 1989} Mathews, M. & Pierce, J. (1989). <i>Current Directions in Computer Music Research</i>. The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>265600</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Steiglitz, 1996} Steiglitz, K. 1996 Digital Signal Processing Primer, New York, Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Roads, 1976} Roads, C. 1976, The Computer Music Tutorial, Cambridge, MIT Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{McAulay & Quatieri, 1986} McAulay, R. and T. Quatieri. 1986. "Speech Analysis/Synthesis Based on a Sinusoidal Representation." IEEE Trans. Acoust. Speech and Sig. Proc. ASSP-34(4): pp. 744--754.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Smith and Serra, 1987} Smith, J. and Serra, X. 1987. "PARSHL: Analysis/Synthesis Program for Non-Harmonic Sounds Based on a Sinusoidal Representation." Proc. International Computer Music Conference, Urbana, pp. 290--297.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Dudley, 1939} Dudley, H. 1939, "The Vocoder," Bell Laboratories Record, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Moorer, 1978} Moorer, A. 1978. "The Use of the Phase Vocoder in Computer Music Applications." Journal of the Audio Engineering Society, 26 (1/2), pp. 42--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Moorer, 1979} Moorer, A. 1979, "The Use of Linear Prediction of Speech in Computer Music Applications," Journal of the Audio Engineering Society 27(3):134--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{Dolson, 1986} Dolson, M. 1986, "The Phase Vocoder: A Tutorial," Computer Music Journal, 10 (4), pp. 14--27.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Makhoul, 1975} Makhoul, J. 1975. "Linear Prediction: A Tutorial Review," Proc. of the IEEE, v 63., pp. 561--580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{Chowning, 1973} Chowning, J. 1973, "The Synthesis of Complex Audio Spectra by Means of Frequency Modulation," Journal of the Audio Engineering Society 21(7): pp. 526--534.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{LeBrun, 1979} LeBrun, M. 1979. "Digital Waveshaping Synthesis," Journal of the Audio Engineering Society, "27(4): 250--266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Adrien, 1988} Adrien, J. 1988. Etude de Structures Complexes Vibrantes, Application-la Synth&#232;se par Modeles Physiques, Doctoral Dissertation. Paris: Universit&#233; Paris VI.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77671</ref_obj_id>
				<ref_obj_pid>76932</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Wawrzynek, J. 1989. "VLSI Models for Sound Synthesis," in Current Directions in Computer Music Research, M. Mathews and J. Pierce Eds., Cambridge, MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Larouche & Meillier 1994} Larouche, J. & J. Meillier 1994. "Multichannel Excitation/Filter Modeling of Percussive Sounds with Application to the Piano," IEEE Trans. Speech and Audio, pp. 329--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{Smith, 1987} Smith, J. 1987. Musical Applications of Digital Waveguides. Stanford University Center For Computer Research in Music and Acoustics. Report STAN-M-39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Karjalainen et al. 1991} Karjalainen, M. Laine, U., Laakso, T. and V. V&#228;3lim&#228;ki, 1991. "Transmission Line Modeling and Real-Time Synthesis of String and Wind Instruments," Proc. International Computer Music Conference, Montreal, pp. 293--296]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1991} Cook, P. 1991. "TBone: An Interactive Waveguide Brass Instrument Synthesis Workbench for the NeXT Machine," Proc. International Computer Music Conference, Montreal, pp. 297--299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1991b} Cook, P. 1991b. "LECTOR: An Ecclesiastical Latin Control Language for the SPASM/singer Instrument," Proc. International Computer Music Conference, Montreal, pp. 319--321.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1992} Cook, P. 1992. "A Meta-Wind-Instrument Physical Model, and a Meta-Controller for Real-Time Performance Control," Proc. International Computer Music Conference, San Jose, pp. 273--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Cook, 1992b} Cook, P. 1992b. "SPASM: a Real-Time Vocal Tract Physical Model Editor/Controller and Singer: the Companion Software Synthesis System," Computer Music Journal, 17: 1, pp 30--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{McIntyre et al. 1983} McIntyre, M., Schumacher, R. and J. Woodhouse 1983, "On the Oscillations of Musical Instruments," Journal of the Acoustical Society of America, 74(5), pp. 1325--1345.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>131155</ref_obj_id>
				<ref_obj_pid>131150</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{Roads, 1991} Roads, C. (1991). Asynchronous Granular Synthesis, In G. De Poli, A. Piccialli, & C. Roads (Eds.), Representations of Musical Signals, pp. 143--185. Cambridge: MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{Gabor, 1947} Gabor, D. (1947). Acoustical Quanta And The Theory Of Hearing. Nature, 159(4044), 591--594.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{Xenakis, 1971} Iannis Xenakis, Formalized Music: Thought and Mathematics in Composition. Bloomington and London: Indiana University Press, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{Truax, 1988} Truax, B. (1988) Real-time granular synthesis with a digital signal processor. Computer Music Journal, 12(2), 14--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{Verplank, 2000} B. Verplank, M. Mathews, R. Shaw, "Scanned Synthesis", "Proceedings of the 2000 International Computer Music Conference", p: 368--371, Berlin, Zannos editor, ICMA, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[STK from Perry Cook's page: http://www.cs.princeton.edu/~prc/NewWork.html#STK]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Perry R. Cook and Gary P. Scavone, The Synthesis ToolKit (STK), Proc of the ICMC, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[G. Scavone and P. Cook, "RtMIDI, RtAudio, and a Synthesis (STK) Update," Proceedings of the International Computer Music Conference, Barcelona, September, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[PureData: http://puredata.info/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[JASS: http://www.cs.ubc.ca/~kvdoel/jass/jass.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Kees van den Doel and Dinesh K. Pai, JASS: A Java Audio Synthesis System for Programmers, Proceedings of the International Conference on Auditory Display, pp. 150--154, 2001, Helsinki.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Max/MSP: http://www.cycling74.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Chuck: http://chuck.cs.princeton.edu/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085915</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Wang & Cook, 2004} Wang, G. and Cook, P. R. 2004. On-the-fly programming: using code as an expressive musical instrument. In Proceedings of the 2004 Conference on New interfaces For Musical Expression (Hamamatsu, Shizuoka, Japan, June 03 - 05, 2004), 138--143.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Supercollider: http://supercollider.sourceforge.net// and http://www.audiosynth.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{Kimura, 2004} Mari Kimura 2004, Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Wynnychuk, 2004} Jordan Wynnychuk 2004, Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085897</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{D'Arcangelo, 2004} D'Arcangelo, G. 2004, Recycling music, answering back: toward an oral tradition of electronic music, Proceedings of the 2004 conference on New interfaces for musical expression, pp. 55--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{Okamoto, 2004} Hisashi Okamoto, 2004, Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085206</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{Gadd & Fels, 2002} Gadd, A. and Fels, S. 2002. MetaMuse: metaphors for expressive instruments, Proceedings of the 2002 conference on New interfaces for musical expression, pp. 1--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972352</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Fels et al., 2003} Fels, S. and Gadd, A. and Mulder, A. 2003. Mapping transparency through metaphor: towards more expressive musical instruments, Organised Sound 7(2): 109--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085730</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Jorda, 2003} Jorda, S. 2003. Sonigraphical instruments: from FMOL to the reacTable, Proceedings of the 2003 conference on New interfaces for musical expression, pp. 70--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{Jorda et al. 2005} Jorda, S. and Kaltenbrunner, M. and Geiger, G. and Bencina, R. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[The reactable*, Proceedings of the International Computer Music Conference (ICMC 2005) pp. 579--582.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{Mulder & Fels, 1998} Axel G. E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing with Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo Horizonte, Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congress of the Brazilian Computer Society, G. Ramalho (ed.)), pp. 151--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Horio, 2004} Kanta Horio 2004. Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{Fujii, 2004} Uriko Fujii 2004. Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>972352</ref_obj_id>
				<ref_obj_pid>972348</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{Fels et al. 2002} Fels, Sidney, Gadd, Ashley, Mulder, Axel (2002). Mapping Transparency through Metaphor: Towards more expressive musical instruments, Organised Sound: Vol. 7, no. 2. Cambridge: Cambridge University Press: 109--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{Mathews & Schloss, 1989} Mathews, M. V. and A. Schloss The Radio Drum as a Synthesizer Controller, Proc. of the 1989 ICMC.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{Boei et al. 1989} Boie, R. A. et al. Gesture Sensing via Capacitive Moments. Work Project No. 311401-(2099,2399) AT&T Bell Laboratories, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{Young, 1991} Young, G. (1991), <i>The Sackbut Blues</i>: University of Toronto Press in Canada, and the University of Chicago Press in the USA. Issued also in French under the title: "Blues pour saqueboute: Hugh Le Caine, pionnier de la musique &#233;lectronique." For further information phone +1/613 991--2983. The National Museum of Science and Technology houses an extensive collection of Le Caine's instruments.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085941</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{Buchla, 2005} Don Buchla. A History of Buchla's Musical Instruments., Proc. of NIME2005, Vancouver, BC, http://www.nime.org/2005/proc/nime2005_001.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Paradiso, J., and Gershenfeld, N. Musical Applications of Electric Field Sensing. In Computer Music Journal 21(2) Summer, pp. 69--89. 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085764</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{Palacio-Quintin, 2003} Palacio-Quintin, Cl&#233;o. The Hyper-Flute. In Proceedings of the New Interfaces for Musical Expression (NIME) Conference. Montreal, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085188</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Young, 2001} Young, D., The hyperbowcontroller: Real-time Dynamics measurement of violin performance. In Proc. NIME, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{Marshall, 2009} Marshall, M., Physical Interface Design for DigitalMusical Instruments. Ph.D. thesis, McGill University, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1588259</ref_obj_id>
				<ref_obj_pid>1586646</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{Fraser et al. 2008} Helene Fraser and Sidney Fels and Robert Pritchard. Walk the Walk, Talk the Talk. 12th IEEE 2008 International Symposium on Wearable Computing (ISWC2008). Pages 117--118. 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142282</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Pritchard & Fels, 2006} Pritchard, B. and Fels, S. 2006. GRASSP: gesturally-realized audio, speech and song performance. In Proceedings of the 2006 Conference on New interfaces For Musical Expression (Paris, France, June 04 - 08, 2006). New Interfaces For Musical Expression. IRCAM --- Centre Pompidou, Paris, France, 272--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2326218</ref_obj_id>
				<ref_obj_pid>2325759</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{Fels & Hinton, 2998} Sidney S. Fels and Geoffrey E. Hinton. Glove-TalkII: A neural network interface which maps gestures to parallel formant speech synthesizer controls. IEEE Transactions on Neural Networks. Volume 9. No. 1. Pages 205--212. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085972</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Levin & Lieberman, 2005} Levin, G. and Lieberman, Z. "Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours in "The Manual Input Sessions". Proceedings of NIME '05, Vancouver, BC, Canada. May 26--28, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{Hoekstra et al. 2009} Hoekstra, A., Bartneck, C., & Lyons, M. J., 2009. "The Hyper-Trapeze - A Physically Active AudioVisual Interface for Performance & Play". Proceedings of INTETAIN-09, Amsterdam pp. 201--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{Mulder & Fels, 1998} Axel G. E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing with Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo Horizonte, Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congres of the Brazilian Computer Society, G. Ramalho (ed.)), pp. 151--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085173</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{Huott, 2002} Huott, R. 2002. An interface for precise musical control. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 - 26, 2002). E. Brazil, Ed., 1--5. http://www.nime.org/2002/proceedings/paper/huott.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085901</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{O'Modhrain & Essl, 2004} O'Modhrain, S. and Essl, G. 2004. PebbleBox and CrumbleBag: tactile interfaces for granular synthesis. In Proceedings of the 2004 Conference on New interfaces For Musical Expression (Hamamatsu, Shizuoka, Japan, June 03 - 05, 2004). M. J. Lyons, Ed., 74--79.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085165</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{Overholt, 2001} Overholt, D. 2001. The MATRIX: a novel controller for musical expression. In Proceedings of the 2001 Conference on New interfaces For Musical Expression (Seattle, Washington, April 01 - 02, 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{Wang, 2009} Wang, G. 2009. Designing Smule's iPhone Ocarina, In Proceedings of the 2009 Conference on New interfaces For Musical Expression (Pittsburgh, PA, June 4-6, 2009).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085770</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{Merrill, 2003} Merrill, D. 2003. Head-tracking for gestural and continuous control of parameterized audio effects. In Proceedings of the 2003 Conference on New interfaces For Musical Expression (Montreal, Quebec, Canada, May 22 - 24, 2003). 218--219. http://www.music.mcgill.ca/musictech/nime/onlineproceedings/Papers/NIME03_Merill.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085886</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{Kapur et al. 2004} Kapur, A., Tzanetakis, G., & P. F. Driessen, "Audio-Based Gesture Extraction on the ESitar Controller," In Proceedings of the Conference on Digital Audio Effects, Naples, Italy, October 5-8, 2004. http://soundlab.cs.princeton.edu/research/controllers/esitar/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085186</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{Vogt et al. 2002} Vogt, F., McCaig, G., Ali, M. A., and Fels, S. 2002. Tongue 'n' Groove: an ultrasound based music controller. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 - 26, 2002). http://www.nime.org/2002/proceedings/paper/vogt.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085183</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{Takaka & Knapp, 2002} Tanaka, A. and Knapp, R. B. 2002. Multimodal interaction in music using the Electromyogram and relative position sensing. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 - 26, 2002. http://www.nime.org/2002/proceedings/paper/tenaka.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{Knapp & Lusted, 1990} R. Benjamin Knapp and Hugh Lusted - A Bioelectric Controller for Computer Music Applications, Computer Music Journal, Volume 14 No. 1, New Performance Interfaces 1 - Spring 1990; pg 42--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085726</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{Nagashima, 2003} Nagashima, Y. 2003. Bio-sensing systems and bio-feedback systems for interactive media arts. In Proceedings of the 2003 Conference on New interfaces For Musical Expression (Montreal, Quebec, Canada, May 22 - 24, 2003), 48--53. http://www.nime.org/2003/onlineproceedings/Papers/NIME03_Nagashima.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085885</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{Fels et al. 2004} Sidney S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to Instrument. 4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1-6. May. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085200</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{Fels & Vogt, 2002} Sidney S. Fels and Florian Vogt. Tooka: Explorations of Two Person Instruments. 2nd International Conference on New Interfaces for Musical Expression (NIME02). Pages 116--121. May. 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1086014</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{Carlile & Hartmann, 2004} Carlile, J. and Hartmann, B. 2004. OROBORO: a collaborative controller with interpersonal haptic feedback. In Proceedings of the 2005 Conference on New interfaces For Musical Expression (Vancouver, Canada, May 26 - 28, 2005), 250--251.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085228</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{Chadabe, 2002} Chadabe, J. 2002. The limitations of mapping as a structural descriptive in electronic instruments. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 - 26, 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{Blaine & Fels, 2003} Tina Blaine and Sidney S. Fels. Collaborative Musical Experiences for Novices. Journal of New Music Research. Volume 32. No. 4. Pages 411--428. Dec. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>347705</ref_obj_id>
				<ref_obj_pid>347642</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[{Blaine & Perkis, 2000} Tina Blaine and Tim Perkis: The Jam-O-Drum Interactive Music System: A Study in Interaction Design. Symposium on Designing Interactive Systems 2000: 165--173]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[{Paradiso, 1999} Paradiso, J., The Brain Opera Technology: New Instruments and Gestural Sensors for Musical Interaction and Performance. Journal of New Music Research, 1999. 28(2): p. 130--149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[{Tarabella, 2004} Leonello Tarabella, 2004. Performance at NIME-04, Hamamatsu, Japan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[{Fels & Mase, 1999} Sidney S. Fels and Kenji Mase. Iamascope: A Graphical Musical Instrument. Computers and Graphics. Volume 2. No. 23. Pages 277--286. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[{Lyons et al., 2001} Lyons, M. J., Haehnel, M., and Tetsutani, N. 2001. The mouthesizer: A facial gesture musical interface. Conference Abstracts, ACM SIGGRAPH 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[{Lyons, 2004} Lyons, M. J., Facial gesture interfaces for expression and communication. 2004 IEEE Conference on Systems, Man, and Cybernetics, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>634250</ref_obj_id>
				<ref_obj_pid>634067</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[{Lyons & Tetsutani, 2001} Lyons, M. J. and Tetsutani, N. 2001, Facing the music: a facial action controlled musical interface, Conference on Human Factors in Computing Systems, (CHI'2001), Extended Abstracts, pp. 309--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085974</ref_obj_id>
				<ref_obj_pid>1085939</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[{Funk et al. 2005} Funk, M. and Kuwabara, K. and Lyons, M. J. 2005. Sonification of facial actions for musical expression, Proceedings of the 2005 conference on New interfaces for musical expression, pp. 127--131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085922</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[{De Silva et al. 2004} A novel face-tracking mouth controller and its application to interacting with bioacoustic models. Proceedings of the 2004 conference on New interfaces for musical expression, pp. 169--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142256</ref_obj_id>
				<ref_obj_pid>1142215</ref_obj_pid>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[{Nishibori & Iwai, 2006} Nishibori, Y. and Iwai, T., 2006. Tenori-On, Proceedings of the 2006 conference on New interfaces for musical expression, pp. 172--175]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085154</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[{Cook, 2001} Cook, P. 2001. Principles for designing computer music controllers, Proceedings of the 2001 conference on New interfaces for musical expression, Seattle WA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[{Cook, 2007} Cook, P. 2007, Keynote Talk at the 2007 Conference on New Interfaces for Musical Expression, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085898</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[{Jorda, 2004} Jorda, S. 2004. Digital instruments and players: part I---efficiency and apprenticeship, Proceedings of the 2004 conference on New interfaces for musical expression pp. 59--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085771</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[{Singer, 2003} Singer, E. 2003. Sonic Banana: A novel bend-sensor-based midi controller, Proceedings of the 2003 conference on New interfaces for musical expression, pp. 220--221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Vienna Vegetable Orchestra http://www.gemueseorchester.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1406583</ref_obj_id>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[{Igoe, 2007} Igoe, T. 2007. Making Things Talk, O'Reilly.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085155</ref_obj_id>
				<ref_obj_pid>1085152</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[{Verplank, Sapp, & Mathews, 2001} Verplank, B. and Sapp, C. and Mathews, M., A Course on Controllers, Proceedings of the 2001 conference on New interfaces for musical expression, Seattle, WA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085936</ref_obj_id>
				<ref_obj_pid>1085884</ref_obj_pid>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[{Lippit, 2004} Lippit, T. M. 2004. Realtime sampling system for the turntablist version 2: 16padjoystickcontroller, Proceedings of the 2004 conference on New interfaces for musical expression, pp. 211--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Laptop Performance  A NIME Performance  What is NIME about? The Problem: Digital Technology &#38; 
computers involved in nearly all forms of contemporary music  But the computer is not a Musical Instrument 
  How to Play the Computer?  Computers offer a wide range of sound and music creation opportunities 
 How can we create new interfaces to play computers in a way that is appropriate to human brains &#38; 
bodies?   How to Play the Computer? This tutorial is all about progress in human- computer interfaces 
for making music from past NIMEs Objectives 1. introduce the theory and practice of NIME 2. NIME community 
is very accessible andgrowing 3. get to know some of the people of NIME 4. easy to start creating NIMEs 
and a lifetime ofenjoyment to master 5. musical expression transcends gender and culture  6. if you 
are not having fun, it s probably not for you  A Brief History of NIME New Interfaces for Musical Expression 
First organized as a workshop of ACM CHI 2001 Experience Music Project - Seattle, April, 2001 Lectures/Discussions/Demos/Performances 
 A Brief History of NIME NIME-02 - Media Lab Europe, Dublin in May 2002 Conference-scale event with 
similar format to the NIME-01 workshop  held annually since 2001  NIME Themes Novel controllers &#38; 
interfaces  Performance &#38; composition with new interfaces  Interfaces for collaborative performance 
 Real-time gestural control of music  Interfaces for musical novices &#38; education  Cognition in 
Musical Interface Design  Haptic &#38; force feedback in musical control  Artistic, cultural, and social 
impact  Tutorial structure Session I - 9:00 - 10:35: Introduction to NIME  Module 1: Building a NIME 
(Fels)  Module 2: Camera-based Interfaces (Lyons)  Module 3: Design &#38; Aesthetics of NIME (Lyons) 
  Discussion Break 10:35 - 10:50  Session II - 10:50 - 12:15  Module 4: NIME after NIME: Case Studies 
(Fels)  Module 5: NIME Theory (Lyons)  Module 6: NIME Education (Lyons)   Conclusion &#38; Discussion 
- 12:00 - 12:15  Six steps to build a NIME 1. Pick control space 2. Pick sound space 3. Pick mapping 
 4. Connect with software 5. Compose and practice 6. Repeat  1 and 2 often switched. Tools to help 
with steps 1-4. An example: Tooka (Fels et al., 2004) Vibrato Pitch Bend mapping with PureData Octave 
Sustain sound synthesis Pitch Volume Pick your control space Plethora of sensors available to measure: 
 motion of body parts position, rotation,velocity andacceleration  translation and rotation (torque)forces 
 isometric and isotonic sensors  pressure  airflow  proximity  temperature  neurophysiologicalsignals 
 heart rate  galvanic skin response  brain waves  muscle activities   light levels  and more 
 Physical property sensors Piezoelectric Sensors  Force Sensing Resistors  Accelerometer (Analog Devices 
ADXL50)  Biopotential Sensors  Microphones  Photodetectors  CCDs and CMOS cameras  Electric Field 
Sensors  RFID  Magnetic trackers (Polhemus, Ascension)  and more  What can I measure?  Human Action 
Oriented Sensors Reach - EMF disturbance  Slide - resistive  TapTile - Force senstive resistor  Tilt 
  electrolytic, single axis (-70-+70 deg)  Touch - 0 travel FSR  TouchGlove  TouchStrip  long touch 
sensor Turn potentiometer    Connecting sensors Sensor response requires transduction and digitizing: 
electrical  voltage Sensor  resistance   Transduction impedance optical Electrical colour Signal 
intensity signal conditioning magnetic  induced current  field direction Voltage  A to D mechanical 
force conversion digital sample  Digitizing converting change in resistance into voltage typical sensor 
has variable resistance (Rsensor) Vsrc (+5V) R sensor A simple voltage Vout to digitizer divider circuit 
R Vout = Vsrc * R (R+ R) sensor   Digitizers for Connecting to Computer  Some MIDI synthesizers, 
i.e., Yamaha mu100  Arduino board Bluetooth module for wireless A/D   ICubeX A/D to MIDI  Phidgets 
 A/D to USB  DAQ boards  A/D to computer bus  Mapping Sensor to Music  The relationship between 
the change in the sensorvalue to the sound output is called a mapping  The mapping defines how much 
effort to learn andplay your NIME  Last step is to control your sound output:  communication protocol 
 sound synthesizer   This is the heart of the course and what NIME community is specialized in. 
 Sound output control: communications Musical Instrument Digital Interface (MIDI)  electronic instrument 
standard defined in 1982  specifies;  connectors, data rates, electrical properties, etc. 1 message/msec 
(approx) note on/off, velocity is typical packet  control messages to change instrument synthesis 
  Open Sound Control (OSC) (Wright and Freed, 1997)  TCP/IP, internet protocol, typically UDP based 
 faster, low latency, variable packet types  computer to computer, computer to hardware   Internal 
protocols, i.e. DAQ driver  Sound Synthesis Techniques  Methods  sampled  FM synthesis  additive/subtractive 
 granular  waveguide/physical modeling  scan   check out Computer Music Tutorial, Roads, C., MIT 
Press, 1996  Sound Synthesizers Hardware MIDI synthesizers Yamaha, Roland, Korg, Casio, Moog, Kowai, 
Symbolic Sound Corporation, Nord modular, and others  Software  STK (Cook)  PureData (Pd, Puckette) 
 JASS (van den Doel)  Max/MSP (cycling74.com)  Chuck (Wang and Cook, 2003)  Supercollider (McCartney, 
1996)  and others   &#38;#169;2011, S. Fels and M. Lyons  A few practical notes Portable:  Batteries 
can be used to make portable  Wireless protocols available for portable   Write pieces for the instrument 
 Aesthetics are important  Plan your checklist for performance too many things can go wrong with technology 
 Plan your staging can severely impact performance of sensors  Plan for producing stable versions 
 hard to learn to play if NIME keeps changing Module 3 has more details. Summary  Making a NIME is 
usually easier thanplaying it (well)  Choose your:  movement type  sound space  sensing   Put together 
your input, mapping and output  Now you are ready to:  practice, practice, practice and perform  
aesthetic principles covered in module 3   Camera-based Interfaces  F1 : visual feedback in the form 
of aligned graphics Imaginary Piano: No visual feedback  Leonello Tarabella, NIME-02  Video camera 
with motion-sensitive zone  No primary feedback  Visual Input Only: Imaginary Piano  Leonello Tarabella, 
NIME-02  Visual Input &#38; Output Iamascope  This gives a colourful kaleidoscopic feedback of part 
of the player. Gestures are used to trigger harmonious chord progressions and arpeggios.  Quite good 
coordination between sound and graphics   Iamascope - video  Facial Gesture Musical Interface  Lyons, 
NIME-01  Mouthesizer  Colour &#38; intensity thresholding Image Morphological transform &#38; filtering 
processing operations  Connected components + shape analysis Lyons et al., NIME-03 Mouthesizer Video 
Guitar Effects Controller  Lyons (2001) Mapping: H Cutoff Frequency of Resonant Low Pass Filter W 
Distortion Mouthesizer Video Guitar Effects Controller  Sonification of Facial Actions (SoFA)  Optical 
Flow triggers samples  Samples mapped to facial zones  Frame is recalibrated with face detection Saccades 
  Funk et al., NIME-05  Sonification of Facial Actions (SoFA)  Reactable  Video tracking of marked 
pucks on a table  Projection of visual feedback   Sergi Jord et al., Universitat Pompeu Fabra  
first presented at NIME-03  Reactable  Summary  Large number of works have used visual input and output 
as a way to enhance new musical interfaces  General principle is that vision offers a powerful way to 
capture gestural input  Visual output using camera input can provide transparency  Technological Expressionism 
 Shock of the New Techno-fetishism  Experimentalism  Human-machine relationship  Mari Kimura w/ 
Lemur Guitarbot  NIME Favors a Return to Process-oriented Music we are in a period of restoring fluidity 
to the musical transformative process of making music more process-oriented again and less artifactoriented. 
Gideon D Arcangelo, NIME-04  New Folk?  Challenge of Performance Audience may not understand your 
NIME  Expectations may be varied  No musical tradition to fall back on  A demo is not a performance 
 Hisashi Okamoto, NIME-04 The First Sailing with Limber-Row  Hisashi Okamoto - Limber Row   Transparency 
for Performer &#38; Audience Complicated mapping OO OT TT  Simplify  OT  Complex mapping  TO T 
= transparent O = opaque TOOO How to achieve TT? (Gadd et al, 2003) Visual Cues &#38; Transparency 
 Visual Appearance of Instrument  Visualization of Interaction  Visualization of Sound Output  Reactable 
Tenori-on Transparency &#38; Interaction Metaphor SoundSculpting (Mulder and Fels, 1998) - two Cybergloves 
and Trackers -map metaphor of rubber sheet onto sound space -transparent for audience and performer 
Axel Mulder - Sound Sculpting  Transparency Simple &#38; Direct Interface Particle Kanta Horio, NIME-04 
 Contact Mics  Magnets  Paper clips   Aesthetics of Failure  Suspense highlights the technological 
challenge  If there are never difficulties, glitches etc then the limits are not being pushed  Technical 
difficulty delayed this performance, but improved the outcome Some Design Guidelines: Perry s Principles 
 Rules of thumb for the design of digital musical instruments  Several of the principles are heavily 
subscribed  Principles for Designing Computer Music Controllers P. Cook, NIME-01 Revised: Principles 
for Controlling Computer Music Designers P. Cook, Keynote talk, Perry s Principles Human/Artistic Principles 
P1: Programmability is a curse P2: Smart instruments are often not smart  P3: Copying an instrument 
is dumb, leveraging expert technique is smart P4: Some players have spare bandwidth, some do not P5: 
Make a piece, not an instrument or controller P6: Instant music, subtlety later Perry s Principles Human/Artistic 
Principles P1: Programmability is a curse P2: Smart instruments are often not smart  P3: Copying an 
instrument is dumb, leveraging expert technique is smart P4: Some players have spare bandwidth, some 
do not P5: Make a piece, not an instrument or controller P6: Instant music, subtlety later P1: Programmability 
is a curse P2: Smart Instruments are Often Not  Easy to add complexity, features, bandwidth  But instruments 
can quickly become complex,unstable, and difficult to learn  It is tempting to A.I. to instruments but 
this can often be bad design if the player feels theinstrument too obviously has a mind of its own  
Perry s Principles Human/Artistic Principles P1: Programmability is a curse P2: Smart instruments are 
often not smart  P3: Copying an instrument is dumb, leveraging expert technique is smart P4: Some players 
have spare bandwidth, some do not P5: Make a piece, not an instrument or controller P6: Instant music, 
subtlety later Perry s Principles Human/Artistic Principles P1: Programmability is a curse P2: Smart 
instruments are often not smart  P3: Copying an instrument is dumb, leveraging expert technique is 
smart P4: Some players have spare bandwidth, some do not P5: Make a piece, not an instrument or controller 
P6: Instant music, subtlety later P5: Make a piece not a controller P6:Instant Music, Subtlety later 
 Making music is the goal  The ideal new musical interfaces has: Low entry fee with no ceiling on 
 virtuosity Wessel &#38; Wright, NIME-01 Perry s Principles Human/Artistic Principles P1: Programmability 
is a curse P2: Smart instruments are often not smart  P3: Copying an instrument is dumb, leveraging 
expert technique is smart P4: Some players have spare bandwidth, some do not P5: Make a piece, not an 
instrument or controller P6: Instant music, subtlety later Perry s Principles* Technological: P7: Miracle, 
Industry Designed, Inadequate P8: Batteries, Die (a command not an observation) P9: Wires are not that 
bad (compared to wireless) Misc.: P10: New algorithms suggest new controllers P11: New controllers suggest 
new algorithms P12: Existing Instruments suggest new controllers P13: Everyday objects suggest amusing 
controllers Perry s Principles* Technological: P7: Miracle, Industry Designed, Inadequate P8: Batteries, 
Die (a command not an observation) P9: Wires are not that bad (compared to wireless) Misc.: P10: New 
algorithms suggest new controllers P11: New controllers suggest new algorithms P12: Existing Instruments 
suggest new controllers P13: Everyday objects suggest amusing controllers P13: Everyday objects suggest 
controllers that are both amusing &#38; good Sonic Banana (E. Singer, NIME-03) Java mug &#38; Fillup 
Glass (P. Cook, NIME-01) Eric Singer - Sonic Banana  Perry s Principles* Technological: P7: Miracle, 
Industry Designed, Inadequate P8: Batteries, Die (a command not an observation) P9: Wires are not that 
bad (compared to wireless) Misc.: P10: New algorithms suggest new controllers P11: New controllers suggest 
new algorithms P12: Existing Instruments suggest new controllers P13: Everyday objects suggest amusing 
controllers Perry s Principles* New (as of 2007) P14: More can be better (but hard)  P15: Music + 
Engineering is a great Teaching (and Marketing) tool P17: Younger students are more fearless Perry s 
Principles* New (as of 2007) P14: More can be better (but hard)  P15: Music + Engineering is a great 
Teaching (and Marketing) tool P17: Younger students are more fearless P15: Music + Engineering is a great 
Teaching Tool  High student interest  Motivation for learning a range of core topicsincluding:  Sensors 
 HCI  DSP  Math skills  Programming  Networking   Joe Paradiso &#38; student (NIME-02)  Summary 
 Technology is increasing the fluidity of musical culture  NIME presents special challenges for performers 
  Well-designed visual feedback can greatly improve mapping transparency for audience andplayer  Interaction 
metaphors another strategy  Initial failure can enhance eventual success  Perry s principles provide 
practical policies  Questions/Discussion Break  Original NIMEs Leon Theremin, 1928 senses hand position 
relative to antennae  controls frequency and amplitude  Clara Rockmore playing   More original NIMEs 
Hugh Le Caine (1940s) electronic sackbut  sensor keyboard  downward and side-to-side  potentiometers 
   Science Dimension volume 9 issue 6 1977  right hand can modulate loudness and pitch  left hand 
modulates waveform  precursor to the mod-wheel Canada Science and Technology Museum  Electronic Sackbut 
 1971 commercial version Buchla s Midi Controllers   Thunder (1990) 36 touch sensors  Lightning 
2 (1996) LED based position sensing  Marimba Lumina (1999)  pads and ribbon controllers (strips) 
 bars are sensitive to proximity, hit location and movement  4 different mallets for different effects 
   &#38;#169;2011, S. Fels and M. Lyons  Buchla 200e Series music controllers Modules can be combined: 
 Control and Signal Router  Multi Dimensional Kinesthetic Input Port   Midi Decoder/Preset Manager 
 System Interface Arbitrary Function Generator (2 panel units)  Complex Waveform Generator  Source 
of Uncertainty  Quad Function Generator  Frequency Shifter / Balanced Modulator  Triple Morphing 
Filter  Quad Dynamics Manager   There s a lot of NIMEs out there  (Marshall, 2009) Most are classed 
in the Alternate category Augmented Instruments Hypercello (Gershenfeld &#38; Chung, 1991) -related Hyperbow 
(Young, 2001)  Augmented Instuments Yamaha Disklavier  MIDI controlled acoustic piano  solenoid 
actuators to play keys  records key press   Radio Baton + Disklavier performance  Jaffe &#38; Schloss, 
The Seven Wonders of the Ancient World, 1991 Augmented Instruments Hyper-Flute (Palacio-Quintin, 2003) 
 standard Boehm flute  sensors:  magnetic field, G# and C# keys  ultrasound tracking  mercury tilt 
switch  pressure sensors (left hand and thumbs)  light sensor  buttons    Alternative Instruments: 
Using different sensors  (Marshall, 2009) Alternative Instruments how to organize: sensed property 
(i.e. wind)  player action (i.e. percussion)  instrument shape  relationship to body  Hands Only 
- free gesture + physical Lady s Glove (Sonami, 1991+) hall effect sensors, microswitches, resistive 
strips, pressure pad, accelerometer  controlled musical effects  free gesture + contact gesture + 
voice The Hands (Waisvisz, 1984) Hands Only - free gesture Manual Input Sessions (Levin and Leibermann, 
2004) camera and OHP  SoundSculpting (Mulder and Fels, 1998) GloveTalkII/GRASSP/DIVA, (Fels et al., 
1994+) cybergloves, tracker, switches  controlled formant synthesizer  and more Hands - Contact gesture 
 Most typical type of NIME  Ski (Huott, 2002)  fibre optic multitouch pad Tactex Inc.  mappings: 
  playback: linear, polar and angular control modes  percussive  pitch tuning:   MIDI controller 
 upright form factor   Hand - Contact gesture Pebblebox (O Modhrain &#38; Essl, 2004) microphone 
+ stones  granular synthesis  play with stones mixes granules  Hand - Contact gesture Crackle box 
(Waisvisz, 1975) analog circuit  op-amp with body resistance connected to pins  in the tradition 
of circuit bending   Hand - Contact gesture Lippold Haken s Continuum touch sensitive - neoprene covered 
  x, y along board  z - pressure   MIDI controller  sound effects  continuous frequency  pitch 
bends    Breath and Hands  Face/Head Control eSitar (Kapur et al, 2004) accelerometer for head tilt 
experimented with volume, duration,and more Mouthesizer (Lyons et al., 2003)  SoFA, (Funk et al., 2005) 
 Tongue n Groove (Vogt et al., 2002)  ultrasound probe to measure tongue movement &#38;#169;2011, 
S. Fels and M. Lyons Body  Miburi from Yamaha, 1994  bend sensors at arm joints  two buttons/finger 
and thumb  two pressure sensors/foot  MIDI controller    Inside Body Biomuse (Knapp and Lusted, 
1990) 8 channel signal amp EMG, EKG, EOG, EEG Tibeten singing bowls (Tanaka and Knapp, 2002) EMG and 
position sensing miniBioMuseIII (Nagashima, 2003) 8 EMG channels mapped to bandpass filters, sinewave 
generators and FM synthesizers  used in BioCosmicStorm-II   Collaborative Instruments  Tooka (Fels 
and Vogt, 2002) pressure for breath  buttons for fingers  bend sensor  touch sensor   two players 
share breath  coordinate movements  MIDI mapping  Collaborative Instruments OROBORO (Carlile &#38; 
Hartmann, 2005) haptic mirror between hand paddles  4 FSRs/hand  mapped using Pd to:  violins sounds 
 sampled sounds    Nime for Novices: Jam-o-drum (Blaine and Perkis, 2000)  4 player audio/visual 
interface drum pads sensors with rotation sensor around rim  Drum circle concept  Various musical 
games  turn taking  collaboration    Jam-o-drum (Blaine and Perkis, 2000)  NIMEs for Novices 
 Interactive instruments embody all of thenuance, power, and potential of deterministic instruments, 
but the waythey function allows for anyone, from themost skilled and musically talentedperformers to 
the most unskilled membersof the large public, to participate in amusical process (Chadabe, 2002)  Walk 
up and play  NIMEs for Novices (Blaine &#38; Fels, 2003) Brain Opera (Machover et al, 1996) Aptitude 
Novice Virtuoso Capacity Single player Single interface Electronic Bullroarer Iamascope Duet on piano 
Multiple interfaces Musical Trinkets Jazz Ensembles Multiple players Single interface Beatbugs Squeezables 
Audio Grove Sound Mapping Speaking Orbs Jamodrum Mikrophonie I, Tooka Multiple interfaces Augmented Groove 
Brain Opera Drum Circle Mikrophonie II &#38;#169;2011, S. Fels and M. Lyons  Multiple NIMEs as part 
of a larger connected set of interaction Forest station  Harmonic driving  Melody easel  Rhythm tree 
 Gesture wall  Digital baton  Audience sensing in performance space  Sensor chair  Brain Opera 
(Paradiso, 1999)  Sensor chair  multiple antenae to track hand positions  two antenae for feet  
buttons  lights   MIDI mapping  Brain Opera NIME  Summary  Creating a NIME is easy to do  Creating 
a good mapping is hard  Playing it well takes practice to be a virtuoso some NIMEs created to be easy 
to play but not so expressive  Without a piece, difficult to gain acceptance  Often audience doesn 
t know what is going on  Many explorations trying different ways to make music   NIME Generic Model 
I  M : Mapping, F1, F2 : Primary &#38; Secondary Feedback Based on: Miranda &#38; Wanderley (2006) 
 Traditional Instrument I Gestural Input F2  Based on: Miranda &#38; Wanderley (2006)  NIMEs decouple 
 Control separate from Synthesis  Mapping (M) is designed  Feedback (F1 and F2) is designed  Controller/Interface 
is designed  NIME representations discrete vs. continuous controllers keys vs knobs  acoustic vs 
electronic sound output vibrating string vs. speaker  digital vs analog representations  bits vs. 
voltage NIME, DMI, Instrument  musical interface and nime used interchangeably  DMI Digital Musical 
Instrument  DMI &#38; MI may be preferable because a NIME will not be new forever  Digital NIME Computer 
enables arbitrary design of interface behaviour: controller  feedback (F1 &#38; F2)  mapping (M) 
 synthesizer  NIME Generic Model I  M : Mapping, F1, F2 : Primary &#38; Secondary Feedback Based 
on: Miranda &#38; Wanderley (2006) Designing Controllers: Gestural Input  Free gesture interfaces 
no physical contact  Physical contact interfaces all acoustic instruments  NIMEs can be in either 
 Free Gesture Interface   Theremin (1919)  Sound feedback (F2) only  No primary tactile or visual 
feedback (F1)  Have been few virtuosos  Considered difficult to master  Lon Theremin  Theremin lacks 
significant primary feedback The Hands  Passive F1 STEIM, Amsterdam (Studio for Electro-instrumental 
Music) NIME Generic Model I  M : Mapping, F1, F2 : Primary &#38; Secondary Feedback Based on: Miranda 
&#38; Wanderley (2006)  Feedback Design: F1 and F2 Sound  F2 Tactile*  Kinesthetic  F1 Visual** 
 *Includes vibro-tactile feedback due to sound waves on the instrument ** Re: Module 2 on Visual Interfaces 
F1 : Visual &#38; Tactile Feedback  Nishiburi &#38; Iwai NIME-06  Tenori-on  NIME Generic Model I 
 M : Mapping, F1, F2 : Primary &#38; Secondary Feedback Based on: Miranda &#38; Wanderley (2006) Instrument 
Mapping  T. Kriese Fairlight CMI, 1980s Polyphonic Digital Sampling Synth Matrix (Overholt, 2001) 
 Shakuhachi  Mapping Problem : How to design the gesture to sound mapping? Mapping  Aspects of the 
Mapping Problem  Dimensionality  Complexity  Mapping Strategy  Other aspects   The mapping layer 
can be considered as the essence of a musical interface Hunt, Wanderley, and Paradis (2003) Dimensionality: 
Types of Mapping  Complexity: Simple &#38; Complex Mappings  Simple Complex Hunt, Wanderley, and Paradis 
(2003) Mapping Complexity complexity can lead to better expression - 1 to 1 usually doesn t do the 
trick * not interesting * not enjoyable * not satisfying  Hunt, Wanderley, &#38; Paradis, NIME-02 
 Understanding Complexity: Three Layer Mapping Strategy  Abstract Mapping Layer example Mouthesizer 
interface (Module 2: Camera-based Interfaces) Controlling a Formant Filter using Mouth Shape [ o ] [ 
a ] [ i ] Lyons et al., NIME-03 Mouthesizer Vowel Mapping  Mapping Design Strategy  Advantage to have 
a control interface which is based on the perceptual qualities of timbre spaces  Better mapping leads 
to more playable interface  How do we characterize playability?  Musical Control Intimacy ... the 
match between the variety of musically desirable sounds produced and the psychophysiological capabilities 
of a practiced per- former. Moore (1988) Control Intimacy depends (somehow) upon gesture to sound mapping 
 Flow in musical expression Quality of Control Experience of Mapping Intimacy Flow  Special contact 
with the instrument  Development of a subtle feeling for sound  Feeling of effortlessness  Playful 
&#38; Free-spirited feeling handling of the material  A. Burzick (2002) Threats to Intimacy  Latency 
between gesture and sound  Lack of primary feedback  Poor mapping  Summary  Generic musical interface 
model is helpful in understanding what makes &#38; breaks a NIME  Mapping constitutes the essence of 
a digital NIME  Mapping is not straightforward and many design strategies have been tried  Multiplayer 
mappings can be better than simple one-to-one mappings  Studies of mapping and feedback are core research 
topics of NIME  Education and NIME  Sound Synthesis  Sensors, Effectors, Microcontrollers  Basic 
Electronics  Communication Protocols (MIDI, OSC, TCP etc.)  Sound Synthesis and Processing  Acoustics 
 Human-Computer Interaction  Music  Where to study this field? IRCAM, Paris  CCRMA, Stanford  CIRMMT, 
McGill  Princeton, CS &#38; Music  NYU Interactive Telecommunications Program  SARC, Queen s, Belfast 
 Growing field  URLs listed in the References  Specific Learning Resources  Miranda &#38; Wanderley 
(2006)  Igoe (2007)  Roads (1996)  NIME Proceedings  ICMC Proceedings  Computer Music Journal 
 Organized Sound  J. New Music Research   Curricula  beginning graduate or senior undergraduate level 
  Courses tend to be project oriented  Students learn what they need  Live performance or Demo is 
necessary for completion of the course (ITP, CCRMA)  NYU ITP NIME Course  Master s program in design 
&#38; technology attracting students from a wide range of backgrounds   Gideon D Arcangelo Hans C. 
Steiner Jamie Allen Taku Lippit (NIME-04)  NIME Curriculum - Topics Historical Survey of Musical Instrument 
Types  Attributes of Musical Expression  Music Theory and Composition  Musical Interface Responsiveness 
 Discrete vs. Continuous Controllers  Gestures and Mapping  Novice and Expert Interfaces  Spectacle 
and Visual Feedback in Performance  Collaborative Interfaces  Substantial resources for learning about 
NIME  NIME courses are usually project based  Number of universities offering programs of study is 
expanding  Next frontier: high schools, science fairs  Summary   How to Play the Computer?  Computers 
offer a wide range of sound and music creation opportunities  How can we create new interfaces to play 
computers in a way that is appropriate to human brains &#38; bodies?  Here s how  NIME tools  NIME 
principles  NIME examples  NIME theory  NIME education  How to get involved NIME community community@nime.org 
subscribe with community-request@nime.org  NIME website www.nime.org  ICMC website www.icmc2009.org 
 Other computer music conferences  Bigger picture 1. introduced the theory and practice of NIME 2. 
NIME community is very accessible andgrowing 3. get to know some of the people of NIME 4. easy to start 
creating NIMEs and a lifetime ofenjoyment to master 5. musical expression transcends gender and culture 
 6. if you are not having fun, it s probably not for you  Questions &#38; Discussions Contact us: 
Sidney Fels, ssfels@ece.ubc.ca  Michael Lyons, lyons@im.ritsumei.ac.jp  www.nime.org References and 
Resource Material Theory [Poupyrev, Lyons, Fels, &#38; Blaine 2001] Poupyrev, I., Lyons, M., Fels, S., 
&#38; Blaine, T., New interfaces for musical expression. Extended Abstracts, CHI-2001, ACM Conference 
on Human Factors in Computing Systems, pp. 309-310. [Fels &#38; Lyons, 2009] Fels, S. and Lyons, M. 2009. 
Creating new interfaces for musical expression: introduction to NIME. In: ACM SIGGRAPH 2009 Courses (SIGGRAPH 
'09). ACM, New York, NY, USA, Article 11, 158 pages. [Wanderley &#38; Battier, 2000] Wanderley, M. and 
Battier, M. 2000. Trends in gestural control of music, IRCAM. [Miranda &#38; Wanderley, 2006] Miranda, 
E.R. and Wanderley, M.M. 2006. New digital musical instruments: control and interaction beyond the keyboard. 
AR Editions. [Wessel &#38; Wright, 2002] Problems and prospects for intimate musical control of computers, 
Computer Music Journal 26(3): 11 22. [Wessel &#38; Wright, 2001] Problems and prospects for intimate 
musical control of computers, In Proceedings of the 2001 Conference on New interfaces For Musical Expression 
(NIME-01) (Seattle, Washington, April 01 - 02, 2001). [Vertegaal et al. 1996] Vertegaal, R. and Ungvary, 
T. and Kieslinger, M.1996. Towards a musician's cockpit: Transducers, feedback and musical function, 
Proceedings of the International Computer Music Conference, pp. 308 311. [Mulder, 2000] Mulder, A. 2000. 
Towards a choice of gestural constraints for instrumental performers. In: Trends in gestural control 
of music, Wanderley, M. and Battier, M., eds., IRCAM. [Camurri, 2000] Camurri, A. and Hashimoto, S. and 
Ricchetti, M. and Ricci, A. and Suzuki, K. and Trocca, R. and Volpe, G. 2000. EyesWeb: Toward gesture 
and affect recognition in interactive dance and music systems, Computer Music Journal 24(1): 57-69. 
[Waiswicz, 1985] Waiswicz, M. 1985. THE HANDS: A Set of Remote MIDI Controllers. Proceedings of the 1985 
International Computer Music Conference (ICMC 85) Vancouver Canada, pp 313 318, San Francisco: ICMA. 
[Marshall, 2009] Marshall, M.T. 2009. Physical Interface Design for Digital Musical Instruments, Ph.D. 
thesis, McGill University, March. http://www.idmil.org/_media/publications/marshall_phdthesis_final.pdf 
[Moore, 1988] Moore, F.R. 1988. The dysfunctions of MIDI, Computer Music Journal 12(1): 19-28. [Burzik, 
2002] Burzik, A. 2002. Practising in Flow The Secret of the Masters, Stringendo 24(2): 18-21. [Hunt &#38; 
Wanderley, 2002] Hunt, A. and Wanderley, M.M. 2002. Mapping performer parameters to synthesis engines. 
Organised Sound 7: 97 108. [Hunt, Wanderley, &#38; Paradis, 2003] Hunt, A. and Wanderley, M.M. and Paradis, 
M. 2003. The importance of parameter mapping in electronic instrument design, Journal of New Music Research 
32(4): 429 - 440. [Rovan et al., 1997] Rovan, J.B., Wanderley, M.M, Dubnov, S. and Depalle, P. 1997. 
Instrumental Gestural Mapping Strategies as Expressivity Determinants in Computer Music Performance. 
In: Kansei, The Technology of Emotion. Proceedings of the AIMI International Workshop, A. Camurri, (ed.), 
Genoa: Associazione di Informatica Musicale Italiana, October 3-4, pp. 68 73. [Arfib et al., 2002] D. 
Arfib, J. M. Couturier, L. Kessous and V. Verfaille 2002. Strategies of mapping between gesture data 
and synthesis model parameters using perceptual spaces. Organised Sound 7: 127-144. [Goudeseune, 2002] 
Goudeseune, C. 2002. Interpolated mappings for musical instruments. Organised Sound 7: 85-96. [Wessel, 
1979] Wessel, D. 1979. Timbre space as a musical control structure, Computer music journal 3(2): 45 
52. [Vertegaal &#38; Eaglestone, 1996] R. Vertegaal and B. Eaglestone 1996. Comparison of input devices 
in an ISEE direct timbre manipulation task, Interacting with Computers, 8(1): 13 30. [Lyons et al., 
2003 Lyons, M.J. and Haehnel, M. and Tetsutani, N. 2003. Designing, playing, and performing with a vision-based 
mouth interface, Proceedings of the 2003 conference on New interfaces for musical expression pp. 116 
121. [Steiner, 2006] H.-C. Steiner. Towards a catalog and software library of mapping methods. Proceedings 
of the 2006 conference on New interfaces for musical expression, pp. 106 109, Paris, France, 2006. IRCAM 
Centre Pompidou. [Cook, 2004] Cook, P. R. 2004. Remutualizing the musical instrument: Co-design of synthesis 
algorithms and controllers. Journal of New Music Research, 33(3):315-320. [Bartneck &#38; Lyons, 2007] 
HCI and the face: towards an art of the soluble. In J. Jacko (Ed.), Human-Computer Interaction, Part 
1, HCII2007, LNCS 4550 (pp. 20-29). Berlin: Springer.  Tools of NIME [Fels et al. 2004] Sidney S. Fels 
and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to Instrument. 
4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1-6. May. 2004. 
 [Wright and Freed, 1997] Wright, M. and A. Freed 1997. Open Sound Control: A New Protocol for Communicating 
with Sound Synthesizers. Proceedings of the International Computer Music Conference, Thessaloniki, Hellas, 
pp. 101-104. Sound Synthesis References: General articles: [Mathews &#38; Pierce, 1989] Mathews, M. 
&#38; Pierce, J. (1989). Current Directions in Computer Music Research. The MIT Press. [Steiglitz, 1996] 
Steiglitz, K. 1996 Digital Signal Processing Primer, New York, Addison Wesley. [Roads, 1976] Roads, C. 
1976, The Computer Music Tutorial, Cambridge, MIT Press Additive synthesis: [McAulay &#38; Quatieri, 
1986] McAulay, R. and T. Quatieri. 1986. "Speech Analysis/Synthesis Based on a Sinusoidal Representation." 
IEEE Trans. Acoust. Speech and Sig. Proc. ASSP-34(4): pp. 744-754. [Smith and Serra, 1987] Smith, J. 
and Serra, X. 1987. "PARSHL: Analysis/Synthesis Program for Non-Harmonic Sounds Based on a Sinusoidal 
Representation." Proc. International Computer Music Conference, Urbana, pp. 290 297. Subtractive Synthesis: 
[Dudley, 1939] Dudley, H. 1939, "The Vocoder," Bell Laboratories Record, December. [Moorer, 1978] Moorer, 
A. 1978. "The Use of the Phase Vocoder in Computer Music Applications." Journal of the Audio Engineering 
Society, 26 (1/2), pp. 42-45. [Moorer, 1979] Moorer, A. 1979, The Use of Linear Prediction of Speech 
in Computer Music Applications, Journal of the Audio Engineering Society 27(3):134-140. [Dolson, 1986] 
Dolson, M. 1986, "The Phase Vocoder: A Tutorial," Computer Music Journal, 10 (4), pp. 14 -27. [Makhoul, 
1975] Makhoul, J. 1975. "Linear Prediction: A Tutorial Review," Proc. of the IEEE, v 63., pp. 561-580. 
 FM synthesis: [Chowning, 1973] Chowning, J. 1973, The Synthesis of Complex Audio Spectra by Means of 
Frequency Modulation, Journal of the Audio Engineering Society 21(7): pp. 526-534. [LeBrun, 1979] LeBrun, 
M. 1979. Digital Waveshaping Synthesis, Journal of the Audio Engineering Society, 27(4): 250-266. Modal 
Synthesis: [Adrien, 1988] Adrien, J. 1988. Etude de Structures Complexes Vibrantes, Application-la Synthse 
par Modeles Physiques, Doctoral Dissertation. Paris: Universit Paris VI. Wawrzynek, J. 1989. VLSI Models 
for Sound Synthesis, in Current Directions in Computer Music Research, M. Mathews and J. Pierce Eds., 
Cambridge, MIT Press. [Larouche &#38; Meillier 1994] Larouche, J. &#38; J. Meillier 1994. Multichannel 
Excitation/ Filter Modeling of Percussive Sounds with Application to the Piano, IEEE Trans. Speech and 
Audio, pp. 329-344.  Physical Modeling Approaches: [Smith, 1987] Smith, J. 1987. Musical Applications 
of Digital Waveguides. Stanford University Center For Computer Research in Music and Acoustics. Report 
STAN-M-39. [Karjalainen et al. 1991] Karjalainen, M. Laine, U., Laakso, T. and V. Vlimki, 1991. Transmission 
Line Modeling and Real-Time Synthesis of String and Wind Instruments, Proc. International Computer Music 
Conference, Montreal, pp. 293-296 [Cook, 1991] Cook, P. 1991. TBone: An Interactive Waveguide Brass Instrument 
Synthesis Workbench for the NeXT Machine, Proc. International Computer Music Conference, Montreal, pp. 
297-299. [Cook, 1991b] Cook, P. 1991b. LECTOR: An Ecclesiastical Latin Control Language for the SPASM/singer 
Instrument, Proc. International Computer Music Conference, Montreal, pp. 319-321. [Cook, 1992] Cook, 
P. 1992. A Meta-Wind-Instrument Physical Model, and a Meta-Controller for Real-Time Performance Control, 
Proc. International Computer Music Conference, San Jose, pp. 273- 276. [Cook, 1992b] Cook, P. 1992b. 
"SPASM: a Real-Time Vocal Tract Physical Model Editor/Controller and Singer: the Companion Software Synthesis 
System," Computer Music Journal, 17: 1, pp 30-44. [McIntyre et al. 1983] McIntyre, M., Schumacher, R. 
and J. Woodhouse 1983, On the Oscillations of Musical Instruments, Journal of the Acoustical Society 
of America, 74(5), pp. 1325-1345. Granular Synthesis: [Roads, 1991] Roads, C. (1991). Asynchronous Granular 
Synthesis, In G. De Poli, A. Piccialli, &#38; C. Roads (Eds.), Representations of Musical Signals, pp. 
143 185. Cambridge: MIT Press. [Gabor, 1947] Gabor, D. (1947). Acoustical Quanta And The Theory Of Hearing. 
Nature, 159(4044), 591 594. [Xenakis, 1971] Iannis Xenakis, Formalized Music: Thought and Mathematics 
in Composition. Bloomington and London: Indiana University Press, 1971. [Truax, 1988] Truax, B. (1988) 
Real-time granular synthesis with a digital signal processor. Computer Music Journal, 12(2), 14-26. 
Scanned Synthesis: [Verplank, 2000] B. Verplank, M. Mathews, R. Shaw, "Scanned Synthesis", "Proceedings 
of the 2000 International Computer Music Conference", p: 368--371, Berlin, Zannos editor, ICMA, 2000. 
 Resources: MIDI website: http://www.midi.org/ Sensors and A/D converters: Infusion Systems: www.infusionsystems.com 
Phidgets: www.phidgets.com Arduino: http://www.arduino.cc/ National Instruments: http://www.ni.com/dataacquisition/ 
Digikey: http://www.digikey.com/ Jameco: www.jameco.com Synthesizers: STK from Perry Cook s page: http://www.cs.princeton.edu/~prc/NewWork.html#STK 
 Perry R. Cook and Gary P. Scavone, The Synthesis ToolKit (STK), Proc of the ICMC, 1999. G. Scavone and 
P. Cook, RtMIDI, RtAudio, and a Synthesis (STK) Update, Proceedings of the International Computer Music 
Conference, Barcelona, September, 2005. PureData: http://puredata.info/ JASS: http://www.cs.ubc.ca/~kvdoel/jass/jass.html 
Kees van den Doel and Dinesh K. Pai, JASS: A Java Audio Synthesis System for Programmers, Proceedings 
of the International Conference on Auditory Display, pp. 150-154, 2001, Helsinki. Max/MSP: http://www.cycling74.com/ 
 Chuck: http://chuck.cs.princeton.edu/ [Wang &#38; Cook, 2004] Wang, G. and Cook, P. R. 2004. On-the-fly 
programming: using code as an expressive musical instrument. In Proceedings of the 2004 Conference on 
New interfaces For Musical Expression (Hamamatsu, Shizuoka, Japan, June 03 - 05, 2004), 138-143. Supercollider: 
http://supercollider.sourceforge.net// and http://www.audiosynth.com/  Aesthetics [Kimura, 2004] Mari 
Kimura 2004, Performance at NIME-04, Hamamatsu, Japan. [Wynnychuk, 2004] Jordan Wynnychuk 2004, Performance 
at NIME-04, Hamamatsu, Japan. [D Arcangelo, 2004] D Arcangelo, G. 2004, Recycling music, answering back: 
toward an oral tradition of electronic music, Proceedings of the 2004 conference on New interfaces for 
musical expression, pp. 55-58. [Okamoto, 2004] Hisashi Okamoto, 2004, Performance at NIME-04, Hamamatsu, 
Japan. [Gadd &#38; Fels, 2002] Gadd, A. and Fels, S. 2002. MetaMuse: metaphors for expressive instruments, 
Proceedings of the 2002 conference on New interfaces for musical expression, pp. 1-6. [Fels et al., 
2003] Fels, S. and Gadd, A. and Mulder, A. 2003. Mapping transparency through metaphor: towards more 
expressive musical instruments, Organised Sound 7(2): 109-126. [Jorda, 2003] Jorda, S. 2003. Sonigraphical 
instruments: from FMOL to the reacTable, Proceedings of the 2003 conference on New interfaces for musical 
expression, pp. 70-76. [Jorda et al. 2005] Jorda, S. and Kaltenbrunner, M. and Geiger, G. and Bencina, 
R. 2005. The reactable*, Proceedings of the International Computer Music Conference (ICMC 2005) pp. 579 
 582. [Mulder &#38; Fels, 1998] Axel G.E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing 
with Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo 
Horizonte, Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congress of the Brazilian Computer 
Society, G. Ramalho (ed.)), pp. 151-164. [Horio, 2004] Kanta Horio 2004. Performance at NIME-04, Hamamatsu, 
Japan. [Fujii, 2004] Uriko Fujii 2004. Performance at NIME-04, Hamamatsu, Japan. [Fels et al. 2002] 
Fels, Sidney, Gadd, Ashley, Mulder, Axel (2002). Mapping Transparency through Metaphor: Towards more 
expressive musical instruments, Organised Sound: Vol. 7, no. 2. Cambridge: Cambridge University Press: 
109-126. Case Studies [Mathews &#38; Schloss, 1989] Mathews, M.V. and A. Schloss The Radio Drum as a 
Synthesizer Controller, Proc. of the 1989 ICMC. [Boei et al. 1989] Boie, R.A. et al. Gesture Sensing 
via Capacitive Moments. Work Project No. 311401-(2099,2399) AT&#38;T Bell Laboratories, 1989. [Young, 
1991] Young, G. (1991), The Sackbut Blues: University of Toronto Press in Canada, and the University 
of Chicago Press in the USA. Issued also in French under the title: "Blues pour saqueboute: Hugh Le Caine, 
pionnier de la musique lectronique." For further information phone +1/613 991-2983. The National Museum 
of Science and Technology houses an extensive collection of Le Caine's instruments. [Buchla, 2005] Don 
Buchla. A History of Buchla's Musical Instruments., Proc. of NIME2005, Vancouver, BC, http://www.nime.org/2005/proc/nime2005_001.pdf 
Paradiso, J., and Gershenfeld, N. Musical Applications of Electric Field Sensing. In Computer Music Journal 
21(2) Summer, pp. 69-89. 1997. [Palacio-Quintin, 2003] Palacio-Quintin, Clo. The Hyper-Flute. In Proceedings 
of the New Interfaces for Musical Expression (NIME) Conference. Montreal, 2003. [Young, 2001] Young, 
D., The hyperbowcontroller: Real-time Dynamics measurement of violin performance. In Proc. NIME, 2001. 
[Marshall, 2009] Marshall, M., Physical Interface Design for DigitalMusical Instruments. Ph.D. thesis, 
McGill University, 2009. [Fraser et al. 2008] Helene Fraser and Sidney Fels and Robert Pritchard. Walk 
the Walk, Talk the Talk. 12th IEEE 2008 International Symposium on Wearable Computing (ISWC2008). Pages 
117--118. 2008. [Pritchard &#38; Fels, 2006] Pritchard, B. and Fels, S. 2006. GRASSP: gesturally-realized 
audio, speech and song performance. In Proceedings of the 2006 Conference on New interfaces For Musical 
Expression (Paris, France, June 04 - 08, 2006). New Interfaces For Musical Expression. IRCAM Centre 
Pompidou, Paris, France, 272-276. [Fels &#38; Hinton, 2998] Sidney S. Fels and Geoffrey E. Hinton. Glove-TalkII: 
A neural network interface which maps gestures to parallel formant speech synthesizer controls. IEEE 
Transactions on Neural Networks. Volume 9. No. 1. Pages 205-212. 1998. [Levin &#38; Lieberman, 2005] 
Levin, G. and Lieberman, Z. "Sounds from Shapes: Audiovisual Performance with Hand Silhouette Contours 
in "The Manual Input Sessions". Proceedings of NIME '05, Vancouver, BC, Canada. May 26-28, 2005. [Hoekstra 
et al. 2009] Hoekstra, A., Bartneck, C., &#38; Lyons, M. J., 2009. The Hyper-Trapeze -A Physically Active 
Audio-Visual Interface for Performance &#38; Play . Proceedings of INTETAIN-09, Amsterdam pp. 201-206. 
[Mulder &#38; Fels, 1998] Axel G.E. Mulder and S. Sidney Fels (1998). Sound Sculpting: Performing with 
Virtual Musical Instruments. Proceedings of the Fifth Brazilian Symposium on Computer Music (Belo Horizonte, 
Minas Gerais, Brazil, 3-5 August 1998, during the 18th Annual Congres of the Brazilian Computer Society, 
G. Ramalho (ed.)), pp. 151-164. [Huott, 2002] Huott, R. 2002. An interface for precise musical control. 
In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 
24 - 26, 2002). E. Brazil, Ed., 1-5. http://www.nime.org/2002/proceedings/paper/huott.pdf [O'Modhrain 
&#38; Essl, 2004] O'Modhrain, S. and Essl, G. 2004. PebbleBox and CrumbleBag: tactile interfaces for 
granular synthesis. In Proceedings of the 2004 Conference on New interfaces For Musical Expression (Hamamatsu, 
Shizuoka, Japan, June 03 - 05, 2004). M. J. Lyons, Ed., 74-79. [Overholt, 2001] Overholt, D. 2001. The 
MATRIX: a novel controller for musical expression. In Proceedings of the 2001 Conference on New interfaces 
For Musical Expression (Seattle, Washington, April 01 - 02, 2001). [Wang, 2009] Wang, G. 2009. Designing 
Smule's iPhone Ocarina, In Proceedings of the 2009 Conference on New interfaces For Musical Expression 
(Pittsburgh, PA, June 4-6, 2009). [Merrill, 2003] Merrill, D. 2003. Head-tracking for gestural and continuous 
control of parameterized audio effects. In Proceedings of the 2003 Conference on New interfaces For Musical 
Expression (Montreal, Quebec, Canada, May 22 - 24, 2003). 218-219. http://www.music.mcgill.ca/musictech/nime/onlineproceedings/Papers/NIME03_Merill.pdf 
 [Kapur et al. 2004] Kapur, A., Tzanetakis, G., &#38; P.F. Driessen, "Audio-Based Gesture Extraction 
on the ESitar Controller," In Proceedings of the Conference on Digital Audio Effects, Naples, Italy, 
October 5-8, 2004. http://soundlab.cs.princeton.edu/research/controllers/esitar/ [Vogt et al. 2002] 
Vogt, F., McCaig, G., Ali, M. A., and Fels, S. 2002. Tongue 'n' Groove: an ultrasound based music controller. 
In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 
24 - 26, 2002). http://www.nime.org/2002/proceedings/paper/vogt.pdf [Takaka &#38; Knapp, 2002] Tanaka, 
A. and Knapp, R. B. 2002. Multimodal interaction in music using the Electromyogram and relative position 
sensing. In Proceedings of the 2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, 
May 24 - 26, 2002. http://www.nime.org/2002/proceedings/paper/tenaka.pdf [Knapp &#38; Lusted, 1990] R. 
Benjamin Knapp and Hugh Lusted - A Bioelectric Controller for Computer Music Applications, Computer Music 
Journal, Volume 14 No. 1, New Performance Interfaces 1 - Spring 1990; pg 42-47. [Nagashima, 2003] Nagashima, 
Y. 2003. Bio-sensing systems and bio-feedback systems for interactive media arts. In Proceedings of the 
2003 Conference on New interfaces For Musical Expression (Montreal, Quebec, Canada, May 22 - 24, 2003), 
48-53. http://www.nime.org/2003/onlineproceedings/Papers/NIME03_Nagashima.pdf [Fels et al. 2004] Sidney 
S. Fels and Linda Kaastra and Sachiyo Takahashi and Graeme McCaig. Evolving Tooka: from Experiment to 
Instrument. 4rd International Conference on New Interfaces for Musical Expression (NIME04). Pages 1-6. 
May. 2004. [Fels &#38; Vogt, 2002] Sidney S. Fels and Florian Vogt. Tooka: Explorations of Two Person 
Instruments. 2nd International Conference on New Interfaces for Musical Expression (NIME02). Pages 116-121. 
May. 2002. [Carlile &#38; Hartmann, 2004] Carlile, J. and Hartmann, B. 2004. OROBORO: a collaborative 
controller with interpersonal haptic feedback. In Proceedings of the 2005 Conference on New interfaces 
For Musical Expression (Vancouver, Canada, May 26 - 28, 2005), 250-251. [Chadabe, 2002] Chadabe, J. 2002. 
The limitations of mapping as a structural descriptive in electronic instruments. In Proceedings of the 
2002 Conference on New interfaces For Musical Expression (Dublin, Ireland, May 24 - 26, 2002). [Blaine 
&#38; Fels, 2003] Tina Blaine and Sidney S. Fels. Collaborative Musical Experiences for Novices. Journal 
of New Music Research. Volume 32. No. 4. Pages 411-428. Dec. 2003. [Blaine &#38; Perkis, 2000] Tina Blaine 
and Tim Perkis: The Jam-O-Drum Interactive Music System: A Study in Interaction Design. Symposium on 
Designing Interactive Systems 2000: 165-173 [Paradiso, 1999] Paradiso, J., The Brain Opera Technology: 
New Instruments and Gestural Sensors for Musical Interaction and Performance. Journal of New Music Research, 
1999. 28(2): p. 130--149. Resources: Theremin: wikipage: http://en.wikipedia.org/wiki/Theremin Oddmusic 
page: http://www.oddmusic.com/theremin/ Theremin enthusiast page: http://theremin.ca/ Where to buy: Moog 
Music -http://www.moogmusic.com/ Clara Rockmore video: http://www.youtube.com/watch?v=pSzTPGlNa5U More 
video of people playing Theremin: http://www.youtube.com/watch?v=h-3lU3bgOgE Hugh Le Caine: Electronic 
Sackbut: http://www.sciencetech.technomuses.ca/english/collection/music7.cfm Hugh Le Caine info site: 
http://www.hughlecaine.com Bucla s instruments: http://www.buchla.com/ Wikipage: http://en.wikipedia.org/wiki/Buchla 
Michel Waisvisz wikipage: http://en.wikipedia.org/wiki/Michel_Waisvisz 2005 talk at CHI2005: http://www.chi2005.org/program/prog_closing.html 
Lady s Glove, Laetitia Sonami http://www.sonami.net/lady_glove2.htm Video: http://www.sonami.net/Clips/VideoPerf_clips/China-Loose.mov 
Pebblebox video: http://www.youtube.com/watch?v=GEJCmrhrBjc http://www.sarc.qub.ac.uk/~somodhrain/palpable/projects.html#enactivemi 
 Info on circuit bending: http://en.wikipedia.org/wiki/Circuit_bending Info on Cracklebox: http://www.crackle.org/ 
 iPhone musical applications: Smule: http://ocarina.smule.com/ Tooka publications: NIME02: http://www.nime.org/2002/proceedings/paper/fels.pdf 
NIME04: http://www.nime.org/2004/NIME04/paper/NIME04_1A01.pdf OROBORO site: http://regexp.bjoern.org/archives/000159.html 
Jamodrum site: http://www.jamodrum.net/ Brain Opera information: http://park.org/Events/BrainOpera/ Penn 
playing with Brain Opera sensor chair video: http://www.media.mit.edu/~joep/MPEGs/penn.mpg Monome Open 
Hardware Interface http://monome.org  Visual Interfaces [Tarabella, 2004] Leonello Tarabella, 2004. 
Performance at NIME-04, Hamamatsu, Japan. [Fels &#38; Mase, 1999] Sidney S. Fels and Kenji Mase. Iamascope: 
A Graphical Musical Instrument. Computers and Graphics. Volume 2. No. 23. Pages 277-286. 1999. [Lyons 
et al., 2001] Lyons, M.J., Haehnel, M., and Tetsutani, N. 2001. The mouthesizer: A facial gesture musical 
interface. Conference Abstracts, ACM SIGGRAPH 2001. [Lyons, 2004] Lyons, M.J., Facial gesture interfaces 
for expression and communication. 2004 IEEE Conference on Systems, Man, and Cybernetics, 2004. [Lyons 
&#38; Tetsutani, 2001] Lyons, M.J. and Tetsutani, N. 2001, Facing the music: a facial action controlled 
musical interface, Conference on Human Factors in Computing Systems, (CHI 2001), Extended Abstracts, 
pp. 309 310. [Funk et al. 2005] Funk, M. and Kuwabara, K. and Lyons, M.J. 2005. Sonification of facial 
actions for musical expression, Proceedings of the 2005 conference on New interfaces for musical expression, 
pp. 127 131. [De Silva et al. 2004] A novel face-tracking mouth controller and its application to interacting 
with bioacoustic models. Proceedings of the 2004 conference on New interfaces for musical expression, 
pp. 169- 172. [Nishibori &#38; Iwai, 2006] Nishibori, Y. and Iwai, T., 2006. Tenori-On, Proceedings of 
the 2006 conference on New interfaces for musical expression, pp. 172 175 Education [Cook, 2001] Cook, 
P. 2001. Principles for designing computer music controllers, Proceedings of the 2001 conference on New 
interfaces for musical expression, Seattle WA. [Cook, 2007] Cook, P. 2007, Keynote Talk at the 2007 Conference 
on New Interfaces for Musical Expression, New York, NY. [Jorda, 2004] Jorda, S. 2004. Digital instruments 
and players: part I---efficiency and apprenticeship, Proceedings of the 2004 conference on New interfaces 
for musical expression pp. 59 63. [Singer, 2003] Singer, E. 2003. Sonic Banana: A novel bend-sensor-based 
midi controller, Proceedings of the 2003 conference on New interfaces for musical expression, pp. 220 
 221. Vienna Vegetable Orchestra http://www.gemueseorchester.org [Igoe, 2007] Igoe, T. 2007. Making 
Things Talk, O Reilly. [Verplank, Sapp, &#38; Mathews, 2001] Verplank, B. and Sapp, C. and Mathews, M., 
A Course on Controllers, Proceedings of the 2001 conference on New interfaces for musical expression, 
Seattle, WA. [Lippit, 2004] Lippit, T.M. 2004. Realtime sampling system for the turntablist version 
2: 16padjoystickcontroller, Proceedings of the 2004 conference on New interfaces for musical expression, 
pp. 211 212.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037639</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>57</pages>
		<display_no>3</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Applying color theory to digital media & visualization]]></title>
		<page_from>1</page_from>
		<page_to>57</page_to>
		<doi_number>10.1145/2037636.2037639</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037639</url>
		<abstract>
			<par><![CDATA[<p>This course highlights the visual impact of specific color combinations, provides practical suggestions on color mixing, and includes a hands-on session that teaches how to build and evaluate color schemes for digital media visualization.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808971</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa]]></first_name>
				<middle_name><![CDATA[Marie]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 APPLYING COLOR THEORY TO DIGITAL MEDIA &#38; VISUALIZATION  Theresa Marie Rhyne theresamarierhyne@gmail.com 
 IN THIS COURSE, WE HIGHLIGHT 5 TOPICS: Additive and Subtractive Color Models. Defining Color Gamut, 
Spaces and Systems. Selected Artistic Movements related to Color Theory. Case Studies pertaining to 
Colorizing Visualizations. Hands on Workshop using Online Color Tools  theresamarierhyne@gmail.com 
 FIRST, LET S REVIEW ADDITIVE AND SUBTRACTIVE COLOR MODELS: Red, Green Blue (RGB) - adding colors with 
light as seen on our color display monitors and with digital cameras.  Cyan, Magenta, Yellow and Key 
Black (CMYK) - subtracting colors with ink as seen from high resolution printouts and with our local 
printing devices.  Red, Yellow and Blue (RYB) - subtracting colors with paint like when we use a real 
paintbrush, crayons or makers.   theresamarierhyne@gmail.com RED, GREEN AND BLUE (RGB) -THE ADDITIVE 
COLOR MODEL OF LIGHTS:  Red, Green and Blue lights showing secondary colors. Open Source Image available 
at WIkipedia and created by en:User: Bb3dxv, see: http://en.wikipedia.org/wiki/File:RGB_illumination.jpg 
  theresamarierhyne@gmail.com CYAN, MAGENTA,YELLOW AND KEY BLACK (CMYK) -THE SUBTRACTIVE COLOR MODEL 
OF PRINTING:  Layers of simulated glass show how semi-transparent Cyan, Magenta and Yellow colors combine 
on paper. Open Source Image available at WIkipedia and created by Mirsad Todorovac, see: http://en.wikipedia.org/wiki/File:Color 
 subtractive-mixing-cropped.png 5 theresamarierhyne@gmail.com RED, YELLOW, AND BLUE (RYB) -THE PAINTER 
S SUBTRACTIVE COLOR MODEL:  Mixture of Red, Yellow and Blue primary colors used in art and design education, 
particularly in painting. Public Domain image available at WIkipedia and created cflm, see: http://en.wikipedia.org/wiki/File:Color_mixture.svg 
  6 theresamarierhyne@gmail.com VISUALLY SUMMARIZING COLOR MODELS: RGB adds with lights.  RYB subtracts 
to mix paints. CMYK subtracts for printing theresamarierhyne@gmail.com SOMETIMES OTHER COLOR MODELS 
ARE DEVELOPED TO SUPPORT SPECIFIC OUTPUT DEVICES:  Hexachrome (CMYKOG) - six color printing process 
once used: Cyan, Magenta, Yellow, Key Black, Orange and Green.  theresamarierhyne@gmail.com  NEXT, 
WE DEFINE SOME COLOR TERMINOLOGY: Color Model + Color Gamut = Color Space. International Commission 
on Illumination Color Space (CIE XYZ). Munsell Color System. Pantone Color Matching The Color Wheel 
and Color Schemes  theresamarierhyne@gmail.com COLOR GAMUT: THE SUBSET OF COLORS THAT CAN BE ACCURATELY 
REPRESENTED IN A GIVEN CIRCUMSTANCE.  The Color Gamut of a typical computer monitor. The grayed out 
portion represents the entire color range available. Open Source Image available at WIkipedia and created 
by Hankwang, see: http://en.wikipedia.org/ wiki/File:CIExy1931_srgb_gamut.png  theresamarierhyne@gmail.com 
 COLOR MODEL + COLOR GAMUT = COLOR SPACE.  RGB MODEL + COLOR GAMUT = COLOR SPACE Comparison of the 
color spectrum (shown as the large oval in the back) with RGB color spaces. This image shows that an 
Epson 2200 printer can produce colors outside sRGB and Adobe RGB color spaces. Open Source Image available 
at WIkipedia and created by Jeff Schewe, see: http://en.wikipedia.org/wiki/File:Colorspace.png  theresamarierhyne@gmail.com 
 COMPARISON OF RGB &#38; CMYK COLOR SPACES  Comparison of the RGB and CMYK colors models. This image 
depicts the differences between how colors appear on a color monitor (RGB) compared to how the colors 
reproduce in the CMYK print process. Public Domain Image available at WIkipedia and created by Annette 
Shacklett, see: http://en.wikipedia.org/wiki/ File:RGB_and_CMYK_comparison.png  theresamarierhyne@gmail.com 
 CIE XYZ COLOR SPACE: FROM THE INTERNATIONAL COMMISSION ON ILLUMINATION  The CIE XYZ color space is 
a based on experimental perception studies conducted by W. David Wright and John Guild in the 1920s. 
The CIE 1931 XYZ color space,shown above, is designed for matching calibrated displays or printers. 
Open Source Image available at WIkipedia and created by Paulschou, see: http://en.wikipedia.org/w/index.php? 
 title=File:Chromaticity_diagram_full.pdf&#38;page=1  13 theresamarierhyne@gmail.com  MUNSELL COLOR 
SYSTEM: A HUE, VALUE AND CHROMA COLOR SPACE  Hue: 5 principal hues of Red, Yellow, Green, Blue and Purple 
with 5 intermediate hues halfway between each principal. Each of these 10 steps is divided into 10 sub-steps 
to yield 100 hues with integer values.  Value: black (value 0) at the bottom to white (value 10) at 
the top.  Chroma: measured radially from the center of each slice.  Lower chroma value is less pure, 
more washed out like a pastel.  MUNSELL COLOR SYSTEM: A HUE, VALUE AND CHROMA COLOR SPACE  Open Source 
Image available at WIkipedia and created by Jacobolus, see: http:// en.wikipedia.org/wiki/File:Munsell 
system.svg Albert H. Munsell developed the Munsell Color System in the early 1900s. The 1929 Munsell 
Book of Color defined the fundamentals of the color space configuration shown above Open Source Image 
available at WIkipedia and created by SharkD, see: http:// commons.wikimedia.org/wiki/File:Munsell_1929_color_solid.png 
  15 theresamarierhyne@gmail.com  PANTONE COLOR MATCHING SYSTEM: USED FOR STANDARDIZING COLORS  1,114 
colors specified by their allocated number such as PMS 130 . Colors based on 15 pigments (13 base color 
pigments along with black &#38; white) that are mixed in specified amounts.  Aids in standardizing colors 
in the CMYK color printing process.  CMYK printing effectively reproduces a special subset of Pantone 
colors.  Open Source Image available at WIkipedia and created by Parhamr, see: http://en.wikipedia.org/wiki/ 
 File:PantoneFormulaGuide-solidMatte-2005edition.png  16 theresamarierhyne@gmail.com  THE COLOR WHEEL: 
ARRANGING COLORS HUES AROUND A CIRCLE  Primary Colors: set of colors combined to make a useful range 
of colors. RGB, CMY, and RYB are the most popular sets of primary colors.  Secondary Colors: produced 
by mixing primary colors. For RGB: Yellow, Cyan, Magenta For CMY: Blue, Green, Red For RYB: Orange, 
Green, Purple  Complementary Colors: colors opposite each other on the color wheel.  Public Domain 
Image available at WIkipedia and created by J. Arthur H. Hatt for The Colorist in 1908. See: http:// 
  en.wikipedia.org/wiki/File:RGV_color_wheel_1908.png  theresamarierhyne@gmail.com 17 USING THE COLOR 
WHEEL TO BUILD COLOR SCHEMES  Monochromatic: Different tints or shades of one color.  Analogous: colors 
adjacent to each other on the Color Wheel.  Split Analogous: a main color and two colors one space away 
from it on each side of the Color Wheel.  Complementary: colors opposite each other on the Color Wheel. 
 Split Complementary: a main color and two colors on each side of its complementary color.  Triadic: 
3 colors equally spaced on the Color Wheel.  Tetradic: Any 4 colors with a logical relationship on the 
Color Wheel such as 2 complementary pairs.  Others to be discussed in our Hands on Session. Public Domain 
Image available at WIkipedia and created by J. Arthur H. Hatt for The Colorist in 1908. See: http://en.wikipedia.org/wiki/ 
 File:RGV_color_wheel_1908.png  18 theresamarierhyne@gmail.com  NOW, LET S EXAMINE SELECTED ARTISTIC 
MOVEMENTS PERTAINING TO COLOR THEORY: Pointillism (as well as Impressionism, Divisionism and Ben Day 
Dots). Fauvism. Color Field Painting. Bauhaus teachings of Color Theory.  theresamarierhyne@gmail.com 
 POINTILLISM: BUILDING AN IMAGE FROM SEPARATE DOTS OF PAINT  Technique of painting developed by George 
Seurat in 1886. Image on right is from his La Parade de Cirque painting. Relies on the eye and mind 
of the viewer to compose the color dots into a broader range of tones.  Pointillism is an outgrowth 
of the Impressionism art movement.  Impressionism paintings noted for visible brush strokes &#38; emphasis 
on light.  Divisionism is a variant of Pointillism that  focuses on color theory. Public Domain Image 
available at WIkipedia (expired Copyright) , see: http://en.wikipedia.org/wiki/File:Seurat La_Parade_detail.jpg 
  theresamarierhyne@gmail.com  MANY OTHER METHODS ARE ANALOGOUS TO POINTILLISM  The CMYK printing 
process used by color printers is dot based.  Television and Computer Monitors use a pointillist method 
to represent image with the RGB color model.  Ben-Day Dots printing process uses colored dots that are 
widely spaced, closely spaced or overlapping to create optical illusions.  The illustrator Benjamin 
Day developed the method that has been used in comic books. The artist, Roy Lichtenstein, enlarged and 
exaggerated the method.  Image created by Theresa-Marie Rhyne to study Divisionism and Ben Day dots. 
See: http:// web.me.com/tmrhyne/Theresa-Marie_Rhynes_Viewpoint/Blog/Entries/ 2009/12/3_Exploring_Divisionism_in_Computer_Graphics.html 
 theresamarierhyne@gmail.com 21 FAUVISM: STRONG COLOR EMPHASIZED OVER REPRESENTATIONAL OR REALISM  
Short lived art movement from 1905 to 1907. Leading artists were Henri Matisse and Andre Derain. Image 
on right is Matisse s Woman with a Hat oil painting created in 1905.  The grouping of artists were called 
Les Fauves or The Wild Beasts .  Painterly qualities of wild brush work and saturated color. The subject 
matter was simplified and sometimes abstract. Public Domain Image in the USA available at WIkipedia 
 (image created before 1923) , see: http://en.wikipedia.org/ wiki/File:Matisse-Woman-with-a-Hat.jpg 
 theresamarierhyne@gmail.com  COLOR FIELD PAINTING: LARGE AMOUNTS OF FLAT SOLID COLOR SPREAD ACROSS 
A CANVAS Abstract Expressionism art movement that emerged during the 1940s and continued into the 1950s 
and 1960s. Leading artists were / are Kenneth Noland, Gene Davis, Ellsworth Kelly, Helen Frankenthaler, 
Anne Truitt, Jack Bush, and Frank Stella. Image on right is Frank Stella s Ragga II painting created 
in 1970. Large canvas areas of pure color that created areas of unbroken surface and a flat picture 
plane. Fair Use Image in the USA taken by Theresa-Marie Rhyne. Ragga II painting and Copyright by Frank 
Stella, 1970. Dimensions: 120 by 300 inches. In the collection of the North Carolina Museum of Still 
an active abstract painting style today. Art, see: http://collection.ncartmuseum.org/collection11/view/ 
 objects/asitem/People$0040185/0? t:state:flow=57b04465-42fb-4180-ab3b-b7063019d013  theresamarierhyne@gmail.com 
23  NEXT, WE HIGHLIGHT SELECTED BAUHAUS TEACHINGS OF COLOR THEORY:  The Bauhaus: School in Germany 
with pioneering approaches to teaching design. School operated from 1919 to 1933. Many instructors continued 
developing their teachings after 1933.  Paul Klee: The Diaries of Paul Klee 1898 -1918 . Wassily Kandinsky: 
Concerning the Spiritual in Art .  Johannes Itten: The Art of Color: the subjective experience and 
objective rationale of color .  Josef Albers: The Interaction of Color  theresamarierhyne@gmail.com 
 SPECIFIC FOCUS ON JOSEF ALBERS:  German born American artist &#38; educator. Taught at the Bauhaus, 
eventually immigrating to the USA to teach at Black Mountain College &#38; Yale University.  Created 
color studies entitled Homage to the Square for a 25 year period starting ~1950.  In 1963, published 
The Interaction of Color detailing his color theories.  Fair Use Image in the USA taken by Theresa-Marie 
Rhyne. Homage to the Square color studies created by Josef Albers and on display at the North Carolina 
Museum of Art. Copyright by the Josef &#38; Anni Albers estate &#38; foundation. http://collection.ncartmuseum.org/collection11/view/ 
objects/asitem/People$0040176/4;jsessionid=8A427C773907338158799E73A053A5C7? t:state:flow=276f0018-1877-46d0-9c16-6cb0ad65949d 
 theresamarierhyne@gmail.com CASE STUDY #1: COLORIZING HOUSEHOLD BROADBAND AVAILABILITY  Visualization 
based on Household Broadband Availability data for the 100 Counties in the State of North Carolina from 
the years of 2002 through 2007. Information and data provided by the e-NC Authority.  theresamarierhyne@gmail.com 
 USING THE COLORBREWER 2.0 TOOL TO DEVELOP COLORMAPS:  The ColorBrewer tool was conceptualized with 
color schemes by Cynthia A. Brewer with interface design and software development by Mark Harrower and 
others (both in the Department of Geography at Pennsylvania State University). See: (http://colorbrewer2.org/). 
 27 theresamarierhyne@gmail.com   COLORBREWER S COLOR SCHEME CONCEPTS:  Sequential Schemes: optimized 
for ordered data from low to high.  Diverging Schemes: places equal emphasis on mid-range critical 
values as well as extreme values.  Qualitative Schemes: does not imply magnitude differences and suited 
for representing nominal or categorial data. The ColorBrewer tool was conceptualized with color schemes 
by Cynthia A. Brewer with interface design and software development by Mark Harrower and others (both 
in the Department of Geography at Pennsylvania State University). See: (http://colorbrewer2.org/). theresamarierhyne@gmail.com 
  CASE STUDY #2: COLORIZING A HURRICANE  Visualization based on a Hurricane Katrina model run at 2 
kilometer grid resolution using the Weather Research Forecast (WRF) model. The animation shows rain isosurfaces, 
with the purple areas being locations of heaviest rainfall. Dark blue areas are land masses.  29 theresamarierhyne@gmail.com 
   HERE, WE HIGHLIGHT 3 TOPICS: Applying Color Theory to a time series animation of Hurricane Katrina. 
Using Adobe s Kuler tool to analyze an existing Color Scheme. Working with the Color Brewer tool to 
build the Tropical Storm Animation Color Scheme.  theresamarierhyne@gmail.com FIRST, LET S USE ADOBE 
S KULER TOOL TO ANALYZE THE COLORS IN OUR HURRICANE VISUALIZATION:  Adobe s Kuler tool allows us to 
analyze the colors in a JPEG image. We can save the resulting color palettes for future  work. See: 
http://kuler.adobe.com/ 31 theresamarierhyne@gmail.com  KEY ELEMENTS OF COLOR MAP DESIGN : Establish 
Color Maps based on flow of Animation Sequences rather than Static Image Displays. ColorBrewer tool 
helps to Mock-Up Color Maps. From the Mock-Up develop the Final Color Maps with the visualization &#38; 
animation tool (VisIt).  theresamarierhyne@gmail.com  USING THE COLORBREWER TOOL TO DEVELOP COLORMAPS: 
 The ColorBrewer tool was conceptualized with color schemes by Cynthia A. Brewer with interface design 
and software development by Mark Harrower (both in the Department of Geography at Pennsylvania State 
University). See: (http://www.personal.psu.edu/cab38/ColorBrewer/ColorBrewer.html).  33 theresamarierhyne@gmail.com 
 FRAME FROM ANIMATION SEQUENCE SHOWING COLOR MAPS:  Visualization programming by Steve Chall with Colorization 
executed by Theresa-Marie Rhyne at RENCI@NCSU. Created in the VisIt open source visualization tool from 
weather model data based on Hurricane Katrina, see: (http:www.llnl.gov/  VisIt). 34 theresamarierhyne@gmail.com 
 A COLOR SCHEME ANALYSIS OF OUR HURRICANE  Using Color Scheme Designer, we see that our hurricane colors 
form an analogous color scheme of Magenta, Purple and Blue. Our wind vectors, in Orange, from a complementary 
color scheme to our Blue ocean background. theresamarierhyne@gmail.com SNAPSHOT OF ANIMATION SEQUENCE 
EVOLVING IN A NON-LINEAR EDITING SYSTEM:  Time series based on Hurricane Katrina model run at a 2 kilometer 
grid resolution using the Weather Research Forecast (WRF) model. The animation shows rain isosurfaces, 
with the purple areas being locations of heaviest rainfall. Dark blue areas are land masses.  theresamarierhyne@gmail.com 
 CASE STUDY #3: COLORIZING A SUPERNOVA  Visualization based on astrophysics data of a supernova shock 
wave. The computational model was executed on a high performance computer and visualized with CEI s Ensight 
Visualization software. (http://www.ensight.com)  theresamarierhyne@gmail.com   HERE, WE HIGHLIGHT 
2 TOPICS: Building the analogous and complementary color schemes with Color Scheme Designer. Using 
Adobe s Kuler tool to analyze an existing Color Scheme.  theresamarierhyne@gmail.com  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037640</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>100</pages>
		<display_no>4</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Build your own glasses-free 3D display]]></title>
		<page_from>1</page_from>
		<page_to>100</page_to>
		<doi_number>10.1145/2037636.2037640</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037640</url>
		<abstract>
			<par><![CDATA[<p>Driven by the recent resurgence of 3D cinema, depth cameras and stereoscopic displays are becoming commonplace in the consumer market. Introduced last October, Microsoft Kinect has already fostered gesture-based interaction for applications well beyond the intended Xbox 360 platform. Similarly, consumer electronics manufacturers have begun selling stereoscopic displays and inexpensive stereoscopic cameras. Most commercial 3D displays continue to require cumbersome eyewear, but inexpensive, glasses-free 3D displays are imminent with the release of the Nintendo 3DS.</p> <p>At SIGGRAPH 2010, the Build Your Own 3D Display course demonstrated how to construct both LCD shutter glasses and glasses-free lenticular screens, providing Matlab-based code for batch encoding of 3D imagery. This follow-up course focuses more narrowly on glasses-free displays, describing in greater detail the practical aspects of real-time, OpenGL-based encoding for such multi-view, spatially multiplexed displays.</p> <p>The course reviews historical and perceptual aspects, emphasizing the goal of achieving disparity, motion parallax, accommodation, and convergence cues without glasses. It summarizes state-of-the-art methods and areas of active research. And it provides a step-by-step tutorial on how to construct a lenticular display. The course concludes with an extended question-and-answer session, during which prototype hardware is available for inspection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808972</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808973</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
    Monocular Depth Cues with Conventional Displays relative and familiar size  perspective and 
occlusion  texture gradient, shading and lighting, atmospheric effects   Additional Monocular Depth 
Cues motion parallax [Hermann von Helmholtz, 1866]  accommodation  It being thus established that 
the mind perceives an object of three dimensions by means of the two dissimilar pictures projected by 
it on the two retinae, the following question occurs: What would be the visual effect of simultaneously 
presenting to each eye, instead of the object itself, its projection on a plane surface as it appears 
to that eye? Binocular Depth Cues retinal disparity [Charles Wheatstone, 1838]  convergence  Also 
sometimes called psychological cues, these effects can be captured by a monocular camera and perceived 
by a viewer of a traditional screen. These effects are due to physical functions of the eye, and not 
interpretation of the scene  magiceye.com Ponzo Illusion: &#38;#169; Walt Anthony 2006 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037641</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>18</pages>
		<display_no>5</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Character rigging, deformations, and simulations in film and game production]]></title>
		<page_from>1</page_from>
		<page_to>18</page_to>
		<doi_number>10.1145/2037636.2037641</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037641</url>
		<abstract>
			<par><![CDATA[<p>This course focuses on rigging, deformations, dynamics, and production practices in animation, visual effects and game development. Topics include analysis of performance requirements, motion system set-up, procedural rigging for secondary animation, and efficient extension of techniques over a wide range of primary and secondary characters.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[deformations]]></kw>
			<kw><![CDATA[muscle simulation]]></kw>
			<kw><![CDATA[real-time]]></kw>
			<kw><![CDATA[rigging]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2808974</person_id>
				<author_profile_id><![CDATA[81332515066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McLaughlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808975</person_id>
				<author_profile_id><![CDATA[81410595359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cutler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808976</person_id>
				<author_profile_id><![CDATA[81488657541]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401095</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grignon, R., Huang, M., and Vogt, R. 2008. Merging bipedal and quadrupedal functionality into one rig for madagascar: Escape 2 africa. In <i>ACM SIGGRAPH 2008 talks</i>, ACM, New York, NY, USA, SIGGRAPH '08, 49:1--49:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276466</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Joshi, P., Meyer, M., DeRose, T., Green, B., and Sanocki, T. 2007. Harmonic coordinates for character articulation. ACM Trans. Graph. 26 (July).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846278</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ka&#269;i&#263;-alesi&#263;, Z., Nordenstam, M., and Bullock, D. 2003. A practical dynamics system. In <i>Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, SCA '03, 7--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344862</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lewis, J. P., Cordner, M., and Fong, N. 2000. Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, SIGGRAPH '00, 165--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281660</ref_obj_id>
				<ref_obj_pid>1281500</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[McLaughlin, T., and Sumida, S. S. 2007. The morphology of digital creatures. In <i>ACM SIGGRAPH 2007 courses</i>, ACM, New York, NY, USA, SIGGRAPH '07.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198692</ref_obj_id>
				<ref_obj_pid>1198555</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Mclaughlin, T. 2005. Taxonomy of digital creatures: interpreting character designs as computer graphics techniques: Copyright restrictions prevent acm from providing the full text for this work. In <i>ACM SIGGRAPH 2005 Courses</i>, ACM, New York, NY, USA, SIGGRAPH '05.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185808</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Mclaughlin, T. 2006. Taxonomy of digital creatures: defining character development techniques based upon scope of use. In <i>ACM SIGGRAPH 2006 Courses</i>, ACM, New York, NY, USA, SIGGRAPH '06.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186287</ref_obj_id>
				<ref_obj_pid>1186223</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Monks, C., Newall, M., Phillips, C., Popravka, N., Smith, J., and Tooley, J. 2004. Posing as a werewolf: the creature matchmove tool used for "van helsing": Copyright restrictions prevent acm from providing the full text for this work. In <i>ACM SIGGRAPH 2004 Sketches</i>, ACM, New York, NY, USA, SIGGRAPH '04, 51--.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1522365</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[O'Neill, R. 2008. <i>Digital Character Development: Theory and Practice</i>. Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pe&#241;a, B. A. 2011. <i>Performance-guided character bind pose for deformations</i>. Master's thesis, Texas A&M University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Character Rigging, Deformations, and Simulations in Film and Game Production Tim McLaughlin* Larry 
Cutler David Coleman Department of Visualization DreamWorks Animation Electronic Arts Texas A&#38;M 
University  Figure 1: Screen shots of digital characters from visual effects, games, and feature animation 
projects. Abstract Digital characters are common in animation, visual effects, and real-time applications 
using computer graphics. As the use of digital character technology has spread approaches to rigging, 
deformations, and simulations continue to share a foundation of best practices, but have also evolved 
in unique ways speci.c to the exhibition format. This course encompasses technical, process oriented, 
aesthetic, and performance concerns in exploration of the setup and use of digital characters. Through 
the explanation of fundamental skills, articulation of common practices, examination of speci.c challenges, 
and illumination through practical examples this course delivers a comprehensive production-oriented 
view of character rigging in .lm and game production. The authors are veteran production artists with 
deep histories in digital character related problem solving. CR Categories: I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism Animation; Keywords: rigging, animation, deformations, real-time, 
muscle simulation Links: DL PDF 1 Introduction In recent years we have been witness to massive growth 
in the use of computer character animation. The diversity of animated characters ranges from photo-realistic 
syn-thespians, such as the sea-creature pirates in the feature .lm series Pirates of the Caribbean (2006 
&#38; 2007), the visually abstracted characters in the Kung Fu Panda *e-mail: timm@viz.tamu.edu e-mail:Larry.Cutler@pdi.dreamworks.com 
e-mail:dcoleman@ea.com animated features (2008 &#38; 2011), and the avatar-athletes, -soldiers, and -animals 
in video games such as the Madden Football series and Fight Night (2011). Despite the variety of performance 
expectations for computer generated characters in visual story-telling, a common fundamental technical 
core is shared between visual effects, feature animation, and interactive game projects. On top of the 
foundation, within the each pipeline, speci.c approaches to skeletal de.nition, control systems, deformation 
systems, dynamics systems, and even terminology differ. Recognition of the similarities and differences 
are a highlight of this course. The speci.c issues covered in this course are considerations and methodologies 
for motion systems, controls systems, deformation systems, and muscle systems as seen through the lenses 
of digital character development production for feature .lm visual effects, feature .lm animation, and 
games. This course is not an academic exploration, but instead takes a production-minded approach. As 
such, each method describe is seen as an expedient to reaching an artistic goal where time, budget, and 
production size are key factors. Despite large growth in the use of digital characters teaching and learning 
in the area of digital character development has not advanced far outside the tomes of software speci.c 
approaches. The craft of facilitating excellent performances from digital characters is often hidden 
behind a description of the power of a tool or the glow of an animator. While powerful and agile software 
is highly valued, and there would be no use for digital characters without their puppeteers, as authors 
of this course we are collectively interested in promoting awareness of the .eld and the spread of knowledge 
therein. We have seen software come and go. We are comfortable with and accommodating of uses of key-framing, 
motion-capture, and procedural animation. We believe that the ability to assess performance requirements 
and production limitations is a skill that can be learned and one that transcends changes in tools and 
visual styles. This course is organized according to three common uses of digital characters: as visual 
effects in feature .lms, as performers in feature animation, and as avatars in games. The respective 
author of each section explores issues regarding rigging, deformations, and the organic movement of skin 
using both practical examples and in general terms. 2 Digital Character Development for Visual Effects 
2.1 Overview of Foundation-Level Issues 2.1.1 Integration with Live Action Integration with live-action 
photography is the primary concern for all computer generated elements in a visual effects project, including 
digital characters. Digital characters in live action projects are surrounded by a real environment full 
of objects, people, animals, and actions that are constrained by the natural laws of physics and evolutionary 
allometry. The photographic elements provide constant visual touchstones for viewers marking the boundaries 
of the willing suspension of disbelief. It is within this constrained space that digital artists must 
work including those responsible for the rigging and deformations of digital characters. A common challenge 
faced by character development artists is managing the designs of digital characters that deviate from 
representations of the animal kingdom into the realm of fancifully abstracted forms [McLaughlin and 
Sumida 2007]. Arguably, creating a fantasy creature is an easier task than creating a believable facsimile 
of an animal of person familiar to the viewers. The approaches to problem solving discussed in this course 
are intended to apply to both conditions. Fortunately for the process of developing digital character 
for visual effects projects, the visual effects production pipeline affords opportunities to solve problems 
at the front end of production and opportunities to correct de.ciencies at the back-end. 2.1.2 Deference 
to the Director s Vision Performances for digital characters in live action projects are dictated by 
the .lm s director, executed by actors, animators, or procedural systems, and constrained by a project 
s budget. In this environment it is possible to understand in very explicit terms the challenges faced 
such as how often, how close, from what angles, and in what lighting conditions the digital characters 
will be seen. This marks a key difference between character development work for visual effects projects 
and character development work for games -in which the viewer controls the camera, and feature animation 
-in which robust solutions are more often required due to the higher number of appearances and greater 
range of performance requirements. A similarity between visual effects, game, and feature animation 
character development work is that animators must be provided with robust and precise controls over 
the character yet not burdened with all of the effects that will eventually combine to create the character 
s performance. Despite the constraints of realism and the capacity for detailed understandings of how 
a character will be seen, a character development artist for visual effects must resist the urge to 
chase solutions that satisfy physics or tightly de.ned understandings of the problem set. At the end 
of the day, when worked is reviewed, only the director can declare a shot .nal, or complete. This means 
that .exibility must be preserved within the production process and internally driven artistic principles 
must be held in check.  2.2 Analyzing Performance Requirements 2.2.1 How Will the Character Be Seen? 
Before embarking on the task of developing a digital character for a visual effects project three key 
questions must be answered [McLaughlin 2005] [McLaughlin 2006]. 1. What does the character do? 2. How 
close the camera will the character be? 3. How many times will the character be seen (how many shots)? 
  Figure 2: Storyboards from Van Helsing showing the camera relationship (size in screen and framing) 
to the transformation effect for Velkan from human to werewolf. c &#38;#169;2004 Universal Pictures. 
All rights reserved. If there will be many occasions for the character to be seen, many angles from which 
it will be seen, and many opportunities for viewer s to compare the character to natural forms the creature 
development task will require a high level of accomplishment across a wide range of computer graphics 
techniques. Storyboards (Figure 2) have long provided excellent reference for CG artists for the size 
in frame and action expected from digital characters. The increasingly wide-spread use of pre-visualization 
provides ever greater speci.cation through more accurate modeling, motion of the character and camera, 
and inferences of the lighting environment (Figure 3). 2.2.2 Design Style Live action visual effects 
projects that utilize green-screen techniques extensively to place live-action actors in synthetic environments 
minimize the number and range of reference points viewers will use to make decisions about the plausibility 
of digital characters. A fully, or majority, synthetic world can create a aesthetic that deviates from 
the natural world with acceptable results for maintaining the viewer s engagement. In such situations 
the design style of the character, in particular the shape language of the character, should be in tune 
with the design style of the environment. When the form of a digital character deviates from the natural 
world so too can performance expectations. Digital characters of unnatural shape, proportion, and size 
are often held by viewer s to more Figure 3: Screenshot from the pre-visualization animatic for War 
of the Worlds providing an accurate guide for camera angle, placement, lens, movement of camera, movement 
of characters and even an approximation of depth of .eld. c &#38;#169;2005 Paramount Pictures. All rights 
reserved. forgiving exhibitions of the laws of physics. Physical implausibility -such as a dog that 
is as large as a dump-truck, or a chimeric creature with anatomical features referencing a variety of 
animals -provides affordances that can and should be used. Figure 4 shows early creature concept art 
from the .lm Dreamcatcher (2003) in which the anatomy, including joint placement, musculature, and skin 
type of the character was abstracted to a high degree away from natural forms. Viewer s either don t 
have accurate memory references or will apply inaccurate references -as is often the case for known animals 
shown over-sized, for how the creature should move. These affordances are extended to the range of motion 
of limbs, rapidity of motion, gait changes due to speed, gaits used, skin behavior (visual cues for thickness 
and weight), and muscle activity. Conversely, physical accuracy in the form of a digital character reduces 
affordances and requires constraints. These statements are not intended to convey the preference for 
one visual style over another, but instead to point out the opportunities and restrictions that differ 
among them.  2.3 Rigging for Visual Effects 2.3.1 Types of Systems &#38; Rigs Within the domain of developing 
digital characters artists must have an understanding of three connected systems, each with its own knowledge 
and skill requirements [O Neill 2008]: Motion system -the combination of joints that provides the characters 
skeletal system.  Control system -the connection of controls to the joints through which the motion 
system can be puppeteered.  Deformations system -the de.nition of how the geometry of the model will 
move relative to the animation of the motion system.  Each has its own set-up process and knowledge 
requirements. Successful creation of a motion system requires a solid understanding of anatomy, range 
of motion, and kinematics. The design of a good control system requires a good understanding of how artists 
interact with a character model, and the capacity to design elegant (easy &#38; fast) solutions to complex 
motion problems. The creation of a good deformation system also requires a solid understanding of anatomy 
and a sculptor s touch when it comes to preserving a model s form and details during motion. The term 
rigging is often used to de.ne all three, or sometimes just the motion and control system. Figure 4: 
Early creature concept art for the character Mr. Gray B from the movie Dreamcatcher showing highly abstracted 
anatomy. c&#38;#169;2003 Castle Rock Entertainment. All rights reserved. Most visual effects companies 
employ a standard rigging tool set accessed through a 3D animation program. The tool set must accommodate 
the form of motion input, variety of artists using the rig(s), and the capacity to output .les or forms 
of data required by other parts of the production pipeline. A rigging tool set varies based upon the 
type of rig to be built and used. Typical rig types used in visual effects production include: Motion 
capture rig -a motion system onto which data from performance capture sessions is keyed; usually used 
as an input to the performance rig.  Motion tracking rig -a motion, control, and deformation (sometimes) 
system used by artists responsible for rotoscoping the action of a digital character over the action 
of a live actor in a .lmed shot [Monks et al. 2004].  Performance rig -a motion and control system providing 
control of the character to animation artists.  Simulation rig -a motion system built of dynamic objects 
such as pins, springs, soft-and rigid-body objects, designed to work with a dynamics solver; usually 
used as either an input to the performance rig or as a receiver of motion from the performance rig [Kaci.c 
et al. 2003]. c-Alesi  Deformation rig -a motion and control system used to provide re.ned input to 
the deformation system for the behavior of the character s skin relative to the motion of the performance 
rig; sometimes combined with sculpted shapes keyed to activate with skeletal motion.  Facial rig -a 
motion, control, and deformation system for the expressive areas of a character s face that may be composed 
of joints, deformers, simulation objects, modeled shapes, or any variation of these techniques.  Muscle 
rig -a motion and control system that can be composed of joints, simulation objects, or a mixture that 
de.nes how the muscular system of the character will work relative to the  motion of the performance 
rig. Clothing and accoutrements rig -a motion and control system of joints, simulation objects, or a 
mixture of both that de.nes how objects such a a character s wardrobe, jewelry, and dangling tools or 
weapons, will move relative to the motion of the performance rig. 2.3.2 Rig Users Each of the above-mentioned 
rigs may be used by a different group of artists, each group with its own set of expectations about the 
display of elements, interaction speed, and technical expertise required to use the rig effectively. 
The character development artist must be cognizant and respectful of the end-user s expectations. A solid 
starting point for determining the elements that vary and those that are consistent is through understanding 
the interaction method. Will the artist(s) primarily be using 3D viewports to interact with the character 
or will the artist primarily be using GUIs with number and text input, sliders, and toggles?  What level 
of surface .delity is needed by the artist(s)?  3D viewport interaction, with WYSIWYG connection between 
the input device (typically a mouse) and rig reaction, requires lowlatency as artists don t want to 
wait for the rig to update. The interaction speed requirement will effect every decision from the number 
of joints through the use of deformation and simulation techniques. User s who primarily interact with 
the rig through sliders, toggles, text boxes, and menus are typically more tolerant of some delay in 
the reaction of the rig. A valuable measure for the ef.ciency of the rig is interactive feedback for 
the frame-rate with which the software/hardware con.guration can play un-cached animation. Tolerances 
vary from artists to artist and from task to task. Interaction speed at 12-fps is generally the lowest 
tolerated limit by visual effects animators tasked with creating active and nuanced performance animation. 
Rig users, particularly when part of a large team, will perform best when the rig elements are consistent 
across the character, between characters, and across rig types. An icon form in the 3D, when selected 
and moved, should perform the same type of transformation on the character. For example: a cube always 
represents a control that can be translated but not rotated; a red control represents an animatable 
object on the right side of the character s body; a ghosted control represents an object that is not 
usable when its parent is in IK mode, etc. Name consistency for nodes within a character rig and for 
the names of rig .les is also important. Consistency across a production is referred to as universality. 
Universality contributes to knowledge transfer and thus to the ef.ciency of a production. 2.3.3 Interoperability 
Universality is not be confused with interoperability. Interoperability is the capacity for information 
and data to .ow loss-lessly through a pipeline. Interoperability is speci.cally important to the development 
of digital characters because control and deformation systems are notoriously speci.c to individual software 
packages. While the de.nition of geometry is reasonably widely portable the de.nition of kinematic systems, 
constraints, expressions, and deformer operations on geometry is not. The character development artist 
must be cognizant of points in the pipeline that reduce the .ow of information. A common example is that 
the animation control system cannot transfer from the software that the performance animator uses to 
the software that the character effects artist uses. The result is that the motion of the effect of 
the performance rig on geometry must be provided on a per-frame basis, independent of hierarchy, and 
respectful of local-space versus global-space transformations. This process is often called baking. The 
result of deformation systems is often baked as per-frame representations of geometry (geometry cache). 
A rig developer must recognize these restrictions on interoperability and provide ef.cient mechanisms 
for end-user artists to include or exclude elements for hand-off. Ideally, a rig and its geometry will 
move live through the pipeline thus providing easily managed opportunities for changes to be made late 
in the production process. There are several notable efforts in progress to provide options for interoperability 
in rigging and deformations. One effort is the result of the uni.cation of a variety of 3D animation 
software packages under a single parent company -Autodesk. Another is through the openSource community 
-openSourceVFX. One of the driving forces behind the push for greater interoperability is the distribution 
of feature .lm projects across multiple visual effects studios. Sometimes the work is divided such that 
assets need not be shared, but the continual drive to greater ef.ciency is forcing productions to explore 
ways in which multiple studios can work together on the same sequences, shots, and using the same assets. 
The work from one studio must be loss-lessly shared with other studios, whether generated through the 
use of proprietary tools or commercial off-the-shelf software. When this requirement exists it can seriously 
limit the options available for determining solutions to character development problems. 2.3.4 Joint 
Number and Placement Many visual effects production houses and a variety of commercial software packages 
have developed procedural rigging tools. The goal of these tools is three-fold: 1. Decrease time spent 
building motion, control, and deformation systems by making common processes repeatable through a button 
push or by running a script. 2. Promote universality through coding hierarchy structure, naming conventions, 
control forms, and the behavior of the motion rig relative to controls. 3. Capture technique development 
for future use as modules that may be combined together and/or expanded upon.  Most procedural rig building 
tools are script-based (Python is a commonly used scripting language). They typically require the user 
to de.ne a set of pivot points for joints that correspond to the anatomy of the character. From that 
template the script will build joint chains, install control systems, and even de.ne basic deformations. 
The foundation of scripted rig building is de.nition of the number and location of joints. For human 
models, such as a digital double used to replace a live actor in a stunt, joint placement can be determined 
by a motion capture rig calibrated to the actor s proportions. If not, careful consideration and compensation 
must be made for the actor s body form (muscle and fat, clothing) to determine correct pivot positions. 
Standard skeletal references can be highly useful though several issues must be considered which will 
lead to necessary deviation from the anatomical guide: Accuracy to the number of bones in the body would 
be wasteful; some bone have no affect on the articulation of the body and/or some movements may not 
be seen on screen.  Accuracy to the number of bones in the body would also be wasteful because a small 
number of CG joints can portray the same visible .exibility as a larger number of anatomical bones (for 
example, the human spine is composed of 5 lumbar vertebrae, 12 thoracic vertebrae, and 7 cervical vertebrae, 
yet a rea  sonable facsimile for a mid-ground or stunt double CG character in visual effects can be 
produced using 3 lumbar joints, 1 spine joint -the rib cage restricts most of the motion of the cervical 
vertebrae, and 3 neck joints). Accuracy to the anatomical placement of joints fails to compensate for 
the role that internal organs, muscle, fat, and skin thickness play de.ning how skin moves relative to 
skeletal movement. Better visual results are often achieved by biasing the location of bones toward the 
center of the geometry being deformed. Whether through scripted rig building or through hand placement 
a naming convention for joints should be established early. The convention used for joints is typically 
extended as the root for successive features of the rig. As mentioned above, multiple rigs are likely 
to be used for a visual effects character. The performance rig, in most cases, will form the foundation 
for skeletal motion. As such, the performance rig should have the capacity to absorb motion from the 
motion capture rig or motion tracking rig as modi.able input. On the output side of the performance rig, 
skeletal motion is used as a modi.able input to the simulation rig and deformation rig. An easy way to 
picture these relationships is to imagine the digital double for a .ying superhero. The performance 
rig is key-frame animated by an animator to .y alongside a jetliner at the beginning of the shot then 
rocket past the the jetliner at the end of the shot. An actor is motion captured on stage performing 
the action of waving at the jetliner. The motion capture performance is layered onto the key-framed path 
of the superhero. When the superhero rockets off high frequency vibration in his trailing legs provides 
visual indicator of his speed. This motion is created as a physical simulation and layered onto the 
output of the performance rig. Within the system, artists have the capacity to modify the performance 
to look exactly like the motion capture, exactly like the key-framed performance, exactly like the simulation, 
or somewhere in-between. 2.3.5 Control System Design The control system is the animator s access point 
for de.ning the action of the character. It is through the control system that large scale motion, localized 
motion, and blended actions, like those described in the section above, are managed. The control system 
is comprised of the objects that animators will interact with and the de.nition of how those objects 
affect the skeletal model. The design of the control system is where character development artists must 
begin to take on the role of service provider and solver of problems. Like good software design, good 
control system design begins with careful attention to the needs to the users. Talking, listening, and 
having an open mind are essential. As important as those, is the capacity to perform rapid-prototyping. 
The goal is to work through options with key players iteratively. It is typically a wasteful experience 
to disappear for long periods of time into the mode of developing a complete rig. Ideas can be tested 
as toy rigs, which are often simpli.cations of a problem or portion of a model stripped to only essential 
ingredients. Animators in visual effects production tend to be more willing than their counterparts in 
feature animation to accept solutions that are relatively technical or cumbersome as long as the upside 
is a high level of control. This attitude likely has its roots in the fact that visual effects work often 
requires one-off solutions. Visual effects character work often focuses on getting one or a handful 
of money shots just right as opposed to focusing on getting hundreds of relatively similar shots completed. 
A simple example is the use of a switch to change the behavior of an animatable appendage, such as 
an arm, from forward-kinematics (FK) to inverses-kinematics (IK). Including an FK/IK switch in a rig 
for feature animation is fundamental. In visual effects, it could easily be true that FK would be useful 
for only one shot out of a dozen so unless the switch is already built in as a component of an automated 
rig generation system the ef.cient choice would be to not include it. The animator for the one shot where 
FK is needed would either counter-animate the action, strip the IK from the rig, or request a one-off 
solution. Control systems undergo many more alterations than do motion systems. Thus, it is wise to employ 
a scripted rig building process in which the control system can be ef.ciently stripped from the motion 
system without altering bone placement, number of bones, or hierarchy. It is also wise to employ an asset 
management system and revision control system. A smart asset control system will allow character development 
artists to release rig revisions for use, noting changes, and keeping track of who is using which rig. 
Revision control prevents multiple character development artists from over-writing each other s work 
by noting who is working on what and locking access when changes are in process. When combined with referencing, 
character development artists have a fairly robust method for continuing development and improvement 
of rigs while in production. The goal is to enact changes as loss-lessly as possible. 2.3.6 Driving 
Points Driving points are the elements within a rig that are the originators of the motion of other 
elements. For example, within bipedal motion the hips are the originators of locomotion, thus the hips 
controller in a rig is a driving point. Hierarchically in the model, and through visual inspection of 
the controls, this fact should be clear. For a .sh or a snake, the head is the primary driving point. 
For a bird the driving point is between the wings. When an animator opens a scene containing a rigged 
model the order of ef.cient animation should be obvious via the visual design and layout of the controllers. 
Debate about which elements are superior driving points relative to others is not nearly as important 
as recognition that driving points are important the the creation of motion and their rapid identi.cation 
is important to ef.cient production. The capacity to change driving points in a model, even mid-shot, 
is valuable (see the bi-quad setup in the section on rigging for feature animation for more on this 
topic). 2.3.7 Limits on Range of Motion In key-frame animation it is possible for animators to strike 
poses with models that are antomically impossible and that exceeds the bounds of physics. When the source 
of action is motion capture one could naively presume that only physically possible poses would result, 
however, if re-targeting is used to apply motion from captured actions to a model that is proportionally 
different than the source it is possible to encounter physically implausible poses. Physics simulations 
for character dynamics, including .esh, hair, and clothing, are notorious for reacting poorly to poses 
that cause interpenetrations and movements that include rapid changes in velocity. These problems raise 
the question of whether or not a rig should contain restrictions on posing. As mentioned earlier, one 
of the affordances of visual effects work is that the capacity to explicitly understand and control how 
a character is seen. Therefore, one approach to the problem of broken poses is to no restrictions other 
than that the character appear normal from the camera s point of view. The rationality behind this approach, 
however, has been undermined over the past 10 years techniques for simulation and rendering have moved 
toward physical simulation. For example, light bounce techniques cast rays around the environment and 
will produce artifacts visible to the camera when interpenetrating geometry is encountered. Muscle systems 
that employ internal collision objects to deform skin can be placed in binds without solvable alternatives. 
Dynamics systems that use ballistic techniques to determine the jiggle of muscle, fat, or skin can create 
highly unrealistic bubble effects, distortions, or worse -fail, when the motion of the body breaks the 
rules of physics for acceleration and deceleration. These problems lead to a call for restrictions on 
posing. After all, it is reasoned, visual effects work is constrained by the physics of the real environment. 
Rationally, this makes sense to just about everyone except the animators. Animators typically are averse 
to the use of controls within a rig that limit the range of motion. They prefer to take responsibility 
for .xing broken poses than to lose any control over the range of expressive motion. They can make good 
arguments in their favor. For example, creating the desired blur, arc of motion, and impact of an action 
often requires over-shooting the beginning and ending positions. If the end position is the farthest 
anatomically possible reach for a character the capacity to overshoot is lost if rig limits are imposed. 
 2.3.8 Automated Action A solution to problem mentioned above is the use of automated connections between 
parts. For example, it is very easy using a rig without limitations for an animator to move a character 
s elbow above the head without adjusting the shoulder. The resulting pose is awkward and usually displays 
collapsed deformations along the top of the deltoid and signi.cant shearing in the underarm region. If 
a variable connection is established between the movement of the arm and the position of the shoulder 
the shoulder can be made to stay still while the elbow is low then automatically begin to adjust when 
the rotation of the upper arm hits the threshold indicating that the elbow is moving above the shoulder. 
This can be accomplished a variety of ways dependent upon the construction of the rig and the software 
package being used. However, it introduces articulation action that the animator is not controlling so 
it may be a technique that is as disliked as limits. A technique that can be employed that does not restrict 
the animator s freedom to create expressive motion but does provide feedback about illegal poses is 
the use of visual indicators. Using the same principle as automated motion a signal can be sent to the 
animator through a change in color on the model or a message on the rig control interface that a threshold 
has been met or exceeded. The signal does not restrict action or cause anything else to happen. It is 
essentially the same idea as a warning light on your car s dashboard letting you know that a door is 
ajar or maintenance is needed. 2.3.9 High Level Control and Local Fidelity Rigging for visual effects 
digital characters is a constant effort to provide high level control and local .delity. High level control 
assists fast blocking of scenes and de.nition of articulated limb movement. When rig is setup well 
it is obvious to the user how to interact with the rig and results are easily attained and easily modi.ed. 
Local .delity is the capacity for an animator to accurately de.ne a character s position, pose, and 
action to a high degree of accuracy relative to the camera s view. Local .delity is in great demand in 
visual effects animation when digital character must interact with props, parts of the set, or live actors 
in the photographic plate. Unlike interaction in feature animation the visual effects animator has control 
over only one side of the action. Accommodating local .delity usually means providing a layered approach 
to animation whereby small adjustments in translation and rotation can be made on top of a larger movement. 
Managing both high level control and local .delity can be cumbersome within a single rig. The capacity 
to loss-lessly swap one rig for another, or to add another layer of articulation and controls over an 
existing rig is highly bene.cial in managing visual effects character work. The ef.ciency with which 
this process can be managed begins with the de.nition of articulation points, procedural rig building, 
a consistent naming approach, and a clear understanding of what a rig is being built to do and who will 
be using it. 2.4 Deformations in Visual Effects Figure 5: Hulk mastiff concept art from Hulk (2003). 
Fur length and density is illustrated to be extremely short and sparse, thus skin deformation problems 
such as shearing or volume loss would be easily seen. c &#38;#169;2003 Universal Pictures. All rights 
reserved. 2.4.1 Evaluating Performance Requirements The .rst issue to be dealt with in setting up deformations 
is determining what is seen. If the creature is furry, what is the fur length? Short-haired animals 
require high quality deformations in which great care is taken to preserve the anatomical integrity of 
the sculpted form when in motion (Figure 5). Long-haired animals typically only require deformations 
that are free of shearing and volume loss. Similarly, for clothed characters the question must be answered 
as to how much of the character s skin will be seen? If the character is fully clothed is there an underlying 
skin that is used as a collision object for the clothing? Deforming collision objects for cloth simulation, 
like the skin under long-haired animals, must be free of volume loss and shearing. A primary goal is 
having a solid base on which other efforts can build, or fall back on if they fail and time is running 
short. The performance requirements for deformations may vary with the rig being used and according to 
the needs of the artists using a rig. For example, animators may not want, or be able to ef.ciently handle, 
high resolution models in their work.ow. Therefore, a focus on developing deformations that work for 
them would be on a low-to medium-res model that maintains the same silhouette as the highresolution 
model. An artist tasked with matching the performance of a digital character to the performance of a 
live actor will be heavily dependent up on high .delity deformations with explicit control and precision 
over the silhouette. It must be noted that silhouettes in visual effects work do not typically play a 
role in expressing the shape language of the .lm as they do in feature animation. Strong posing is certainly 
an issue, but silhouettes are heavily art-directed. Such shape changes in visual effects would likely 
be seen as unrealistic relative to a real-world environment. For a performance rig deformations should 
be free of shearing and volume loss across a robust set of poses. Once this is achieved a deformation 
rig can be added to address the problem areas remaining. A deformation rig will typically be employed 
in areas where articulation of the performance rig pulls the skin in directions that it would be prevented 
from going by bones, muscles and other tissues. Common articulation points for deformation rigs include: 
 The slide of skin along a rib cage as is seen when a human character raises an arm or when a quadruped 
extends a front shoulder forward. In these cases the articulation of the joint is a rotation (clavicle 
towards ear in the former, and scapula towards neck in the latter), but the affected skin translates 
along the surface of the ribs.  The .ank area is the loose skin on the torso just in front of the hind 
leg of a quadruped. This skin is pushed forward and pulled backward by the rotation of the leg but also 
visually appears to compress and stretch.  Underneath the neck of long necked creatures skin reacts 
to movement of the neck but is held back from collapsing into the torso by the chest muscles.  Knees 
and elbows can be problematic when the upper and lower limbs are thick. The primary problem is preventing 
interpenetration as the angle decreases (knee or elbow is bent). Elbows, in particular, tend to appear 
in the camera s frame often.  The list above is not comprehensive. It is a set of regularly encountered 
rigging issues that are independent of skeletal articulation and thus best solved through the use of 
a deformation rig. Ideally, the movement of the joints or transforms in the deformation rig will be automated 
to behave properly relative to movement in the performance rig. 2.4.2 Rest Poses When setting up a 
character to apply a deformation system, the geometry of the model and the motion system need a common 
state de.ning their spatial relationship to one another. This is commonly called the rest pose, neutral 
pose, default pose, or bind pose because the relationship between the geometry and the motion system 
is initialized in this pose. Deformations occur when the model is changed from this initial state. The 
bind pose can be any con.guration of the model, but common standards are the T-Pose and A-Pose. When 
modeled in the T-Pose, a characters arms are straight out to the sides, away from the body and parallel 
to the ground plane. The feet are close together, pointing forward, and the body resembles the shape 
of a T when viewed from the front. One bene.t of the T-Pose is that allows joint placement for the motion 
system to occur along orthogonal planes of the coordinate system in the animation software. This also 
makes it easier to create controls for the motion system that occur along axial planes. The T-Pose is 
also preferred for use in creating a cloth system for the character, in which the cloth can be easily 
tailored and draped on the character through simulations before character motion is provided by animation. 
Another bene.t of the T-Pose is that it provides surfacers easy access to high deformation areas of 
the model such as the armpits, knees and waist. The A-Pose differs from the T-Pose in that the arms are 
angled down to the side, about 45 degrees away from the body and the legs are either together or shoulder-width 
apart. There are no rules for determining rest-poses, but there are standards and these vary from studio 
to studio and sometimes from project to project within a studio. The palms of a character can face forward 
or face the ground plane in the T-Pose. The A-Pose can have the arms at 45 degrees away from the body, 
or at some other arbitrary angle. The A-Pose can also introduce slight bends in the elbows and knees, 
which is contrasting to the stiff-looking positions of the T-Pose. Some riggers prefer to have the model 
in a relaxed stance where the hands are slightly cupped with .ngers curved, the arms bent down and slightly 
away from the body, and the legs slightly bent. A bene.t of the A-Pose is that the model is in the middle 
of a typical range of motion for most of the major joints. For example, the most common range of motion 
for the arms of a human character is below the shoulder. It is rare for a character to reach above its 
head. The A-Pose matches a middle-position for the common range of motion. Using the T-Pose will put 
more pressure on deformations in the shoulder area: when the arms are relaxed alongside the characters 
body such as in walking or standing the shoulder will need to handle deformations for up to 90 degrees 
of rotation from the corresponding joints; from the A-Pose, the shoulder would only need to handle less 
than 45 degrees of rotation from joint-based deformations. Conversely, a bene.t of using the T-Pose 
is that joint chains for arms and legs can be de.ned orthogonal or perpendicular to planes in cartesian 
space thus making procedural management of the rigging process and rig maintenance easier to handle. 
The T-Pose is also a standard calibration pose for motion capture sessions. This increases pressure on 
visual effects productions, as heavy users of motions capture, to use the T-Pose so that mapping of motion 
capture data to the performance rig can be accomplished with fewer concerns for accommodating offsets 
between the models. Figure 6: A rest pose generated by determining the mean position of bones within 
the performance space of a run cycle as seen from the front, 3/4-perspective, and side views. Another 
option for the rest pose, or bind pose, is to use a pose closely related to the performance space of 
the character rather than an A-Pose or T-Pose [Pe na 2011]. This option has higher value in character 
development for visual effects because digital characters in visual effects work often are seen in a 
limited number of shots performing tightly speci.ed actions. For example, a hero character, played by 
a live action actor, is strapped to a devious torture device by his nemesis and withstands the administration 
of experimental rays. The effect of the rays transforms the hero into raging were-hyena, (executed as 
a CG character). Assuming that the werehyena is only seen on screen strapped down using an A-Pose or 
a T-Pose introduces extra deformation handling work. A pose that reduces potential deformation issues 
would be close to the pose of the character when strapped down. Figure 6 shows a pose gener ated by 
.nding the mean position of joints within the performance of a motion captured run. Note that the pose 
is assymetrical, which could, in turn, lead to extra work if the rigging system relies on the ability 
to mirror work from one side of the body to the other. 2.4.3 In-Plane and Out-of-Plane Deformations 
One way to categorize deformations is by dividing them between those operating within the surface plane 
of the geometry of the character and those that operated by pushing or pulling geometry out of the surface 
plane. The value is making this distinction lies in the fact that the techniques used and the visual 
appearance of the effects are different. Skin slide is an in-plane effect. It provides the appearance 
of skin moving across forms underneath the skin such as bones, muscles, and fat. An example of this effect 
on real skin can be seen very clearly by lightly placing the .ngers of one hand on the opposite lower 
arm and pushing just hard enough to move the skin back and forth. Super.cial fascia is tissue that loosely 
connects the visible skin to muscles underneath. The key term is loosely . One technique for creating 
this effect for digital characters is to treat the skin geometry as a dynamic spring mesh. Without ballistics 
applied the mesh will seek to minimize the spring tensions between the mesh vertices thus reducing pinching 
and shearing and creating the visual impression of an organic surface. With ballistics applied the mesh 
will react to accelerations and decelerations in the movement of the body and will be seen to jiggle. 
A key differentiation of in-plane deformations from out-of-plane deformations is that in-plane deformations 
do not alter the shape of the model. All of the relaxation and/or jiggle happens across the sculpted 
surface. Out-of-plane deformations create bulges, ridges, wrinkles, and creases on the model s surface 
that were not part of the features of the rest model. The change may be subtle, as how the volume of 
a character s forearm changes as the wrist is turned from palm-up to palm-down. The change could be localized 
wrinkles or ridges resulting from skin compression, as seen when the wrist is rotated backward relative 
to the forearm. Out-of-plane deformations can also be large scale as is seen in the sway of a large belly. 
Like in-plane deformations, the techniques employed can include physical ballistic properties or not. 
Some out-of-plane deformations are handled similarly to skeletal articulation, through the animation 
of joints or control objects. Some are handled as sculpted shapes driven by the animated skeleton. The 
small and/or large volume masses move and change shape relative to articulated limb movement. 2.4.4 
Deformation Techniques Deformations on a character model can be created through different methods depending 
on the performance requirements of the model and the complexity of the production pipeline. Each commercial 
3D software package has its own methods to perform deformations and many visual effects houses have developed 
internal proprietary methods that are either a stand-alone or expand the capabilities of a commercial 
solution. Most solutions include one or more of the the following techniques: Skeleton-subspace deformation 
(SSD) -a common technique in which weighted vertex values based on skeletal joint in.uences determine 
skin motion relative to animation of the performance rig.  Pose space deformation (PSD) -typically applied 
on top of SSD; poses are stored as offsets in transform and joint space instead of object space; the 
method provides for direct manipulation of the desired shapes through sculpting, creating an elegant 
solution to the limitations of SSD [Lewis et al. 2000].  Cage deformers -any of a number of solutions 
that explicitly de.ne how geometry will behave relative to control objects that are external to the surface 
of the geometry; examples include lattices, weighted clusters, wrap, and harmonic coodinates [Joshi 
et al. 2007].  Muscle systems -any number of solutions ranging from simple to complex that de.ne how 
geometry will behave relative to objects that are internal to the surface of the geometry (see section 
below).  The key to determining the deformation technique to use should be driven by performance requirements. 
In Jurassic Park (1993), at the dawn of the use of realistic digital characters in feature .lms, SSD 
along with small amounts of secondary action of muscle masses, were used for the dinosaurs. Arguably, 
those skin effects continue to hold up to scrutiny today within the context where they re seen. However, 
it would be naive to think that even highly polished use of SSD will achieve believability for a human 
or human-like character with large amounts of visible skin standing next to a live action actor. The 
effort to create more realism and complexity in skin movement has led to signi.cant advances in the use 
of systems that mimic the behavior of real bodies. Foremost among these is the development and use of 
muscle systems.  2.5 Muscle Systems in Visual Effects Like rigging and deformations discussed earlier, 
the key to successful use of muscle systems is clear evaluation and understanding of the performance 
requirements. It would be a waste of resources to develop a complex muscle system for a long-haired animal, 
a mostly-clothed human, or a character that is only seen from a distance where the behavior of muscles 
is not apparent. Muscle systems are often treated as enhancements to standard deformation work. Therefore, 
the capacity to turn them on or off without substantial impact on the image or the production pipeline 
is important. Where as motion systems and deformation systems incur a cost that is primarily loaded 
on pre-production budgets, muscle systems incur both a set-up cost in pre-production and hit the per 
shot budget in post production. Muscles are therefore, more than either rigging or deformations, evaluated 
using a bang-for-the-dollar mentality. Cost is one of the primary reasons why muscle systems developed 
for use on digital characters rarely resemble anatomical muscles. There are over 650 muscles in the human 
body. Of this number, 2540 could arguably be described as having an effect on the surface of the skin 
(Figure 7) (Note that we are not discussing facial mus cles here, though the muscles of the neck are 
considered and have a connection to facial animation). Of these perhaps one-quarter to one-half could 
logically be expected to be seen in shot, assuming traditional approaches to shot composition. This 
argues heav Figure 7: Anatomical illustration of the muscles of the human torso. As digital artists 
it is tempting to explore the anatomical accuracy that can be achieved through rigging, modeling, and 
procedural systems. By Bernardino Genga (1620-1690), [Public domain], via Wikimedia Commons. ily against 
the development of a muscle system that imitates the skeletal-muscular of the human body. Most visual 
effects projects use a mixed approach ranging from key-framed shape animation to robust simulations. 
The scope of use of the characters varies from background to hero, from a closeup of straining torso 
muscles to a medium shot of the relaxed character. It would be highly inef.cient to apply the same approach, 
set of tools, muscle system across all shots. For example, why develop a muscle system to create the 
.ex of a bicep muscle on digital double when shape animation is much more simple to execute and exacting 
in its results? At the other end of the spectrum are situations where the appearance and use of muscles 
is integral to the success of the project. Figure 8 shows an example of very speci.c visual descriptions 
for both shape and behavior of muscles. 2.5.1 Muscle Behavior The visual result of muscle action on the 
exterior of the human or animal body is goal. Therefore, systems that mimic the behavior of muscles need 
only go so far in terms of matching the interrelationships of muscles to other muscles, tendons, and 
bones, but Figure 8: Concept art from Hulk (2003) with art direction callouts noting muscle shapes, 
skin deformations, and skin surface details. c &#38;#169;2003 Universal Pictures. All rights reserved. 
must take care to capture the variety of shape changes that can be expressed. The forms of muscles are 
modeled into the surface of a character in its rest post. Once in motion, the muscles act as a collision 
objects for the skin. The form of the muscle, and whether or not the muscle is seen, is dependent upon 
the pose and activity of the character. Muscles exist in two forms: relaxed and loaded. Within the category 
of loaded muscles is the change in shape that occurs from being under load while extended and under load 
while fully contracted. Transient shapes, forms that appear and disappear, occur as the muscle moves 
from relaxed to loaded and from under load while extended to under load while contracted. These transient 
shapes are often the thin ends of the muscles, the insertions, that connect the muscles to tendons and 
bones. Tendons, while not muscles per se, can be treated as muscles when determining the exposure of 
shapes on the skin. Most physics engines work well with simple objects such as spheres as collision objects 
and perform progressively more slowly as collision shapes with higher .delity or changes in shape are 
used. The complexity of shape changes begs for a solution that employs a system of objects operating 
under the skin -some representing bone, others muscle and tendons. There are two practical approaches 
to producing solid results. The proper approach depends upon the scope of the project. If the project 
consists of a handful of high-.delity shots ef.ciency and economy will be best served through the use 
of a simple muscle system on mid-ground and foreground characters combine with shape animation work 
to add visual details into the few shots that require it. If the project consists of a large number of 
characters and shots that will be seen in high detail the better approach is to build a system that 
can use simulation and shape animation in pre-production to prepare baked results that can be referenced 
during shot production and driven by skeletal animation. As was discussed concerning the development 
of the motion system and deformation system, development of a muscle system is an art that begins with 
assessment of the .nal image. The tools and processes fall into place as the pipeline is analyzed in 
reverse from the image, to the users responsible for creating the image, and through the lens of time 
and budget. Thankfully, for visual effects work, nature provides a ready-guide for determining the visual 
target.  3 Digital Character Development for Games 3.1 Procedural Secondary Rigging for Game Games 
present a variety of challenges given their real-time, interactive nature. These challenges often apply 
many technical restrictions when it comes to rigging digital characters. This section of the course 
will provide a broader understanding of these technical restrictions, and approaches for creating visually 
appealing, procedural secondary rigging for digital characters in games. 3.1.1 Real-Time Rendering Its 
important for a rigger to understand the nature of real-time rendering to empower himself to innovating 
new solutions to existing problems. First it is important to understand that real-time rendering often 
means different things to different people. Often for academia, if a dynamic cloth system is evaluating 
and running in real-time on a PC, its considered real-time rendering. For games, real-time rendering 
absolutely requires that all systems are running and rendering in real-time, typically at 30Hz, often 
at 60Hz on the targeted hardware platform. This real-time requirement is the main challenge of games 
from a technical perspective, and it imposes limitations and requirements upon digital character rigging. 
Game consoles, PCs, and handhelds including cellular phones, all have different hardware designs, and 
limitations to work within. It is outside the scope of this course to cover the speci.c differences between 
the varieties of hardware platforms, drawing our focus to be on the generalities of rigging for real-time 
gaming. The .rst general concept of real-time rendering for games is the notion of run-time performance. 
Due to games running in real-time, the more skinned polygons and joints, as well as all of the other 
components that need to be rendered, the more time it takes to render, and therefore the slower the 
frame rate of the game. Generally speaking, most games run at 30 or 60 Hz (frames per second). This 
immediately enforces the need for determining and adhering to budgets for the number of joints in a skeleton 
hierarchy and the number of skinned vertices weighted to them. Of course, there are a number of other 
art components that affect run-time performance, but we will leave them outside of the scope of this 
course in order to focus on digital character rigging. Still, measuring the performance impact solely 
on the number of joints and skinned vertices is only part of the picture. The manner in which those vertices 
are skinned to the joints also affects performance. Depending on the hardware, or rendering system, 
the number of joint in.uences that each vertex can be weighted to may have a signi.cant performance 
impact. Generally speaking, Xbox360 and PS3 hardware performs optimally when each vertex is limited to 
a maximum of four joint in.uences. Hardware in the range of PS2 or Wii range generally performs optimally 
with a maximum of three joint in.uences. Mobile can be as low as one or two joint in.uences. Another 
important performance impact to consider is weight quantization. The precision of joint in.uence may 
also impact runtime performance. Generally speaking, there is a performance cost versus skinning quality 
trade-off depending on the level of precision for skinning weight values. For example, skin weight values 
that are rounded off to 0.1 will generally perform faster than 0.01, although the skinning visual quality 
may suffer depending on the proximity of the camera view to the character. The number of joint in.uences, 
and weight quantization arent hard rules, rather a general practice that affects overall runtime performance. 
Balancing these values against run-time performance is a key factor in achieving a higher degree of quality. 
When building assets for games, there are often optimizations in the export pipeline that automatically 
applies and renormalizes joint in.uences and weight quantization limitations into the skinning information. 
Its important for a rigger working on games to be aware of these pipeline optimizations, so they may 
tune their rigging work in a WYSIWYG work.ow. Without an understanding and awareness of these limitations 
and pipeline optimizations, there often results in misunderstandings as to why the desired results arent 
as expected. The second general concept to understand is the memory limitations imposed by hardware targets. 
Each variation of game hardware has different amounts of .nite memory available for a wide variety of 
different assets and features within a game. Each and every art asset, whether it be geometry, texture 
map, joint or animated channel uses some of that .nite memory, not to mention all the memory overhead 
of the various game systems. There are a variety of compression technologies that may be applied to 
reduce the memory footprint of texture maps, and animation channels. Ultimately, while compression allows 
for more art assets to be running in a game, it comes with a visual degradation cost. A careful balance 
is required to maintain visual quality while working within the memory budgets allowed. This memory limitation 
directly applies to rigging in the cost of animation channel data. Simply stated, increasing the number 
of animated joints, and animations being played back on those joints, the more animation memory budget 
required. With these real-time rendering limitations in mind, well next examine the interactive nature 
of games as it pertains to character rigging. 3.1.2 Rigging for Interactive Game-play Animation The 
other main challenge with games, at least in the context of games which include user controlled characters, 
is the interactive nature of character animation. Through a wide variety of input systems, the user 
controls the actions, and thereby, the animations of a character. The challenge, and goal, of digital 
character rigging in games is to support and enable visual animation quality. Achieving quality animation 
typically requires extensive animation blending, deformation and secondary motion systems, all of which 
require a strong foundation in digital character rigging. Its important to understand the nature of animation 
blending for games, as it has a direct impact on character rigging, particularly deformation and secondary 
motion systems. When a digital character switches from one animation state to another, there is a transitory 
blending between those states to avoid visually disturbing abrupt changes in a characters pose from frame 
to frame. For example, as a character switches from an idle stance to a walk forward motion, the idle 
and walk forward motions may blend to transition through the abrupt nature of the pose change, or better 
yet, a speci.c transition animation may be played which properly animates between a idle animation and 
the walking animation. This transition animation provides the greatest level of animator control, and 
visual quality, but may impair the responsiveness, or interactive nature of the users ability to control 
the character, in addition to requiring more animation memory budget. Finding a balance between control 
responsiveness, animation visual quality while working within a .nite memory budget, is a key fundamental 
of a quality game. In addition to animation transition blending, different regions of a characters animations 
may be blended to achieve a wider range of animation variety, or for optimization purposes. This is best 
illustrated through an example. In the case of a character walking, while also reaching into a jacket 
pocket, we could take the approach of transitioning from a looping walk animation to a walk animation 
that also has a reach into the jacket pocket animation on the arm. This of course requires an increased 
animation production, and more animation memory. A typical animation optimization would be to layer 
the characters reach into the jacket pocket arm animation over the characters walk cycle. Dynamically 
layering region animations onto a character, reduces animation memory requirements. It also enables 
a wider range of animation variety, since that arm animation could be layered over a variety of other 
animations. 3.1.3 Character Design and Performance for Games While it is true that character design 
and animation performance greatly impacts the rigging of digital characters for games. It is also true 
that the technical requirements of games may also impact the design and animation performance of a digital 
character. Due to the nature of the technical restrictions already covered, there may be a requirement 
to focus run-time performance budgets on speci.c features of a character, while investing little in 
less important features. For example, a prominent story driven game, with compelling character animation 
performances will likely demand an emphasis on quality facial animation. Maintaining a balanced performance 
budget may require lowering quality expectations on less important features of the character such as 
muscle deformation in the body, or secondary cloth animation, to allow for a higher .delity in the face. 
This is often not an easy balance to maintain during interactive game-play. For situations where the 
level of quality is not achievable due to technical restrictions, non-interactive sequences offer a 
viable alternative. Non-interactive sequences refers to those moments when the user isnt interacting 
with the game, instead they are watching a linear rendering. At .rst glance these non-interactive sequences 
may appear to be opportunities to ignore the many technical restrictions of games, and rely on the unlimited 
rigging simulation solutions of .lm to increase the visual quality. Indeed, there are many examples of 
this approach for some games. Unfortunately for a broader range of games, its not always so simple, and 
there are issues to consider with this approach. First, there is the issue of game immersion. When playing 
a quality game, the user is immersed into the world of the game. The digital character becomes their 
iconic avatar, representing themselves in the virtual world of the game. Switching visual quality levels 
from game-play to non-interactive sequences, often removes the user from the illusion of immersion in 
that virtual world. Secondly, the user may customize the visual appearance of their character, or possibly 
the users actions may have had an impact on the environment or the character itself such as damage, prop 
selection, etc. A pre-rendered short movie doesnt take the potential large variety of visual options 
of characters and environment into account, which again may remove the user from the illusion of immersion. 
Also, aside from the production costs of supporting two levels of quality, there is also the need for 
storing HD pre-rendered videos on disk, while leaving room for the game content itself. To .t the video 
content onto the disk, high levels of compression may need to be applied, which inadvertently degrades 
the quality of the prerendered video, directly opposing the intent of pre-rendering in the .rst place. 
With the understanding of the disadvantages of pre-rendered videos for games in mind, we may return to 
the advantage of noninteractive sequences. Game-play often demands a high frame rate of 60Hz to maintain 
a smooth and freely interactive game play experience. On the other hand, non-interactive sequences offer 
the potential advantage of rendering at a reduced frame rate of 30Hz. By lowering the frame rate, run-time 
performance becomes less restrictive, allowing for a higher .delity of rigging run-time performance 
budget. Digital character rigging for games requires art directors to prioritize features of the characters, 
where to focus the camera and how to present story-telling elements of the game. It also requires game 
design to be aware of technical limitations, and work with them to allow the overall game to .ourish. 
Ultimately, rigging in games requires balancing run-time performance budgets, sound art direction prioritization, 
and game design decisions that support the character animation performance and game design.  3.2 Skeletons 
for Games 3.2.1 The Tools of the Trade When it comes to character rigging for games, the general tool 
of the trade are joints and skinned geometry. Hardware is optimized for skinning geometry to joints. 
That doesnt mean that it isnt possible to use blendshapes and other deformation techniques in games, 
but its generally joints and skinned geometry that is the main source of deformation due to the performance 
limitations of the hardware. 3.2.2 Animated Joints Character animation is comprised of a hierarchy of 
joints with keyframes. A typical digital skeleton includes a hierarchy for the hips, spine, arms, legs, 
neck and head. Depending on the level of detail and animation system, .ngers and facial joints may also 
be required for more detailed expression. All of these joints require an animator to apply keyframes 
to animate them. As discussed earlier, those keyframes require animation memory, so making wise decisions 
as to the hierarchy and number of joints is important. I will refer to the portion of the skeleton that 
is animated through keyframes as the primary skeleton. 3.2.3 Procedurally Driven Joints Of course, there 
is much more to human anatomy than a torso and appendanges. High quality digital character rigging must 
account for a rib cage, muscles and tendons that maintain volume, and fat which moves in a secondary 
way to the primary motion. Achieving the visual indication of these anatomic features requires complex 
deformation systems which are performance inhibitive, or the addition of joints to mimic volume maintenance 
and secondary fat motion. I will refer to this portion of the skeleton as the secondary skeleton since 
it reacts to the motion of the primary skeleton. The addition of secondary skeleton joints may become 
problematic in terms of memory cost, and animation production costs. With more joints, we use more memory, 
and it takes production time for animators to animate them. A solution to those problems is to drive 
secondary joints procedurally. A speci.c example would be to create a procedural secondary bicep joint 
that translates outwards from the arm as the animated elbow joint rotates, providing the visual cue that 
there is a bicep muscle .exing when the arm is bending. A procedural system allows the animators to 
focus on animating the character while the procedural secondary rigging automatically deforms the character 
in an anatomically correct manner. Also, implementing a procedural secondary rigging system in the game 
requires less animation memory on those secondary joints since they arent animated by keyframes. Further, 
in situations where a physics systems must take control of the characters skeleton to simulate collisions, 
the physics solve need only operate on the primary skeleton. If the physics system had to evaluate the 
secondary joints, the system would become much more complex and ultimately performance hindering. A better 
approach is for the procedural secondary rigging system to operate post-physics solve, just as it would 
in a keyframed animation situation.  3.3 Procedural Rigging for Games in Practice With an understanding 
of the context of game requirements, and procedural secondary rigging, this chapter will cover some of 
the speci.c strategies of setting up procedural secondary rigging systems. 3.3.1 Hard Coded Procedural 
Work.ow A hard coded procedural work.ow refers to a procedural secondary rigging system that is written 
within the rendering or animation system itself. Stated simply, a hard coded procedural work.ow is implemented 
as expression-like code which creates a relationship between an animated joint and a procedurally driven 
joint. Implementing the prior bicep .ex example as described above, its most basic hard-coded Maya expression 
form may look like this: This example is very similar to Maya s set driven key functionality where 
it normalizes the range of a driver object to a different output range of a driven object. Basically 
this example dynamically drives the translateY of the Muscle Left Bicep joint as the LeftF oreArm rotates 
in Z. The numbers in the formula represent the ranges that are normalized. Figure 9: Bicep .ex procedurally 
driven by elbow rotation. Working within a digital content creation (DCC) software package such as Maya, 
the rigger may use this expression to tune the values of the amount of elbow rotation and bicep translation, 
as well the skin weight information to create a visually appealing bicep .ex result. Implementing that 
same expression in a game typically requires a rigger to work closely with a software engineer to convert 
expression code written in a language appropriate for DCC iteration, into a language appropriate for 
the games rendering or animation system. Further, the conversion process may need to convert rotational 
differences between euler angles and quaternions. The problem with this re-implementation of code from 
DCC to game is poor iteration, and a redundant waste of software engineer time. Further, this expression 
is dif.cult to read, and prone to syntax errors. 3.3.2 Procedural Rigging Operation Library An improvement 
on this code driven approach is to write a series of procedural rigging operation functions that a rigger 
may use to create a variety of secondary rigging effects. For example, instead of hard-coding the expression 
to the names of speci.c joints and speci.c values, the linear blend set driven key operation may be written 
as a function. Once this function is written and made available for use in the game, the rigger may 
write and iterate on code that references this rigging operation function library, like this: Though 
this system design allows for a much improved iteration loop, and is much more readable than the hard-coded 
expression, it is still not ideal in the sense that it requires writing code, which still needs to be 
converted into a game-friendly language, which may be prone to errors.  3.4 Data-driven Procedural Rigging 
To realize a truly robust data driven system, the language that communicates with the rigging operation 
library should be a DCC exported format that the in-game library can directly read and execute from. 
Simply stated, the rigger should be able to export a .le from their DCC, which may be imported into a 
game database in a manner that automatically establishes procedural relationships between the primary 
and secondary skeleton. There are a number of different methods for implementing this such as XML data, 
proprietary .le formats etc. The idea is to externalize the data in such a way that it allows the rigger 
to iterate on the procedural secondary rigging without the need for a software engineer to implement 
his changes into the game.  4 Character Rigging for an Animated Feature Film The requirements for rigging 
on animated feature .lms differ somewhat from those in VFX feature .lms or in games. In VFX .lms, digital 
characters need to seamlessly blend into live action plates. As such, the emphasis tends to be on building 
realistic character setups and on integration. In games, signi.cant constraints are placed on the rigs 
to run in real-time and to integrate into an interactive game engine. Animated .lms place high emphasis 
on animation, both in terms of the quality of the performance and in terms of the sheer quantity of 
animation required to complete a .lm. Character rigging attempts to provide animators with as much .exibility 
and control as possible. Animated characters need to deform believably through an extreme range of motion. 
Substantial rigging effort is devoted to making setups look nice in highly exaggerated poses. Furthermore, 
these poses are highly art directed by animators, production designers, and directors. A desired look 
often deviates from what is physically accurate (much to our chagrin sometimes). Animated .lms are highly 
stylized and non-photorealistic, requiring unique deformation solutions. A hero character setup is animated 
in many hundreds of shots in a .lm. Our solutions need to be robust and scalable, especially since we 
rarely provide per-shot or sequence character rigs. We share our core setup components not just across 
characters on a .lm but also across the many movies that are in production. At our studio we also have 
to deal with the unique requirements of sequel .lms. We often have to maintain (and augment) a hero character 
rig through multiple productions, spanning many years. In other cases, we rebuild a sequel character 
rig from the ground up. The goal is to make improvements yet still match the look of the previous .lm. 
This section of the course covers some key aspects of character rigging for animated .lms. We .rst explore 
the relationship between character design and rigging. We then discuss skeletal motion systems. We next 
cover body deformation techniques and then .nish by exploring facial setups for animated .lms. 4.1 Character 
Design Considerations An important dynamic exists between character design, rigging, and animation. Each 
in.uence the other and all three groups need to work in tandem to produce an appealing digital character. 
The process of developing a character works best when it is iterative instead of linear. Rigging and 
animation are typically involved in early designs and participate in character art reviews. Both groups 
need to understand the story and design requirements for a character. Similarly we may make design requests 
based on how rigging and animation requirements will affect the 2D design. Very often the design aesthetic 
affects rigging requirements. For example, the characters in Madagascar have clean graphic lines. The 
integrity of the graphic lines needs to be maintained through the full range of motion. These design 
elements contrast from a more anatomical .lm like Shrek and lead to different deformation requirements. 
The production designer for Madagascar wanted to veer away from a more anatomical eyelid structure to 
support the strong graphic nature of the .lm. Art, animation, and rigging worked together to create a 
shutter eyelid system that was a unifying design element across all character. The shutter eyelids became 
a unique rigging requirement for the show. Design is also in.uenced by animation style. On the .lm Megamind, 
the director kept giving feedback that he wanted the lead characters eyes to go completely round in certain 
expressions (Figure 10). We added an eye rounding facial control to all characters. This ended up being 
a key design component to facial animation for the show.  Figure 12: Minions leg design required minor 
adjustments to accommodate a larger range of motion at the knee and ankle. Left: Minions revised design; 
Right: Minions original design. Rigging and animation requirements affect the character design as well. 
A common case involves range of motion. An artist can easily cheat a 2D drawing to achieve a pose. Once 
we bring the character into 3D, we often .nd that speci.c movements will not work without revisiting 
elements of the design. As an example, the character Minion on the .lm Megamind has a robot gorilla body 
consisting of rigid plates (Figure 11). The original design did not allow for extensive range of motion 
without introducing interpenetrations between the plates. We presented minor adjustments to the design 
to signi.cantly improve the characters .exibility (Figure 12).  4.2 Skeletal Motion System The skeletal 
motion system of a character encompasses the skeletal joint structure as well as the set of interfaces 
we provide to animators so that they can manipulate and pose these joints. A well designed skeletal 
system enables the animator to easily make the character walk, hit dynamic poses, and most importantly 
emote in a believable way. While the skeletal systems vary from studio to studio, some highlevel principles 
typically apply: 1. Understand and capture the anatomy of the character via a joint hierarchy, 2. Build 
an intuitive set of controls and interfaces to animators that provide the right level of animator control. 
 3. Enable consistency across characters and across shows as much as possible.  4.2.1 Skeletal Structure 
The .rst step in setting up the skeletal system is to de.ne a hierarchy of joints that represent the 
articulation points of the character. We often refer to anatomy textbooks as well as video (or live) 
reference for inspiration. Of course, our characters whether human, animal, or .ctional are usually 
stylized in some way. So, we often exaggerate, misrepresent, or in some cases ignore the pure anatomy. 
We also receive inspiration from character art, pose sheets, expression sheets, etc from our character 
designer and animators. We build template .tting systems for characters of similar type such as bipeds 
or quadrupeds. To .t a new character, the artist places a minimal set of joint markers. In most cases, 
only positional information is required. The .tting system then procedurally places a number of secondary 
joints that drive the motion system components. These secondary joints often handle orientation constraints 
such as aiming at a child joint. We also build automatic joint placement systems for background characters 
that share similar topology. 4.2.2 Animation Controls The motion system is comprised of a set of sub-systems 
which handle an area of the character setup. Each subsystem is responsible for a set of animation controls, 
the behavior of the controls on the joints, and the associated animator UI. For biped characters we have 
sub-systems for the spine, belly, shoulders, neck, arms, .ngers, hips, legs, and toes. Unique characters 
can mix-and-match components as well as de.ne alternative motion sub-systems. Our setups also have matching 
systems so that animators can seamlessly switch between modes in our setups, such as between FK and IK 
and between different orientation spaces. The most interesting aspect of any motion subsystem is de.ning 
the best set of animator controls and the behavior associated with these controls. We delve into building 
a strand system as an example.  4.2.3 A Strand System An interesting motion problem is how to animate 
a long strand of joints. Our primary use case for a strand system is on quadruped tails (Figure 13). 
From an animators perspective, the high-level goals for animating the tail are: Easily pose a dense 
set of joints via a small number of highlevel FK controls  Enable the tail to follow a path (relative 
to the characters overall motion) and drag on the ground   Lock a region in space to enable contact 
(such as a tail lying on a rock)  Add secondary animation  Control stretching of the tail  Enable 
shot-speci.c interactions with the tail such as grabbing the middle of the tail  Add procedural animation 
and simulation  The core posing of the strand system is achieved through a small set of high-level FK 
rotation and scaling controllers (usually between 3 and 10) that drive a large number of low-level joints 
(usually between 20 and 50). These controllers can each be positioned parametrically along the tail. 
For full .exibility, the controllers are non-sequential meaning ordering along the tail does not matter. 
The animator can adjust the sharpness to control the range of in.uence the controller has on the underlying 
joints. Each controller can be blended on and off as needed. This gives animators the .exibility to create 
key-frame animation with a minimal number of controllers and then add in extra detail with additional 
controllers as needed. The tail joints can alternatively be driven by a set of IK controllers. An animator 
uses the IK controllers to keep part of the tail in contact with other objects or characters in the 
scene. The IK controllers can also specify a path that the tail will travel through. The IK controllers 
either follow the characters body movement or move independent of the characters motion. As with the 
FK controllers, each IK controller can be blended on and off for full animator .exibility. The system 
also provides a layered set of FK offset controllers on top of the IK. This enables animators to easily 
apply secondary motion. When the strand has been posed with IK, it can signi.cantly stretch or compress. 
The animator can both control the overall amount of stretching for the strand as well as localized stretch. 
The system provides a pin point which speci.es the region of the strand where all the stretch will occur, 
and the area that will remain constant. The pin point control is also used as a pivot point when the 
strand is being grabbed at a speci.c location. The system architecture supports post-animation procedural 
controls. Animators may want to apply procedural motion to the strand to either create simple cycle 
motion or to add complexity to an existing animation. Finally, a simulation pass can be optionally run 
on the strand to add believable secondary motion. Based on a simple mass-spring simulation engine, the 
solver has the ability to preserve the animators poses by redistributing the simulation results along 
4.3.1 Fat Characters: Po the strand only in regions of high energy.  Our strand system was originally 
developed for quadruped tails but we have used the components to build unique systems for many characters 
on our .lms. In some cases we primarily include the FK controllers as a posing mechanism: for example 
the dragon necks on How To Train Your Dragon. In other instances we embed the full strand functionality 
into an existing rig. For one sequence in the .lm Megamind, Minion s arm needed to extend out like a 
tube. We incorporated the full strand functionality into Minions arm system (Figure 14). Finally, we 
have setups like the Viper from Kung Fu Panda 2 where we construct a unique system from our strand components 
to enable the snake to move along an animated path yet still hit kung-fu poses.  4.3 Body Deformations 
Our body deformation techniques have evolved over the years to accommodate the animation style and character 
designs for our .lms. Typical deformation requirements for our movies include: High degree of cartoon 
squash and stretch  Stylized character designs  Exaggerated, non-realistic range of motion  Art directed 
results  Immediate feedback and control in the animation environment  We apply a layered deformation 
approach with proprietary relaxation techniques. This enables us to maintain art directed designs as 
our characters are pushed and pulled through a wide range of movement. We typically do not build a muscle 
system into our setups as we have found them ill suited for stylized and graphic character designs. We 
also do not run off-line skin or muscle simulations. Any simulation elements in our deformations need 
to be fast enough to run in the animation environment and to be tunable by the animators. One of the 
biggest challenges we face with deformations is preserving volume, especially through a full range of 
movement. The character Po from Kung Fu Panda is an extreme example: he is an enormous panda who has 
to perform kung fu (Figure 15). While our deformation techniques incorporate some basic volume preservation, 
we needed a more robust solution for Po s torso and belly. First, we layered automatic behavior on top 
of our spine system to prevent the belly from collapsing as the hips and shoulders are moved through 
a full range of motion. We also built a custom deformer that determines interpenetrations between Po 
s limbs and his belly. The deformer squashes and stretches regions of the belly to react in a believable 
way. Finally, we perform surface-based collision avoidance between the arms and legs and the torso to 
further prevent self-collisions. Po s arms provided additional deformation challenges. He has thick arms 
that have signi.cant weight to them. We needed to always feel the effects of gravity on his arms, but 
we did not want to run a complex simulation to achieve the look. Instead we layered fake gravity forces 
on top of our deformation setup on the arm regions. The silhouette of his shoulders was very important 
to achieving pleasing graphic shapes. We customized the deformations through a broad range of poses to 
art-direct the arm silhouette. 4.3.2 Bi-quad Setups Some of our characters need to switch seamlessly 
between bipedal and quadrupedal performance. Typically this occurs when we have a quadruped character 
that is designed primarily as a biped such as Alex in Madagascar. With a character like Alex we create 
a single bi-quad rig that allows for on-screen transitions between biped and quadruped motion (Figure 
16). We encounter two unique challenges for bi-quad deformations: 1. Accommodating design differences 
between the bipedal and quadrupedal versions of the character. 2. Delivering an increased range of motion. 
 The character designs will typically differ between the bipedal and quadrupedal version of a character. 
For example, Alex as a biped has a thumb, whereas quadrupedal Alex does not. Quadrupedal Alex has larger 
feet, broader shoulders, and its chest volume is pushed further up his torso. Alex s neck connects to 
the back of the skull when he is a quadruped whereas it connects near the base of the head when he is 
a biped. We handle these differences by providing animators with custom shape interpolation controls 
that enable them to locally blend between the two designs [Grignon et al. 2008]. Combining range of 
motion for a bi-quad rig stresses deformations in key areas of the body. The hips are typically challenging 
on any character, but on bi-quad characters they require even greater range of movement. The upper legs 
swings back in a bipedal push pose and forward for a quadrupedal sitting pose. The result is 240 degrees 
of total motion. The neck and heads range of motion increases approximately 90 degrees when combining 
the two rigs to roughly 270 degrees. Our relaxation-based techniques are designed to handle exaggerated 
range of motion requirements. However, we often spend extra character setup time art-directing the results 
for these extreme poses.  4.4 Facial Setup 4.4.1 Overview Modern face systems typically provide a large 
set of individual facial controls that an animator key-frames in combination to build expressions and 
facial animation. These controls are usually based in some form on human facial musculature. A distinction 
between different systems is in how the individual controls are created by the character artist. A common 
approach is to explicitly model each control as a blend shape target on the .nal skin model. These systems 
use shape interpolation to blend between the modeled shapes. An alternate approach is to combine a set 
of deformations that generate the .nal shapes. The deformations might be done on the skin model, on lower 
resolution intermediate meshes, or on both. Recently, performance capture has been combined with key-frame 
animation to produce impressive results. Facial capture systems layer nicely on top of muscle-based face 
setups. 4.4.2 A FACS-based Approach DreamWorks animated .lms run the gamut from very cartoony and stylized 
(ex: Madagascar) to more realistic and anatomicallybased (ex: Shrek and How To Train Your Dragon). We 
employ a single face system that can be adapted to the diverse requirements each .lm. Our face system 
takes a muscle-based approach derived from FACS (facial action coding system). We have augmented the 
system with layers of cartoon controls so that we can veer away from human anatomy as needed. FACS is 
an anatomically-based methodology for identifying and analyzing human facial expressions. FACS provides 
a detailed framework of all the muscles in the human face. First FACS identi.es the range of fundamental 
human expressions. Next the research describes how each expression is triggered by a set of underlying 
muscles (or action units). We have built a four-layered deformation engine that mimics our human anatomy 
and musculature (Figure 17). The bone layer (Figure 17, upper left) represents the cranial struc ture 
as well as the skeletal muscles that move the jaw. We model portions of the cranium so that they can 
be used as collision objects to approximate .esh sliding over bone. The mask layer (Figure 17, upper 
right) corresponds to the numer ous facial muscles that move the .esh of the face. We build the mask 
as a low-resolution surface where each edge corresponds to one of the facial muscles. Rigging artists 
program animator controls by adjusting the lengths of these muscle edges on the mask surface. For example, 
if we wanted to create a disgusted expression as described by FACS (Figure 18), we would trigger the 
corrugators to move the brow, tighten the eye-ring, lift the nostril, raise the upper lip towards the 
nose, and trigger the mentalis to slide the lower lip. The .esh layer (Figure 17, lower left) represents 
the .eshy fat that gets pushed around by the facial muscles. This layer is driven by the low-resolution 
mask. The .esh layer also adds controls for elements like puf.ng the cheeks or thickening the lips. 
The .nal skin is attached to the .esh layer and includes .ne detailed functionality like skin wrinkles. 
Typically, we apply these wrinkle details by measuring surface compression. The system also provides 
the animators with a set of sculpting controls to make detailed adjustments to the overall shape of 
the face. 4.4.3 Cartoon Behaviors Most of our .lms exaggerate reality to a great extent. As such the 
art direction and animation style veer signi.cantly away from the physics of human anatomy. Speci.cally 
we identify three requirements for non-realistic cartoon facial behavior: 1. Add squash and stretch 
to the face 2. Enable extremely exaggerated expressions 3. Create graphic shapes, pleasing silhouettes, 
and smooth surfaces  In order to ful.ll these requirements, we have made three adjustments to our 
face setups. First, we break up the bone layer into region and add squash and stretch scaling controls. 
We also add rotational controls for the bones so that animators can further exaggerate the central line 
of axis for the face. Second, we take the muscle origin points that would attach into the bone and instead 
move them well outside the bone. This gives each muscle the ability to greatly expand its range of motion. 
Finally, we add the equivalent of a soft bag underneath the skin to smooth out the expressions. This 
enables the system to keep nice graphic shapes and clean surface contours even under extreme deformations 
(Figure 19). 4.4.4 Anatomical Details Depending on the .lm, we may want to include a number of anatomical 
details that help make the face setups feel .eshy and believable. Here are some examples of features 
that we can add on a show or character basis: Sterno-mastoid muscle and adams apple for more realistic 
looking necks  Dimples  Lips sticking together as they close  Signature face wrinkles  Neck folds 
 Skin sliding over bones (forehead and jawbone)   4.5 Animation and Rigging Directions in Animated 
Films This is a very exciting time for animation and character setup. Industry trends will have a signi.cant 
impact on rigging techniques and animator work.ow. As an example, animation and rigging systems are 
being redesigned to take full advantage of multi-processor computer speed. This offers interactive character 
execution speeds that were not possible a few years ago. This should also enable a range of full-.delity 
clothing, skin, and muscle simulations to run in real-time in the animation environment. These possibilities 
will fundamentally change how we rig characters for animated .lms. We are .nally breaking out of the 
desktop keyboard and mouse paradigm with the advent of unique input devices, multi-touch screens, and 
other hardware options. This opens up a wide range of opportunities to integrate two-handed interactions, 
gestural, and multi-touch into animator work.ows. All these innovations will hopefully make the animator 
experience more artistic, so that the animators interact with character setups as if they are drawing 
or sculpting the animation.  5 Conclusion This course covered a range of issues related to the development 
of digital characters for use in visual effects, feature animation, and games. Speci.c attention was 
given to rigging, deformations, and the methods for creating the organic motion of skin. Similarities 
and differences between approaches taken for each format were discussed. This course was an overview 
and not intended to cover each area in detail. Information with speci.city falls out of the scope of 
this course both due to time/space constraints and due to the proprietary nature of the work at various 
studios. The approaches and techniques described are primarily software agnostic. It is hoped that artists 
working in this area at other studios or as students will understand the methodology involved and be 
able to implement similar approaches in their own tools and pipelines. Acknowledgements Thanks to Jason 
Smith and Miles Perkins at Industrial Light &#38; Magic for their help in framing the issues and providing 
illustrations for the section on rigging for creatures in visual effects. 6 Notes About the Authors 
DAVID COLEMAN is a Senior CG Supervisor at Electronic Arts Canada, leading a team of character riggers 
working on multiple EA Sports titles such as FIFA, FightNight, and NHL. In prior roles, David was the 
CGSupervisor of central character art teams, various animation research and development teams, and also 
a Senior Motion Capture specialist. David has taught animation and motion capture at the Center for Digital 
Imaging and Sound, the Art Institute of Vancouver, in addition to teaching Autodesk Masterclasses in 
Australia, Korea, China and Japan. LARRY CUTLER is currently the Global Head of Character Development 
at DreamWorks Animation, where he is responsible for the Character TD department and for global character 
development across all projects at the studio. Prior to this, Larry led the rigging effort as the Character 
TD Co-Supervisor for the animated .lms Megamind, Shrek the Third, and Shrek 2. Prior to joining Dream-Works, 
Larry worked at Pixar Animation Studios as a Technical Director on such .lms as Monsters, Inc, Toy Story 
2, and A Bug s Life. Larry s passion for computer graphics and .lm-making was fostered as a student at 
Stanford, where he received a BS and MS in Computer Science and Human-Computer Interaction. TIM MCLAUGHLIN 
is the Head of the Department of Visualization at Texas A&#38;M University, home to the Master of Science 
in Visualization Sciences and the Bachelor of Science in Visualization programs. Prior to joining the 
faculty at Texas A&#38;M University, Tim worked in the visual effects and animation industries for thirteen 
years. At Industrial Light &#38; Magic, a division of Lucas.lm Ltd. in San Francisco, California, he 
led teams of artists and research scientists developing processes and producing groundbreaking award 
winning visual effects for .lms. His credit list includes 15 theatrically released feature .lm projects 
including Star Wars: Episode I, War of the Worlds, Van Helsing and Mars Attacks!. At Texas A&#38;M, Tims 
research interests focus on methods of creating expressive animation using the perception of motion 
as a guide, using technology to facilitate creative problem solving among distant collaborators, and 
evidence-based visual development of video games for training and simulation. Tim serves on the Board 
of Directors of the Visual Effects Society, and is a member of the Association for Computing Machinery, 
and the International Game Developers Association.  References GRIGNON, R., HUANG, M., AND VOGT, R. 
2008. Merging bipedal and quadrupedal functionality into one rig for madagascar: Escape 2 africa. In 
ACM SIGGRAPH 2008 talks, ACM, New York, NY, USA, SIGGRAPH 08, 49:1 49:1. JOSHI, P., MEYER, M., DEROSE, 
T., GREEN, B., AND SANOCKI, T. 2007. Harmonic coordinates for character articulation. ACM Trans. Graph. 
26 (July). KA.C-ALESI Z., NORDENSTAM, M., AND BULLOCK, D. CI C, 2003. A practical dynamics system. 
In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation, Eurographics Association, 
Aire-la-Ville, Switzerland, Switzerland, SCA 03, 7 16. LEWIS, J. P., CORDNER, M., AND FONG, N. 2000. 
Pose space deformation: a uni.ed approach to shape interpolation and skeleton-driven deformation. In 
Proceedings of the 27th annual conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley 
Publishing Co., New York, NY, USA, SIGGRAPH 00, 165 172. MCLAUGHLIN, T., AND SUMIDA, S. S. 2007. The 
morphology of digital creatures. In ACM SIGGRAPH 2007 courses, ACM, New York, NY, USA, SIGGRAPH 07. MCLAUGHLIN, 
T. 2005. Taxonomy of digital creatures: interpreting character designs as computer graphics techniques: 
Copyright restrictions prevent acm from providing the full text for this work. In ACM SIGGRAPH 2005 
Courses, ACM, New York, NY, USA, SIGGRAPH 05. MCLAUGHLIN, T. 2006. Taxonomy of digital creatures: de.ning 
character development techniques based upon scope of use. In ACM SIGGRAPH 2006 Courses, ACM, New York, 
NY, USA, SIGGRAPH 06. MONKS, C., NEWALL, M., PHILLIPS, C., POPRAVKA, N., SMITH, J., AND TOOLEY, J. 2004. 
Posing as a werewolf: the creature matchmove tool used for van helsing : Copyright restrictions prevent 
acm from providing the full text for this work. In ACM SIGGRAPH 2004 Sketches, ACM, New York, NY, USA, 
SIGGRAPH 04, 51 . O NEILL, R. 2008. Digital Character Development: Theory and Practice. Morgan Kaufmann 
Publishers. PE NA, B. A. 2011. Performance-guided character bind pose for deformations. Master s thesis, 
Texas A&#38;M University. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037642</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>329</pages>
		<display_no>6</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Filtering approaches for real-time anti-aliasing]]></title>
		<page_from>1</page_from>
		<page_to>329</page_to>
		<doi_number>10.1145/2037636.2037642</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037642</url>
		<abstract>
			<par><![CDATA[<p>For more than a decade, supersample anti-aliasing (SSAA) and multisample anti-aliasing (MSAA) have been the gold-standard anti-aliasing solutions in games. However, these techniques are not well suited for deferred shading or fixed environments like the current generation of consoles. In recent years, industry and academia have been exploring alternative approaches, where anti-aliasing is performed as a post-processing step. The original, CPU-based morphological anti-aliasing (MLAA) method gave birth to an explosion of real-time anti-aliasing techniques that rival MSAA.</p> <p>Most of these techniques share concepts and ideas, so the main goal of this course is to establish a conceptual link between them, identifying novelties and differences. The presenters explain how sub-pixel data can be used to improve quality and performance tradeoffs at post-processing steps, which is a cutting-edge research area today. The course includes an overview of both research and industry filter-based anti-aliasing techniques in games for all modern platforms (AMD and NVIDIA GPUs, PlayStation 3, and Xbox 360), low-level insight to ease adoption of these techniques and give attendees a complete concept-to-implementation roadmap, and deep quality, performance, and ease-of-integration comparisons of each technique.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Real time</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010570</concept_id>
				<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Documentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808977</person_id>
				<author_profile_id><![CDATA[81443594174]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jorge]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jimenez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808983</person_id>
				<author_profile_id><![CDATA[81100022708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gutierrez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808984</person_id>
				<author_profile_id><![CDATA[81440593019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808985</person_id>
				<author_profile_id><![CDATA[81314494641]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reshetov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808986</person_id>
				<author_profile_id><![CDATA[81488667264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Pete]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Demoreuille]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808987</person_id>
				<author_profile_id><![CDATA[81488669059]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tobias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berghoff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808988</person_id>
				<author_profile_id><![CDATA[81488670724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Cedric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perthuis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808989</person_id>
				<author_profile_id><![CDATA[81488664605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808990</person_id>
				<author_profile_id><![CDATA[81327490282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Morgan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGuire]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808978</person_id>
				<author_profile_id><![CDATA[81488670012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lottes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808979</person_id>
				<author_profile_id><![CDATA[81453655927]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Hugh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808980</person_id>
				<author_profile_id><![CDATA[81442598009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Emil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Persson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808981</person_id>
				<author_profile_id><![CDATA[81466642793]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Dmitry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andreev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808982</person_id>
				<author_profile_id><![CDATA[81100306440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>14</seq_no>
				<first_name><![CDATA[Tiago]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sousa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andreev, D. 2011. Anti-aliasing from a different perspective. In <i>Game Developers Conference 2011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944748</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chajdas, M., McGuire, M., and Luebke, D. 2011. Sub-pixel reconstruction antialiasing. In <i>Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572789</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Iourcha, K., Yang, J. C., and Pomianowski, A. 2009. A directionally adaptive edge anti-aliasing filter. In <i>Proceedings of the Conference on High Performance Graphics 2009</i>, ACM, New York, NY, USA, HPG '09, 127--133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Jimenez, J., Masia, B., Echevarria, J. I., Navarro, F., and Gutierrez, D. 2011. <i>GPU Pro 2</i>. AK Peters Ltd., ch. Practical Morphological Anti-Aliasing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Malan, H. 2010. <i>GPU Pro: Advanced Rendering Techniques</i>. AK Peters Ltd., ch. Edge Anti-aliasing by Post-Processing, 265--289.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Persson, E., 2011. Geometric post-process anti-aliasing. http://www.humus.name/index.php?page=3D&ID=86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572787</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Reshetov, A. 2009. Morphological antialiasing. In <i>Proceedings of High Performance Graphics</i>, 109--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Filtering Approaches for Real-Time Anti-Aliasing www.iryoku.com/aacourse Jorge Jimenez1 Diego Gutierrez1 
Jason Yang2 Alexander Reshetov3 Pete Demoreuille4 Tobias Berghoff5 Cedric Perthuis6 Henry Yu7 Morgan 
McGuire8,9 Timothy Lottes8 Hugh Malan10 Emil Persson11 Dmitry Andreev12 Tiago Sousa13 8 NVIDIA 5 Sony 
Computer En 1 Universidad de Zaragoza 2 AMD 3 Intel Labs 6 Sony Computer Entertainment W9 Williams College 
10 CCP 11 Avalanche Studios tertainment WWS ATG 4 Double Fine WS Santa Monica 12 Lucas Arts 7 Kalloc 
Studios 13 Crytek  Figure 1: Examples from Star Wars: The Force Unleashed 2 (DLAA) and God of War III 
(MLAA). Original and anti-aliased images are split horizontally; zoom-in to appreciate the details. We 
encourage downloading the latest version of these course notes in http://www.iryoku.com/aacourse. 1 Intro 
For more than a decade, Supersample Anti-Aliasing (SSAA) and Multisample Anti-Aliasing (MSAA) have been 
the gold standard anti-aliasing solution in games. However, these techniques are not well suited for 
deferred shading or .xed environments like the current generation of consoles. Recently, Industry and 
Academia have begun to explore alternative approaches, where anti-aliasing is performed as a post-processing 
step. The original, CPU-based Morphological Anti-Aliasing (MLAA) method gave birth to an explosion 
of real-time anti-aliasing techniques that rival MSAA. This course will cover the most relevant ones, 
from the original MLAA to the latest cutting edge advancements. In the following text, we will describe 
each technique, and establish the conceptual links between them to give the rationale behind the selection 
of techniques. One of the .rst works that awakened the interest in .lter-based approaches may be that 
of Jason Yang and colleagues (A Directionally Adaptive Edge Anti-Aliasing Filter), given its implementation 
at driver level. Using a custom, weighted resolve it increases the number of steps in the gradients generated 
by MSAA, increasing the perceived anti-aliasing quality. The resolve is done by using the length of the 
isolines that cross a pixel as subpixel weights. This allows quality equal to a standard hardware resolve 
with 2 to 3 times less the number of samples. Jason Yang (AMD) will present the details of this technique. 
Unfortunately, many popular rendering techniques such as deferred shading cannot directly take advantage 
of MSAA, and are best suited to non-multisampled framebuffers. The advent of MLAA demonstrated that high-quality 
antialiasing results can be obtained by inferring subpixel coverage and forming a plausible antialiased 
edge from such a buffer. Alex Reshetov (Intel Labs) will explain the core idea of MLAA, and describe 
the problems that following implementations solve. Although MLAA was aimed at of.ine ray-tracers, this 
changed in late 2009, when the Playstation 3 game God of War III started using MLAA as its AA solution. 
The algorithm was moved to the PS3s Synergistic Processing Units, freeing up signi.cant amounts of GPU 
time over MSAA, while delivering very high image quality. The code was subsequently made available to 
all PS3 developers and is being deployed in games such as Killzone 3, LittleBig-Planet 2, as well as 
a host of other titles. Tobias Berghoff (SCE WWS ATG) will for the .rst time reveal the inner workings 
of this method and share recent improvements and lessons learned. Cedric Perthuis (SCE WWS Santa Monica) 
will discuss the integration into God of War III, showing the bene.ts and pitfalls of the technique 
live on a special build of the game. However, SPU hardware is only available on the PS3 platform. The 
work of Jorge Jimenez and colleagues addressed this issue, being the .rst GPU adaptation that performed 
in practical execution times for the PC platform, by transforming the algorithm to use texture structures 
and making extensive use of hardware facilities and precomputation. Jorge Jimenez (Universidad de Zaragoza) 
will explain the mapping of the original algorithm to a GPU, as well as all the details behind Jimenez 
s MLAA. In concurrent work, Demoreuille devised a very ef.cient Hybrid CPU/GPU implementation for the 
Xbox 360, that was deployed with the game Costume Quest, the .rst known game to ship with MLAA in the 
Xbox 360. Pete Demoreuille (Double Fine) will describe this hybrid approach, including edge detection 
routines and integration issues for Double Fine games. Conceptually, MLAA-like algorithms consist of: 
a) detecting edges; b) calculating neighborhood weights; and c) blending with the neighborhood. From 
now on, a series of alternative techniques will be presented that replace, improve or approximate various 
of these components, but that are in essence, closely related. When working with a non-multisampled image, 
undersampling problems with subpixel features are unavoidable. Chajdas et al. introduced Subpixel Reconstruction 
Anti-Aliasing (SRAA), which extends MLAA with additional buffers that provide this information at low 
cost. The key difference with MLAA is that SRAA calculates blending weights from a multisampled depth 
buffer by using a continous edge detection, which allows blending weights to be determined without an 
expensive pattern search. Morgan McGuire (Williams College and NVIDIA) will give an insight into this 
technique. Fast approXimate Anti-Aliasing (FXAA) is a cutting-edge, unreleased technique being developed 
in NVIDIA by Timothy Lottes. FXAA deals with edge aliasing in similar way to MLAA, but to maximize performance 
it adapts the algorithm to take advantage of speci.c hardware features. For example, FXAA takes advantage 
of anisotropic texture fetch for an approximate end of edge search. It can be considered a dual algorithm, 
as it is able to detect and reduce both sub-pixel and edge aliasing problems. Timothy Lottes (NVIDIA) 
will describe GPU optimizations and FXAA s method to handle sub-pixel aliasing. In concurrent work with 
MLAA, Hugh Malan devised Distance-to-Edge Anti-Aliasing (DEAA), which is very similar in spirit. However, 
DEAA departs from the MLAA pattern search, eliminating it from the pipeline: instead, the forward rendering 
pass pixel shaders calculate the distance to each triangle edge with subpixel precision, and stores them 
in a separate rendertarget. The postprocess pass uses this information to derive blend coef.cients. Hugh 
Malan (CCP) will present this technique, giving additional results from the initial work to integrate 
this anti-aliasing technique with the MMO APB. In the middle of 2010, Dmitry Andreev introduced Directionally 
Localized Anti-Aliasing (DLAA), which was the method of choice for Star Wars: The Force Unleashed 2. 
This technique makes a further simpli.cation over MLAA, by working in a perceptual space: exact gradients 
produced by the exact covered areas may not be required. Instead of calculating weights, it makes use 
of vertical and horizontal blurs to produce gradients in the aliased edges. Gradients with different 
number of steps are used for the two kind of edges considered: short and long. DLAA doesn t replace pattern 
search but totally eliminates it. Dmitry Andreev (Lucas Arts) will present an in-detail description of 
DLAA. All the techniques presented so far use spatial .lters to prevent aliasing. However, there is also 
a growing trend towards the use of temporal .lters to attenuate aliasing in games, Halo: Reach and Crysis 
2 being two pioneers in this respect. In the Crysis 2 approach, anti-aliasing is calculated by accumulating 
frames over time, by using temporal reprojection. Tiago Sousa (Crytek) will provide all the details 
of the Crysis 2 approach, .nishing the course with a slightly different approach to make sure the audience 
knows that the sky is the limit. The course is aimed at all attendees, from casual users who just want 
to better understand post-processing anti-aliasing techniques to researchers and game developers, for 
whom we provide implementation details and code-snippets to allow them to quickly explore their own 
ideas in this explosively growing area. We believe this course may lead to more coordinated efforts in 
the following years, and serve as a solid base for future research. 2 About the Lecturers Jorge Jimenez 
Universidad de Zaragoza http://www.iryoku.com/ Jorge Jimenez is a real-time graphics researcher at the 
Universidad de Zaragoza, in Spain, where he received his BSc and MSc degrees, and where he is pursuing 
a PhD in real-time graphics. His interests include real-time photorealistic rendering, special effects, 
and squeezing rendering algorithms to be practical in game environments. He has various contributions 
in books and journals, including Transaction on Graphics, where our skin renderings made the front cover 
of the SIGGRAPH Asia 2010 issue. Diego Gutierrez Universidad de Zaragoza http://giga.cps.unizar.es/ diegog/ 
 Diego Gutierrez is an Associate Professor at the Universidad de Zaragoza, where he got his PhD in computer 
graphics in 2005. He now leads his group s research on graphics, perception and computational photography. 
He is an associate editor of three journals, has chaired and organized several conferences and has served 
on numerous committees including the SIGGRAPH and Eurographics conferences. Jason Yang AMD Jason Yang 
is a Principal Member of Technical Staff for Advanced Technology Initiatives at AMD where he has contributed 
to graphics, physics, video, encryption, and GPGPU technologies. Recently he worked on the Radeon 6000 
series demo titled HK2207 . He received his BS and PhD from MIT. Alexander Reshetov Intel Labs http://visual-computing.intel-research.net/ 
people/alex.htm Alex Reshetov received his Ph.D. degree from Keldysh Institute for Applied Mathematics 
(in Russia). He joined Intel Labs in 1997 as a senior staff researcher after working for two years at 
the Super-Conducting Super-Collider Laboratory in Texas. His research interests span 3D graphics algorithms 
and applications, and physically based simulation. Pete Demoreuille Double Fine Pete Demoreuille is a 
Lead Programmer at Double Fine Productions, working on all aspects of the engine and graphics technology. 
Prior to Double Fine he was at Pixar Animation Studios, working on the internal animation and modeling 
systems, and developing new rendering and lighting tools for Cars. Tobias Berghoff Sony Computer Entertainment 
WWS ATG Tobias Berghoff .rst got paid for making games when he joined Ascaron Entertainment in 2005. 
Later, he spent a year making GPUs stare at the sun for the Royal Observatory of Belgium, before moving 
on to the Advanced Technology Group of Sony Computer Entertainment s World Wide Studios. He now works 
on graphics related technologies for PS3 and NGP. Tobias holds an Informatik Diplom from the University 
of Paderborn. Cedric Perthuis Sony Computer Entertainment WWS Santa Monica Cedric Perthuis started his 
work in the graphics community at Intrinsic Graphics in 2002. He quickly joined Sony Computer Entertainment 
to build the original Playstation Cross Media Bar. After some work on embedded devices in the startup 
Emdigo, he co-developed Cg for PS3 in the PSGL group. He then became lead PS3 engineer for the Swedish 
studio Grin, before joining Sony Santa Monica Studio where he has been actively researching, developing 
and optimizing new rendering techniques. He recently shipped God of War 3. Cedric has a Master s degree 
in Applied Mathematics and Computer Science from ENSEEIHT, Toulouse, France. Henry Yu Kalloc Studios 
Henry Yu is the founder and technical director of Kalloc Studios, in developing game engine technology 
and tool pipeline for consoles and PC platforms. Morgan McGuire NVIDIA and Williams College http://www.cs.williams.edu/ 
morgan/ Morgan McGuire is a professor at Williams College and visiting professor at NVIDIA Research. 
He co-chaired the I3D 2008, 2009, and NPAR 2010 conferences, is a member of the Journal of Graphics, 
Game, and GPU Tools editorial board, and the lead author of Creating Games: Mechanics, Content, and 
Technology. He has contributed to many commercial products including the E-Ink display for the Amazon 
Kindle, the PeakStream high-performance computing infrastructure acquired by Google, the Titan Quest 
role playing game, and the Marvel Ultimate Alliance 2 video game for Xbox 360. His current research is 
in high-performance parallel algorithms and sampling techniques. Timothy Lottes NVIDIA http://timothylottes.blogspot.com/ 
 Timothy Lottes is an Engineer in the Developer Technology group at NVIDIA. Prior Timothy was a console 
game developer at Human Head Studios, in Systems Research and Development at Industrial Light and Magic, 
and owner/photographer/developer of Farrar Focus, a .ne art landscape photography and digital photo development 
tools business. Hugh Malan CCP Hugh Malan is a graphics programmer working on Dust 514 at CCP, in Newcastle. 
Previously he worked as graphics lead for Crackdown and MyWorld for Realtime Worlds. Hugh is a graduate 
of Victoria University and Otago University, New Zealand. Emil Persson Avalanche Studios http://www.humus.name/ 
 Emil Persson is a graphics programmer at Avalanche Studios where he is working on advanced rendering 
techniques and optimizations. Previously he worked as an ISV Engineer at ATI/AMD developer relations 
where he assisted the top game developers with rendering techniques and optimizations, in addition to 
R&#38;D and SDK development. Emil also runs the site www.humus.name where he provides open source graphics 
samples to the community. Dmitry Andreev Lucas Arts http://and.intercon.ru/ Dmitry Andreev is a rendering 
engineer and video games developer with more than 10 years of experience. He started his journey from 
8-bit machines and got famous for his 64k intros in the demoscene, pushing the bar of competition at 
the time. Worked on all variety of game genres including FPS, RPG and Action games, leading development 
of core technology and graphics. Over past years he has been pushing real-time graphics in high-end AAA 
titles to its limits on PlayStation3 and XBox360 consoles with elegant and simple high-performance solutions. 
B.S. in Applied Mathematics and Mechanics. Tiago Sousa Crytek Tiago Sousa is Crytek s Principal R&#38;D 
Graphics Engineer, where he has worked for past 8 years, on all Crytek numerous demos, shipped game titles 
and engines, including Far Cry, Crysis and more recently .nished Crysis 2 -Crytek s .rst multiplatform 
game. He is a self-taught graphics programmer, who before joining Crytek army on the cause of world domination, 
cofounded a pioneering game development team in Portugal and very brie.y studied computer science at 
Instituto Superior Tecnico, which he still has hopes to .nish one day. 3 Morphological Antialiasing 
Speaker: Alexander Reshetov (Intel Labs) Morphological AntiAliasing (MLAA) algorithm belongs to a family 
of data-dependent .lters allowing ef.cient anti-aliasing at a post-processing step. The algorithm infers 
sub-pixel coverage by Figure 2: Fairy Forest model: morphological antialiasing improves the quality 
of the rendered image without having a significant impact on performance. The algorithm uses separation 
lines falling between perceptually different pixels to infer silhouette lines and then blend colors around 
such silhouettes. estimating plausible silhouettes from a collection of axis-aligned separation lines 
that fall between perceptually different pixels (see Figure 2). The algorithm consists of the following 
steps: 1. Noticeably different pixels are identi.ed. Figure 3 shows a sample image with solid axis-aligned 
lines separating different pixels. 2. Piecewise-linear silhouette lines are derived from the separation 
lines. This is illustrated in Figure 3 with a Z-shape formed by b-c-d lines and a U-shape de.ned by d-e-f 
lines. In MLAA, silhouette segments originate at the edges of pixels that have both horizontal and vertical 
separation lines (all such pixels are shown with stripped shading). Not all potential end-points are 
used. The exact placement of end-points is somewhat arbitrary, with half-edge points producing satisfactory 
results. 3. Color .ltering is performed for all pixels intersected by silhouette lines. Essentially, 
this is done by propagating colors on opposite sides of separation lines into polygons formed by silhouettes 
and separation lines, as illustrated at the bottom of Figure 3. The areas of these polygons are used 
for color mixing.  Any data that helps quantify differences between pixels can be used as an input at 
the .rst step of the algorithm (z-depth, normals, material ids, etc.) At the same time, the smallest 
amount of data to use is, obviously, color data itself. Since similarly-colored pixels tend to group 
together, this information can be used to infer plausible silhouettes between such groups. We use luminance 
according to ITU-R recommendations. To identify all valid silhouettes (at the second step of the algorithm), 
we .nd all possible horizontal and vertical separation lines (between different pixels) and then 2.1. 
look at all start/end points on adjacent orthogonal lines and 2.2. choose the longest segment (preferring 
Z-shapes over Ushapes if multiple shapes are possible). This is illustrated in Figure 4; 2.3. if both 
horizontal and vertical silhouette lines intersect a pixel, we process only the longest silhouette line 
(preferring the horizontal one in the case of a tie). In Figure 3, both light-blue and dark-blue end-points 
can generate silhouette lines, but we choose only those bounded by dark-blue points. Super-sampling is 
the gold standard for antialiasing, since it emulates integration processes in a camera or a human eye 
by averaging multiple samples per pixel. In MLAA, we assume that samples on the same side of a given 
silhouette line have the same color (pink and khaki colors in Figure 3). This allows evaluating the integral 
by computing areas of trapezoids formed by the silhouette line and the corresponding separation line. 
These areas vary linearly from pixel to pixel, allowing an ef.cient implementation. Of course, this approximation 
breaks when multiple silhouette lines cross the pixel, resulting in overblurring. Among MLAA advantages, 
 it is a universal algorithm, which can operate on the least possible amount of data pixel colors; 
 it is embarrassingly parallel and independent from the rendering pipeline, allowing ef.cient implementation 
on modern hardware;  the quality is comparable with 4X supersampling.  At the same time, by relaying 
on a single sample per pixel, MLAA is predisposed to creating image artifacts in areas where image features 
are commeasurable with pixel sizes (Nyquist limit);  it is susceptible to temporal artifacts;  varying 
lighting can trigger silhouette changes in static scenes (if a color-based edge detector is used);  
it can mangle small text;  there could be additional artifacts near image border as for any other screen-space 
technique.  We refer the reader to Morphological Antialiasing [Reshetov 2009] for full details. 4 A 
Directionally Adaptive Edge Anti-Aliasing Filter Speaker: Jason Yang (AMD) Authors: Konstantine Iourcha 
and Jason Yang and Andrew Pomianowski In current GPU multisample anti-aliasing (MSAA), a pixel is limited 
by the number of available hardware samples. So, with 8x AA, meaning eight hardware samples, with traditional 
MSAA you can achieve at most eight levels of color gradation along an edge. Given the horsepower of todays 
GPUs, can we do better? Our .lter uses existing MSAA hardware and improves .ltering by going outside 
the pixel. The main idea is that in areas of low edge Figure 3: A sample picture illustrates the main 
MLAA concepts. Solid axis-aligned lines separate different pixels. Top: reconstructed silhouettes are 
shown as red lines. Bottom: .ltering through color propagation. Figure 4: Ambiguity resolution: even 
though multiple silhouette lines are possible, the longest one is used. curvature, you can imagine isolines 
of the image function. You can then use samples outside of a pixel on these isolines to reconstruct a 
pixel. How do you .nd the isolines? You can use a linear approximation by reducing this to a least squares 
problem. Basically .tting a plane to the color samples. Or think of it as a linear regression. This should 
be the A-ha moment to solving the problem. So, once this problem you know the gradient, then you use 
the integration method from the paper. The basic idea is you use samples both inside and outside of 
the pixel where the weight of the sample is determined by how much the isoline intersects with the pixel. 
See A directionally adaptive edge anti-aliasing .lter [Iourcha et al. 2009] for full details. 5 Practical 
Morphological Anti-Aliasing (Jimenez s MLAA) Speaker: Jorge Jimenez (Universidad de Zaragoza) Authors: 
Jorge Jimenez and Belen Masia and Jose I. Echevarria and Fernando Navarro and Diego Gutierrez Multisample 
anti-aliasing (MSAA) remains the most extended solution to deal with aliasing, crucial when rendering 
high quality graphics. Even though it offers superior results in real time, it has a high memory footprint, 
posing a problem for the current generation of consoles, and it implies a non-negligible time consumption. 
Further, there are many platforms where MSAA and MRT (multiple render targets, required for fundamental 
techniques Figure 5: Images obtained with Jimenez s MLAA. Insets show close-ups with no anti-aliasing 
at all (left) and processed with our technique (right). Images from Fable R . III courtesy of Lionhead 
Studios. such as deferred shading) cannot coexist. The majority of alternatives to MSAA which have been 
developed, usually implemented in shader units, cannot compete in quality with MSAA, which remains the 
gold standard solution. This work introduces an alternative anti-aliasing method offering results whose 
quality averages at 16x MSAA (from a gradients quality perspective) at a fraction of its memory and time 
consumption (see Figure 5 for some ex amples). Besides, the technique works as a post-process, and can 
therefore be easily integrated in the rendering pipeline of any game architecture. The technique is an 
evolution of the work Morphological Antialiasing , which is designed for the CPU and unable to run in 
real time. The method presented here departs from the same underlying idea, but was developed to run 
in a GPU, resulting in a completely different, extremely optimized, implementation. We shift the paradigm 
to use texture structures instead of lists (See Figure 6), which in turn allows to handle all pattern 
types in a symmetric way, thus avoiding the need to decompose them into simpler ones, as done in previous 
approaches. In addition, pre-computation of certain values into textures allows for an even faster implementation. 
The algorithm detects borders (using color, depth, normals or instance id s information) and then .nds 
speci.c patterns in these. Anti-aliasing is achieved by blending pixels in the borders intelligently, 
according to the type of pattern they belong to and their position within the pattern. Pre-computed textures 
(see Figure 7) and extensive use of hardware bilinear interpolation (see Figures 8 and 9) to smartly 
fetch multiple values in a single query are some of the key factors for keeping processing times at a 
minimum. Typical execution times are 1.3 ms on Xbox 360 and 0.44 ms on a nVIDIA GeForce 9800 GTX+, for 
a resolution of 720p. Memory  Original Image Edges texture Blending weights texture Anti-aliased Image 
Figure 6: In Jimenez s MLAA, and starting from an aliased image (left), edges are detected and stored 
in the edges texture (center left). The color of each pixel depicts where edges are: green pixels have 
an edge at their top boundary, red pixels at their left boundary, and yellow pixels have edges at both 
boundaries. The edges texture is then used in conjunction with the precomputed area texture to produce 
the blending weights texture (center right) in the second pass. This texture stores the weights for the 
pixels at each side of an edgel in the RGBA channels. In the third pass, blending is performed to obtain 
the .nal anti-aliased image (right). Figure 7: Patterns processed by Jimenez s MLAA (left) and their 
corresponding pre-calculated weights depending on their size (right). Each 9  9 subtexture corresponds 
to a pattern type. Inside each of these subtextures (u, v) coordinates encode distances to the left and 
to the right, respectively. Figure 8: In Jimenez s MLAA, hardware bilinear .ltering is used when searching 
for distances from each pixel to the end of the line. The color of the dot at the center of each pixel 
represents the value of that pixel in the edges texture. In the case shown here, distance search of the 
left end of the line is performed for the pixel marked with a star. Positions where the edges texture 
is accessed, fetching pairs of pixels, are marked with rhombuses. This allows us to travel double the 
distance with the same number of accesses. footprint is 2x the size of the backbuffer on Xbox 360 and 
1.5x on the 9800 GTX+. Meanwhile, 8x MSAA takes an average of 5 ms per image on the same GPU at the same 
resolution, 1180% longer for the PC case (i.e. processing times differ by an order of magnitude). The 
method presented can therefore challenge the current gold standard in real time anti-aliasing. See Practical 
Morphological Anti-Aliasing [Jimenez et al. 2011] for full details. Visit http://www.iryoku.com/mlaa 
for latest news, source code releases, a showcase gallery, performance tables, a 0.0 0.25 0.75 1.0 Figure 
9: Examples of the four possible types of crossing edge and corresponding value returned by the bilinear 
query of the edges texture (Jimenez s MLAA). The color of the dot at the center of each pixel represents 
the value of that pixel in the edges texture. The rhombuses, at a distance of 0.25 from the center of 
the pixel, indicate the sampling position, while their color represents the value returned by the bilinear 
access. F.A.Q., and more. 6 Hybrid Morphological Anti-Aliasing Speaker: Peter Demoreuille (Double Fine) 
Despite the original Morphological Anti-Aliasing (Reshetov) approach being purely CPU-based and seemingly 
unsuitable for realtime use, the attractive properties of a high-quality post-process antialiasing solution 
have driven the development of several implementations suitable for real-time applications. Practical 
Morphological Anti-Aliasing (Jimenez et al) presents a optimized GPU adaptation, including implementation 
details for PC and the Xbox 360. In their approach, all three fundamental stages of the algorithm (edge 
detection, blend weight calculation and image .ltering) are performed on the GPU in separate passes. 
Memory requirements are low, and performance is very good (0.44 ms on a nVIDIA GeForce 9800 GTX+ at 
720p), though GPU utilization may be unpredictable, as it is a function of the number of edges found 
in the scene. Low latency MLAA in God of War III and PlayStation EDGE MLAA (Perthuis and Berghoff) present 
an optimized implementation for the PlatStation3 where the entire algorithm is performed in parallel 
on several SPUS, of.oading all work from the GPU. Hybrid Morphological Anti-Aliasing presents an implementation 
of MLAA for the Xbox 360 where the edge detection and .ltering stages of the algorithm are performed 
on a GPU, and the blend weight calculations are performed on CPUs. This approach shares the low memory 
requirements of Practical Morphological Anti-Aliasing, and of.oads a great deal of work from the GPU 
just as PlatStation EDGE MLAA. However, a hybrid CPU/GPU algorithm has several advantages when compared 
to a pure CPU or GPU algorithm. Utilizing the GPU trivially allows the use of non-color data when detecting 
edges (such as depth, material and normal data), allowing more stable edges and avoiding some jittering 
artifacts common when using only color. Use of a GPU also allows the use of linear-intensity colors, 
both when detecting edges and when blending. Lastly, using the CPU to compute blend weights saves a great 
deal of GPU time, and leaves the GPU performing only those passes that have small, .xed costs, allowing 
GPU performance of the algorithm to be strictly bound. While using the GPU for some stages has advantages, 
using the CPUs for others has tradeoffs, including non-trivial CPU/GPU communication and potentially 
heavy utilization of CPU time. We present techniques to optimize calculations by reducing the bandwidth 
requirements of the algorithm, including packing data and performing a very fast transpose of the input 
data to optimize cache utilization during vertical edge calculations. CPU utilization is further reduced 
by adapting calculations to be better suited to the Xbox s PowerPC architecture. Additional controller 
latency may be added due to overhead in CPU/GPU communication, and we describe how our implementation 
rearranges work to avoid adding latency to our titles. As each of our titles has unique visual styles 
and different average scene construction, several edge detection techniques were used to ensure .ltering 
produced acceptable results. We will present these techniques and various tips to avoid overblurring, 
missing edges, and maintaining stable .ltering. As each stage of the MLAA algorithm may be tuned for 
individual applications, we hope to provide additional options for developers creating their own anti-aliasing 
solutions, so that they may best .t the performance, quality and memory restrictions placed upon them. 
 7 PlayStation R  @Edge MLAA Speakers: Tobias Berghoff and Cedric Perthuis Authors: Tobias Berghoff, 
Cedric Perthuis and Matteo Scapuzzi PlayStation R Edge MLAA is an implementation of the Morphological 
Anti-Aliasing (MLAA) algorithm for the Sony PlayStation R 3 console. It is used in a number of high-pro.le 
games and has undergone a series of improvements and optimizations since its inception. In this talk, 
we will provide an overview of this process and its results, focusing on how to .t the algorithm to the 
PS3TM s unique architecture and signi.cantly improve the image quality provided by the method. Both 
successful and unsuccessful approaches will be examined, to provide the listener with a well rounded 
understanding of the problem space. 7.1 MLAA on SPUs Our implementation runs on the main processing units 
of the Cell Broadband EngineTM, the Synergistic Processing Units, or SPUs. These units are interconnected 
through a high speed ring bus and have each 256KB of private fast low-latency memory (Local Store, or 
LS). The processing elements themselves consist of a highperformance dual-issue SIMD processor coupled 
with a DMA engine. Each of these features had signi.cant in.uence on the design of the system. Initially, 
we experimented with a tile-based implementation, where the image would be divided into rectangular regions, 
each small enough to .t into LS. This would allow the processing of arbitrarily sized images and an 
easy balancing of LS budgets, as the tile size could be changed depending on other needs for memory. 
A library of post-process effects implemented in this fashion is already available in the form of PlayStation 
R  Edge Post, making a uni.ed approach attractive. Due to artifacts caused by the tiling, this implementation 
was discarded early on, and a more classical scan-line approach was chosen and later shipped in God 
of War RThis scan-line based III. approach only implements a horizontal MLAA step and uses an in-place 
pseudo-transpose to implement the vertical step. The algorithm itself begins with an edge detection 
step, followed by a number of sequence algorithms on the edges found. The goal of this approach is to 
quickly reduce the length of the sequence using comparatively cheap loops before executing more expensive 
loops. This approach is viable, as LS accesses are very fast and are executed by a different pipe (the 
odd pipe) in the processor as the ALU operations (which are executed in the even pipe). We can thus use 
the odd pipe to load and prepare data for consumption by the even pipe, as well as pack and store it 
for the next processing step. 7.2 Image Quality Improvements For the original MLAA paper, a very simple 
edge detection system was intentionally used for reasons of simplicity. While this was intended as a 
placeholder for more sophisticated edge detection algorithms, the method is still widely used today. 
While working on God of War R III we quickly realized that the quality of the edge detection was one 
of the most important ingredients for image quality and temporal stability. As a .rst step, the basic 
edge detection system was replaced with a method that is better suited for images with varying levels 
of brightness along features and over time. Furthermore, we introduced the ability to cut an edge into 
multiple parts, thus allowing a better handling of adjacent edges. This is primarily an improvement 
in areas with a very high edge density, where it make the edge cases approach a slight blur instead 
of adding additional noise to them, as shown in .gure 12. As there are limits to how well we can detect 
an edge in color data alone, we introduced the concept of a predicated edge detection, which .rst performs 
edge detection on non-color data and then controls the sensitivity of the color data edge detection 
based on this. The non-color data can be generated for this speci.c purpose, like object IDs, or be a 
byproduct of the rendering process like depth. Of course, the availability of suitable data is highly 
dependent on the rendering systems of individual games, and as such this method is less generic than 
a purely color based one. Figure 11 shows a comparison of the thresholding methods. Pred icated thresholding 
has important advantages over approaches that do not base the .nal decision of processing a edge on the 
color data. 8 Subpixel Reconstruction Anti-Aliasing (SRAA) Speaker: Morgan McGuire (NVIDIA and Williams 
College) Authors: Matthaus G. Chajdas, Morgan McGuire, David Luebke Using the color/depth buffer at 
pixel-resolution has the problem that edges may .icker as the edge .ips back and forth between adjacent 
pixels. Without using additional information, for instance backprojection, this is an issue which makes 
pure post-processing algorithms unstable. SRAA tries to circumvent the problem by taking sub-pixel samples 
to .nd edges while keeping the shading still at 1 sample/pixel to guarantee good coherence and minimal 
changes to the existing pipeline (see Figure 13). In particular, SRAA requires some edge detector similar 
to Jimenez MLAA. The edge detector is used to determine sample similarity in order to reconstruct the 
color at each sample. Once that is done, the samples get .ltered typically using a box .lter and can 
be sent to the post-processing pipeline. SRAA works on geometric edges only. That is, no shader or texture 
aliasing can be resolved. Discontinuities are found by investigating a super-resolution buffer, so both 
pixel as well as sub-pixel aliasing can be resolved. This makes SRAA more stable under animation, in 
particular, slow-moving edges are less prone to swimming artifacts. SRAA works best on deferred renderers, 
as all information required for SRAA is readily available in the G-Buffers. However, SRAA can be also 
used with forward rendering as long as the information necessary to determine edges is generated. This 
would be typically done as part of a z-prepass. The most basic implementation of SRAA requires the following 
inputs: Depth and normal buffers with MSAA enabled  Colour buffer without any MSAA (i.e. one shaded 
sample per pixel)  One important requirement is that the colour buffer sample must line up with the 
MSAA samples; the easiest way to guarantee is to simply only shade the very .rst MSAA sample, for instance 
by resolving the buffer with a pixel shader. SRAA works in two steps: For each sample from the MSAA input: 
 If the sample is shaded, continue  Otherwise: Compare the geometric information against the neighboring 
shaded samples.  Copy the color from the shaded sample, weighted by similarity.   For each pixel: 
Filter all samples inside the .lter support. In order to minimize blurring, a Gaussian with strong falloff 
should be used, or a comparatively sharp .lter. It conceptually reconstructs color at each MSAA position 
and then resolves, but since the computation of each happens simultaneously, the high-resolution color 
is never actually stored. That is, there s a single register that accumulates the result of the resolve 
within an unrolled loop. The key part of the algorithm is the similarity metric, which determines whether 
the value should be copied across from the shaded sample or not. A simple metric uses the distance between 
the plane de.ned by the fragment position and normal and the position of the other fragment. Let P (x, 
.n) be the plane at point x with normal .n. We can de.ne a distance between (x0, .n0) and (x1, .n1) 
as max(d(P (x0, .n0),x1),d(P (x1, .n1),x0) with d being the distance from the point to the plane. This 
works reliable in practice, but requires MSAA ed depth and normal buffers. A simple optimization is 
to use the SV PrimitiveID, which is unique for each triangle: simply comparing each sample using the 
primitive ID provides a good approximation and is very fast. The primitive ID can be hashed down to 
8 bit and is thus comparatively cheap. See Subpixel Reconstruction Anti-Aliasing [Chajdas et al. 2011] 
for full details.  9 Fast approXimate Anti-Aliasing (FXAA) Speaker: Timothy Lottes (NVIDIA) The fastest 
of three algorithms which make up FXAA will be covered, FXAA Console: Local Contrast Adaptive Directional 
Edge Blur (see Figure 15 for an example). Algorithm applies a 2 or 4-tap variable length box .lter 90 
degrees to luma gradient, with an adjustment on .lter length and direction to help remove sub-pixel aliasing. 
Works as one full screen pixel shader pass taking color as input and writing color as output. Extremely 
fast algorithm, averaging just 0.11 ms per million pixels on NVIDIA GTX480. Details: Optimized early 
exit on pixels which do not need antialiasing. Early exit test is based on a comparison of local luma 
contrast to local maximum luma.  Estimates .lter direction as perpendicular to local luma gradient. 
Maximum gradient is 1, which corresponds to one pixel in width.  Estimates .lter width by scaling direction 
such that the shortest axis magnitude is 1. This has the effect of lengthening the .lter on nearly horizontal 
or vertical edges, while keeping a very short .lter on the diagonal. Length is decreased as local contrast 
decreases, and .nally maximum .lter width is clamped to 9 pixels.  Estimate single pixel detail using 
the difference between local luma contrast and 2x2 box .ltered local luma contrast. Modify .lter vector 
by increasing length in both x and y by this estimation of single pixel detail. If length was zero, then 
add in the positive direction. This extends the .lter diagonally to increase the amount of blur.  Calculate 
a full-width 4-tap box .lter, and a half-width 2-tap box .lter along the .lter vector. If the luma of 
the full-width .lter result exceeds the range of local luma, discard the fullwidth result and return 
the half-width result. Otherwise return the full-width result. This step removes noise associated with 
bad .lter vector estimates.  10 Distance-to-edge Anti-Aliasing (DEAA) Speaker: Hugh Malan (CCP) Distance-to-edge 
AA (DEAA) simulates antialiasing by selective blurring, similar to MLAA. The main difference with respect 
to MLAA is that the pixel coverage values are derived from distanceto-edge values calculated during 
the forward pass. Scalar texture coordinates have been up so each edge of each triangle has a scalar 
value taking the value 0 along that edge, and 1 on the opposite vertex. During the forward pass pixel 
shader, the value and screen-space derivatives of each scalar are used to estimate the horizontal and 
vertical distance onscreen to the corresponding edge. The process is repeated for each of the interpolated 
scalars, yielding distances to each edge of the triangle. Distances to triangle edges in the four directions 
up, down, left and right are found, and these four distances are written out to a separate rendertarget. 
The postprocess set uses this information to calculate pixel coverage. By considering the distance to 
each edge, the overlap area for each neighbouring pixel is estimated, and the .nal pixel color is a corresponding 
blend of the neighbouring pixel colors. This method has somewhat different strengths and weaknesses to 
MLAA. Since the distance-to-edge values are calculated with subpixel precision, the postprocess blur 
can simulate subpixel effects that are not possible by simple inspection of a framebuffer. (For instance, 
imagine a vertical edge sweeping across the image: with DEAA columns of pixels will fade in and out to 
indicate the subpixel location of the edge. MLAA and other methods that are based on the framebuffer 
can only provide an edge pro.le that advances in single-pixel steps.) Realtime MLAA is limited to a localized 
neighbourhood, so it is unable to provide plausible antialiasing for edges with pixel steps longer than 
this neighbourhood. In comparison, DEAA is able to provide plausible antialiasing effects for edges regardless 
of gradient. Conversely, DEAA is unable to provide antialiasing in several situations where MLAA can. 
DEAA requires all edges to have verts on them, so the distance-to-edge logic can estimate coverage. Interpenetrating 
geometry will produce an edge with no verts; shadow edges and texture edges are other situations where 
DEAA cannot antialias the edge but MLAA can. DEAA will fail to antialias edges correctly in a few other 
cases too. If there is a subpixel triangle beside the edge -perhaps due to foreshortening, or due to 
a very thin piece of geometry -then the distance-to-edge information is incorrect, and this tends to 
produce an aliased region. Very thin gaps are another problem case. See Edge Anti-aliasing by Post-Processing 
[Malan 2010] for full details. 11 Geometry Buffer Anti-Aliasing (GBAA) Speaker: Emil Persson (Avalanche 
Studios) Anti-aliasing in real-time applications (such as games) has in recent years primarily been addressed 
with multisampling (MSAA) or similar approaches such as CSAA. Aside from substantial memory consumption, 
these methods are practical and general for traditional forward rendering approaches, and the characteristics 
are widely known and understood. As more and more game engines switch to a deferred shading model the 
appeal of MSAA has been significantly reduced with added complexity and the memory consumption further 
multiplied. With limited memory on current game consoles alternative approaches get more interesting. 
Recently a number of approaches for performing anti-aliasing as a post-processing step have been proposed. 
This includes MLAA, FXAA, SRAA and DLAA, all with different pros and cons. The unifying idea behind all 
these techniques is that the image is analyzed to .nd discontinuities to smooth, optionally using the 
depth buffer to aid the process. In contrast, we propose that the game engine use its knowledge of the 
underlying geometry instead to smooth edges.  The author has previously proposed GPAA [Persson 2011] 
as a ge ometric post-process anti-aliasing solution. This technique also has a pre-process step and 
potential scaling issues which may reduce its attractiveness for real-time applications such as games. 
To address these problems an alternative approach has been implemented that store geometric edge information 
to a fullscreen render target during the main rendering pass (see Figure 16 for an example). This is 
easiest accomplished with a geometry shader, although a vertex shader implementation is also possible. 
Edge orientation is passed down to the pixel shader that writes the information to an edge buffer. In 
the end the buffer is analyzed and resolved in a fullscreen pass, similar in concept to how traditional 
MSAA works. In the resolve stage each pixel is checked for information about any edge intersecting it, 
and if so, what coverage the primitive that shaded the pixel had (see Figure 17). Depending on the edge 
orien tation relative the pixel center a suitable neighbor pixel is selected for blending. For pixels 
missing intersection information the immediate neighborhood is searched for edges that might apply. 
This is because silhouette edges will only have edge information on pixels on one side of the edge, 
whereas pixels on both sides need to be anti-aliased. Final blending is performed by simply shifting 
the texture coordinate such that the texture unit can blend using a regular linear .lter. As such, only 
a single sample is required from the color buffer. 12 Directionally Localized Anti-Aliasing (DLAA) Speaker: 
Dmitry Andreev (Lucas Arts) Multisample Anti-Aliasing has been the holy grail solution in games for many 
years. But unfortunately, it s not always applicable. The more multi-pass and deferred techniques we 
put in place, to keep increasing visual complexity, the more costly it becomes. Especially on consoles, 
directly and indirectly, when adjusting for all the post-processing effects. Anti-Aliasing From a Different 
Perspective is a story behind Directionally Localized Anti-Aliasing. It shows how a technical problem 
of anti-aliasing could be solved in an artistic way. By going through a series of prototypes, failures 
and successes, geting to the .nal idea of the algorithm and its console speci.c implementation details. 
The proposed solution is a novel anti-aliasing technique which was used in The Force Unleashed 2. It 
is designed with simplicity in mind, that makes it GPU and CPU friendly and allows to have ef.cient 
implementations on modern gaming consoles such as the PlayStation3 and XBox360. It is temporally stable 
and very effective, offering very high quality to performance ratio. See Anti-Aliasing From a Different 
Perspective [Andreev 2011] for full details. 13 Anti-Aliasing Methods in CryENGINE 3.0 Speaker: Tiago 
Sousa (Crytek) 13.1 Introduction Real time rendering Anti-Aliasing has been for many years depending 
on a set of strict rules imposed by graphics hardware. When rendering technology deviates from the golden 
standard, for which hardware was originally designed, issues arise, which require effort and workarounds, 
introducing complexity, increased memory footprint and performance penalties. Such has motivated exploration 
of alternative and robust solutions, where the most popular trend is becoming Anti-Aliasing by post processing, 
popularized by the CPU-based Morphological Anti-Aliasing (MLAA) method. For CryENGINE 3.0 iterations, 
we are exploring different alternatives allowing sub-pixel accuracy and which are orthogonal to a multiplatform 
environment, from performance, video memory and implementation perspective. Our presented work is heavily 
inspired by real time rendering approach to temporal anti-aliasing and by OpenGL s Accumulation Buffer 
and its many distributed ray tracing effects usages, like temporal super-sampling among others. 13.2 
Background The introduction of the consoles PS3 and Xbox 360, opened a big opportunity window for Deferred 
Rendering and its derivatives to become the new standard for real time rendering. Decoupling shading 
or lighting from primitives rendering, brings many bene.ts for all platforms: decreasing amount of drawcalls; 
many light sources can be used, resulting in improved lighting quality and .exibility for lighting artists; 
decreased amount of shader permutations; and an immense set of algorithms approximated in screen space, 
from Screen Space Ambient Occlusion, Screen Space Skin Surface Scattering to Stereo Rendering, are few 
of the examples. This console generation also allowed for High Dynamic Range Rendering to become a wider 
option for real time rendering. Unfortunately render target formats across multiple platforms are not 
standardized and their performance varies signi.cantly, requiring additional effort. Such adds implications 
as well for hardware anti-aliasing due to the aforementioned increased memory and performance requirement. 
 13.3 OpenGL Accumulation Buffer SSAA OpenGL Accumulation Buffer SSAA is achieved by adding a subpixel 
jittering to the view frustum and a linear combination of subsamples. For example, 4x SSAA means rendering 
the scene 4 times, which is not yet very practical for realtime purposes. 13.4 Real Time Rendering Temporal 
AA Real time temporal anti-aliasing, popularly denominated as Motion Blur, has become very popular in 
video games and extensively used in games like Crytek s Crysis 1 and 2, Killzone 2 and Halo 3. Such TAA 
approximation is done in screen space, by applying a directional blur from previous frame to current 
frame using the screen space velocity vector. This velocity vector is computed by projecting previous 
frame pixel, or vertices, world space position to screen space and computing respective delta from current 
and previous frame screen-space position. 13.5 Combining Concepts By combining both concepts, we can 
achieve 2x SSAA or higher, at cost of higher latency for .nal result. This is done by interleaving non-uniform 
sub-pixel jittering to the view frustum for different frames, storing previous frame (s). At end of frame, 
post-tone mapping, previous frame sub samples are fetched by means of velocity vector and all sub samples 
are then blended selectively, based on depth, velocity or color. A Quincunx sampling pattern was also 
used for the 2x SSAA version for approximating 4x SSAA results just with 2 available subsamples. 13.6 
Results Orthogonal multiplatform SSAA solution with sub-pixel accuracy, costing 1ms, at 720p, on PS3/X360 
hardware. At 1080p on PC hardware, costs 0.2 ms.  References ANDREEV, D. 2011. Anti-aliasing from a 
different perspective. In Game Developers Conference 2011. CHAJDAS, M., MCGUIRE, M., AND LUEBKE, D. 2011. 
Subpixel reconstruction antialiasing. In Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D 
Graphics and Games, ACM Press. IOURCHA, K., YANG, J. C., AND POMIANOWSKI, A. 2009. A directionally adaptive 
edge anti-aliasing .lter. In Proceedings of the Conference on High Performance Graphics 2009, ACM, New 
York, NY, USA, HPG 09, 127 133. JIMENEZ, J., MASIA, B., ECHEVARRIA, J. I., NAVARRO, F., AND GUTIERREZ, 
D. 2011. GPU Pro 2. AK Peters Ltd., ch. Practical Morphological Anti-Aliasing. MALAN, H. 2010. GPU Pro: 
Advanced Rendering Techniques. AK Peters Ltd., ch. Edge Anti-aliasing by Post-Processing, 265 289. PERSSON, 
E., 2011. Geometric post-process anti-aliasing. http: //www.humus.name/index.php?page=3D&#38;ID=86. RESHETOV, 
A. 2009. Morphological antialiasing. In Proceedings of High Performance Graphics, 109 116.  Stair stepping 
In this example at most 8 gradations (not including no coverage).  This pixel is considered  filled 
by primitive even though it s about  filled 4 samples at most 4 levels of gradation (5 including non 
covered)  HWAA only deals with geometry edges, not textures.  Edge-Detect Custom Filter AA using 
8x MSAA coverage samples can achieve up to 24 gradations of color. If there s an edge there will be 
isolines, so can use samples outside of the pixel. Effectively doing a linear fit of the RGB samples 
least squares problem. Iourcha, K., Yang, J., and Pomianowski, A. 2009. A Directionally Adaptive Edge 
Anti-Aliasing Filter. High Performance Graphics.   You know there s an edge of the samples are not 
fully covered. Notice elimination of pixels from masking Aids in performance and look. Don t want to 
overfilter corners.   Do the least squares here.    This is a quote from one of the AnandTech forums 
(orthography preserved). Well, there is some truth to this statement. MLAA is, indeed, not patented. 
Unfortunately, it makes small text look terrible. And, obviously, Nvidia wanted, and actually did, something 
similar. Let s talk about what MLAA actually is. If I show you this image, you might think: What a nice 
fella! But if we zoom in on this image, we will see that it is actually ugly. All these zigzag edges... 
you would never see this in real life. On the other hand, we can still recognize eyes, the general face 
oval, and other features. So this is our plan. We want to: 1. Somehow find silhouettes in images. 2. 
Blend colors around the silhouettes.  The chief reason for doing this is to minimize expensive shading 
operations, ideally by doing it only once per pixel. Super-sampling is the gold standard for anti-aliasing, 
since it emulates integration processes in a camera or a human eye. Characteristically, pixels on the 
same side of a given silhouette usually have similar color. (It is certain if we use colors to find silhouettes.) 
Accordingly, we can approximate the integral by: -assigning exactly the same color to all samples on 
the same side of the silhouette, and -computing the integral geometrically (using areas). It will work 
with a real silhouette and with a fake one as well, as long as it is plausible.  In all these methods 
plausible silhouettes are first hallucinated and then used for image improvement. At the same time, 
in rendering, the exact silhouette can actually be recovered from the available 3D data. Such approaches 
will be presented later in the course. If we forego all this additional data and go with a single sample 
per pixel, we will get the simplest possible and the most universal algorithm, but the price for this 
is quality. If we choose to use only a single sample per pixel, we still may try to use data other than 
color to create a more robust algorithm. Still, the smallest amount of data to use is, obviously, the 
color data itself. Since similarly colored pixels tend to group together, this information can be used 
to infer plausible silhouettes between such groups. Different ways to process color data are possible. 
Luminance, perhaps, is the easiest criterion (the International Telecommunication Union standard is more 
appropriate for LCDs, compared with the CIELAB definition for CRTs). Furthermore, since the performance 
may be throttled by bandwidth anyway, additional non-linear transformations may be used as well without 
any adverse effects. One possibility is to downplay differences in darker regions that are not noticeable 
by the human eye, as was done in the game God of War III. With a single sample per pixel, we have to 
go outside the pixel to acquire additional knowledge. It might be rather complicated, since we may not 
have nice, easily traceable silhouettes in all cases. If we go back to our fella, his eyes are easily 
identifiable. At the same time, he definitely has some dental problems, perhaps from an unhealthy diet. 
 Finding discontinuities between pixels is easy, but we need some creativity to extract silhouettes from 
such data.  In MLAA, there are two rules for extracting silhouettes. First, silhouettes start and end 
at edges of pixels where horizontal and vertical separation lines intersect (all such pixels are shown 
with stripped shading). In HPG 2009 paper some fancy way to find actual intercepts was presented, using 
color balance. Half-edge points will work as well, and it will result in a simpler algorithm. Once we 
find end-points for silhouette lines, we have to connect them. We do so by processing each separation 
line independently. If both horizontal and vertical silhouette lines intersect a pixel, we process only 
the longest silhouette line. In this figure, both light-blue and dark-blue end-points can generate silhouette 
lines, but we choose only those bounded by dark-blue points size matters! If the silhouette end-points 
are on opposite sides of the separation line, we will get a socalled Z-shape. Otherwise, a U-shape will 
be created, which consists of 2 linear segments. These shapes are used for actual color blending by 
pretending that colors on opposite sides of separation lines leak into polygons formed by silhouettes 
and separation lines. Essentially, areas of trapezoids have to be computed. It is all consistent with 
the geometrical approximation of the integral over pixel, except in the situations when multiple lines 
cross the same pixel. This case is shown in the middle. Since we re computing each trapezoid area independently, 
we will overblur this pixel. In one important case, though, the situation is recoverable. When both horizontal 
and vertical silhouette segments are defined by the same points, we just have to choose either one to 
avoid overblurring.   It turned out that for deferred shading and fixed hardware MSAA is not that 
convenient. So, at least for now, post-processing filtering has an edge.   Intel s CPU-based implementation 
searches for specific patterns (U-shaped, Z-shaped and L-shaped patterns) that are then decomposed into 
simpler ones, an approach which would be impractical on current-generations GPU architectures. We make 
the key observation that the pattern type, and thus the anti-aliasing to be performed, only depends on 
four values (the four possible crossing edges), which can be obtained for each edgel with only two memory 
accesses. This way, Intel s algorithm is transformed such that it uses texture structures instead of 
lists. Furthermore, our approach allows handling all pattern types in a symmetric way, thus avoiding 
the need to decompose them into simpler ones and the use of a complex, branchy pattern-matching algorithm 
at run-time. Pre-computed textures and extensive use of hardware bilinear interpolation to smartly fetch 
multiple values in a single query are some of the key factors for keeping processing times at a minimum. 
As the anti-aliasing to be performed depends on the edges between pixels, and not on the pixels by themselves, 
if we operate on edges we can share calculations and cut times by half. By masking the operations using 
the stencil buffer, we are able to decrease running times, by executing the shader only where required. 
All these key ideas translate to an execution time of 0.44ms for a 720p framebuffer, which is 1180% faster 
than using MSAA in our test scenario. Our algorithm consists of three passes. Starting from an original 
source image (with aliasing present) (left), we perform edge detection in the first pass. This yields 
a texture containing edgels (center left). In the second pass, we process each edgel in the edges texture, 
obtaining the corresponding blending weights of each pixel adjacent to the edgel being smoothed. To do 
this, we first calculate the distances from each edgel to the end of the lines to which it may belong 
(actually two: horizontal and vertical). Then, the crossing edges are fetched and used, together with 
the distances, to query the precomputed area texture (we can think of that as a look-up table), which 
returns the corresponding blending weights (center right). The third and final pass involves blending 
each pixel with its 4-neighborhood using the blending weights from the previous pass to obtain the final 
anti-aliased image (right). Edge detection is a critical step for the quality of the final image. Each 
undetected edge will remain aliased in the final image, so we need to detect as many edges as possible. 
Robustness in this step is also desirable, given that good edge detection enhances temporal stability. 
However, this is not as easy as it may sound. Optimally, we just want to detect edges visible for the 
human eye (no need to spare time anti-aliasing edges which won t be seen, right?). And not only that, 
but we need clean edges as well, in order to detect their patterns properly. The shader we will show 
implements a color-based edge detection, as it is the most straightforward option. Depth, normals or 
object IDs could also be used, as they are better estimators for geometrical edges; however, they are 
sometimes tricky and need extra information available in form of maps. Working with color additionally 
provides seamless handling of shading aliasing, which may improve quality in some scenarios. Instance 
ids + normals is the best option since ids allow detecting the exact boundaries between objects, and 
normals provide additional info for edge detecting inside each object. Note that, in the color edge detection 
case, R',G',B' are gamma-corrected values; this is crucial to perform accurate edge detection, so pay 
attention to your DXGI_FORMAT_R8G8B8A8_UNORM_SRGB textures and SRGBTexture flags, in DirectX 9 and 10 
respectively. Gather 4 can be used to optimize the depth case, or when calculating lumas in a previous 
pass (tonemapping).  For blending weights calculation, we need to obtain the distances to the ends of 
the line segment that each edgel belongs to, using the edges texture from the previous pass. Once we 
know these distances, we will use them to fetch the crossing edges at both ends of the line. These crossing 
edges indicate the type of pattern we are dealing with. Both the distances to the ends of the line segment 
and the type of pattern are used to access the pre-calculated area texture used for blending in the final 
pass. Since two adjacent pixels share the same boundary, this allows sharing calculations between them, 
performing area calculation on a per-edgel basis. However, even though two adjacent pixels share the 
same calculation, the resulting a value is different for each of them (a and 1-a respectively). As a 
consequence, the output of this pass is a texture that, for each pixel, stores the areas at each side 
of its corresponding edgels (by the areas at each side we mean those of the actual pixel and its opposite). 
This yields two values for north edgels and two values for west edgels in the final blending weights 
texture, perfectly fitting in the allocated RGBA storage. These weights will be used in the third pass 
to perform the final blending. The search for the distances to the ends of the line is done using an 
iterative algorithm. In each iteration, it checks whether the end of the line has been reached. To accelerate 
this search, we leverage the fact that the information stored in the edges texture is binary (as it simply 
encodes whether an edgel exists), and query at positions between pixels using bilinear filtering for 
fetching two pixels at a time, thus advancing two pixels per iteration. The result of the query can be: 
a) 0.0, which means that neither pixel contains an edgel, b) 1.0, which implies an edgel exists in both 
pixels, or c) 0.5, which is returned when just one of the two pixels contains an edgel. We therefore 
stop the search if the returned value is lower than one (in practice we use 0.9 due to bilinear filtering 
precision issues). By using a simple approach like this, we are introducing two sources of inaccuracy: 
first, we do not stop the search when encountering an edgel perpendicular to the line we are following, 
but when the line comes to an end instead; second, when the returned value is 0.5 we cannot distinguish 
which of the two pixels contains an edgel. While these inaccuracies may introduce errors in some cases, 
we found it unnoticeable in practice. Moreover, the speed-up resulting from jumping two pixels per iteration 
is considerable. The figure shows an example where the color of the dot at the center of each pixel represents 
its value in the edges texture. Distance search for the left end of the line is performed for the pixel 
marked with a star. Positions where the edges texture is accessed, fetching pairs of pixels, are marked 
with rhombuses (its color represents the fetched value).  Once we have the distances to the ends of 
the line, we use them to obtain the crossing edges. A nave approach for fetching the crossing edge of 
an end of line would imply querying two edgels. Instead, a more efficient approach is to use bilinear 
filtering for fetching both edgels at a time, similar to the distance search. However, in this case we 
must be able to distinguish the actual value of each edgel, so we query with an offset of 0.25, allowing 
us to distinguish which edgel is equal to 1.0 when only one of the edgels is present. The figure shows 
the crossing edge corresponding to each of the different values returned by the bilinear query. The color 
of the dot at the center of each pixel represents the value of that pixel in the edges texture. The rhombuses 
indicate the sampling position, while their color represents the value returned by the bilinear access. 
 Calculating the area corresponding to the current pixel is an expensive operation, so we pre-compute 
it in a 4D table, stored as a conventional 2D texture (right). This texture is divided into subtextures 
of size 9x9, each of them corresponding to a pattern type coded by the fetched crossing edges e1 and 
e2 at each end of the line. Left shows you the sixteen different patterns we handle (each one with direct 
correspondence to a subtexture); the orange lines indicate the perceptual re-vectorization to be performed 
in each case. Inside each of these subtextures, (u, v) coordinates correspond to distances to the ends 
of the line, eight being the maximum for this example. This not only allows to avoid the expensive area 
calculaton, but also to avoid dynamic branching depending on the pattern type.  Ok, so we already have 
the edges where anti-aliasing needs to be performed, plus the blending weights. In this last pass, we 
will obtain the final color of each pixel by blending the current color with its four neighbors according 
to the area values stored in the weights texture. To do this we have to access three positions in the 
blending weights texture: a) the current pixel, which gives us the north and west blending weights; b) 
the pixel at the south; and c) the pixel at the east. This yields the blending weights with the complete 
4-neighborhood. It happens that the blending equation we want to use is the same as the one used for 
bilinear filtering. So, once more, we exploit hardware capabilities, and use four bilinear filtered accesses 
to blend the current pixel with each of its four neighbors, saving four additional accesses. If you want 
all this blending calculated properly, you have to ensure you are working in linear space. Using bilinear 
filtering and DXGI_FORMAT_R8G8B8A8_UNORM_SRGB textures for calculating this step in DirectX 10 enforces 
linear blending. In DirectX 10 hardware running DirectX 9 code, this bilinear filtering blending will 
be performed, again, in linear space. However, DirectX 9 hardware running in DirectX 9 will perform the 
blending in gamma space. Thus, in this case, manual blending using lerps is advised for accurate results. 
Finally, given that one pixel can belong to four different lines, we perform a weighted average between 
the contributing lines. The cubed blending weight (a cubed) of each possible line is used as the weight 
of this average, which favors blending and works well in practice.    As our algorithm works as a 
post-process, we have run it on a batch of screenshots of several commercial games, in order to gain 
insight about its performance in different scenarios. Given the dependency of the edge detection on image 
content, processing times are variable. We have noticed that each game has a more or less unique look-andfeel, 
so we have taken a representative sample of five screenshots per game. Screenshots were taken at 1280x720, 
which we take as the typical case in the current generation of games. We used the slightly more expensive 
luminance-based edge detection, since we did not have access to depth information. Table 1.1 shows the 
average time and standard deviation of our algorithm on different games and platforms (Xbox 360/DirectX 
9 and PC/DirectX 10), as well as the speed-up factor with respect to MSAA. On average, our method implies 
a speed-up factor of 11.80x with respect to 8x MSAA. Values marked with a star indicate 4x MSAA, since 
8x was not available, and the grand average of these includes only values for 8x MSAA. So, what are 
the benefits of this MLAA implementation for your graphics engine? Well, you can run this technique as 
a regular pixel shader in DirectX 9 and above (including the Xbox 360). When edge detection fails, our 
technique can be as bad as 1x (in fact, in these cases, it is 1x). In presence of sub-pixel features, 
MSAA can be superior (although proper care in the art direction can solve some cases). However, our implementation 
is comparable, in general, to 16x MSAA, while only requiring a memory consumption of 2.0x the size of 
the backbuffer. Typical execution times are 0.44 ms on a nVIDIA GeForce 9800 GTX+, for a resolution of 
720p (tested in DirectX 10). According to our measurements, 8x MSAA takes an average of 5 ms per image 
on the same GPU at the same resolution, that is, our algorithm is 11.80x faster. The method presented 
has a minimal impact on existing rendering pipelines and is entirely implemented as an image post-process. 
Resulting images can be on par with 16x MSAA in terms of quality, while requiring a fraction of their 
time and memory consumption. Furthermore, it can antialias transparent textures such as the ones used 
in alpha testing for rendering vegetation, whereas MSAA can only smooth vegetation when using alpha to 
coverage. We believe that the quality of the images produced by our algorithm, its speed, efficiency 
and pluggability, make it an attractive choice for rendering high quality images in today s game architectures, 
including platforms where benefiting from anti-aliasing together with outstanding techniques like deferred 
shading was difficult to achieve. Do not forget to visit us :-)  Prior presenters have well explained 
the MLAA algorithm and some implementation approaches, as well as some of the motivations for its use 
(alternative to MSAA, lower memory, application to deferred shading) and some of the drawbacks (fonts, 
unnecessary blurring, cost etc). In this talk we ll describe an implementation that has been used in 
several titles for the Xbox 360, one that has very low (and fixed) GPU costs and makes MLAA an option 
on the platform. We ll also touch on some of the techniques developed to adapt the edge filtering to 
each title s visual style, and general implementation and deployment notes relevant for any real-time 
application. Briefly, we were forced to abandon Costume Quest s original antialiasing approach of supersampling 
(at 1520x848), which was originally chosen in the interest of implementation speed. As the game sped 
towards release, the heavy performance toll placed on the GPU by SSAA, compounded by the increasing quantity 
of art, heavy use of lights and transparency made the approach no longer tenable (not just the 50% increase 
in pixels, but additional passes required due to the lack of tiling support). It also didn t improve 
quality as much as desired (especially compared to the cost), particularly on the PS3. But there was 
a very strong desire to ship with some sort of antialiasing, and not enough time to get tiling functioning 
on the XBOX. A month or two before, I had read about MLAA on the realtimerendering.com blog, and thought 
about a few ideas that might make the technique feasible for realtime use. Quickly those loose ideas 
became a risky, last ditch plan to improve rendering performance and hopefully retain good image quality. 
We hoped to get a pre-release version of Sony s SPU MLAA, to make sure both SKUs had high quality visuals. 
Many of the details of the implementation bear the mark of this time pressure efficient time use was 
of the essence, since by this time the studio s total graphics programmer bandwidth was 50% of one person, 
responsible for 4 games in production! The original implementation using the reference code took a very, 
very long time (though on my U7300 1.3ghz laptop, it only took ~40-50ms). A first-pass at converting 
the code to a more PPC/Console friendly form was done, and though it was still quite slow, it showed 
some promise. At this point the idea to split the implementation and do the edge and blending calculations 
on the GPU was first seriously considered, while the remainder would run on the CPU. I freely admit 
the idea may have been swayed by the often enjoyable challenges encountered when making the CPU and GPU 
communicate in such a way!  Some key points in the prototype: a first pass at making the code run more 
than 1fps (not counting tiling), and after the edge detection had moved to the GPU and blend weight computation 
had improved a great deal. After this the implementation moved in-engine and further optimizations and 
threading was done. Our implementation consists of three major steps. Edge detection, using a variety 
of depth, normal, and id buffers in conjunction with raw color data is performed on the GPU. When complete, 
the GPU notifies a CPU via an interrupt that the data is ready. This edge data is then read on the CPU 
to form two blend masks (horizontal and vertical). The major issue in doing this quickly is reducing 
the amount of bandwidth used by the algorithm, in part by avoiding a vertical edge scan by performing 
a fast transpose of the input edge data. Once complete, the CPUs will notify the GPU that the blend data 
is available using an asynchronous resource lock (or async command buffer). These final buffers (blend 
masks) containing blending weights are then used by the GPU to filter the near-final image. This example 
timing capture from Stacking (on a reasonably light frame, to help make this legible) illustrates the 
various stages of the algorithm and the relative time used. Each of the horizontal bars below represent 
the various hardware threads on the Xbox 360, and the two blue boxes in the center represent MLAA processing. 
The dotted blue arrows show the flow of data. Along the top, surrounded in green and red boxes, is the 
GPU workload. The timing starts and stops at vertical blanks, as this image shows a single frame. Note 
there is some amount of time left before the GPU would stall on CPU results, even though this frame is 
already somewhat heavy on blend mask time, making it very unlikely the GPU would stall on edge data. 
 The two images which hold horizontal and vertical blending weights for neighbor pixels we will call 
the blend masks. These are computed from edge and luminance data provided by the GPU. From the original 
test implementation, we knew that bandwidth would be as large an issue, if not more, than the amount 
of calculations going on, so reducing it was a primary goal.  The first step in doing this was passing 
less data from the GPU to the CPU. We use 8bpp, which is split into a 2 bits of horizontal and edge flags, 
and 6 bits of luminance data. In our actual implementation, we actually use a 16bpp target which contains 
the edge data along with other unrelated data into the second channel. This was mainly to save a little 
GPU time. It should be noted that this data is passed from the GPU in a tiled format, making it unsuitable 
for use on the CPU. After receiving the data, a (very slow) function from the XDK was originally used 
to untile the image, which it was assumed could be made faster later. This greatly reduces the cost 
for horizontal blend mask generation (and enables some additional optimizations), the vertical blend 
mask generation is still horribly slow due to the cache-unfriendly access pattern. An obvious solution 
would be to transpose the image, but exactly how to accomplish that in an efficient manner was slightly 
less obvious. What was originally just an annoyance ended up providing an excellent solution to the 
problem. We could utilize the fact that a tiled image has blocks of the image arranged linearly in memory, 
so we could read in small square regions, transpose them, and write out the blocks in a transposed pattern. 
Doing this over the whole image allows us to transpose it in a very cache and CPU efficient manner. 
Instead of going into the intricacies of Xbox texture tiling, we ll use the familiar example of DXT blocks. 
The key insight is that encoding patterns like this allow a single aligned, linear, easily prefetchable 
read to load a rectangular block of pixels. One can then take care to load a square block using VMX128/Altivec/ 
your vector instruction set of choice, transpose that, and write out both an untiled and untiled+transposed 
image simultaneously. Prefetching carefully and trying to make the best tradeoff of block size, instructions 
count to transpose each block, read bandwidth per instruction, etc is simply labor from that point on. 
The untile in our implementation also discards 8 bits of the 16bpp texture produced by the GPU, and operates 
on 32x32 blocks which are processed as 4 8x8 subblocks, two of which fit into a set of vector registers 
at once. The new FastUntile sample application in the XDK is an excellent starting point for performing 
the untile quickly. In summary, we take something which looks nothing like an image and convert it into 
two 8bpp images containing horizontal and vertical edges. We then run the MLAA algorithm on these two 
images. Our implementation uses an optimized version of the reference code not many functional modifications 
were originally made besides a few changes to blend weight calculations. The fact that we only perform 
horizontal blend mask generation makes additional optimizations possible, and makes the code much easier 
to maintain. Threading was the one of the last steps, which does further complicate the careful balancing 
of cache prefetching and bandwidth usage. The speedup was certainly not 1:1 with each thread added, and 
titles so far end up using two threads total. The threading pattern arrived at is similar to the reference 
code s, interleaved horizontal blocks of the image are processed in a way that two adjacent blocks are 
not used simultaneously. A single thread is responsible for locking the GPU resources, untiling the image 
and submitting work, and will then processes blocks once untiling is complete. It should be noted that 
the core MLAA code has many remaining opportunities for optimizations, both with respect to the speed 
of the code and additional preprocessing that could be cheaply done on the edge data to allow more efficient 
skipping of empty space, line length searches, etc. Many of the ideas presented in the GPU implementations 
could be adapted to optimize the CPU code (particularly those that limit the total size of processed 
edges). Blending is straightforward. In one of our final post-processing shaders (that is ALU heavy), 
we read the blend masks and blend between adjacent pixels in the image to produce the final, antialiased 
image. We currently blend all four neighbors, but based on preliminary results from Jimenez s MLAA, using 
bilinear filtering to blend only the dominant horizontal and vertical neighbor may produce similar results 
while reducing total blending time. As our edge detection is processed on the GPU, a whole wealth of 
new options is available to us. Most notably, it becomes cheap to use additional buffers and linearintensity 
values when computing edge data. Adapting the edge detection to the individual style of each game was 
essential to get stable filtering and avoid too much overblurring. All that said, overblurring can still 
be an issue, particularly on fonts and Costume Quest s toon-like visual style. Fine texture detail could 
become much less legible due to MLAA, along with the cell-shaded outlines around the characters. All 
reasons why Costume Quest used stencil (material) edges in addition to color edges. We found that pure 
color edges often did not provide very stable or desirable results (both missing edges and overblurring 
areas of the image). We had some experience with edge detection from depth and materials from the antialiasing 
approach used in Brutal Legend, and this provided a starting point to improve the quality of edge detection. 
 Using a relative difference from the center pixel provides a way to avoid two issues with depth-based 
edges -Avoiding flat sloping surfaces from being flagged as edges -Edge tolerance falling off with distance 
 Note that these assume your near values are at 0, far values 1.  Flat areas can Using material-based 
edges will detect edges that depth edges would normally miss, and is very stable. However, it may add 
more edges than normally desired and cause overblurring. This said, for games without large view distances 
and no need to reduce edge detection tolerance in the distance, material edges may be preferable due 
to their stability and simplicity. But neither handles everything. (Though adding normal-based edges 
would find the horizontal edge this particular case) Costume Quest ended up using a scheme where stencil/material 
edges were used to increase the sensitivity of color edges. This lead to stable edge detection for almost 
all situations, but without adding edges that wouldn t visibly reduce aliasing. Stacking went even simpler 
and used only the color buffer, but added a few tweaks to improve quality, help performance and avoid 
some degenerate scenarios. Stacking also used a linear lighting pipeline, so linear color values were 
needed but we only used an approximation to save texture fetching cost, the difference is very minor 
(in some cases it actually helps).  One of the tricks used in Stacking was a local adjustment of the 
edge threshold based on the luminance of the pixel and its neighbors, reducing edges to just the most 
contrast-y areas. This is particularly helpful reducing the amount materials with high frequency details 
are blurred. Stacking also omitted edge detection at depths that would be fully out of focus, as the 
depth of field adequately hides aliasing. Note that filtering the edges in the depth range where objects 
are transitioning from in focus to out of focus does provide noticeable quality improvements. It may 
however make the latency of the blend weight calculations more difficult to hide, any DOF blurring passes 
would need to wait for MLAA results.  In some titles, there were certain materials and areas where the 
player could get a screen-full of edges, causing a noticeable decrease in performance due to the heavy 
CPU load. In some situations, a simple PID controller was used to adjust the edge tolerance dynamically, 
reducing it whenever CPU time was growing, and increasing it to normal whenever load was below average. 
To handle the worst situations, a hard wall-clock budget of CPU time was enforced, simply skipping any 
remaining blend mask generation work and allowing the GPU to complete. This could cause some small amounts 
of flicker, but these were rarely very noticeable as the skipped work was generally contained within 
the last several vertical tiles. Memory requirements are fairly low, and could be reduced further by 
untiling the horizontal image into a smaller set of temporary buffers, or performing the horizontal untile 
in-place, reusing the input edge buffer. Latency is hidden by performing other work (bloom, dof precalculations, 
and parts of the next frame) while the blend masks are being computed. We perform MLAA before any UI 
or postprocessing (though taking into account any postprocessing color changes, etc may have in edge 
detection may be worthwhile). Final GPU cost varied between each title, and was mainly dependent on the 
type of edge detection used and how much MLAA work could be combined into existing shaders (for example, 
putting texture-heavy MLAA work in ALU-heavy shaders, or shaders that already fetched most of the data 
needed by MLAA, using the MLAA blend as cheap depth of field blur, etc). The lowest integration cost 
was approximately .34ms, while on average it costs .5-.6ms. In our integration, this cost is supplemented 
with the cost of a z-buffer reload. It should be clear that many additional optimizations are still 
available, and many of the approaches suggested in this course provide additional avenues for optimizations 
and quality improvements.  Example image with many edges being smoothed. Detail of smoothing Note that 
the cell shading on the characters is difficult to antialias well.          Powerpoint animation: 
UI should be using Edge frame 0 , but uses Edge frame 1 as it greatly simplifies the pipeline    
      This talk is split into two parts. First we ll have a look at the development of the system 
and how we fit the algorithm onto the platform. In the second part, I ll discuss our techniques for improving 
the image quality.  Killzone 2 shaded all MSAA samples, which is wasteful inside triangles. So the idea 
was to classify the screen into areas that are either pixel rate or sample rate, and then shade these 
separately. At the same time, MLAA was presented and seemed like a much more elegant way of adding AA 
to a deferred renderer, so it was investigated instead. It provides some important benefits, but its 
fitness for a technology showcase title like Killzone was uncertain. The ATG often provides its run-time 
technologies to internal and external teams as part of the PlayStationEdge package, if we think it makes 
sense. If we managed to get MLAA to work, it would be very useful for other developers, so we planned 
doing this from the start. (Current record for integrating it into a game for an external team is about 
2 hours.) The first question we needed to answer was if the algorithm could be ported over to the SPUs 
while achieving the required speed. As Edge Post, a framework for tile-based posteffects on the SPUs, 
was already available, Matteo used it as the framework to do the first experimental implementation. A 
tile based implementation has several nice properties because all processing is done on complete tiles. 
This means that the size of the actual image is not really a concern and that the tile needs to only 
be fetched onto the SPU once, where it can be fully and independently processed. On the other hand, an 
edge may span multiple lines, and we do not have the ability to properly process them. We can mitigate 
this somewhat by adding a border of pixels that are not actually part of the tile, so we can look at 
the neighborhood. Of course, these result in extra work, reduce the maximum tile size, etc. While overlapping 
is a valid approach to stitch together areas processed by different SPUs, you do not want to throw away 
a significant percentage of the pixels you process. Thus you ideally only want one tile per SPU with 
as thin a border as you can manage. The oldest record I have gives the speed at about 75ms* for a relatively 
simple 720p image, which we later optimized down to about 20ms in this implementation. The same image 
now takes less than 10ms with the current code. (* Whenever I give a performance number, it is the total 
time spent on all SPUs. The time it takes to finish the task after it was kicked off I will refer to 
as latency .) The first team to ship with our MLAA code (and probably the first to ship with any MLAA 
implementation) was SCE WWS|A Santa Monica Studio. They were interested in MLAA and heard that we were 
working on it, so we gave them a preview version of the tiled code. Some analysis on their part quickly 
revealed that the quality provided by our implementation wasn t what they needed. The tiling was causing 
all kinds of artifacts. The God of War team went for having the best graphics on the platform, so while 
MLAA would free up GPU time for other tasks, it was also crucial that it would not have a negative impact 
on the overall visual quality. We still hadn t really taken a look at image quality at that time and 
were just reaching the point where performance started to look promising. As this was about 8 weeks before 
codefreeze for God of War III, we needed to shift our focus to developing a production-ready version 
as quickly as possible. Santa Monica also provided us with images that would take up to 50% longer to 
process than anything else we d seen to date, due to the visual complexity (i.e. number and length of 
edges). Their performance goal was an average of 15ms and peaks not above 40ms, which was not too far 
off from where we were. We would end up delivering significantly better performance that that. To test 
MLAA, Christer Ericson of Santa Monica Studio made this image... ... and ran it through Alex s reference 
code. Note that the edges are nicely smoothed. Not a whole lot to complain about. And this is what our 
code produced, with Christer s actual analysis shown for the reader s amusement. Marked are example 
area where the tile-based approach conflicts with the reference implementation. This is really the worst 
case for the tiling artifacts, which are a lot less noticeable in complex images. However, this image 
was the end for the tile based approach. Much cringing was done. (Image and analysis by Christer Ericson) 
 As we now had a client that wanted to ship quite soon, we needed to get serious. There was very little 
space for experimentsleft. -Scanlines -We needed to abandon tiling, to be able to process lines that 
extend over the entire screen. Alex s code had demonstrated thata scanline approach works, but we were 
unsure if it would fit onto the SPUs. Several lines are needed in flight to pipelineprocessing and hide 
DMAs and because some parts of the algorithm can be written much more efficiently, if up to 4 lines areprocessed 
at the same time. So it was going to be tight.This also meant that the size of images we can process 
is now limited (currently to 1280x1280), but the limit is high enough forthe vast majority of games. 
-Horizontal vs. Vertical Originally we had separate implementations for horizontal and vertical MLAA 
passes. This worked reasonably well, as the entiretile was in local store memory, which is extremely 
high-bandwidth and low-latency. We could not keep the entire image in LS,so horizontal and vertical lines 
would now need to be fetched as needed. This made writing a vertical implementation reallycomplex, due 
to the way memory fetches work. Memory reads from LS by the SPU are always 16 bytes. So if we were to 
work vertically, we d still always fetch 4 horizontalpixels and process those 4 simultaneously, making 
a single scanline 4*height pixels. For our n-buffering, this would mean(roughly) 4 times the memory consumption, 
which would not fit into our LS budget. Furthermore, DMA fetches from mainmemory to LS only reach peak 
performance when about 512B of continuous memory are fetched, so we would actually need tofetch 128*height 
stripes. There are ways to get around this, but they all seemed rather unpleasant. Instead we decided 
to transpose the image betweenpasses and only implement a horizontal version of the algorithm. We are 
effectively trading latency for ease of implementation,but initially it was a performance decision. We 
estimated that focusing optimizations on one pass would get us overall betterperformance and we still 
think it was the right call. The initial implementation took 180ms to process the most complex God of 
War III image, but that was in C. A day later we hadthe first assembly version, which did the same in 
55ms. Sadly, we never again achieved such a speedup in such little time. We actually abandoned the C 
reference implementation quickly after we had the basic algorithm worked out. It was a lot ofwork to 
maintain interface compatibility between the C and assembly versions, partially because a lot of the 
more interestingbit-manipulations are not easily expressed in C. So we ended up with an assembly version 
and a C version consisting mostly ofinline assembly, which isn t much help. We usually process using 
5 tasks during the actual MLAA and 2 tasks for transpose, as it is memory bandwidthbound. Each task can 
run on its own SPU, but doesn t have to. Some of our customers change the number of tasksbetween frames 
and even between the vertical and horizontal MLAA pass, as well as the task priorities, toimprove their 
task scheduling. The only point at which we support co-operative yielding of the SPUs is between the 
MLAA and transpose phases, so scheduling can be non-trivial. When splitting the image for the different 
tasks, there is the possibility that an edge is detected that belongs totwo tasks. To handle this, each 
task will fetch some lines adjacent to its part of the image and smooth thetransitions. However, a major 
design requirement for us was in-place processing, because everyone s always short on memory. Combined 
with the overlap this means that we have to be careful, as the order in which the SPU tasks areprocessed 
is not actually guaranteed. To prevent flickering, we ensure that edge-flags are invariant to ordering 
byassuming known good values in the overlap regions. The results are not invariant to changing the number 
oftasks, so some care must be taken. The transpose itself is quite interesting, and explained on a bonus 
slide for everyone interested in in-placetransposing non-square images. Algorithmically, we turn everything 
into a sequence of edges (i.e. continuous edge pixels) very early on and thenstart working on those. 
Each loop will then perform some work on these edges and store the result into one ormore target buffers. 
So this is very much a process-filter-split approach, which maps nicely to the strengths of the SPUs. 
We fill up local store almost completely both for the actual MLAA and the transpose phases. Data that 
persistsbetween modes is stored on the stack, which is also the only part of LS we store when context 
switching. So weonly save 2k of stack per task. During MLAA itself, there are up to 8 output and 10 input 
lines in memory at a time, plus auxiliary data, which isabout the minimum of data we need to hide DMAs. 
 -What worked- One example of our general approach: We looked at what kinds of intermediate data we 
were generating. The MLAA paper already differentiates between single-pixel and multi-pixel features, 
but multi-pixel features are quite expensive for us. We need to walk over the entire edge looking for 
patterns and computing the weights, as early-outs do not play nice with software pipelining. What we 
realized is that for most multi-pixel features, we really only need to look at the first and last pixels, 
because they are already valid start and end points. We call these simple features , as opposed to complex 
features which need the full process. This allows us to process the features of a line as a three step 
process. First we split the single-pixels and multi-pixel features, then the multi-pixel features into 
simple and complex. Along the line, we process the features as we find them. This can be seen as nested 
if-then-else statements, with the then-parts being executed and the else-parts being collected for the 
next loop. -What didn t work- We also tried an approximation in classifying features as simple if only 
the pattern matched, by defaulting to a 0.5 height. This reduced the number of complex features significantly, 
but introduced flickering artifacts as heights could jump from close to 1.0 or 0.0 to 0.5. We will 
now look at the other side of the algorithm, the image quality. Obviously, any AA method is supposed 
to improve the image quality and make the playing experiencemore pleasant. Once we hit the performance 
targets, the visual quality became the primary focus ofattention. We tried to not have the quality improvements 
negatively impact performance, soexpensive new features would need to be balanced by performance optimizations. 
Apart from that,there was a lot of sporadic peephole optimizations. -Aliasing Perception One slightly 
unscientific observation we made is that having very few aliasing artifacts can make theremaining artifacts 
appear much worse than they are. At times I would have to switch back to no-AA or MSAA, to recalibrate 
my AA-perception. It would be interesting to do a larger test and see howjarring different levels of 
aliasing are to people. -Placing MLAA in the Frame- Compared to MSAA, MLAA performance is a lot less 
sensitive to where in the frame the resolve happens. This allows us to process much of the frame before 
MLAA is applied. There is a tradeoff here,however, as more effects increase the chance of something causing 
temporal artifacts. For example,god-rays in SOCOM 4 were the source of some headaches, as they cause 
slow moving slight variationsin luminance. We use highly quantized luminance in some parts of the algorithm, 
so this can be quitebad, even if we ignore the effect it has on the edge detection itself. Alex s original 
MLAA comes in two flavors, black&#38;white and color. The latter is significantly morecomplex to compute 
(25%+ of our computation time is used for this), but produces noticeably betterresults. We sporadically 
tried going to the fixed 0.5 height of the black&#38;white version, but it wasalways too big a reduction 
in quality. On the other hand, it can reduce temporal artifacts, so there isanother tradeoff here. The 
rest of the slides are mostly aimed at people who want to work on their own (ML)AA techniques. If I 
can get across only one point, it should be that edge detection is the most crucial component for a productized 
system. It s less glorious and less academically interesting than a lot of the other stuff, but if the 
edge detection isn t rock-solid, the best algorithm in the world is still going to produce unacceptable 
results. This is what makes edge-detection free algorithms so attractive. Minimizing the noise introduced 
by the algorithm is an important thing to optimize for. The goal of anti-aliasing is ultimately to reduce 
visual noise. Basically, if your algorithm causes something previously smooth to look jaggy, you need 
to fix that. And it s really easy to have these issues make it past your tests. It happened to me several 
times. Our first significant improvement was going for a relative thresholding approach. This is very 
much what one would expect from the name, with the value of the threshold being a function of the color 
value. We compute the threshold of a pixel with a simple scale of its numerically highest channel and 
use a lower limit to prevent the low resolution near black resulting in superfluous edges. Given these 
threshold values, we compute the absolute differences of each color channel for two pixels and compare 
them against the minimum of the threshold values. As the computation of the threshold values is an independent 
per-pixel operation, we can do this during the transpose pass, which is otherwise not using a lot of 
ALU power. This is our standard test image. It is the most costly image we have in our tests and it 
has some very challenging areas. (Image courtesy of SCE Santa Monica Studio) For today, we will concentrate 
on this area of the image. It has some good edges and a very over-bright area with fine detail, where 
the flame effect is added to the soldier s armor. First, we have a look at the absolute thresholding 
that we started with. On the right side is the edge buffer, with green representing horizontal edges, 
red vertical edges and yellow pixels that have both a horizontal and a vertical edge. After applying 
MLAA, the aliasing artifacts are gone, but so is a lot of the texture detail, which is very noticeable 
on the character behind the flame effect and the red lines on the weapon. This is caused by the high 
luminosity which easily gives high numerical differences. For example, the head of the character seems 
to be all-edges. Another unfortunate effect is that the red jewel on the weapon has changed shape quite 
a bit. With relative thresholding, the texture detail on the background character comes back, but the 
weapon and the gem are still not really ideal. This is how the code worked at the time we shipped God 
of War III. Also note that edges in the background are now being picked up. The bright area of the flame, 
however, is almost free of edges. This is the exact opposite of what we saw in the previous slide, and 
much closer to what we want as a result. One area that is less well-filtered now is left side of the 
edge blade, which is caused by the double edge in this area. We can tune the algorithm to not do that, 
but it requires careful tuning for each image, which is a bit unrealistic in an actual game. A better 
edge detection would have to take a larger neighborhood into account to be able to detect the difference 
between an edge and a smooth surface that just has a very steep color gradient, or maybe some texture 
variation. This is of course very hard to do reliably and temporally coherent. Consider the case of a 
plane with a smooth gradient that is rotated around an axis parallel to the screen. As the gradient shortens 
due to projection , edges will appear that should be discarded. (In this particular case, a rendering 
artifact is causing a slight increase in brightness for the outmost pixels of the blade.) Source image 
again, so I can go back and forth to show the impact. The next improvement came out of a bored weekend 
which saw the creation of a new reference C-implementation, this time designed to ease the prototyping 
of image-quality improvements. As such, the code has very little in common with the assembly version 
or the original C reference code. As one would expect, having an easy to change laboratory-code to go 
with the complex high-performance code really is invaluable. The underlying observation is that two adjacent 
edges will be treated as one edge, which results in artifacts in many cases. One could simply split where 
an orthogonal edge is found, but this will not deal very well with color gradients and it feeds the noise 
of the edge detection into the system a second time. Nevertheless, it s an option. We got the best result 
with splitting when the gradient of the sum of color channels changed. That sounds pretty noisy, but 
actually works really well. If there is a Gordian Knot of edges, this gracefully degenerates to making 
everything single-pixel features, turning the operation into a slight 4-tap blur. We re just looking 
at the horizontal edges now, vertical edges are not processed in this image. The middle cut-out shows 
that when treated as a single edge, this pattern will results in no actual AA at all, just a color gradient 
checker-board. The lowest image shows what we get when we split, which is the desired behaviour.  This 
is the same cut-out we used before. ...and this is relative thresholding again. Things to notice: -Kratos 
s eyes, which lose the eyeliner. -The gems on the blades, which change shape -The red details on the 
weapons, which gain additional noise. -Single pixel artifacts, e.g. on the golden helmet, which are not 
removed. -The blade edge near Kratos head, which gets very uneven.  This is still not perfect, but a 
lot of the issues we had before are significantly improved. Notice that we re still losing texture detail 
e.g. on the chains wrapped around his forearms. We will get back to that issue later, although not with 
Kratos, in the next section. One might argue that the improvement is not as substantial as what we achieved 
when going from absolute to relative thresholding. However, this is not really the worst case we re looking 
at.  Again, repeat the source image to be able to go back and forth. This cut-out shows a seriously 
undersampled metal grill. The middle image is using standard relative thresholding while the right image 
splits features if the sign of the gradient changes. While the image in the middle produces mostly noise, 
the right image produces a very good filtering of the source image. Note the two specular aliasing artifacts 
that did not get smoothed out. While the algorithm does reduce specular aliasing, it is not its primary 
function and it is not tuned to do so reliably. (Image courtesy of Guerrilla Games) Killzone 3 had used 
our MLAA code for quite a while, but the addition of the feature splitting caused a last minute flurry 
of activity. They had experimented with using extra data before, but with the large changes caused by 
the feature splitting, we revisited the topic together. The initial Edge MLAA code already supported 
controlling the edge detection by reading the scale-parameter of the relative thresholding from the otherwise 
unused alpha channel of the source image. It was obvious that we could store other data in there and 
do something more sophisticated, but it was not at all clear how two sources of edges could be combined 
in a way that would always be an improvement.  Killzone 3 environments often have high-frequency high-contrast 
textures, which result in many false edges. (Image courtesy of Guerrilla Games) If we just use the system 
utilized before, KZ3 s art and our algorithm do not work together very well.  However, Guerrilla Games 
had another source of edges available, shown here. It doesn t have all the edges we need, or even all 
the important ones, but it has a lot of the important ones.  This is the source of these edges. It s 
part of the lighting computation. (Image courtesy of Guerrilla Games. It was normalized to make it easier 
to view on a projector, which increased the quantization along gradients.)  We ll have a look at the 
highlighted cutout. (Image courtesy of Guerrilla Games. It was normalized to make it easier to view on 
a projector, which increased the quantization along gradients.)  The underlying problem is shown here. 
If an edge is detected in the non-color data that does not have a color analogue and intersects with 
an edge in the color-data, the algorithm will try to make a smooth transition between the two edges, 
resulting in fractured lines and seemingly random noise and artifacts. As stated earlier, introducing 
noise is about the worst thing an AA technique can do. This is a general issue with getting edges from 
non-color data. At the end of the day, the decision whether something really needs to be blended should 
always be based on the color information, or you get artifacts as soon as a virtual edge cuts a real 
edge. (The target quality image is actually produced by our production system.) The solution we came 
up with was to use the edges in the additional data as a hint for the edge detection to look extra closely 
for edges in the color data. Algorithmically, we run relative thresholding twice, once for the alpha 
channel and once for the color channels. On the second pass, we modify the threshold values after we 
have computed them and before we compare them to the absolute difference of the color channel, by shifting 
them a few bits to the right. The effect can be controlled by choosing how many bits we shift, which 
might seem a bit coarse, but has provided more than enough fine control, somewhat surprisingly. This 
now opens the door for many interesting predicates. The most simple one is to write object or material 
IDs into the color channel and simply look for differences. But we can also use less specialized data 
and rely on the relative thresholding to find edges, or even use a non-binary edge detection filter as 
the source of the predicates. For games with high-contrast textures, it is now possible to prevent the 
algorithm from picking up a user-controlled amount of these texture edges, making it possible to prevent 
a washing out of the textures. In this example, the contrast of the color image has been significantly 
enhanced, as it s a bit hard to see the edges otherwise (which is sort of the point). Note that there 
are still holes in the edges on the right, as for some pixels the color difference is just that small. 
 It would be interesting to try to use edges in the alpha data to close gaps in otherwise visible edges, 
i.e. make edges continuous. Of course, this brings back the spectre of false edges. (Image courtesy 
of Zipper Interactive)  This is the result of the filtering without predication. Note that there are 
still many hard edges. With predication, practically all edges are picked up, without overly blurring 
the image. This is the original edge buffer again. Here we use predication and reduce the base sensitivity 
of the edge detection. Killzone 3 is still a challenging game to filter. Please note that this is not 
an edge buffer from the actual game, so the edges seen here are not representative of what Killzone 3 
shipped with. To close this presentation, we would like to share a few things we ve learned during our 
time with MLAA and give some performance numbers. Right now, our algorithm usually takes about 10ms of 
SPU time for a modern title. Some have less visual complexity, some have more, but we usually hover somewhere 
between 7 and 14ms. The performance difference between running on 1 and 6 SPUs is pretty slim and mostly 
comes from the fact that two SPUs are not fully busy transposing the image. Placing MLAA is very tricky, 
especially with when refraction or depth-of-field come into play. At the end of the day, it needs to 
be tested what gives the lowest amount of artifacts, but luckily moving MLAA around for a quick test 
is very simple. As we were new to developing anti-aliasing techniques, we were in for quite a few surprises. 
The most interesting one is just how subtle bugs can be, especially if you do not have a ground-truth 
to compare against. Often we would have MLAA artifacts that turned out to be either solvable issues, 
or outright bugs. As we shipped several titles during an ongoing R&#38;D project, we needed to split 
these two aspects of the project at some point. Right now, our R&#38;D platform is some 2000 lines of 
PCbased reference code, which is used for all sorts of experiments. On the other hand we have the SPU 
implementation, which gets features added either because we developed an improvement, or because of the 
needs of production teams. The PC code is also used to verify that the SPU code functions correctly. 
It s not very helpful if all you know is that somewhere in 6000 lines of assembly code something goes 
wrong, so we made sure we could verify individual steps in the algorithm.      Adding MSAA normals 
is more expensive for forward rendering but only slightly more expensive for deferred shading / light 
prepass Adding MSAA normals is more expensive for forward rendering but only slightly more expensive 
for deferred shading / light prepass        Adding MSAA normals is more expensive for forward 
rendering but only slightly more expensive for deferred shading / light prepass Adding MSAA normals 
is more expensive for forward rendering but only slightly more expensive for deferred shading / light 
prepass Adding MSAA normals is more expensive for forward rendering but only slightly more expensive 
for deferred shading / light prepass    Wanted to cover all the algorithms, but not enough time!! 
 MEMORY PROBLEM At huge resolutions with deferred rendering, memory used for render targets and back 
buffers can be very large without MSAA. Even 4xMSAA might not be practical (for example with larger FP16 
precision G-buffers). Software post-process filtering AA relieves this memory pressure. TEXTURE PERFORMANCE 
PROBLEM PC GPU texture performance/pixel for huge resolutions can be under what is common on the 5-year-old 
consoles. Need something faster than what is required performance-wise for consoles. Good post-process 
AA budget is under 1.5 ms per frame. RESOLUTION SET TO RISE MORE Could see another huge bump in resolution 
when iPhone4 pixels/inch levels reach desktop displays. Estimating unlike mobile phones, desktop has 
not seen the end of the resolution race.  FXAA CONSOLE Updated version of the first FXAA II Console 
algorithm. Runs in a single full-screen-triangle fragment shader pass. (use an oversized triangle to 
cover the screen instead of a quad to avoid over-shading along diagonal) Designed for 32-bit per pixel 
input. Input is one RGBA texture. Output is one RGBA render target.  FXAA attempts to filter out not 
just aliasing on edges but also filter out sub-pixel aliasing. IF HARDWARE SUPPORTS GATHER4 OF THE ALPHA 
CHANNEL Store filtered luma in alpha channel to enable later discard optimization. Otherwise early-exit 
will have to write out a swizzled color (GBA to RGB), and therefore cannot use discard. In OpenGL swizzle 
(to get gather4 access to alpha channel) and gather4 are supported via, http://www.opengl.org/registry/specs/ARB/texture_swizzle.txt 
http://www.opengl.org/registry/specs/ARB/texture_gather.txt http://www.opengl.org/registry/specs/ARB/gpu_shader5.txt 
 DX9 has no HLSL support for gather4. DX10.1 adds support for gather4 for the R channel via Gather(). 
DX11 SM5 supports gather4 from any channel, and specifically alpha via GatherAlpha(). LUMA CONVERSION 
Using the following for color to luma conversion: luma = dot(color.rgb, float3(0.299, 0.587, 0.114)); 
 SAVINGS On hardware with gather4 support, this optimization will save 2 texture fetches and luma conversion 
math. Should effectively remove 25% of the possible overhead of this algorithm. However this optimization 
is not applicable in all cases. LUMA CAN BE COMPUTED FROM A FRAME WHICH IS DIFFERENT THAN OUTPUT COLOR 
For example, say the prior pass incorporates motion blur, depth of field, bloom, and applies tone-mapping 
and color grading. In this case computing the half-pixel offset luma is very expensive. However the half-pixel 
offset luma could be fetched from the unprocessed source frame (the input to this prior pass). In theory 
this might result in some noise in areas of depth of field or motion blur. However this noise is likely 
noticeable. Note, this case requires an adjustment to a later step in the algorithm (which is noted 
in that slide s comments).  MINTHREASHOLD AND THRESHOLD FXAA defaults threshold = 0.125, and minThreshold 
= 0.05. GATHER4 SUPPORT? The 4 filtered luma values can be fetched with one gather4 if supported by 
the GPU (see notes in prior slide). If gather4 is not available, store non-filtered luma in A in the 
prior pass to save luma conversion instructions in the FXAA pass. INCREASE ABILITY TO FILTER SINGLE 
PIXELS The luma for M needs to be unfiltered. Luma for M is calculated in the shader after fetching the 
RGB color for M. The unfiltered M is included to insure the algorithm does not exit early on single pixel 
 detail. Note, maxLuma does NOT include M. This is to provide a higher chance that dark single pixel 
dots on a light background also get filtered.  As is hopefully obvious here visually, early exit is 
the single most important tunable factor for performance for this algorithm. Note the actual Endless 
City Demo used a very early version of FXAA and added a lowpass filter over that. This source screen 
shot was taken with this older version of FXAA and the lowpass filter turned off.  ON XBOX 360 Prior 
pass would have normally resolved EDRAM to texture, then the next pass (FXAA) would start reading this 
texture, and writing anew to EDRAM for each pixel. In this optimized case, don t clear EDRAM during resolve. 
Then the optimized pass can early discard fragments which don t need AA. The discard in theory will not 
save in EDRAM bandwidth (as it isn t the limiter), however it might be a faster path in hardware? ON 
PS3 Should be easy to do. OPENGL OpenGL supports fetching from a texture currently bound as a framebuffer, 
 http://www.opengl.org/registry/specs/NV/texture_barrier.txt DX9 Supposedly not allowed? DX10/11 Supposedly 
not allowed?  The /2 factor is to maintain {-1 to 1} range for dir to make algorithm easier to explain 
in these slides. Direction magnitude is proportional to local edge contrast.  ADAPT LENGTH TO LOCAL 
CONTRAST The contrastFactor reduces filter width as local contrast decreases. Local contrast is defined 
as the ratio of the luma gradient to the average neighborhood luma. This is a critical factor in the 
algorithm. FUTURE WORK Certainly a next step is to treat the local neighborhood {NW,NE,SW,SE} as a bit 
vector and attempt to use an offline training and clustering process to compute the correct filter direction 
and length given the correction factor in the next slide. Then attempt to fit an equation to the results 
of this research. ADDING BLUR? Switching to 2/minDir will increase blurriness across the image. Might 
be useful as a soft focus post effect.  Specifically this pattern becomes a large problem, ooo oZo ooo 
For any o and Z , the algorithm sees dir.x = dir.y = 0. Subtracting out max(abs(dir.x),abs(dir.y)) removes 
edges which are already detected as non-subpixel detail. This visualizes singlePixelDetail factor directly. 
 This step happens after limiting filter width. So single pixel detail will increase the width of the 
filter (adding more blur). The (4.0 / max(nw,ne,sw,se,m)) normalizes by the local luma. The 4.0 controls 
the amount of diagonal push.  Note the actual Alien vs Triangles Demo used a very early version of FXAA. 
This source screen shot was taken with this older version of FXAA turned off.  If full-width filter 
width estimation is too large, then there is a chance the filter kernel will sample from regions off 
the local edge. In this case noise will be introduced by the filter kernel. This step attempts to remove 
this noise. Given the limited maximum filter width and the contrastFactor from the prior slide, the half-width 
filter is designed to be below the threshold of objectionable noise. IF OPTIMIZED LUMA WAS COMPUTED 
FROM A FRAME WHICH IS DIFFERENT THAN OUTPUT COLOR If luma is computed from a different frame than color, 
then make sure to use the luma stored in the RGBA values (and filtered by the half for full width box 
filter) instead of re-calculating luma from the RGB values. ELSE Otherwise definitely recalculate the 
luma from the RGB values, as the gather4 optimized luma is from a 2x2 box lowpass filtered version of 
the scene and will cause some small dithering artifacts.  This visualization includes discard. A majority 
of pixels are not modified by FXAA. Of the pixels modified, a the full-width filter invalidate is removing 
a lot of noise.  Performance numbers taken from an OpenGL test app. Timing using glBeginQuery(GL_TIME_ELAPSED_EXT,)/glEndQuery(). 
Timing taken as average of many runs with a set of 26 images. Early exit with conservative settings. 
Applications might be able to extract better performance with tighter thresholds. Tests running with 
32-bit per pixel RGBA texture and render targets. Note the gather4 cases do not include an extra texture 
fetch which must be placed in the prior pass. Hopefully that fetch can be hidden by shader math!  FXAA 
QUALITY Updated version of the first FXAA algorithm. Runs in a single full-screen-triangle fragment 
shader pass. (use an oversized triangle to cover the screen instead of a quad to avoid over-shading 
along diagonal) Designed for 32-bit per pixel input. Input is one RGBA texture. Output is one RGBA render 
target.  FXAA attempts to filter out not just aliasing on edges but also filter out sub-pixel aliasing. 
 IF HARDWARE SUPPORTS GATHER4 OF THE ALPHA CHANNEL Store luma in alpha channel to enable later discard 
optimization. Otherwise early-exit will have to write out a swizzled color (GBA to RGB), and therefore 
cannot use discard. In OpenGL swizzle (to get gather4 access to alpha channel) and gather4 are supported 
via, http://www.opengl.org/registry/specs/ARB/texture_swizzle.txt http://www.opengl.org/registry/specs/ARB/texture_gather.txt 
http://www.opengl.org/registry/specs/ARB/gpu_shader5.txt DX9 has no HLSL support for gather4. DX10.1 
adds support for gather4 for the R channel via Gather(). DX11 SM5 supports gather4 from any channel, 
and specifically alpha via GatherAlpha(). LUMA CONVERSION Using the following for color to luma conversion: 
luma = dot(color.rgb, float3(0.299, 0.587, 0.114));  Slightly different than early exit for FXAA console. 
Same discard optimization applies here as well. MINTHREASHOLD AND THRESHOLD FXAA defaults threshold = 
1/6, and minThreshold = 1/12. GATHER4 SUPPORT? This uses gather4 to fetch the full neighborhood of 9 
luma values in 4 fetches, xxx xXx xxx  Similar to FXAA Console, but higher quality. Builds singlePixelDetail 
factor from ratio of lowpassed contrast and regular contrast. Lowpass in this case is a + shaped filter 
instead of a box filter. The trim factor filters out low contrast single pixel detail. The cap factor 
insures some single pixel detail is not filtered away later.  CHOOSING ONE DIRECTION Sampling the 3x3 
neighborhood is required for early exit. Reusing these fetches for edge detection. Choosing one direction 
enables a reduction in search time later. CODE IS EASIER TO UNDERSTAND float edgeVert = abs(lumaNW + 
(-2.0 * lumaN) + lumaNE) + 2.0 * abs(lumaW + (-2.0 * lumaM) + lumaE ) + abs(lumaSW + (-2.0 * lumaS) + 
lumaSE); float edgeHorz = abs(lumaNW + (-2.0 * lumaW) + lumaSW) +  2.0 * abs(lumaN + (-2.0 * lumaM) 
+ lumaS ) + abs(lumaNE + (-2.0 * lumaE) + lumaSE); bool horzSpan = edgeHorz >= edgeVert;  Notice there 
is a case where edge direction test fails: where two spans on different lines or columns connect. This 
is a limitation of the 3x3 kernel size. However this is ok, as direction choice at the connection does 
not matter.  The threshold factor is 0.25. The shader samples in both directions in parallel as an optimization. 
  Again M is the pixel being filtered (the middle pixel). The red bar is the computed span. The orange 
line is the computed center of the span. The no filtering both sides case is the surrounding pixels 
for the filter both sides case (don t want to double filter).  Position on span is used to compute sub-pixel 
filter offset. The offset is limited to the range of 0.0 to 0.5 pixel shift in the direction of highest 
contrast.  Eye is much less sensitive to chroma, so the luma optimization here works.  Performance 
numbers taken from an OpenGL test app. Timing using glBeginQuery(GL_TIME_ELAPSED_EXT,)/glEndQuery(). 
Timing taken on just one image (didn t have time to run entire batch). Early exit with conservative settings. 
Applications might be able to extract better performance with tighter thresholds. Tests running with 
32-bit per pixel RGBA texture and render targets.  Simple method to mix some super-sampling with FXAA. 
 (1.) Render at {width*1.3333, height*1.3333} resolution (1.77x the number of pixels). (2.) Apply FXAA. 
(3.) Down-sample to display resolution {width, height}. (4.) Apply HUD/UI. (5.) Scan-out to display (perhaps 
even using a hardware scalar to up-sample on Xbox).  FXAA is engineered to be applied towards the end 
of engine post processing after conversion to low dynamic range and after conversion to the sRGB color 
space for display. Attempting to apply FXAA to a HDR image prior to tone-mapping will result in the 
same artifacts as resolving an MSAA surface prior to tone-mapping: anti-aliasing will effectively be 
turned off on edges which have both in-LDR-range and out-of-LDR-range intensity. For example a 0.0 
intensity pixel and a 16.0 intensity pixel will average to a 8.0 intensity pixel which later might get 
tone-mapped to 1.0 intensity (which results in no blend between the two pixels). NON-LINEARLY CORRECT 
REQUIRED FOR FXAA QUALITY FXAA Quality searches for the end of spans using average of pixels. This average 
needs to be the perceptual average. Linear average isn t perceptual and causes problems with end of span 
search. NON-LINEAR IS BETTER FOR FXAA PERFORMANCE On some GPUs, sRGB labeled formats require a slower 
and higher precision data path for filtering or blending (for the sRGB->linear or linear->sRGB conversion). 
This can have the effect of either texture or ROP performance cutting in half. Not good when an algorithm 
is already bound on texture fetch! NON-LINEAR COUNTERACTS FXAA CONSOLE ALGORITHM S TENDENCY TO BLOOM 
HIGH- LUMA EDGES The algorithm, by design, increases blur on higher contrast edges. With linearly-correct 
blending, the lighter side of an edge will expand outward. With non-linearly-correct blending, the darker 
side of an edge will expand outward. SIDE EFFECTS OF BEING NON-LINEARLY CORRECT (1.) Slight darkening 
of edges. (2.) Slight reduction in saturation of edges. (3.) Hardly noticeable in practice. DX11 ALIAS 
SRGB TEXTURE FORMATS AS UNORM When integrating FXAA to sample from a DXGI_FORMAT_R8G8B8A8_UNORM_SRGB 
formatted texture, modify the texture resource to be DXGI_FORMAT_R8G8B8A8_TYPELESS, and use a DXGI_FORMAT_R8G8B8A8_UNORM 
shader resource view for FXAA, and DXGI_FORMAT_R8G8B8A8_UNORM_SRGB shader resource views for the rest 
of the engine. Make sure to apply the same type aliasing when FXAA needs to write into a DXGI_FORMAT_R8G8B8A8_UNORM_SRGB 
formatted render target. DX9 ADJUST SAMPLER AND RENDER STATE // turn off sRGB->linear conversion when 
fetching from TEX SetSamplerState(sampler, D3DSAMP_SRGBTEXTURE, 0); // turn off linear->sRGB conversion 
when writing to ROP SetRenderState(D3DRS_SRGBWRITEENABLE, 0); OPEN GL USE EXTENSIONS Frame buffer sRGB 
to non-sRGB aliasing is provided by the EXT_framebuffer_sRGB extension. http://www.opengl.org/registry/specs/EXT/framebuffer_sRGB.txt 
 // disable linear->sRGB conversion when writing to ROP glDisable(GL_FRAMEBUFFER_SRGB_EXT); Texture 
sRGB to non-sRGB aliasing is provided by the EXT_texture_sRGB_decode extension, which can enable/disable 
for both textures and samplers. Texture enable/disable below. http://www.opengl.org/registry/specs/EXT/texture_sRGB_decode.txt 
 // turn off sRGB->linear conversion when fetching from TEX glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SRGB_DECODE_TEX, 
GL_SKIP_DECODE_EXT);  Source blog post: http://copypastepixel.blogspot.com/2010/12/anti-aliasing.html 
Source image also composed of work from Alvaro Luna Bautista and Joel Andersdon (the natural history 
museum) from http://www.3drender.com/challenges/ (Challenge #17) Stefan s blog post explores the unusual 
idea of mixing unsharp mask to enhance detail with antialiasing (unsual in that the unsharp mask typically 
has the net effect of increasing aliasing). This slide shows a mockup of a crop of one of Stefan s images 
processed in two ways, LEFT Shows the result of applying a non-linearly correct heavy unsharp mask 
to luma, followed by applying FXAA. The result is less aliased, but suffers from non-linear blending 
artifacts (see the yellow arrow). Areas turned black by the unsharp mask, in FXAA, blend with lighter 
areas and loose saturation and have a color shift. RIGHT Shows the result of applying FXAA, then applying 
a non-linearly correct heavy unsharp mask. Colors are better this way, and the result is sharper, but 
more aliased.  Top row is the original image; bottom row is the antialiased result. This antialiasing 
method is part of the same family as MLAA and all the other antialiasing effects that operate as a postprocess. 
The approach we re taking is to compute distance to edge in the pixel shader during the beauty pass and 
write out blur hints alongside the color. The big advantage to this technique is that it has the capability 
to deliver very smooth edges, with up to 256 different levels of blur, and it avoids many of the temporal 
problems that MLAA techniques have like crawling along rotating edges. It has quite different strengths 
and weaknesses to standard MLAA. Since the distanceto-edge is computed analytically there s no search 
for edges in the pixel s neighbourhood, so the expensive local-search step is skipped completely. The 
corresponding downside is that it needs extra space in the framebuffer to store the distance-to-edge 
information: 8 bits is OK, 32 bits would be ideal. It does have a couple of other downsides though, which 
I ll cover. We investigated applying it to APB, and I ll go over the problems that we hit. The obvious 
question is, why didn t this make it in? The reasons are pretty boring: this method had a couple of problems 
when we ran it with their content, we were still working on those problems and were not ready to start 
thinking about integrating it when they were locking down the assets for launch. The most important 
part of this technique is the method for computing the distance to the nearest triangle edge in the pixel 
shader, so we ll start there. For a given triangle, if the vertex program outputs the value v=1 on one 
vertex and v=0 on the other two verts/opposite edge, then the interpolated value v is roughly proportional 
to distance from that edge. The values distance_x=-v/(dv/dx) and distance_y=-v/(dv/dy) give the signed 
distance to the edge of the triangle in screen pixels. If distance_x is positive, the triangle edge is 
to the right of the current pixel; if it's negative is to the left of the current pixel. So this method 
allows the pixel shader to figure out the distance to one particular edge of the triangle, with pretty 
good precision. Due to perspective correction v is not linearly interpolated across the triangle, so 
the calculation gives an estimate rather than an exact value. There s a lot of interesting uses for the 
basic ddx/ddy screen-space distance to edge maths, but we won t go there in this talk. Anyway, the next 
step is to set up some fast, robust code in the vertex program to output the value 0 on silhouette verts 
and 1 everywhere else, that supports skinned and deforming meshes, procedurally generated geometry, and 
everything else that shows up in games these days. Unfortunately, I don t know of any such code. I experimented 
with a bunch of methods and couldn t find anything both robust and fast enough to be an option. I d suggest 
taking a different approach altogether, which is to write out distance-to-edge hints for all potential 
aliased edges and let the postprocess decide whether to blur based on color difference. Computing the 
distance-to-edge value for all three edges is straightforward. We do this by making v be a three-component 
vector instead of a scalar. This image shows the vector v as a color. At each of the three verts one 
component will be 1, and the others 0. To put it another way, along each edge of the triangle exactly 
one component will be zero egin the image the top edge is implied by the blue component, the left edge 
is implied by the green component. We can then figure out the distance to the edge of the triangle in 
the four directions left, right, above and below the pixel we re currently shading. To be precise, for 
each direction we measure the distance to each of the three triangle edges, and take the minimum of the 
three values. As you might guess, all the maths can be vectorized and made completely branch free. This 
diagram shows a large triangle being drawn. The black line is the edge of the triangle; pixels below 
the line are inside the triangle and are rasterized. The arrows indicate pixels where the distance upwards 
to a triangle edge is less than one pixel-we don t care about distances greater than one pixel, for 
reasons that ll become clear shortly. We calculated distance values for four directions-left, right, 
up and down. Storing them in an RGBA rendertarget with 8 bits per component is the natural choice. This 
is a pretty expensive item if it s a dealbreaker, there are some other options. If this triangle was 
part of a surface, then we d expect a complementary triangle to be rendered on the other side of the 
edge indicated by the black line, and to write out distance-to-edge values for the upward direction that 
were the exact inverse of the downward distances shown here. But if the black line is a silhouette edge 
then this won t happen, the distance values for the complementary pixels could be anything. These cases 
will come up again when we talk about the postprocess. Also note that these distance hints are written 
no matter what, irrespective of whether the edge needs antialiasing or not. To explain how the postprocess 
works let s start with the 1D case. The image above shows two cases. The green and blue rectangles indicate 
two pieces of geometry that have been rasterized; they re slightly different in the two cases. Three 
pixels are involved: p0, p1 and p2. The long red vertical lines indicate the pixel center, corresponding 
to the positions where the rasterizer samples the geometry and the forward pass pixel shader is run. 
The short black vertical lines indicate midpoints between pixel centers. The arrows show the distance-to-edge 
stored for each pixel center; the maximum distance is clamped to 1 pixel. In the diagrams, imagine the 
cube is the green region and the sky is the blue. Pixel p1 is part of the cube, pixel p2 is part of the 
sky. When the cube was rasterized it wrote to pixel p1 and set up the distance-to-edge values; but pixel 
p2 is part of a large sky triangle and has no relevant distance-to-edge hints. In this case only one 
pixel of a neighbouring pair has a distance-to-edge value: this will happen along all silhouette edges, 
and near nonmanifold edges. There are two other cases cases to consider. Two neighbouring pixels might 
have complementary distance-to-edge values-this will happen when the two pixels are in two neighbouring 
triangles that share an edge. Alternatively the distances might both be defined, but not be complementary: 
once case where this will happen if there are lots of subpixel triangles. In the upper case in the diagram, 
the only pixel that needs to be blurred is pixel p1. The distance-to-edge values encoded for that pixel 
give us all the information required about pixel coverage. A bit of pixel p2 needs to be blended in. 
The lower case is more complex: note pixel p2. This pixel should be a mixture of green and blue, even 
though the distance-to-edge values for pixel p2 suggest there s no edges nearby. To correctly handle 
this case, the postprocess has to look at the hint values for neighbouring pixels, and choose between 
the two competing values. One straightforward way to support all these cases is for the postprocess to 
choose the minimum of the two competing distance-to-edge values to compute the blend. So the whole process 
is this. For each pixel, consider the region within half a pixel in each direction. Examine the two neighbours 
and compute the distance-to-edge value in each direction. If the distance-to-edge is less than half a 
pixel, then that neighbour will need to be blended in. The contribution of each of the three pixels is 
the fraction of the onepixel area it covers. Extending to 2D is simple: for each pixel, apply the 1D 
case either vertically or horizontally. Decide which by summing the distance-to-edge values involved 
in each approach, use the axis with the smaller sum. A more natural option is to compute both horizontal 
and vertical blends and combine them. For instance: multiply the calculated two coverage values for the 
center pixel, and renormalize the resulting weights. All these options costs more and none of the options 
I tried improved the quality; they often introduced artifacts. Lastly, the blur only needs to be applied 
to edges that are potentially aliased. This can be decided by comparing pixel colors and skipping the 
blend if the color difference across an edge isn t big enough to cause noticeable aliasing, in a similar 
spirit to MLAA. Demo here. We get a very smooth gradient that correctly matches the steps of the edge. 
Duplicating the quality of the gradient with MLAA would need a very large neighbourhood search. One situation 
where MLAA's local neighbourhood is especially noticeable is for an edge slowly rotating past an axis. 
With MLAA the local neighbourhood reveals itself with localized blurs on each step, which travel along 
the edge as it rotates. With DEAA we get a seamless transition as it copes with the very long edge steps. 
One case that MLAA can t support is subpixel movement of an edge for instance, where a perfectly vertical 
line moves sweeps across the screen. Methods that only use the color buffer can t infer the subpixel 
position of the edge; in comparison DEAA will correctly simulate new columns of pixels fading in as the 
line moves. Any thin feature (such as a ledge, or doorframe) will become thinner than a pixel when viewed 
from side-on or from a distance. Triangles that are thinner than a pixel lead to problems because the 
percieved edge won't have a consistent set of distance-to-edge values. A cluster of pixels along an edge 
with unnaturally low distance-to-edge values creates a section of edge that looks aliased. When all other 
edges are nicely antialiased these sections really stand out. One way to minimize the problem is to make 
the tools that creates the vertex data not set up a varying silhouette parameter towards an edge that 
will never cause aliasing problems. This isn t an option smoothly curving surfaces where neighbouring 
triangles have only a slight bend between them, such as character models. In this case subpixel triangles 
are unavoidable near the silhouette edge. In the GPU Pro article I suggested extruding backfacing verts 
outwards by a pixel or half a pixel to ensure that silhouette triangles are large enough to avoid the 
undersampling problems. This does work, but when used on characters it makes them look fat. In that article 
I described a two-pixel blend for the postprocess, and it led to really bad quality problems in these 
cases. The three-pixel blend that I described here copes much better. Subpixel gaps between geometry 
leads to aliasing problems because the gap is undersampled. For example, a thin gap between two pillars 
showing sky. Although the distance-to-edge values might be completely consistent, the pixel being blended 
in won't consistently be sky; sometimes it'll be from the other pillar. Lastly, I've listed three cases 
where there is a visible edge but there's no distance-to-edge information, so this method won't help. 
In comparison, MLAA will provide some antialiasing in each of those cases. Simulated alphablending. 
Overwrite the distance-to-edge hint values to fade the new object in progressively. The first image in 
the sequence is the original, the other six have a 50% alpha-to coverage checkerboard rendered over 
the top. No alphablending is used. The distanceto-edge values vary from 0.0 to 1.0 over the sequence; 
the postprocess blur blends in neighbouring pixels by varying amounts to simulate the alpha blend. With 
some limitations, this means you could apply deferred lighting to semitransparent surfaces. Alex Evans 
described how LittleBigPlanet fakes deferred lighting on 50% transparent surfaces in a similar way. 
Another use is better quality alpha-to-coverage effects. A second idea is for improving alpha-tested 
edges, like on foliage. This is pretty simple: compute the alpha-test value, and kill the pixel if it 
s below zero as usual. If the pixel is not killed, feed the alpha-test value into the existing distance-toedge 
evaluation function and framebuffer encode process, and you re done.                 
             
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037643</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>136</pages>
		<display_no>7</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Introduction to modern openGL programming]]></title>
		<page_from>1</page_from>
		<page_to>136</page_to>
		<doi_number>10.1145/2037636.2037643</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037643</url>
		<abstract>
			<par><![CDATA[<p>OpenGL is the most widely available library for creating interactive computer graphics applications across all of the major computer operating systems. Its uses range from creating applications for scientific visualizations to computer-aided design, interactive gaming, and entertainment, and with each new version, its capabilities reveal the most up-to-date features of modern graphics hardware.</p> <p>This course is an accelerated introduction to programming OpenGL, emphasizing the most modern methods for using the library. In recent years, OpenGL has evolved and fundamentally changed how programmers interact with the application programming interface (API). The most notable change was the introduction of shader-based rendering, which has expanded to subsume almost all functionality in OpenGL. The course reviews each of the shader stages in OpenGL and how to specify data for rendering with OpenGL. And it summarizes how OpenGL's wealth of new functionality and features enables creation of ever-richer content.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074.10011092</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software development techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808991</person_id>
				<author_profile_id><![CDATA[81100366265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Edward]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808992</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Introduction to Modern OpenGL Programming SIGGRAPH 2011 Ed Angel University of New Mexico Dave 
Shreiner ARM, Inc. Presented August 7th, 2011   OpenGL is a library of function calls for doing computer 
graphics. With it, you can create interactive applications that render high-quality color images composed 
of 3D geometric objects and images. Additionally, the OpenGL API is window and operating system independent. 
That means that the part of your application that draws can be platform independent. However, in order 
for OpenGL to be able to render, it needs a window to draw into. Generally, this is controlled by the 
windowing system on whatever platform you are working on. While OpenGL has been around for close to 
20 years, a lot of changes have occurred in that time. This course concentrates on the latest versions 
of OpenGL specifically OpenGL 4.1. In these modern versions of OpenGL (which we defined as versions 
starting with version 3.1), OpenGL applications are shader based. In fact most of this course will discuss 
shaders and the operations they support. If you re familiar with previous versions of OpenGL, or other 
rasterization-based graphics pipelines that may have included fixed-function processing, we won t be 
covering those techniques. Instead, we ll concentrate on showing how we can implement those techniques 
on a modern, shader-based graphics pipeline. In this modern world of OpenGL, all applications will need 
to provide shaders, and as such, providing some perspective on how the pipeline evolved and its phases 
will be illustrative. We ll discuss this next.  The initial version of OpenGL was announced in July 
of 1994. That version of OpenGL implemented what s called a fixed-function pipeline, which means that 
all of the operations that OpenGL supported were fully-defined, and an application could only modify 
their operation by changing a set of input values (like colors or positions). The other point of a fixed-function 
pipeline is that the order of operations was always the same that is, you can t reorder the sequence 
operations occur. This pipeline was the basis of many versions of OpenGL and expanded in many ways, and 
is still available for use. However, modern GPUs and their features have diverged from this pipeline, 
and support of these previous versions of OpenGL are for supporting current applications. If you re developing 
a new application, we strongly recommend using the techniques that we ll discuss. Those techniques can 
be more flexible, and will likely preform better than using one of these early versions of OpenGL. While 
many features and improvements were added into the fixed-function OpenGL pipeline, designs of GPUs were 
exposing more features than could be added into OpenGL. To allow applications to gain access to these 
new GPU features, OpenGL version 2.0 officially added programmable shaders into the graphics pipeline. 
This version of the pipeline allowed an application to create small programs, called shaders, that were 
responsible for implementing the features required by the application. In the 2.0 version of the pipeline, 
two programmable stages were made available: vertex shading enabled the application full control over 
manipulation of the 3D geometry provided by the application  fragment shading provided the application 
capabilities for shading pixels (the terms classically  used for determining a pixel s color). OpenGL 
2.0 also fully supported OpenGL 1.X s pipeline, allowing the application to use both version of the pipeline: 
fixed-function, and programmable. Note: some OpenGL implementations also include a debug context which 
provides enhanced debugging information about. Debug contexts are currently an extension to OpenGL, and 
not a required type of context. Until OpenGL 3.0, features have only been added (but never removed) 
from OpenGL, providing a lot of application backwards compatibility (up to the use of extensions). OpenGL 
version 3.0 introduced the mechanisms for removing features from OpenGL, called the deprecation model. 
It defines how the OpenGL design committee (the OpenGL Architecture Review Board (ARB) of the Khronos 
Group) will advertise of which and how functionality is removed from OpenGL. You might ask: why remove 
features from OpenGL? Over the 15 years to OpenGL 3.0, GPU features and capabilities expanded and some 
of the methods used in older versions of OpenGL were not as efficient as modern methods. While removing 
them could break support for older applications, it also simplified and optimized the GPUs allowing better 
performance. Within an OpenGL application, OpenGL uses an opaque data structure called a context, which 
OpenGL uses to store shaders and other data. Contexts come in two flavors: full contexts expose all 
the features of the current version of OpenGL, including features that are marked deprecated.  forward-compatible 
contexts enable only the features that will be available in the next version of OpenGL (i.e., deprecated 
features pretend to be removed), which can help developers make sure their applications work with future 
version of OpenGL.  Forward-compatible contexts are available in OpenGL versions from 3.1 onwards. 
OpenGL version 3.1 was the first version to remove deprecated features, and break backwards compatibility 
with previous versions of OpenGL. The features removed from included the old-style fixed-function pipeline, 
among other lesser features. One major refinement introduced in 3.1 was requiring all data to be placed 
in GPU-resident buffer objects, which help reduce the impacts of various computer system architecture 
limitations related to GPUs. While many features were removed from OpenGL 3.1, the OpenGL ARB realized 
that to make it easy for application developers to transition their products, they introduced an OpenGL 
extensions, GL_ARB_compatibility, that allowed access to the removed features. Until OpenGL 3.2, the 
number of shader stages in the OpenGL pipeline remained the same, with only vertex and fragment shaders 
being supported. OpenGL version 3.2 added a new shader stage called geometry shading which allows the 
modification (and generation) of geometry within the OpenGL pipeline. We briefly discuss geometry shaders 
later in the presentation. In order to make it easier for developers to choose the set of features they 
want to use in their application, OpenGL 3.2 also introduced profiles which allow further selection of 
OpenGL contexts. The core profile is the modern, trimmed-down version of OpenGL that includes the latest 
features. You can request a core profile for a Full or Forward-compatible profile. Conversely, you could 
request a compatible profile, which includes all functionality (supported by the OpenGL driver on your 
system) in all versions of OpenGL up to, and including, the version you ve requested. The OpenGL 4.X 
pipeline added another pair of shaders (which work in tandem, so we consider it a single stage) for supporting 
dynamic tessellation in the GPU. Tessellation control and tessellation evaluation shaders were added 
to OpenGL version 4.0. The current version of OpenGL is 4.1, which includes some additional features 
over the 4.0 pipeline, but no new shading stages.  To begin, let us introduce a simplified model of 
the OpenGL pipeline. Generally speaking, data flows from your application through the GPU to generate 
an image in the frame buffer. Your application will provide vertices, which are collections of data that 
are composed to form geometric objects, to the OpenGL pipeline. The vertex processing stage uses a vertex 
shader to process each vertex, doing any computations necessary to determine where in the frame buffer 
each piece of geometry should go. The other shading stages we mentioned tessellation and geometry shading 
 are also used for vertex processing, but we re trying to keep this simple at the moment. After all the 
vertices for a piece of geometry are processed, the rasterizer determines which pixels in the frame buffer 
are affected by the geometry, and for each pixel, the fragment processing stage is employed, where the 
fragment shader runs to determine the final color of the pixel. In your OpenGL applications, you ll usually 
need to do the following tasks: specify the vertices for your geometry  load vertex and fragment shaders 
(and other shaders, if you re using them as well)  issue your geometry to engage the OpenGL pipeline 
for processing Of course, OpenGL is capable of many other operations as well, many of which are outside 
of the  scope of this introductory course. We have included references at the end of the notes for your 
further research and development. You ll find that a few techniques for programming with modern OpenGL 
goes a long way. In fact, most programs in terms of OpenGL activity are very repetitive. Differences 
usually occur in how objects are rendered, and that s mostly handled in your shaders. There four steps 
you ll use for rendering a geometric object are as follows: 1. First, you ll load and create OpenGL shader 
programs from shader source programs you create 2. Next, you will need to load the data for your objects 
into OpenGL s memory. You do this by creating buffer objects and loading data into them. 3. Continuing, 
OpenGL needs to be told how to interpret the data in your buffer objects and associate that data with 
variables that you ll use in your shaders. We call this shader plumbing. 4. Finally, with your data 
initialized and shaders set up, you ll render your objects  We ll expand on those steps more through 
the course, but you ll find that most applications will merely iterate through those steps. While OpenGL 
will take care of filling the pixels in your application s output window or image, it has no mechanisms 
for creating that rendering surface. Instead, OpenGL relies on the native windowing system of your operating 
system to create a window, and make it available for OpenGL to render into. For each windowing system 
(like Microsoft Windows, or the X Window System on Linux [and other Unixes]), there s a binding library 
that lets mediates between OpenGL and the native windowing system. Since each windowing system has different 
semantics for creating windows and binding OpenGL to them, discussing each one is outside of the scope 
of this course. Instead, we use an open-source library named Freeglut that abstracts each windowing system 
s specifics into a simple library. Freeglut is a derivative of an older implementation called GLUT, and 
we ll use those names interchangeably. GLUT will help us in creating windows, dealing with user input 
and input devices, and other window-system activities. You can find out more about Freeglut at its website: 
http://freeglut.sourceforge.net Just like window systems, operating systems have different ways of working 
with libraries. In some cases, the library you link your application exposes different functions than 
the library you execute your program with. Microsoft Windows is a notable example where you compile your 
application with a .lib library, but use a .dll at runtime for finding function definitions. As such, 
your application would generally need to use operating-system specific methods to access functions. In 
general, this is troublesome and a lot of work. Fortunately, another open-source library comes to our 
aid, GLEW, the OpenGL Extension Wrangler library. It removes all the complexity of accessing OpenGL functions, 
and working with OpenGL extensions. We use GLEW in our examples to simplify the code. You can find details 
about GLEW at its website: http://glew.sourceforge.net  In OpenGL, as in other graphics libraries, objects 
in the scene are composed of geometric primitives, which themselves are described by vertices. A vertex 
in modern OpenGL is a collection of data values associated with a location in space. Those data values 
might include colors, reflection information for lighting, or additional coordinates for use in texture 
mapping. Vertices must be organized in OpenGL server-side objects called vertex buffer objects (also 
known asVBOs), which need to contain all of the vertex information for all of the primitives that you 
want to draw at one time. VBOs can store vertex information in almost any format (i.e., an array-ofstructures 
(AoS) each containing a single vertex s information, or a structure-of-arrays (SoA) where all of the 
same type of data for a vertex is stored in a contiguous array, and the structure stores arrays for each 
attribute that a vertex can have). The data within a VBO needs to be contiguous in memory, but doesn 
t need to be tightly packed (i.e., data elements may be separated by any number of bytes, as long as 
the number of bytes between attributes is consistent). VBOs are further required to be stored in vertex 
array objects (known as VAOs). Since it may be the case that numerous VBOs are associated with a single 
object, VAOs simplify the management of the collection of VBOs. To form 3D geometric objects, you need 
to decompose them into geometric primitives that OpenGL can draw. OpenGL only knows how to draw three 
things: points, lines, and triangles, but can use collections of the same type of primitive to optimize 
rendering. OpenGL Primitive Description Total Vertices for n Primitives GL_POINTS Render a single point 
per vertex (points may be larger than a single pixel) n GL_LINES Connect each pair of vertices with 
a single line segment. 2n GL_LINE_STRIP Connect each successive vertex to the previous one with a line 
segment. n+1 GL_LINE_LOOP Connect all vertices in a loop of line segments. n GL_TRIANGLES Render a triangle 
for each triple of vertices. 3n GL_TRIANGLE_STRIP Render a triangle from the first three vertices in 
the list, and then create a new triangle with the last two rendered vertices, and the new vertex. n+2 
GL_TRIANGLE_FAN Create triangles by using the first vertex in the list, and pairs of successive vertices. 
n+2  The next few slides will introduce our first example program, one which simply displays a cube 
with different colors at each vertex. We aim for simplicity in this example, focusing on the OpenGL techniques, 
and not on optimal performance. In order to simplify our application development, we define a few types 
and constants to make our code more readable and organized. Our cube, like any other cube, has six square 
faces, each of which we ll draw as two triangles. In order to sizes memory arrays to hold the necessary 
vertex data, we define the constant NumVertices. Additionally, as we ll see in our first shader, the 
OpenGL shading language, GLSL, has a built-in type called vec4, which represents a vector of four floating-point 
values. We define a C++ class for our application that has the same semantics as that GLSL type. Additionally, 
to logically associate a type for our data with what we intend to do with it, we leverage C++ typedefs 
to create alias for colors and positions. In order to provide data for OpenGL to use, we need to stage 
it so that we can load it into the VBOs that our application will use. In your applications, you might 
load these data from a file, or generate them on the fly. For each vertex, we want to use two bits of 
data vertex attributes in OpenGL speak to help process each vertex to draw the cube. In our case, each 
vertex has a position in space, and an associated color. To store those values for later use in our VBOs, 
we create two arrays to hold the per vertex data. In our example we ll copy the coordinates of our cube 
model into a VBO for OpenGL to use. Here we set up an array of eight coordinates for the corners of a 
unit cube centered at the origin. You may be asking yourself: Why do we have four coordinates for 3D 
data? The answer is that in computer graphics, it s often useful to include a fourth coordinate to represent 
three-dimensional coordinates, as it allows numerous mathematical techniques that are common operations 
in graphics to be done in the same way. In fact, this four-dimensional coordinate has a proper name, 
a homogenous coordinate. Just like our positional data, we ll set up a matching set of colors for each 
of the model s vertices, which we ll later copy into our VBO. Here we set up eight RGBA colors. In OpenGL, 
colors are processed in the pipeline as floating-point values in the range [0.0, 1.0]. Your input data 
can take any for; for example, image data from a digital photograph usually has values between [0, 255]. 
OpenGL will (if you request it), automatically convert those values into [0.0, 1.0], a process called 
normalizing values. As our cube is constructed from square cube faces, we create a small function, quad(), 
which takes the indices into the original vertex color and position arrays, and copies the data into 
the VBO staging arrays. If you were to use this method (and we ll see better ways in a moment), you would 
need to remember to reset the Index value between setting up your VBO arrays. Here we complete the generation 
of our cube s VBO data by specifying the six faces using index values into our original vertex_positions 
and vertex_colors arrays. It s worth noting that the order that we choose our vertex indices is important, 
as it will affect something called backface culling later. We ll see later that instead of creating the 
cube by copying lots of data, we can use our original vertex data along with just the indices we passed 
into quad() here to accomplish the same effect. That technique is very common, and something you ll use 
a lot. We chose this to introduce the technique in this manner to simplify the OpenGL concepts for loading 
VBO data. Similarly to VBOs, vertex array objects (VAOs) encapsulate all of the VBO data for an object. 
This allows much easier switching of data when rendering multiple objects (provided the data s been set 
up in multiple VAOs). The process for initializing a VAO is similar to that of a VBO, except a little 
less involved. 1. First, generate a name VAO name by calling glGenVertexArrays() 2. Next, make the VAO 
current by calling glBindVertexArray(). Similar to what was described for VBOs, you ll call this every 
time you want to use or update the VBOs contained within this VAO.  The above sequence calls shows 
how to create and bind a VAO. Since all geometric data in OpenGL must be stored in VAOs, you ll use this 
code idiom often. While we ve talked a lot about VBOs, we haven t detailed how one goes about creating 
them. Vertex buffer objects, like all (memory) objects in OpenGL (as compared to geometric objects) are 
created in the same way, using the same set of functions. In fact, you ll see that the pattern of calls 
we make here are similar to other sequences of calls for doing other OpenGL operations. In the case of 
vertex buffer objects, you ll do the following sequence of function calls: 1. Generate a buffer s name 
by calling glGenBuffers() 2. Next, you ll make that buffer the current buffer, which means it s the 
selected buffer for reading or writing data values by calling glBindBuffer(), with a type of GL_ARRAY_BUFFER. 
There are different types of buffer objects, with an array buffer being the one used for storing geometric 
data. 3. To initialize a buffer, you ll call glBufferData(), which will copy data from your application 
into the GPU s memory. You would do the same operation if you also wanted to update data in the buffer 
 4. Finally, when it comes time to render using the data in the buffer, you ll once again call glBindVertexArray() 
to make it and its VBOs current again. In fact, if you have multiple objects, each with their own VAO, 
you ll likely call glBindVertexArray() once per frame for each object.  The above sequence of calls 
illustrates generating, binding, and initializing a VBO with data. In this example, we use a technique 
permitting data to be loaded into two steps, which we need as our data values are in two separate arrays. 
It s noteworthy to look at the glBufferData() call; in this call, we basically have OpenGL allocate an 
array sized to our needs (the combined size of our point and color arrays), but don t transfer any data 
with the call, which is specified with the NULL value. This is akin to calling malloc() to create a buffer 
of uninitialized data. We later load that array with our calls to glBufferSubData(), which allows us 
to replace a subsection of our array. This technique is also useful if you need to update data inside 
of a VBO at some point in the execution of your application. The final step in preparing you data for 
processing by OpenGL (i.e., sending it down for rendering) is to specify which vertex attributes you 
d like issued to the graphics pipeline. While this might seem superfluous, it allows you to specify multiple 
collections of data, and choose which ones you d like to use at any given time. Each of the attributes 
that we enable must be associated with an in variable of the currently bound vertex shader. You retrieve 
vertex attribute locations was retrieved from the compiled shader by calling glGetAttribLocation(). We 
discuss this call in the shader section. To complete the plumbing of associating our vertex data with 
variables in our shader programs, you need to tell OpenGL where in our buffer object to find the vertex 
data, and which shader variable to pass the data to when we draw. The above code snippet shows that process 
for our two data sources. In our shaders (which we ll discuss in a moment), we have two variables: vPosition, 
and vColor, which we will associate with the data values in our VBOs that we copied form our vertex_positions 
and vertex_colors arrays. The calls to glGetAttribLocation() will return a compiler-generated index which 
we need to use to complete the connection from our data to the shader inputs. We also need to turn the 
valve on our data by enabling its attribute array by calling glEnableVertexAttribArray() with the selected 
attribute location. This is the most flexible approach to this process, but depending on your OpenGL 
version, you may be able to use the layout construct, which allows you to specify the attribute location, 
as compared to having to retrieve it after compiling and linking your shaders. We ll discuss that in 
our shader section later in the course. In order to initiate the rendering of primitives, you need to 
issue a drawing routine. While there are many routines for this in OpenGL, we ll discuss the most fundamental 
ones. The simplest routine is glDrawArrays(), to which you specify what type of graphics primitive you 
want to draw (e.g., here we re rending a triangle strip), which vertex in the enabled vertex attribute 
arrays to start with, and how many vertices to send. This is the simplest way of rendering geometry in 
OpenGL Version 3.1. You merely need to store you vertex data in sequence, and then glDrawArrays() takes 
care of the rest. However, in some cases, this won t be the most memory efficient method of doing things. 
Many geometric objects share vertices between geometric primitives, and with this method, you need to 
replicate the data once for each vertex. We ll see a more flexible, in terms of memory storage and access 
in the next slides.  As with any programming language, GLSL has types for variables. However, it includes 
vector-, and matrix-based types to simplify the operations that occur often in computer graphics. In 
addition to numerical types, other types like texture samplers are used to enable other OpenGL operations. 
We ll discuss texture samplers in the texture mapping section.  The vector and matrix classes of GLSL 
are first-class types, with arithmetic and logical operations well defined. This helps simplify your 
code, and prevent errors. For GLSL s vector types, you ll find that often you may also want to access 
components within the vector, as well as operate on all of the vector s components at the same time. 
To support that, vectors and matrices (which are really a vector of vectors), support normal C vector 
accessing using the square-bracket notation (e.g., [i] ), with zero-based indexing. Additionally, vectors 
(but not matrices) support swizzling, which provides a very powerful method for accessing and manipulating 
vector components. Swizzles allow components within a vector to be accessed by name. For example, the 
first element in a vector element 0 can also be referenced by the names x , s , and r . Why all the 
names to clarify their usage. If you re working with a color, for example, it may be clearer in the 
code to use r to represent the red channel, as compared to x , which make more sense as the x-positional 
coordinate In addition to types, GLSL has numerous qualifiers to describe a variable usage. The most 
common of those are:  in qualifiers that indicate the shader variable will receive data flowing into 
the shader, either from the application, or the previous shader stage. out qualifier which tag a variable 
as data output where data will flow to the next shader stage, or to the framebuffer  uniform qualifiers 
for accessing data that doesn t change across a draw operation  Like the C language, GLSL supports 
all of the logical flow control statements you re used to. GLSL also provides a rich library of functions 
supporting common operations. While pretty much every vector-and matrix-related function available you 
can think of, along with the most common mathematical functions are built into GLSL, there s no support 
for operations like reading files or printing values. Shaders are really data-flow engines with data 
coming in, being processed, and sent on for further processing. Fundamental to shader processing are 
a couple of built-in GLSL variable which are the terminus for operations. In particular, vertex data, 
which can be processed by up to for shader stages in OpenGL are all ended by setting a positional value 
into the built-in variable, gl_Position. Similarly, the output of a fragment shader (in version 3.1 of 
OpenGL) is set by writing values into the built-in variable gl_FragColor. Later versions of OpenGL allow 
fragment shaders to output to other variables of the user s designation as well. Here s the simple vertex 
shader we use in our cube rendering example. It accepts two vertex attributes as input: the vertex s 
position and color, and does very little processing on them; in fact, it merely copies the input into 
some output variables (with gl_Position being implicitly declared). The results of each vertex shader 
execution are passed further down the OpenGL pipeline, and ultimately end their processing in the fragment 
shader. Here s the associated fragment shader that we use in our cube example. While this shader is 
as simple as they come merely setting the fragment s color to the input color passed in, there s been 
a lot of processing to this point. In particular, every fragment that s shaded was generated by the rasterizer, 
which is a built-in, non-programmable (i.e., you don t write a shader to control its operation). What 
s magical about this process is that if the colors across the geometric primitive (for multi-vertex primitives: 
lines and triangles) is not the same, the rasterizer will interpolate those colors across the primitive, 
passing each iterated value into our color variable. Shaders need to be compiled in order to be used 
in your program. As compared to C programs, the compiler and linker are implemented in the OpenGL driver, 
and accessible through function calls from within your program. The diagram illustrates the steps required 
to compile and link each type of shader into your shader program. A program can contain either a vertex 
shader (which replaces the fixed-function vertex processing), a fragment shader (which replaces the fragment 
coloring stages), or both. If a shader isn t present for a particular stage, the fixed-function part 
of the pipeline is used in its place. Just a with regular programs, a syntax error from the compilation 
stage, or a missing symbol from the linker stage could prevent the successful generation of an executable 
program. There are routines for verifying the results of the compilation and link stages of the compilation 
process, but are not shown here. Instead, we ve provided a routine that makes this process much simpler, 
as demonstrated on the next slide. To simplify our lives, we created a routine that simplifies loading, 
compiling, and linking shaders: InitShaders(). It implements the shader compilation and linking process 
shown on the previous slide. It also does full error checking, and will terminate your program if there 
s an error at some stage in the process (production applications might choose a less terminal solution 
to the problem, but it s useful in the classroom). InitShaders() accepts two parameters, each a filename 
to be loaded as source for the vertex and fragment shader stages, respectively. The value returned from 
InitShaders() will be a valid GLSL program id that you can pass into glUseProgram().  InitShader.cpp 
// Create a NULL-terminated string by reading the provided file #include Angel.h 5 static char* readShaderSource(const 
char* shaderFile) { FILE* fp = fopen(shaderFile, "r"); 10 if ( fp == NULL ) { return NULL; } fseek(fp, 
0L, SEEK_END); long size = ftell(fp); 15 fseek(fp, 0L, SEEK_SET); char* buf = new char[size + 1]; fread(buf, 
1, size, fp); 20 buf[size] = '\0'; fclose(fp); return buf; } 25 // Create a GLSL program object from 
vertex and fragment shader files GLuint InitShader(const char* vShaderFile, const char* fShaderFile) 
30 { struct Shader { const char* filename; GLenum type; GLchar* source; 35 } shaders[2] = { { vShaderFile, 
GL_VERTEX_SHADER, NULL }, { fShaderFile, GL_FRAGMENT_SHADER, NULL } }; 40 GLuint program = glCreateProgram(); 
for ( int i = 0; i < 2; ++i ) { Shader&#38; s = shaders[i]; s.source = readShaderSource( s.filename ); 
 45 if ( shaders[i].source == NULL ) { std::cerr << "Failed to read " << s.filename << std::endl; exit( 
EXIT_FAILURE ); } 50 GLuint shader = glCreateShader( s.type ); glShaderSource( shader, 1, (const GLchar**) 
&#38;s.source, NULL ); glCompileShader( shader ); GLint compiled; 55 glGetShaderiv( shader, GL_COMPILE_STATUS, 
&#38;compiled ); if ( !compiled ) { std::cerr << s.filename << " failed to compile:" << std::endl; GLint 
logSize; glGetShaderiv( shader, GL_INFO_LOG_LENGTH, &#38;logSize ); 60 char* logMsg = new char[logSize]; 
glGetShaderInfoLog( shader, logSize, NULL, logMsg ); std::cerr << logMsg << std::endl; delete [] logMsg; 
65 exit( EXIT_FAILURE ); } delete [] s.source; 70 glAttachShader( program, shader ); } /* link and error 
check */ glLinkProgram(program); 75 GLint linked; glGetProgramiv( program, GL_LINK_STATUS, &#38;linked 
); if ( !linked ) { std::cerr << "Shader program failed to link" << std::endl; 80 GLint logSize; glGetProgramiv( 
program, GL_INFO_LOG_LENGTH, &#38;logSize); char* logMsg = new char[logSize]; glGetProgramInfoLog( program, 
logSize, NULL, logMsg ); std::cerr << logMsg << std::endl; 85 delete [] logMsg; exit( EXIT_FAILURE ); 
} 90 /* use program object */ glUseProgram(program); return program; }  OpenGL shaders, depending on 
which stage their associated with, process different types of data. Some data for a shader changes for 
each shader invocation. For example, each time a vertex shader executes, it s presented with new data 
for a single vertex; likewise for fragment, and the other shader stages in the pipeline. The number of 
executions of a particular shader rely on how much data was associated with the draw call that started 
the pipeline if you call glDrawArrays() specifiying 100 vertices, your vertex shader will be called 
100 times, each time with a different vertex. Other data that a shader may use in processing may be constant 
across a draw call, or even all the drawing calls for a frame. GLSL calls those uniform varialbes, since 
their value is uniform across the execution of all shaders for a single draw call. Each of the shader 
s input data variables (ins and uniforms) needs to be connected to a data source in the application. 
We ve already seen glGetAttribLocation() for retrieving information for connecting vertex data in a VBO 
to shader variable. You will also use the same process for uniform variables, as we ll describe shortly. 
 Once you know the names of variables in a shader whether they re attributes or uniforms you can determine 
their location using one of the glGet*Location() calls. If you don t know the variables in a shader (if, 
for instance, you re writing a library that accepts shaders), you can find out all of the shader variables 
by using the glGetActiveAttrib() function.  You ve already seen how one associates values with attributes 
by calling glVertexAttribPointer(). To specify a uniform s value, we use one of the glUniform*() functions. 
For setting a vector type, you ll use one of the glUniform*() variants, and for matrices you ll use a 
glUniformMatrix *() form. You ll find that many OpenGL programs look very similar, particularly simple 
examples as we re showing in class. Above we demonstrate the basic initialization code for our examples. 
In our main() routine, you can see our use of the Freeglut and GLEW libraries. Here are two of our GLUT 
callbacks: display() which controls the drawing of our objects. While this is an extremely simple display() 
function, you ll find that almost all functions will have this form: 1. clear the window 2. render 
 3. swap the buffers   keyboard() which provides some simple keyboard-based user input.  Cube.cpp 
// // Display a color cube // 5 // Colors are assigned to each vertex and then the rasterizer interpolates 
// those colors across the triangles. We us an orthographic projection // as the default projetion. #include 
"Angel.h" 10 typedef Angel::vec4 color4; typedef Angel::vec4 point4; const int NumVertices = 36; //(6 
faces)(2 triangles/face)(3 15 vertices/triangle) point4 points[NumVertices]; color4 colors[NumVertices]; 
20 // Vertices of a unit cube centered at origin, sides aligned with axes point4 vertices[8] = { point4(-0.5,-0.5, 
0.5, 1.0 ), point4(-0.5, 0.5, 0.5, 1.0 ), point4( 0.5, 0.5, 0.5, 1.0 ), 25 point4( 0.5,-0.5, 0.5, 1.0 
), point4(-0.5,-0.5,-0.5, 1.0 ), point4(-0.5, 0.5,-0.5, 1.0 ), point4( 0.5, 0.5,-0.5, 1.0 ), point4( 
0.5,-0.5,-0.5, 1.0 ) 30 }; // RGBA olors color4 vertex_colors[8] = { color4( 0.0, 0.0, 0.0, 1.0 ), 
// black 35 color4( 1.0, 0.0, 0.0, 1.0 ), // red color4( 1.0, 1.0, 0.0, 1.0 ), // yellow color4( 0.0, 
1.0, 0.0, 1.0 ), // green color4( 0.0, 0.0, 1.0, 1.0 ), // blue color4( 1.0, 0.0, 1.0, 1.0 ), // magenta 
40 color4( 1.0, 1.0, 1.0, 1.0 ), // white color4( 0.0, 1.0, 1.0, 1.0 ) // cyan }; //--------------------------------------------------------------------------45 
- // quad generates two triangles for each face and assigns colors // to the vertices int Index = 0; 
 50 void quad( int a, int b, int c, int d ) { colors[Index] = vertex_colors[a]; points[Index] = vertices[a]; 
Index++; colors[Index] = vertex_colors[b]; points[Index] = vertices[b]; Index++; 55 colors[Index] = vertex_colors[c]; 
points[Index] = vertices[c]; Index++; colors[Index] = vertex_colors[a]; points[Index] = vertices[a]; 
Index++; colors[Index] = vertex_colors[c]; points[Index] = vertices[c]; Index++; colors[Index] = vertex_colors[d]; 
points[Index] = vertices[d]; Index++; } 60 //--------------------------------------------------------------------------- 
// generate 12 triangles: 36 vertices and 36 colors 65 void colorcube() { quad( 1, 0, 3, 2 ); quad( 2, 
3, 7, 6 ); 70 quad( 3, 0, 4, 7 ); quad( 6, 5, 1, 2 ); quad( 4, 5, 6, 7 ); quad( 5, 4, 0, 1 ); } 75 //--------------------------------------------------------------------------- 
// OpenGL initialization 80 void init() { colorcube(); 85 // Create a vertex array object GLuint vao; 
glGenVertexArrays( 1, &#38;vao ); glBindVertexArray( vao ); 90 // Create and initialize a buffer object 
GLuint buffer; glGenBuffers( 1, &#38;buffer ); glBindBuffer( GL_ARRAY_BUFFER, buffer ); glBufferData( 
GL_ARRAY_BUFFER, sizeof(points) + sizeof(colors), 95 NULL, GL_STATIC_DRAW ); glBufferSubData( GL_ARRAY_BUFFER, 
0, sizeof(points), points ); glBufferSubData( GL_ARRAY_BUFFER, sizeof(points), sizeof(colors), colors 
); 100 // Load shaders and use the resulting shader program GLuint program = InitShader( "vshader31.glsl", 
"fshader31.glsl" ); glUseProgram( program ); // set up vertex arrays 105 GLuint vPosition = glGetAttribLocation( 
program, "vPosition" ); glEnableVertexAttribArray( vPosition ); glVertexAttribPointer( vPosition, 4, 
GL_FLOAT, GL_FALSE, 0, BUFFER_OFFSET(0) ); 110 GLuint vColor = glGetAttribLocation( program, "vColor" 
); glEnableVertexAttribArray( vColor ); glVertexAttribPointer( vColor, 4, GL_FLOAT, GL_FALSE, 0, BUFFER_OFFSET(sizeof(points)) 
); 115 glEnable( GL_DEPTH_TEST ); glClearColor( 1.0, 1.0, 1.0, 1.0 ); } //--------------------------------------------------------------------------120 
- void display( void ) { 125 glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ); glDrawArrays( GL_TRIANGLES, 
0, NumVertices ); glutSwapBuffers(); } 130 //--------------------------------------------------------------------------- 
void keyboard( unsigned char key, int x, int y ) 135 { switch( key ) { case 033: // Escape key case 'q': 
case 'Q': exit( EXIT_SUCCESS ); 140 break; } } //--------------------------------------------------------------------------145 
- int main( int argc, char **argv ) { 150 glutInit( &#38;argc, argv ); glutInitDisplayMode( GLUT_RGBA 
| GLUT_DOUBLE | GLUT_DEPTH ); glutInitWindowSize( 512, 512 ); glutInitContextVersion( 3, 2 ); glutInitContextProfile( 
GLUT_CORE_PROFILE ); 155 glutCreateWindow( "Color Cube" ); glewInit(); init(); 160 glutDisplayFunc( display 
); glutKeyboardFunc( keyboard ); glutMainLoop(); 165 return 0; } We begin delving into shader specifics 
by first taking a look at vertex shaders. As you ve probably arrived at, vertex shaders are used to process 
vertices, and have the required responsibility of specifying the vertex s position in clip coordinates. 
This process usually involves numerous vertex transformations, which we ll discuss next. Additionally, 
a vertex shader may be responsible for determine additional information about a vertex for use by the 
rasterizer, including specifying colors. To begin our discussion of vertex transformations, we ll first 
describe the synthetic camera model. This model has become know as the synthetic camera model. Note 
that both the objects to be viewed and the camera are three-dimensional while the resulting image is 
two dimensional.  The processing required for converting a vertex from 3D space into a 2D window coordinate 
is done by the transform stage of the graphics pipeline. The operations in that stage are illustrated 
above. The purple boxes represent a matrix multiplication operation. In graphics, all of our matrices 
are 44 matrices (they re homogenous, hence the reason for homogenous coordinates). When we want to draw 
an geometric object, like a chair for instance, we first determine all of the vertices that we want to 
associate with the chair. Next, we determine how those vertices should be grouped to form geometric primitives, 
and the order we re going to send them to the graphics subsystem. This process is called modeling. Quite 
often, we ll model an object in its own little 3D coordinate system. When we want to add that object 
into the scene we re developing, we need to determine its world coordinates. We do this by specifying 
a modeling transformation, which tells the system how to move from one coordinate system to another. 
Modeling transformations, in combination with viewing transforms, which dictate where the viewing frustum 
is in world coordinates, are the first transformation that a vertex goes through. Next, the projection 
transform is applied which maps the vertex into another space called clip coordinates, which is where 
clipping occurs. After clipping, we divide by the w value of the vertex, which is modified by projection. 
This division operation is what allows the farther-objects-being-smaller activity. The transformed, clipped 
coordinates are then mapped into the window. Note that human vision and a camera lens have cone-shaped 
viewing volumes. OpenGL (and almost all computer graphics APIs) describe a pyramid-shaped viewing volume. 
Therefore, the computer will see differently from the natural viewpoints, especially along the edges 
of viewing volumes. This is particularly pronounced for wide-angle fish-eye camera lenses. By using 
44 matrices, OpenGL can represent all geometric transformations using one matrix format. Perspective 
projections and translations require the 4th row and column. Otherwise, these operations would require 
an vector-addition operation, in addition to the matrix multiplication. While OpenGL specifies matrices 
in column-major order, this is often confusing for C programmers who are used to row-major ordering for 
two-dimensional arrays. OpenGL provides routines for loading both column-and row-major matrices. However, 
for standard OpenGL transformations, there are functions that automatically generate the matrices for 
you, so you don t generally need to be concerned about this until you start doing more advanced operations. 
For operations other than perspective projection, the fourth row is always (0, 0, 0, 1) which leaves 
the w-coordinate unchanged.. Another essential part of the graphics processing is setting up how much 
of the world we can see.We construct a viewing frustum, which defines the chunk of 3-space that we can 
see. There are two types of views: a perspective view, which you re familiar with as it s how your eye 
works, is used to generate frames that match your view of reality things farther from your appear smaller. 
This is the type of view used for video games, simulations, and most graphics applications in general. 
The other view, orthographic, is used principally for engineering and design situations, where relative 
lengths and angles need to be preserved. For a perspective, we locate the eye at the apex of the frustum 
pyramid. We can see any objects which are between the two planes perpendicular to eye (they re called 
the near and far clippingplanes, respectively). Any vertices between near and far, and inside the four 
planes that connect them will be rendered. Otherwise, those vertices are clipped out and discarded. In 
some cases a primitive will be entirely outside of the view, and the system will discard it for that 
frame. Other primitives might intersect the frustum, which we clip such that the part of them that s 
outside is discarded and we create new vertices for the modified primitive. While the system can easily 
determine which primitive are inside the frustum, it s wasteful of system bandwidth to have lots of primitives 
discarded in this manner. We utilize a technique named cullingto determine exactly which primitives need 
to be sent to the graphics processor, and send only thoseprimitives to maximize its efficiency. In OpenGL, 
the default viewing frusta are always configured in the same manner, which defines the orientation of 
our clip coordinates. Specifically, clip coordinates are defined with the eye located at the origin, 
looking down the z axis. From there, we define two distances: our near and far clip distances, which 
specify the location of our near and far clipping planes. The viewing volume is then completely by specifying 
the positions of the enclosing planes that are parallel to the view direction . The images above show 
the two types of projection transformations that are commonly used in computer graphics. The orthographic 
view preserves angles, and simulates having the viewer at an infinite distance from the scene. This mode 
is commonly used in used in engineering and design where it s important to preserve the sizes and angles 
of objects in relation to each other. Alternatively, the perspective view mimics the operation of the 
eye with objects seeming to shrink in size the farther from the viewer they are. The each projection, 
the matrix that you would need to specify is provided. In those matrices, the six values for the positions 
of the left, right, bottom, top, near and far clipping planes are specified by the first letter of the 
plane s name. The only limitations on the values is for perspective projections, where the near and far 
values must be positive and non-zero, with near greater than far. LookAt() generates a viewing matrix 
based on several points. LookAt() provides natrual semantics for modeling flight application, but care 
must be taken to avoid degenerate numerical situations, where the generated viewing matrix is undefined. 
 An alternative is to specify a sequence of rotations and translations that are concatenated with an 
initial identity matrix. Note: that the name modelview matrix is appropriate since moving objects in 
the model front of the camera is equivalent to moving the camera to view a set of objects.  Using the 
values passed into the LookAt() call, the above matrix generates the corresponding viewing matrix. Here 
we show the construction of a translation matrix. Translations really move coordinate systems, and not 
individual objects. Here we show the construction of a scale matrix, which is used to change the shape 
of space, but not move it (or more precisely, the origin). The above illustration has a translation to 
show how space was modified, but a simple scale matrix will not include such a translation. Here we 
show the effects of a rotation matrix on space. Once again, a translation has been applied in the image 
to make it easier to see the rotation s affect. The formula for generating a rotation matrix is a bit 
more complex that for scales and translations. Naming the axis of rotation v, we begin by normalizing 
v and storing the result in the vector u. From there, we create a 3  3 matrix M, which is composed of 
the sum of three terms. 1. The outer product of the vector u with its transpose ut 2. The difference 
of the identity matrix, I, with u s outer product, scaled the by the cosine of the input angle . 3. 
Finally, we scale the matrix S which is composed of the elements of the rotation matrix. The complete 
rotation matrix is formed by composing M as the upper 3  3 matrix in R.  Here s an example vertex 
shader for rotating our cube. We generate the matrices in the shader (as compared to in the application), 
based on the input angle theta. It s useful to note that we can vectorize numerous computations. For 
example, we can generate a vectors of sines and cosines for the input angle, which we ll use in further 
computations. Completing our shader, we compose two of three rotation matrices (one around each axis). 
In generating our matrices, we use one of the many matrix constructor functions (in this case, specifying 
the 16 individual elements). It s important to note in this case, that our matrices are column-major, 
so we need to take care in the placement of the values in the constructor. We complete our shader here 
by generating the last rotation matrix, and ) and then use the composition of those matrices to transform 
the input vertex position. We also pass-thru the color values by assigning the input color to an output 
variable. Finally, we merely need to supply the angle values into our shader through our uniform plumbing. 
In this case, we track each of the axes rotation angle, and store them in a vec3 that matches the angle 
declaration in the shader. We also keep track of the uniform s location so we can easily update its value. 
 Lighting is an important technique in computer graphics. Without lighting, objects tend to look like 
they are made out of plastic. OpenGL divides lighting into three parts: material properties, light properties 
and global lighting parameters. Lighting is available in both RGBA mode and color index mode. RGBA is 
more flexible and less restrictive than color index mode lighting.  OpenGL can use the shade at one 
vertex to shade an entire polygon (constant shading) or interpolated the shades at the vertices across 
the polygon (smooth shading), the default. The orientation of a surface is specified by the normal at 
each point. For a flat polygon the normal is constant over the polygon. Because normals are specified 
by the application program and can be changed between the specification of vertices, when we shade a 
polygon it can appear to be curved. OpenGL lighting is based on the Phong lighting model. At each vertex 
in the primitive, a color is computed using that primitives material properties along with the light 
settings. The color for the vertex is computed by adding four computed colors for the final vertex color. 
The four contributors to the vertex color are: Ambient is color of the object from all the undirected 
light in a scene.  Diffuse is the base color of the object under current lighting. There must be a light 
shining on the object to get a diffuse contribution.  Specular is the contribution of the shiny highlights 
on the object.  Emission is the contribution added in if the object emits light (i.e., glows)  The 
lighting normal tells OpenGL how the object reflects light around a vertex. If you imagine that there 
is a small mirror at the vertex, the lighting normal describes how the mirror is oriented, and consequently 
how light is reflected. Material properties describe the color and surface properties of a material 
(dull, shiny, etc). The properties described above are components of the Phong lighting model, a simple 
model that yields reasonable results with little computation. Each of the material components would be 
passed into a vertex shader, for example, to be used in the lighting computation along with the vertex 
s position and lighting normal. Here we declare numerous variables that we ll use in computing a color 
using a simple lighting model. All of the uniform values are passed in from the application and describe 
the material and light properties being rendered. In the initial parts of our shader, we generate numerous 
vector quantities to be used in our lighting computation. pos represents the vertex s position in eye 
coordinates  L represents the vector from the vertex to the light  E represents the eye vector, which 
is the vector from the vertex s eye-space position to the origin  H is the half vector which is the 
normalized vector half-way between the light and eye vectors  N is the transformed vertex normal Note 
that all of these quantities are vec3 s, since we re dealing with vectors, as compared to  homogenous 
coordinates. When we need to convert form a homogenous coordinate to a vector, we use a vector swizzle 
to extract the components we need. Here we complete our lighting computation. The Phong model, which 
this shader is based on, uses various material properties as we described before. Likewise, each light 
can contribute to those same properties. The combination of the material and light properties are represented 
as our product variables in this shader. The products are merely the component-wise products of the light 
and objects same material propreties. These values are computed in the application and passed into the 
shader. In the Phong model, each material product is attenuated by the magnitude of the various vector 
products. Starting with the most influential component of lighting, the diffuse color, we use the dot 
product of the lighting normal and light vector, clamping the value if the dot product is negative (which 
physically means the light s behind the object). We continue by computing the specular component, which 
is computed as the dot product of the normal and the half-vector raised to the shininess value. Finally, 
if the light is behind the object, we correct the specular contribution. Finally, we compose the final 
vertex color as the sum of the computed ambient, diffuse, and specular colors, and update the transformed 
vertex position.  The final shading stage that OpenGL supports is fragment shading which allows an application 
perpixel-location control over the color that may be written to that location. Fragments, which are 
on their way to the framebuffer, but still need to do some pass some additional processing to become 
pixels. However, the computational power available in shading fragments is a great asset to generating 
images. In a fragment shader, you can compute lighting values similar to what we just discussed in vertex 
shading per fragment, which gives much better results, or add bump mapping, which provides the illusion 
of greater surface detail. Likewise, we ll apply texture maps, which allow us to increase the detail 
for our models without increasing the geometric complexity. As an example of what we can do in a fragment 
shader, consider using our lighting model, but for every pixel, as compared to at the vertex level. Doing 
fragment lighting provides much better visual result, but using almost identical shader code (except 
you need to move it from your vertex shader into your fragment shader). The only trick required is that 
we need to have the rasterizer provide us updated normal values for each fragment. However, that s just 
like iterating a color, so there s almost nothing to it. Here we show an example of simple fragment 
shading that yields a result similar to the shading you might find in an animated cartoon. Textures 
are images that can be thought of as continuous and be one, two, three, or four dimensional. By convention, 
the coordinates of the image are s, t, r and q. Thus for the two dimensional image above, a point in 
the image is given by its (s, t) values with (0, 0) in the lower-left corner and (1, 1) in the top-right 
corner. A texture map for a two-dimensional geometric object in (x, y, z) world coordinates maps a point 
in (s, t) space to a corresponding point on the screen. The advantage of texture mapping is that visual 
detail is in the image, not in the geometry. Thus, the complexity of an image does not affect the geometric 
pipeline (transformations, clipping) in OpenGL. Texture is added during rasterization where the geometric 
and pixel pipelines meet. Above we show a simple example of mapping the OpenGL logo (stored as a texture) 
onto a rectangular polygon. Textures can be any size (up to an implementation maximum size), and aspect 
ratio. A major point to realize is that an image file is different than a texture. OpenGL has no capabilities 
for reading or writing image files that s something left to external libraries. The only data that OpenGL 
requires from an image file is the image s width, height, number of color components, and the pixel data. 
 In the simplest approach, we must perform these three steps. Textures reside in texture memory. When 
we assign an image to a texture it is copied from processor memory to texture memory where pixels are 
formatted differently. Texture coordinates are actually part of the state as are other vertex attributes 
such as color and normals. As with colors, OpenGL interpolates texture inside geometric objects. Because 
textures are really discrete and of limited extent, texture mapping is subject to aliasing errors that 
can be controlled through filtering. Texture memory is a limited resource and having only a single active 
texture can lead to inefficient code. The first step in creating texture objects is to have OpenGL reserve 
some indices for your objects. glGenTextures() will request n texture ids and return those values back 
to you in texIds. To begin defining a texture object, you call glBindTexture() with the id of the object 
you want to create. The target is one of GL_TEXTURE_{123}D(). All texturing calls become part of the 
object until the next glBindTexture() is called. To have OpenGL use a particular texture object, call 
glBindTexture() with the target and id of the object you want to be active. To delete texture objects, 
use glDeleteTextures( n, *texIds ), where texIds is an array of texture object identifiers to be deleted. 
 After creating a texture object, you ll need to bind to it to initialize or use the texture stored 
in the object. This operation is very similar to what you ve seen when working with VAOs and VBOs. Specifying 
the texels for a texture is done using the glTexImage{123}D() call. This will transfer the texels in 
CPU memory to OpenGL, where they will be processed and converted into an internal format. The level parameter 
is used for defining how OpenGL should use this image when mapping texels to pixels. Generally, you ll 
set the level to 0, unless you are using a texturing technique called mipmapping, which we will discuss 
in the next section. When you want to map a texture onto a geometric primitive, you need to provide 
texture coordinates. Valid texture coordinates are between 0 and 1, for each texture dimension, and usually 
manifest in shaders as vertex attributes. We ll see how to deal with texture coordinates outside the 
range [0, 1] in a moment. The general steps to enable texturing are listed above. Some steps are optional, 
and due to the number of combinations, complete coverage of the topic is outside the scope of this course. 
 Here we use the texture object approach. Using texture objects may enable your OpenGL implementation 
to make some optimizations behind the scenes. As with any other OpenGL state, texture mapping requires 
that glEnable() be called. The tokens for texturing are: GL_TEXTURE_1D -one dimensional texturing GL_TEXTURE_2D 
-two dimensional texturing GL_TEXTURE_3D -three dimensional texturing 2D texturing is the most commonly 
used. 1D texturing is useful for applying contours to objects ( like altitude contours to mountains ). 
3D texturing is useful for volume rendering. Just like vertex attributes were associated with data in 
the application, so too with textures. In particular, you access a texture defined in your application 
using a texture sampler in your shader. The type of the sampler needs to match the type of the associated 
texture. For example, you would use a sampler2D to work with a two-dimensional texture created with glTexImage2D( 
GL_TEXTURE_2D, ); Within the shader, you use the texture() function to retrieve data values from the 
texture associated with your sampler. To the texture() function, you pass the sampler as well as the 
texture coordinates where you want to pull the data from. Note: the overloaded texture() method was added 
into GLSL version 3.30. Prior to that release, there were special texture functions for each type of 
texture sampler (e.g., there was a texture2D() call for use with the sampler2D). Similar to our first 
cube example, if we want to texture our cube, we need to provide texture coordinates for use in our shaders. 
Following our previous example, we merely add an additional vertex attribute that contains our texture 
coordinates. We do this for each of our vertices. We will also need to update VBOs and shaders to take 
this new attribute into account. The code snippet above demonstrates procedurally generating a two 64 
 64 texture maps. The above OpenGL commands completely specify a texture object. The code creates a 
texture id by calling glGenTextures(). It then binds the texture using glBindTexture() to open the object 
for use, and loading in the texture by calling glTexImage2D(). After that, numerous sampler characteristics 
are set, including the texture wrap modes, and texel filtering. In order to apply textures to our geometry, 
we need to modify both the vertex shader and the pixel shader. Above, we add some simple logic to pass-thru 
the texture coordinates from an attribute into data for the rasterizer. Continuing to update our shaders, 
we add some simple code to modify our fragment shader to include sampling a texture. How the texture 
is sampled (e.g., coordinate wrap modes, texel filtering, etc.) is configured in the application using 
the glTexParameter*() call.  We ll now analyze a few case studies from different applications. The 
first simple application we ll look at is rendering height fields, as you might do when rendering terrain 
in an outdoor game or flight simulator. We d first like to render a wire-frame version of our mesh, 
which we ll draw a individual line loops. To begin, we build our data set by sampling the function f 
for a particular time across the domain of points. From there, we build our array of points to render. 
Once we have our data and have loaded into our VBOs we render it by drawing the individual wireframe 
quadrilaterals. There are many ways to render a wireframe surface like this give some thought of other 
methods.  Here s a rendering of the mesh we just generated. While the wireframe version is of some 
interest, we can create better looking meshes by adding a few more effects. We ll begin by creating a 
solid mesh by converting each wireframe quadrilateral into a solid quad composed of two separate triangles. 
Turns out with our pervious set of points, we can merely changed our glDrawArrays() call or more specifically, 
the geometric primitive type to render a solid surface. However, if we don t do some additional modification 
of one of our shaders, we ll get a large back blob. To produce a more useful rendering, we ll add lighting 
computations into our vertex shader, computing a lighting color for each vertex, which will be passed 
to the fragment shader. Details of lighting model are not important to here. The model includes the 
standard modified Phong diffuse and specular terms without distance. Note that we do the lighting in 
eye coordinates and therefore must compute the eye position in this frame. All the light and material 
properties are set in the application and are available through the OpenGL state.  Here s a rendering 
of our shaded, solid mesh. In this example, we use some of the standard diffuse computation to find 
the cosine of the angle between the light vector and the normal vector. Its value determines whether 
we color with red or yellow. A silhouette edge is computed as in the previous example. Without the silhouette 
edge. The idea is that if the angle between the eye vector and the surface normal is small, we are near 
an edge. But we can even better if we use a fragment shader  The rasterizer interpolates both the 
texture coordinates and reflection vector to get the respective values for the fragment shader. Note 
that all the texture definitions and parameters are in the application program.  Details are a little 
complex Need lighting model Usually do computations in a local frame that changes for each fragment Put 
code in an appendix Single rectangle with moving light source. Bump map is derived from a texture map 
with which is a step function.  OpenGL version 4.0 added two new shader stages that operate cooperatively 
called tessellation shaders. Tessellation uses a patch-based mechanism to generate a connected mesh within 
the OpenGL pipeline. When you want to render using a tessellation patch, you specify a new primitive 
type called GL_PATCHES, which is merely an ordered list of vertices. OpenGL doesn t really know what 
you want to do with the vertices in a patch that s entirely up to you to implement in the tessellation 
shader stages. This flexibility, however, allows for the use of many different patch algorithms (like 
Bezier patches, for example). The first tessellation shader stage is called the tessellation control 
shader, which has two main responsibilities: generating the output patch vertices, and specifying the 
tessellation levels. The other tessellation shader stage is the tessellation evaluation shader, which 
specifies the final (unless geometry shading is employed) vertex position. In many cases, this is done 
by combining the output patch vertices generated by the tessellation control shader, and using generated 
tessellation coordinates. Here s a schematic of the data flow when using tessellation shading.   
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037644</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>84</pages>
		<display_no>8</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Liquid simulation with mesh-based surface tracking]]></title>
		<page_from>1</page_from>
		<page_to>84</page_to>
		<doi_number>10.1145/2037636.2037644</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037644</url>
		<abstract>
			<par><![CDATA[<p>Animating detailed liquid surfaces has always been a challenge for computer graphics researchers and visual effects artists. Over the past few years, researchers in this field have focused on mesh-based surface tracking to synthesize extremely detailed liquid surfaces as efficiently as possible. This course provides a solid understanding of the steps required to create a fluid simulator with a mesh-based liquid surface.</p> <p>The course begins with an overview of several existing liquid-surface-tracking techniques and the pros and cons of each method. Then it explains how to embed a triangle mesh into a finite-difference-based fluid simulator and describes several methods for allowing the liquid surface to merge together or break apart. The final section showcases the benefits and further applications of a mesh-based liquid surface, highlighting state-of-the-art methods for tracking colors and textures, maintaining liquid volume, preserving small surface features, and simulating realistic surface-tension waves.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Documentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808993</person_id>
				<author_profile_id><![CDATA[81323497785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wojtan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Science and Technology Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828563</person_id>
				<author_profile_id><![CDATA[81319498126]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller-Fischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2255275</person_id>
				<author_profile_id><![CDATA[81447602530]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tyson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brochu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276437</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bart Adams, Mark Pauly, Richard Keiser, and Leonidas J. Guibas. Adaptively sampled particle fluids. <i>ACM Trans. Graph</i>., 26, July 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. B. Barber and H. Huhdanpaa. Qhull Software Package, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122503</ref_obj_id>
				<ref_obj_pid>1122501</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Adam W. Bargteil, Tolga G. Goktekin, James F. O'Brien, and John A. Strain. A semi-Lagrangian contouring method for fluid simulation. <i>ACM Trans. Graph</i>., 25(1):19--38, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218111</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Adam W. Bargteil, Funshing Sin, Jonathan E. Michaels, Tolga G. Goktekin, and James F. O'Brien. A texture synthesis method for liquid animations. In <i>Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, Sept 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Christopher Batty, Stefan Xenos, and Ben Houston. Tetrahedral embedded boundary methods for accurate and flexible adaptive fluids. In <i>Proceedings of Eurographics</i>, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1735606</ref_obj_id>
				<ref_obj_pid>1735603</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Bernstein and D. Fussell. Fast, exact, linear booleans. In <i>Proceedings of the Symposium on Geometry Processing</i>, pages 1269--1278. Eurographics Association, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357310</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. A generalization of algebraic surface drawing. <i>ACM Trans. Graph</i>., 1:235--256, July 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1457699</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Robert Bridson. <i>Fluid Simulation for Computer Graphics</i>. A K Peters, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566623</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Robert Bridson, Ronald Fedkiw, and John Anderson. Robust treatment of collisions, contact and friction for cloth animation. <i>ACM Trans. Graph</i>., 21(3):594--603, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Tyson Brochu. Fluid animation with explicit surface meshes and boundary-only dynamics. Master's thesis, University of British Columbia, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778784</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Tyson Brochu, Christopher Batty, and Robert Bridson. Matching fluid simulation elements to surface geometry and topology. <i>ACM Trans. Graph</i>., 29(4):1--9, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1654951</ref_obj_id>
				<ref_obj_pid>1654948</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Tyson Brochu and Robert Bridson. Robust topological operations for dynamic explicit surfaces. <i>SIAM Journal on Scientific Computing</i>, 31(4):2472--2493, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M. Campen and L. Kobbelt. Exact and robust (self-) intersections for polygonal meshes. In <i>Computer Graphics Forum</i>, volume 29, pages 397--406. John Wiley & Sons, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272720</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Nuttapong Chentanez, Bryan E. Feldman, Fran&#231;ois Labelle, James F. O'Brien, and Jonathan R. Shewchuk. Liquid simulation on lattice-based tetrahedral meshes. In <i>SCA '07: Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 219--228, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1145985</ref_obj_id>
				<ref_obj_pid>1145976</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Du, B. Fix, J. Glimm, X. Jia, X. Li, Y. Li, and L. Wu. A simple package for front tracking. <i>Journal of Computational Physics</i>, 213(2):613--628, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[N. Dyn, D. Levine, and J. A. Gregory. A butterfly subdivision scheme for surface interpolation with tension control. <i>ACM transactions on Graphics (TOG)</i>, 9(2):160--169, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1668555</ref_obj_id>
				<ref_obj_pid>1668459</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D. Enright, F. Losasso, and R. Fedkiw. A fast and accurate semi-Lagrangian particle level set method. <i>Computers and Structures</i>, 83(6--7):479--490, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[D. Enright, D. Nguyen, F. Gibou, and R. Fedkiw. Using the particle level set method and a second order accurate pressure boundary condition for free surface flows. In <i>Proc. 4th ASME-JSME Joint Fluids Eng. Conf., number FEDSM2003--45144. ASME</i>. Citeseer, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641285</ref_obj_id>
				<ref_obj_pid>641282</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Douglas Enright, Ronald Fedkiw, Joel Ferziger, and Ian Mitchell. A hybrid particle level set method for improved interface capturing. <i>J. Comput. Phys</i>., 183(1):83--116, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566645</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Douglas P. Enright, Stephen R. Marschner, and Ronald P. Fedkiw. Animation and rendering of complex water surfaces. In <i>Proceedings of ACM SIGGRAPH 2002</i>, volume 21, pages 736--744, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Ronald Fedkiw, Jos Stam, and Henrik Wann Jensen. Visual simulation of smoke. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 15--22, New York, NY, USA, 2001. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Nick Foster and Ronald Fedkiw. Practical animation of liquids. In <i>SIGGRAPH '01</i>, pages 23--30, New York, NY, USA, 2001. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Garland and P. S. Heckbert. Surface simplification using quadric error metrics. In <i>Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, pages 209--216. ACM Press/Addison-Wesley Publishing Co., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614487</ref_obj_id>
				<ref_obj_pid>614282</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[A. Gu&#233;ziec, G. Taubin, F. Lazarus, and B. Horn. Cutting and stitching: Converting sets of polygons to manifold surfaces. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 7(2):136--151, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360622</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[D. Harmon, E. Vouga, R. Tamstorf, and E. Grinspun. Robust treatment of simultaneous collisions. <i>ACM Trans. Graph. (Proc. SIGGRAPH)</i>, 27(3):1--4, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866198</ref_obj_id>
				<ref_obj_pid>1882262</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Nambin Heo and Hyeong-Seok Ko. Detail-preserving fully-eulerian interface tracking framework. In <i>ACM SIGGRAPH Asia 2010 papers</i>, SIGGRAPH ASIA '10, pages 176:1--176:8, New York, NY, USA, 2010. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Jeong-Mo Hong and Chang-Hun Kim. Animation of bubbles in liquid. <i>Comput. Graph. Forum</i>, 22(3):253--262, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073283</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Jeong-Mo Hong and Chang-Hun Kim. Discontinuous fluids. <i>ACM Trans. Graph</i>., 24(3):915--920, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1240019</ref_obj_id>
				<ref_obj_pid>1240005</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Jeong-Mo Hong, Tamar Shinar, Myungjoo Kang, and Ronald Fedkiw. On boundary condition capturing for multiphase interfaces. <i>J. Sci. Comput</i>., 31(1-2):99--125, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965561</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[B. Houston, C. Bond, and M. Wiebe. A unified approach for modeling complex occlusions in fluid simulations. In <i>ACM SIGGRAPH 2003 Sketches & Applications</i>, pages 1--1. ACM, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[X. Jiao, A. Colombi, X. Ni, and J. Hart. Anisotropic mesh adaptation for evolving triangulated surfaces. In <i>Proceedings of the 15th international meshing roundtable</i>, pages 173--190. Springer, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1223706</ref_obj_id>
				<ref_obj_pid>1223679</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Xiangmin Jiao. Face offsetting: A unified approach for explicit moving interfaces. J. <i>Comput. Phys</i>., 220(2):612--625, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276500</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Byungmoon Kim, Yingjie Liu, Ignacio Llamas, Xiangmin Jiao, and Jarek Rossignac. Simulation of bubbles in foam with the volume control method. <i>ACM Trans. Graph</i>., 26(3):98, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187767</ref_obj_id>
				<ref_obj_pid>1187626</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[ByungMoon Kim, Yingjie Liu, Ignacio Llamas, and Jarek Rossignac. Advections with significantly reduced dissipation and diffusion. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 13(1):135--144, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Doyub Kim, Oh young Song, and Hyeong-Seok Ko. A semi-lagrangian cip fluid solver without dimensional splitting. <i>Computer Graphics Forum (Proc. Eurographics</i>), 27:467--475, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231915</ref_obj_id>
				<ref_obj_pid>2231878</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Vivek Kwatra, David Adalsteinsson, Theodore Kim, Nipun Kwatra, Mark Carlson, and Ming Lin. Texturing fluids. <i>IEEE Trans. Visualization and Computer Graphics</i>, 13(5):939--952, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[J. O. Lachaud and B. Taton. Deformable model with adaptive mesh and automated topology changes. In <i>Proc. 4th Int. Conference on 3D Digital Imaging and Modeling, Banff, Canada, IEEE</i>, pages 12--19, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. In <i>SIGGRAPH '87</i>, pages 163--169, New York, NY, USA, 1987. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015745</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Frank Losasso, Frederic Gibou, and Ron Fedkiw. Simulating water and smoke with an octree data structure. In <i>Proceedings of ACM SIGGRAPH 2004</i>, pages 457--462. ACM Press, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141960</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Frank Losasso, Tamar Shinar, Andrew Selle, and Ronald Fedkiw. Multiple interacting liquids. <i>ACM Trans. Graph</i>., 25(3):812--819, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[S. McKee, M. F. Tome, V. G. Ferreira, J. A. Cuminato, A. Castelo, Sousa F. S., and N. Mangiavacchi. The mac method. <i>Computers & Fluids</i>, 37(8):907--930, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[G. H. Meisters. Polygons have ears. <i>American Mathematical Monthly</i>, pages 648--651, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1632595</ref_obj_id>
				<ref_obj_pid>1632592</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Jeroen Molemaker, Jonathan M. Cohen, Sanjit Patel, and Jonyong Noh. Low viscosity flow simulations for animation. In <i>SCA '08: Proceedings of the 2008</i> ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pages 9--18, Aire-la-Ville, Switzerland, Switzerland, 2008. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276459</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Patrick Mullen, Alexander McKenzie, Yiying Tong, and Mathieu Desbrun. A variational approach to eulerian geometry processing. <i>ACM Trans. Graph</i>., 26(3):66, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1599501</ref_obj_id>
				<ref_obj_pid>1599470</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[M. M&#252;ller. Fast and robust tracking of fluid surfaces. In Proceedings of the 2009 ACM <i>SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pages 237--245. ACM, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006087</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[M. M&#252;ller and M. Gross. Interactive virtual materials. In <i>the Proccedings of Graphics Interface</i>, pages 239--246, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846298</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Matthias M&#252;ller, David Charypar, and Markus Gross. Particle-based fluid simulation for interactive applications. <i>Proc. of the ACM Siggraph/Eurographics Symposium on Computer Animation</i>, pages 154--159, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Stanley Osher and Ronald Fedkiw. <i>The Level Set Method and Dynamic Implicit Surfaces</i>. Springer-Verlag, New York, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Stanley Osher and James Sethian. Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations. <i>Journal of Computational Physics</i>, 79:12--49, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Darko Pavi&#263;, Marcel Campen, and Leif Kobbelt. Hybrid booleans. <i>Computer Graphics Forum</i>, 29(1):75--87, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>775683</ref_obj_id>
				<ref_obj_pid>775673</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Blair Perot and Ramesh Nallapati. A moving unstructured staggered mesh method for the simulation of incompressible free-surface flows. <i>J. Comput. Phys</i>., 184(1):192--214, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. <i>Numerical Recipes in C</i>. Cambridge University Press, second edition, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[X. Provot. Collision and self-collision handling in cloth model dedicated to design garment. <i>Graphics Interface</i>, pages 177--89, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>254125</ref_obj_id>
				<ref_obj_pid>254115</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Elbridge Gerry Puckett, Ann S. Almgren, John B. Bell, Daniel L. Marcus, and William J. Rider. A high-order projection method for tracking fluid interfaces in variable density incompressible flows. <i>J. Comput. Phys</i>., 130:269--282, January 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401764</ref_obj_id>
				<ref_obj_pid>1401696</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Andrew Selle, Ronald Fedkiw, Byungmoon Kim, Yingjie Liu, and Jarek Rossignac. An unconditionally stable maccormack method. <i>J. Sci. Comput</i>., 35(2-3):350--371, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[James Albert Sethian. <i>Level Set Methods and Fast Marching Methods</i>. Cambridge Monograph on Applies and Computational Mathematics. Cambridge University Press, Cambridge, U. K., <i>2</i>
&#60;sup&#62;
<i>nd</i>
&#60;/sup&#62; edition, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Jonathan R. Shewchuk. What is a good linear element? interpolation, conditioning, and quality measures. In 11&#60;sup&#62;
<i>th</i>
&#60;/sup&#62; <i>Int. Meshing Roundtable</i>, pages 115--126, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1228161</ref_obj_id>
				<ref_obj_pid>1227874</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Seungwon Shin. Computation of the curvature field in numerical simulation of multiphase flow. <i>J. Comput. Phys</i>., 222(2):872--878, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Hang Si. <i>TetGen: A Quality Tetrahedral Mesh Generator and Three-Dimensional Delaunay Triangulator</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1599502</ref_obj_id>
				<ref_obj_pid>1599470</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Funshing Sin, Adam W. Bargteil, and Jessica K. Hodgins. A point-based method for animating incompressible flow. In <i>Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, SCA '09, pages 247--255, New York, NY, USA, 2009. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Stable fluids. In <i>SIGGRAPH '99</i>, pages 121--128, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1654950</ref_obj_id>
				<ref_obj_pid>1654948</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Mark Sussman and Mitsuhiro Ohta. A stable and efficient method for treating surface tension in incompressible two-phase flow. <i>SIAM Journal on Scientific Computing</i>, 31(4):2447--2471, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[M. Teschner, B. Heidelberger, M. M&#252;ller, D. Pomeranets, and M. Gross. Optimized spatial hashing for collision detection of deformable objects. In <i>Proceedings of Vision, Modeling, Visualization VMV03</i>, pages 47--54, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778785</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Nils Th&#252;rey, Chris Wojtan, Markus Gross, and Greg Turk. A multiscale approach to mesh-based surface tension flows. <i>ACM Trans. Graph</i>., 29(4):1--10, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1978433</ref_obj_id>
				<ref_obj_pid>1978252</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[C. C. L. Wang. Approximate Boolean Operations on Large Polyhedral Solids with Partial Mesh Reconstruction. <i>IEEE transactions on visualization and computer graphics</i>, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Brent Williams. Fluid surface reconstruction from particles. Master's thesis, University of Waterloo, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2126210</ref_obj_id>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Chris Wojtan. <i>Animating Physical Phenomena with Embedded Surface Meshes</i>. PhD thesis, Georgia Institute of Technology, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2126210</ref_obj_id>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Chris Wojtan. <i>Animating physical phenomena with embedded surface meshes</i>. PhD thesis, Georgia Institute of Technology, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531382</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Chris Wojtan, Nils Th&#252;rey, Markus Gross, and Greg Turk. Deforming meshes that split and merge. <i>ACM Trans. Graph</i>., 28(3):1--10, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778787</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Chris Wojtan, Nils Th&#252;rey, Markus Gross, and Greg Turk. Physics-inspired topology changes for thin fluid features. <i>ACM Trans. Graph</i>., 29(4):1--8, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360646</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Chris Wojtan and Greg Turk. Fast viscoelastic behavior with thin features. <i>ACM Trans. Graph</i>., 27(3):47, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1921459</ref_obj_id>
				<ref_obj_pid>1921427</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Jihun Yu and Greg Turk. Reconstructing surfaces of particle-based fluids using anisotropic kernels. In <i>Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, SCA '10, pages 217--225, Aire-la-Ville, Switzerland, Switzerland, 2010. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218109</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Wen Zheng, Jun-Hai Yong, and Jean-Claude Paul. Simulation of bubbles. In <i>Proc. of the ACM Siggraph/Eurographics Symposium on Computer Animation</i>, pages 325--333, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073298</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Yongning Zhu and Robert Bridson. Animating sand as a fluid. <i>ACM Trans. Graph</i>., 24(3):965--972, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[D. Zorin, P. Schr&#246;der, and W. Sweldens. Interpolating subdivision for meshes with arbitrary topology. In <i>ACM SIGGRAPH 1996 papers</i>, page 192. ACM, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LIQUID SIMULATION WITH MESH-BASED SURFACE TRACKING Siggraph 2011 Course Notes Half Day Course Course 
organizer: Chris Wojtan Institute of Science and Technology Austria Lecturers: Matthias Muller-Fischer 
NVIDIA Chris Wojtan Institute of Science and Technology Austria Tyson Brochu University of British Columbia 
Course Description Animating detailed liquid surfaces has continually been a challenge for computer graphics 
researchers and visual e.ects artists. Over the past few years, a strong trend has emerged among researchers 
in this .eld towards mesh-based surface tracking in order to synthesize extremely detailed liquid surfaces 
as e.ciently as possible. This course will provide attendees with a solid understanding of the steps 
necessary to create a .uid simulator with a meshbased liquid surface. The course will begin with an 
overview of several existing liquid surface tracking techniques, discussing the pros and cons of each 
method. We will then provide instructions and a simple demonstration on how to embed a triangle mesh 
into a .nite-di.erence-based .uid simulator. Once this groundwork has been laid, the next section of 
the course will stress the importance of surface quality and review techniques for maintaining a high 
quality triangle mesh. Afterward, we will describe several methods for allowing the liquid surface to 
merge together or break apart. The .nal section of this course showcase the bene.ts and further applications 
of a mesh-based liquid surface, highlighting state-of-the-art methods for tracking colors and textures, 
maintaining liquid volume, preserving small surface features, and simulating realistic surface tension 
waves. Level of Di.culty: Advanced. Intended Audience This course is intended for both researchers and 
developers in industry who want to implement and have a solid understanding of the state of the art 
in .uid simulation for computer animation. Preprequisites A familiarity with Eulerian .uid simulation 
techniques for computer animation. The necessary background material can be found in the book Fluid 
Simulation for Computer Graphics by Robert Bridson (available from A K Peters), or the SIGGRAPH 2007 
course notes on Fluid Simulation by Robert Bridson and Matthias Mller-Fischer. In addition, a passing 
knowledge of basic triangle mesh algorithms like subdivision and edge collapses will be useful. About 
the Lecturers Chris Wojtan Institute of Science and Technology Austria wojtan@ist.ac.at http://pub.ist.ac.at/group 
wojtan/ Dr. Chris Wojtan is an assistant professor at the Institute of Science of Technology Austria 
(IST Austria), where he is establishing a computer graphics lab with a research focus on physically-based 
animation, geometric modelling, and numerical techniques. His computer graphics contributions include 
methods for animating detailed viscoplastic materials, several techniques for controlling physics simulations, 
and an algorithm for e.ciently computing topological changes in deforming triangle meshes. His research 
into mesh-based .uid surface tracking has helped produce extremely detailed liquid surface animations, 
allowing arbitrarily thin features and detailed crown splashes. Prior to his work at IST Austria, Chris 
received his Ph.D. in Computer Science from the Georgia Institute of Technology in 2010, and he worked 
as a visiting scientist at Carnegie Mellon University and ETH Zurich. Matthias Muller-Fischer NVIDIA 
matthias@mueller-.scher.com http://www.matthiasmueller.info/ Matthias Muller-Fischer is a principal 
software engineer at nvidia and head of the PhysX research team. He received his PhD in atomistic simulation 
of polymers in 1999 from ETH Zurich. During his post-doc with the MIT Computer Graphics Group (1999-2001) 
he changed .elds to macroscopic real-time simulations. Since then, his main research .eld has been the 
development of fast and robust physically based simulation methods for computer games. He has published 
key papers in computer graphics on real-time particle based water simulation and visualization as well 
as .nite element and geometry based soft body, cloth and fracture simulation. In 2002, he co-founded 
the game middleware company NovodeX (acquired in 2004 by AGEIA Technologies, Inc.), where he was head 
of research and responsible for the extension of the physics simulation library PhysX by innovative new 
features. He has been with nvidia since the acquisition of AGEIA in early 2008. Tyson Brochu University 
of British Columbia tbrochu@cs.ubc.ca www.cs.ubc.ca/ tbrochu/ Tyson Brochu is a PhD candidate in the 
department of Computer Science at the University of British Columbia. His research focuses on purely 
mesh-based representations of surfaces undergoing extreme deformations and changes in topology, with 
a focus on tracking liquid surfaces. Additionally, he developed a novel approach to constructing .uid 
simulation elements, which captures .ne surface details present in mesh-based surface tracking. His 
work has been published in scienti.c computing and computer graphics journals. Collaboration with VFX 
studios such as Weta Digital and Digital Domain informs his research and provides industry practitioners 
with access to cutting-edge applied research. Course Overview 5 minutes: Introduction and welcome Chris 
Wojtan 25 minutes: Liquid surface tracking review Matthias Muller-Fischer -Review of Eulerian surface 
tracking methods -Review of point-based Lagrangian surface tracking methods -Problems with previous methods 
that can be overcome with a Lagrangian surface mesh -Bene.ts of a mesh-based Lagrangian representation 
(Examples with .xed topology) 30 minutes: Embedding a surface mesh into an Eulerian .uid simulation Matthias 
Muller-Fischer -Mesh-based surface tracking in a simulation using regular cubic grid cells -Boundary 
conditions for solids and the free surface -Live demonstration -Problems with di.ering surface and simulation 
resolutions -Adapting Eulerian grid geometry to better .t the surface mesh 15 minute break 15 minutes: 
Maintaining surface mesh quality Chris Wojtan -Why? Visual artifacts, memory limitations, and stable 
computation -Quality measures -Operations: Edge splits, edge .ips, edge collapses, and null-space smoothing 
50 minutes: Topology changes Chris Wojtan -Why do we need topology changes? -Topological operations on 
the mesh itself -Global grid-based re-meshing of the surface -Local grid-based re-meshing of the surface 
(marching cubes &#38; convex hulls) 50 minutes: Advantages of a mesh surface Tyson Brochu -Color and 
texture tracking -Volume preservation (both global and local) -Preserved surface details -Thin features 
-Surface tension (several methods) -The future of mesh-based surface tracking 5 minutes: Conclusion Chris 
Wojtan Figure 1: This course will provide all of the imlpementation details necessary to implement a 
.uid simulator capable of preserving thin liquid sheets and e.ciently exhibiting detailed surface tension 
behavior.  1 Introduction Animating detailed liquid surfaces is continually an important challenge 
for computer graphics researchers and visual e.ects artists. Until recently, purely Lagrangian techniques 
(like triangle meshes or particles) for simulating the liquid surface had been limited by several challenging 
problems. The di.culties associated with tangling surfaces were quite unattractive, and the complicated 
mesh surgery techniques necessary to split and merge surfaces were often plagued with robustness problems 
and promptly dismissed. On the other hand, Eulerian techniques (such as the level set method) were practically 
bulletproof and quite simple to implement. As a result, surface tracking techniques based on dynamic 
implicit surfaces became extremely popular and helped revolutionize the .eld of physics-based computer 
animation. Over the past few years, however, several of the seemingly insurmountable problems associated 
with dynamic explicit surfaces have become more tractable steady research in engineering, applied mathematics, 
computer vision, computational geometry, and computer graphics has slowly chipped away at the problem. 
Methods now exist for e.ciently manipulating complex meshes while preventing self-collisions, and recent 
advances have allowed for the fast and robust computation of topological changes on deforming meshes. 
As a result of this progress, dynamic explicit meshes can now be integrated in a .uid simulation environment. 
While explicit meshes may require more care for certain operations than standard implicit surface methods, 
dynamic meshes have the potential to become the new standard tool for Figure 2: This course will explain 
how to couple a grid-basd .uid solver with a mesh-based surface in order to simulate materials with extremely 
detailed surface features. creating beautiful .uid simulations. By default, mesh-based dynamic surfaces 
preserve an exquisite amount of visual detail, and they naturally provide a straightforward computational 
environment for simulating di.erential equations on a moving surface. In addition, although care is required 
to robustly handle topological changes, we now have the ability to control the topology of a dynamic 
surface, allowing for the persistence of thin liquid features and the e.cient formation of tiny water 
droplets. As a result of these compelling advantages of dynamic explicit surfaces, a trend has emerged 
among computer graphics researchers towards simulating liquids with mesh-based surface tracking. In the 
interest of keeping researchers and special e.ects developers up to date with the latest technology, 
this course aims to provide attendees with a solid understanding of the steps necessary to create their 
own .uid simulator with a mesh-based liquid surface. The course will begin with an overview of several 
existing liquid surface tracking techniques, discussing the pros and cons of each method. We will then 
provide instructions and a simple demonstration on how to embed a triangle mesh into a .nite-di.erence-based 
.uid simulator. Once this groundwork has been laid, the next section of the course will stress the importance 
of surface quality and review techniques for maintaining a high quality triangle mesh. Afterward, we 
will describe several methods for allowing the liquid surface to merge together or break apart. The .nal 
section of this course showcase the bene.ts and further applications of a mesh-based liquid surface, 
highlighting state-of-the-art methods for tracking colors and textures, maintaining liquid volume, preserving 
small surface features, and simulating realistic surface tension waves. 2 Surface tracking review The 
main visual feature of a liquid is its surface. The sharp boundary distinguishes it from gases and creates 
all the fascinating visual e.ects that make liquids popular entities in computer graphics. Therefore, 
a large body of work on tracking liquid surfaces exists both in the computational .uid dynamics and in 
the computer graphics literature. The problem of tracking a surface does not appear in .uid simulations 
only. It arises whenever a surface follows an underlying deformation or velocity .eld. In .uid simulations, 
this .eld is the solution of the Navier-Stokes equations but it can also be procedurally generated or 
created by a mesh deformation tool. As in the case of .uid simulation approaches, there are Eulerian 
(grid based) and Lagrangian (particle based) surface tracking methods. In computer graphics, Eulerian 
tracking methods are often used with Eulerian simulations and Lagrangian tracking methods with Lagrangian 
simulations. Nonetheless, simulation and surface tracking are two independent problems. All the surface 
tracker needs from the simulator is the velocity vector at an arbitrary location in space. An Eulerian 
simulator provides this information by interpolating the discretized velocity .eld, while particle based 
simulators extrapolate the velocities of the particles. In turn, the tracker needs to provide a function 
that tells the simulator whether an arbitrary point in space is inside or outside the volume enclosed 
by the surface. In this course we focus on surface tracking of liquids simulated with an Eulerian solver. 
However, as mentioned above, the methods we are going to present might as well be used in connection 
with particle based .uid simulations or mesh deformation tools. They could even be applied to solid simulations 
where the surface mesh is driven by a surrounding tetrahedral mesh as in [46, 71]. The mesh based surface 
tracking methods presented in this course belong to the class of Lagrangian methods because the vertices 
of the mesh are treated as particles that are advected with the .uid .ow. The main di.erence to previous 
particle tracking methods is that mesh based methods maintain connectivity information during the simulation. 
2.1 Previous work on Eulerian surface tracking Grid based tracking methods operate on a scalar .eld that 
de.nes the surface implicitly. Typically, this .eld is discretized using a regular hexaderal grid. Instead 
of moving the surface explicitly, the tracker updates the .eld via Eulerian advection along the velocities 
provided by the simulator. The scalar .eld is used to answer the inside/outside query and by the simulation 
framework to render the surface. We will discuss three popular grid based surface tracking approaches. 
They di.er mainly in what they store as the scalar .eld and the advection method used. The Level Set 
method (LSM)  The Volume Of Fluid (VOF) method  Density based approaches.  Figure 3: Representation 
of a 1D-liquid (blue bar) using (a) level sets [56], (b) Volume Of Fluid (VOF) [54] and (c) a mass conserving 
density .eld [44]. The Level Set method is the most popular Eulerian approach to surface tracking in 
computer graphics because it handles topological changes of the surface automatically, as all grid based 
tracking methods do, and is easy to implement, at least in its basic form. Unfortunately, the method 
su.ers volume loss in underresolved, high-curvature regions due to numerical di.usion. In practice, this 
means that in the course of a simulation thin features dissolve and, eventually, the entire liquid disappears. 
One way to solve this problem is to introduce a quantity that is conserved by the operations that evolve 
the surface. In VOF methods, this quantity is the total volume of the liquid, while in density based 
approaches, it is the total mass. Another idea to reduce volume loss is to extend the original LSM itself 
using Lagrangian elements such as particles. 2.1.1 The Level Set method The basic LSM, originally developed 
by [49], is described in detail in the two excellent and comprehensive books [56] and [48]. Here we only 
give a brief introduction. The LSM de.nes the surface via its signed distance .eld f. Each grid cell 
stores the distance to the surface as a positive or negative scalar depending on whether the cell is 
outside or inside the liquid respectively. Hence, the surface itself is de.ned implicitly as the zero 
level-set of this scalar .eld (see Figure 3(a)). The inside/outside query can be answered by interpolating 
the distance .eld and checking its sign. To advance the surface, f is passively advected along the velocity 
.eld u of the .uid via ft = -u Vf. (1) To save computation time, f is typically tracked only in a narrow 
band near the surface. With the advection along the .uid .ow, f loses the property of being a distance 
.eld, i.e. |Vf| 1. Therefore, it has to be reinitialized periodically. In [56], the authors use Fast 
= Marching to do this. First, the values of f are explicitly re-computed near the surface. Then, the 
equation |Vf| = 1 is solved in a process that propagates known values of f away from the surface (see 
[56] for the details of this elegant method). Surface tracking itself is performed by solving Equation 
1 either using .nite di.erences or semi-Lagrangian advection [61]. The advantage of the latter method 
is its unconditional stability. It updates the value of f at location x by sampling f where the .uid 
at x was located in the previous time step. In the simplest version, f is updated using linear backward 
path tracing f(x,tn+1)= f(x - .tu(x,tn+1),tn) (2) and tri-linear interpolation to evaluate the right 
hand side. Interpolation from the surrounding grid values is necessary because the backtraced location 
is, in general, not aligned with the grid. In this basic form, semi-Lagrangian advection introduces substantial 
damping, meaning that details of the shape of the surface get lost quickly. There are several ways to 
improve the situation. To increase accuracy of the backward tracing step to second order in time, linear 
backward path tracing can be replaced by a second-order Runge-Kutta scheme. Here, the current position 
is .rst linearly backtraced for half a time step only. The .uid velocity at this intermediate position 
is then used to perform the original linear backtracing step. The superiority of the Runge-Kutta scheme 
over linear backtracing is especially apparent in regions where the .uid rotates. A way to lift the backtracking 
step to second order accuracy in space is to use the MacCormack method. It performs a forward tracing 
step after backward tracing which yields an estimate of the numerical error introduced by using a linear 
path. This error estimate is then used to correct the original result [55]. Damping in the interpolation 
step can be reduced by replacing simple tri-linear interpolation with the Cubic Interpolated Propagation 
(CIP) method [35]. Here, the spatial derivatives of f are advected along with f and used to replace tri-linear 
interpolation by higher order polynomial interpolation. Yet another idea was introduced recently by Heo 
et al.[26]. Instead of sampling f regularly in space, they store the discrete values of f at the Gauss 
quadrature locations within each cell and use multi-dimensional Lagrange interpolation to evaluate f 
at arbitrary locations within the cell. All these methods are purely Eulerian. Volume and feature loss 
can also be reduced by introducing Lagrangian elements. Enright et al. proposed the particle level set 
method in [19, 20] and re.ned it in [17]. They place spherical particles randomly near the surface with 
a sign indicating which side of the surface they lie on. These particles are advected passively with 
the .uid .ow along with the level set function f. An advection error in f can now be detected when the 
sign of a particle does not correspond to the sign of f at its location. In this case, the level set 
values near the particle are corrected using the sign and radius of the escaped particle. To make sure 
that the particles remain evenly distributed near the surface, they are reseeded periodically after a 
.xed number of frames. Semi-Lagrangian Contouring (SLC) [3] is another method that combines Lagrangian 
elements with Eulerian surface tracking. In this method, the surface is represented by an explicit triangle 
mesh. In contrast to mesh-based surface tracking, as described in this course, the explicit mesh used 
in SLC is not persistent but is recreated at every time step as the zero iso-surface of the signed distance 
.eld using the Marching Cubes method [38]. The main advantage of having an explicit mesh is that in the 
advection step distances to the surface the triangle mesh can be measured exactly so no interpolation 
is needed. In contrast to mesh based surface tracking, however, subgrid detail gets lost in the resampling 
step. To counteract volume loss, Kim et al.[33] devised a method for conserving the volume of a liquid 
de.ned by a level set. They use a controller that generates an arti.cial pressure term based on the volume 
deviation. However, while the method reduces volume loss globally, it does not preserve small features. 
The problem of small feature loss in connection with the LSM was addressed by Losasso et al.[39]. They 
proposed to use an oct-tree data structure instead of a regular grid to discretize the signed distance 
function. This way, subgrid features can be kept alive by subdividing the surrounding cells to the necessary 
resolution. Instead of modifying the grid, Chentanez et al.[14] proposed to thicken features that are 
about to shrink to a size smaller than the grid spacing. 2.1.2 The Volume Of Fluid method Figure 4: 
To compute the volume of .uid crossing a cell through the right face in one time step, the rectangle 
d  h is intersected with the .uid fraction, given by the surface plane (see [54]). As we have seen in 
the previous section, one of the main disadvantages of the LSM is volume loss. Since the LSM uses the 
signed distance .eld f to de.ne the liquid surface and most often in a narrow band only, there is no 
direct way to derive the total volume or mass of the liquid from the scalar .eld of the tracker. This 
problem is depicted in 1D in Figure 3(a). The signed distance .eld represented by the dashed bars is 
not immediately aware of the amount of liquid between the two bands. A fundamentally new idea to solve 
the volume loss problem is to replace the signed distance .eld by another scalar quantity. The Volume 
Of Fluid (VOF) method [54] uses the volume fraction (see Figure 3 (b)). Let us consider a 2D staggered 
grid with cell size h and the velocity components u and v in x-and y-direction respectively. The volume 
fraction fi,j is a discretized quantity de.ned per grid cell, not a continuous .eld. It is derived from 
the continuous characteristic function . as 1 fi,j = h2 .(x, y) dxdy, (3) cell i,j where .(x, y) = 1 
0 if (x, y) is covered by liquid otherwise . (4) The total volume of the liquid is now given by Vliquid 
= h2fi,j. (5) i,j To conserve the total volume we simply have to make sure that whenever the advection 
operation subtracts a certain amount from fi,j it distributes the exact same amount somewhere else on 
the grid. In the continuous case, the advection equation that conserves a quantity f is given by .f = 
-V  (uf)= -u Vf - (V u)f. (6) .t Note that this equation is a generalization of Equation 1 with the 
extra term on the far right which vanishes for divergence free velocity .elds. This more general form 
conserves the advected quantity even if the velocity .eld is not divergence free. Discretizing this advection 
equation using .rst order .nite di.erences makes it more intuitive: .t fn+1 = fn i,j i,j + - Fi+ - Gi,j+ 
(Fi- ,j + Gi,j- ), (7) 1111 ,j h 2222 where Fi (fu)i ,j denotes the .ux of f across the right/left 
edge of cell (i, j), and = 1 1 ,j 2 2 Gi,j 1 2 =(fu)i,j denotes the .ux of f across the top/bottom 
edge of cell (i, j). This update 1 2 scheme conserves the quantity fi,j because .rst, the di.erence 
of the .uxes into and out of the cell is accumulated in fi,j and and second, the .uxes are de.ned on 
the cell faces so the .t same quantity that leaves cell (i, j) on the right, i.e. Fi+ ,j, enters cell 
(i +1,j) on the 1 2 h left. For the evaluation of the .uxes we need to know the shape of the surface 
.rst. From the de.nition of fi,j it is apparent that 0 = fi,j = 1. Also, a cell contains the surface 
if and only if 0 <fi,j < 1. The surface itself can be extracted using the least squares volume-of-.uid 
interface reconstruction algorithm (LVIRA) [54]. This algorithm computes a plane in each surface cell 
(i, j) as follows. A given normal vector n together with the volume fraction fi,j uniquely de.ne a plane 
through cell (i, j) with normal n that divides it accord to fi,j. This plane also cuts cells in the neighborhood 
of cell (i, j) but in general not according to their stored volume fractions. LIVRA picks the normal, 
and with it the plane, that minimizes this error in the least squares sense in the 3x3 neighborhood of 
cell (i, j). Once we have computed the planes in all cells, we are ready to compute the .uxes needed 
to advect the surface. Let us consider Fi+ The distance the .uid travels through ,j. 1 2 the right side 
of cell (i, j) in one time step is d = ui+ .t, and the volume that leaves the 1 2 ,j cell is V = dh. 
Since cell (i, j) is only partially .lled, we need to know the volume of .uid V.uid inside V . To compute 
this, we intersect the rectangle of size d  h inside cell (i, j) aligned to the right with the .uid 
volume de.ned by the surface plane inside that cell (see Figure 4). The volume fraction that leaves the 
cell and must be subtracted from fi,j is, thus, .tV.uid/h2 = V.uid/h2 and Fi+ , so 1 2 ,jh Fi+ 1 2 = 
 V.uid ,j h.t . (8) The construction shown in Figure 4 is only valid if d<h which means that the CFL 
condition must hold. This makes VOF only conditionally stable in contrast to the LSM. An additional drawback 
of VOF is that LVIRA is obviously not the only way to determine surface normals and that the VOF representation 
of the surface does not permit accurate curvature estimates, which are essential to surface tension computations. 
In contrast, normals and f curvature are easily and uniquely extracted from a signed distance .eld as 
n =and | f| . = V f (although this Laplacian operator VV becomes inaccurate and unstable for | f| small 
surface features only one or two grid cells wide). 2.1.3 Density based approaches The problem of de.ning 
smooth normals and curvature present in the VOF method are removed by replacing the volume fraction .eld 
by a density .eld, as proposed by Mullen et al.[44] (see Figure 3 (c)). In their approach, the liquid 
surface is de.ned as the 0.5 -isosurface of the density .eld .. This density is not to be confused with 
the .uid density. When tracking the surface of an incompressible .uid, the .uid density ..uid is one 
everywhere while the surface density . can show a smooth transition from one to zero near the surface 
as in Figure 3. The advantage of this de.nition is that no special treatment is necessary near the surface 
as in the VOF method. The density .eld . is simply advected with the underlying .uid .ow with the conserving 
advection equation .. = -V  (.u). (9) .t Analogous to the VOF method, the value that is stored on the 
discrete grid is the averaged density of the cell 1 di,j = . dx dy (10) h2 cell i,j The .ux through the 
right side is given by .ht Fi+ ,j = .i,jui+ 1 2 ,j .t h/h2 or 1 2 Fi+ = .i,jui+ 1 2 ,j (11) 1 ,j 2 
if ui+ ,j is positive and .i+1,jui+ 1 2 ,j if ui+ 1 2 ,j is negative. This way, the total mass of the 
.uid is conserved exactly over time. As in the VOF method, and in contrast to the LSM, the density approach 
is only stable when the CFL condition holds. An issue of the density approach not present in the VOF 
method is that the density pro.le near the liquid surface gets blurred over time due to numerical di.usion. 
This is problematic because cells with densities less then 0.5 are considered to be air cells. This way, 
the volume enclosed by the iso-surface can shrink even though the total mass is conserved. Mullen et 
al.[44] solve this problem by introducing a sharpening .ow.  2.2 Previous work on Lagrangian surface 
tracking Figure 5: Left: A density kernel with .nite support radius h. Right: Sum of two mutually shifted 
kernels. Figure 6: Methods to create smoother surfaces. Left: Adams et al.[1] average particle positions 
xi and distance-to-surface estimates di to derive the red surface plane. Middle: Williams [66] smoothes 
a surface estimate while restricting the vertices to stay inside the gray area. Right: Yu et al.[72] 
use anisotropic kernels. Lagrangian tracking methods typically represent the surface by a set of particles. 
The advection step is therefore simpler and without stability problems because the particles are moved 
passively with the .uid .ow as in the Markers and Cells (MAC) method (see [41] for an excellent review). 
In particle based simulations, the tracking particles are often chosen to be a subset of the simulation 
particles so advection is not needed because it is performed by the simulator. Figure 7: Comparison 
between di.erent surface reconstruction approaches for particle based liquids. From top left to bottom 
right: the basic isotropic kernel method used by Muller et al.[47], the method of averaged kernel centers 
proposed by Zhu et al.[74], the method by Adams et al.[1] and the anisotropic kernel method of Yu et 
al.[72]. Images curtesy of Yu et al.[72]. However, constructing a closed surface from the point cloud 
is challenging. In Eulerian tracking methods, the surface is uniquely de.ned as an iso-surface and can 
be easily extracted using Marching Cubes. In contrast, reconstructing a surface from a point cloud is 
an entire research .eld not restricted to surface tracking and the shape of the surface is not uniquely 
de.ned by the points. Blinn [7] introduced a popular method to create a surface from a set of points. 
This is at the heart of most Lagrangian liquid surface tracking methods in computer graphics. The main 
building block is a kernel. A kernel is a scalar function that describes the in.uence of a single particle 
i on its neighborhood. A popular example taken from [47] is 1(h2 - ||x - xi||2)3 ||x - xi|| = h wi(x) 
= (12) h6 0 otherwise, where xi is the position of particle i. This function has .nite support, meaning 
it is non-zero only within distance h from the particle (see Figure 5). The fact that a particle only 
in.uences a bounded neighborhood of .xed size reduces the complexity of kernel-based algorithms from 
O(n2) to O(n). Two other nice properties of this speci.c kernel are that it is C2-continuous at the boundary 
and that it depends on the square of the distance, not the distance, which avoids an expensive square 
root operation. We can now de.ne a density .eld for a set of particles using the kernels wi as .(x)= 
wi(x) (13) i and the implicit surface S = {x : .(x)=0.5}. (14) This de.nition handles topological changes 
automatically as before. Muller et al. used it in [47] to extract the liquid surface from an SPH-based 
particle simulation and created a triangle mesh for rendering using Marching Cubes [38]. Unfortunately, 
this simple and straight forward density .eld yields blobby surfaces especially in regions where the 
.uid sampling is low as the top left image in Figure 7 shows. Zhu et al.[74] addressed this problem. 
For a given position x in space, they .rst compute a normalized weighted average of the nearby particle 
positions as a(x)= wi(x)xi/wi(x) (15) ii and then evaluate the kernel centered at this position. This 
modi.cation results in far smoother surfaces (see top right image in Figure 7). Adams et al.[1] further 
improved the method of Zhu et al.. They maintain an additional scalar attribute di per particle storing 
an approximate particle-to-surface distance. With this additional information, they de.ne the liquid 
surface as the zero-level set of f(x)= d(x) - ||a(x) - x|| (16) d(x)= wi(x)di/wi(x) (17) ii (see Figure 
6 left). As the bottom left image in Figure 7 shows, the resulting surface is further improved but at 
the price of higher computation complexity because updating the distance estimates di involves a propagation 
process that has to be performed at each time step. Williams [66] .rst creates an explicit triangle mesh 
similar to [47] using Marching Tiles which is an extension of Marching Cubes. He then iterates through 
the vertices multiple times and applies a Laplacian smoothing operator while making sure that each vertex 
stays within rmin and rmax to the closest particle (see Figure 6 middle). The optimization process slows 
down surface extraction but creates perfectly .at surfaces for a liquid at rest. Recently, Yu et al.[72] 
presented another method that yields high quality results. The basic idea is to use anisotropic kernels 
that have the following form: 1(h2 - Gi ||x - xi||2)3 ||Gi (x - xi)|| = h wi(x,Gi) = (18) h6 0 otherwise, 
where Gi is a symmetric 3  3 matrix. The application of Gi stretches the kernels along the Eigenvectors 
Gi by the inverse of the respective eigenvalues (see Figure 6 right). Gi is determined by applying Principal 
Component Analysis to the nearby particle positions. The resulting elliptic kernels reduce the bumpiness 
of the resulting surface, as the bottom right image in Figure 7 shows. In this course we are going to 
study mesh based surface tracking methods. Here, the surface is represented as an explicit, closed triangle 
mesh. These methods are Lagrangian because the vertices of the mesh are treated as particles, passively 
advected with the .uid. The main di.erence to the methods above is the reconstruction step. This step 
is trivial in mesh based tracking because connectivity information is available as a triangle mesh. The 
mesh can be used directly for rendering and to answer the inside/outside-query. The di.cult part is the 
maintenance of the connectivity structure, which is the primary subject of this course. As we will see, 
handling topological changes, e.g. self intersections and keeping well shaped triangles are the most 
challenging problems. 2.3 Conclusion In this chapter we have reviewed Eulerian and Lagrangian methods 
to track the surface of a liquid. Eulerian methods de.ne the surface implicitly as the iso-surface of 
a scalar function. The Lagrangian methods that were used in computer graphics before the introduction 
of mesh based tracking used the same approach in the reconstruction step. So the core di.erence between 
previous tracking methods and mesh based tracking is that the former represent the closed surface implicitly 
as an iso-surface, while mesh based tracking uses an explicit triangle mesh. The advantage of implicit 
surface representations are Topology changes are handled automatically. No special operations are needed 
when the liquid merges with itself.  The quality of the representation stays the same throughout the 
simulation. Even though volume or small scale features might get lost, the underlying grid does not change 
its shape.  For these reasons implicit methods are relatively simple to implement, at least in their 
basic formulation.  Implicit methods also have a parallel structure because they operate on regular 
grids. This makes them good candidates for the implementation on a GPU.  What plagues implicit surface 
representations is feature loss. Working with a grid restricts the smallest features to have the size 
of the underlying grid sampling. It was the feature loss problem that originally initiated research in 
explicit surface tracking. This relatively new approach has several advantages that make it an attractive 
alternative to implicit surface tracking: Detail is perfectly preserved. It is not feasible to keep 
features alive throughout the simulation but if they are removed it is done deliberately by the algorithm. 
 The same is true for topological changes. Handling topological changes explicitly is challenging but 
has the advantage that merges and splits can be performed in a controlled way.  Mesh based methods 
simplify tracking of colors and texture coordinates.  Explicit surface meshes allow the computation 
of the enclosed volume exactly, which is the basic building block of direct volume conservation methods. 
 Triangle meshes allow an easy way to compute PDE s on the surface. This feature was used by Thuerey 
et al.[64] to add simulated small scale detail on liquid surfaces.  Mesh based surface tracking also 
poses some challenging problems. One of the main challenges is to catch and resolve all possible cases 
of mesh self intersections and splits.  The solver has to be modi.ed to make sure it is aware of the 
detail preserved in the mesh.  The quality of the surface mesh degrades over time so the triangulation 
has to be constantly improved.  Most operations on the mesh are geometric in nature. Special care has 
to be taken  w.r.t. numerical robustness in degenerate and singular cases. In this class we will address 
these topics and propose various solutions. We will present the state of the art in explicit surface 
tracking and show the fascinating e.ects and level of detail it can produce; a level of detail not seen 
in computer graphics .uid simulations before.  3 Embedding a surface mesh into an Eulerian .uid simulation 
This section lays the groundwork for a liquid simulation with mesh-based surface tracking. We will begin 
with a standard .uid simulation framework used frequently throughout computer animation [61, 21, 22, 
8], and then we will explain how to embed a triangle mesh inside of this .uid simulation. The velocity 
of the .uid simulation is used to advect the vertices of the triangle mesh, and the way that the mesh 
partitions space into inside and outside regions will be used to de.ne the domain of the .uid simulation. 
The result is a mesh that splashes and sloshes around under the in.uence of .uid forces. 3.1 Physical 
model We start by building upon a common technique for simulating .uid dynamics in computer graphics: 
We aim to simulate the partial di.erential equations which govern subsonic .uid .ow, known as the incompressible 
Navier-Stokes equations: .u = -(u V)u - 1 Vp + .V2 u + g (19) .t . V u = 0 (20) where u is the velocity 
of the .uid, t is time, . is the density of the .uid, p is the .uid s pressure, . is the .uid s viscosity, 
and g is the acceleration due to gravity. We store these variables on a regular hexahedral grid, with 
the pressure values stored at the centers of each grid cell and the velocity components stored on the 
faces of each grid cell. The details of storing these variables are discussed further in [8]. We can 
think of the Equation 19 as a collection of competing accelerations that push around the .uid. This equation 
governs the momentum of the .uid, and we will refer to it as the momentum equation. Equation 20 is a 
constraint on the momentum equation that forces the .uid s velocity .eld to have a divergence of zero 
( divergence-free ). When the .uid s velocity has no divergence, it is considered incompressible, and 
there are no sources or sinks of momentum in this system. Additionally, as long as the density of the 
.uid is constant, this divergence-free constraint implies that the .uid must conserve mass and volume. 
To simulate the .uid, we can break up the momentum equation into a series of terms using a technique 
called operator-splitting. This e.ectively means that each term on the right hand side of Equation 19 
can be applied independently, one after the other. To account for the gravity term, we simply add a constant 
downward acceleration to our velocity .eld. To account for the viscosity term, we solve the di.usion 
equation ..t u = ..u. The advection term -(u V)u, has been studied extensively in the applied mathematics 
and computer graphics literature, and we can use any of several techniques ([61, 21, 34, 43]). We used 
a higherorder variant on semi-Lagrangian advection known as unconditionally stable MacCormack advection 
[55] for most of our simulations. Because the pressure .eld p is unknown at the beginning of each time 
step, we can use it as a Lagrange multiplier to satisfy the the divergence-free constraint (Equation 
20). We solve the remaining system of equations using the Conjugate Gradient method, giving us the .nal 
velocity .eld u. See [8] for details. 3.2 Embedded surface mesh In order to simulate a liquid, we also 
need to simulate the motion of the the visible surface at the .uid boundary. This is where our mesh-based 
surface tracking comes in we choose to use an explicit triangle mesh for this .uid surface. The mesh 
is simply a collection of vertices connected by triangles. This surface mesh partitions space into two 
regions: a portion of space that is inside the simulated liquid, and a vast region of space that is outside 
of the liquid. Because of this clean partition, we use this surface mesh to mark the simulation domain 
in our .uid solver. In other words, the surface mesh tells the .uid simulation which cells should be 
simulated as .uid and which should not, depending on whether the cells lie inside or outside of the mesh 
surface. As such, we insist that this surface is manifold and closed, in order to have a clear de.nition 
of what regions are inside of the simulated .uid, and which regions are outside. 3.2.1 Updating the surface 
mesh Once we have solved for the velocity .eld u at the end of each time step, use it to move the .uid 
surface. Each surface vertex is implicitly associated with a velocity, because it has a unique position 
xi and the velocity .eld is a function of space, u(x). In practice, u is only de.ned at discrete regular 
intervals in our .uid grid, but we can use simple interpolation techniques to .t a smoothly-varying function 
to this data. In practice, we use tri-linear interpolation to .nd a velocity for a given position in 
space. At this point, we assume that no topological changes occur during the motion of the surface, so 
we do not need to update any triangle connectivity; we simply need to solve the ordinary di.erential 
equation dxi = ui (21) dt for each surface vertex i with position xi and velocity ui. We can rearrange 
this equation to get the integral new old xi = xi + uidt (22) The simplest technique for performing this 
numerical integration for each surface vertex is to use the forward Euler method: new old xi = xi + ui.t 
(23) The forward Euler method works well enough for most purposes, however we choose to use a fourth 
order Runge Kutta method (RK4) [52] because it performed best in analytical tests (Figure 8, for example) 
and it is relatively inexpensive to compute. RK4 can be written as: 1 new old xi = xi +(k1 + 2k2 + 2k3 
+ k4) (24) 6 with: k1 = u(x old) i k2 = u(x old + 1.tk1) i 2 = u(x old + 1.tk2) k3 i 2 old k4 = u(xi 
+.tk3) where u(x) is the velocity .eld u evaluated at the position x. Because we make extensive use of 
velocity interpolation when integrating the surface vertices, we need to be able to access the velocity 
.eld u in many regions just outside of the boundary of the .uid simulation. To make sure that our velocity 
.eld u adequately samples space, we extrapolate u outward using a fast marching method [8]. Lastly, when 
we update the positions of the surface vertices, some vertices may lie within solid obstacles. One way 
to address this problem is to resolve the collision by projecting the vertices onto the surface of the 
obstacle. This works well in most cases, though it can cause problems when an entire thin sheet of liquid 
lies inside an obstacle. In such cases, simply projecting vertices onto the surface of the obstacle tends 
to create in.nitely thin sheets of liquid. These degenerate cases can cause problems later (during volume 
or convex hull calculations, for example). In practice, we used free-slip boundary conditions at these 
solid obstacles and ignored any resulting collisions. While this approach worked surprisingly well for 
us, a more principled approach to collision handling may work better. 3.2.2 Updating the .nite di.erence 
grid As mentioned in the previous section, the .uid simulation uses the surface mesh to determine which 
cells will be active .uid cells, and which cells will be inactive air cells. This is essentially the 
same problem as voxelizing a surface mesh, or returning a set of grid cells that overlap the volume contained 
within a surface mesh. However, we can take this method a step further and actually compute a signed 
distance function in order to give ourselves some more information about the surface geometry. The scalar 
values of this signed distance function (negative values inside, and positive values outside) are collocated 
with the cellcentered pressure values in the .uid grid. We employ a voxelization-style method [45] to 
compute the signed distance function. We .rst voxelize the triangle mesh onto a grid by computing intersections 
with a rays in the x, y, and z directions. For each ray, we keep track of its inside/outside status with 
a counter. At the start of the ray (which is guaranteed to be outside of the mesh), the counter has a 
value of zero. For each intersection with the triangle mesh, we increment the counter Figure 8: How does 
our mesh-based surface tracking scheme compare with other common surface trackers? This .gure compares 
our mesh-based surface (far right column) with three other schemes (from left to right: level set method, 
particle level set method, and semi-Lagrangian contouring). The top row shows the initial conditions 
of the Zalesak sphere test. As time goes on (subsequent lower rows), the notched sphere rotates. The 
analytical solution to this problem is that the .nal condition after one rotation should be idential 
to its initial shape and position. The mesh-based surface tracker described in this section behaves signi.cantly 
better than other methods. if it intersects a triangle whose normal is facing the opposite direction 
of the ray, and we decrement it if it intersects a triangle with a normal facing the same direction of 
the ray. This way, all regions outside of the mesh will have a counter value of zero, regions inside 
the mesh will have a value of one, regions where multiple surfaces overlap each other will have a value 
greater than one, and regions that are inside-out will be negative. We store this counter value at each 
grid point to assign an inside/outside status, and then we compute the distance from the point to the 
nearest surface triangle in order to complete the signed distance calculation. Because our algorithms 
only need an accurate signed distance in the thin band surrounding the surface, we save time by only 
performing this distance calculation for grid cells that overlap surface geometry. At this point, we 
can use this signed distance function to determine which .uid cells are inside of the surface (treating 
all inside cells as active .uid cells in the next time step). However, some regions of the surface may 
have been too thin to be adequately sampled by the .uid grid, so these thin regions would not be surrounded 
by any active .uid cells with this method. We use an explicit geometric technique to ensure that all 
thin features will be represented by negative values on the signed distance function grid. First, we 
.nd the set of all .uid cells that intersect or completely envelop any surface triangles this can be 
done e.ciently by looking at triangle vertex locations and seeing which grid cells they lie within. We 
then take the union of that set of cells and the original set of cells associated with negative values 
in the signed distance function. This .nal set of cells will adequately sample all thin features.  3.3 
Free-surface boundary conditions Like any Eulerian grid-based .uid simulation, we need to handle boundary 
conditions for the free surface. This can be done with several di.erent techniques, and we will discuss 
two speci.c methods here. 3.3.1 Constant cell weights The easiest way to implement the boundary conditions 
at the liquid surface is to simply assume that every .uid cell represents a unit of mass ..x3, where 
.x is the width of a .uid cell. Unfortunately, the embedded surface mesh will not necessarily occupy 
the exact same volume as the simulation elements in general, so the simulation will overestimate the 
total mass in the system. This is noticeable in thin regions of liquid, when a surface sheet splashes 
into a larger body of water. The arti.cially large mass injects additional momentum into the system and 
creates a larger-than-expected splash. Although this behavior may actually be desirable in a special 
e.ects environment and more pleasing to watch, it is nevertheless inaccurate. Errors in this constant-cell-weight 
strategy are clearly visible when thin surface regions stretch out into even thinner surfaces. Here, 
the volume of these surface features is preserved, so the volume per unit length drops as the length 
increases. However, the mass per unit length stays the same with this strategy, so a large amount of 
mass and momentum is added into the system whenever a small region thins out. The constant cell weight 
strategy also causes problems when the liquid settles down and creates a .at surface. When one .uid cell 
has small amount of liquid, the mass gets rounded up to ..x3 . This overestimated mass causes extra pressure 
at random spots along the settling surface and prevent a .at surface from forming. Although the constant 
cell weight method is less accurate, it is quite simple to implement. We used this method in the simulations 
by Wojtan et al. [69, 70] and Thurey et al. [64]. 3.3.2 Ghost .uid method Alternatively, we can use 
a more accurate .uid discretization, called the ghost .uid method [18, 5]. This is a second-order approximation 
that essentially uses a linear extrapolation to as Compute inside/outside classification of mesh Assign 
new fluid cells Calculate fluid velocity Advect surface vertex positions If mesh collides with solid 
boundary Update vertex positions Figure 9: Pseudo-code for one timestep of Eulerian .uid simulation sign 
pressure values outside of the free surface. Because the .uid pressure should drop to zero exactly at 
the surface, an extrapolation will give negative pressures outside of the .uid surface. Combining the 
ghost .uid method with a surface mesh has yielded high quality visual results in the simulations of [11]. 
 3.4 Algorithm overview Figure 9 shows pseudo-code for a single time step of an Eulerian .uid simulation 
with a mesh-based embedded surface. The simulation .rst computes an inside/outside test for each .uid 
cell. Next, each simulation cell that is located inside of the mesh is considered a .uid cell, while 
the cells outside of the mesh are ignored. Next, we solve one time step of the incompressible Navier-Stokes 
equations to get a new .uid velocity for each .uid cell. Finally, we advect the surface vertices through 
the Eulerian .uid cell velocities using standard techniques (e.g. 4th order Runge-Kutta), and then adjust 
these vertex positions in the case of any collisions with solid obstacles. 3.5 Problems with di.ering 
surface and simulation resolutions When a very high resolution surface is advected through a comparatively 
low resolution physics simulation, interesting visual artifacts can occur. In particular, the high resolution 
features captured by the surface (a high resolution mesh or high resolution level set) cannot be communicated 
to the physics simulation. As a result, the physics simulation does nothing to correct unphysical high 
resolution features, such as surface kinks and small voids. Some examples of this type of phenomena can 
be seen in the right column of Figure 10. One way to solve this problem is to adapt the Eulerian simulation 
grid to match up with the surface geometry. Later in this course, we will describe a very e.ective method 
for achieving this matching between the surface and grid resolutions using Voronoi regions for the Eulerian 
.uid elements [11]. 3.6 Limitations In this section, we introduced a straightforward method for coupling 
a grid-based Eulerian .uid simulation to a mesh-based Lagrangian surface. As such, it inherits the bene.ts 
of both the Eulerian and Lagrangian simulation techniques. By using a semi-Lagrangian advection scheme, 
we can guarantee that the .uid simulation will be unconditionally stable for large time steps. The Lagrangian 
nature of the surface mesh also avoids the accumulation of re-sampling errors that are common in Eulerian 
surface tracking schemes. Although it is quite basic, the surface tracking method described in this 
section is already quite good. Unlike many Eulerian surface tracking schemes, this method has no smoothing 
errors caused by continually re-sampling the surface from a grid. The only numerical errors in this scheme 
are due to the .uid simulation or the time integration. Figure 8 shows how this simple mesh-based surface 
tracker compares to other common surface-tracking schemes in the Zalesak sphere test. One major problem 
with this approach as presented so far is that it does nothing to address topology changes in the surface 
geometry. Lively .uid animations can have drastic changes in the connectivity of surface geometry, so 
it is important that we address this issue. We will discuss this problem and present some solutions in 
more detail later in the course. Another problem with this approach is that surface triangles can become 
very distorted, causing many computational problems. We will discuss some ways to maintain a high triangle 
mesh in the next section of these course notes.  4 Maintaining surface mesh quality As we update the 
position of surface vertices during a .uid simulation, the surface mesh will deform. If we do not change 
the connectivity of the triangle mesh, then triangles in the surface mesh can become severely distorted 
 some triangles may become much too large, while other triangles may have near-zero area. To address 
this problem, we perform various mesh-maintenance operations at the end of each advection step. This 
section of the course will elaborate on this idea and provide some tools to improve the quality of the 
surface mesh. 4.1 Why do we care about mesh quality? The importance of mesh quality has been addressed 
thoroughly by others [57, 31], so we will only lightly touch on the concept here. Triangles with poor 
quality metrics can adversely a.ect a simulation in many ways: Visual artifacts: If we allow triangles 
to become arbitrarily large, then giant jagged spikes may pervade the visible surface in our .uid animations. 
 Memory and time limitations: Triangles with small areas use space very ine.ciently. Without any restrictions 
on triangle area, our simulations can get bogged down by millions of invisible triangles. This is bad 
for memory usage, and it is a waste of time to run computations on so many vertices.  Geometric computations: 
We may decide to use di.erent forms of geometric computation, such as collision detection, ray tracing, 
topology changes, or boundary integration. Triangles with poor quality can create serious robustness 
problems for these operations.  Numerical stability: If we wish to use any numerical integration techniques 
on our surface mesh, such as surface smoothing or surface tension calculations, then we need to maintain 
a high quality triangle mesh.  4.2 Measuring triangle quality A very simple way to detect troublesome 
triangles is by measuring the lengths of all edges in the mesh. If any edge is smaller than some minimum 
edge length, then it may create problems for our simulation. Similarly, if any edge is larger than some 
maximum length, then it may be too bloated and distorted to accurately sample the surface. Some triangles 
may have perfectly acceptable edge lengths but have very small areas (see Figure 11), so a simple edge-length 
measurement is not enough to .nd poorly-shaped triangles. In addition to measuring edge length, we can 
also measure the area and penalize very large or very small area measurements, or we can measure triangle 
angles and penalize extreme angles that are too large or too small. The problem of .nding a metric for 
triangle quality in a way that is meaningful and useful is very well studied. This section only brie.y 
touches on some of the simplest measurements just enough to detect the problematic triangles in our mesh 
so we can get rid of them. For a much more thorough discussion of triangle quality measures (and linear 
element quality in general), see [57]. 4.3 Mesh quality operations Once we have determined which triangles 
are unacceptable, we need to actually remove them from our triangle mesh. This section explains how to 
perform several di.erent mesh operations in order to locally improve triangle quality. The operations 
covered in this section are the edge split, the edge .ip, the edge collapse, and null-space smoothing. 
These operations are relatively independent from each other and can be performed in any order, though 
certain orders are more sensible than others. For a point of reference, the mesh-based surface tracking 
software El Topo [12] uses these operations in the same order that we present them in this chapter. The 
simulations in [71, 69, 64, 70] use only the edge split, followed by the edge collapse operations. Each 
of these operations may be performed in a way that guarantees the resulting triangle mesh to be collision-free, 
and we include those details here these details here for completeness. These collision-free constraints 
can be safely disregarded if you don t care whether your surface has self-intersections, or if those 
intersections will be .xed by some topology-changing operation later. 4.3.1 Edge split If an edge is 
longer than a user-de.ned maximum edge length, we subdivide it by introducing a new vertex (see Figure 
12). The new vertex can be placed at the edge midpoint, which will not introduce any new intersections. 
However, we may wish to o.set the new vertex from the current surface using a subdivision scheme to maintain 
curvature. We begin by introducing the new vertex at the edge midpoint, and we then compute the predicted 
location of the new vertex via the subdivision scheme. These two points de.ne a pseudomotion. We check 
the new vertex and its incident triangles and edges as it moves from the edge midpoint to its predicted 
point to ensure that it does not collide with any existing mesh elements (which do not move during this 
pseudomotion). If a collision does occur, we can revert to using the edge midpoint, which is guaranteed 
to not introduce any new intersections.  4.3.2 Edge .ip We employ an edge .ip operation as a way of 
maintaining good triangle aspect ratios. For each edge incident on two triangles, we check whether the 
distance between the two points not on the edge is less than the length of the edge. If so, we remove 
the edge and create a new edge between these two points (see Figure 13). A simple way of ensuring that 
this operation does not introduce any intersections is to check that no existing edge intersects the 
two new triangles and that no point lies inside the tetrahedron formed by the two new and two old triangles. 
We also reject the edge .ip if it introduces a change in volume greater than a user-de.ned maximum volume 
change (we usually set this maximum volume change to be 0.1.3, where . is the average desired edge length; 
for simulations involving extremely thin surfaces, this may need to be further reduced). Flipping a single 
edge may introduce new triangles with poor aspect ratios, so we iteratively sweep over all edges in 
the mesh until no .ip is performed or until we reach a maximum number of sweeps (in [12], we set this 
maximum to .ve). We also require that the new edge length decrease by a minimum amount to prevent the 
same edge from .ipping back and forth on subsequent sweeps.  4.3.3 Edge collapse If an edge is shorter 
than a user-de.ned minimum edge length, we attempt to collapse it by replacing it with a single vertex 
as shown in Figure 14. As with edge splitting, we treat only manifold edges, skipping edges incident 
on more than two triangles. We use a subdivision scheme to choose the location of the new single vertex 
in the general case. However, we also use an eigen-decomposition of the quadric metric tensor to detect 
vertices that lie on ridges or creases [32]. If one edge end point lies on a ridge and the other lies 
on a smooth patch of the surface, we set the new vertex position to be the position of the existing vertex 
on the ridge. In other words, we wish to prevent vertices moving o. of the ridge, which tends to introduce 
bumps or jagged edges. To ensure collision safety, we can use the same pseudomotion collision detection 
described during our edge split explanation earlier, this time with two vertices in motion: the end points 
of the edge. These end vertices will have the same predicted location: the location chosen by the subdivision 
algorithm. If a collision is detected during this pseudomotion, we can try again, this time moving the 
vertices towards the edge midpoint. Unlike edge splitting, however, we have no safe fallback vertex location. 
If we insist on having a collision-free mesh and cannot .nd a collision-free trajectory, then the edge 
collapse must be abandoned. We use simple minimum and maximum edge lengths for determining when to split 
and collapse edges. In practice, we compute the average edge length when the mesh is initialized and 
set the minimum and maximum edge length parameters to be some fractions of the initial average length. 
This has the e.ect of keeping the edge lengths within some range of the initial average using split and 
collapse operations. In our examples, we allow edge lengths to vary between 0.5 and 1.5 of the initial 
average edge length; however, these parameters did not require tuning and our system remains stable for 
other values. More sophisticated criteria for triggering a split or collapse exist, such as detecting 
triangles whose areas are too small or too large, or aspect ratios that are too far from unity (c.f. 
[32]). When choosing locations for new vertices during an edge collapse or split operation, there are 
a number of schemes that can be used. We use traditional butter.y subdivision [16] due to the simplicity 
of its implementation and because it is free of parameters. Quadric error minimization schemes [23] are 
promising, but in our experience the simplicity and quality of butter.y subdivision made it more attractive. 
Butter.y subdivision determines the location of a new edge midpoint Pnew as 1 Pnew = (8(P1 + P2) + 2(Q1 
+ Q2) - (R1 + R2 + R3 + R4)), 16 where P1 and P2 are end points of the edge, Q1 and Q2 are the vertices 
on the two triangles incident on the edge which are not the edge end points, and R1 ... R4 are the vertices 
on the four triangles adjacent to the triangles incident on the edge (see Figure 15). We have also found 
that the adaptive butter.y subdivision scheme of [75] worked very well, because it does not assume that 
all vertices have a valence of six. This adaptive subdivision scheme helps ensure high degrees of smoothness 
even for abnormal triangle con.gurations. One potential danger to keep in mind is that non-manifold geometry 
can be created by collapsing an edge with a valence-3 vertex (see Figure 16. The edge collapse will result 
in two non-manifold triangles (each sharing the same three vertices). We must ensure that our surface 
geometry is manifold, so we can do one of two things: we can either forbid this edge collapse, or we 
can allow the collapse and delete any resulting non-manifold geometry. The .rst option is far simpler 
to implement. 4.3.4 Null-space smoothing A powerful mesh improvement technique was recently introduced 
by Jiao [32]. Applying a Laplacian .lter to the vertex locations would move each vertex to the average 
of its neighbors locations. This usually has the desirable e.ect of equalizing edge lengths. However, 
it will also shrink the volume enclosed by the surface and smooth away sharp features. We instead move 
the vertices only in the null-space of their associated quadric metric tensors. If the vertex is on a 
.at or smoothly curved patch of surface, the null space will correspond to the plane tangential to the 
surface at the vertex. If the vertex is on a ridge, the null space will be the in.nite line de.ned by 
the ridge, and the smoothing operation preserves the ridge feature. If the vertex is at a corner, the 
null space will be empty and the vertex will not move, preserving the corner. To ensure no mesh intersection, 
we treat the global smoothing operation as a pseudotrajectory on all vertices and apply collision resolution 
as if the surface was moving under the in.uence of an external velocity .eld.   5 Topology changes 
Up to this point in the course, we have shown some methods for tracking a simulated liquid surface by 
deforming a triangle through time. As these surfaces evolve, they not only change shape and move around, 
but they can also merge together and split apart. Such events are called changes in the surface s topology. 
To learn more about the behavior of a surface through time, we can plot the its position at every point 
in space and in time on the same graph. In Figure 17, we visualize a one-dimensional surface swept through 
time, with its position in space represented on the horizontal axis and time on the vertical axis. This 
time-swept .gure creates a polygon in space-time. A vertical line in this graph represents a .xed point 
in space, and horizontal line represents a .xed instant in time. Whenever a topological change occurs, 
parts of the surface will either appear, disappear, split apart, or merge together. These instances can 
be identi.ed as critical points in space-time. In Figure 17, a dashed line is drawn at each instant a 
topological change occurs. Line A marks the global minimum in time, signifying the initial creation of 
the surface. As time advances, the surface gets wider (occupies more space) until it splits into two 
pieces at instant B. In the period of time between B and C, the two new surfaces drift away from each 
other, as the right surface in.ates. Instant C represents another local minimum, as the right surface 
splits apart once again. Line D marks our .rst local maximum, as the leftmost two surfaces merge together. 
Finally, the global maximum is found at time E, where the remaining surfaces shrink until they disappear. 
5.1 Why do we need topological changes? In order to faithfully simulate interesting natural phenomena, 
we must allow our dynamic surfaces to change their topology. These topological changes are not only necessary 
to capture realistic physical e.ects, but they are also a useful tool for simplifying overwhelmingly 
complicated systems for the purposes of computer animation. 5.1.1 Importance in nature Topological changes 
occur naturally in several settings. If we are concerned with the dynamics of .uids and liquid surfaces, 
then topological changes occur whenever the surface breaks into droplets, or whenever di.erent surfaces 
merge together upon contact. Cohesive forces between liquid molecules cause surfaces to merge together 
the instant they touch each other, and surface tension forces cause surfaces to rip apart whenever they 
form thin tendrils (this phenomena is known as a Rayleigh-Plateau instability, and it is primarily responsible 
for the degeneration of a splashing surface of water into a mist .lled with tiny droplets). If our simulations 
do not allow topological changes such as merging, then surfaces will build up several layers of water 
separated by nothing but a vacuum. Similarly, if we do not allow surfaces to tear apart, then surface 
tension e.ects will routinely create in.nitely thin tendrils of liquid. This type of behavior is distractingly 
unnatural, and it can also cause numerical stability problems in our simulations. 5.1.2 Importance in 
animation Topological changes are also important for ensuring that our simulations have a manageable 
workload. In particular, if we neglect to merge together any surfaces, then only a few seconds of a dynamic 
splashing liquid will generate an overwhelming number of folded surfaces that should have naturally 
merged together. Aside from this behavior being unnatural, the computational load necessary to store 
and operate on each of these surface primitives will skyrocket. As mentioned above, we can also run into 
numerical problems if we allow surfaces to become arbitrarily thin without imposing topological changes. 
These problems result from the inevitable division by very small numbers as the angles and edge lengths 
of surface triangles shrink. In order to allow for fast and stable computer animations of physical phenomena 
with dynamic surfaces, we must properly handle their changing topology. 5.1.3 Dynamic topology changes 
in practice In practice, because we use a triangle mesh that evolves through time as our dynamic surface, 
we are interested in methods for explicitly enforcing topological changes on a triangle mesh. Unfortunately, 
the solution is not straightforward. Many arbitrarily complicated shape con.gurations can arise, and 
our code must handle every one of them robustly. It is not always as simple as identifying two surfaces 
that are close to each other and then sewing them together self-intersecting surfaces can form extremely 
complex shapes. We must also guarantee that the surface is closed, manifold, and consistently oriented 
after every one of our topological operations otherwise, many important assumptions will be violated 
and the physics simulation will fail. Within this course, we will discuss three practical solutions to 
this problem of handling topological changes with a triangle mesh. Section 5.2 will explain how to enforce 
topological changes by reconnecting triangles when they come close together and guarantee a collisionfree 
state of the mesh. Section 5.3 will explain how to leverage our .uid simulation grid in order to replace 
the surface mesh with a topologically corrected one. Finally, Sections 5.4 and 5.5 will explain how to 
only locally re-sample the surface in order to limit the amount of surface re-sampling.  5.2 Topological 
operations on the mesh itself In this section we will describe a few operations that will locally modify 
the mesh connectivity to result in a change in topology. The key idea to make this tractable is to require 
that every mesh operation should leave the mesh in a consistent, non-intersecting state as opposed to 
attempting to recover such a state after the fact. Therefore, in this section we will show how to use 
robust collision detection methods to ensure we detect every possible violation. Once a collision is 
detected, we either roll back the operation if it is deemed non-critical and may be delayed to a subsequent 
time step when it may succeed, or otherwise minimally perturb mesh positions to guaranteeably avoid the 
problem. Our mesh perturbation is patterned after frictionless inelastic collision response in a physical 
contact problem. (In upcoming sections we will see alternative approaches that do allow the mesh to self 
intersect, then reconstruct a consistent surface. Each approach has its own set of trade-o.s.) We will 
.rst describe a method for allowing surface patches which are close to each other to safely merge without 
introducing any intersections, then introduce two methods allowing meshes to separate when they become 
too thin. We will .nish this section by describing how to keep the mesh intersection-free even if topology 
changes fail. 5.2.1 Mesh merge To achieve a surface merge operation, we seek out edges that are close 
in space but are on distinct surface patches, and attempt to merge the surface. We .rst search for edges 
that are closer than a speci.ed tolerance, then sort the pairs of edges in order of increasing separation 
distance so that the nearest edges are merged .rst. For each pair in the sorted list, we .rst remove 
the triangles incident on each edge. This introduces two temporary holes in the mesh, each hole consisting 
of a loop of four boundary edges. We create eight new triangles between the two holes, using a closed-form 
triangulation. (This sequence is shown in .gure 18.) We then use intersection testing to determine if 
these new triangles (a) (b) (c) intersect any existing mesh elements or each other (treating degenerate 
cases as intersections for safety). If an intersection is found, we discard the new triangles and replace 
the original triangles incident on the proximal edges, abandoning the topology change. This merge operation 
may introduce degenerate tetrahedra which must be detected and deleted. The operation can also be aborted 
if the edge neighbourhoods are not distinct or otherwise degenerate in such a way that we cannot perform 
our closed-form remeshing. If we must abandon a topology change, we will need to rely on collision detection 
and resolution in order to maintain the intersection-free invariant (described below). 5.2.2 Mesh separation 
The mesh zippering operation just described cannot separate or pinch a mesh to create two disjoint volumes, 
so we must treat this with a separate operation. Two alternative approaches to cutting the mesh at thin 
necks or spindles are detailed here. The .rst approach, as described by Wojtan et al. [70] is to detect 
thin spindles of liquid while performing the local surface maintenance operations. These thin spindles 
are detected by .agging edges that will produce non-manifold geometry if collapsed. Of these non-manifold 
cases, we can easily identify any thin spindles by their triangular cross-section. To perform the mesh 
surgery, we cut the mesh at the triangular cross-section, seal each end with a new triangle, and perturb 
the two new strands away from each other by a small o.set to avoid intersection. A similar operation 
is explained in detail by Lachaud et al. [37]. An alternative approach to separating meshes was introduced 
by Brochu et al. [12]. We must .rst relax our requirement that the surface meshes must be manifold. In 
particular, we allow more than two triangles to be incident on an edge. We do not, however, allow two 
triangles to share the same three vertices, thus creating a zero-volume tetrahedron, and we do not allow 
open surfaces, since we must still have an inside and an outside for the .uid simulation to work. An 
edge collapse or merge operation may introduce degenerate tetrahedra, so after performing either of these 
operations, we search the surface meshes for degenerate tetrahedra, and delete the two o.ending triangles. 
We also delete triangles that may have repeated vertices ( collapsed triangles). After this sweep, we 
deal with surfaces which may be connected only at a single vertex. These so-called singular vertices 
can be detected if their incident triangles are not all connected. If this is the case, we partition 
the set of incident triangles into connected components. For each component, we create a duplicate vertex 
and map all triangles in the component to this new vertex. A similar procedure is described by Gueziec 
et al. [24]. We also move the duplicate vertices very slightly towards the centroid of their associated 
triangles to avoid problems with collision detection and resolution which may occur when two vertices 
occupy exactly the same point in space. 5.2.3 Maintaining the intersection-free invariant Up to this 
point, we have discussed how to locally improve mesh quality and change surface topology without introducing 
self-intersections in the surface. However, since we have taken the stance that we will not attempt to 
.x self-intersections after they occur, we must still deal with collisions that may occur when the mesh 
is advected according to the .uid simulation velocity .eld. The topology change operations are designed 
to prevent this from happening by merging surfaces that are close to each other before they collide. 
However, there may be topological operations that must be prevented because they would introduce self-intersections, 
or degenerate con.gurations. Also, since the the mesh moves in discrete steps, it may move from a non-intersecting 
state into an intersecting state, without .rst entering into a proximal state if the .uid velocities 
are great enough or the interval between discrete states is large enough. To deal with these problems, 
we use continuous collision detection and resolution, adapted from the cloth simulation literature. 
5.2.4 Interference detection We di.erentiate between three types of geometric interference detection: 
intersection detection, proximity detection, and collision detection. We use all three of these types 
at di.erent steps in the collision processing algorithm. Static intersection detection detects if and 
where a mesh intersects itself for a given mesh con.guration (i.e. at one instant in time). This can 
generally be decomposed into primitive tests discovering where an edge is penetrating a triangle, but 
we must take care to identify degenerate cases, such as an edge penetrating a surface only at an edge 
or at a vertex. Static proximity detection detects when mesh elements are closer than a speci.ed tolerance 
(in particular, when a vertex is close to a triangle or when two edges are close to each other). We will 
name this tolerance fp. Our proximity detection function can also return a collision normal , n, which, 
when an impulse is applied along it, will increase the distance between elements. Proximity detection 
.nds the two points on the pair of mesh elements that are closest to each other. If we denote the set 
of four barycentric coordinates of these two points as a (setting ai = 1 if i is the vertex in a vertex 
triangle collision), then to .nd the distance between the mesh elements, we multiply the barycentric 
coordinates by -1 if they refer to a point on the triangle or on the second edge in an edge-edge proximity, 
to get a new set of coordinates, a. Then taking the sum of vertex locations weighted by these scaled 
barycentric coordinates yields a vector between these closest points. If p are the indices of the vertices 
involved, then the shortest distance is given by the length of this vector: d =  4 aixpi i=1  Continuous 
collision detection (CCD) detects whether a collision between a moving vertex and a moving triangle or 
between two moving edges will occur during some speci.ed time span. In our framework, two mesh con.gurations 
are passed into the collision detection routines: one at time tn and one at time tn +.t. We assume vertices 
move in a linear trajectory from their positions at time tn to their positions at time tn +.t, and that 
the mesh connectivity does not change over this time step. Given these two con.gurations, continuous 
collision detection will return any point-triangle and edge-edge collisions, as well as the time that 
the collision occurs (sometime between tn and tn+.t), the collision normal, the set of barycentric coordinates 
describing the point of contact, and the computed relative displacement. The El Topo surface tracking 
library currently uses the collision detection approach introduced by Provot [53]. This method makes 
the simplifying assumption that, within time steps, mesh vertices move on constant, straight-line velocity 
paths, and the edges and triangles between them are linearly interpolated at every intermediate time. 
The times at which a point and a triangle or two edges become coplanar (a necessary condition for collision) 
are the roots of a simple cubic equation in this model, which is simple enough to solve. At these times, 
the proximity of the elements can be evaluated to determine if a collision occurs. Bridson et al. [9] 
introduced error tolerances in the cubic solver, and an error tolerance on proximity testing at the coplanarity 
times, to account for rounding errors in the process, e.ectively eliminating any false negatives (undetected 
collisions) in the CCD process. 5.2.5 Collision resolution Detecting collisions is only half the story 
 we must also perturb the mesh to avoid intersections. Our collision resolution procedure is based on 
the .ltering approach for handling collisions [9], and operates in three phases. First, we run proximity 
detection as described in section 5.2.4 to obtain pairs of elements that are closer to each other than 
fp. For each pair of proximal elements, we compute the relative normal velocity of the elements. We then 
perturb the vertex velocities so that the new relative normal velocity is positive, and large enough 
to carry the vertices at least fp away from each other if they were integrated forward for .t without 
further interference. Attempting to maintain this small minimum separation signi.cantly helps in avoiding 
degenerate geometric cases which would otherwise slow subsequent .oating-point-based collision detection 
and resolution. As described in section 5.2.4, for a pair of elements, proximity detection returns a 
distance d and a set of scaled barycentric coordinates a. If p are the indices of the element vertices 
and u are the vertex velocities, then the relative velocity is: 4 urel = aiu[pi] i=1 If n is the unit-length 
collision normal, the impulse J we apply is computed as: fp - d d = - n  urel .t d J = (a, a)M-1 where 
M is the diagonal matrix of vertex masses. (In problems where there is no natural mass for a surface 
vertex, we simply use a unit weighting: M = I.) Then for each vertex in proximity, we distribute the 
impulse J to perturb the predicted velocity .eld: a upi = upi + J n Mpi Note that this will not immediately 
resolve any of the proximities detected, as the current vertex positions are left untouched; it aims 
to resolve the proximity at the next time step. More importantly, it tends to dramatically reduce the 
number of collisions that must be dealt with in the next phase. In the second phase of collision resolution, 
we use continuous collision detection to determine pairs of colliding elements. Our CCD function returns 
the relative displacement of the elements in the direction of the collision normal (which we can scale 
by 1/.t to compute the relative normal velocity), as well as the barycentric coordinates that should 
be used to distribute the corrective impulse. For each pair of colliding elements we encounter, we apply 
an impulse that sets the relative normal velocity between the elements to zero, thus preventing the collision 
from occurring. This is similar to the repulsion impulses applied in the previous phase, except that 
the impulse magnitude is: n  .xrel d = - .t This is equivalent to introducing an impulse that instantaneously 
changes the velocity, while minimizing the velocity change in the normal direction in a least-squares 
sense. (Minimizing the normal velocity change in this way ensures that momentum is conserved, if the 
least-squares metric is kinetic energy.) One sweep through all mesh elements will not prevent all collisions, 
as resolving one collision may introduce a new collision between a pair of elements that was already 
checked. We have found that applying three sweeps of this individual collision resolution handles most 
collisions: however we must use a fail-safe to ensure that all collisions are handled. For our fail-safe, 
we use the simultaneous treatment of collisions developed by Harmon et al. [25]. After three passes of 
individual collision resolution, we detect all pairs of elements that are still colliding. We lump colliding 
pairs of elements into impact zones based on adjacency and resolve all collisions in each zone simultaneously 
using one linear solve. We can think of our desired new velocities u' as being the solution to a constrained 
minimization problem: min ||u' - u||2 M ' subject to n  urel = 0 for all collisions We can re-write 
the constraint as a linear operator on the vertex velocities by building a matrix C, where each row, 
Ci, corresponds to one collision, and has non-zero entries in block columns j = [3v, 3v +1, 3v + 2], 
where v is one of the four vertices involved in collision i. Setting Ci,j =avnT , our constrained optimization 
problem becomes: min ||u' - u||2 M subject to Cu = 0 Solving this using the method of Lagrange multipliers 
yields the system: CM-1CT . = Cu We can think of . as the set of impulses which, when applied, yields 
zero relative normal velocities for all collisions. We update the vertex velocities according to: u ' 
= u + M-1CT . The application of these impulses may result in new collisions, so we run collision detection 
again and add any additional collisions to the set of impact zones, repeating the process until no new 
collisions are detected. This is guaranteed to terminate, assuming adequately accurate linear solves, 
since each additional constraint reduces the .nite dimension of the solution space; in practice it proves 
to be very e.cient.  5.3 Global grid-based re-meshing of the surface In this section, we will outline 
a very di.erent method for changing the topology of a meshbased .uid surface. Here, we start with the 
mesh-based .uid simulation outlined in Section 3, and then use the Eulerian grid to globally reconstruct 
the surface mesh. The basic strategy for globally re-sampling this triangle mesh looks like this: Determine 
the intersections of the mesh with the edges of an implicit regular grid  For the corners of each intersected 
cell, determine whether they are inside or outside the surface  Based on the locations of intersections 
and inside-outside information, create the triangles of the new mesh using marching cubes templates. 
 5.3.1 Core algorithm The surface to be tracked is represented at all times by a closed manifold triangle 
mesh. At each time step, the vertices of the mesh are .rst advected. Advection potentially introduces 
self intersections, so, in a second stage, we .x the mesh before the next time step starts. This stage 
does not need to be executed at each time step necessarily. It might be feasible to run and render few 
advection steps before the self-intersections are resolved. In any case, the problem we have to solve 
could be stated as follows: Given a closed potentially self-intersecting manifold mesh  Create a closed 
non-self-intersecting manifold mesh which approximates the outside of the input mesh.   To solve this 
problem, a regular grid is used. The size of the cubical cells h is a userspeci.ed parameter. We use 
a sparse data structure based on spatial hashing [63] and only store the cells that are intersected by 
the input mesh (see Figure 22). First we determine the intersections of the input triangles with the 
edges of the grid cells. Each intersection has a type depending on the triangle normal. If the component 
of the normal along the edge is positive, the intersection is of type exit, otherwise it is of type entry. 
Note that there is potentially more than one intersection per cell edge. Therefore, with each cell edge 
we store a state change counter which is initialized with zero. For all intersections of that particular 
edge, the state change variable is increased or decreased by one dependent on whether the intersection 
is of type entry or exit, respectively. With this information we can now determine the states of the 
nodes of the grid as either inside or outside. To do this, only the edges pointing in x-direction are 
necessary. For each pair of y-and z-coordinates present in at least one grid cell we follow the edges 
in x-direction summing up the state di.erence counters (see Fig. 19). Each grid node for which the sum 
is greater zero is marked as inside. All others are outside nodes. Interpreting all marks greater than 
zero as inside is our trick to get rid of all self-intersecting parts of the input mesh (see Figure 22). 
It is important that singular cases are handled properly. If the cell edge runs through edges or vertices 
of the input mesh, one has to make sure that only one intersection is counted. We do this by lumping 
together cuts that are closer than an f. This simple approach is prone to numerical errors though. To 
make the process more robust, we not only use the x-direction as just described but all three principal 
edge directions in positive and negative direction and mark a node as inside if more than 3 out of the 
6 tests vote for inside. This way, we can also handle defect meshes with holes robustly. A similar approach 
was used in [30] for modeling occlusions. In our tests, the method was robust enough to handle the potentially 
ill conditioned normals produced by the marching cubes method. Finally we apply the marching cubes templates 
[38] to each cell to create the triangles of the new mesh. These templates require a vertex on each cell 
edge for which the states of the adjacent grid nodes di.er. We create these vertices uniquely for all 
the cells adjacent to the particular edge to make sure that the resulting mesh is connected. In order 
to handle multiple intersections per edge, we choose the positions of these vertices to be the average 
of the positions of all intersections of the edge Note that, by construction, the resulting mesh is manifold, 
closed and non-self-intersecting as required. 5.3.2 Subgrid feature preservation Using a grid for mesh 
creation has the e.ect that subgrid features disappear. This is true for the level set approach as well. 
For cases where this is problematic, we propose an extension of our basic method. One of our applications 
is an unbounded Eulerian liquid simulation using a sparse dynamically changing simulation grid. In contrast 
to the easier case where the .uid is con.ned to a box, the unbounded liquid typically spreads on the 
.oor and turns into a thin layer. The thickness of the layer decreases rather evenly so as soon as it 
goes below the grid spacing the entire layer disappears more or less at once. To alleviate such problems, 
we now present a technique that can track arbitrarily thin structures on the uniform grid used so far 
without the need of local subdivision. To bound the complexity and the number of vertices created at 
each time step, we restrict the type of subgrid geometry a single cell can hold to one arbitrarily thin 
layer. In our tests, the ability to handle this type of detail resolves a majority of the problematic 
cases. The technique allows us to handle thin sheets of water (separated by at least one grid cell) as 
well as shallow puddles for instance (see Figure 20). The 2d case Let us .rst look at the 2d case. The 
top row in Figure 24 shows the standard marching squares templates modulo rotation. On each cell edge 
with adjacent nodes of di.erent type a vertex is created (shown in green). As Figure 19 shows, it is 
possible that edges are cut and still end up having adjacent nodes of the same type. This is detail that 
is lost in the standard algorithm. We preserve this detail as follows: For each edge with adjacent nodes 
of the same type for which we have registered at least two cuts, we create two additional vertices (shown 
in red in Figure 24). As their positions we choose the minimum and maximum locations of all registered 
cuts on the edge. An enlarged set of templates is necessary to account for the additional vertices (Figure 
24). In addition to the two states of the cell nodes, cell edges have two states as well they can contain 
no or two red vertices. This enlarges the number of templates from 24 to 28 . Not all of the 28 con.gurations 
are valid though because red vertices are potentially created only on cell edges with adjacent nodes 
of equal type. There are certain cases where additional vertices are necessary. Let us have a look at 
template 1 - b for instance. Here, a thin layer ends inside the cell so we need the yellow vertex for 
not losing the front. The case can only occur if the input 2d surface takes a turn inside the cell which 
is only possible if there is an input vertex inside the cell. We choose this vertex of the input surface 
directly for the construction of the new surface. There might be more than one vertex of the input surface 
in the cell. In order to control the complexity of the new surface, we do not want to use all of them. 
Instead, the user can specify an upper bound k for how many internal vertices should be used (we chose 
k = 2 in the samples). Figure 23 shows the cases k = 1 (a), k = 2 (b) and k = 3 (c). The vertices are 
chosen such that the enclosed region has maximal area. We use internal vertices of the input mesh in 
other templates as well as image (d) shows. This is an e.ective way to preserve sharp edges and conserve 
volume (see Figures 27 and 28). Some of the templates have dual con.gurations which are shown on the 
right side of Figure 24. While there is only one ambiguity in standard marching squares (templates 4a 
and 10-a), a few more are present in the extended set. The ambiguity is solved in the standard approach 
by testing the value of the center of the cell. If it is inside the surface template 4-a is chosen, otherwise 
template 10-a. The same strategy could be used with the extended template set because the templates in 
on the left of Figure 24 tend to cover the cell centers while their dual counterparts tend to leave the 
center open. We use columns 1,2,3,4 and 11,12 independent of the state of the center. This way we reduce 
the number of yellow vertices needed and even out the bias of inside / outside area in the set of templates 
of Figure 24. Our method does not recover all subgrid features. Sharp corners inside grid cells are 
lost for instance. It does, however, preserve arbitrarily thin layers using a bounded number of additional 
vertices. Keeping arbitrarily thin layers is not always desirable. We introduce an additional level of 
control for the user. If the distance of the two red vertices goes below a threshold, they are discarded. 
Also, in the case of liquid simulation, it is feasible to ignore thin layers of void altogether by only 
creating red vertices if both ends of an edge are marked as outside. From 2d to 3d In the 3d case we 
need a way to create triangles for each grid cell. We do this in three steps (see Figure 25). First, 
we use the 2d templates on all 6 faces of the cell independently to create the green, red and yellow 
vertices plus line segments between them. In the 3d case, the yellow vertices are located where edges 
of the input mesh intersect the faces of the cells. In order for the triangulation to be compatible across 
grid cells, the con.guration on a cell face must not depend on features of the cell not contained in 
that face. Therefore it is correct to handle the sides independently reducing the problem to 2d. Second, 
we connect the line segments across all faces of the cell to form closed segment chains.  Finally each 
of those chains is triangulated separately using ear clipping [42]. This has to be done carefully. Figure 
26 shows three ways of triangulating the boundary (a). To avoid case (b) we never cut an ear that lies 
completely inside one face or a cut that leaves triangles in one side only.We do this by using a bitmask 
per vertex that stores one bit for each face the vertex belongs to (yellow vertices have one, non-yellow 
vertices have two bits set). If the bit-wise AND of the vertex masks of an ear is non zero, the cut is 
illegal. Avoiding case (c) is a bit trickier. When selecting the next ear, we choose the one for which 
a maximal number of vertices is below the ears plane, thereby maximizing the enclosed volume. Template 
tables The original marching cubes algorithm uses only 28 = 256 templates. These can be stored in a table 
to prevent creating them during runtime. Unfortunately, this is not possible in our case. Our method 
uses on the order of 28  212 templates (yellow vertices not considered). Also, the triangulation depends 
on the actual positions of the vertices in space. One way to improve performance is to check whether 
there are any red or yellow vertices and to fall back to the standard marching cube template table if 
this is not the case. Parallelization is another e.ective way to speed up the process because triangulation 
can be done independently for each cell.   5.4 Local grid-based re-meshing of the surface using marching 
cubes In the previous section, we introduced a method for handling topological changes to the deforming 
.uid surface by replacing the entire surface mesh with a new one derived from marching cubes templates. 
While this technique is extremely fast, e.ective, and easy to implement, it unnecessarily re-samples 
the surface mesh where no topology changes are needed. This section is concerned with only locally re-meshing 
the surface in order to handle topology changes, while leaving the rest of the surface untouched. This 
local handling of topological changes has the advantage of never re-sampling the surface unless it is 
involved in a topology change, which can limit numerical smoothing errors. However, this local resampling 
algorithm requires us to de.ne exactly which cells need to be re-sampled, and we need to guarantee that 
we don t leave any holes in the mesh when we sew the new mesh together with the old one. Much like the 
previous section used local marching cubes templates to replace the mesh surface in each grid cell, this 
section will explain how to use marching cubes templates for local surface replacement. However, because 
the algorithm in this section can not retain thin surface sheets, we introduce a more general method 
based on local convex hulls that reproduces thin surface sheets and strands in Section 5.5. 5.4.1 Overview 
of approach This method begins very similarly to the method described in Section 5.3. We .rst deform 
the .uid surface mesh by advecting it through the .uid s velocity .eld. Next, similar to the inside / 
outside classi.cation in Section 5.3.1, we compute a signed distance .eld from the mesh. We then compare 
the surface mesh to its grid-sampled signed distance .eld in order to detect where to locally change 
the topology of the surface. We then locally recompute the surface mesh in these areas and connect the 
new surface patches to the original surface. We are then free to repeat this process for the next step 
of the .uid simulation. Figure 29 shows a diagram of this algorithm. 5.4.2 Detection of topological 
events After advecting the .uid surface mesh as explained in Section 3, we then decide whether we should 
split any surfaces apart or merge them together. We .rst calculate a signed distance .eld D from our 
surface mesh, and then we examine the structure of this .eld to help decide where any topological events 
should take place. Signed Distance Field Calculation We choose to place our signed distance function 
on a regular grid that encloses our surface mesh. Note that the only places where topological changes 
can occur are at grid cells that intersect the mesh (the surface cells). Thus we calculate the distance 
to the triangle mesh at each grid point that touches a surface cell by calculating the exact distance 
to each nearby triangle and taking the minimum. We then use the voxelization method described in Section 
 5.3.1 to determine which grid points lie inside of the mesh, and which lie outside. We assign a positive 
signed distance to the grid points outside of the mesh and a negative sign to the points inside of the 
mesh. Because we only calculate the signed distance at grid cells touching the surface, and because we 
only sample the nearest triangles for each distance query, this calculation of the signed distance function 
can be made quite e.cient. Complex cell test At this point, we have two surface representations: an explicit 
surface mesh M, and a signed distance function D that implicitly de.nes a surface wherever the distance 
is zero. We can contrast these two surface representations to give us an idea of where the surface should 
be resampled. Any grid cell where the explicit mesh surface is connected signi.cantly di.erently than 
the implicit grid surface will be referred to as a complex cell. More speci.cally, A complex edge is 
an edge in the grid that intersects the mesh more than once.  A complex face is a square face in the 
grid that intersects the mesh in the shape of a closed loop or touches a complex edge.  A complex cell 
is a cubic grid cell that has any complex edges or complex faces, or any cell that has the same sign 
of the signed distance function at all of its corners while also having explicit geometry from the mesh 
embedded inside of it. We will refer to the act of testing a cell for complexity as the complex cell 
test.  This complex cell test can be used to mark a region in space where any topological changes occur 
in our surface. However, a straightforward application of this test will also mark detailed surfaces 
as topologically complex (see Figure 30). In order to preserve surface details like sharp corners, we 
must signi.cantly modify this test. We will now describe our modi.ed version of this test, the deep cell 
test. Deep cell test Because we are primarily interested in the .delity of the visual surface, we would 
prefer that most surface re-sampling occurs only in invisible regions or in the presence of major topological 
changes. To avoid the excessive re-sampling of highly detailed surfaces, we do not wish to mark all complex 
cells as topological events. Instead, we start by contrasting the interface of our explicit surface M 
with the interface of our implicit surface D. In the interest of surface detail preservation, we ignore 
any complex cells that are su.ciently close to both the interfaces. We only mark a complex cell that 
is at least one cell away from the isosurface interface, as illustrated in Figure 31. This deep cell 
test is necessary because subtle bumps in surface geometry can still trigger any complex cell test, but 
only cells with geometry .uctuations larger than a cell length will be marked by our deep cell test. 
Self-intersection tests In addition to topological changes triggered by surface proximity, we also choose 
to mark cells that indicate signi.cantly large self-intersections in the mesh. This test is performed 
while Figure 31: A two-dimensional illustration of our deep cell test. Figure (a) shows an input surface 
mesh M (dark blue line) with a visualization of the corresponding signed distance .eld D, where orange 
points are inside of the surface, and light blue points are outside. Next is a .gure showing all complex 
cells (b). The rightmost .gure shows all deep cells (c). Note that the deep cells only label geometry 
necessary for a topological change, while the complex cells aggressively label important surface details. 
 marching rays through the mesh in the voxelization phase of our signed distance calculation (Section 
5.3.1). Whenever a ray (grid edge) intersects the surface, we determine whether the ray is entering or 
leaving the surface by calculating the dot product of the ray direction with the triangle normal: a negative 
dot product indicates that the ray is entering the surface, while a positive dot product indicates an 
exit ray. In our implementation, we initialize an integer variable to zero at the start, and then we 
increment the value if the ray enters the surface and decrement the value if the ray leaves. As the ray 
passes through each grid node, every node that does not have an integer value of zero or one necessarily 
is in a region of multiple overlapping or inside-out surfaces. We mark any cells touching these grid 
nodes as topologically complex. An example of such inside-out or self-intersecting surfaces can be seen 
in Figure 22. Once we have marked every cell that we wish to re-sample, we want to replace M in these 
cells with a topologically-simple isosurface extraction. However, we need to ensure that every cell on 
the boundary of our marked cell region provides a topologically simple interface with marching cubes. 
In other words, the boundary must contain no complex edges and no complex faces. We execute a .ood .ll 
algorithm, starting with the initially marked cells and marching outward across complex faces, until 
the entire region of marked cells is bound by topologically simple cube faces (see Figure 32). Once this 
region has a clean interface with marching cubes, we can perform surgery on the mesh. 5.4.3 Altering 
the mesh topology At this stage in the algorithm, we have a mesh M, a distance .eld D, and a list of 
marked grid cells. Each of these marked cells describes a region of space in which we will locally remove 
the topologically complex explicit surface M and replace it with the topologically simple extracted isosurface 
of D. The surface inside of these cells is computed with a marching cubes algorithm and connected to 
the original mesh at the cell boundaries. To maintain a manifold surface mesh, it is important to ensure 
that the interface between the isosurface and the explicit surface matches up perfectly. We spend the 
rest of this section explaining how to compute this interface e.ciently and robustly. Sewing meshes 
together Here, we describe the basic method for matching the interface between an explicit triangle mesh 
and an extracted isosurface. We also explain this algorithm graphically in Figures 33 and 34. 1. After 
marking topologically complex cells, we .nd each triangle that intersects a cell edge on the boundary 
of the marked region. We then calculate the intersection point between the triangle and edge, and we 
subdivide the triangle into three new triangles that share a vertex at the intersection point. We call 
this newly created vertex a type 1 vertex. 2. Next, we .nd all triangle edges that intersect a cell 
face on the boundary of the marked region. We split each triangle edge at the point where it intersects 
the face, subdividing the two original triangles into four and inserting a new vertex on the face. We 
call this face vertex a type 2 vertex. 3. After all subdivisions have been performed, no triangles will 
cross the faces of any marked cells. That is, each triangle will lie completely inside or completely 
outside of the marked region. We delete all triangles that lie completely inside of the marked region. 
  Figure 33: We .rst create complex cells (yellow, left) wherever invalid topology is detected. We create 
type 1 vertices by subdividing triangles where they intersect the edges of a complex cell (middle), and 
then we create type 2 vertices by subdividing triangles where they intersect the faces of a complex cell 
(right). After these steps, the mesh triangles are either completely inside or completely outside of 
the complex cell region. We will delete the triangles inside of these complex cells and replace them 
with new triangles from marching cubes templates. 4. We use marching cubes to generate a triangle mesh 
in the marked region, and we connect the meshes together at the type 1 vertices. 5. Now we want to sew 
the surfaces together along the faces of each complex cell. We .rst ensure that each new triangle shares 
an edge with at most one triangle outside of the complex cell region for each new triangle that shares 
edges with two or more triangles outside of the complex region, we place a vertex at its barycenter and 
split it into three new triangles. 6. For each new triangle that does not perfectly match up with a 
segment of the boundary curve, we subdivide it in two by adding a point on its boundary edge and snapping 
it to one of the vertices on the boundary curve. We then recursively subdivide each of these newly-created 
triangles in the same way until the curves perfectly match (each curve eventually reduces to a single 
line segment in the base case). See Figure 34 for a visual aid.   Robustness On rare occasions, numerical 
errors due to inexact arithmetic can prevent us from cleanly sewing together surfaces and producing a 
watertight surface mesh. In such situations, we simply add the surrounding cells to the existing list 
of marked cells and repeat the .ood .ll steps in Section 5.4.2, replacing the incorrect geometry with 
the topologically simple isosurface. This correction step typically adds one or two complex cells and 
re-samples slightly more surface geometry as a result.  5.5 Preserving thin sheets with local convex 
hulls As mentioned in Section 5.3, marching cubes templates are not enough to preserve thin features 
in a .uid simulation. As a result, the method described in the previous section cannot preserve thin 
sheets and strands of liquid. However, we can .x this by using more general local grid connectivity, 
similar to the modi.ed marching cubes tables described in Section 5.3.2. In this section, we generalize 
these marching cubes tables by using a convex hull algorithm to automatically compute new surfaces on 
the .y, and we use a di.erent topological argument to ensure that we can sew together our surfaces at 
the cell boundaries. Essentially, we use the same algorithm as de.ned in Section 5.4, but we rede.ne 
how to detect complex cells and reconnect the surface. In this section, the main goal of changing the 
topology of the mesh is to ensure that the liquid surface is connected to other regions of .uid in the 
same way as the pressure values. If these topologies di.er, then distracting visual artifacts will occur. 
For example, if two disconnected surface components lie within the same cell in the .uid grid, then the 
.uid pressure values will be unable to distinguish between these independent components. The low resolution 
.uid velocities will then move both surface pieces together in the same direction, creating an invisible 
link between them that tends to persist for the entirety of the simulation. We want to detect these situations 
and get rid of them by changing surface topology. 5.5.1 De.ning valid topology We detect disagreements 
between the explicit surface mesh and the .uid grid by locally contrasting the topology of the explicit 
surface with a surface that possesses the same topology as the .uid simulation. We de.ne any region where 
these topologies disagree as topologically invalid (note we are creating a direct analogy to the term 
topologically complex from the Section 5.4.2). Before specifying what it means for surface geometry to 
be topologically valid, we .rst de.ne a topological cell as a cube with its eight corners located at 
sample points in the signed distance function (the corners are co-located with the .uid pressure values). 
The cube is bounded by six faces, twelve edges, and eight corners. We can better understand the topology 
of the explicit surface mesh by examining the intersection between the the solid geometries of the triangle 
mesh and each topological cell. This intersection is empty if the cell is completely outside of the surface, 
the intersection is identical to the original cube if the cell is completely inside the surface, and 
the intersection is more complicated if the cell overlaps the surface. In order for the cell to be topologically 
valid, the surface of this intersection should have the same connectivity with its neighbors as the .uid 
cell does, and it must not have any more topological features than a single .uid cell. That is, it must 
contain at most one surface component, with no holes or voids the surface of this intersection should 
be homeomorphic to a sphere (more formally, a 2-sphere). Similarly, the transitions from this topological 
cell to its neighbors must be topologically valid, so the surface intersections with faces and edges 
of this cell should be homeomorphic to 1-spheres (circles) and 0-spheres (intervals), respectively. Lastly, 
a corner of the cell is topologically valid if it is not located in an inside-out region of the surface. 
Figure 38 shows some examples of valid and invalid geometry. We can e.ciently detect the topological 
validity of a cell corner by checking its counter from the signed distance function calculation (Figure 
19). The counter of a valid corner must be either zero or one, otherwise it means the surface is inside 
out or self-intersecting. Next, we determine the validity of a cell edge by counting the number and orientation 
of its intersections with triangles in the surface mesh and comparing the result with a single line segment 
with outward-oriented endpoints (0-sphere). If there are too many components, or if the surface is oriented 
the wrong way at any of the intersection points, then the edge is invalid. This step is performed at 
the same time as the corner validity test (during the creation of the signed distance .eld). We can check 
the validity of a face by computing its intersection with the mesh and counting the number of components, 
and we can test the topological status of a cell by similarly counting connected components and ensuring 
that the Euler characteristic detects no holes. The corner validity test samples the signed distance 
function at several regularly-spaced sample points, and it is guaranteed to identify any self-intersections 
larger than the grid spacing. This test will catch any topological .aws that are well-resolved in the 
x, y, and z dimensions, just like marching cubes will faithfully reproduce a surface as long as there 
are no features smaller than the grid size. The edge validity test checks all of the edges in the grid, 
so it is guaranteed to identify any topological .aws that are well-resolved in at least two dimensions. 
This means that self-intersections and pockets of air that look like thin sheets will be identi.ed by 
the edge validity test. This test can also catch thin spindles and voids if they happen to intersect 
one of the grid edges. The face validity test checks for intersections with all faces in the grid, so 
it is guaranteed to catch topological .aws that look like thin spindles (which span more than one cell 
in a single dimension, but are very thin in the other two). However, the face validity test cannot catch 
topological .aws smaller than a single grid cell unless they happen to intersect the face. Finally, the 
cell validity test is guaranteed to identify all topological .aws, because it checks within every cell. 
 5.5.2 Topological detection in practice In practice, such topological problems smaller than a grid cell 
rarely ever occur, because we start with a well-resolved surface and then smoothly deform it according 
to a low resolution .uid velocity .eld. This means that large features morph into small features through 
a gradual process, by .rst becoming thin sheets and then thin spindles. Because topological inconsistencies 
are caught by lower-dimensional validity tests before they have time to shrink smaller than a grid cell, 
we have not found it necessary to perform the full cell validity test. Face validity tests are mostly 
redundant for our purposes as well, because thin structures eventually intersect cell edges after some 
perturbations from the .uid simulation the edge and corner validity tests tend to quickly catch all 
problems. As a result, we have found it practical to bypass the testing of any cells and faces unless 
we speci.cally have to guarantee valid topology in a particular region, as we will describe shortly. 
To summarize the work done in practice by our implementation, we only perform the topological validity 
tests on the corners and edges for every corner and edge in the .uid grid.  That is, we check all corners 
to see whether they represent inside-out geometry, and we check all edges to see if the surface of their 
solid intersection is homeomorphic to a 0-sphere. To optimize our implementation at the expense of missing 
some small geometry within a single cell, we do not perform topological validity tests within any cells. 
The test to guarantee that the surface of the solid geometry is homeomorphic to a 2-sphere requires computing 
the boolean intersection between a cell and the mesh, counting the number of holes, and then counting 
the number of components. Although this test would help guarantee that all of our geometry is simple 
enough to be represented by a .uid cell, it does not catastrophically break our algorithm if we neglect 
it either. As a further optimization, we do not test every face for topological validity either. Because 
this test is not trivial (it requires intersecting the mesh with a face and counting the number of connected 
components in this intersection), we only perform it at the boundary between topologically valid and 
topologically invalid cells, as explained in the next paragraph. After an invalid cell is detected, we 
will soon replace its intersecting surface with a similar surface that has topologically valid geometry. 
Because this cell shares its boundaries with other cells that may not have been classi.ed as topologically 
invalid, we have to speci.cally guarantee the validity of its faces. In this case, we count the face 
components and pass topological validity information to neighboring cells, similar to the complex cell 
propagation strategy in Section 5.4.2 [69]. To summarize the frequency of topological tests in our implementation: 
we exhaustively test every corner and edge in the signed distance grid, we never check cells, and we 
only check faces when it is absolutely necessary to ensure valid connectivity between an invalid cell 
and its topologically valid neighbors. 5.5.3 Correcting invalid topology After deciding that a cell 
has invalid topology, we must replace the surface/cube intersection in that cell with a new one. We 
require that this new intersection surface meets two constraints: it should be topologically valid, and 
it should cleanly connect with the original surface. In addition, because each vertex of the original 
surface represents a valuable piece of Lagrangian simulation data, we also desire that the new surface 
preserves as many of the original surface vertices as possible. According to our de.nition of topological 
validity, a valid intersection must have a surface which is either either empty or homeomorphic to a 
2-sphere, its intersection with each cell face must either be empty or have a boundary that is homeomorphic 
to a 1-sphere, and its intersection with each cell edge must be either empty or have a boundary which 
is homeomorphic to a 0-sphere. A straightforward strategy for reducing the geometric complexity of a 
surface/cube intersection is to wrap a single surface around all of the original surface components. 
Fortunately, this new surface can be created e.ciently by replacing the original surface/cube intersection 
with its convex hull. If the input surface intersected the cell boundary, then the convex hull preserves 
this connectivity. Furthermore, the convex hull s intersections with faces and edges are lower dimensional 
convex hulls, so they are also topologically valid. In addition, all of the vertices on this convex hull 
are preserved from the input surface, so we reduce re-sampling errors during this process, as illustrated 
in Figure 40. In practice, we use the qhull library [2] to compute the convex hull of the following 
points: all original surface vertices that lie within that cell, the vertices created by intersecting 
the original mesh with the edges and faces of the cell (the type 1 and type 2 vertices described in Section 
5.4.3), and all inside corners of the cell. Finally, because the convex hull represents the intersection 
between a cube and the new surface, we extract the .nal surface by deleting any facets that are co-planar 
with the cell boundary. See Figure 41 for an example. This strategy of reducing invalid surface regions 
to topological spheres has physical implications when used in a .uid simulation: (1) thin sheets of 
air will be detected as topologically invalid and destroyed, and (2) thin sheets and strands of liquid 
will persist throughout the simulation. We will discuss how to further control the .nal surface topology 
in the next section. 5.5.4 Topological control The method presented in Section 5.5.3 will preserve all 
thin sheets and spindles unless there are signi.cant self-intersections. In addition, we found it useful 
to manually perform topological changes to the surface by explicitly cutting o. exceptionally thin spindles 
of liquid. We perform the explicit mesh separation operation explained in Section 5.2.2 from Wojtan et 
al. 2010 [70]. As explained in [67], thin spindles of liquid consistently lead to topological changes 
in real-world liquids. Due to a surface-tension-based Rayleigh-Plateau instability, the liquid spindle 
rapidly collapses until it is extremely thin and then breaks in two. This explicit cutting operation 
precisely mimics this behavior in the discrete setting by separating the thinnest possible unit of a 
discrete surface. This cutting method is also quite e.cient it is detected for free during standard 
surface maintenance operations, and it requires about as much work as a single edge collapse. 5.5.5 
Sewing meshes together In order to sew together the convex hull surface with the original surface outside 
of the topologically invalid region, we follow the exact same steps in Section 5.4.3. However, instead 
of replacing the surface with marching cubes templates in step 4, we replace the surface with a convex 
hull as described in Section 5.5.3. 5.5.6 Smooth surface interpolation Both marching cubes-style lookup 
tables as well as our convex hull algorithm in Section 5.5.3 use piecewise linear surfaces to connect 
vertices together. These low-order connecting surfaces are indistinguishable from the original surface 
mesh when the triangles in the surface mesh are about the same size as a .uid grid cell. However, our 
method allows the surface mesh to have a much higher resolution than the simulation grid, so the triangles 
output by our convex hull will often seem disproportionately large. To maintain a detailed surface with 
many small triangles, we repeatedly subdivide these large triangles until their edges are as short as 
the maximum edge length allowed in our surface mesh. These subdivisions create new vertices, and we are 
free to place them wherever we like. We used butter.y subdivision when placing our new vertices, similar 
to [12]. We found that the modi.ed scheme of Zorin et al. [75] worked best, because a signi.cant portion 
of the vertices in our simulations did not have a valence of six. After applying this interpolating subdivision 
to the triangles output by our topological correction algorithm, the surfaces exhibit both correct topology 
and high degrees of smoothness.  5.5.7 Input parameters This algorithm has relatively few parameters 
to set. The topological detection depends on the grid size, which is identical to the .uid grid resolution 
in this section. We are free to change the topological resolution if we like, but then we lose any guarantees 
about topological validity if the topological grid is not the dual of the .uid simulation grid. The main 
tool for .xing topology in this section is the convex hull, which does not depend on any input parameters. 
The topological algorithm is unconditionally stable, so it does not depend on any minimum time step 
size. Some degenerate con.gurations like completely .at shapes can cause errors when sewing together 
the convex hull to the rest of the surface, but this problem can be solved by jittering vertices or by 
more intelligent collision resolution. The explicit mesh cutting procedure described in Section 5.5.4 
requires the animator to specify the maximum allowed edge length in order to force the breakup of thin 
liquid spindles.   6 Advantages of a mesh surface In the previous sections, we explained how to integrate 
a deforming surface mesh into a .uid simulation while robustly handling changes in the surface topology. 
This mesh-based surface tracking strategy gives us a great deal of advantages over previous methods. 
In this section, we will discuss these bene.ts in detail. 6.1 Color and texture tracking When using .uid 
simulations for visual e.ects and animation, it may be desirable to attach a renderable texture to the 
surface mesh. This is a di.cult problem when using implicit surfaces, as the texture information must 
be carried on the grid and interpolated onto the surface at render time. This approach incurs smoothing 
due to numerical di.usion and interpolation (although work by Bargteil et al. has mitigated these issues 
for grid-based surfaces [3]). For deforming mesh surfaces with unchanging connectivity, maintaining texture 
information per vertex is trivial. For surfaces undergoing remeshing and changes in topology, we require 
a method of assigning texture information to new vertices. For an edge split operation, we could simply 
interpolate texture values from nearby vertices for the newly created vertex (for example, taking the 
average of the values on the two edge end points, or the four new vertex neighbours). A more complicated 
problem occurs when using a surface reconstruction method for dealing with topology changes [45, 69], 
since vertices may be added without any neighbours immediately available. Muller proposes a solution 
to this, noting that each vertex of the new mesh is created by an intersection of a grid edge with a 
triangle [45]. We can compute the texture coordinates of the new vertex as the barycentric sum of the 
texture coordinates of the vertices adjacent to that intersecting triangle. The barycentric weights are 
given by the location where the edge hits the triangle. If multiple triangles intersect an edge, texture 
advection is not well de.ned any more. In that case we choose an arbitrary triangle. Figure 45 shows 
the result of applying this approach in a practical system.  6.2 Preserved surface details  Besides 
maintaining renderable texture information, high-resolution explicit surfaces can also maintain geometric 
detail that would quickly be smoothed out by an implicit surface representation. Figure 46 shows several 
frames from an animation produced by using explicit surface tracking in a grid-based .uid simulation. 
In this particular surface tracking approach (due to Wojtan et al. [69]), the surface is only regularized 
by reconstruction where topology changes occur. Surface patches which do not merge are left alone, allowing 
the rest of the surface to maintain these .ne-scale features. 6.3 Thin features One of the most compelling 
reasons for choosing an explicit triangle mesh over a traditional level set is that triangle meshes can 
capture thin surface features much more easily. For example, numerical dissipation in level set methods 
smooths out high curvature regions and can cause parts of the surface to vanish, even under rigid motion 
(recall the Zalesak sphere test in .gure 8). In fact, very thin yet smooth (low curvature) surfaces, 
such as sheets of liquid, require excessively re.ned grids to avoid this problem. As a quick example, 
consider the Enright test, which is often used to test the accuracy of surface tracking methods [19]. 
A sphere is advected through a smooth, periodic velocity .eld, which stretches the volume to a thin sheet 
and back into a sphere (see .gure 47). The volume is measured at the start and end of one period, and 
the di.erence in volume is taken as a rough estimate of the surface tracking quality. When using a regular 
grid, the surface begins to disappear as its thickness falls below the grid resolution. Using an unmodi.ed 
level set, the original paper by Enright et al. reported a volume loss of 80%, even using a moderately 
high-resolution grid (1003). By contrast, an implementation of explicit, mesh-based surface tracking 
undergoing the same test reports a volume loss of less than 1% [12]. A more extreme extreme example 
of thin feature preservation in a practical .uid animation system is shown in .gure 48. 6.3.1 Thin features 
and robust topology changes Though it o.ers many bene.ts, the presence of thin features also presents 
a set of new challenges. To deal with topological changes, several methods will temporarily construct 
an implicit surface representation on a regular grid, then extract a new mesh surface from this implicit 
surface [15, 3, 45, 69]. This can be done globally, or only locally around inconsistent surface patches. 
This approach results in consistent surfaces and clean topology changes, however it carries some of the 
disadvantages of implicit surface tracking. In particular, thin features in the surface may not be registered 
on the grid, and so will be lost in reconstruction if care is not taken. Much research e.ort over the 
past few years has been devoted to preserving thin features while using these approaches [45, 69, 70]. 
 6.3.2 Resolution mismatch When integrating an explicit surface tracking method into a .uid simulation, 
another dif.culty arises: the mismatch between the resolution of the .uid simulation and that of the 
surface. When the surface is coupled to a standard Eulerian simulator, the liquid volume must .rst be 
resampled onto the simulation mesh or grid to provide geometric information for boundary conditions. 
As this resampling process typically destroys small details, they are invisible to the .uid solver and 
cannot be advanced appropriately. This can lead to a variety of visible artifacts including lingering 
surface noise, liquid behaving as if it were connected when it is not (and vice versa), and thin features 
simply halting in mid-air because the simulator fails to see them. When combined with surface tension 
forces, noisy sub-mesh details can also severely hamper stability if they are not arti.cially smoothed 
out (see section 6.6). Dealing with sub-grid geometry is one of the most critical issues when using explicit 
surface tracking in .uid simulation. Di.erent researchers have approached this problem in di.erent ways. 
Make the surface match the simulation: To handle changes in topology, Wojtan et al. [69] use local implicit 
reconstruction, which tends to regularize the surface mesh and smooth out features below the scale of 
the grid. Similarly, Muller [45] uses a regular grid to construct a new mesh from a given mesh at each 
time step, using a Marching Cubes-like alogrithm. In these approaches, if the implicit surface grid matches 
the simulation grid, then the output of the reconstruction will be resolvable by the simulation. Extensions 
of this idea allow for more sub-grid features than would be allowed by applying traditional Marching 
Cubes during reconstruction [45, 70], but there is always some limit on the complexity of the topology 
within each cell. This complexity limit is what allows the regular grid simulation to still be so e.ective, 
as demonstrated in these papers. This idea has been explored in the course up to this point. Make the 
simulation match the surface: Brochu et al. [11] come at the problem from another direction, and instead 
attempt to capture the surface geometry directly by adaptively placing simulation nodes around the surface 
vertices. The main advantage of this approach is that it treats all surface geometry with the same physical 
rules, and that it can resolve .ne details and complex topologies. The big disadvantages are that it 
requires an irregular simulation mesh, so algorithms designed for regular grids need to be re-thought, 
and it involves expensive Delaunay meshing of the simulation domain at each time step. This approach 
will be described in more detail in section 6.4. Run another simulation on the sub-grid geometry: An 
idea which could complement either of these approaches is to separate grid-scale physics from the sub-grid-scale 
geometry. The usual Eulerian .uid simulation runs on the simulation grid, while a small-scale surface 
tension or wave simulation runs using the surface polygons as simulation elements. This approach was 
most fully realized in work by Thurey et al. [64]. This has the e.ect of regularizing surface noise, 
as well as producing plausible motion for .ne surface features which do not show up on the grid. This 
separation becomes crucial when dealing with surface tension, as we will soon see.  6.4 Matching the 
simulation to the surface In this course we have discussed how to incorporate explicit surface tracking 
into an Eulerian, regular-grid .uid simulation. Although regular grid sims are very popular, unstructured 
and semi-structured meshes have a long history in computational .uid dynamics, and have gained traction 
in computer animation as well. In this section we will see how to construct a simulation that captures 
every detail of an explicit surface using an unstructured mesh. The key idea of this scheme is to carefully 
generate sample points near the liquid surface. These points will be the locations where pressure values 
are stored during the projection step. We then build a Voronoi simulation mesh from these points and 
a background lattice, and apply a ghost .uid/.nite volume pressure discretization which captures the 
precise position of the liquid interface. Coupling this with a standard semi-Lagrangian velocity advection 
scheme and surface tension model completes the .uid simulator. We will discuss the adaptive pressure 
sampling and pressure projection step using Voronoi elements in this section. More details are available 
in the original paper [11].  6.4.1 Adaptive pressure sampling Careful pressure sample placement with 
respect to the surface helps in three important ways. First, we can inform the solver of all local geometric 
extrema, allowing the physics to act upon them correctly. This eliminates the accumulation of erroneous 
surface noise without requiring non-physical smoothing; this is especially vital for incorporating surface 
tension. Second, we can ensure that the solver sees the correct surface topology so that the physics 
responds to merging or splitting only when the surface mesh itself merges or splits (see .gure 49). Lastly, 
grid-scale features often disappear and reappear in regular grid sampling, from the perspective of the 
solver, as the surface translates through the grid. By speci.cally placing points inside such small features, 
we ensure they cannot be missed. We begin by choosing a characteristic length scale for the simulation, 
.x, and con.gure our explicit surface tracking library to try to maintain triangle edge lengths in the 
range [ 1 2 .x, 3 2 .x]. To resolve all surface details with our volumetric mesh, we need to place 
pressure samples so that they capture the surface s local geometric extrema, i.e. around surface mesh 
vertices. In particular, we try to ensure that one edge of the Delaunay triangulation passes through 
each surface vertex, with one sample inside and one outside. Therefore we take Figure 51: Sampling Thin 
Features. A pressure sample is seeded along the outward normal direction from a surface vertex (black 
square). The initial proposed pressure location (empty black circle) would land in the wrong component 
and potentially fail to resolve the intervening air gap. We instead place the .nal pressure sample (.lled 
black circle) midway between the starting vertex and the .rst intersection point (red X). the inward 
and outward normal at each surface vertex (averaged from the incident surface triangles), and attempt 
to place a pressure sample a short distance along each (see .gure 50). We placed outward samples at 
1 2 .x and inner samples at 1 4 .x, though other ratios would work as well. As a result, surface mesh 
normal directions will often align exactly with a velocity sample in the simulation mesh; this lends 
additional accuracy to the vertex s normal motion, and to the incorporation of the normal force due to 
surface tension calculated at the vertex. This placement may miss very thin sheets or other .ne structures: 
to robustly sample such features, we check line segments of length .x from each surface vertex in both 
o.set directions for intersection with the rest of the surface mesh. If we .nd any triangle closer than 
.x, we store the distance d to the closest intersection, and use d in place of .x in the o.set distance 
calculations above (see .gure 51). We further reject new pressure samples which are too close to an existing 
sample by some epsilon, which would cause a very short edge in the .nal mesh. If the distance between 
the surface vertex and the .rst intersection is below some thresh old (e.g. 20 1 .x) at which we consider 
the two surfaces to have e.ectively collided, and the proposed sample is an air sample, we also discard 
it. This is necessary because the divergence constraint is not enforced on air cells, so they can act 
as liquid sinks [40] and destroy liquid volume until the geometry .nally merges. Unfortunately, merging 
in this scenario can often take several time steps to resolve because the interpolated velocity in the 
air gap still averages to zero, thereby preventing surface geometry from actually intersecting and .agging 
a collision. By not placing a sample point in these very small gaps, our simulator treats the two liquid 
bodies as merged and prevents volume loss; the geometric merge is usually then processed within a few 
timesteps. (With regular sampling, merging will depend on where grid points happen to fall with respect 
to the surface; hence the physics can respond as if merged when the surfaces are still as much as .x 
apart, as in .gure 49. This generates non-physical air bubbles which linger for many timesteps before 
they self-collide and are eliminated.) After placing the surface-adapted pressure samples, we complete 
the sampling of the domain by adding regularly-spaced points from a BCC lattice with cell size 2.x, again 
rejecting samples which fall too near existing samples of course, a graded octree or any other strategy 
could also be used to .ll the domain. All samples are then run through a Delaunay mesh generator such 
as TetGen [59]. Figure 52 illustrates in 2D how this sampling approach is able to capture thin features 
such as splashes. Further experimentation with relative mesh spacing parameters could yield improved 
results.  6.4.2 Pressure projection with embedded boundaries We use a .nite volume method on a Voronoi 
mesh for the pressure projection step, similar to Sin et al. [60]. However, rather than applying boundary 
conditions as they describe, we adapt the embedded boundary methods of Batty et al. [5] to Voronoi meshes. 
Conveniently, the duality/orthogonality relationship between Voronoi and Delaunay meshes lets the accuracy 
bene.ts of the method carry over. Figure 53 illustrates our mesh con.guration, and the computation of 
the required weights, as discussed below. We solve the resulting symmetric positive de.nite linear system 
using incomplete Cholesky-preconditioned Conjugate Gradient. To enforce embedded solid boundary conditions, 
we need to estimate the partial unobstructed area of each element face (.gure 53(d)). Batty et al. [5] 
used marching triangles cases for computing tetrahedra face fractions from signed distance values on 
the vertices. However, in the Voronoi setting, the faces are arbitrary convex planar polygons rather 
than triangles. To handle this, we temporarily place an extra vertex at the face centroid, and use it 
to triangulate the face. We then use signed distance estimates at the vertices to compute each sub-triangle 
s partial area, and sum them to determine the partial area for the complete face. The embedded (ghost 
.uid) free surface condition uses signed distance estimates at pressure samples to estimate the surface 
position; these are now located at Voronoi sites rather than tetrahedra circumcenters, but the method 
is otherwise unchanged (.gure 53(c)). A slight improvement can be achieved by casting rays to .nd the 
exact position of the surface (a) (b) (c) (d) (c) Computation of ghost .uid weights on the edges of 
the triangulation. (d) Computation of non-solid weights on the faces of the Voronoi diagram. In 2D, Voronoi 
faces are simply line segments, so solid weights are just fractions of segment lengths. In 3D, Voronoi 
faces are convex polygons, so determining non-solid weights involves computing polygon areas. mesh between 
pressure samples. In some cases this is much more accurate than the estimate derived from signed distances, 
but in practice we found it made minimal visual di.erence. To actually compute the liquid signed distance 
.eld on the tetrahedral mesh, we compute exact geometric distance for a narrow band of tetrahedra near 
the surface, then use a graph-based propagation of closest triangle indices to roughly .ll in the rest 
of the mesh. This family of redistancing schemes is described by Bridson [8], and is easily adapted to 
tetrahedra. Several frames from a complete implementation of this approach is shown in .gure 54.  6.5 
Volume correction As mentioned above, an important characteristic of any good surface tracking package 
is its ability to preserve volume. An additional side bene.t of using an explicit surface is that the 
volume enclosed by a surface can be computed accurately and easily. To take advantage of this, a frequently-used 
correction is to maintain a target volume for each volume of .uid, and as the system gradually loses 
or gains volume, perturb the surface slightly to drive it towards this target volume. To compute the 
volume of .uid, .rst notice that the integral of 1 over a closed domain is equal to the total volume 
of that domain: volume(O) = 1 dx O We transform this volume integral into a surface integral using the 
Gauss divergence theorem, which can be written as: F V F dV F= F  n dS O .O To apply this theorem, 
we specify a function whose divergence equals 1, for example: FFx F (x)= 3 V FF(x)=1 We can now apply 
the divergence theorem to get a formula for volume in terms of a surface integral: Fx Fx volume(O) = 
1 dV = V dV = n dS  33 OO .O Thus we have a method for computing volume using only surface positions 
and normals. Since we are dealing with a triangulated surface, we can compute this integral exactly, 
as: ntriangles 1 volume(O) = (xt1  xt2)  xt3, 6 t where xtj is vertex j in triangle t. To drive the 
surface from its current volume V to a target volume V0, we can apply an o.set in the normal direction 
at each vertex, xi: d =(V0 - V )/A xi = xi + dni where A is the surface area [10, 45]. This is admittedly 
non-physical, but is a convenient way to counter drift in volume due to numerical error. Thurey et al. 
use this idea in an iterative approach, recomputing V and updating the mesh vertex locations until the 
relative error e =(V0 - V ) /V0 is less than some user-de.ned threshold [64]. 6.6 Surface tension Recall 
the Navier-Stokes equations, as introduced previously: .u 1 Vp + .V2 = -(u V)u - u + g .t . V u =0 
Surface tension is incorporated into this mathematical model as an additional force, s., where s is the 
surface tension coe.cient, and . is the mean curvature normal at the surface: .u = -(u V)u - 1 Vp + 
.V2 u + g + s. .t . V u =0 Approaches to surface tension generally fall into two categories: those which 
apply surface tension as a body force in a region around the interface via smeared delta functions [27, 
73, 71], and those which apply surface tension discontinuously at the interface, typically as a boundary 
condition in the pressure projection step. The latter is exempli.ed by the ghost .uid method and related 
approaches [18, 28, 29], and has been shown to provide more realistic results. Surface tension models 
can also be compared in terms of how the force itself is approximated. In level set schemes, .nite di.erences 
are often used to estimate mean curvature, though this can be quite inaccurate without careful modi.cation 
[58], and cannot capture small details. If a surface mesh is available, a more accurate approach is either 
to use meshbased curvature [11, 64], or to model a physical tension directly in the surface mesh geometry 
[51, 10, 71]. 6.6.1 Body force approach We will focus on methods which are uniquely suited to take advantage 
of explicit surface meshes. One idea is to treat surface tension as an actual tension per unit length. 
To accomplish this, we add three forces to a given triangle, corresponding to all neighboring triangles. 
The force is proportional to the surface tension coe.cient times the edge length, in the direction normal 
to the edge and coplanar to the neighboring face (see .gure 55) [51]. (In a two dimensional .uid sim 
with polygons acting as surfaces, we would add two forces to each surface edge, proportional to a surface 
tension coe.cient, parallel to the directions of the two neighboring edges, as in .gure 56.) We note 
this exactly conserves the total momentum of the volume of .uid, since the sum of all applied forces 
is identically zero. If the triangles themselves are simulation elements, the forces can be directly 
applied [10]. If the surface is embedded in a volumetric simulation, the forces can be summed up over 
each surface mesh vertex, and then added into the simulation as a body force [71]. Figure 57 shows a 
viscoelastic simulation incorporating this approach. 6.6.2 Boundary condition approach with an adaptive 
simulation mesh Eulerian .uid simulations can incorporate surface tension by modifying the boundary conditions 
of the pressure projection problem. The projection step involves solving a Poisson problem for pressure, 
p, subject to a boundary condition on the free surface: V2 p = V u subject to: pfs = pair where pfs 
is the pressure at the free surface and pair is the constant air pressure. To incorporate surface tension 
forces, we instead set the free surface pressure pfs = pair + s.fs, where .fs is the (scalar) mean curvature 
of the surface [18]. This ensures that the forces due to surface tension and the internal pressure forces 
are balanced, so no net volume loss occurs. If we are using a level set based surface, the curvature 
can be computed using .nite di.erences. However, we would like to take advantage of the triangle surface 
mesh at our disposal. Brochu et al. used surface-based curvature to enforce the pressure boundary condition 
while using a Ghost Fluid method. They .rst compute a curvature value for each vertex on the surface 
mesh using a local curvature estimate. To transfer these values to the simulation mesh where the pressure 
values are stored, they evaluate curvature at the intersection point between the the triangle mesh surface 
and the line joining an interior pressure sample to an exterior one. If this intersection point does 
not coincide with a surface mesh vertex, they use simple linear interpolation between the vertices of 
the surface triangle mesh. Note that this approach can run into di.culties when using a regular Eulerian 
grid as the underlying simulation mesh. This is another artifact of the resolution mismatch problem described 
earlier. Brochu et al. show disaster results when using curvature measured on a high-res surface inside 
a low-res .uid sim. Because this low-res simulation cannot respond and correct high frequency sub-grid 
details present in the curvature estimates, the simulation becomes unstable almost immediately. Spurious 
noise a.ects the curvature estimates and induces increasingly large yet futile compensating velocities 
that destabilize the simulation. To prevent problems due to spurious surface noise, Brochu et al. rely 
on their adaptive simulation mesh to account for all surface details. Figure 58 shows this approach results 
in accurate, undamped surface tension behaviour. 6.6.3 Boundary condition approach with multiscale surface 
tension An alternative approach to computing the pressure boundary condition was introduced by Sussman 
&#38; Ohta [62] for implicit surfaces, and extended for use on explicit surfaces by Thurey et al. [64]. 
This approach evolves the surface forward by one time step using a volume-preserving mean curvature .ow 
of the liquid interface. The distance of the original towards the evolved surface is then used to compute 
the pressure boundary conditions. This results in better stability than applying a force proportional 
to the curvature measured from the surface. This approach also su.ers from artifacts due to the high-resolution 
surface when attempting to use it directly on a regular grid. Thurey et al. report that their regular 
grid Eulerian .uid solver has trouble resolving features on the order of one or two grid cells. Rather 
than taking the adaptive simulation approach of Brochu et al., they deal with this using a multiscale 
approach to surface tension. We will now brie.y outline the di.erent steps of this algorithm, which are 
also illustrated in .gure 60. For simulating the .uid, we use the standard Eulerian .uid solver with 
an embedded surface mesh. The input to our algorithm is a triangle mesh F representing the liquid interface. 
First, we advect F through the velocity .eld given by the previous step in the .uid simulation, using 
any of the advection methods described in previous chapters. Next, we compute sub-grid cell surface tension 
dynamics by solving a wave equation on the surface. To set up the wave equation, we .rst .lter out the 
surface features of F that are too high resolution to be captured by the .uid grid, in order to create 
a low-resolution smooth surface, S. We then initialize the wave equation problem with wave heights equal 
to the di.erence vectors between points on F and S. We solve the wave equation on F using an implicit 
Newmark scheme, resulting in surface dynamics that make F ripple and oscillate about the smoother surface 
S.  After that, because S has a low enough curvature to be fully resolved on the .uid grid, we use S 
to compute Eulerian surface tension forces in the .uid simulation. We show that surface tension forces 
can be treated as the time derivative of volume-preserving mean curvature .ows in order to allow for 
much larger time steps than with other explicit schemes. We run a volume-preserving mean curvature .ow 
on S proportional to the time step size and surface tension strength, producing another new surface, 
T . The di.erence vectors between T and S give us the surface tension forces that can be included as 
boundary conditions for the Eulerian pressure solve. Finally, we solve this system to get our .nal divergence-free 
velocity .eld and then move onto the next time step. Figure 59 shows an implementation of this approach. 
Please see the paper by Thurey et al. [64] for implementation details.  6.7 The future of mesh-based 
surface tracking Incorporating triangulated surface meshes into .uid simulation is an exciting area of 
research, since it crosses several areas of interest in the graphics community. The literature on mesh 
optimization alone is very deep, and there are many ideas that could be incorporated into future work. 
We end this section with a few suggested topics for future work. Mesh surfaces for particle simulation: 
This course has focused on incorporating triangle mesh surfaces into Eulerian .uid simulations. The 
simulations we have discussed have been mainly on regular grids, or on unstructured simulation meshes 
(tetrahedral or Voronoi). However, there are many popular particle-based methods, such as SPH [47], which 
could bene.t from using an explicit surface. Possible uses could include more accurate surface tension 
and other boundary conditions, or reseeding simulation particles to match the mesh surface. Texture synthesis: 
Beyond advecting texture coordinates as discussed in section 6.1, another possibility is to generate 
textures as the surface deforms and moves. Work by Bargteil et al. [4] and Kwatra et al. [36] could be 
used to generate texture information on a liquid surface. Robust boolean operations: A possible alternative 
approach to handling changes in topology is to perform boolean operations on the surface meshes. Although 
this has traditionally been quite di.cult due to the complexity in handling degenerate geometry and 
robustness in the face of numerical error, some recent work has made this approach a real possibility 
in the near future [6, 65, 50, 13]. Parallel algorithms and real-time applications: Some methods discussed 
in this course lend themselves well to parallel implementions. An avenue of research in the near future 
could be exploring the power o.ered by GPUs and multi-core CPUs to move some of these ideas into the 
real-time domain. Surface-based simulation: Although fairly uncommon in computer graphics, there is extensive 
work in scienti.c computing on using mesh surfaces as simulation elements. Thurey et al. [64] showed 
one approach, running a wave simulation on the surface mesh. Combining the techniques for fast and robust 
remeshing and topology changes we have discussed here with a surface-only simulation is another promising 
area of research.  7 Conclusion This course has outlined the essential steps necessary to implement 
a state-of-the-art Eulerian .uid simulator with mesh-based surface tracking. These course notes have 
been assembled primarily from the lecturers personal experiences while performing related research [10, 
12, 11, 45, 71, 69, 64, 70, 68]. Section 3 of these notes discusses e.ective strategies for embedding 
a deforming surface mesh into a grid-based Eulerian simulator, Section 4 describes the key algorithms 
for maintaining a high quality surface mesh as it deforms, and Section 5 discusses four major strategies 
for allowing the deforming surface mesh to change topology. Each of these strategies has proven e.ective 
for advancing the state of the art in computer animation research, and we hope that these notes will 
help any course attendees implement and experiment with their own mesh-based .uid simulator. While implementing 
a mesh-based .uid surface tracker may require more work than more elementary techniques, the bene.ts 
are enormous. The .nal section of these course notes outlines a great list of advantages to using a mesh-based 
.uid surface tracker. As explained in Section 6, explicit surface tracking allows for better preservation 
of visual detail, guaranteed volume conservation, and a rich class of topological features like thin 
sheets and strands that were not possible with any other simulation method. The goal of this course is 
that these notes are presented in enough detail to help developers implement mesh-based .uid surface 
tracking algorithms for use in motion pictures and video games, and that the notes are thorough enough 
to aid new researchers in building upon the presented techniques. References [1] Bart Adams, Mark Pauly, 
Richard Keiser, and Leonidas J. Guibas. Adaptively sampled particle .uids. ACM Trans. Graph., 26, July 
2007. [2] C.B. Barber and H. Huhdanpaa. Qhull Software Package, 1995. [3] Adam W. Bargteil, Tolga G. 
Goktekin, James F. O Brien, and John A. Strain. A semi-Lagrangian contouring method for .uid simulation. 
ACM Trans. Graph., 25(1):19 38, 2006. [4] Adam W. Bargteil, Funshing Sin, Jonathan E. Michaels, Tolga 
G. Goktekin, and James F. O Brien. A texture synthesis method for liquid animations. In Proceedings of 
the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Sept 2006. [5] Christopher Batty, Stefan 
Xenos, and Ben Houston. Tetrahedral embedded boundary methods for accurate and .exible adaptive .uids. 
In Proceedings of Eurographics, 2010. [6] G. Bernstein and D. Fussell. Fast, exact, linear booleans. 
In Proceedings of the Symposium on Geometry Processing, pages 1269 1278. Eurographics Association, 2009. 
[7] James F. Blinn. A generalization of algebraic surface drawing. ACM Trans. Graph., 1:235 256, July 
1982. [8] Robert Bridson. Fluid Simulation for Computer Graphics. A K Peters, 2008. [9] Robert Bridson, 
Ronald Fedkiw, and John Anderson. Robust treatment of collisions, contact and friction for cloth animation. 
ACM Trans. Graph., 21(3):594 603, 2002. [10] Tyson Brochu. Fluid animation with explicit surface meshes 
and boundary-only dynamics. Master s thesis, University of British Columbia, 2006. [11] Tyson Brochu, 
Christopher Batty, and Robert Bridson. Matching .uid simulation elements to surface geometry and topology. 
ACM Trans. Graph., 29(4):1 9, 2010. [12] Tyson Brochu and Robert Bridson. Robust topological operations 
for dynamic explicit surfaces. SIAM Journal on Scienti.c Computing, 31(4):2472 2493, 2009. [13] M. Campen 
and L. Kobbelt. Exact and robust (self-) intersections for polygonal meshes. In Computer Graphics Forum, 
volume 29, pages 397 406. John Wiley &#38; Sons, 2010. [14] Nuttapong Chentanez, Bryan E. Feldman, Francois 
Labelle, James F. O Brien, and Jonathan R. Shewchuk. Liquid simulation on lattice-based tetrahedral meshes. 
In SCA 07: Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 219 
228, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics Association. [15] J. Du, B. Fix, J. 
Glimm, X. Jia, X. Li, Y. Li, and L. Wu. A simple package for front tracking. Journal of Computational 
Physics, 213(2):613 628, 2006. [16] N. Dyn, D. Levine, and J.A. Gregory. A butter.y subdivision scheme 
for surface interpolation with tension control. ACM transactions on Graphics (TOG), 9(2):160 169, 1990. 
[17] D. Enright, F. Losasso, and R. Fedkiw. A fast and accurate semi-Lagrangian particle level set method. 
Computers and Structures, 83(6-7):479 490, 2005. [18] D. Enright, D. Nguyen, F. Gibou, and R. Fedkiw. 
Using the particle level set method and a second order accurate pressure boundary condition for free 
surface .ows. In Proc. 4th ASME-JSME Joint Fluids Eng. Conf., number FEDSM2003 45144. ASME. Citeseer, 
2003. [19] Douglas Enright, Ronald Fedkiw, Joel Ferziger, and Ian Mitchell. A hybrid particle level set 
method for improved interface capturing. J. Comput. Phys., 183(1):83 116, 2002. [20] Douglas P. Enright, 
Stephen R. Marschner, and Ronald P. Fedkiw. Animation and rendering of complex water surfaces. In Proceedings 
of ACM SIGGRAPH 2002, volume 21, pages 736 744, July 2002. [21] Ronald Fedkiw, Jos Stam, and Henrik Wann 
Jensen. Visual simulation of smoke. In SIGGRAPH 01: Proceedings of the 28th annual conference on Computer 
graphics and interactive techniques, pages 15 22, New York, NY, USA, 2001. ACM. [22] Nick Foster and 
Ronald Fedkiw. Practical animation of liquids. In SIGGRAPH 01, pages 23 30, New York, NY, USA, 2001. 
ACM. [23] M. Garland and P.S. Heckbert. Surface simpli.cation using quadric error metrics. In Proceedings 
of the 24th annual conference on Computer graphics and interactive techniques, pages 209 216. ACM Press/Addison-Wesley 
Publishing Co., 1997. [24] A. Gueziec, G. Taubin, F. Lazarus, and B. Horn. Cutting and stitching: Converting 
sets of polygons to manifold surfaces. IEEE Transactions on Visualization and Computer Graphics, 7(2):136 
151, 2001. [25] D. Harmon, E. Vouga, R. Tamstorf, and E. Grinspun. Robust treatment of simultaneous collisions. 
ACM Trans. Graph. (Proc. SIGGRAPH), 27(3):1 4, 2008. [26] Nambin Heo and Hyeong-Seok Ko. Detail-preserving 
fully-eulerian interface tracking framework. In ACM SIGGRAPH Asia 2010 papers, SIGGRAPH ASIA 10, pages 
176:1 176:8, New York, NY, USA, 2010. ACM. [27] Jeong-Mo Hong and Chang-Hun Kim. Animation of bubbles 
in liquid. Comput. Graph. Forum, 22(3):253 262, 2003. [28] Jeong-Mo Hong and Chang-Hun Kim. Discontinuous 
.uids. ACM Trans. Graph., 24(3):915 920, 2005. [29] Jeong-Mo Hong, Tamar Shinar, Myungjoo Kang, and Ronald 
Fedkiw. On boundary condition capturing for multiphase interfaces. J. Sci. Comput., 31(1-2):99 125, 2007. 
[30] B. Houston, C. Bond, and M. Wiebe. A uni.ed approach for modeling complex occlusions in .uid simulations. 
In ACM SIGGRAPH 2003 Sketches &#38; Applications, pages 1 1. ACM, 2003. [31] X. Jiao, A. Colombi, X. 
Ni, and J. Hart. Anisotropic mesh adaptation for evolving triangulated surfaces. In Proceedings of the 
15th international meshing roundtable, pages 173 190. Springer, 2006. [32] Xiangmin Jiao. Face o.setting: 
A uni.ed approach for explicit moving interfaces. J. Comput. Phys., 220(2):612 625, 2007. [33] Byungmoon 
Kim, Yingjie Liu, Ignacio Llamas, Xiangmin Jiao, and Jarek Rossignac. Simulation of bubbles in foam with 
the volume control method. ACM Trans. Graph., 26(3):98, 2007. [34] ByungMoon Kim, Yingjie Liu, Ignacio 
Llamas, and Jarek Rossignac. Advections with signi.cantly reduced dissipation and di.usion. IEEE Transactions 
on Visualization and Computer Graphics, 13(1):135 144, 2007. [35] Doyub Kim, Oh young Song, and Hyeong-Seok 
Ko. A semi-lagrangian cip .uid solver without dimensional splitting. Computer Graphics Forum (Proc. Eurographics), 
27:467 475, 2008. [36] Vivek Kwatra, David Adalsteinsson, Theodore Kim, Nipun Kwatra, Mark Carlson, and 
Ming Lin. Texturing .uids. IEEE Trans. Visualization and Computer Graphics, 13(5):939 952, 2007. [37] 
J.O. Lachaud and B. Taton. Deformable model with adaptive mesh and automated topology changes. In Proc. 
4th Int. Conference on 3D Digital Imaging and Modeling, Ban., Canada, IEEE, pages 12 19, 2003. [38] William 
E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. 
In SIGGRAPH 87, pages 163 169, New York, NY, USA, 1987. ACM. [39] Frank Losasso, Frederic Gibou, and 
Ron Fedkiw. Simulating water and smoke with an octree data structure. In Proceedings of ACM SIGGRAPH 
2004, pages 457 462. ACM Press, August 2004. [40] Frank Losasso, Tamar Shinar, Andrew Selle, and Ronald 
Fedkiw. Multiple interacting liquids. ACM Trans. Graph., 25(3):812 819, 2006. [41] S. McKee, M.F. Tome, 
V.G. Ferreira, J.A. Cuminato, A. Castelo, Sousa F.S., and N. Mangiavacchi. The mac method. Computers 
&#38; Fluids, 37(8):907 930, 2008. [42] G.H. Meisters. Polygons have ears. American Mathematical Monthly, 
pages 648 651, 1975. [43] Jeroen Molemaker, Jonathan M. Cohen, Sanjit Patel, and Jonyong Noh. Low viscosity 
.ow simulations for animation. In SCA 08: Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium 
on Computer Animation, pages 9 18, Aire-la-Ville, Switzerland, Switzerland, 2008. Eurographics Association. 
[44] Patrick Mullen, Alexander McKenzie, Yiying Tong, and Mathieu Desbrun. A variational approach to 
eulerian geometry processing. ACM Trans. Graph., 26(3):66, 2007. [45] M. Muller. Fast and robust tracking 
of .uid surfaces. In Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 
pages 237 245. ACM, 2009. [46] M. Muller and M. Gross. Interactive virtual materials. In the Proccedings 
of Graphics Interface, pages 239 246, 2004. [47] Matthias Muller, David Charypar, and Markus Gross. 
Particle-based .uid simulation for interactive applications. Proc. of the ACM Siggraph/Eurographics Symposium 
on Computer Animation, pages 154 159, 2003. [48] Stanley Osher and Ronald Fedkiw. The Level Set Method 
and Dynamic Implicit Surfaces. Springer-Verlag, New York, 2003. [49] Stanley Osher and James Sethian. 
Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations. 
Journal of Computational Physics, 79:12 49, 1988. [50] Darko Pavic, Marcel Campen, and Leif Kobbelt. 
Hybrid booleans. Computer Graphics Forum, 29(1):75 87, 2010. [51] Blair Perot and Ramesh Nallapati. A 
moving unstructured staggered mesh method for the simulation of incompressible free-surface .ows. J. 
Comput. Phys., 184(1):192 214, 2003. [52] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. 
Numerical Recipes in C. Cambridge University Press, second edition, 1994. [53] X. Provot. Collision and 
self-collision handling in cloth model dedicated to design garment. Graphics Interface, pages 177 89, 
1997. [54] Elbridge Gerry Puckett, Ann S. Almgren, John B. Bell, Daniel L. Marcus, and William J. Rider. 
A high-order projection method for tracking .uid interfaces in variable density incompressible .ows. 
J. Comput. Phys., 130:269 282, January 1997. [55] Andrew Selle, Ronald Fedkiw, Byungmoon Kim, Yingjie 
Liu, and Jarek Rossignac. An unconditionally stable maccormack method. J. Sci. Comput., 35(2-3):350 371, 
2008. [56] James Albert Sethian. Level Set Methods and Fast Marching Methods. Cambridge Monograph on 
Applies and Computational Mathematics. Cambridge University Press, Cambridge, U.K., 2nd edition, 1999. 
[57] Jonathan R. Shewchuk. What is a good linear element? interpolation, conditioning, and quality measures. 
In 11th Int. Meshing Roundtable, pages 115 126, 2002. [58] Seungwon Shin. Computation of the curvature 
.eld in numerical simulation of multiphase .ow. J. Comput. Phys., 222(2):872 878, 2007. [59] Hang Si. 
TetGen: A Quality Tetrahedral Mesh Generator and Three-Dimensional Delaunay Triangulator, 2006. [60] 
Funshing Sin, Adam W. Bargteil, and Jessica K. Hodgins. A point-based method for animating incompressible 
.ow. In Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA 09, pages 
247 255, New York, NY, USA, 2009. ACM. [61] Jos Stam. Stable .uids. In SIGGRAPH 99, pages 121 128, New 
York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co. [62] Mark Sussman and Mitsuhiro Ohta. A 
stable and e.cient method for treating surface tension in incompressible two-phase .ow. SIAM Journal 
on Scienti.c Computing, 31(4):2447 2471, 2009. [63] M. Teschner, B. Heidelberger, M. Muller, D. Pomeranets, 
and M. Gross. Optimized spatial hashing for collision detection of deformable objects. In Proceedings 
of Vision, Modeling, Visualization VMV03, pages 47 54, 2003. [64] Nils Thurey, Chris Wojtan, Markus 
Gross, and Greg Turk. A multiscale approach to mesh-based surface tension .ows. ACM Trans. Graph., 29(4):1 
10, 2010. [65] C.C.L. Wang. Approximate Boolean Operations on Large Polyhedral Solids with Partial Mesh 
Reconstruction. IEEE transactions on visualization and computer graphics, 2010. [66] Brent Williams. 
Fluid surface reconstruction from particles. Master s thesis, University of Waterloo, 2008. [67] Chris 
Wojtan. Animating Physical Phenomena with Embedded Surface Meshes. PhD thesis, Georgia Institute of Technology, 
2010. [68] Chris Wojtan. Animating physical phenomena with embedded surface meshes. PhD thesis, Georgia 
Institute of Technology, 2010. [69] Chris Wojtan, Nils Thurey, Markus Gross, and Greg Turk. Deforming 
meshes that split and merge. ACM Trans. Graph., 28(3):1 10, 2009. [70] Chris Wojtan, Nils Thurey, Markus 
Gross, and Greg Turk. Physics-inspired topology changes for thin .uid features. ACM Trans. Graph., 29(4):1 
8, 2010. [71] Chris Wojtan and Greg Turk. Fast viscoelastic behavior with thin features. ACM Trans. Graph., 
27(3):47, 2008. [72] Jihun Yu and Greg Turk. Reconstructing surfaces of particle-based .uids using anisotropic 
kernels. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA 10, 
pages 217 225, Aire-la-Ville, Switzerland, Switzerland, 2010. Eurographics Association. [73] Wen Zheng, 
Jun-Hai Yong, and Jean-Claude Paul. Simulation of bubbles. In Proc. of the ACM Siggraph/Eurographics 
Symposium on Computer Animation, pages 325 333, 2006. [74] Yongning Zhu and Robert Bridson. Animating 
sand as a .uid. ACM Trans. Graph., 24(3):965 972, 2005. [75] D. Zorin, P. Schroder, and W. Sweldens. 
Interpolating subdivision for meshes with arbitrary topology. In ACM SIGGRAPH 1996 papers, page 192. 
ACM, 1996. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037645</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>261</pages>
		<display_no>9</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Modeling 3D urban spaces using procedural and simulation-based techniques]]></title>
		<page_from>1</page_from>
		<page_to>261</page_to>
		<doi_number>10.1145/2037636.2037645</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037645</url>
		<abstract>
			<par><![CDATA[<p>Traditionally, modeling urban spaces has been a mostly manual task that consumes significant amounts of resources. With the growing requirements of quantity and quality in urban content, there is an imperative need for alternative solutions that allow for fast, semiautomatic urban modeling. This course explains new modeling techniques for urban environments as an important complement to traditional modeling software. It explains how to use procedural, image-based, and simulation-based techniques to efficiently create highly detailed three-dimensional urban models for computer games, movies, architecture, and urban planning.</p> <p>The course surveys five major topics:</p> <p>&#8226; Urban layouts and road modeling</p> <p>&#8226; Computational building design</p> <p>&#8226; Image-based modeling of facades and buildings</p> <p>&#8226; Urban simulation and visualization work</p> <p>&#8226; Procedural urban modeling in industry</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808994</person_id>
				<author_profile_id><![CDATA[81100473306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wonka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808995</person_id>
				<author_profile_id><![CDATA[81100218141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aliaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808996</person_id>
				<author_profile_id><![CDATA[81502733879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808997</person_id>
				<author_profile_id><![CDATA[81367594473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vanegas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques SIGGRAPH2011CourseProposal 
 Organizers: PeterWonka ArizonaStateUniversity DanielAliaga PurdueUniversity PascalMller 
ProceduralInc. CarlosVanegas PurdueUniversity  Abstract Thiscoursepresentsthestate-of-the-artinurbanmodeling, 
includingthemodelingofurbanlayouts,architecture,image-basedbuildingsandfaades,andurbansimulationandvisualization.Digitalcontent 
creationisa significant challengein manyapplicationsofcomputergraphics.Thiscoursewillexplainproceduralmodeling 
techniquesforurbanenvironmentsasanimportantcomplementtotraditionalmodelingsoftware.Attendeesofthiscoursewilllearn 
proceduraltechniquestoefficientlycreatehighlydetailedthreedimensionalurbanmodels. Thecourseisplannedasa 
half-daycourse, andassumesknowledgeofbasiccomputergraphicstopics,suchastrianglemeshes,hierarchicaldatastructures,texturemappingand 
raytracing,aswellasbasicknowledgeofproceduralmodelingtechniquessuchasparticleandL-systems.Thecoursewillfirstcoverfundamentaltopicsinurbanmodeling,including 
urbanlayoutsandroadmodeling,image-basedmodelingoffacadesandbuildings,andgrammar-basedproceduralmodelingofbuildings.Then,itwilldiscussrecentworksonsimulation-basedtechniquesforurbanmodeling,planning,andvisualization,followedbytheuseofurbanmodelingmethodsinindustry. 
 About the Lecturers Peter Wonka Dr.WonkaisAssociateProfessoratArizonaStateUniversity.HejoinedtheCSEfacultyofASUin2004aftertwoyearsasapost-doctorateresearcherattheGeorgiaInstituteofTechnology.HereceivedhisPh.D.incomputersciencefromtheViennaUniversityofTechnologyin2001andamastersdegreeinurbanplanningin2002.Hisresearchinterestsincludevarioustopicsrelatedtocomputergraphics,suchasreal-timerendering,proceduralurbanmodeling,imageprocessing,andvisualization.PeterWonkaisamemberofthePRISMlab. 
 Daniel Aliaga Dr.AliagaisanAssociateProfessorofComputerScienceatPurdueUniversity.HeobtainedhisPh.D.andM.S.degreefromUNCChapelHillandhisB.S.degreefromBrownUniversity.Dr.Aliaga 
sresearch isprimarilyintheareaof3Dcomputergraphicsbutoverlapswithcomputervision.Hisresearchareaisof 
centralimportancetomanycriticallyimportantindustries,includingcomputer-aideddesign andmanufacturing,telepresence,scientificsimulations,andeducation. 
Todate Prof. Aliagahasover 60 peerreviewedpublicationsandhaschairedandservedonnumerousACM 
andIEEEconferenceand workshop committees. Pascal Mller Dr.Mllerisco-founderandCEOofProceduralInc.,anETHspin-offcompanyspecializedinsoftwarefortheefficientcreationof3Dcities.DuringhisPhDthesisattheComputerVisionLabatETHZurich,PascalMuellerpioneerednovelmethodsfortheproceduralmodelingofarchitecturewhicharenowthecore 
ofProcedural'sflagshipproductCityEngine.Hehaspublishedmorethan35scientificpapersincludingSIGGRAPHandhasheld 
numerous invitedtalksatinternationalconferences,universitiesandcompaniesallovertheworld. BeforehisPhD,heworkedtwoyearsintheindustryastechnicaldirectorforseveralSwiss 
productioncompanies. Carlos Vanegas Carlosisa fourth-yearPhDstudentintheDepartmentofComputerScienceatPurdueUniversityandaresearchassistantatPurdue 
sComputerGraphicsandVisualizationLab. Hisresearch isfocusedonconcurrent behavioralandgeometricmethodsforfastdesignandeditingof3Durbanmodels.Hehaspublished12peer-reviewedpapers,7oftheminurban 
modeling. Carlosobtained his B.S.degree inAppliedMathematicsfromEAFITUniversity(Colombia)in2007,hasbeenanacademicguestat 
theDepartmentofArchitectureatETHZrich,andaninternsoftwareengineeratProceduralInc.  Course 
Contents 1. Introduction (10 min) 2. Techniques for Modeling Urban Spaces  2.1. Urban Layouts and Road 
Networks (30 min) Lecturer:DanielAliaga Topics: Relevant Papers: -Definitionofurbanlayouts -AliagaCGA'08,SGAsia'08,-EditingandsynthesisofurbanlayoutsChenSG'08,GalinEG 
10,LippEG 11-Proceduralroadnetworkgeneration ParishSG'01,VanegasSGAsia'09 2.2. Computational 
Building Design (25 min) Lecturer:PeterWonka Topics: -Shapegrammarsforbuildingmodeling-Example-basedbuildings-Freeformbuildingsandbuildingreshaping 
Relevant Papers: -AliagaTVCG'07,CabralEG'09,MerrellSGAsia'08,SGAsia 10,MllerSG'06,PottmanSG'08,WhitingSGAsia'09,WonkaSG'03,MerrellSGAsia 
10  2.3. Image-based Buildings and Faades (25 min) Lecturers:DanielAliaga,PeterWonka Topics: Relevant 
Papers: -Proceduralfacademodeling-DebevecSG 96,FurukawaCVPR 09,-Image-basedfacademodelingLippSG'08,LiuICCV 
07,-Computervision/LIDARworksMicusikCVPR09,MllerSG'07,PollefeysIJCV'08,WangICCV'07,XiaoSGAsia'08,SGAsia'09,CVPR 
10,VanegasCVPR 10 Break (15 min) 3. Bridging the gap between Urban Simulation and Urban Modeling (40 
min) Lecturer:CarlosVanegas Topics: Relevant Papers: -UrbanSimulation-ChangTVCG 07,DykesTVCG 07, 
(mainapproaches,studycase:UrbanSim)VanegasTVCG 09,SGAsia'09,-UrbanVisualization(Traditionalapproaches) 
WeberEG'09-Simulation-assistedUrbanModeling 4. Procedural Urban Modeling in the Industry (40 min) 
Lecturer:PascalMller+GuestSpeakerfromPixar Topics: Relevant Papers/Tools: -Proceduralcontentcreationtools-ParishSG'01,MllerSG'06-Designandmodelingworkflows-Softimage(Autodesk),Houdini-Procedural3DandGISdataaggregation(SideFX),CityEngine(Procedural),-ProductionpipelinesinentertainmentandRenderMan(Pixar),mentalrayarchitecture. 
(NVIDIA),Unity,UDK(Epic),  Final Remarks &#38; Discussion (10 min) 1. Introduction Modelingtheappearanceandbehaviorofurbanspacesisagreatchallenge.Anurbanspaceisacomplexcollectionofarchitecturalstructuresarrangedintobuildings,parcels,blocks,andneighborhoodsinterconnected 
bystreets.Understanding, describing,and predictingtheappearance(e.g.,creating2D/3Dgeometricmodels)andbehavior(e.g.,simulatingurbandevelopmentovertime)ofcitiesisusefulinagrowingnumberofapplications.Traditionally,modelingurbanspaceshas 
beenarathermanualtaskthat consumessignificantamountsofresources.Withthegrowingrequirementsofquantityandqualityinurbancontent,there 
isanimperativeneed foralternativesolutionsthatallowforfast, semiautomaticurbanmodeling. Urbanmodelingmethodsareimportantinagrowingnumberofapplications.Someofthemare 
. mapping and visualization-reconstructingexistingurbanspacesformappingand navigationtools,visualizing 
previously-existingcities for whichonlypartialdata exists,andallowingarchitectstovisualizeanewcity, 
 . entertainment-fastgenerationofdetaileddigitalcontentforpopulatingurban areasinvideogamesandmovies, 
 . emergency response -creatingmodelstotrainemergencyresponsepersonnelin currentandspeculativeurbanlayouts,includingplanningevacuationroutesfor 
variouscatastrophes,andsuggestingemergencydeploymentsofresources,and . urban planning -predictingoutcomesoflandusepoliciesandtheireffecton 
existing neighborhoods,andcreatinghypotheticalviewsofanurbanspaceafter applyingdevelopmentandgrowthalgorithms. 
 Urbanspacesaredifficulttomodelbecausetheunderlyingstructureisdeterminedbya verylargenumberofhard-toquantifyvariablesincluding 
landpolicies,marketbehavior, transportationinfrastructure,governmentalplans,andpopulationchanges.Moreover,denseurbanenvironmentsare 
particularlycomplextomodelbecausetheyaresimultaneouslydenseandlarge,spanningfromafewtohundreds 
ofsquarekilometers.Whileworksin computervision and photogrammetry havefocusedonthechallengeofacquiringandreconstructingexistingurbanspaces(e.g.,fromLIDARdata,fromaerialandterrestrialimagery),inthiscoursewefocusonthechallengesofsimulating,modeling,andrenderingdigitalcontentofnewurbanspaces.Asopposedtoshortertermsimulationsoftraffic 
flowandcrowdsin3Denvironments,ouruseoftheterm simulationreferstoimitatinglongtermbehaviorsofanurbanspacesuchasurbandevelopment,theresultoflandusepoliciesandtheinfluenceofthetransportationnetwork. 
Ononehand,thechallengingproblemofthe3Dmodelingandrenderingofurbanspacesisbeingtackledincomputergraphics.Thecoursepresentsasummaryofprocedural 
techniquesto generatevisuallyappealing synthetic and real-worldurban scenarios. Ontheotherhand,urbansimulationandvisualizationareusedbyurbanplannersasatoolfor 
decision-makingregardingland use policiesin currentandprojectedurbanareas.Suchurbansimulatorsgeneratelargeamountsofdatathatneedstobeinterpretedbydecisionmakers.Thechallengingproblemofvisualizingsuchdataisaddressedbyseveralworksthatwe 
alsopresent. However,theaboveefforts havebeenlargelyindependent,withcomputergraphicsresearchersfocusingoncomplexandvisuallyappealing3Dmodels,whileurbanplanners 
focusonaccurateurbandynamicsand behaviorally-validated simulations.Thus,this coursealso providesabriefinsightintonewsimulation-based 
techniques thataddressthechallengeofbridgingthegapbetweenthesetwogroupsofapproaches. 2. Techniques 
for Modeling Urban Spaces Thissectionpresentsrecentlydevelopedmethodsforcreatingmodelsofurbanspaces.We 
havegroupedthemethodsbasedonthepartoftheurbanmodelingpipeline(Figure1)thattheyaddressandthedimensionofthegeometricentitiesthattheyproduce.Theurbanmodelingpipelineroughlyconsistsofproducingaroad 
network, subdividingtheblocksextractedfromtheroadnetworkintolots,andgeneratingabuildinginsideeachlot.Differentinputstothisprocesshavebeenusedincludingtarget 
architecturaldesigns,example3Dmodelsorimagery,socioeconomicdatasets,andtensorfields. Figure 
1. General pipeline for modeling urban spaces.  2.1. Urban Layouts and Road Networks Themodelingofroadsandlayoutshasfocusedmostlyoncreatingplausibleaerialimagesandstreetnetworksforurbanspaces.Similartomodelsynthesis,thecreationofaerialimagerybuildsofftheconceptoftexturesynthesisbutinadditiontothesynthesisofpixel 
data,associatedvectordataisalsosynthesized.Thedesign ofthestreetsthemselves attemptstomimicthevisualstyleofthestreetnetworksinreal-worldurbanspaces. 
 Urban Layouts and Urban Layouts and Road Networks Daniel G Aliaga Daniel G. Aliaga Associate Professor 
of Computer Science Purdue University Purdue University  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Contents Contents  L-system modeling Procedural Modeling of Cities, 
Parish et al. 2001  Example-based modeling  Image Analogies, Hertzmann et al. 2001 ag ge,  Example-based 
Urban Layouts, Aliaga et al. 2008   Tensor-based modeling  Procedural Modeling of Streets Chen et 
al 2008 Procedural Modeling of Streets, Chen et al. 2008  Shortest path based modeling Galin et al. 
2010 L bdt l i/h it f ti  Layer-based topology preserving/changing transformations based on graph cuts 
 Lipp et al. 2011 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Urban Layouts and Road Networks Urban Layouts and Road Networks Procedural Modeling of Cities Procedural 
Modeling of Cities -Parish and Mller SIGGRAPH 2001SIGGRAPH 2001 Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Procedural Modeling of Cities Procedural Modeling 
of Cities Input: Various image maps Input: Various image maps Terrain elevation  Population density 
 Population density Output: Urban Model System of highways and streets  Blocks and lots  Building 
geometry   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Procedural 
Modeling of Cities Procedural Modeling of Cities ApproachApproach Road network: Extended Lsystems considering 
global systems considering global goals and local constraints  Global: Street patterns and population 
density  Local: Land/Water/Park boundaries elevation  boundaries, elevation, crossing of streets 
  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Procedural Modeling 
of Cities Procedural Modeling of Cities  Approach Approach Lots: Recursive subdivision algorithm along 
longest edges of lots longest edges of lots  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Procedural Modeling of Cities Procedural Modeling of Cities Approach Approach Buildings: 
Parametric stochastic L-system  One building generated per lot One building generated per lot  Three 
types of buildings: Skyscrapers, commercial, residential  Several modules are used: Extrusion, branching, 
termination  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Procedural 
Modeling of Cities Procedural Modeling of Cities Approach Approach Facades: Division into simple gridgrid-like 
structureslike structures  A layer is formed by two base functions and every layer defines a facade 
element  Stacked layers are used to generate facade textures  generate facade textures  Each style 
texture defined manually (no grammars)    Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Procedural Modeling of Cities Procedural Modeling of Cities LL-systemssystems Generation 
of plants Prusinkiewicz, Lindenmayer; 1990  Environment-sensitive  Prusinkiewicz, James, Mech; 1994 
It ti (O LSt) Interaction (Open L-System) Mech, Prusinkiewicz; 1996 Ecosystems Ecosystems Deussen, et 
al.; 1998  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban 
Layouts and Road Networks Urban Layouts and Road Networks Image Analogies Image Analogies -Hertzmann, 
Jacobs, Oliver, Curless, Salesin SIGGRAPH 2001SIGGRAPH 2001 Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques Image Analogies Image Analogies  TwoTwo-stage design framework 
for imagestage design framework for image modeling: design phase: a pair of images with one image design 
phase: a pair of images, with one image purported to be a filtered version of the other, is presented 
as trainingg data p application phase: the learned filter is applied to some new target image in order 
to create an analogous filtered result Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques Image Analogies Image Analogies  Application to synthesis of novel aerial views Application 
to synthesis of novel aerial views of urban spaces by example Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques  Urban Layouts and Road Networks Urban Layouts and Road Networks 
ExampleExample-based Urban Layout Synthesisbased Urban Layout Synthesis -Aliaga, Vanegas, Benes SIGGRAPH 
Asia 2008SIGGRAPH Asia 2008 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 ExampleExample-based Urban Layoutsbased Urban Layouts Input: Example urban layout Input: Example urban 
layout Images (aerial view)+ Structure (streets, parcels) Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  ExampleExample-based Urban Layoutsbased Urban Layouts Input: Example 
urban layout Input: Example urban layout  Output: New synthesized urban layout that looks like the 
example layout  looks like the example layout  Course: Modeling 3D Urban Spaces using Procedural and 
Simulation-based Techniques  Example-based Urban Layouts Example based Urban Layouts Observation: 
Both image and structure Observation: Both image and structure information about the urban layout available 
Courtesy of Google Maps IImage: aerilial view St t t t l  i Structure: street + parcels Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques ExampleExample-based Urban LayoutsbasedUrbanLayouts 
Approach: Simultaneously synthesizeApproach:Simultaneouslysynthesizestructureandimage I il StStructure:stttreet+parcells 
Image:aerialviiew t Course:Modeling3DUrbanSpacesusingProceduralandSimulation-basedTechniques  Example-based 
Urban Layouts Example based Urban Layouts Input: Example urban layout Input: Example urban layout 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Example-based Urban 
Layouts Example based Urban Layouts Characterize GIS vector data Characterize GIS vector data Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Example-based Urban Layouts 
 Example based Urban Layouts Compute per-parcel imagery Compute per parcel imagery Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques  Example-based Urban Layouts Example 
based Urban Layouts Synthesize new streets Synthesize new streets Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Example-based Urban Layouts Example based Urban Layouts 
 Generate new blocks and parcels Generate new blocks and parcels Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques  Example-based Urban Layouts Example based Urban Layouts 
 Produce new aerial view imagery Produce new aerial view imagery Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques  Example-based Urban Layouts Example based Urban Layouts 
Output: A new synthesized urban layout Output: A new synthesized urban layout  Course: Modeling 3D 
Urban Spaces using Procedural and Simulation-based Techniques  Urban Layouts and Road Networks Urban 
Layouts and Road Networks Interactive Reconfiguration of Urban Layouts Interactive Reconfiguration 
of Urban Layouts Aliaga, Benes, Vanegas, Andrysco IEEE CG&#38;A 2008IEEE CG&#38;A 2008 Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques Interactive Reconfiguration of Urbban 
Layouts  An editor providing tools to An editor providing tools to expand, scale, replace and move parcels 
and blocks of existing layouts parcels and blocks of existing layouts Exploits connectivity and zoning 
of parcels   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Interactive 
Reconfiguration of Urbban Layouts Uses a solver to find a planar transformation Uses a solver to find 
a planar transformation for each tile that best accommodates the changes caused by the editing operations 
changes caused by the editing operations  Two types of error: G Df Gap error + Deformatiti on error 
  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Layouts 
and Road Networks Urban Layouts and Road Networks Procedural Modeling of Streets Procedural Modeling 
of Streets -Chen, Esch, Wonka, Mller, Zhang SIGGRAPH 2008SIGGRAPH 2008 Course: Modeling 3D Urban 
Spaces using Procedural and Simulation-based Techniques  Procedural Modeling of Streets Procedural Modeling 
of Streets ObservationObservation Relation between street patterns and street patterns and tensor field 
  Real street patterns T fi ld tt Real street patterns Tensor field patterns Course: Modeling 3D Urban 
Spaces using Procedural and Simulation-based Techniques  Procedural Modeling of Streets Procedural 
Modeling of Streets Tensor fields Tensor fields Second order symmetric tensor fields Eigenvectors of 
tensor Eigenvectors of tensor values for two orthogonal families Topology Singularities  Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques  Procedural Modeling of Streets Procedural 
Modeling of Streets Tensor fields Tensor fields Second order symmetric tensor fields Eigenvectors of 
tensor Eigenvectors of tensor values for two orthogonal families  Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques  Procedural Modeling of Streets Procedural Modeling of Streets 
 Tensor fields Tensor fields Second order symmetric tensor fields Eigenvectors of tensor Eigenvectors 
of tensor values for two orthogonal families Topology Singularities  Hyperstreamlines   Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Procedural Modeling of Streets 
Procedural Modeling of Streets Tensor field designTensor field design Define basis fields Ti  Combine 
using radial basis functions  Combine using radial basis functions  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Procedural Modeling of Streets Procedural Modeling 
of Streets  System Pipeline Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Procedural Modeling of Streets Procedural Modeling of Streets Example result Example result 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Layouts and 
Road Networks Urban Layouts and Road Networks Procedural Generation of Roads Procedural Generation of 
Roads -Galin, Peytavie, Marechal, Guerin  Eurographics 2010  Eurographics 2010  Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques Procedural Generation of Roads Procedural 
Generation of Roads Overall goal is an interactive algorithm for Overall goal is an interactive algorithm 
for generating a road connecting an initial and a final point that adapts to the characteristics of final 
point that adapts to the characteristics of an input scene. Smoother path Sharper path Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques Procedural Generation of Roads Procedural 
Generation of Roads  Contributions Contributions  a class of parameterized and controllable cost functions 
that takes into account the different parameters/characteristics of the terrain  an efficient method 
to compute a weighted anisotropic shortest path problem using an  anisotropic shortest path problem 
using an optimization over an implicit finite graph  compact procedural models models for representing 
representing  compact procedural for roads, tunnels and bridges with a few parameters describing their 
geometrical characteristics Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Procedural Generation of Roads Procedural Generation of Roads Continuous Cost Function Continuous Cost 
Function Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Procedural 
Generation of Roads Procedural Generation of Roads Discretization of the Cost Function Discretization 
of the Cost Function  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Procedural Generation of Roads Procedural Generation of Roads Results: Results: Without or with bridges 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Procedural Generation 
of Roads Procedural Generation of Roads Results: Results: Altering cost function parameters  Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Procedural Generation of Roads 
Procedural Generation of Roads Results: Results:  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Urban Layouts and Road Networks Urban Layouts and Road Networks Interactive 
Modeling of City Layouts using Interactive Modeling of City Layouts using Layers of Procedural Content 
Lipp Scherzer Wonka and Wimmer Lipp, Scherzer, Wonka, and Wimmer Eurographics 2011 Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques Interactive Modeling of City Layouts 
Interactive Modeling of City Layouts Overall goal is an interactive city modeling Overall goal is an 
interactive city modeling system that is built on persistent editing operations that remain in the space 
of valid operations that remain in the space of valid urban layouts. Editing system supports  Editing 
system supports  Direct control and editing of procedural layouts  Cbi i bl Combining urban layouts 
 Persistent changes   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Interactive Modeling of City Layouts Interactive Modeling of City Layouts Result of moving a street 
using a nave Result of moving a street using a nave approach Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques Interactive Modeling of City Layouts Interactive Modeling 
of City Layouts Result of moving a street using the Result of moving a street using the proposed approach 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Interactive Modeling 
of City Layouts Interactive Modeling of City Layouts Use graph-cut analogy to change the street Use 
graph cut analogy to change the street topology by merging two different urban layoutslayouts Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Interactive Modeling of City 
Layouts Interactive Modeling of City Layouts  Support multiple layers and use their merging Support 
multiple layers and use their merging to combine edits and obtain persistent changeschanges Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Interactive Modeling of City 
Layouts Interactive Modeling of City Layouts Results Results Interactive system  Course: Modeling 3D 
Urban Spaces using Procedural and Simulation-based Techniques Interactive Modeling of City Layouts Interactive 
Modeling of City Layouts Results Results Various editing stages of an example layout  Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques  2.2. Computational Building Design 
Severalworkshaveaddressedtheproblemofgenerating3Dbuildingmodels.Mostofthesemethodsuseshapegrammarsandexample-basedtechniquesforgeneratingbuildingmass,andproposeavarietyoftoolsforefficientuserinteractionandcontrollabilityoftheoutputgeometry. 
 Modeling 3D Urban Spaces Using Procedural and Simulation-Based Techniques Computational Building Design 
 Design Peter Wonka Arizona State University CGACGA-ShapeShape Procedural Modeling of BuildingsProcedural 
Modeling of Buildings Pascal Mueller, Peter Wonka, Simon Haegler, Andreas Ulmer Luc Van Gool Andreas 
Ulmer, Luc Van Gool. Siggraph 2006  Results: Shape Interaction Results: Shape Interaction  Mayan 
Architecture and Temples Mayan Architecture and Temples   Editing of CGA-Shape Grammars Editing of 
CGA Shape Grammars Interactive Visual Editing of Grammars forInteractive Visual Editing of Grammars for 
Procedural Architecture Markus Lipp Peter Wonka Michael Wimmer Markus Lipp, Peter Wonka, Michael Wimmer 
Siggraph 2008 Overview Overview  Problem #1: no direct artistic control  Solution: instance locators 
  Problem #2: text-based grammars  Solution: visual grammar editing  Direct Artistic Control!  
Visual Rule Editing  Modeling Buildings from Floorplans Modeling Buildings from Floorplans ComputerComputer-Generated 
Residential BuildingGenerated Residential Building Layouts Merrell Schkufza Koltun Merrell, Schkufza, 
Koltun Siggraph Aisa 2010  Design Overview Design Overview  Architectural Program dl 3D Model Rooms 
&#38; Adjacencies Set of Floor Plans Learning Structural Relationships Learning Structural Relationships 
 Train a probabilistic graphical Train a probabilistic graphical model. Compactly represents the t t 
fth dt  structure of the data.  Bayesian Network  Nodes probabilities  Edges conditional dependencies 
 Bayesian Network Sample from conditional Sample from conditional distributions Use high level specifications 
  Metropolis Algorithm Metropolis Algorithm Objective Function Objec  Building Layout Constant Cost 
Function  Each iteration, propose a new building layout  Accept with probability   Proposal Moves 
 Proposal Moves  Slide the entire wall  Snap walls together Split into two collinear Split into two 
collinear walls walls Snap walls together The Cost Function The Cost Function . Evaluates the quality 
of the layout Evaluates the quality of the layout  Accessibility Dimension Floor Compatibility Shape 
Term Term Term Term Floor Plan Optimization Floor Plan Optimization  200 2,000 20,000 100,000 Iterations 
 Iterations Iterations Iterations Different Styles of Architecture Different Styles of Architecture 
  Cottage Italinate  Tudor Craftsman Results Results  Results Results  Results Results  Procedural 
Extrustions Procedural Extrustions Interactive Architectural Modeling with Interactive Architectural 
Modeling with Procedural Extrusions Kelly and Wonka Kelly and Wonka ACM TOG  Example  Example Example 
  Example Example  Results Results  Results Results   Deforming Architecture Deforming Architecture 
 StructureStructure-Preserving Reshape for TexturedPreserving Reshape for Textured Architectural Scenes 
Marcio Cabral, Sylvain Lefebvre, Carsten Dachsbacher, George Drettakis  Geometry Reshape piece = set 
of textured faces   one or several openings (portals)  User control  User controls few vertices. 
  System computes other vertices positions.  Trying to satisfy constraints  Preserve wall angles 
P tt  Preserve contacts (e.g. pillars)  Allow some flexibility in edge length  Longg edgges more 
flexible than short ones  As little change as possible   To be preserved  System computes other 
vertices positions. Reasonable behavior  Geometry Reshape  To be preserved Contacts Contacts Flexible 
Flexible Edge lengths  Rellative positions of contacts  Limitations and Future Work  Portals must 
be compatible Portals must be compatible  Detail tiles have strong limitations  33D modldels must 
hhave idindexedd texturedd ffaces  It is the case with most game models  Self collision / Intersection 
  Add feedback from texture riggidityy constraints to geometry  Masonry Building Design Masonry Building 
Design Procedural ModelingofProcedural Modelingof Structurally-SoundMasonryBuildings Whiting Ochsendorf 
Durand Whiting, Ochsendorf, Durand Siggraph Asia 2009  Procedural Buildings for Simulation Procedural 
Buildings for Simulation structurally stable  structurally stable  will look more realistic suitable 
for physical simulations  react to external forces  suitable for physical simulations  Goals Goals 
Generate models that are structurallyy sound  Inverse Statics Special case of brick structures  ParametricModelsasinput 
 Special case of brick structures unstable input stable output Related work structural analyysis 
analyze m aterial s tress analyze material stress  wrong physical model for masonry not deformable vs 
vs.  geometric configuration  rigid block assemblage [Heyman 1995] linear constraint formulation linear 
constraint formulation [Livesley 1978, 1992; RING software]  Overview  procedural building generation 
 procedural building generation analysis method for masonry analysis method for masonry inverse problem 
 inverse problem Optimization loop   Typypical Parameters   building height  thickness of columns, 
walllls, archhes  window size  anggle of flyyingg buttresses  performance  performance  Modeling 
of Facades Modeling of Facades Instant ArchitectureInstant Architecture Wonka, Wimmer, Sillion, Ribarsky 
SIGGRAPH 2003SIGGRAPH 2003  Modeling of Facades Modeling of Facades Input: Target building design 
Input: Target building design  Output: Textured 3D models of building facades  facades  Modeling of 
Facades Modeling of Facades Approach:Approach: Split grammarsSplit grammars  Used instead of L-systems 
  L systems simulate growth in open spaces (better L-systems simulate growth in open spaces (better 
for plants and road networks)  Buildings have stricter spatial constraints and their  Buildings have 
stricter spatial constraints and their structure does not reflect a growth process Modeling of Facades 
Modeling of Facades Facade Subdiv( Y ,3.5,0.3,1r){ firstfloor | ledge | Facade Subdiv( Y ,3.5,0.3,1r){ 
firstfloor | ledge | floors} Floors Reppeat(( Y ,,3){ ){floor}}  Modeling of Facades Modeling of Facades 
 Tile SubdivSubdiv((XY , XY , ){ Wall | Wall ){ Wall | Wall | | A | Wall | } A | Wall | } Tile | 
|  Synthesis of Mass Models Synthesis of Mass Models Continuous Model SynthesisContinuous Model Synthesis 
Merrell, Manocha SIGGRAPH Asia 2008SIGGRAPH Asia 2008  Modeling of Mass with Facades Modeling of Mass 
with Facades Inspired by texture synthesis Inspired by texture synthesis  Output Model Output Model 
 Modeling of Mass with Facades Modeling of Mass with Facades Approach: Approach: maintain adjacency 
constraints between boundary features (e g  between boundary features (e.g. faces, edges, and vertices) 
  create planes parallel to the  planes par faces of the example model that subdivide the space into 
basic components to generate novel models   Modeling of Mass with Facades Modeling of Mass with Facades 
 Example Example  Modeling of Mass with Facades Modeling of Mass with Facades Applied to buildings 
 Applied to buildings  Modeling of Mass with Facades Modeling of Mass with Facades Applied to buildings 
 Applied to buildings  FreeFree-form Architectureform Architecture Procedural Mesh Labeling Procedural 
Mesh Labeling  2.3. Image-based Buildings and Facades Severalproceduralandimage-basedmethodshavebeenproposedformodelingbuildingfacades.Thefirsttypeusesshapegrammarstoproducea3Dmodelofthefaade.Modelsofhighgeometriccomplexitycanbeobtainedbydefiningarelativelysmallsetofruleswiththisapproach.Image-basedmethodsroughlyfallintotwocategories:thoseusingphotographsofreal-worldurbanspacestoreconstructtextured3Dmodelsofsuchspaces,and 
those takingthephotograph ofafaade asareferenceto infer therulesand parametersdescribingthe3Dgeometryofthefaade. 
 ImageImage-Based BuildingsBased Buildings and Facades Peter Wonka Associate Professor of Computer 
Science Arizona State University Daniel G Aliaga Daniel G. Aliaga Associate Professor of Computer Science 
Purdue University  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 ImageImage-Based Buildings and FacadesBased Buildings and Facades Style Grammars for Interactive Visualization 
Style Grammars for Interactive Visualization of Architecture Aliaga Rosen Bekins Aliaga, Rosen, Bekins 
IEEE TVCG 2007 (and IEEE Vis 2005)  Style Grammars for Interactive Visuallization off Archhitecture 
 An example of semi-automatic inverse An example of semi automatic inverse procedural modeling of arbitrary 
buildings Inspiration: Paint-by-Number Inspiration: Paint by Number  Create detailed ppaintinggsbyy 
fillingg in numbered reggions corresppondingg to paint colors. Inspiration: Texture-by-Number Inspiration: 
Texture by Number  Create realistic images using a source photograph and color coding, along withwith 
aa target color coding. target color coding a) b) a ) a)  This work: Build-by-Number This work: Build 
by Number  Create realistic architecture using a source image set and model, along with atargetmodel. 
 System Overview System Overview  1. A ggeometric model is recovered from a spparse imaggeset.  
System Overview System Overview  2. The model is subdivided into feature regions such as brick, windows, 
and doors. Identical or similar features are grouped together. System Overview System Overview  3. 
A face schema, comprising a symbolic growth rule and geometric properties of a single face, is derived 
and used System Overview System Overview  3. A face schema, comprising a symbolic growth rule and 
geometric properties of a single face, is derived and used  F can be represented symbolically as F = 
C1C2 Cn,where Cj = (FF F is a column of subfaces. ) is a column of subfaces (F1jF2j Fmj)  System Overview 
 System Overview  3. A face schema, comprising a symbolic growth rule and geometric properties of a 
single face, is derived and used This face is described by the string F = ABCBCBCBA, with possible growth 
rule F . A(BC)*BA System Overview System Overview  3. A face schema, comprising a symbolic growth 
rule and geometric properties of a single face, is derived and used F = ABCBCBCBA F = ABCBCBCBCBCBCBCBCBA 
F . A(BC)*BA System Overview System Overview  4. Similarly, a floor schema is derived and used as 
well  capptured model model floors System Overview System Overview  5. Similarly, a model schema 
is derived and used as well proper floor 3 proper floor 2 (*) proper floor 1 base floor  System Overview 
 System Overview  6. Features from the original image set are textured onto the new model with occlusions 
removed and shading equalized.  basic rendering basic rendering occlusion-free color equalized System 
Overview System Overview  7. A new model is created and automatically subdivided and colored using 
rules derived from the captured model s subdivision scheme. (new model)  copy and paste design in one 
operation  Video Video Examples   Examples   Examples Examples  Examples Examples   ImageImage-Based 
Buildings and FacadesBased Buildings and Facades Building Reconstruction using Manhattan- Building Reconstruction 
using Manhattan World Grammars Vanegas Aliaga Benes Vanegas, Aliaga, Benes CVPR 2010  Manhattan World 
Buildings Manhattan World Buildings An example of automatic inverse procedural An example of automatic 
inverse procedural modeling of Manhattan-like buildings Manhattan World Buildings Manhattan World Buildings 
 Images of urban areas and street networks all Images of urban areas and street networks all around 
the world are readily available (e.g., Bing Maps Google Maps ) Bing Maps, Google Maps )  Manhattan World 
Obbservation Manhattan World (MW) transitions: Manhattan World (MW) transitions:  24 Manhattan World 
Obbservation  These three transitions can be represented These three transitions can be represented 
with just one rule that we call generalized rewriting rule (GRR):rewriting rule (GRR):  25  Our Approach 
Our Approach Generate initial 3D building envelope Generate initial 3D building envelope Extrude bounding 
box of the building footprint extracted from GIS data extracted from GIS data  26  Our Approach Our 
Approach Generate initial 3D building envelope Generate initial 3D building envelope Extrude bounding 
box of the building footprint extracted from GIS data extracted from GIS data  27  Our Approach Our 
Approach  Generate initial 3D building envelope Generate initial 3D building envelope  Divide initial 
model into a sequence of floors   28  Our Approach Our Approach  Generate initial 3D building envelope 
 Generate initial 3D building envelope  Divide initial model into a sequence of floors hfl ill dif 
  For each floor, automatically modify geometry to match that observed in the images  Assume facades 
have different relative intensities  Use the generalized rewriting rule      29  Our Approach 
Our Approach ExampleExample resultresult  30  Our Approach Our Approach ExampleExample resultresult 
 31  Our Approach Our Approach ExampleExample resultresult  32  Our Approach Our Approach ExampleExample 
resultresult  33  Manhattan World Buildings Manhattan World Buildings  Interactive Coherence Based 
Faade Modeling Musialinski, Wimmer, Wonka  3. Bridging the gap between Urban Simulation and Urban 
Modeling Recently,theareasofcomputergraphicsandofurbansimulationandvisualizationhaveseenaproliferationof 
urban modelingpublications intopconferencesandjournals.However,theeffortsintheseseparatefieldshavebeenlargelyindependent,withcomputer 
graphicsresearchersfocusingoncomplexandvisuallyappealing3Dmodels,whileurban plannersfocusonaccurateurbandynamicsandbehaviorally-validatedsimulations.Someinitialeffortshavebeenmadetowardstheintegrationofgeometricmethodswithurban 
simulationmethods,andfuture workin thisdirectionis desired. Thissectionpresentssomeof suchefforts,andfurthermotivateshowconcurrentbehavioralandgeometricsimulationsignificantlybenefitsthedesign,editing,andpredictionoflarge-scale3Durbanmodels. 
Bridging the Gap between Urban Simulation and Urban Modeling Carlos Vanegas PhD Candidate Purdue University 
  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Bridging the Gap 
between  Urban Simulation and Urban Modeling   Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Bridging the Gap between  Urban Simulation and Urban Modeling  3D 
Model   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Bridging 
the Gap between  Urban Simulation and Urban Modeling   Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Bridging the Gap between  Urban Simulation and Urban Modeling  Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Bridging the Gap between 
 Urban Simulation and Urban Modeling Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Bridging the Gap between  Urban Simulation and Urban Modeling  Course: Modeling 3D Urban 
Spaces using Procedural and Simulation-based Techniques  Bridging the Gap between  Urban Simulation 
and Urban Modeling   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Urban Modeling  Covered in the previous sessions of this course  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Urban Simulation  Very brief overview   Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Simulation  Models 
the behavioral and spatial patterns of urban economic agents  Jobs  Population  Housing  Land use 
  Aims to predict behavior of a city over time  Outputs massive spatially distributed data  Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Simulation  General 
simulation model  Image recreated from: Michael Wegener, Operational urban models state of the art 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Simulation 
  A brief overview of urban simulation paradigms:  Cellular automata  Agent-based models  Dynamic 
microsimulation   Example:  UrbanSim  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Urban Simulation  Cellular Automata Simulate the conversion of non-urban land to urban 
use based on the characteristic of cells and their immediate spatial context  City is represented as 
an arrangement of individual automata in a regular tessellated space  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Urban Simulation  Cellular Automata Transition rules 
determine how automata states adapt over time  Information is exchanged between cells and spread through 
neighborhoods  Do not address changes to the built environment or is occupants, or the travel that 
connects agents  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Urban Simulation  Cellular Automata  Image from: Sharaf Alkheder, Jie Shan, ellular Automata Urban 
Growth Simulation and Evaluation  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Urban Simulation  Cellular Automata Cellular automata and urban simulation Torrens, Sullivan, 
2001 Loose-coupling a cellular automaton model and GIS Clarke, 1998 Fuzzy inference guided cellular 
automata urbangrowth modeling Al-Kheder, Wang, Shan, 2008  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Urban Simulation  Agent-based Models Extended cellular automata framework 
to include mobile, interacting agents in an urban spatial context  Examine cities as self-organizing 
complex systems  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Urban Simulation  Agent-based Models Properties of agents explored with relatively simple behavioral 
rules  Most agent-based urban simulation models have behavior that is influenced only by localized 
context  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban 
Simulation  Dynamic Microsimulation Combination of urban economic analysis with statistical modeling 
of choices made by agents in the urban environment E.g., Households choosing residential location  
 Builds on:  Random Utility Theory (McFadden, 1974)  Discrete choice models   Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Simulation  Dynamic Microsimulation 
Integrated urban models Putman, 1991 General equilibrium models of polycentric urban land use Anas, 
Kim, 1996 A land use model for Santiago City Martinez, 1996  Course: Modeling 3D Urban Spaces using 
Procedural and Simulation-based Techniques  Urban Simulation  Example: UrbanSim UrbanSim: Modeling 
urban development for land use, transportation, and environmental planning Paul Waddell, 2002 Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Watson Urban Simulation 
 Example: UrbanSim Simulates the choices of Individual households  Businesses  Parcel landowners 
 Developers interacting in real estate markets  Course: Modeling 3D Urban Spaces using Procedural and 
Simulation-based Techniques  Urban Simulation  Example: UrbanSim Differs from Cellular Automata and 
agent-based models by integrating Discrete choice methods  Explicit representation of real estate markets 
  Statistical methods to estimate model parameters and to calibrate uncertainty in the model system 
  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Visualization 
 Visualizations of computed datasets Used by regional planning agencies to evaluate Alternative transportation 
investments  Land use regulations  Environmental protection policies   Course: Modeling 3D Urban 
Spaces using Procedural and Simulation-based Techniques  Urban Visualization  Visualizations of computed 
datasets Interest several groups of population with different levels of expertise in handling data Policy 
makers  The public  Modelers running the simulation   Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Urban Visualization  Traditional urban visualization techniques 
 Focused on handling large urban simulation datasets  Making their analysis more intuitive to urban 
planners   In the following, we outline a few representative techniques  Course: Modeling 3D Urban 
Spaces using Procedural and Simulation-based Techniques  Urban Visualization  Traditional urban visualization 
techniques Choroplethic maps: Areas shaded in proportion to the values of the displayed variables  
Example simulation output: Map-based indicator display for Puget Sound region (Total land value per acre, 
2000) Image from: Alan Borning, University of Washington  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques Urban Visualization  Traditional urban visualization techniques Cartograms: 
Distort a map by resizing its regions according to a statistical parameter, but keeping the map recognizable 
 Image from: Daniel Keim, Stephen North, Christian Panse, CartoDraw: A Scanline based artogram !lgorithm 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Urban Visualization 
 Legible Cities Chang, Wessel, Kosara, Sauda, Ribarsky TVCG 2007  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Urban Visualization  Visualize an urban model in 
a focusdependent, multi-resolution fashion, while retaining the legibility of the city Original Model 
45% polygons 18% polygons  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Urban Visualization  Integrate 3D model view and data view Relationships between the geospatial information 
of the urban model and the related urban data can be more intuitively identified  Course: Modeling 3D 
Urban Spaces using Procedural and Simulation-based Techniques  Urban Visualization  Geographically 
Weighted Visualization Dykes, Brunsdon TVCG 2007  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Urban Visualization  Visually encode information about geographic 
and statistical proximity and variation through  geographically weighted (GW)-choropleth maps  multivariate 
GW-boxplots  GW-shading and scalograms   New graphic types reveal information about GW statistics 
at several scales concurrently   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Bridging the gap between urban simulation, visualization and modeling Visualization of 
Simulated Urban Spaces Vanegas, Aliaga, Benes, Waddell TVCG 2009  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques   Bridging the gap between urban  simulation, visualization 
and modeling Infer an urban layout Images (aerial view) + Structure (streets, parcels) from the values 
of a set of simulation variables at any given time step  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Bridging the gap between urban simulation, visualization and modeling 
 Approach Spatially match socioeconomic data set with input aerial images and structure of the urban 
space  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Bridging 
the gap between urban simulation, visualization and modeling Approach Create new structure that matches 
a set of attributes inferred from the simulation variables  New blank lots are created    Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Bridging the gap between 
urban simulation, visualization and modeling Approach !erial view imagery is borrowed from existing 
 lots of the city with similar socioeconomic attributes as the new blank lot  Course: Modeling 3D Urban 
Spaces using Procedural and Simulation-based Techniques   Bridging the gap between urban simulation, 
visualization and modeling Example result  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques   Bridging the gap between urban  simulation, visualization and modeling Example result 
 Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Bridging the gap 
between urban simulation, visualization and modeling Interactive Geometric Simulation of 4D Cities 
Weber, Mller, Wonka, Gross Eurographics 2009  Course: Modeling 3D Urban Spaces using Procedural and 
Simulation-based Techniques  Interactive Geometric Simulation of 4D Cities  Problem: How to model 
cities that are changing over time?  How to use the urban simulation data to infer the geometry of the 
city (roads, lots, buildings)?   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Interactive Geometric Simulation of 4D Cities  Traffic simulation for street generation 
  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Interactive Geometric 
Simulation of 4D Cities  Land use simulation Optimization of a land use value function  Global and 
Local land use goals    Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Interactive Geometric Simulation of 4D Cities  Validation Growth of real cities  Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques Simulation-based techniques Interactive 
Design of Urban Spaces using Geometrical and Behavioral Modeling Vanegas, Aliaga, Benes, Waddell SIGGRAPH 
Asia 2009  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Interactive 
Design of Urban Spaces using Geometrical and Behavioral Modeling Interactive Design of Urban Spaces 
using Geometrical and Behavioral Modeling   Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Interactive Design of Urban Spaces using Geometrical and Behavioral Modeling System Consists 
of N variables defined over a spatial domain  Each variable sampled over a 2D spatial grid G of size 
W x H  Grid j vk(i,j) denotes the value of k-th variable at grid cell (i,j)    Interactive Design 
of Urban Spaces using Geometrical and Behavioral Modeling Operations Location and de-location of behavioral 
variables using location choice and mobility algorithms  Grid User adds population jobs   Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Interactive Design of Urban 
Spaces using Geometrical and Behavioral Modeling  Expansion of Streets  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Interactive Design of Urban Spaces using Geometrical 
and Behavioral Modeling Grid Radial  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Interactive Design of Urban Spaces using Geometrical and Behavioral Modeling  Course: 
Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques  Interactive Design of Urban 
Spaces using Geometrical and Behavioral Modeling Validation of urban simulation model Application: 
UrbanVision  Goal: develop an open-source extension to UrbanSim to include geometric modeling and behavioral 
modeling for use in urban planning scenarios  Course: Modeling 3D Urban Spaces using Procedural and 
Simulation-based Techniques  Application: UrbanVision  Deployment: San Francisco Bay Area 7+ million 
people, 1.5 million parcels, 7000 square miles Deadline: September 2011 Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Application: UrbanVision  Workforce: PI Berkeley 
(lead): Paul Waddell  Ian Carlton , Federico Fernandez, Fletcher Foti, Hyungkyoo Kim, Pedro Peterson, 
Liming Wang, and others  PIs Purdue: Daniel Aliaga, Bedrich Benes  Michel Abdul, Robert Cutler, Ignacio 
Garcia-Dorado, Carlos Vanegas, Innfarn Yoo, and others  Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Why is this system useful? Limitations of existing urban simulation 
systems Difficult to specify what is to be simulated Simulation scenario (time)  Area of interest (space) 
 Example: Real Estate   Why is this system useful? Limitations of existing urban simulation systems 
Difficult to specify what is to be simulated  Why is this system useful?  Limitations of existing urban 
simulation systems Difficult to specify what is to be simulated  Visualization of results  User interaction 
 Limited to tables in databases  Lacks immersive navigation    Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques  Why is this system useful?  Limitations of existing 
urban simulation systems Difficult to specify what is to be simulated  Visualization of results  
User interaction  Isolation  No common framework for integration of different behavioral and geometric 
simulation models  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Goals (1) Open Source  Develop an open-source platform for a highresolution representation and simulation 
of future urban landscapes for use in urban Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Goals (2) Behavioral Simulation  Read, write, and simulate changes to buildings, streets, 
and patterns of urban development and transportation and environmental conditions over time.  ased on 
extension to UrbanSim  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques 
 Goals (3) -Geometric Simulation Model current and future simulated scenarios with geometric structures 
including streets, buildings, vegetation, pedestrians, and vehicles.  Course: Modeling 3D Urban Spaces 
using Procedural and Simulation-based Techniques Goals (4) -Robust Integration  Develop a common API 
to make it easy to interface current and future models and visualization functionality in ways that are 
efficient (fast) and modular now Course: Modeling 3D Urban Spaces using Procedural and Simulation-based 
Techniques  Goals (4) -Robust Integration  Develop a common API to make it easy to interface current 
and future models and visualization functionality in ways that are efficient (fast) and modular later 
  Course: Modeling 3D Urban Spaces using Procedural and Simulation-based Techniques Who will use this 
system?  Initially  Metropolitan Transportation Commission (MTC)  Association of Bay Area Governments 
(ABAG),  Bay Area local governments  Other stakeholders   What for?  To support public engagement 
in the Sustainable Communities Strategies planning process Course: Modeling 3D Urban Spaces using Procedural 
and Simulation-based Techniques  Who will use this system?  Later (hopefully!) City Governments and 
planning agencies  Community  Other research projects in urban simulation/modeling   Course: Modeling 
3D Urban Spaces using Procedural and Simulation-based Techniques  4. Procedural Urban Modeling in the 
Industry Inthispartofthecourse,wewilldiscussthepracticalapplicationofproceduralurbanmodelingmethodsinindustry-rangingfromentertainmentproductiontoarchitecturalandurbandesign. 
First,differentuserinterfaceconceptsforcreatingproceduralenvironmentsareillustrated.ThiswillincludeaspectsfromcommercialtoolssuchasSoftimage,Houdini,Rhino,orCityEngine.ThelatterisdevelopedbyProceduralInc.,andistheresultofnearlyadecadeofurbanproceduralmodelingworkpublishedatSIGGRAPHandotherconferences. 
Second,basedonthelatestproceduraltools,wewillgiveanoverviewofworkflowsfortheparameterizationofbuildingstyles,theconceptualdesignandsustainableplanningofcityscapes,andtheusageofreal-worldcitydatarangingfromfreelyavailable2DGISdatatostate-of-the-artreconstructionsfromobliqueairborneimagery. 
Afterwardswepresenttypicalproceduralmodelingpipelinesinentertainmentproduction(movies,TVandgames)andarchitecturalvisualization.Weshowindustrypracticesandreal-worldusecasessuchastheefficientcreationofdesignproposals,workingwithmassive3Dscenes,rule-basedreportgeneration,instanceandmaterialmanagementforofflinerenderersandgameengines,orcloud-basedstorageandvisualizationofurban3D. 
     Often in productions a 3D model that only looks or feels like a real city is needed. These so-called 
'atmospherically-correct' city models only mimic the architectural style of the building shapes and facades 
but a building does not correspond exactly to a real-world opponent. Often productions do not have access 
to the corresponding building shapes/geometries nor do you want to buy the (expensive) building footprints. 
As a consequence, a typical modeling strategy to reconstruct existing cities at low costs is to: Take 
street data from OpenStreetMap.org and a satellite picture.  Generate blocks from the street data and 
procedurally subdivide the blocks into lots (to roughly the same size as on the satellite picture). 
 Write simple rules which generate the shapes and texture the facades with photos of a few buildings 
common in such a city.  Control the generation process via image maps to fine-tune e.g. landuse (open 
spaces or green spaces) to insert trees etc.  Place manually landmarks at the right places.  Such a 
generation of approximate buildings in combination with the manual placement of landmarks makes it sufficent 
for bird view applications such as helicopter flight simulators. The left picture shows the satellite 
picture on the left, the OpenStreetMap data in the center and the generated city on the right. The right 
picture shows a bird view of the model. The pictures show the procedural modeling solution of GTA Geoinformatik 
GmbH based on CityEngine. Using GTA's detailed real-world GIS data (including faade attributes), buildings 
are generated in different levels of detail which can be adjusted interactively depending on the requirements. 
Buildings with high level of detail feature 3D facade assets as well as roof bricks. The resulting city 
scene consists of 3209 buildings in the center of Munich, Germany. V-City is a project supported by 
the European Commission and aims to research, develop and validate an innovative system integrating the 
latest advances in Computer Vision, 3D Modeling and Virtual Reality for the rapid and cost-effective 
reconstruction, visualization and exploitation of complete, large-scale and interactive urban environments. 
The focus of the project on urban environments is not only made possible by the latest technological 
advances, but also justified. Urban environments represent one of the most important and valuable cultural 
heritage as acknowledged by the UNESCO. More info on http://vcity.diginext.fr. Generally, textures from 
photos lead to more realistic looks and should be used whenever possible. The figures depict possible 
level of detail strategies: Top left: For real cities, airborne 3d models come with low polygon counts 
and low resolution of textures (not suited for groundlevel views). However, since the textures cannot 
be reused, airborne city models require a large texture memory footprint.  Bottom left: For pseudo-real 
cities, groundbased photos on simple geometry (=one quad-polygon per facade with UVs adjusted to the 
corresponding texture) can be used. A few dozens of recitfied facade textures are needed to texture a 
city i.e. the textures can be reused.  Right: Groundbased photos aligned with more complex geometries 
generate realisitc models suited for groundlevel views.  If no photos are available or (realtime) rendering 
performance is important, an asset library with prepared models and textures has to be applied. Based 
on the quality of assets detailed and realistic groundlevel views can be generated (see figures below) 
The figures on top depict possible level of detail strategies using assets. 3d facade models can be easily 
simplified by: Making the facades 2d e.g. by not modeling three dimensional window openings  Leaving 
out small assets completely in low levels of detail  Preparing the assets in different level-of-detail 
representations i.e. the procedural generator selects which level-of-detail of the asset should be inserted 
  The following obstacles make the formalization and parameterization of building designs difficult: 
 Variations: The appearance of buildings is often based on personal design choices of architects. They 
reuse parts of existing design patterns and change other elements according to their aesthetic perception. 
As a consequence, we encounter a lot of arbitrary design variations and exceptions, which make a general 
formalization or classification difficult.  Incompleteness: Architectural sketches or descriptions typically 
only convey selected measurements of interest and do not cover elements of less importance. Another source 
of incompleteness is ruination: ancient buildings often have been destroyed or significantly damaged 
which limits the possibility to gather additional geometric data. Hence, the missing data has to be retrieved 
(often in a tedious task) from multiple sources.  Conflicts: Architectural reference literature often 
has conflicting data. This makes it difficult to combine measurements from different sources. And unfortunately, 
the naming conventions often differ in the literature.  A good strategy to formally break an architectural 
design down comes from one of the most famous architects of the 20th century, Le Corbusier: Mass and 
surface are the elements by which architecture manifests itself. Mass and surface are determined by the 
plan. The plan is the generator. [Le Corbusier 1923, page 47] which means in the first place that usually 
an architectural design can be classified into mass and surface. General concepts for mass and surface 
modeling have been presented already in the previous sections of this course (e.g. mass models are usually 
constructed as an assembly of simple solids to model the basic L-, H-, U-, T-shaped building types, and 
surface modeling usually follows the general subdivision scheme (facade -> floor -> tile -> wall/window). 
In the following, we will present a few guidelines and examples. Top right: General Parcel Subdivision 
Scheme by Jos Duarte, TU Lisbon. Bottom: Same procedural building style on same parcel with different 
procedual footprints guided by parameters such as distance to street or buildings. The Cruciform Skyscraper 
of Le Corbusier are taken as example. Le Corbusier designed, in great detail, several plans of different 
variations between 1920 and 1930. Most famous incarnations can be found in the master plans for his Contemporary 
City (1922) or the Plan Voisin (1925). The (back then) enormous sixtystorey cruciform skyscrapers are 
built on steel frames and encased in huge curtain walls of glass. They housed both offices and the flats 
of the most wealthy inhabitants of the cities. These skyscrapers were set within large, rectangular park-like 
green spaces. In a first step, it is important to name individual elements of the design. In the example, 
we choose the names core, spine, wing, and tooth for the mass model (see figure 5.9). After the components 
have been identified, their spatial dependence has to be analyzed and formalized. Often it is clearly 
obvious how they are assembled, for instance a tooth element is repeated along the wing in our example. 
In the second step, the important parameters of the design have to be identified. It is recommended to 
declare these parameters as control attributes in the rule set. The box shows the control attributes 
for the mass model of the Cruciform skyscraper. For example, the control attribute building_height is 
the overall height of the skyscraper and Wings_n defines how many wings are arranged around the core 
(note that there are paintings from Le Corbusier of the skyscraper design with only three wings). Afterwards, 
the length and width of the wing and spine element are defined and so on. In some productions mass models 
are already available which then has to be refined using procedural techniques. The figure above illustrates 
a hybrid concept where a massing vocabulary has been modeled as simple polygonal assets (shown on the 
bottom row). These have then been inserted at the very beginning of the rule set. Note that the different 
volumes do not influence the overall building design e.g. the horizontal sections are the same -regardless 
of which volume is applied at the beginning.  Hipped roofs have been classified (gable, hip, mansard 
etc) and can be parameterized accordingly. The resulting geometry can be easily extended with additional 
assets. Modern flat roofs consists of a significant amount of different assets ranging from small pipes 
up to large vents and facility constructions. In the remaining sections different use case strategies 
are presented by describing applied pipelines. Although CityEngine is used as procedural content generation 
tool in the presented examples, the pipelines apply to other tools as well i.e. CityEngine could be substituted 
with other procedural generation solutions. The general pipeline of procedural geometry generators: 
 Input: assets and geospatial data (if any)  Output: 3d models generated procedurally by rules or code 
 The figure shows the import/export capabilities of CityEngine which includes the most common industrystandard 
GIS and 3d formats. High polygon counts become less and less of a problem due to today s graphics hardware. 
Depending on the pipeline and end application, the usage of instances is strongly recommended. However, 
please note that it might conflict with point 3, high object counts. In most 3d editing tools, each instance 
is handled as an object (even if the geometry is shared). As a consequence, 100 000 s of instances might 
affect heavily the performance of such tools and are hard to manage. High object counts could be avoided 
by e.g. merging objects with same materials. Therefore, depending on the pipeline and end application 
(again), it is sometimes recommended to expand instances by freezing their vertex coordinates and merging 
them into a few objects (as many as corresponding materials). However, as a consequence, local coordinate 
systems of the merged objects are lost. High material counts have to be avoided generally. Procedural 
operations such as for example thousands of set(material.diffuse.r,rand(0.0,1.0)) operation calls, where 
the red diffuse channel is set with a random number have to be avoided. Instead use discrete random values 
such as for example set(material.diffuse.r,floor(rand(0.0,11.0))/10.0) where only 10 different materials 
are generated. Procedural modeling tools allow for the simple generation of complex material networks 
using different UV sets (texture coordinate layers on geometry). However, the material exchange capabilities 
via industrystandard formats are still limited and furthermore not many formats support multiple UV 
sets. In case complex material setups are generated, it is therefore recommended to export the output 
geometry in the native format of the targeted 3d application. If this is not possible, the widely popular 
Collada or FBX formats are recommended. Collada and FBX support both a.o.: Common shading models  
Ambient, diffuse, specularity as scalar or image  Reflectivity as scalar or image  Bump mapping (not 
normal mapping)  Opacity as scalar or image (some tools interpret it as transparency (=1-opacity)) 
  Multiple UV sets Note however, although specified, that not all tools read these attributes.   
  Note: To procedurally distribute and position prepared model assets, their ratios and scales have 
to be taken into account. Therefore, the procedural generation consists mainly of a selection mechanisms 
deciding which asset to place on a given parcel and how much it can be scaled (if any). Epic's Unreal 
Engine is one of the most popular and successful game engines. Epic s Unreal Development Kit (UDK), an 
free easy-to-use integrated graphic environment for authoring games or interactive architectural content, 
is capable of creating standalone applications/games. Instanced assets can be export from CityEngine 
to UDK via Unreal's t3d format using the Python-based exporter. The t3d format is a simple text-based 
format where the transformations (position, rotation, scale) of the assets is described. A possible production 
pipeline from CityEngine to UDK could consist of the following steps: 1. Prepare your own set of textured 
building assets and street elements 2. In CityEngine, generate a city scene using these assets 3. Define 
report variables to your CGA file that store necessary information such as asset ID, position, scale 
and rotation 4. Define additional render-specific attributes such as shadow, collision or light parameters 
 5. Prepare an exporter Python script that generates a t3d file reading the reported values. 6. Export 
from CityEngine with the python export script enabled 7. In the Content Browser of the Unreal Editor, 
prepare static meshes from the building assets. 8. In Unreal Editor, import the generated t3d file into 
your scene.  With such a pipeline, procedural city scenes can quickly be imported into UDK which allows 
to iteratively design and test different versions of the city layout. And besides instanced geometry, 
additional Unrealspecific metadata such as collision or lighting attributes can be defined arbitrarily 
and are generated and exported per asset instance in the same step.   The ambitious Masdar City will 
rely entirely on solar energy and other renewable energy sources, with a sustainable, zero-carbon, zero-waste 
ecology. The city is being constructed 17 kilometers south-east of the city of Abu Dhabi. The master 
plan for the whole city (6km2) was created by Foster + Partners, combining traditional Arabic city building 
principles, combined with current and future technologies. In the following, the applied procedural design 
principles for the planning of Masdar s quarter Swiss Village are described. The Swiss Village will be 
a distinct neighborhood within Masdar City, which will integrate and serve as the home of Swiss companies 
with expertise in clean technology (210'000m2 of built up area). The development of the architectural 
gestalt of the Swiss Village is being researched at the Chair For Information Architecture at the ETH 
Zurich. The team around Prof Dr. Gerhard Schmitt developed an architectural grammar which incorporated 
the main volumetric principles defined by Foster &#38; Partners. Once the main volumetric language was 
found, the design has been encoded as CGA Shape rules inside CityEngine and the buildings were automatically 
generated accordingly. The rule-based top-down modeling of CityEngine inherently maintains a consecutive 
order in the levels of detail of the created building shapes. To statistically evaluate the quality 
of the designs, rule-based reports have been generated using CityEngine e.g. the impact of changes in 
the mass model parameters to the gross floor area has been measured and accordingly the parametric mass 
model has been optimized to fit the targeted needs. Furthermore, agent-based simulation methods have 
been applied to study the behavior of (note that the figure does not show the Masdar model). More information 
can be found in: Aschwanden G, Haegler S, Halatsch J, Jeker R, Schmitt G and Van Gool L. Evaluation of 
3D City Models Using Automatic Placed Urban Agents. CONVR Conference Proceedings, University of Sydney, 
5-6 November 2009. Solar impact analysis can be done in various commercial tools, in this example Autodesk 
s Ecotect has been applied. After the final parameters for the volumetric design have been determined, 
the basic facade patterns have been encoded as rules as well and offline renderings have been created 
in Vue from Eon Software. Note that it is important in such architectural visualizations (where only 
early concepts are communicated) to produce different design alternatives. Otherwise the client gets 
the impression of seeing a visualization of the final construction.   In this urban planning example, 
traditional 3D and procedural models as well as geo-referenced GIS data are aggregated into a detailed 
model of the city of Rotterdam. The CityEngine screenshot shows on the left the given GIS data and in 
the middle the whole city model with reconstructed 3D buildings as well as procedural streets, trees, 
and buildings. On the right, the new developments are depicted. This example showcases a typical urban 
planning example where various sources of data at different representations and quality are available 
which have to be combined using procedural techniques. The following layers have been included into the 
city model in CityEngine: Traditional 3D layer: Landmarks and building models with oblique textures 
(Collada files) are available in the center of the city and are placed in the scene according to their 
geo-referenced footprints.  Building extrusions: Based on given building footprints with height attributes, 
a ruleset is used to extrude the buildings of the outer area. Generic random texturing of the facades 
is applied for a more realistic look.  Vegetation: A ruleset selects and inserts corresponding tree 
assets on the given real-world locations. Each feature point consists of several attributes such as the 
type and age of the tree. Since the height of a tree is not provided, the ruleset has to compute the 
height according to its age and type.  Procedural streets: Street center lines are used as base for 
the generation of procedural street geometry with the CGA shape grammar. Depending on the needed level 
of detail, additional street elements such as vehicles, pedestrians, street lamps, traffic lights and 
other street furniture can be arbitrarily enabled or disabled for specific areas using control attributes. 
  On the Wilhelminapier, the peninsula in the center of Rotterdam new buildings are constructed. These 
new developments have been designed by magnificent architects Rem Koolhaas, Norman Foster, or Alvaro 
Siza. From these building designs, procedural models have been created with the CGA shape grammar. They 
consist of four levels of detail: extrusions, textured facades, detailed facades and floor plans, and 
detailed interior with tables, sofas, people etc. To model the interiors, the rule set first generates 
the building core and subdivides afterwards the floor plan into rooms and open areas. In the next step, 
the CGA shape grammar is applied to distribute furniture assets along the generated walls and place human 
assets between sofas and chairs and other furniture. A close up of the resulting building model is shown 
in the figure. Afterwards, shadow volumes of existing and planned buildings have been calculated with 
ArcGIS 3D Analyst and exported for visualization. The resulting city model has been exported to mental 
images RealityServer which renders the scene at interactive rates using GPUs in the cloud. Such cloud-based 
rendering solutions allow for a simple communication of the new developments to citizens using standard 
browser technology.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037646</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>22</pages>
		<display_no>10</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[PhysBAM]]></title>
		<subtitle><![CDATA[physically based simulation]]></subtitle>
		<page_from>1</page_from>
		<page_to>22</page_to>
		<doi_number>10.1145/2037636.2037646</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037646</url>
		<abstract>
			<par><![CDATA[<p>This course is as an introduction to the PhysBAM simulation library developed at Stanford University and used in both academic and industrial settings, including Intel Corporation, Industrial Light & Magic, Walt Disney Animation Studios, and Pixar Animation Studios. The course contains information on the release of PhysBAM as well as information on how to obtain the source code, set up the library, and use it to run example smoke and water simulations. It also summarizes a visualization tool and a rendering tool included in the release of the library.</p> <p>Physically based simulation is a vital part of the special-effects toolkit. Traditionally, special effects are obtained by constructing scale or full-size models, whether the scene calls for a sinking ship or a burning building. Unfortunately, this is very expensive and at times not feasible. Physical simulation is also used for special effects in animated films, where traditional special effects methods cannot easily be applied. And it is increasingly used in the gaming community as enhanced processing power enables simulations to occur at real-time rates.</p> <p>In addition to the PhysBAM library, the course explains the underlying techniques that make these simulations possible, in particular level set methods such as fast marching, fast sweeping, and the particle level set method. It also addresses the important aspects of a fluid simulation, including advection, viscosity, and projection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.6.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Physics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P2808998</person_id>
				<author_profile_id><![CDATA[81332496734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pradeep]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dubey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2808999</person_id>
				<author_profile_id><![CDATA[81488648713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809000</person_id>
				<author_profile_id><![CDATA[81100612327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fedkiw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809001</person_id>
				<author_profile_id><![CDATA[81365597634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lentine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809002</person_id>
				<author_profile_id><![CDATA[81300399601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schroeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>201614</ref_obj_id>
				<ref_obj_pid>201609</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Chen, D. Johnson, and P. Raad. Velocity boundary conditions for the simulation of free surface fluid flow. <i>J. Comput. Phys</i>., 116:262--276, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641285</ref_obj_id>
				<ref_obj_pid>641282</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Enright, R. Fedkiw, J. Ferziger, and I. Mitchell. A hybrid particle level set method for improved interface capturing. <i>J. Comput. Phys</i>., 183:83--116, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>329653</ref_obj_id>
				<ref_obj_pid>329646</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. Fedkiw, T. Aslam, B. Merriman, and S. Osher. A non-oscillatory Eulerian approach to interfaces in multimaterial flows (the ghost fluid method). <i>J. Comput. Phys</i>., 152:457--492, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[F. Harlow and J. Welch. Numerical Calculation of Time-Dependent Viscous Incompressible Flow of Fluid with Free Surface. <i>Phys. Fluids</i>, 8:2182--2189, 1965.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>363446</ref_obj_id>
				<ref_obj_pid>359189</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G.-S. Jiang and D. Peng. Weighted ENO schemes for Hamilton-Jacobi equations. <i>SIAM J. Sci. Comput</i>., 21:2126--2143, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S. Osher and R. Fedkiw. <i>Level Set Methods and Dynamic Implicit Surfaces</i>. Springer-Verlag, 2002. New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>114611</ref_obj_id>
				<ref_obj_pid>114609</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Osher and C.-W. Shu. High order essentially non-oscillatory schemes for Hamilton-Jacobi equations. <i>SIAM J. Num. Anal</i>., 28:902--921, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>338913</ref_obj_id>
				<ref_obj_pid>338852</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Peng, B. Merriman, S. Osher, H. Zhao, and M. Kang. A PDE-based fast local level set method. J. <i>Comput. Phys</i>., 155:410--438, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[P. Raad, S. Chen, and D. Johnson. The introduction of micro cells to treat pressure in free surface fluid flow problems. <i>J. Fluids Eng</i>., 117:683--690, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>51348</ref_obj_id>
				<ref_obj_pid>51340</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[C.-W. Shu and S. Osher. Efficient implementation of essentially non-oscillatory shock capturing schemes. <i>J. Comput. Phys</i>., 77:439--471, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Stam. Stable fluids. In <i>Proc. of SIGGRAPH 99</i>, pages 121--128, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>182718</ref_obj_id>
				<ref_obj_pid>182683</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Sussman, P. Smereka, and S. Osher. A level set approach for computing solutions to incompressible two-phase flow. <i>J. Comput. Phys</i>., 114:146--159, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>512588</ref_obj_id>
				<ref_obj_pid>512576</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[G. Tryggvason, B. Bunner, A. Esmaeeli, D. Juric, N. Al-Rawahi, W. Tauber, J. Han, S. Nas, and Y. J. Jan. A front-tracking method for the computations of multiphase flow. <i>J. Comput. Phys.</i>, 169:708--759, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PhysBAM: Physically Based Simulation SIGGRAPH 2011 Course #0181 Date TBD Pradeep Dubey Intel Corporation 
 Pat Hanrahan Ronald Fedkiw Michael Lentine Craig Schroeder Stanford University Abstract This course 
serves as an introduction to the PhysBAM simulation library developed at Stanford University and used 
in both academia and industry, including Intel, Industrial Light + Magic, Disney Animation, and Pixar 
Animation Studios. The .rst part of these notes are aimed at readers interested in using a learning about 
and using a professional numerical simulation library. The course contains information on the release 
of PhysBAM as well as information on how to obtain the source code, set up the library, and use it to 
run example smoke and water simulations. The course also covers a visualization tool and a rendering 
tool included in the release of the library. Physically based simulation is a vital part of the special 
e.ects toolkit. Traditionally, special e.ects are obtained by constructing scale or full size models, 
whether the scene calls for a sinking ship or a burning building. Unfortunately, this is very expensive 
and at times infeasible. Physical simulation is also used for special e.ects in animated .lms, where 
traditional special e.ects methods cannot easily be applied. Physically based simulation is also increasingly 
becoming relevant to the gaming community as increasing processing power enables simulations to occur 
at real time rates. This course is structured as a half day course. We will therefore assume that the 
participants have some basic knowledge of numerical methods and simulation. In addition to discussing 
the PhysBAM library, we will go into detail of the underlying techniques that make these simulations 
possible. In particular, we will discuss level set methods such as fast marching, fast sweeping, and 
the particle level set method. We will also address the other important aspects of a .uid simulation, 
including advection, viscosity, and projection. About the Lecturers Pradeep Dubey Pradeep Dubey is a 
Senior Principal Engineer and Director of Parallel Computing Lab (PCL), part of Intel Labs. His research 
focus is computer architectures to e.ciently handle new application paradigms for the future computing 
environment. Dubey previously worked at IBM s T.J. Watson Research Center, and Broadcom Corporation. 
He was one of the principal architects of the AltiVec* multimedia extension to Power PC* architecture. 
He also worked on the design, architecture, and performance issues of various microprocessors, including 
Intel(r) i386TM, i486TM, and Pentium(r) processors. He holds over 35 patents and has published extensively. 
Dr. Dubey received a BS in electronics and communication engineering from Birla Institute of Technology, 
India, an MSEE from the University of Massachusetts at Amherst, and a PhD in electrical engineering from 
Purdue University. He is a Fellow of IEEE. Pat Hanrahan Pat Hanrahan is a computer graphics researcher, 
the Canon USA Professor of Computer Science and Electrical Engineering in the Computer Graphics Laboratory 
at Stanford University. His research focuses on rendering algorithms, graphics processing units, as well 
as scienti.c illustration and visualization. Hanrahan received a Ph.D. in Biophysics from the University 
of Wisconsin-Madison in 1985. In the 1980s, he worked at the New York Institute of Technology Computer 
Graphics Laboratory, Digital Equipment Corporation, and at Pixar. In 1989, he joined the faculty of Princeton 
University. In 1995, he moved to Stanford University. As a founding employee at Pixar Animation Studios 
in the 1980s, Hanrahan was part of the design of the RenderMan Interface Speci.cation and the RenderMan 
Shading Language. More recently, Hanrahan has served as a co-founder and CTO of Tableau Software. He 
has been involved with several Pixar productions, including Tin Toy, The Magic Egg, and Toy Story. Ronald 
Fedkiw Ronald Fedkiw received his Ph.D. in Mathematics from UCLA in 1996 and did postdoctoral studies 
both at UCLA in Mathematics and at Caltech in Aeronautics before joining the Stanford Computer Science 
Department. He was awarded an Academy Award from The Academy of Motion Picture Arts and Sciences, the 
National Academy of Science Award for Initiatives in Research, a Packard Foundation Fellowship, a Presidential 
Early Career Award for Scientists and Engineers (PECASE), a Sloan Research Fellowship, the ACM SIGGRAPH 
Signi.cant New Researcher Award, an O.ce of Naval Research Young Investigator Program Award (ONR YIP), 
the Okawa Foundation Research Grant, the Robert Bosch Faculty Scholarship, the Robert N. Noyce Family 
Faculty Scholarship, two distinguished teaching awards, etc. Currently he is on the editorial board of 
the Journal of Computational Physics, Journal of Scienti.c Computing, and he participates in the reviewing 
process of a number of journals and funding agencies. He has published over 80 research papers in computational 
physics, computer graphics and vision, as well as a book on level set methods. For the past seven years, 
he has been a consultant with Industrial Light + Magic. He received screen credits for his work on Terminator 
3: Rise of the Machines, Star Wars: Episode III -Revenge of the Sith, Poseidon, and Evan Almighty. 
Michael Lentine Michael Lentine received his B.S. in Computer Science from Carnegie Mellon University 
in 2007. While there, he was an undergraduate research assistant working on computer animation research. 
He is currently pursuing his Ph.D. at Stanford University where he was awarded an Intel Graduate Fellowship. 
He has also been a consultant for Industrial Light + Magic for the last two years working in the research 
and development group on physical simulation. Craig Schroeder Craig Schroeder received his B.S. in computer 
science and mathematics and his M.S. in computer science at Drexel University in 2006. He is currently 
pursuing his Ph.D. in computer science at Stanford University, where he was awarded a Stanford Graduate 
Fellowship. He also works at Pixar Animation Studios and has received a screen credit on Up. 10 minutes: 
The new Intel center I Pradeep Dubey Discussion of the new Intel center and its importance. 10 minutes: 
The new Intel center II Pat Hanrahan Discussion of the importance of the Intel center to academia. 15 
minutes: The new Intel center III. Ronald Fedkiw Brief discussion of PhysBAM and its importance to the 
Intel center. 40 minutes: PhysBAM Tools Library Michael Lentine Release of PhysBAM Tools  Obtaining 
and setting up  Smoke simulation  Visualization  Rendering  40 minutes: PhysBAM Geometry Library 
Craig Schroeder Release of PhysBAM Geometry  Water simulation  Visualization  Rendering  50 minutes: 
Level Set Algorithms Craig Schroeder Level Set Overview  Level Set Equation and Advection  Reinitialization 
 * Fast Marching * Fast Sweeping * Higher Order   Curvature and Surface Tension  Particle Level 
Set  30 minutes: Water Simulation Michael Lentine Navier-Stokes  Advection  Viscosity  Projection 
 Surface Tracking  Contents 1 PhysBAM Tools 7 1.1 SmokeSimulation........................................... 
7 1.2 Visualization ............................................. 7 1.3 Rendering............................................... 
8 2 PhysBAM Geometry 9 2.1 WaterSimulation ........................................... 9 2.2 Visualization 
............................................. 10 2.3 Rendering............................................... 
10 3 Maintaining an Interface with Level Sets 10 3.1 BasicLevelSetProperties ...................................... 
11 3.2 NoteonEulerianandLagrangianFormulations .......................... 11 3.3 LevelSetEquationandAdvection 
................................. 12 3.3.1 Semi-LagrangianAdvection ................................. 
12 3.3.2 HigherOrderMethods .................................... 13 3.4 LevelSetReinitialization ...................................... 
13 3.4.1 LevelSetReinitializationEquation ............................. 14 3.4.2 FastMarching ........................................ 
14 3.4.3 FastSweeping......................................... 16 3.4.4 HigherOrderMethods .................................... 
17 3.5 ParticleLevelSet........................................... 17 4 Practical Methods for Water 
Simulation 18 4.1 Navier-Stokes ............................................. 18 4.2 TimeSplitting ............................................ 
20 4.3 Viscosity................................................ 20 4.4 Projection ............................................... 
21 4.5 SurfaceTracking ........................................... 21 1 PhysBAM Tools The tools part 
of the PhysBAM Library is a set of basic data structures and algorithms. This includes a set of basic 
math tools such as algorithms for vector and matrix operations. Additionally, there are many simulation 
based algorithms such as tools for evolving points forward in time, or advecting velocities through a 
grid. Thus far we have already released the tools part of the PhysBAM library which can be found at http://physbam.stanford.edu/. 
In order to use this library one simply needs to visit the website and download the source code. Then 
simply follow the instructions given online to install the library. Note that in order to do so you will 
need to install the scons build system which is used for the compilation of PhysBAM. Once everything 
is installed there will be an options .le (SConstruct.options) that is installed with the library. This 
.le allows for the use of a wide range of options ranging from the type of library created (static or 
dynamic) to which libraries the tools library depends on. Some the the important options are: USE_MPI 
and USE_LAM which need to be enabled if one wishes to run the code with parallel support, shared which 
can be set to 1 for a shared library or 0 for a static one, and USE_LIBJPEG, USE_BOOST, and USE_FFMPEG 
which determine the libraries that PhysBAM links against. Note that disabling some options such as USE_LIBJPEG 
or USE_FFMPEG reduces the functionality of the PhysBAM library. In particular these options need to be 
enabled by the viewer in order to save frame images to disk. If you are unsure of what to use, the default 
options work well. 1.1 Smoke Simulation In this course we will provide a demo of how to use the tools 
library and in particular this demo will be running a basic smoke simulation. Note however that there 
are a number of other demos that can be downloaded on the PhysBAM website. Once installing the basic 
smoke project from the website there are a number of di.erent options that can be used to run a simulation. 
Perhaps the most useful one is the -scale parameter which allows the speci.cation of the resolution of 
the simulation. One can also specify the dimension of the simulation by using the -1d, -2d, and -3d .ags. 
We have also included a few options to aid with usability such as the ability to restart a simulation 
from a frame. This is particularly useful if the simulation parameters need to be altered but only during 
a small fraction of the simulation. In this case one could simply restart the simulation from the point 
in time when the simulation needs to be altered. We have also included the ability to write out additional 
data using substeps. In order to run a smoke simulation, there are a number of discrete time steps that 
are taken each frame. Using substeps writes out each of these discrete steps which can give a better 
sense of what exactly is going on. However, running with substeps writes out a lot of data which requires 
both a lot of disk space and time. To run a basic simulation simply execute the binary, and for demonstration 
purposes we can simply elect to specify no options and use the defaults. This will then write out data 
at each frame into an output directory. Inside the output directory there are a number of folders: one 
for each frame as well as a common directory. Under each frame directory, the frame dependent data such 
as the MAC grid velocity and smoke density are stored. This data can then be used with either the visualization 
tool or the renderer, both of which are discussed below. Additionally a number of small .les whose contents 
are not speci.c to individual frames are written to the common directory. This is simply a copy of what 
is output to the terminal when the simulation is run but in XML format. There is also a last_frame .le 
which outputs the last frame that was fully completed in the simulation as well as a .rst frame_file 
which outputs where the simulation was started from. Additionally, there is a grid which stores the position 
in space as well as constant information such as the number of cells used in the simulation. With all 
of this data written, we can now view the results. 1.2 Visualization Using these projects one can run 
a simulation of one-phase incompressible .ow, but in order to see the results we have created a basic 
OpenGL viewer. There are three independent viewers, one for each dimension. Because the demonstration 
we ran above was ran with the default arguments, we have two dimensional data and will thus run the opengl_2d 
application. To use it simply run the executable giving the aforementioned output directory from the 
smoke simulation as the argument. This will then bring up a viewer where you will see the grid of the 
simulation. There are many ways to view the data, and we will refer the reader to the PhysBAM website 
for all the details and only go over some of the more basic options in this course. First, to bring up 
a list of possible commands one can simply press the ? key which will then display all possible commands 
along with descriptions. To simply play the simulation from start to .nish one can simply press the p 
key. Other useful commands include the s key which steps one frame forward in time, or the crtl+s keys 
which steps one frame backwards. In order to get a clearer view of the smoke one can press the 6 key 
to show/hide the simulation grid. Although this is fairly intuitive and simple, in general it is much 
simpler to view two dimensional data than three dimensional data so we will also brie.y discuss some 
helpful options which are only applicable for opengl_3d. The most useful feature of the three dimensional 
viewer that lower dimensional viewers do not have is slice mode, which can be enabled using the crtl+h 
keys. This then takes a two dimensional slice of the three dimensional data and shows the result in a 
similar manner to the two dimensional viewer. In slice mode one can also change between which axis to 
slice by pressing the \ key and can cycle through slices using the [ and ] keys. Although this is an 
e.cient and e.ective method of viewing the data, the quality of the OpenGL viewer can be lacking. For 
this we provide a full renderer. 1.3 Rendering In addition to the an opengl based visualization, PhysBAM 
provides a ray tracer which can be used to generate high quality images of the results. Unlike the visualization 
software, the ray tracer can only be used in three dimensions (as one would expect from a typical ray 
tracer). Using the demonstration from above we can then setup a scene .le which de.nes the material properties 
of the smoke and objects (such as the ground) being rendered. The scene .ne is a seperate text .le that 
contains properties for a camera, a set of lights, and a set of objects to be rendered. The Camera is 
de.ned as follows: Camera{ Location= [x y z] Look_At= [x y z] Pseudo_Up= [x y z] Field_Of_View= d Focal_Distance= 
d Aspect_Ratio= d Width= i Height= i Output_Filename= "str" } where x,y,z are .oating point values representing 
each axis in space, d is a .oating point value, i is an iterger value, and str is a string value. Here 
we de.ne a set of attributes that deterine the camera as well as the output resolution and .lename. Next 
we want to de.ne a set of lights. Below is an example of one: Light{ Name="str" Type="Point" Color=[r 
g b] Position=[x y z] Power=i Casts_Shadows=b } where r,g,b are .oating point values between 0 and 1 
that de.ne the color and b is a boolean either set to true or false. This particular light is a point 
light which we can use to light up our scene. Alternative physbam does have support for other light sources 
including spotlights which can be found online. Now that we have a camera and a light we need to renger 
an object and in our case we would like that to be smoke. This can be done with the following: Object{ 
Name="str" Type="Voxel_Data" Grid_Filename="$PHYSBAM/Projects/Smoke/output/common/grid" Density_Filename="$PHYSBAM/Projects/Smoke/output/%d/density" 
Volume_Shader="SmokeShader" Volume_Step=d } where SmokeShader is a material shader for the smoke that 
we will de.ne below. In this object provide the path to our grid and density data from the example simulation. 
In this case $PHYSBAM refers to the path of the base directory PhysBAM was installed in. The SmokeShader 
is de.ned as: Volume_Material{ Name="SmokeShader" Type="Voxel_Shader" Absorption=i Scattering=i Inscattering_Amplification=i 
} This de.nes the shader for the smoke with prespeci.ed values for absorbtion and scattering. The higher 
the absorbtion the thicker the smoke will look, the higher the scattering the darker the smoke will look, 
and the higher the inscattering the brighter the smoke will look. Once we have this scene .le we can 
use it to ray trace the smoke example we have generated. To do this we simply run the ray_tracing_nocona 
binary and supply the scene .le we have generated as an argument. The results then get saved to the speci.ed 
.lename and can be viewed with any stadard image display software. PhysBAM supports a variety of formats 
for output including png, jpg, ppm, etc.  2 PhysBAM Geometry The geometry level of the PhysBAM library 
is scheduled for release at SIGGRAPH 2011 alongside this course. The geometry level of the PhysBAM library 
builds on top of the Tools layer. It introduces many usedful geometric primitives, such as triangulated 
surfaces, tetrahedralized volumes, spheres, and tori. These primitives are useful for both numerical 
simulation and for later visualization and rendering. The geometry level also includes support for level 
sets, including commonly used algorithms for .uid simulation and the particle level set method. The Geometry 
level includes all of the major storage types from the larger Phys-BAM library. Among other things, this 
means the released visualization and rendering tools are essentially feature complete at the Geometry 
level. The geometry library can be obtained from http://physbam.stanford.edu/. Setup and compilation 
instructions are the same as for the tools library. Note that the two libraries are intended to be installed 
side by side and compiled together. 2.1 Water Simulation The geometry level extends the smoke simulation 
facilities from the tools layer with the addition of level sets and corresponding level set algorithms, 
including the particle level set method. The sample simulator handles single-phase incompressible water 
simulations. Its usage is similar to that of the smoke simulator. This course will provide a demo of 
how to run a basic water simulation. 2.2 Visualization The visualization utility is capable of handling 
much more than was was required for the smoke simulator. In this portion of the course, we will demonstrate 
the usage of the visualization utility for visualizing other types of objects at the geometry level: 
 Level sets  Single-phase water  Triangulated surfaces  Tetrahedralized volumes  2.3 Rendering The 
rendering utility is intended to render the same data that the visualization tool views. This portion 
of the course will demonstrate some of the features of the rendering utility, including setting up and 
rendering additional types of objects introduces at the geometry level: Level sets  Triangulated surfaces 
 Simple primitives (spheres, cylinders, etc.)   3 Maintaining an Interface with Level Sets In this 
course, interfaces will be used to represent the free surface of water or some other .uid. As water splashes 
around, parts of it may pinch o. and merge back in. In this way, the interface may undergo frequent and 
often complex topological changes over time. There are many potential ways of representing an interface, 
and the method that is most suitable depends on the application. An obvious way to represent a surface 
is to track it with a surface mesh or with marker particles. This .rst approach is called front tracking 
and has received a signi.cant amount of attention, which is summarized well in the survey paper [13]. 
Front tracking has the advantage that the surface is easily updated based on the .uid velocity. Unfortunately, 
robust front tracking requires remeshing as well as additional routines to handle the potentially complex 
topological changes the surface may undergo. For these reasons, front tracking has fallen out of favor 
for many applications. The second approach is to track the interface using marker particles [4, 1, 9]. 
An alternative to front tracking is the use of level sets (see [6] for a good overview). A level set 
is a signed distance function stored on a computational grid. That is, there is a function f de.ned on 
each cell of a grid. Its magnitude |f| indicates the distance from the grid node to the interface. The 
level set s sign determines whether a point is inside the interface (f< 0), outside the interface (f> 
0), or on the interface (f = 0). A level set is an implicit representation of a surface, since the actual 
location of the surface is not represented directly. Rather, the location of an interface along a segment 
can be computed by computing the location where f = 0. Level sets provide very e.cient answers to inside-outside 
queries and handle topological changes robustly without any special consideration. This part of the course 
is focused on steps involved in maintaining a surface with level sets. This is followed by a description 
of the particle level set method, an extension of the level set method designed to reduce the problem 
of volume loss in .uid simulations. 3.1 Basic Level Set Properties The magnitude IVfIis the rate of change 
of f along the gradient direction, which is the direction of greatest increase. Since f represents a 
signed distance function, moving one unit closer or further from the interface along the gradient direction 
will make f one unit larger or smaller, so that IVfI= 1. Another useful property to observe about level 
sets is that the local normal can be computed as n = Vf. Since IVfI= 1, this expression for n is already 
properly normalized. 3.2 Note on Eulerian and Lagrangian Formulations Before describing the dynamical 
aspects of level sets, it is helpful to introduce the two high level approaches to describing dynamical 
systems. The most familiar of these is the Lagrangian description. In a Lagrangian description, a system 
is described in the frame of reference of what is being observed. An observer .oating down a river on 
a raft privides a Lagrangian description of the water in the river. When the observer measures the temperature 
of the water, the same volume of water is being measured. If the temperature is observed to rise, it 
implies that the water itself is being warmed. One may contrast this with the Eulerian description, where 
the observer remains .xed in space. An observer who describes the water from the river s banks provides 
an Eulerian description of the water in the river. When this observer measures the water s temperature, 
the volume of water being measured changes from moment to moment as the water .ows downstream. When this 
observer records a rise in the water temperature, it may be caused by a combination of two sources. As 
in the case of the observer on the raft, the water may be warming. Alternatively, a patch of warmer water 
may be .owing past the thermometer, but the temperature of any volume of water may not be changing. This 
highlights an important di.erence between the two descriptions. The relationship between the two observers 
is entirely analogous to two di.erent types of derivatives. When a Lagrangian observer measures what 
is called the material derivative, which is basically just a total derivative. The total derivative of 
a quantity . can be de.ned as D. d. .. dt .. dx .. dx .. dx .. ==+ + + =+ u V.. Dt dt .t dt .x dt .x 
dt .x dt .t The notation V. represents the gradient of .. The change that an Eulerian observer measures 
is the partial derivative. In the context of the water s temperature, the relationship is DT .T =+ u 
VT. Dt .t The observer on the river s banks observes .T DT (thermometer change) = = -uVT = (temperature 
change of water)-(change due to moving water). .t Dt Consider a one-dimensional river .owing to the right 
with velocity u = 1. The water itself is not changing in temperature, but it has a thermal gradient. 
The water upstream is warmer upstream (to the left), and its temperature changes by one degree for each 
additional meter upstream, so that VT = -1. The derivative is negative because the temperature is smaller 
when observed farther to the right. Now, .T DT = -u VT =0 -(1)(-1) = 1. .t Dt Thus, one should expect 
the temperature of the water .owing past the river s banks to rise at a rate of one degree per second, 
which is what one would expect. We now have a relationship between Eulerian and Lagrangian decriptions. 
We will use an Eulerian description throughout these notes, since that is the preferred description for 
.uids. We point out this relationship because it will occasionally be convenient to utilize the Lagrangian 
description. 3.3 Level Set Equation and Advection As the .uid moves, its interface will move, too. To 
see more clearly how the interface moves based on the velocity of the .uid, consider the .uid velocity 
as composed of two orthogonal components, u = unn + ui, where un is the magnitude of the velocity along 
the normal direction n and ui is the portion of the velocity orthogonal to the normal direction. This 
component of the velocity .ows along the .uid s interface. The tangential term satis.es the property 
n  ui = 0. Next, we consider the e.ect each of the two components individually. Then, we will put the 
two pieces together to form the .nal level set evolution equation. First, consider that the velocity 
points in the normal direction. That is, u = unn. This case can be compared waves as they advance onto 
a beach. The water .ows in a direction orthogonal to the leading edge of the wave. In this case, the 
leading edge of the wave moves forward at the same velocity as the .uid. If the wave is observed over 
a small time window .t, then the .uid, and its leading edge, will move a distance of .tun. The normal 
direction points out of the .uid, so when the wave is advancing onto the beach, un > 0. When the wave 
is retreating back out to sea, un < 0. Next, consider a rock sitting in the sand of the same beach. At 
any point in time, one can measure the distance of this rock from the waves s leading edge as it advances 
onto the beach. If the rock is out of the water, then f> 0. Let fn be the measurement at the beginning 
of the time window and fn+1 be the measurement after the time window. As the wave advances along the 
beach towards the rock, the distance between the rock and the water will decrease. The amount by which 
it decreases is the distance that the water advanced, so that fn+1 - fn = -.tun. Note that if the rock 
were submerged, then f< 0, and the distance increases as the wave advances further along the beach, so 
that |f| is observed to increase. Again, we conclude that fn+1 - fn = -.tun. Next, consider that the 
.uid velocity is along the .uid s interface. That is, u = ui. This is the situation that is observed 
as water .ows down a river. The water moves along the bank of the river, and the interface between the 
land and water is not observed to change. As in the case of the beach, a rock on the bank of the river 
can serve as a reference point from which we can measure the signed distance f to the water over some 
interval of time. This time, however, we observe that the distance does not change, so fn+1 = fn . Finally, 
consider the general case. The velocity contains a component in the normal direction and a component 
along the interface. This will cause a displacement .tu over the course of a small window of time. If 
we divide this displacement into two parts, a displacement .tunn in the normal direction and a displacement 
.tui along the interface, we can consider the change in f separately from each piece. The normal component 
changes the signed distance according to fn+1 - fn = -.tun, and the tangential component leaves it alone. 
Thus, the interface moves as fn+1 - fn = -.tun for any velocity .eld. Dividing through by .t and taking 
the limit as .t . 0 produces ft + un = 0. Noting that un = n  u and n = Vf produces the equation ft 
+ u Vf =0. (1) This is an advection equation, where the level set is being advected with the .uid velocity. 
The advection equation is very important and occurs frequently. The methods used to solve the advection 
equation do not depend on the quantities being advected, whether they be scalars like density or a level 
set or vectors like momentum or velocity. What remains of this section on level set advection does not 
assume that the quantity being advected is a level set. 3.3.1 Semi-Lagrangian Advection A popular means 
for solving advection equations is called semi-Lagrangian advection and was introduced to the graphics 
community by [11]. It is baesd on the observation that advection is very simple when viewed in a Lagrangian 
viewpoint. Indeed D. = .t + u V. =0. Dt We use . here instead of f to emphasize that the quantity being 
advected need not be a level set. In the Lagrangian description, the advection equation simply says that 
the particles themselves are not changing. Any observed change in the grid is due to the .uid moving 
around. Consider a particular location in the n+1 grid xwhere the value of .n+1 is to be determined at 
the end of the time step. If . is stored at cell centers, then xn+1 will be chosen to be at a cell center. 
If one imagines a particle located at xn+1 at the end n of the time step, the location of this particle 
could be traced back to some location xat the beginning of the time step. At the beginning of the time 
step, that particle would have had a . value of .n(xn), by which n we mean the time n .eld . sampled 
at location x. Since the value of . attached to any particle does not change, we should expect the . 
value at the end of the time step to be .n+1(xn+1)= .n(xn). This is the central observation in semi-Lagrangian 
advection. What remains are the details of how to discretize this. nn+1 The .rst question that must be 
addressed is how to compute xfrom x. The simplest is to assume that it has been moving for the entire 
time step with the .uid velocity u at some location in the grid. Since xn+1 lives at conveniently chosen 
locations in the grid, it is convenient to use the velocity at the same nn+1 - .tu. location. Then, x= 
xThis approximation is .rst order accurate. n Normally, the location xwill not lie at a convenient location 
in the grid, and its value must be interpolated from nearby . values in the grid. Linear interpolation 
is frequently used for this step. Linear interpolation has the nice property that the computed value 
of . will never be larger than the values used to compute it. Linear interpolation is also .rst order 
accurate. In the case of a MAC grid, there is an additional complication with semi-Lagrangian advection 
scheme described above. It assumes that a velocity u is available where the values of . are stored. Velocities 
in a MAC grid, however, are staggered. Linear interpolation is required to colocate them at the desired 
locations. Regardless of whether they are required at cell centers, faces, or nodes, linear interpolation 
can be used to obtain a second order accurate velocity estimate. 3.3.2 Higher Order Methods The Semi-Lagrangian 
advection scheme described in Section 3.3.1 is only .rst order accurate. Some applications require a 
more accurate level set than this. One such application is surface tension. The curvature of a level 
set can be computed as . = V2f, and the force applied to the .uid surface is proportional to .. Since 
a second derivative of the level set will be computed, a .rst order accurate level set will not su.ce. 
Thus, higher order methods are sometimes required. One way to do this is to modify semi-Lagrangian advection 
to n be more accurate. This can be done by computing a more accurate estimate of xand replacing the linear 
interpolation with quadratic interpolation. Alternatively, one may obtain a higher order accurate result 
by discretizing Equation 1 directly. One popular way is to use a Runge Kutta method for time. In practice, 
third order TVD Runge Kutta is used [10]. Similarly, the spatial derivatives must be discretized accurately, 
and third order Hamilton Jacobi ENO [7] works well in practice. The speci.cs of these more advanced higher 
order methods are beyond the scope of the course. I mention them because they are sometimes necessary 
and to provide a general direction for addressing the occasional need for additional accuracy.  3.4 
Level Set Reinitialization Anyone who has watched a turbulent .uid will notice that distance from a particle 
of .uid to the interface changes all the time. This seems to go against what we have determined about 
the advection equation. What is more problematic, the important level set property IVfI = 1 is not preserved. 
Indeed, if Vf Vf = 1, then .. 0= (Vf Vf)=2Vf  (Vf)=2Vf Vft =2Vf V(-u Vf)= -2n V(n  u). .t .t 
This states that the level set property will be preserved if the normal component of velocity does not 
change along the normal direction. This will not, in general, be true. This may seem to contradiction 
the conclusion that the level set evolves according to the advection equation. The advection equation 
was derived based on how the interface moves due to the underlying velocity .eld. Indeed, the level set 
equation correctly describes the motion of the f = 0 isocontour. The advection equation only approximately 
describes the evolution of the level set near the interface. In areas far from the interface, it is completely 
useless. The standard way of handling this problem is to perform an additional step after advecting the 
level set to restore the level set property. This extra step is called reinitialization. The goal of 
level set reinitialization is to restore the level set property while leaving the interface location 
as undesturbed as possible. The interface location is the part most accurately updated by level set advection, 
so it is generally worthwhile to try preserving it. Note also that the location of the interface de.nes 
the level set everywhere. 3.4.1 Level Set Reinitialization Equation As we did with level set advection, 
we will treat reinitialization by .rst deriving the level set reinitialization equation, even though 
some of the most popular methods do not actually use it directly. The reinitialization equation is a 
PDE that describes how the level set should be changed so that, if evolved su.ciently long, the level 
set property will be restored. The reinitialization equaton is described with a notion of time, but this 
is not the real physical time. It is a sort of fake that is just used to perform reinitialization. To 
determine how the level set should be changed, we must .rst determine what is wrong with it in the .rst 
place. For this, we consider the quantity IVfI- 1. For a true level set, this is zero. Consider a location 
inside the .uid, so that f< 0, and assume that IVfI > 1. This means that f is increasing in magnitude 
too quickly as we move farther from the interface. Thus, since f< 0, its value will be too small. We 
need to increase the value of f at that location, so we want ft > 0. Similarly, we were outside the .uid, 
f> 0. In this case, the value of f would be too large, and we should decrease it. Then, we would want 
ft < 0. The di.erent cases are summarized in the table IVfI < 1 IVfI > 1 f > 0 ft > 0 ft < 0 f < 0 ft 
< 0 ft > 0 If we put these together into an equation, ignoring any consideration of how fast to change 
f, we obtain (see also [12, 3]) ft + sign(f)(IVfI- 1) = 0. (2) Ideally, we would have sign(f)= -1 if 
f< 0, sign(f) = 1 if f> 0, and sign(f) = 0 if f = 0. In practice, it works better if the sign function 
is smoothed out near the interface (see [12, 8]). This had the e.ect of producing slower changes near 
the interface to avoid moving it as much. Now it is time to make some observations about the reinitialization 
equation to see if it actually does what we would like. The reinitialization equation is to be evolved 
in time until it has reached a steady state. When this occurs, ft = 0. Then sign(f)=0 or IVfI = 1. The 
.rst is true at the interface, but it will not be true elsewhere in the domain. We are left to conclude 
that the level set property has been restored. There are a couple other important properties to observe 
from the reinitialization equation. The .rst is that sign(f) = 0 at the interface, so that ft = 0. Thus, 
the interface will not change. In practice, the level set will not be sampled exactly at the interface, 
but in practice it still does not move too much. The second observation is that if reinitializaiton is 
performed on a level set, IVfI- 1 = 0, so that again ft = 0. This implies that this approach to level 
set reinitialization is, at least ideally, idempotent. For more details on solving the level set equation, 
see . 3.4.2 Fast Marching Solving the reinitialization equation directly involves updating each value 
in the level set many times as information about how far a given cell is from the interface propagates 
through the mesh. Let s take a more careful look at how this information propagates through the mesh. 
Information Propagation Consider that you were standing in a park next to a statue and you wished to 
determine your distance to that statue. The obvious way to do this would be to stretch a tape measure 
or some other device from you to the statue. From there, you can simply read o. the distance. What if 
you were standing instead in a crowded room and wished to determine your distance from the entrance, 
which you cannot see? What if everyone else knows their distance to the door? You can ask your neighbors 
what their distances are, and knowing how far they are from you allows you to determine your own distance. 
If your neighbor is 20 m from the door, and you are 1 m from your neighbor, you might conclude that you 
are 21 m from the door yourself. What if you then asked another neighbor, who is the same distance from 
you but turned out to be only 19 m from the door? Then you would revise your estimate to 20 m. Now, imagine 
that nobody else in the room knows their distance, but they are trying to determine this at the same 
time you are. You ask one neighbor for his distance, and your neighbor believes himself to be 20 m away. 
You now estimate that you are 21 m away. When your neighbor asks you and .nds that you are even further 
away, he pays little attention. This time, when you ask a di.erent neighbor, his estimate is only 16 
m. You update your estimate to 17 m. When the other neighbor asks you again what your distance is, he 
.nds that you are actually closer than him. In fact, the .nds that you are su.ciently close to the door 
that he must be closer, too. He revises his distance estimate to 18 m. As people in the room discover 
neighbors with smaller distances, they revise their smaller estimates, and that information propagates 
through the room, from neighbor to neighbor, from people with smaller distances to people with larger 
distances. This provides us with our .rst rule: compute your only from neighbors whose distance is less. 
The next observation is that wave after wave of revised distance estimates might propagate through the 
room. This takes time. What if instead of passing on to your neighbor information about how far you think 
you are from the door, you instead wait until you know what your distance really is? Those standing next 
to the door know what their distances are. Then, they provide that information to their neighbors, who 
can now determine precisely what their distances are. The information spreads across the room once, and 
then everyone knows what their distance is. This leaves one important question, though. You only tell 
your neighbors your distance when you know for sure what that distance is, but how do you know if your 
distance estimate is correct? The key observation here is that your estimate depends only on those around 
you whose distance is less. If everyone else in the room whose distance is less than yours knows their 
distance to be correct, then yours must be correct, too. We now have all of the pieces required to describe 
the fast marching method. Begin by intializing a layer of level set values next to the interface by some 
other means. For example, you might assume that the advected values in these cells are close enough. 
These level set values are assumed to be known. We refer to cells whose level set values are known as 
active cells. The neighbors of active cells (that are not themselves active) are called close cells. 
Close cells can compute an estimate of their distance, though this distance might not be right. All other 
cells are called far cells. These carry a distance estimate of in.nity. Choose the close cell with the 
smallest distance estimate. Since every cell with a smaller distance is active, and therefore known, 
the level set value of that close cell is know to be correct. The cell is reclassi.ed as active. Then, 
its neighbors level set estimates are updated (except those that are already active). If any of its neighbors 
are far, they are reclassi.ed as close. This process is then repeated until all cells are active. When 
this occurs, reinitialization is completed. Note that at each step in the algorithm, there are a bunch 
of active cells near the interface, surrounded by a thin layer of close cells, beyond which lies a sea 
of far cells. Over time, the active region in the middle grows to include the entire domain. In practice, 
an accurate level set is only required in a narrow band around the interface. In this case, the fast 
marching algorithm can be terminated when the least active cell value is greater than this distance. 
The remainder of the domain is then simply .lled with the maximum value with the appropriate sign. Updating 
Distance Estimates We have not so far discussed how to update the distance estimates. This is where we 
discretize the level set property IVfI = 1 that we wish to enforce. For simplicity, we consider the 2D 
case, noting that 3D is entirely analygous. Squaring both sides and writing this out, we want to satisfy 
f2 + f2 =1. xy This leaves the question of discretizing the partial derivatives. There are three popular 
discretizations worth considering: central di.erencing, forward di.erencing, and backward di.erencing. 
fi+1,j - fi-1,j fi+1,j - fi,j fi,j - fi-1,j f0 f+ f- = == . x xx 2.x .x .x To determine which to choose, 
we refer back to one of the lessons from above. We only want to look at neighbors whose value is less 
than our own. That is, we only want to use a forward di.erence if fi+1,j <fi,j , and we only want to 
use a backward di.erence if fi,j >fi-1,j. We never want to use the central di.erence. Note that these 
criteria are just f+ < 0 and f- > 0. If both are true, we can choose one of them to use. If xx neither 
are true, then both neighbors are greater. In this case, we appear to be at a local minimum along that 
direction, and we can simply approximate the derivative by zero. Noting that signs go away when squared, 
we can choose f2 = max(f- , -f+ , 0). x xx Finally, we arrive at our discretization max(f- , -f+ , 0)2 
+ max(f- , -f+ , 0)2 =1. xx yy At .rst, it appears that computing a distance from this equation will 
be hopeless. However, if we pull out a couple of the cases, we see that it is not so. First, we observe 
that we cannot choose a zero for both terms. This leaves eight cases. Lets examine the case (f-)2 +(-f+)2 
= 1. Substituting in the di.erence xy formulas we get fi,j - fi-1,j fi,j - fi+1,j ()2 +( )2 =1. .x .x 
We are assuming that our neighbors values are known, so only fi,j needs to be determined. This is just 
a quadratic equation, which we can solve to obtain a new distance estimate. The other signi.cantly di.erence 
case occurs when one of the derivatives is approximated as zero. In this cae, we get a case like (f-)2 
= 1, which is even easier to solve. In fact, its solution is just fi,j = fi-1,j +.x. x Note that there 
are two solutions. We always want the larger solution. This applies to the case above, too. The case 
yielding the smallest feasible estimate is chosen. This completes the update procedure. Computing the 
Minimum Close Node At this point, only one important detail remains. We have not mentioned how to compute 
which of the close nodes has the minimum f estimate. This is done using a priority queue. When implemented 
using a binary heap, the minimum node can be determined in O(log n) time. Since each step of the algorithm 
takes constant time and produces one additional active node, we conclude that the entire algorithm runs 
in O(n log n) time. 3.4.3 Fast Sweeping The fast sweeping algorithm takes a di.erent approach to initializing 
the level set. To understand what this approach is and how it works, lets return to the crowded room 
analogy. This time, however, everyone is trying to determine their distance to a table near the center 
of the large room. Lets assume that the table is located northwest of your location. Your neighbor with 
the least distance to this table will be located to north or west of you. You may even use both of these 
estimates in determining your own distance. Those to the south or east will have distances larger than 
yours. In fact, everyone southeast of the table will .nd out their distance by looking north and west 
of their location. If everyone in the room computes their distance estimate one by one, starting along 
the north wall, working west to east. After everyone along the north wall has updated their estimate, 
everyone in the row immediately south of them does the same thing, working from west to east. This continues 
until everyone in the room has computed an estimate of their distance once. Clearly those standing along 
the north wall or the west wall will still have no idea how far they are from the table. They have no 
way of knowing how far that table is, since none of their neighbors do, either. If we now consider someone 
standing southeast of the table, the story looks very di.erent. These people need to look north and west 
to correctly compute their distance to the table, and everyone in that directly has already updated their 
distance estimates. In fact, everyone standing southeast of the table will correctly compute their distance. 
What if now we repeat the process, making another sweep through the room, asking everyone to update their 
distance estimate in sequence. As before, we start along the north wall, but this time we sweep east 
to west instead. As we might expect, this time everyone south and west of the table will compute a correct 
estimate. Those along the north wall still have no idea how far away the table is. What about those were 
were southeast of the table? They already had correct estimates. Note that in this pass, nobody s distance 
estimate is going to get smaller, since they already have the correct distance. As such, none of these 
people will change their estimate at all. Now everyone southwest or southeast of the table has their 
distance right. Repeating this sweeping process through the room twice more, this time starting from 
the south wall, once west to east and once east to west, completes the process. Everyone knows their 
distance to the table. The same process works even if distance is being measured to a set of tables. 
This time, however, there are a few places where someone is roughly the same distance to more than one 
of the tables. At these locations, the distance estimate might not quite be right, but it will be very 
close. The accuracy at these locations could be improved somewhat by repeating the sweeps, but in practice 
a single set of sweeps is su.cient. As with fast marching, a method for updating distance estimates is 
required. Conveniently, the algorithm for updating distance estimates for fast marching works for fast 
sweeping, too. The extension to 3D is straightforward. There are now 8 orderings instead of 4, which 
correspond to the di.erent combinations of traversing forwards or backwards along each axial direction. 
Practically speaking, fast sweeping di.ers from fast marching in two important ways. The .rst is that 
fast sweeping does not require a priority queue and so runs in O(n) time and is easier to implement. 
The second di.erence is that fast marching can be terminated early if only a band of the level set is 
desired. This is not the case for fast sweeping. This means that fast marching may in practice actually 
be faster than fast sweeping. For example, if the simulation mesh is m  m  m, then fast sweeping will 
require O(m3) time, whereas if only a thin band of the level set requires reinitialization, then fast 
marching will only take O(m2 log m) time. 3.4.4 Higher Order Methods The fast marching and fast sweeping 
methods are only .rst order accurate. As with level set advection, some applications require more than 
.rst order accuracy, with surface tension being an example. Surface tension requires second derivatives 
of the level set to be computed near the interface, so a .rst order reinitialization will not su.ce. 
One may obtain a higher order accurate result by discretizing Equation 2 directly. The recommended way 
of doing this is to use third order TVD Runge Kutta in time [10] and either third order Hamilton Jacobi 
ENO [7] or the preferred .fth order Hamilton Jacobi WENO [5] for space. The speci.cs of these more advanced 
higher order methods are beyond the scope of the course, but the interested reader is directed to [6] 
for a thorough overview of more accurate methods for reinitialization.  3.5 Particle Level Set In practice, 
the level set method for .uids has a rather problematic feature. Over time, a dynamic .uid with lots 
of splashing will tend to lose a signi.cant amount of volume. Bubbles and thin sheets will tend to disappear 
if their dimensions are comparable to those of the grid size. This issue in this case is one of sampling. 
Features that are too small to be represented with the available level set resolution are lost. There 
has been signi.cant interest in avoiding volume loss errors. One may contrast this situation with Lagrangian 
methods, where the degrees of freedom track the material. Features that are small or thin continue to 
be resolved and are not lost. In practice, it is possible to use Lagrangian particles to improve the 
accuracy of the level set method. One such method for doing this is the particle level set method [2]. 
The particle level set method represents the .uid interface using a level set, and Lagrangian particles 
are maintained near the interface and are used to correct errors in the level set. Two types of particles 
are maintained, one for the positive region and one for the negative region. These particles are spheres 
whose union approximates the location of the interface. The radii of the spheres are variable within 
a range [rmin,rmax], which is normally chosen to be some fraction of the cell size like [.1.x, .5.x]. 
Using a range of radii allows the interface to be approximated from multiple distances at the same time. 
The particle level set method consists of three main stages: initialization, particle evolution, and 
level set correction. During the initialization stages, particles are seeded near the interface. All 
cells within a narrow band around the interface are seeded uniformly with particles. Typically, a band 
of around three grid cells is su.cient. In each cell, both positive and negative particles are seeded. 
The number of particles seeded into each cell represents a tradeo. between e.ciency and accurate representation 
of the .uid interface. In practice, 64 particles per cell in 3D or 16 particles for 2D provides a good 
compromise. At this point, the particles are scattered about on either side of the interface. We would 
like to put the positive particles on the positive side and the negative particles on the negative side. 
We also want to provide a uniform sampling in the normal direction. To do this, we choose an isocontour 
on the correct side of the interface and move the particles along the normal direction to place them 
on this isocontour. We then set the particle radii to match their distance to the interface, subject 
to being clamped to the acceptable radius range. The second part of the particle level set method is 
particle evolution. Lagrangian particles are very easy to evolve once particle velocities are determined. 
These velocites are simply computed from the Eulerian mesh using linear interpolation. The particles 
are then evolved using third order Runge Kutta. The .nal but most important part is the level set correction 
step.  4 Practical Methods for Water Simulation Up to this point, we have discussed a number of the 
methods that have been developed for tracking a moving interface using level sets. We have not, however, 
mentioned the evolution of the .uid itself. That will be the topic of this section. We will discuss the 
incompressible Navier-Stokes equations for the evolution of .uids. As with the sections on level sets, 
we will not focus too much on implementation details, since this course will accompany the release of 
a part of the PhysBAM library, which contains implementations of these algorithms. Instead, this section 
will motivate the equations, what they mean, and how they are solved. 4.1 Navier-Stokes The Navier-Stakes 
equations describe the evolution of .uids under some simplifying assumptions. The most important of these 
assumptions is that the .uid does not compress appreciably. This is a good approximation for .uids like 
water. Even under high pressures, water will not compress signi.cantly, and its compression does not 
appreciably a.ect its motion. This is not to say that water does not compress or that its compression 
is not noticeable. The limited compressibility is what transmits sound, allowing one to hear sounds while 
under water. Similarly, air does not compress signi.cantly under normal circumstances and for many purposes 
may be approximted as incompressible. Incompressibility implies that volume is locally conserved. If 
we .x a control volume O, then any .uid .owing in must be precisely balanced by .uid .owing out. The 
net amount of .uid .owing out of the region is then 0=u  dS =V u. .OO This should be true for any portion 
of the .uid, so that V u = 0. This is the incompressibility constraint. The Navier-Stokes equations 
are a statement of Newton s second law, F = ma. To aid in piecing them together, lets assume that the 
.uid is divided into chunks of equal volume V . The mass of one of these chunks can be expressed as m 
= .V , where . is the .uid density. We assume that the .uid density is constant. If the .uid were described 
in a Lagrangian frame of reference, the acceleration would just be the time derivative of the velocity, 
Du a = . Dt Recalling the relationship between Eulerian and Lagrangian frames, we .nd Du = ut +(u V)u. 
Dt Combining this with Newton s second law and dividing through by volume, .ut + .(u V)u = V -1F. What 
remains now is to formulate the forces that apply to the .uid. This is the point when the .uid equations 
start to depend on the application, since di.erent forces become relevant under di.erent circumstances. 
There is, however, one force that is always used, and that is the force due to pressure. Pressure in 
a .uid is a scalar .eld; at any location within a .uid, there is a scalar p that describes the pressure 
at that location. The value of the pressure does not itself cause motion in .uid. If we .ll a take with 
gas and examine its motion, we will .nd that it remains at rest. If we instead double the pressure in 
the tank, we .nd that it still remains at rest. Clearly it is not the magnitude of the pressure that 
makes the di.erence. What matters is how the pressure is changing. A useful analogy is the weather report. 
Some regions of the country will have high pressure, and some will have low pressure. Areas of high pressure 
force air to .ow to regions where the pressure is low, much like how air rushes from a pressurized tank 
if a valve is opened. The force will point in the direction of greatest local decrease in pressure, since 
forces area always local. This suggests that the force should be related to the gradient of the pressure 
Vp. The gradient points in the direction of greatest increase, so we want the negative gradient. This 
leads to the equation .ut + .(u V)u = -Vp. These along with the incompressibility constraint form the 
Euler equations. When considering short length scales or viscous materials, viscosity becomes important. 
Viscosity dissipates energy and turns the kinetic energy into heat. When the viscosity is added, the 
equation becomes .ut + .(u V)u = -Vp + V2 u, where we assume the viscosity coe.cient  is constant. 
These are the Navier-Stokes equations. To see the e.ect of this new term, lets ignore the other terms 
and consider how the total kinetic energy KE in the .uid region O changes over time: .. 1 KE = u  u= 
u  ut = u  V2 u =  V (u Vu) -  tr(Vu T Vu). .t .t2 OOOOO O If the velocity at the boundaries are 
zero, then the .rst term vanishes since  V (u Vu)=  u Vu dS =0. O .O The second term is nonpositive, 
so viscosity removes kinetic energy. In addition to these, there may be additional forces. Gravity is 
a common example of such a force. With these forces, the Navier-Stokes equations can be summarized advectionpressureviscositybody 
accel .ut + .(u V)u= -Vp + V2 u + f, inertial terms force terms subject to the incompressibility 
constraint V u =0. 4.2 Time Splitting The Navier-Stokes equations contain a very diverse set of terms, 
each with its own character and challenges, that make it di.cult to solve. The advection term is nonlinear, 
and information travels in only one direction at a .nite speed. The viscosity and pressure terms are 
linear but propagate information in all directions at in.nite speed. The standard approach for dealing 
with this is called time splitting. Consider the simple ' di.erential equation y= A(y)+ B(y). Discretize 
y' temporally to produce n+1 - yn y = A(y)+ B(y). .t This equation is equivalent to the sum of the two 
equations n+1 - y*n yy* - y = A(y)= B(y). .t .t Now the evolution of a more complicated is reduced to 
evolving the simpler underlying pieces. This makes the Navier-Stokes equations much easier to solve. 
Note that the order in which A(y) and B(y) were applied in the example above was arbitrary. There are 
four terms in the Navier-Stokes equations as we have presented them, so there are 4! = 24 possible orders 
in which to apply the individual forces. The best ordering turns out to be 1. Advection 2. Gravity 
3. Viscosity 4. Pressure  The logic that goes into this ordering is roughly as follows. The .nal velocity 
.eld should satisfy the incompressibility constraint. The pressure force enforces this constraint, and 
putting it last ensures that the constraint will be satis.ed in the .nal velocities. Advection requires 
a velocity .eld that is very nearly divergence free to avoid errors relating to volume loss. The way 
to ensure this is to put it directly after the pressure solve, which places it as the .rst step of the 
next iteration. As will be discussed later, viscosity n+1 needs to be applied implicitly. This means 
it should directly use u. This con.icts with the placement of pressure. It is possible to resolve this 
con.ict by applying both pressure and viscosity simultaneously, but this is more di.cult than is strictly 
required. In practice, applying viscosity before pressure produces acceptable results. This leaves gravity 
as the second step. This leaves us with four types of di.erential equations to solve. The .rst of these 
is just the advection equation, which was addressed in Section 3.3. The gravity equation is a decoupled 
set of ODE s and is evolved by simply adding gravity to the intermediate advected velocity. This leaves 
viscosity and pressure. These are the topics of the next two sections. 4.3 Viscosity Unlike advection, 
which moves information around at a .nite speed, viscosity moves information with in.nite speed in all 
directions. These di.erences have implications for the numerical methods used. Since information propagates 
at in.nite speed, an explicit method will not be stable unless .t = O(.x2). Such a small time step would 
bring the simulation to a near standstill. Instead, an implicit method is used. Since information does 
not travel in characteristic directions, careful numerical methods are not required. The operator can 
be discretized with central di.erences. Let L represent a discretized Laplace operator. Then, the equation 
that must be updated for the viscosity step is n+1 - un u Lun+1 = . .t. Written as a linear system, 
.t n+1 n I - Lu = u . . Since the Laplace operator is symmetric and negative de.nite, the linear system 
will be SPD, which allows it to be solved using e.cient methods like CG. 4.4 Projection What remains 
is the pressure force. The equation to be updated for the pressure force is n+1 - un u1 = -Vp (3) .t. 
Taking the divergence of both sides produces n V u n+1 -V u = -V  .t Vp . Then, noting from the incompressibility 
condition that V un+1 = 0, .t n V Vp = V u . . If the gradient operator is discretized as G and the 
density is discretized as M, then the divergence operator will be -GT . This produces the system n GT 
M-1Gp = GT u . As was the case for the viscosity system, this linear system is also SPD. Once the new 
pressure is computed, it is used to update the new velocity with (3). In the case of the linear system 
for viscosity, the conditioning is typically good enough that CG will converge quickly. In the case of 
the system that must be solved for pressures, the conditioning is very poor. In this case, a preconditioner 
is required in order for CG to converge in a reasonable amount of time. Typically an Incomplete Cholesky 
preconditioner is used. 4.5 Surface Tracking In the case of a .uid simulation with a free surface or 
two-phase .ow, we must track an interface for the .uid. We use the particle level set method for this. 
With the particle level set method, the steps involved in a single step of the time integration algorithm 
are 1. Advect level set 2. Advance PLS particles 3. Update level set from PLS particles 4. Perform 
level set reinitialization 5. Advect .uid velocity 6. Apply gravity and other body forces 7. Apply 
.uid viscosity 8. Enforce incompressibility using pressures   References [1] S. Chen, D. Johnson, 
and P. Raad. Velocity boundary conditions for the simulation of free surface .uid .ow. J. Comput. Phys., 
116:262 276, 1995. [2] D. Enright, R. Fedkiw, J. Ferziger, and I. Mitchell. A hybrid particle level set 
method for improved interface capturing. J. Comput. Phys., 183:83 116, 2002. [3] R. Fedkiw, T. Aslam, 
B. Merriman, and S. Osher. A non-oscillatory Eulerian approach to interfaces in multimaterial .ows (the 
ghost .uid method). J. Comput. Phys., 152:457 492, 1999. [4] F. Harlow and J. Welch. Numerical Calculation 
of Time-Dependent Viscous Incompressible Flow of Fluid with Free Surface. Phys. Fluids, 8:2182 2189, 
1965. [5] G.-S. Jiang and D. Peng. Weighted ENO schemes for Hamilton-Jacobi equations. SIAM J. Sci. Comput., 
21:2126 2143, 2000. [6] S. Osher and R. Fedkiw. Level Set Methods and Dynamic Implicit Surfaces. Springer-Verlag, 
2002. New York, NY. [7] S. Osher and C.-W. Shu. High order essentially non-oscillatory schemes for Hamilton-Jacobi 
equations. SIAM J. Num. Anal., 28:902 921, 1991. [8] D. Peng, B. Merriman, S. Osher, H. Zhao, and M. 
Kang. A PDE-based fast local level set method. J. Comput. Phys., 155:410 438, 1999. [9] P. Raad, S. Chen, 
and D. Johnson. The introduction of micro cells to treat pressure in free surface .uid .ow problems. 
J. Fluids Eng., 117:683 690, 1995. [10] C.-W. Shu and S. Osher. E.cient implementation of essentially 
non-oscillatory shock capturing schemes. J. Comput. Phys., 77:439 471, 1988. [11] J. Stam. Stable .uids. 
In Proc. of SIGGRAPH 99, pages 121 128, 1999. [12] M. Sussman, P. Smereka, and S. Osher. A level set 
approach for computing solutions to incompressible two-phase .ow. J. Comput. Phys., 114:146 159, 1994. 
[13] G. Tryggvason, B. Bunner, A. Esmaeeli, D. Juric, N. Al-Rawahi, W. Tauber, J. Han, S. Nas, and Y. 
J. Jan. A front-tracking method for the computations of multiphase .ow. J. Comput. Phys., 169:708 759, 
2001.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
</content>
</proceeding>
