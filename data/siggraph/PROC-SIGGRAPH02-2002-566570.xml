<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/23/2002</start_date>
		<end_date>07/26/2002</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[San Antonio]]></city>
		<state>Texas</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>566570</proc_id>
	<acronym>SIGGRAPH '02</acronym>
	<proc_desc>Proceedings of the 29th annual conference</proc_desc>
	<conference_number>29</conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>1-58113-521-1</isbn>
	<issn>0730-0301</issn>
	<eissn></eissn>
	<copyright_year>2002</copyright_year>
	<publication_date>07-23-2002</publication_date>
	<pages>574</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[Welcome to the SIGGRAPH 2002 Papers Program. It's an exciting collection that pushes the boundaries of our field. Several areas have an increased presence: the numerical simulation of natural phenomena; data- driven synthesis, including texture synthesis from images and motion synthesis from motion-capture data; and image- and video-processing methods.There were 358 submissions and 67 acceptances. About six dual submissions to other conferences were discovered and rejected automatically. The 358 submissions represent a 20% increase over last year.The papers are arranged in their order of presentation, grouped into sessions. Sessions are created after the paper selection process and thus reflect the result of the selection. We take all of the papers that are good enough and then figure out sessions for them. I once heard of a conference in which reviewers give one of three ratings: "Accept," "Reject," and "Accept as Filler." By avoiding the preselection of topics, i hope we can forever avoid that third category.There are several notable changes this year. First and foremost, as the start of a two-year experiment, this year's papers are published as an issue of ACM Transactions on Graphics (TOG). Because TOG is a journal, papers had to be refereed rather than just reviewed. Each submission was either accepted, accepted with minor changes, or rejected. Kurt Akeley's idea, for S1GGRAPH 2000, of only accepting papers "as is" was a great step towards fairness. But I felt that it also let some bad things happen -- papers were rejected because of just a few misplaced words. Letting reviewers require minor changes addresses this issue. The authors made these changes and a referee checked them. If the author felt that the minor changes were not acceptable, s/be was welcome to withdraw the paper. The advantages to this new approach are threefold: papers are double-checked for mistakes; small mistakes or omissions can be fixed; and papers, having been refereed, can be published in a journal, which is important in some academic tenure cases.There are disadvantages too. The refereeing process was rushed and sometimes inconvenient, and different reviewers had different notions of "small changes," despite my best efforts to outline what these were. In retrospect, Kurt's rule of "acceptable as is" has the enormous advantage of being unambiguous? Jessica Hodgins, next year's chair, will have to evaluate this experiment carefully.A second change was in the selection process: the non-committee reviewers were selected by both committee members responsible for a paper instead of just one as in earlier years.A third and final change was that by expanding the technical program to start one day earlier, we have no overlapping sessions. When Jim Kajiya shifted to overlapping sessions, he noted that it was a positive change: no longer was the committee "counting papers" near the end of the selection process and getting tougher about accepting them because "there isn't room." I asked my committee not to count, but rather to work with the "accept all the papers that you think should be SIGGRAPH papers rule" and see how it turned out. Even with the extra day, not everything fit, so we lengthened some sessions to accommodate. I fear that with the growth of our field, scheduling is likely to become more and more difficult. Starting the technical program a day earlier raised problems for some other people, most notably Valerie Miller, the Courses chair: she had to work with courses that overlapped with papers, which caused major scheduling problems.]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P283344</person_id>
			<author_profile_id><![CDATA[81100578452]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Tom]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Appolloni]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Harris Corporation]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2002</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>566571</section_id>
		<sort_key>243</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Images and video]]></section_title>
		<section_page_from>243</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39043704</person_id>
				<author_profile_id><![CDATA[81100457973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566572</article_id>
		<sort_key>243</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Video matting of complex scenes]]></title>
		<page_from>243</page_from>
		<page_to>248</page_to>
		<doi_number>10.1145/566570.566572</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566572</url>
		<abstract>
			<par><![CDATA[This paper describes a new framework for <i>video matting,</i> the process of pulling a high-quality alpha matte and foreground from a video sequence. The framework builds upon techniques in natural image matting, optical flow computation, and background estimation. User interaction is comprised of garbage matte specification if background estimation is needed, and hand-drawn keyframe segmentations into "foreground," "background" and "unknown". The segmentations, called <i>trimaps,</i> are interpolated across the video volume using forward and backward optical flow. Competing flow estimates are combined based on information about where flow is likely to be accurate. A Bayesian matting technique uses the flowed trimaps to yield high-quality mattes of moving foreground elements with complex boundaries filmed by a moving camera. A novel technique for smoke matte extraction is also demonstrated.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[alpha channel]]></kw>
			<kw><![CDATA[blue-screen matting]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[layer extraction]]></kw>
			<kw><![CDATA[matting and compositing]]></kw>
			<kw><![CDATA[video processing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Pixel classification</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P308194</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022963</person_id>
				<author_profile_id><![CDATA[81100035467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Aseem]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agarwala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington and Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15023234</person_id>
				<author_profile_id><![CDATA[81100122769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szeliski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>181452</ref_obj_id>
				<ref_obj_pid>181447</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARRON, J. L., FLEET, D. J., AND BEAUCHEMIN, S. S. 1994. Performance of optical flow techniques. International Journal of Computer Vision 12, 1, 43-77.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BERMAN, A., DADOURIAN, A., AND VLAHOS, P., 2000. Method for removing from an image the background surrounding a selected object. U.S. Patent 6,134,346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>229157</ref_obj_id>
				<ref_obj_pid>229144</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLACK, M. J., AND ANANDAN, P. 1996. The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. Computer Vision and Image Understanding 63, 1, 75-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLAKE, A., AND ISARD, M. 1998. Active Contours. Springer Verlag, London.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1172401</ref_obj_id>
				<ref_obj_pid>1170748</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHUANG, Y.-Y., CURLESS, B., SALESIN, D., AND SZELISKI, R. 2001. A Bayesian approach to digital matting. In Proceedings of Computer Vision and Pattern Recognition (CVPR 2001), vol. II, 264 - 271.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218441</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1995. Image snapping. In Proceedings of ACM SIGGRAPH 95, 183-190.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[HILLMAN, P., HANNAH, J., AND RENSHAW, D. 2001. Alpha channel estimation in high resolution images and image sequences. In Proceedings of Computer Vision and Pattern Recognition (CVPR 2001), vol. I, 1063-1068.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[KELLY, D. 2000. Digital Composition. The Coriolis Group.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2322744</ref_obj_id>
				<ref_obj_pid>2322483</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[LEE, M.-C., ET AL. 1997. A layered video object coding system using sprite and affine motion model. lEEE Transactions on Circuits and Systems for Video Technology 7, 1, 130-145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218450</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MITSUNAGA, T., YOKOYAMA, T., AND TOTSUKA, T. 1995. Autokey: Human assisted key extraction. In Proceedings of ACM SIGGRAPH 95, 265-272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218442</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E. N., AND BARRETT, W. A. 1995. Intelligent scissors for image composition. In Proceedings of ACM SIGGRAPH 95, 191-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[PORTER, T., AND DUFF, T. 1984. Compositing digital images. In Computer Graphics (Proceedings of ACM SIGGRAPH 84), vol. 18, 253-259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[RUZON, M. A., AND TOMASI, C. 2000. Alpha estimation in natural images. In Proceedings of Computer Vision and Pattern Recognition (CVPR 2000), 18-25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237263</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R., AND BLINN, J. F. 1996. Blue screen matting. In Proceedings of ACM SIGGRAPH 96, 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[SUN, S., HAYNOR, D., AND KIM, Y. 2000. Motion estimation based on optical flow with adaptive gradients. In Proceedings of International Conference on Image Processing (ICIP 2000), vol. I, 852-855.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258861</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SZELISKI, R., AND SHUM, H.-Y. 1997. Creating full view panoramic mosaics and environment maps. In Proceedings of ACM SIGGRAPH 97, 251-258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[WANG, J. Y. A., AND ADELSON, E. H. 1994. Representing moving images with layers. IEEE Transactions on Image Processing 3, 5, 625-638.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Video Matting of Complex Scenes Yung-Yu Chuang1 Aseem Agarwala1 Brian Curless1 David H. Salesin1,2 Richard 
Szeliski2 1 University of Washington 2 Microsoft Research Abstract This paper describes a new framework 
for video matting, the pro­cess of pulling a high-quality alpha matte and foreground from a video sequence. 
The framework builds upon techniques in natural image matting, optical .ow computation, and background 
estima­tion. User interaction is comprised of garbage matte speci.cation if background estimation is 
needed, and hand-drawn keyframe seg­mentations into foreground, background, and unknown . The segmentations, 
called trimaps, are interpolated across the video vol­ume using forward and backward optical .ow. Competing 
.ow es­timates are combined based on information about where .ow is likely to be accurate. A Bayesian 
matting technique uses the .owed trimaps to yield high-quality mattes of moving foreground elements with 
complex boundaries .lmed by a moving camera. A novel tech­nique for smoke matte extraction is also demonstrated. 
CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation Bitmap and framebuffer operations; 
I.4.6 [Image Processing and Computer Vision]: Segmentation Pixel classi.cation Keywords: Alpha channel, 
blue-screen matting, image-based rendering, layer extrac­tion, matting and compositing, video processing. 
 1 Introduction Video matting is a critical operation in commercial television and .lm production, giving 
a director the power to insert new elements seamlessly into a scene or to transport an actor into a completely 
new location. In the matting or matte extraction process, a fore­ground element of arbitrary shape is 
extracted, or pulled, from a background image. The matte extracted by this process describes the opacity 
of the foreground element at every pixel. Combined with the foreground color, the matte allows an artist 
to modify the background or to composite (i.e., transfer) the foreground onto a new background. Still-image 
matting, a somewhat user-intensive process, is useful for photo editing operations, and several tech­niques 
have been demonstrated for this application. In this paper, we address the more challenging problem of 
video matting: pulling a matte from a video sequence of a foreground element against a natural background. 
The challenge is to achieve the quality cur­rently attainable with recently developed still-image matting 
tech­niques without requiring the user to edit each frame and without introducing temporal artifacts. 
The most common methods for pulling video mattes are blue screen matting, difference matting, and rotoscoping. 
In blue screen mat­ting, also known as chroma keying, foreground elements are .lmed in front of a solid 
color background. Vlahos (as summarized by Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Smith and Blinn [1996]) pioneered this technique and 
developed a number of heuristics with tunable parameters for extracting the matte from each frame. This 
method can be fairly effective but is restricted to tightly controlled studio environments. For natural 
backgrounds, an alternative approach is to begin with a clean plate, i.e., a shot of the scene without 
the actors present, and to then subtract the clean plate from the frames containing the actors. Mapping 
the difference values at each pixel to opacities yields a dif­ference matte [Kelly 2000]. The mapping 
is user-de.ned and fails where there are similarities in color between foreground and back­ground, requiring 
additional user interaction to correct the matte. The last commonly used production matting technique 
is rotoscop­ing. In this process, the user draws an editable (e.g., B-spline) curve around the foreground 
element at each frame or at selected keyframes, often with the help of an image snapping tool that ad­heres 
to high gradient areas [Gleicher 1995; Mortensen and Bar­rett 1995; Blake and Isard 1998]. When placed 
at keyframes, the curves can then be interpolated or tracked over time so as to ad­here to foreground 
contours [Mitsunaga et al. 1995; Blake and Isard 1998]. Tracked results tend to require less editing 
than interpolated results, but they often still require frame-by-frame hand adjustment in order to pull 
a high-quality matte. In addition, post-processing is needed to convert the contour into opacity pro.les 
using either ad hoc feathering or smoothness assumptions about the background and foreground. One of 
the most signi.cant limitations of rotoscoping is its inabil­ity to work in regions where the matte is 
complex and not easily tracked over time with curves, e.g., around wisps of hair. By con­trast, recent 
advances in still image matting, especially Bayesian matting [Chuang et al. 2001], have proven quite 
successful in pulling mattes in such dif.cult areas. Bayesian matting begins with a user-supplied trimap, 
i.e., a segmentation of the scene into three regions: de.nitely foreground, de.nitely background, and 
un­known (Figure 3). By collecting nearby foreground and back­ground statistics, the opacity, as well 
as foreground and background colors, can be estimated at each pixel in the unknown region. The dif.culty 
in using this technique for video matting, however, is the need for the user to create a trimap at each 
frame. In this paper we describe a video matting approach that builds upon the Bayesian matting method 
and leads to a new kind of rotoscop­ing tool. The approach employs computer vision algorithms to in­corporate 
as much information from other frames as possible. In particular, we leverage optical .ow techniques 
to .ow trimaps be­tween hand-drawn trimap keyframes, thus reducing user involve­ment. Further, when the 
background can be estimated with mo­saicking techniques, we improve both the trimap and the matte by 
borrowing the background colors from the nearest frame in which those colors are known to be uncontaminated 
by the foreground. Our algorithm for .owing the trimaps based on optical .ow is novel. However, the primary 
contribution of this paper is .nding and adapting a good set of existing algorithms and devising an over­all 
framework for applying them together. Using our framework and a modest amount of user interaction, we 
demonstrate detailed matte extractions for actors with complex silhouettes .lmed against nat­ural backgrounds 
by both stationary and moving cameras. In addi­tion, we demonstrate a simple extension that allows matte 
extrac­tion of foreground smoke. The remainder of the paper is organized as follows. First, we dis­cuss 
the existing techniques we build upon (Section 2) and then describe our video matting algorithm (Section 
3). Next, we show results of applying our matting algorithm to video footage both with and without background 
estimation (Section 4). Finally, we summa­rize the work and describe a number of future research directions 
(Section 5). 2 Background In this section, we describe the three main components of prior work that we 
use in building our video matting algorithm: Bayesian matting, optical .ow, and background estimation. 
In each case, we discuss related work and the particular method we have selected. 2.1 Bayesian matting 
Recently, several techniques have emerged to solve the natural im­age matting problem, i.e., extracting 
a matte from a still image of a foreground object in front of a natural background. More formally, given 
the compositing equation at a pixel [Porter and Duff 1984], C = aF + (1 - a)B, (1) compute the foreground 
color F , background color B, and opacity (a.k.a., alpha) . that yields the observed color C. Clearly, 
this prob­lem is underconstrained (3 knowns and 7 unknowns). To simpli.y matters, the latest techniques 
require the user to specify a trimap. The matte is then extracted only in the unknown region using the 
nearby background and foreground colors to constrain the problem. Corel s Knockout package appears to 
do this by computing weighted sums of foreground and background pixels that abut the unknown region, 
followed by a simple projection step to estimate alpha, as described in patents (e.g., [Berman et al. 
2000]). Ruzon and Tomasi [2000] introduced a statistical method that partions the unknown band into regions 
and builds multi-modal foreground and background distributions within these regions. These distributions 
are .rst interpolated to estimate alphas, and then the mean fore­ground and background colors are perturbed 
until they satisfy the compositing equation. More recently, Hillman et al. [2001] and Chuang et al. [2001] 
improved on Ruzon and Tomasi s technique. Hillman et al. use principal component analysis (PCA) to estimate 
the optimal alpha, foreground and background simultaneously, while Chuang et al. use a Bayesian approach. 
In this paper, we adopt the technique of Chuang et al. which appears to yield the best mattes. Given 
an observation C for a pixel, the algorithm tries to .nd the most likely values for a, F and B. Using 
Bayes s rule, we can express the problem as the maximization over a sum of log­likelihoods: arg max L(F, 
B, a|C) F,B,. = arg max L(C|F, B, a)+ L(F )+ L(B)+ L(a) (2) F,B,. where L(·) is the log-likelihood function, 
i.e., the log of probability L(·) = log[P (·)], and the L(C) term is dropped, because it is a constant 
with respect to the optimization parameters. The algorithm proceeds by growing, contour by contour, into 
the unknown region, heading inward both from the foreground the background borders. At each unknown pixel, 
a circular region en­compasses a set of trimap foreground and background pixels, as well as any foreground 
and background values previously computed nearby in the unknown region. The foreground samples are then 
separated into clusters, and weighted mean and covariance matrices are used to derive Gaussian distributions 
that describe P (F ). The same is done for the background colors. Given these distributions, the Bayesian 
matting approach solves for the maximum-likelihood foreground, background, and alpha at the unknown pixel. 
Hillman et al. [2001] have additionally applied their alpha matting approach to moving image sequences. 
They match a low-resolution version of the current image to the previous image and classify each pixel 
as foreground or background if the corresponding pixels are mostly of the same foreground/background 
class. Undecided pixels are classi.ed by searching for corresponding edges in the previous frame and 
using their foreground and background color statistics. Unfortunately, it is hard to gauge the quality 
of their approach since only three static frames from a simple sequence are presented. 2.2 Optical .ow 
Optical .ow algorithms can be used to estimate the inter-frame motion at each pixel in a video sequence. 
Given two neighboring frames, Ci and Ci+1 , we can think of each pixel x in Ci+1 as com­ing from a shifted 
location x + u in Ci: Ci+1(x)= Ci(x + u) (3) where u describes the velocity of the pixel and is itself 
a spatially varying function over the image. We refer to u(x) as the .ow .eld between two frames. Over 
the years, researchers have developed a number of techniques for estimating the .ow .eld, many of which 
are compared and sum­marized by Barron et al. [1994]. Two assumptions common to many of these techniques 
are that the color of a source and destination pixel should be similar, and that the .ow .eld should 
exhibit some amount of spatial coherence. Thus, the problem becomes one of optimizing a data term (color 
similarity) plus a regularization term (smooth .ow). One of the better-performing optical .ow techniques 
is due to Black and Anandan [1996]. In addition to estimating regularized .ow, their technique employs 
robust statistics to avoid large errors caused by outliers and to allow for discontinuities in the .ow 
.eld. Their method handles large motions using a multi-scale approach to .ow estimation. In our video 
matting process, we have incorpo­rated Black s optical .ow algorithm, for which an implementation is 
available on the author s Web page. In order to use optical .ow to its fullest advantage, we make two 
important observations. The .rst observation arises when we con­sider disocclusions in an image sequence, 
i.e., when a feature not present in one frame appears in the next frame. Optical .ow breaks down here 
because it cannot .nd source pixels that explain the new feature. However, if we view this same event 
when running through the frames in the reverse direction, the event becomes an occlusion, and optical 
.ow does not have a problem. This insight has also been noted and used in the image coding literature 
[Sun et al. 2000]. We ll refer to it as Observation 1. The second observation is that optical .ow tends 
to perform poorly at the boundaries between foreground and background layers that have distinct motions, 
yet are blended together. As noted above, Black s algorithm does allow for discontinuities in the .ow 
.eld. However, we have found that this allowance does not always work, particularly for complex silhouettes. 
This second insight will be called Observation 2. Later in the paper (Section 3.2), we use these observations 
to decide how to .ow information through the spatiotemporal video volume. 2.3 Background estimation 
Clean plate techniques are straightforward when the camera is locked down or when it is attached to a 
motion control rig that per­mits reproducing camera motion both with and without the actors. In some 
cases, however, a partial clean plate can be assembled from nearby video frames, if the background motion 
can be reliably esti­mated from frame to frame. Such image mosaicking techiques have recently been used 
in MPEG-4 video coding to compactly trans­mit a static portion of the scene viewed from a panning camera 
[Lee et al. 1997]. Once a conservative foreground mask (sometimes called a garbage matte) has been speci.ed, 
through either manual [Lee et al. 1997] or automated [Wang and Adelson 1994] means, the remaining background 
fragments can be assembled using per­spective image mappings to form a composite mosaic [Szeliski and 
Shum 1997], which can then be reprojected into each original frame to form a dynamic clean plate. While 
these previous approaches have been succesfully applied in image coding and surveillance, they have not 
been used to obtain pixel-accurate alpha mattes. In this paper, we require user input to establish a 
garbage matte, and we adopt the mosaicking method of Szeliski and Shum [1997] to compute frame-to-frame 
registration. This method works under the assumption that the background undergoes only planar-perspective 
transformation. This assumption is equivalent to requiring that the background be planar or, less restrictively, 
that the camera s optical center translate a negligible amount relative to the distance to the background 
between frames being registered.  3 Video matting Our approach to video matting combines these earlier 
techniques with a modest amount of user interaction. Figure 1 illustrates the .ow of user interaction, 
data, and computation, which we summa­rize here. Where possible, a background plate B can appreciably 
improve the quality of the mattes obtained. To aid this process, the user draws a set of garbage mattes 
G that conservatively eliminate the fore­ground and enable background estimation. Next, the user draws 
trimaps at selected keyframes. These keyframe trimaps K can be fairly crude, as shown in Figure 3, and 
thus can be drawn quickly. In particular, the user draws a thick boundary that encompasses the regions 
where alphas need to be estimated and also partitions the image into foreground and background. To distinguish 
the two, the user selects which partition or partitions are to be .ood-.lled as foreground, and the remainder 
is .ood-.lled as background. The choice of keyframes is something the user adapts to with experi­ence; 
for example, keyframes are helpful in areas where the topol­ogy of the foreground layer changes. Once 
the initial set of trimaps is speci.ed, the labelings are passed through the volume using optical .ow, 
resulting in trimaps T at ev­ery frame. The .ow of information considers where optical .ow is likely 
to succeed and where it might fail. In order to narrow the bands of uncertainty in the trimaps as they 
.ow through the vol­ume, they are converted to alpha mattes a by the Bayesian matting process at each 
step of the process and then converted back into trimaps. If background information is available, the 
.ow process can be improved using better alpha mattes (and thus trimaps) and by using a form of difference 
matting that improves the trimaps in regions where .ow fails. Finally, if the matte is not satisfactory, 
the user can select a frame and edit the trimap with a simple painting tool. In practice, we pro­vide 
an image of the alpha matte to edit, as this tends to expose the structure of the image more than the 
trimap, but we permit the user to paint alphas of only 0 or 1. The edited alpha matte is then converted 
to a trimap and becomes a new keyframe. We then re-run the trimap interpolation method to make maximum 
use of the new information. The output of the system is the estimated foreground F , estimated background 
B , and alpha a for every frame. Note that, even when the background is available, the estimated background 
color may be slightly different in order to satisfy the maximum-likelihood cri­terion in Bayesian matting. 
In the remainder of this section, we discuss in greater detail the C K a F, B, a Figure 1 Video matting 
.ow chart. The primary computational blocks of our process are the background estimation, trimap interpolation, 
and Bayesian matting components. Each block receives the original image sequence C as input. For background 
estimation, the user also provides a garbage matte G to remove the foreground; then, a background B is 
estimated and used in trimap .ow and matting. To obtain a video matte, the user provides keyframe trimaps 
K, which are then converted to alpha mattes a and passed to the trimap interpolator. The interpolated 
trimaps T are re.ned through alpha matting at each step. Once the trimaps are complete, the Bayesian 
matting algorithm generates an estimated foreground F , background B , and a for all frames. fundamental 
computational blocks: background estimation, trimap interpolation, and alpha estimation. We conclude 
by describing a simple extension for extracting a smoky foreground layer from a known background. 3.1 
Background estimation Before beginning background estimation, a garbage matte is nec­essary to mask out, 
in a conservative manner, all pixels that could possibly contain foreground contributions. While automatic 
meth­ods have been published for estimating a garbage matte, and indeed we have tried using the keyframe 
trimaps to assist in this process, they do not always work. Making errors in the background plate can 
seriously degrade the quality of the alpha matte. Thus, we fol­low the approach commonly used in the 
.lm industry: we require the user to provide the garbage mattes. For our system, the user must draw a 
rectangle at a handful of keyframes. These rectangles are automatically interpolated over all the frames, 
and can then be adjusted immediately by the user to obtain conservative garbage mattes. For a sequence 
of 100 frames, it took about 5 minutes to specify garbage mattes. Given the garbage mattes, we follow 
the registration procedure de­scribed in Section 2.3. However, instead of constructing a single clean 
plate, we .ll in the missing parts of each frame with the color from the temporally nearest frame that 
contains a background color for that pixel. In practice, we .nd that copying pixels from nearby keyframes 
has the advantage of reducing errors due to small amounts of parallax or gradual temporal variations 
that may arise with illumination changes, motion blur, and defocus. 3.2 Trimap interpolation To pull 
a complete video matte, we require a trimap for each frame. Constructing the trimaps manually is tedious 
and time-consuming; on the other hand, a fully automatic approach is unlikely to suc­ceed in giving high-.delity 
mattes. Thus, we have developed a semi-automatic system that calculates trimaps for every frame us­ing 
hand-drawn trimaps for selected keyframes. To take advantage of spatiotemporal coherence within the video 
volume, we employ optical .ow. The .ow .eld acts as a guide for passing trimap labelings through the 
volume between keyframes. In principle, we could simply start from the .rst keyframe and .ow its trimap 
forward in time. Observation 1, however, tells us that dis­occlusions will cause errors in .ow that can 
frequently be resolved by viewing .ow in the opposite direction. The solution is clear: run .ow in both 
directions forward from one keyframe and back­ward from the next and combine the observations according 
to a measure of per-pixel accuracy for each prospective .ow. In the following sections we describe how 
we measure the accu­racy of optical .ow and then present our algorithm for running and combining bi-directional 
.ow. 3.2.1 Accuracy of optical .ow In this section, we devise a method for determining the accuracy of 
optical .ow based on the observations in Section 2.2. This ac­curacy test is built on precomputed error 
maps, i.e., the per-pixel prediction errors from one frame to another. To create a forward error map 
Eif for a .ow from frame i - 1 to i, we .rst compute the image predicted by .ow, Ci(x - u), bilinearly 
resampled onto the pixel grid. At each pixel of this warped image, we can com­pute the difference between 
the predicted color and the observed color as the L2 distance between the pixels in RGB color space, 
Eif = ||Ci(x) - Ci (x - u)||. A similarly computed backward error map Eib measures the accuracy of .ow 
from frame i +1 to i. After computing error maps for each frame in both directions of optical .ow, we 
combine these to create two sets of accumulated error maps. Since we are propagating trimap values from 
the near­est keyframes, it is not enough to know the error for single frames of optical .ow. Instead, 
we need the accumulated error in optical .ow in both directions from the nearest keyframes. If frame 
i is a keyframe, the forward accumulated error map Aif +1 is simply set ff f to E. To compute A, we .rst 
warp the values of Afor­ i+1 i+2 i+1 ward in time using the calculated .ow. Then, we set Afi+2 equal 
to this warped accumulated error map plus Eif +2. This set of calcula­tions is performed from each keyframe 
forward until the following keyframe is reached. The backward accumulated error maps Abi are computed 
similarly. Thus, at a frame j, the accumulated error maps Afj and Abj give us a measure at each pixel 
of the accuracy of .ow estimation from the previous and following keyframes. 3.2.2 Combining forward 
and backward .ow Once optical .ow and the error maps have been calculated, we .ow the trimaps forward 
in time from the hand-drawn keyframes. That is, a trimap is formed at frame i +1 by warping the keyframe 
trimap i using the calculated forward .ow vectors. We add an ad­ditional validity bit for each pixel 
of the .owed trimaps that indi­cates whether the calculated trimap value is trusted; this validity bit 
Vi(x, y) is set to 0 if Eif (x, y) is greater than a certain thresh­old (experimentally set to 30). This 
bit indicates whether a .owed trimap value is to be trusted. When calculating trimap i +2 from trimap 
i+1, the validity bits are also warped forward and combined conjunctively. That is, if forward .ow vectors 
indicate that pixel (x, y) in frame i +1 .ows to (x . ,y .) in frame i +2, the validity bit ..f .. at 
(x ,y ) in trimap i+2 is set to Vi+1(x, y).(Ei+2(x ,y ) < 30). After calculating each warped trimap, 
we improve it by performing Bayesian alpha estimation. Untrusted pixels whose validity bits are 0 are 
labelled as unknown : we are not con.dent of the labelling of these pixels and do not want them corrupting 
the color distribu­tions. The result of the alpha estimation is thresholded (within 10 of de.nitely foreground 
or de.nitely background ) and used to improve the trimap. If alpha estimation con.rms the labelling of 
an untrusted pixel, the validity bit is set to 1. If a pixel labelled as un­ known is identi.ed as foreground 
or background , the label is changed accordingly. In the second pass, we start from each keyframe and 
.ow back­ wards in time. Now, however, we must combine the trimap pre­diction in the forward direction 
with the trimap prediction in the backward direction. This is made simple by the accumulated error maps 
that we calculated earlier. First, the backward trimap predic­tion for frame i is calculated using a 
technique symmetric to the forward trimap calculation. Then, at each pixel, we use the lesser of Aif 
and Abi to select a trimap value. In practice, we add an addi­tional penalty term to this comparison; 
if the forward trimap label is unknown we add a penalty of 50 to Aif , and likewise for the backward 
trimap. Our results were improved by this penalty term because of Observation 2; unknown pixels are near 
depth discon­tinuities and should be trusted less. If a background plate is available, it is useful to 
incorporate this information when .ow is invalid from both directions. In the event that the forward 
and backward validity bits are both 0, we can resort to a simple form of difference matting. In particular, 
we compute the RGB L2 distance between the observed color and the back­ground color and apply user de.ned 
thresholds to map the distance to a trimap value. Finally, during the second pass each computed trimap 
is passed through alpha estimation, and the results are used to improve the trimap as described earlier. 
 3.3 Bayesian matting with background When the background is not available, we run the matting algo­rithm 
on the video frames and trimaps exactly as described in Sec­tion 2.1. When available, however, the estimated 
background offers three distinct advantages. First, the distribution of the background is tighter and 
more accurate. (However, due to sensor noise, the back­ground color is still not a point in color space, 
but is modeled by a Gaussian distribution with a small standard deviation centered at the estimated color.) 
Figure 3 shows that the extracted alpha matte is much improved with the help of the clean plate. Second, 
we no longer need to compute background statistics by sampling neigh­borhoods and computing means and 
covariances, thus speeding up the matting process. Finally, the neighborhood windows no longer have to 
be large enough to span the unknown region and include pixels in the known background region. These last 
two factors yield a factor of 10 speedup in the matting process. 3.4 Smoke matting We have also developed 
a simple extension for extracting mattes of .owing, participating media, such as smoke, given a known 
background. Figure 4 illustrates the smoke matting process for a smoking actor. First, applying the video 
matting technique de­scribed in this section to just the actor, we pull his matte and re­move him from 
the scene. We then compute the difference between the matted-out image and the background image, and 
all pixels that are different by more than a threshold (5% in our example) are selected for estimating 
foreground statistics. By treating the color of the smoke as a constant value that is simply composited 
with a varying alpha over the background, we need only discover that foreground color in order to estimate 
the matte. For each selected pixel, if it has been mixed with smoke, then we expect its color to lie 
somewhere along a line in RGB color space between the pixel s known background color and the foreground 
smoke color. By tak­ing all of the selected pixels, we can construct a set of these lines that, barring 
degenerate con.gurations, will roughly intersect at the foreground smoke color. Thus, we compute an initial 
estimate of the foreground color as the least-squares nearest intersection of all the lines. We then 
project the approximate intersection point onto each of the original lines and form a foreground distribution 
used by Bayesian matting. The smoke matte is calculated at every pixel in the image without the actor, 
and then the actor and smoke mattes are combined.  4 Results We have applied our new video-matting algorithm 
to a number of video sequences. The .nal results are best viewed in video form, but we present stills 
for several instructive examples in this section. sequence frames initial keys edited keys frames/sec 
Amira (Fig 2) 91 10 2 15 Kim (Fig 3) 101 11 4 15 Smoke (Fig 4) 176 10 1 15 Baseball (video) 161 12 3 
30 Jurassic (video) 96 11 1 30 Table 1 Details for the .ve test sequences. Figure 2 demonstrates the 
importance of using bi-directional opti­cal .ow for computing interpolated trimaps. For this example, 
no background estimation is performed. In .owing from keyframe 27 to 28, a disocclusion of the foreground 
occurs, causing errors in the optical .ow, which lead to unknown labels in these regions. Flowing from 
29 to 28 solves this problem, but introduces some disocclusion in the background. Combining the two .ows 
using the forward/backward sweep described in Section 3.2 gives a more ac­curate trimap and a better 
matte and composite. Figure 3 illustrates the utility of background estimation. After the user provides 
the garbage matte, a background sequence is con­structed. The keyframe trimap, combined with the input 
image, can be used to create a matte using the Bayesian method even with­out the background, but the 
matte contains a number of errors, as shown in the composite over blue. When including the background, 
but using the method of Ruzon and Tomasi, the matte also exhibits artifacts. In fact, when playing this 
result back as a video, tempo­ral .ashing artifacts arise, which are likely due to the static neigh­borhoods 
assembled independently for each frame. Finally, using the Bayesian method combined with the background 
yields a matte that, while not perfect, is free of many of the artifacts of the other two methods, and 
composites well over novel backgrounds. Figure 4 shows a result of smoke matting and illustrates a composite 
over an edited background. In this case, we acquired the original background by locking down the camera 
and .lming in the absence of the actor. Note that while the resulting matte is not perfect around the 
silhouette of the actor, for the purposes of background editing in a particular region, the matte is 
good enough. If more edits were required, the user could focus them only in the regions that would be 
composited over the background edits. Both time of execution and the amount of user interaction depend 
heavily on the nature and resolution of the sequence. Overall, our system can be divided into an unsupervised 
pre-processing phase and an online phase. For the 640x480 Kim sequence (Figure 4), pre­processing took 
about 80 seconds per frame; the calculation of op­tical .ow was by far the largest component of this 
time. For the on­line phase, drawing a keyframe trimap for this sequence took about 2 minutes per trimap. 
Interpolating these trimaps took 12 seconds per frame, with alpha estimation as the bottleneck. The interpola­tion 
process is unsupervised, and the user can draw more keyframes in parallel. As mentioned in Section 3, 
the user can then hand-edit any errors to form additional keyframes; this is quick and requires only 
a couple minutes for the entire sequence. All of our algorithms scale linearly with pixels of resolution, 
but other factors such as the area of unknown regions in the trimaps also affect running time. Choosing 
the frequency of keyframes also depends on the sequence. Complicated geometry such as wispy hair generally 
requires a keyframe every 10 frames. Sequences of sim­pler geometry require keyframes every 20 30 frames. 
Care should be taken to add keyframes when objects enter or leave the .eld of view. The number of keyframes 
required for each sequence, along with the number of additional hand-edited keyframes after the .rst 
pass of the algorithm are given in Table 1. Two sequences listed are found only on the accompanying video. 
 5 Conclusion In this paper, we have presented a new process for video matting that is capable of pulling 
mattes of foregrounds with complex sil­houettes .lmed in motion over natural backgrounds. Our primary 
contribution is a framework that pulls together pieces of existing research and combines their strengths 
while working around their weaknesses. The result is a new kind of rotoscoping approach that .ows trimap 
image segmentations over time and enables the ex­traction of detailed mattes around complex foreground 
silhouettes. In the process, we have introduced a novel method for combining bi-directional optical .ow 
to interpolate trimaps. Further, we have introduced a simple procedure for extracting mattes of participating 
media .lmed against a known background. In the future, we hope to develop an optical .ow algorithm that 
incorporates the notion of blended, complex foreground and back­ground layers, thus improving .ow estimates 
and allowing us to accumulate foreground and background color distributions tempo­rally as well as spatially. 
In addition, when the user edits selected trimaps, it should be possible to generate new trimaps and 
mattes more quickly, perhaps interactively, by taking advantage of the lo­cality of these edits. Ultimately, 
we would like to develop a com­plete tool with an integrated and powerful user interface that could be 
tested by and improved with the help of rotoscoping artists. Acknowledgments The authors would like 
to thank Dan Goldman and actors Amira Fahoum, Kim Bishop and Adam Skrine. Thanks to Universal Stu­dios 
and Amblin Entertainment for permission to use their material. This work was supported by NSF grants 
CCR-987365 and DMS­9803226 and by industrial gifts from Intel, Microsoft and Pixar.  References BARRON, 
J. L., FLEET, D. J., AND BEAUCHEMIN, S. S. 1994. Performance of optical .ow techniques. International 
Journal of Computer Vision 12, 1, 43 77. BERMAN, A., DADOURIAN, A., AND VLAHOS, P., 2000. Method for 
removing from an image the background surrounding a selected object. U.S. Patent 6,134,346. BLACK, M. 
J., AND ANANDAN, P. 1996. The robust estimation of multiple mo­ tions: Parametric and piecewise-smooth 
.ow .elds. Computer Vision and Image Understanding 63, 1, 75 104. BLAKE, A., AND ISARD, M. 1998. Active 
Contours. Springer Verlag, London. CHUANG, Y.-Y., CURLESS, B., SALESIN, D., AND SZELISKI, R. 2001. A 
Bayesian approach to digital matting. In Proceedings of Computer Vision and Pattern Recog­ nition (CVPR 
2001), vol. II, 264 271. GLEICHER, M. 1995. Image snapping. In Proceedings of ACM SIGGRAPH 95, 183 190. 
HILLMAN, P., HANNAH, J., AND RENSHAW, D. 2001. Alpha channel estimation in high resolution images and 
image sequences. In Proceedings of Computer Vision and Pattern Recognition (CVPR 2001), vol. I, 1063 
1068. KELLY, D. 2000. Digital Composition. The Coriolis Group. LEE, M.-C., ET AL. 1997. A layered video 
object coding system using sprite and af.ne motion model. IEEE Transactions on Circuits and Systems for 
Video Tech­ nology 7, 1, 130 145. MITSUNAGA, T., YOKOYAMA, T., AND TOTSUKA, T. 1995. Autokey: Human as­sisted 
key extraction. In Proceedings of ACM SIGGRAPH 95, 265 272. MORTENSEN, E. N., AND BARRETT, W. A. 1995. 
Intelligent scissors for image composition. In Proceedings of ACM SIGGRAPH 95, 191 198. PORTER, T., AND 
DUFF, T. 1984. Compositing digital images. In Computer Graphics (Proceedings of ACM SIGGRAPH 84), vol. 
18, 253 259. RUZON, M. A., AND TOMASI, C. 2000. Alpha estimation in natural images. In Proceedings of 
Computer Vision and Pattern Recognition (CVPR 2000), 18 25. SMITH, A. R., AND BLINN, J. F. 1996. Blue 
screen matting. In Proceedings of ACM SIGGRAPH 96, 259 268. SUN, S., HAYNOR, D., AND KIM, Y. 2000. Motion 
estimation based on optical .ow with adaptive gradients. In Proceedings of International Conference on 
Image Processing (ICIP 2000), vol. I, 852 855. SZELISKI, R., AND SHUM, H.-Y. 1997. Creating full view 
panoramic mosaics and environment maps. In Proceedings of ACM SIGGRAPH 97, 251 258. WANG, J. Y. A., AND 
ADELSON, E. H. 1994. Representing moving images with layers. IEEE Transactions on Image Processing 3, 
5, 625 638.  trimap27-28 trimap28 trimap28+29 alpha 28 Figure 2 Combining bi-directional .ow. For frames 
27, 28, and 29 (shown above) and the current trimaps for frames 27 and 29 (neither shown), we estimate 
the trimap for frame 28. Flowing the trimap in the forward direction (27 -28) yields a trimap with extra 
uncertainty due to disocclusion as the actor s face turns to the right. The trimap predicted by .owing 
backward (28 + 29) has less uncertainty in those regions, but suffers from disocclusions where the background 
is newly exposed. By combining the information in both these trimaps, we can compute trimap 28 automatically, 
which is better than either one alone. The right column shows a composite into a new scene (top) using 
the pulled matte (bottom) based on the combined trimap. (a) (b) (c) (d) Figure 3 Background estimation 
in Bayesian matting. On the far left are the keyframe trimap and estimated background for a single frame. 
Using the original image (a) and the trimap without the background, Bayesian matting pulls a matte with 
errors, as shown in a composite over blue (b). Including the background but using instead the method 
of Ruzon and Tomasi gives an improved result (c), but .aws in the matte are still visible. Applying the 
Bayesian matting with background results in a higher-quality matte and composite (d). (a) (b) (c) (d) 
Figure 4 Smoke matting. The input image (a) is part of a sequence for which keyframe trimaps and trimap 
.ow have been computed, and for which a background plate is available. Using the alpha matte as a garbage 
matte, the foreground actor is removed (b). After applying the participating-media matting algorithm 
described in Section 3.4, we obtain a matte for the smoke, which is combined with the actor s matte to 
yield a complete matte (c). We can then composite the foreground over an edited version of the background 
as shown here (d).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566573</article_id>
		<sort_key>249</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Gradient domain high dynamic range compression]]></title>
		<page_from>249</page_from>
		<page_to>256</page_to>
		<doi_number>10.1145/566570.566573</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566573</url>
		<abstract>
			<par><![CDATA[We present a new method for rendering high dynamic range images on conventional displays. Our method is conceptually simple, computationally efficient, robust, and easy to use. We manipulate the gradient field of the luminance image by attenuating the magnitudes of large gradients. A new, low dynamic range image is then obtained by solving a Poisson equation on the modified gradient field. Our results demonstrate that the method is capable of drastic dynamic range compression, while preserving fine details and avoiding common artifacts, such as halos, gradient reversals, or loss of local contrast. The method is also able to significantly enhance ordinary images by bringing out detail in dark regions.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[digital photography]]></kw>
			<kw><![CDATA[high dynamic range compression]]></kw>
			<kw><![CDATA[image processing]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[signal processing]]></kw>
			<kw><![CDATA[tone mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40026606</person_id>
				<author_profile_id><![CDATA[81100376142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Raanan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fattal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hebrew University of Jerusalem]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14153841</person_id>
				<author_profile_id><![CDATA[81311486606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lischinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hebrew University of Jerusalem]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43115602</person_id>
				<author_profile_id><![CDATA[81100051028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Werman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hebrew University of Jerusalem]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGGARWAL, M., AND AHUJA, N. 2001. High dynamic range panoramic imaging. In Proc. IEEE ICCV, vol. I, 2-9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AGGARWAL, M., AND AHUJA, N. 2001. Split aperture imaging for high dynamic range. In Proc. IEEE ICCV, vol. II, 10-17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHIU, K., HERF, M., SHIRLEY, P., SWAMY, S., WANG, C., AND ZIMMERMAN, K. 1993. Spatially nonuniform scaling functions for high contrast images. In Proc. Graphics Interface '93, Morgan Kaufmann, 245-253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732299</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., TCHOU, C., HAWKINS, T., AND DEBEVEC, P. 2001. Real-time high-dynamic range texture mapping. In Rendering Techniques 2001, S. J. Gortler and K. Myszkowski, Eds. Springer-Verlag, 313-320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. E., AND MALIK, J. 1997. Recovering high dynamic range radiance maps from photographs. In Proc. ACM SIGGRAPH 97, T. Whitted, Ed., 369-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. 1998. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In Proc. ACM SIGGRAPH 98, M. Cohen, Ed., 189-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DICARLO, J. M., AND WANDELL, B. A. 2001. Rendering high dynamic range images. In Proceedings of the SPIE: Image Sensors, vol. 3965, 392-401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>48906</ref_obj_id>
				<ref_obj_pid>48904</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FRANKOT, R. T., AND CHELLAPPA, R. 1988. A method for enforcing integrability in shape from shading algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence 10, 4 (July), 439-451.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>548894</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HARRIS, J. W., AND STOCKER, H. 1998. Handbook of Mathematics and Computational Science. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HORN, B. K. P. 1974. Determining lightness from an image. Computer Graphics and Image Processing 3, 1 (Dec.), 277-299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319385</ref_obj_id>
				<ref_obj_pid>2318950</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[JOBSON, D. J., RAHMAN, Z., AND WOODELL, G. A. 1997. A multi-scale Retinex for bridging the gap between color images and the human observation of scenes. IEEE Transactions on Image Processing 6, 7 (July), 965-976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LAND, E. H., AND MCCANN, J. J. 1971. Lightness and Retinex theory. Journal of the Optical Society of America 61, 1 (Jan.), 1-11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[NAYAR, S. K., AND MITSUNAGA, T. 2000. High dynamic range imaging: Spatially varying pixel exposures. In Proc. IEEE CVPR.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PATTANAIK, S. N., FERWERDA, J. A., FAIRCHILD, M. D., AND GREENBERG, D. P. 1998. A multiscale model of adaptation and spatial vision for realistic image display. In Proc. ACM SIGGRAPH 98, M. Cohen, Ed., 287-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78304</ref_obj_id>
				<ref_obj_pid>78302</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PERONA, P., AND MALIK, J. 1990. Scale-space and edge detection using anisotropic diffusion. IEEE Transactions on Pattern Analysis and Machine Intelligence 12, 7 (July), 629-639.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>29046</ref_obj_id>
				<ref_obj_pid>29040</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PIZER, S. M., AMBURN, E. P., AUSTIN, J. D., CROMARTIE, R., GESELOWITZ, A., GREER, T., TER HAAR ROMENY, B., ZIMMERMAN, J. B., AND ZUIDERVELD, K. 1987. Adaptive histogram equalization and its variations. Computer Vision, Graphics, and Image Processing 39, 3 (Sept.), 355-368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, W. T., AND FLANNERY, B. P. 1992. Numerical Recipes in C: The Art of Scientific Computing, 2nd ed. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND HANRAHAN, P. 2001. A signal-processing framework for inverse rendering. In Proc. ACM SIGGRAPH 2001, E. Fiume, Ed., 117-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SCHECHNER, Y. Y., AND NAYAR, S. K. 2001. Generalized mosaicing. In Proc. IEEE ICCV, vol. I, 17-24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SCHLICK, C. 1994. Quantization techniques for visualization of high dynamic range pictures. In Photorealistic Rendering Techniques, Springer-Verlag, P. Shirley, G. Sakas, and S. M&#252;ller, Eds., 7-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[STOCKHAM, J. T. G. 1972. Image processing in the context of a visual model. In Proceedings of the IEEE, vol. 60, 828-842.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[TANAKA, T., AND OHNISHI, N. 1997. Painting-like image emphasis based on human vision systems. Computer Graphics Forum 16, 3, 253-260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND RUSHMEIER, H. E. 1993. Tone reproduction for realistic images. IEEE Computer Graphics and Applications 13, 6 (Nov.), 42-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311544</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND TURK, G. 1999. LCIS: A boundary hierarchy for detail-preserving contrast reduction. In Proc. ACM SIGGRAPH 99, A. Rockwood, Ed., 83-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300783</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., HODGINS, J. K., AND GUENTER, B. K. 1999. Two methods for display of high contrast images. ACM Transactions on Graphics 18, 1 (Jan.), 56-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[WARD LARSON, G., RUSHMEIER, H., AND PIATKO, C. 1997. A visibility matching tone reproduction operator for high dynamic range scenes. IEEE Transactions on Visualization and Computer Graphics 3, 4, 291-306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>180934</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. 1994. A contrast-based scalefactor for luminance display. In Graphics Gems IV, P. S. Heckbert, Ed. Academic Press Professional, 415-421.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gradient Domain High Dynamic Range Compression Raanan Fattal Dani Lischinski Michael Werman School of 
Computer Science and Engineering* The Hebrew University of Jerusalem Abstract We present a new method 
for rendering high dynamic range im­ages on conventional displays. Our method is conceptually simple, 
computationally ef.cient, robust, and easy to use. We manipulate the gradient .eld of the luminance image 
by attenuating the mag­nitudes of large gradients. A new, low dynamic range image is then obtained by 
solving a Poisson equation on the modi.ed gra­dient .eld. Our results demonstrate that the method is 
capable of drastic dynamic range compression, while preserving .ne details and avoiding common artifacts, 
such as halos, gradient reversals, or loss of local contrast. The method is also able to signi.cantly 
enhance ordinary images by bringing out detail in dark regions. CR Categories: I.3.3 [Computer Graphics]: 
Picture/image generation display algorithms, viewing algorithms; I.4.3 [Im­age Processing and Computer 
Vision]: Enhancement .ltering, grayscale manipulation, sharpening and deblurring Keywords: digital photography, 
high dynamic range compression, image-based rendering, image processing, signal processing, tone mapping 
1 Introduction High dynamic range (HDR) radiance maps are becoming increas­ingly common and important 
in computer graphics. Initially, such maps originated almost exclusively from physically-based lighting 
simulations. Today, however, HDR maps of real scenes are very easy to construct: all you need is a few 
differently exposed pho­tographs of the scene [Debevec and Malik 1997], or a panoramic video scan of 
it [Aggarwal and Ahuja 2001a; Schechner and Nayar 2001]. Furthermore, based on recent developments in 
digital imag­ing technology [Aggarwal and Ahuja 2001b; Nayar and Mitsunaga 2000], it is reasonable to 
assume that tomorrow s digital still and video cameras will capture HDR images and video directly. HDR 
images have many advantages over standard low dynamic range images [Debevec and Malik 1997], and several 
applications have been demonstrated where such images are extremely use­ful [Debevec 1998; Cohen et al. 
2001]. However, HDR images also pose a dif.cult challenge: given that the dynamic range of various common 
display devices (monitors, printers, etc.) is much smaller than the dynamic range commonly found in real-world 
scenes, how can we display HDR images on low dynamic range (LDR) display *e-mail: {raananf | danix | 
werman}@cs.huji.ac.il Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 devices, while preserving as much of their visual content as possi­ble? This 
is precisely the problem addressed in this paper. The problem that we are faced with is vividly illustrated 
by the series of images in Figure 1. These photographs were taken using a digital camera with exposure 
times ranging from 1/1000 to 1/4 of a second (at f/8) from inside a lobby of a building facing glass 
doors leading into a sunlit inner courtyard. Note that each exposure reveals some features that are not 
visible in the other photographs1. For example, the true color of the areas directly illuminated by the 
sun can be reliably assessed only in the least exposed image, since these areas become over-exposed in 
the remainder of the sequence. The color and texture of the stone tiles just outside the door are best 
captured in the middle image, while the green color and the texture of the .cus plant leaves becomes 
visible only in the very last image in the sequence. All of these features, however, are si­multaneously 
clearly visible to a human observer standing in the same location, because of adaptation that takes place 
as our eyes scan the scene [Pattanaik et al. 1998]. Using Debevec and Malik s method [1997], we can compile 
these 8-bit images into a single HDR radiance map with dynamic range of about 25,000:1. How­ever, it 
is not at all clear how to display such an image on a CRT monitor whose dynamic range is typically below 
100:1! In this paper, we present a new technique for high dynamic range compression that enables HDR 
images, such as the one described in the previous paragraph, to be displayed on LDR devices. The proposed 
technique is quite effective, as demonstrated by the top image in Figure 2, yet it is conceptually simple, 
computationally ef.cient, robust, and easy to use. Observing that drastic changes in luminance across 
an HDR image must give rise to luminance gradi­ents of large magnitudes, our approach is to manipulate 
the gradient .eld of the luminance image by attenuating the magnitudes of large gradients. A new, low 
dynamic range image is then obtained by solving a Poisson equation on the modi.ed gradient .eld. The 
current work is not the .rst attempt to tackle this important problem. Indeed, quite a few different 
methods have appeared in the literature over the last decade. A more detailed review of previous work 
is provided in Section 2. In this paper we hope to convince the reader that our approach has some de.nite 
advantages over previous solutions: it does a better job at preserving local contrasts than some previous 
methods, has fewer visible artifacts than others, and yet it is fast and easy to use. These advantages 
make our technique a highly practical tool for high dynamic range compression. We do not attempt to faithfully 
reproduce the response of the human visual system to the original high dynamic range scenes. Nevertheless, 
we have achieved very good results on HDR radiance maps of real scenes, HDR panoramic video mosaics, 
ordinary high­contrast photographs, and medical images.  2 Previous work In the past decade there has 
been considerable work concerned with displaying high dynamic range images on low dynamic range dis­ 
1All of the images in this paper are provided at full resolution on the proceedings CD-ROM (and also 
at http://www.cs.huji.ac.il/ danix/hdrc), as some of the .ne details may be dif.cult to see in the printed 
proceedings.  Figure 1: A series of .ve photographs. The exposure is increasing from left (1/1000 of 
a second) to right (1/4 of a second). play devices. In this section we provide a brief review of previous 
work. More detailed and in-depth surveys are presented by DiCarlo and Wandell [2001] and Tumblin et al. 
[1999]. Most HDR compression methods operate on the luminance channel or perform essentially the same 
processing independently in each of the RGB channels, so throughout most of this paper we will treat 
HDR maps as (scalar) luminance functions. Previous approaches can be classi.ed into two broad groups: 
(1) global (spatially invariant) mappings, and (2) spatially variant op­erators. DiCarlo and Wandell 
[2001] refer to the former as TRCs (tone reproduction curves) and to the latter as TROs (tone repro­duction 
operators); we adopt these acronyms for the remainder of this paper. The most naive TRC linearly scales 
the HDR values such that they .t into a canonic range, such as [0,1]. Such scaling preserves relative 
contrasts perfectly, but the displayed image may suffer se­vere loss of visibility whenever the dynamic 
range of the display is smaller than the original dynamic range of the image, and due to quantization. 
Other common TRCs are gamma correction and histogram equalization. In a pioneering work, Tumblin and 
Rushmeier [1993] describe a more sophisticated non-linear TRC designed to preserve the appar­ent brightness 
of an image based on the actual luminances present in the image and the target display characteristics. 
Ward [1994] sug­gested a simpler linear scale factor automatically determined from image luminances so 
as to preserve apparent contrast and visibil­ity around a particular adaptation level. The most recent 
and most sophisticated, to our knowledge, TRC is described by Ward Larson et al. [1997]. They .rst describe 
a clever improvement to histogram equalization, and then show how to extend this idea to incorporate 
models of human contrast sensitivity, glare, spatial acuity, and color sensitivity effects. This technique 
works very well on a wide variety of images. The main advantage of TRCs lies in their simplicity and 
compu­tational ef.ciency: once a mapping has been determined, the image may be mapped very quickly, e.g., 
using lookup tables. However, such global mappings must be one-to-one and monotonic in order to avoid 
reversals of local edge contrasts. As such, they have a funda­mental dif.culty preserving local contrasts 
in images where the in­tensities of the regions of interest populate the entire dynamic range in a more 
or less uniform fashion. This shortcoming is illustrated in the middle image of Figure 2. In this example, 
the distribution of luminances is almost uniform, and Ward Larson s technique results in a mapping, which 
is rather similar to a simple gamma correction. As a result, local contrast is drastically reduced. Spatially 
variant tone reproduction operators are more .exible than TRCs, since they take local spatial context 
into account when deciding how to map a particular pixel. In particular, such operators can transform 
two pixels with the same luminance value to different display luminances, or two different luminances 
to the same display intensity. This added .exibility in the mapping should make it pos­sible to achieve 
improved local contrast. The problem of high-dynamic range compression is intimately related to the problem 
of recovering re.ectances from an image [Horn 1974]. An image I(x,y)is regarded as a product I(x, y)=R(x,y)L(x,y), 
where R(x,y)is the re.ectance and L(x,y)is the illuminance at each point (x,y). The function R(x,y) is 
commonly referred to as the intrinsic image of a scene. The largest luminance variations in an HDR image 
come from the illuminance function L, since real-world re.ectances are unlikely to create contrasts greater 
than 100:12. Thus, dynamic range compression can, in principle, be achieved by separating an image I 
to its R and L components, scaling down the L component to obtain a new illuminance function L , and 
re­multiplying: I (x,y)=R(x,y)L (x, y). Intuitively, this reduces the contrast between brightly illuminated 
areas and those in deep shadow, while leaving the contrasts due to texture and re.ectance undistorted. 
Tumblin et al. [1999] use this approach for displaying high-contrast synthetic images, where the material 
properties of the surfaces and the illuminance are known at each point in the image, making it possible 
to compute a per­fect separation of an image to various layers of lighting and surface properties. Unfortunately, 
computing such a separation for real images is an ill posed problem [Ramamoorthi and Hanrahan 2001]. 
Conse­quently, any attempt to solve it must make some simplifying as­sumptions regarding R, L, or both. 
For example, homomorphic .ltering [Stockham 1972], an early image enhancement technique, makes the assumption 
that L varies slowly across the image, in con­trast to R that varies abruptly. This means that R can 
be extracted by applying a high-pass .lter to the logarithm of the image. Exponenti­ating the result 
achieves simultaneous dynamic range compression and local contrast enhancement. Similarly, Horn [1974] 
assumes that L is smooth, while R is piecewise-constant, introducing in.­nite impulse edges in the Laplacian 
of the image s logarithm. Thus, L may be recovered by thresholding the Laplacian. Of course, in most 
natural images the assumptions above are violated: for ex­ample, in sunlit scenes illuminance varies 
abruptly across shadow boundaries. This means that L also has high frequencies and intro­duces strong 
impulses into the Laplacian. As a result, attenuating only the low frequencies in homomorphic .ltering 
may give rise to strong halo artifacts around strong abrupt changes in illumi­nance, while Horn s method 
incorrectly interprets sharp shadows as changes in re.ectance. More recently, Jobson et al. [1997] presented 
a dynamic range compression method based on a multiscale version of Land s retinex theory of color vision 
[Land and McCann 1971]. Retinex estimates the re.ectances R(x,y) as the ratio of I(x,y) to its low­pass 
.ltered version. A similar operator was explored by Chiu et al. [1993], and was also found to suffer 
from halo artifacts and dark bands around small bright visible light sources. Jobson et al. compute the 
logarithm of the retinex responses for several low-pass .lters of different sizes, and linearly combine 
the results. The lin­ear combination helps reduce halos, but does not eliminate them entirely. Schlick 
[1994] and Tanaka and Ohnishi [1997] also exper­imented with spatially variant operators and found them 
to produce halo artifacts. Pattanaik and co-workers [1998] describe an impressively com­prehensive computational 
model of human visual system adaptation 2For example, the re.ectance of black velvet is about 0.01, while 
that of snow is roughly 0.93.  Figure 2: Belgium House: An HDR radiance map of a lobby com­pressed for 
display by our method (top), the method of Ward Larson et al. (middle) and the LCIS method (bottom). 
and spatial vision for realistic tone reproduction. Their model en­ables display of HDR scenes on conventional 
display devices, but the dynamic range compression is performed by applying different gain-control factors 
to each bandpass, which also results in halos around strong edges. In fact, DiCarlo and Wandell [2001], 
as well as Tumblin and Turk [1999] demonstrate that this is a fundamental problem with any multi-resolution 
operator that compresses each resolution band differently. In order to eradicate the notorious halo artifacts 
Tumblin and Turk [1999] introduce the low curvature image simpli.er (LCIS) hi­erarchical decomposition 
of an image. Each level in this hierarchy is generated by solving a partial differential equation inspired 
by anisotropic diffusion [Perona and Malik 1990] with a different dif­fusion coef.cient. The hierarchy 
levels are progressively smoother versions of the original image, but the smooth (low-curvature) re­gions 
are separated from each other by sharp boundaries. Dynamic range compression is achieved by scaling down 
the smoothest ver­sion, and then adding back the differences between successive lev­els in the hierarchy, 
which contain details removed by the simpli­.cation process. This technique is able to drastically compress 
the dynamic range, while preserving the .ne details in the image. How­ever, the results are not entirely 
free of artifacts. Tumblin and Turk note that weak halo artifacts may still remain around certain edges 
in strongly compressed images. In our experience, this technique sometimes tends to overemphasize .ne 
details. For example, in the bottom image of Figure 2, generated using this technique, certain features 
(door, plant leaves) are surrounded by thin bright outlines. In addition, the method is controlled by 
no less than 8 parameters, so achieving an optimal result occasionally requires quite a bit of trial-and-error. 
Finally, the LCIS hierarchy construction is compu­tationally intensive, so compressing a high-resolution 
image takes a substantial amount of time.  3 Gradient domain HDR compression Informally, our approach 
relies on the widely accepted assumptions [DiCarlo and Wandell 2001] that the human visual system is 
not very sensitive to absolute luminances reaching the retina, but rather responds to local intensity 
ratio changes and reduces the effect of large global differences, which may be associated with illumination 
differences. Our algorithm is based on the rather simple observation that any drastic change in the luminance 
across a high dynamic range im­age must give rise to large magnitude luminance gradients at some scale. 
Fine details, such as texture, on the other hand, correspond to gradients of much smaller magnitude. 
Our idea is then to iden­tify large gradients at various scales, and attenuate their magnitudes while 
keeping their direction unaltered. The attenuation must be progressive, penalizing larger gradients more 
heavily than smaller ones, thus compressing drastic luminance changes, while preserv­ing .ne details. 
A reduced high dynamic range image is then re­constructed from the attenuated gradient .eld. It should 
be noted that all of our computations are done on the logarithm of the luminances, rather than on the 
luminances them­selves. This is also the case with most of the previous methods reviewed in the previous 
section. The reason for working in the log domain is twofold: (a) the logarithm of the luminance is a 
(crude) approximation to the perceived brightness, and (b) gradients in the log domain correspond to 
ratios (local contrasts) in the luminance domain. We begin by explaining the idea in 1D. Consider a high 
dynamic range 1D function. We denote the logarithm of this function by H(x). As explained above, our 
goal is to compress large magnitude changes in H, while preserving local changes of small magnitude, 
as much as possible. This goal is achieved by applying an appro­priate spatially variant attenuating 
mapping F to the magnitudes of the derivatives H'(x). More speci.cally, we compute: G(x)=H'(x)F(x). Note 
that G has the same sign as the original derivative H. every­where, but the magnitude of the original 
derivatives has been al­tered by a factor determined by F, which is designed to attenuate large derivatives 
more than smaller ones. Actually, as explained in Section 4, F accounts for the magnitudes of derivatives 
at different scales.   (a) (b) (c) (d) (e) (f) Figure 3: (a) An HDR scanline with dynamic range of 
2415:1. (b) H(x)=log(scanline). (c) The derivatives H'(x). (d) Attenuated derivatives G(x); (e) Reconstructed 
signal I(x)(as de.ned in eq. 1); (f) An LDR scanline exp(I(x)): the new dynamic range is 7.5:1. Note 
that each plot uses a different scale for its vertical axis in order to show details, except (c) and 
(d) that use the same vertical axis scaling in order to show the amount of attenuation applied on the 
derivatives. We can now reconstruct a reduced dynamic range function I (up to an additive constant C) 
by integrating the compressed derivatives: [ x I(x)=C +G(t)dt. (1) 0 Finally, we exponentiate in order 
to return to luminances. The entire process is illustrated in Figure 3. In order to extend the above 
approach to 2D HDR functions H(x,y)we manipulate the gradients .H, instead of the derivatives. Again, 
in order to avoid introducing spatial distortions into the im­age, we change only the magnitudes of the 
gradients, while keeping their directions unchanged. Thus, similarly to the 1D case, we com­pute G(x,y)=.H(x,y)F(x,y). 
 Unlike the 1D case we cannot simply obtain a compressed dynamic range image by integrating G, since 
it is not necessarily integrable. In other words, there might not exist an image I such that G =.I! In 
fact, the gradient of a potential function (such as a 2D image) must be a conservative .eld [Harris and 
Stocker 1998]. In other words, the gradient .I =(. I/. x,. I/. y)must satisfy . 2I . 2I = , . x. y . 
y. x which is rarely the case for our G. One possible solution to this problem is to orthogonally project 
G onto a .nite set of orthonormal basis functions spanning the set of integrable vector .elds, such as 
the Fourier basis functions [Frankot and Chellappa 1988]. In our method we employ a more direct and more 
ef.cient approach: search the space of all 2D potential func­tions for a function I whose gradient is 
the closest to G in the least­squares sense. In other words, I should minimize the integral [[ F(.I,G)dx 
dy, (2) (t(t 22 .I .I where F(.I,G)=I.I - GI2 =- Gx+- Gy. .x .y According to the Variational Principle, 
a function I that mini­mizes the integral in (2) must satisfy the Euler-Lagrange equation . Fd . Fd . 
F -- =0, . I dx . Ix dy . Iy which is a partial differential equation in I. Substituting F we obtain 
the following equation: ()() . 2I . Gx . 2I . Gy 2 - +2 - =0. . x2 . x . y2 . y Dividing by 2 and rearranging 
terms, we obtain the well-known Poisson equation .2I =divG (3) .Gx .Gy divergence of the vector .eld 
G, de.ned as div G =+. This .x .y is a linear partial differential equation, whose numerical solution 
is described in Section 5.  4 Gradient attenuation function As explained in the previous section, our 
method achieves HDR compression by attenuating the magnitudes of the HDR image gra­dients by a factor 
of F(x,y) at each pixel. We would like the at­tenuation to be progressive, shrinking gradients of large 
magnitude more than small ones. Real-world images contain edges at multiple scales. Conse­quently, in 
order to reliably detect all of the signi.cant inten­sity transitions we must employ a multi-resolution 
edge detection scheme. However, we cannot simply attenuate each gradient at the resolution where it was 
detected. This could result in halo artifacts around strong edges, as mentioned in Section 2. Our solution 
is to propagate the desired attenuation from the level it was detected at to the full resolution image. 
Thus, all gradient manipulations occur at a single resolution level, and no halo artifacts arise. We 
begin by constructing a Gaussian pyramid H0,H1,...,Hd , where H0 is the full resolution HDR image and 
Hd is the coarsest level in the pyramid. d is chosen such that the width and the height of Hd are at 
least 32. At each level k we compute the gradients using central differences: () Hk(x +1,y)- Hk(x - 1,y) 
Hk(x,y +1)- Hk(x,y - 1) .Hk =,. 2k+12k+1 At each level k a scaling factor .k(x,y)is determined for each 
pixel based on the magnitude of the gradient there: ()ß a I.Hk(x,y)I .k(x,y)= . I.Hk(x,y)I a This is 
a two-parameter family of functions. The .rst parameter a determines which gradient magnitudes remain 
unchanged (mul­tiplied by a scale factor of 1). Gradients of larger magnitude are attenuated (assuming 
ß < 1), while gradients of magnitude smaller than a are slightly magni.ed. In all the results shown in 
this pa­per we set a to 0.1 times the average gradient magnitude, and ß between 0.8 and 0.9. The full 
resolution gradient attenuation function F(x, y) is com­puted in a top-down fashion, by propagating the 
scaling factors .k(x,y) from each level to the next using linear interpolation and accumulating them 
using pointwise multiplication. More formally, the process is given by the equations: Fd (x,y)= .d (x,y) 
() Fk(x,y)= LFk+1(x, y).k(x,y) F(x,y)= F0(x,y) where d is the coarsest level, Fk denotes the accumulated 
attenua­tion function at level k, and L is an upsampling operator with linear interpolation. As a result, 
the gradient attenuation at each pixel of the .nest level is determined by the strengths of all the edges 
(from different scales) passing through that location in the image. Figure 4 shows attenuation coef.cients 
computed for the Belgium House HDR radiance map. It is important to note that although the computation 
of the gra­dient attenuation function is done in a multi-resolution fashion, ul­timately only the gradients 
at the .nest resolution are manipulated, thus avoiding halo artifacts that typically arise when different 
reso­lution levels are manipulated separately. 5 Implementation In order to solve a differential equation 
such as (3) one must .rst specify the boundary conditions. In our case, the most natural choice appears 
to be the Neumann boundary conditions .I · n =0 (the derivative in the direction normal to the boundary 
is zero). With these boundary conditions the solution is now de.ned up to a single additive term, which 
has no real meaning since we shift and scale the solution in order to .t it into the display device limits. 
Since both the Laplacian .2 and div are linear operators, approx­imating them using standard .nite differences 
yields a linear system of equations. More speci.cally, we approximate: .2I(x,y) I(x+1, y)+I(x-1, y)+I(x, 
y+1)+I(x,y-1)-4I(x, y) taking the pixel grid spacing to be 1 at the full resolution of the im­age. The 
gradient .H is approximated using the forward difference .H(x,y) (H(x +1,y)- H(x,y),H(x,y +1)- H(x,y)), 
 while for divG we use backward difference approximations div G Gx(x, y)- Gx(x - 1,y)+Gy(x,y)- Gy(x, 
y - 1). This combination of forward and backward differences ensures that the approximation of div G 
is consistent with the central difference scheme used for the Laplacian. At the boundaries we use the 
same de.nitions, but assume that the derivatives around the original image grid are 0. For example, for 
each pixel on the left image boundary we have the equation I(-1,y)- I(0, y)=0. The .nite difference scheme 
yields a large system of linear equa­tions one for each pixel in the image, but the corresponding ma­trix 
has only .ve nonzero elements in each row, since each pixel is coupled only with its four neighbors. 
We solve this system using the Full Multigrid Algorithm [Press et al. 1992], with Gauss-Seidel smoothing 
iterations. This leads to O(n) operations to reach an ap­proximate solution, where n is the number of 
pixels in the image. Another alternative is to use a rapid Poisson solver , which uses the fast Fourier 
transform to invert the Laplacian operator. How­ever, the complexity with this approach would be O(nlogn). 
As mentioned earlier, our method operates on the luminances of an HDR radiance map. In order to assign 
colors to the pixels of the compressed dynamic range image we use an approach similar to those of Tumblin 
and Turk [1999] and Schlick [1994]. More speci.cally, the color channels of a pixel in the compressed 
dy­namic range image are computed as follows: () sCin Cout =Lout Lin for C =R,G,B. Lin and Lout denote 
the luminance before and after HDR compression, respectively, and the exponent s controls the color saturation 
of the resulting image. We found values between 0.4 and 0.6 to produce satisfactory results.  6 Results 
Multiple exposure HDRs. We have experimented with our method on a variety of HDR radiance maps of real 
scenes. In all cases, our method produced satisfactory results without much pa­rameter tweaking. In certain 
cases we found that the subjective quality of the resulting image is slightly enhanced by running a standard 
sharpening operation. The computation times range from 1.1 seconds for an 512 by 384 image to 4.5 seconds 
for an 1024 by 768 image on a 1800 MHz Pentium 4. The top row in Figure 5 shows three different renderings 
of a streetlight on a foggy night radiance map3. The dynamic range in this scene exceeds 100,000:1. The 
left image was produced us­ing the method of Ward Larson et al. [1997], and the right image4 was produced 
by Tumblin and Turk s [1999] LCIS method. The middle image was generated by our method. The left image 
loses visibility in a wide area around the bright light, details are lost in the shadowed regions, and 
the texture on the ground is washed out. The LCIS image (right) exhibits a grainy texture in smooth areas, 
and appears to slightly overemphasize edges, resulting in an embossed , non-photorealistic appearance. 
In our image (middle) smoothness is preserved in the foggy sky, yet at the same time .ne details are 
well preserved (tree leaves, ground texture, car outlines). Our method took 5 seconds to compute this 
751 by 1130 image, while the LCIS method took around 8.5 minutes. The second row of images in Figure 
5 shows a similar compari­son using an HDR radiance map of the Stanford Memorial church5. The dynamic 
range in this map exceeds 250,000:1. Overall, the same observations as before hold for this example as 
well. In the left image the details in the dark regions are dif.cult to see, while the skylight and the 
stained glass windows appear over-exposed. In the LCIS image (right) the .oor appears slightly bumpy, 
while our image (middle) shows more details and conveys a more realistic impression. The last row of 
images6 in Figure 5 and the .rst row in Fig­ure 6 show several additional examples of HDR compression 
by 3Radiance map courtesy of Jack Tumblin, Northwestern University. 4Image reprinted by permission, c 
@ 1999 Jack Tumblin [1999]. 5Radiance map courtesy of Paul Debevec. 6Source exposures courtesy of Shree 
Nayar.  Figure 5: The top two rows compare results produced by our method (middle column) to those 
of Ward Larson et al.(left column) and those of Tumblin and Turk (right column). The differences are 
discussed in Section 6. The bottom row shows three more examples of results produced by our method (the 
thumbnails next to each image show some of the LDR images from which the HDR radiance map was constructed). 
 Figure 6: Top row: more examples of HDR radiance map compression. Our method successfully combines features 
that can only be captured using very different exposures into a single image. Second row: an HDR panoramic 
video mosaic. The remaining images demonstrate that our method can also be used for ordinary image enhancement. 
See Section 6 for more detailed explanations. our method. Next to each image there are thumbnail images 
show­ing some of the exposures used to construct the HDR map. Note that our method manages to combine 
in a realistic manner details that can only be captured with very different exposures. The top right 
image in Figure 6 (a stained glass window in the National Cathedral in Washington, DC) was made using 
only the two ex­posures shown on its left7. These two exposures are roughly four stops apart; they were 
taken by a professional photographer, who manually blended these two images together in order to display 
both the bright window and the dark stone surfaces simultaneously (http://users.erols.com/maxlyons). 
Our method achieves a similar effect automatically, while revealing more detail in the dark regions. 
HDR panoramic video mosaics. A popular way to acquire a panoramic image is to scan a scene using a video 
camera and then construct a mosaic from the video frames. If we let the camera s auto-exposure control 
set the correct exposure for each frame, each scene element is imaged at multiple aperture settings and 
we can construct an HDR as in [Debevec and Malik 1997]. Using special­ized hardware [Schechner and Nayar 
2001; Aggarwal and Ahuja 2001a] also produced HDR panoramic video mosaics. The second row in Figure 6 
shows an HDR panorama com­pressed by our method. The top left image simulates what the panorama would 
have looked like with an exposure suitable for the left part of the panorama, while the one below it 
simulates an ex­posure suitable for the right part of the panorama. Clearly, none of these two exposure 
settings yield satisfactory results. With our HDR compression method we were able to obtain the panorama 
on the right, in which detail is visible across the entire .eld of view. LDR image enhancement. Our method 
can also be used to enhance ordinary (LDR) images. By attenuating strong gradients and rescaling the 
reconstructed image back to the original 0..255 range, small contrasts in dark regions become easier 
to see. The .ve images of the Notre Dame de Paris (Figure 6) demon­strate image enhancement using our 
method. The top left image is the original; the top right image is the result produced by our method. 
The bottom row shows the best results we could obtain with gamma correction (left), histogram equalization 
(middle), and contrast limited adaptive histogram equalization [Pizer et al. 1987] (right). Notice that 
our result brings out more details from the shad­owed areas, while maintaining good contrasts elsewhere 
(brick wall in the foreground, .ne details on the building). Adaptive histogram equalization is almost 
as good, but it introduces halo artifacts in the sky along the roofs and the treetop on the right. The 
bottom row of Figure 6 shows two more examples. The example on the left is a typical example of an image 
containing sunlight and shadows. Again, our method (on its right) succeeds in bringing out the details 
from the shadowed areas. The pair on the right shows a dark, low contrast .uoroscopic fe­mur image. After 
enhancement using our method the bone structure is visible much more clearly (note that the femur canal 
becomes clearly visible). 7 Conclusions and Future Work We have described a new, simple, computationally 
ef.cient, and robust method for high dynamic range compression, which makes it possible to display HDR 
images on conventional displays. Our method attenuates large gradients and then constructs a low dy­namic 
range image by solving a Poisson equation on the modi.ed gradient .eld. Future work will concentrate 
on the many different exciting pos­sible applications of the construction of an image from modi.ed gradient 
.elds. Preliminary results show promise in denoising, edge manipulation and non-photorealistic rendering 
from real im­ages. In addition, we would like to extend our work so as to in­ 7Exposures courtesy of 
Max Lyons, c@ 2001 Max Lyons. corporate various psychophysical properties of human visual per­ception 
in order to make our technique more useful for applications such as lighting design or visibility analysis. 
Acknowledgments We would like to thank Paul Debevec, Max Lyons, Shree Nayar, Jack Tumblin, and Greg Ward 
for making their code and images available. Thanks also go to Siggraph s anonymous reviewers for their 
comments. This work was supported in part by the Israel Sci­ence Foundation founded by the Israel Academy 
of Sciences and Humanities.   References AGGARWAL, M., AND AHUJA, N. 2001. High dynamic range panoramic 
imaging. In Proc. IEEE ICCV, vol. I, 2 9. AGGARWAL, M., AND AHUJA, N. 2001. Split aperture imaging for 
high dynamic range. In Proc. IEEE ICCV, vol. II, 10 17. CHIU, K., HERF, M., SHIRLEY,P., SWAMY, S., WANG, 
C., AND ZIMMERMAN, K. 1993. Spatially nonuniform scaling functions for high contrast images. In Proc. 
Graphics Interface 93, Morgan Kaufmann, 245 253. COHEN, J., TCHOU, C., HAWKINS,T., AND DEBEVEC, P. 2001. 
Real-time high­dynamic range texture mapping. In Rendering Techniques 2001, S. J. Gortler and K. Myszkowski, 
Eds. Springer-Verlag, 313 320. DEBEVEC, P. E., AND MALIK, J. 1997. Recovering high dynamic range radiance 
maps from photographs. In Proc. ACM SIGGRAPH 97, T. Whitted, Ed., 369 378. DEBEVEC, P. 1998. Rendering 
synthetic objects into real scenes: Bridging tradi­tional and image-based graphics with global illumination 
and high dynamic range photography. In Proc. ACM SIGGRAPH 98, M. Cohen, Ed., 189 198. DICARLO, J. M., 
AND WANDELL, B. A. 2001. Rendering high dynamic range images. In Proceedings of the SPIE: Image Sensors, 
vol. 3965, 392 401. FRANKOT,R. T., AND CHELLAPPA, R. 1988. A method for enforcing integrability in shape 
from shading algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence 10, 4 (July), 
439 451. HARRIS,J. W., AND STOCKER, H. 1998. Handbook of Mathematics and Computa­tional Science. Springer-Verlag. 
HORN, B. K. P. 1974. Determining lightness from an image. Computer Graphics and Image Processing 3, 1 
(Dec.), 277 299. JOBSON, D. J., RAHMAN, Z., AND WOODELL, G. A. 1997. A multi-scale Retinex for bridging 
the gap between color images and the human observation of scenes. IEEE Transactions on Image Processing 
6, 7 (July), 965 976. LAND, E. H., AND MCCANN, J. J. 1971. Lightness and Retinex theory. Journal of the 
Optical Society of America 61, 1 (Jan.), 1 11. NAYAR, S. K., AND MITSUNAGA, T. 2000. High dynamic range 
imaging: Spatially varying pixel exposures. In Proc. IEEE CVPR. PATTANAIK, S. N., FERWERDA, J. A., FAIRCHILD, 
M. D., AND GREENBERG,D. P. 1998. A multiscale model of adaptation and spatial vision for realistic image 
dis­play. In Proc. ACM SIGGRAPH 98, M. Cohen, Ed., 287 298. PERONA,P., AND MALIK, J. 1990. Scale-space 
and edge detection using anisotropic diffusion. IEEE Transactions on Pattern Analysis and Machine Intelligence 
12,7 (July), 629 639. PIZER, S. M., AMBURN,E. P., AUSTIN, J. D., CROMARTIE, R., GESELOWITZ, A., GREER,T., 
TER HAAR ROMENY, B., ZIMMERMAN, J. B., AND ZUIDERVELD, K. 1987. Adaptive histogram equalization and its 
variations. Computer Vision, Graphics, and Image Processing 39, 3 (Sept.), 355 368. PRESS, W. H., TEUKOLSKY, 
S. A., VETTERLING,W. T., AND FLANNERY,B. P. 1992. Numerical Recipes in C: The Art of Scienti.c Computing, 
2nd ed. Cambridge University Press. RAMAMOORTHI, R., AND HANRAHAN, P. 2001. A signal-processing framework 
for inverse rendering. In Proc. ACM SIGGRAPH 2001, E. Fiume, Ed., 117 128. SCHECHNER,Y. Y., AND NAYAR, 
S. K. 2001. Generalized mosaicing. In Proc. IEEE ICCV, vol. I, 17 24. SCHLICK, C. 1994. Quantization 
techniques for visualization of high dynamic range pictures. In Photorealistic Rendering Techniques, 
Springer-Verlag, P. Shirley, G. Sakas, and S. M¨uller, Eds., 7 20. STOCKHAM, J. T. G. 1972. Image processing 
in the context of a visual model. In Proceedings of the IEEE, vol. 60, 828 842. TANAKA,T., AND OHNISHI, 
N. 1997. Painting-like image emphasis based on human vision systems. Computer Graphics Forum 16, 3, 253 
260. TUMBLIN, J., AND RUSHMEIER, H. E. 1993. Tone reproduction for realistic images. IEEE Computer Graphics 
and Applications 13, 6 (Nov.), 42 48. TUMBLIN,J., AND TURK,G.1999.LCIS:Aboundaryhierarchyfordetail-preserving 
contrast reduction. In Proc. ACM SIGGRAPH 99, A. Rockwood, Ed., 83 90. TUMBLIN, J., HODGINS, J. K., AND 
GUENTER, B. K. 1999. Two methods for display of high contrast images. ACM Transactions on Graphics 18, 
1 (Jan.), 56 94. WARD LARSON, G., RUSHMEIER, H., AND PIATKO, C. 1997. A visibility matching tone reproduction 
operator for high dynamic range scenes. IEEE Transactions on Visualization and Computer Graphics 3, 4, 
291 306. WARD, G. J. 1994. A contrast-based scalefactor for luminance display. In Graphics Gems IV, P. 
S. Heckbert, Ed. Academic Press Professional, 415 421. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566574</article_id>
		<sort_key>257</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Fast bilateral filtering for the display of high-dynamic-range images]]></title>
		<page_from>257</page_from>
		<page_to>266</page_to>
		<doi_number>10.1145/566570.566574</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566574</url>
		<abstract>
			<par><![CDATA[We present a new technique for the display of high-dynamic-range images, which reduces the contrast while preserving detail. It is based on a two-scale decomposition of the image into a base layer, encoding large-scale variations, and a detail layer. Only the base layer has its contrast reduced, thereby preserving detail. The base layer is obtained using an edge-preserving filter called the <i>bilateral filter.</i> This is a non-linear filter, where the weight of each pixel is computed using a Gaussian in the spatial domain multiplied by an influence function in the intensity domain that decreases the weight of pixels with large intensity differences. We express bilateral filtering in the framework of robust statistics and show how it relates to anisotropic diffusion. We then accelerate bilateral filtering by using a piecewise-linear approximation in the intensity domain and appropriate subsampling. This results in a speed-up of two orders of magnitude. The method is fast and requires no parameter setting.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[contrast reduction]]></kw>
			<kw><![CDATA[edge-preserving filtering]]></kw>
			<kw><![CDATA[image processing]]></kw>
			<kw><![CDATA[tone mapping]]></kw>
			<kw><![CDATA[weird maths]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14030619</person_id>
				<author_profile_id><![CDATA[81100055904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fr&#233;do]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Durand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P150906</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADAMS, A. 1995. The Camera+The Negative+The Print. Little Brown and Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>568224</ref_obj_id>
				<ref_obj_pid>568214</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARASH, D. 2001. A fundamental relationship between bilateral filtering, adaptive smoothing and the nonlinear diffusion equation. IEEE PAMI. in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARROW, H., AND TENENBAUM, J. 1978. Recovering intrinsic scene characteristics from images. In Computer Vision Systems. Academic Press, New York, 3-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319509</ref_obj_id>
				<ref_obj_pid>2318958</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLACK, M., SAPIRO, G., MARIMONT, D., AND HEEGER, D. 1998. Robust anisotropic diffusion. IEEE Trans. Image Processing 7, 3 (Mar.), 421-432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHIU, K., HERF, M., SHIRLEY, P., SWAMY, S., WANG, C., AND ZIMMERMAN, K. 1993. Spatially nonuniform scaling functions for high contrast images. In Proc. Graphics Interface, 245-253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732299</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., TCHOU, C., HAWKINS, T., AND DEBEVEC, P. 2001. Real-time high-dynamic range texture mapping. In Rendering Techniques 2001: 12th Eurographics Workshop on Rendering, Eurographics, 313-320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. E., AND MALIK, J. 1997. Recovering high dynamic range radiance maps from photographs. In Proceedings of SIGGRAPH 97, ACM SIGGRAPH / Addison Wesley, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Series, 369-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., MEYER, M., SCHR&#214;DER, P., AND BARR, A. H. 2000. Anisotropic feature-preserving denoising of height fields and bivariate data. In Graphics Interface, 145-152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DICARLO, J., AND WANDELL, B. 2000. Rendering high dynamic range images. Proceedings of the SPIE: Image Sensors 3965, 392-401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508550</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DURAND, F. 2002. An invitation to discuss computer depiction. In Proc. NPAR'02.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320322</ref_obj_id>
				<ref_obj_pid>2319013</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[ELAD, M. to appear. On the bilateral filter and ways to improve it. IEEE Trans. on Image Processing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FERWERDA, J. 1998. Fundamentals of spatial vision. In Applications of visual perception in computer graphics. Siggraph '98 Course Notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HAMPEL, F. R., RONCHETTI, E. M., ROUSSEEUW, P. J., AND STAHEL, W. A. 1986. Robust Statistics: The Approach Based on Influence Functions. Wiley, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HUBER, P. J. 1981. Robust Statistics. John Wiley and Sons, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319385</ref_obj_id>
				<ref_obj_pid>2318950</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[JOBSON, RAHMAN, AND WOODELL. 1997. A multi-scale retinex for bridging the gap between color images and the human observation of scenes. IEEE Trans. on Image Processing: Special Issue on Color Processing 6 (July), 965-976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LARSON, G. W., RUSHMEIER, H., AND PIATKO, C. 1997. A visibility matching tone reproduction operator for high dynamic range scenes. IEEE Transactions on Visualization and Computer Graphics 3, 4 (October - December), 291-306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MADDEN, B. 1993. Extended intensity range imaging. Tech. rep., U. of Pennsylvania, GRASP Laboratory.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>318015</ref_obj_id>
				<ref_obj_pid>318009</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MCCOOL, M. 1999. Anisotropic diffusion for monte carlo noise reduction. ACM Trans. on Graphics 18, 2, 171-194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MITSUNAGA, T., AND NAYAR, S. K. 2000. High dynamic range imaging: Spatially varying pixel exposures. In IEEE CVPR, 472-479.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383310</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[OH, B. M., CHEN, M., DORSEY, J., AND DURAND, F. 2001. Image-based modeling and photo editing. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 433-442.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PATTANAIK, S. N., FERWERDA, J. A., FAIRCHILD, M. D., AND GREENBERG, D. P. 1998. A multiscale model of adaptation and spatial vision for realistic image display. In Proceedings of SIGGRAPH 98, ACM SIGGRAPH / Addison Wesley, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 287-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78304</ref_obj_id>
				<ref_obj_pid>78302</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PERONA, P., AND MALIK, J. 1990. Scale-space and edge detection using anisotropic diffusion. IEEE PAMI 12, 7, 629-639.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[RUDMAN, T. 2001. The Photographer's Master Printing Course. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192189</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[RUSHMEIER, H. E., AND WARD, G. J. 1994. Energy preserving non-linear filters. In Proceedings of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 131-138.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SAINT-MARC, P., CHEN, J., AND MEDIONI, G., 1991. Adaptive smoothing: a general tool for early vision.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SCHECHNER, Y. Y., AND NAYAR, S. K. 2001. Generalized mosaicing. In Proc. IEEE CVPR, 17-24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SCHLICK, C. 1994. Quantization techniques for visualization of high dynamic range pictures. 5th Eurographics Workshop on Rendering, 7-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SOCOLINSKY, D. 2000. Dynamic range constraints in image fusion and visualization. In Proc. Signal and Image Processing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>939190</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[TOMASI, C., AND MANDUCHI, R. 1998. Bilateral filtering for gray and color images. In Proc. IEEE Int. Conf. on Computer Vision, 836-846.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND RUSHMEIER, H. 1993. Tone reproduction for realistic images. IEEE Comp. Graphics & Applications 13, 6, 42-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311544</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND TURK, G. 1999. Lcis: A boundary hierarchy for detail-preserving contrast reduction. In Proceedings of SIGGRAPH 99, ACM SIGGRAPH / Addison Wesley Longman, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Series, 83-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300783</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., HODGINS, J., AND GUENTER, B. 1999. Two methods for display of high contrast images. ACM Trans. on Graphics 18, 1, 56-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>931534</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J. 1999. Three methods of detail-preserving contrast reduction for displayed images. PhD thesis, College of Computing Georgia Inst. of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. 1994. The radiance lighting simulation and rendering system. In Proceedings of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 459-472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[YANG, D., GAMAL, A. E., FOWLER, B., AND TIAN, H. 1999. A 640x512 cmos image sensor with ultrawide dynamic range floating-point pixel-level adc. IEEE Journal of Solid State Circuits 34, 12 (Dec.), 1821-1834.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Bilateral Filtering for the Display of High-Dynamic-Range Images Fr´edo Durand and Julie Dorsey 
Laboratory for Computer Science, Massachusetts Institute of Technology Abstract We present a new technique 
for the display of high-dynamic-range images, which reduces the contrast while preserving detail. It 
is based on a two-scale decomposition of the image into a base layer, encoding large-scale variations, 
and a detail layer. Only the base layer has its contrast reduced, thereby preserving detail. The base 
layer is obtained using an edge-preserving .lter called the bilateral .lter. This is a non-linear .lter, 
where the weight of each pixel is computed using a Gaussian in the spatial domain multiplied by an in.uence 
function in the intensity domain that decreases the weight of pixels with large intensity differences. 
We express bilateral .lter­ing in the framework of robust statistics and show how it relates to anisotropic 
diffusion. We then accelerate bilateral .ltering by using a piecewise-linear approximation in the intensity 
domain and ap­propriate subsampling. This results in a speed-up of two orders of magnitude. The method 
is fast and requires no parameter setting. CR Categories: I.3.3 [Computer Graphics]: Picture/image generation 
Display algorithms; I.4.1 [Image Processing and Com­puter Vision]: Enhancement Digitization and image 
capture Keywords: image processing, tone mapping, contrast reduction, edge-preserving .ltering,weird 
maths 1 Introduction As the availability of high-dynamic-range images grows due to ad­vances in lighting 
simulation, e.g. [Ward 1994], multiple-exposure photography [Debevec and Malik 1997; Madden 1993] and 
new sensor technologies [Mitsunaga and Nayar 2000; Schechner and Nayar 2001; Yang et al. 1999], there 
is a growing demand to be able to display these images on low-dynamic-range media. Our vi­sual system 
can cope with such high-contrast scenes because most of the adaptation mechanisms are local on the retina. 
There is a tremendous need for contrast reduction in applica­tions such as image-processing, medical 
imaging, realistic render­ing, and digital photography. Consider photography for example. A major aspect 
of the art and craft concerns the management of contrast via e.g. exposure, lighting, printing, or local 
dodging and burning [Adams 1995; Rudman 2001]. In fact, poor management of light under-or over-exposed 
areas, light behind the main char­acter, etc. is the single most-commonly-cited reason for rejecting 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 Figure 1: High-dynamic-range photography. No single global ex­posure can preserve both the colors 
of the sky and the details of the landscape, as shown on the rightmost images. In contrast, our spatially-varying 
display operator (large image) can bring out all details of the scene. Total clock time for this 700x480 
image is 1.4 seconds on a 700Mhz PentiumIII. Radiance map courtesy of Paul Debevec, USC. [Debevec and 
Malik 1997]  Base Detail Color Figure 2: Principle of our two-scale decomposition of the input intensity. 
Color is treated separately using simple ratios. Only the base scale has its contrast reduced. photographs. 
This is why camera manufacturers have developed sophisticated exposure-metering systems. Unfortunately, 
exposure only operates via global contrast management that is, it recenters the intensity window on 
the most relevant range. If the range of in­tensity is too large, the photo will contain under-and over-exposed 
areas (Fig. 1, rightmost part). Our work is motivated by the idea that the use of high-dynamic­range 
cameras and relevant display operators can address these is­sues. Digital photography has inherited many 
of the strengths of .lm photography. However it also has the potential to overcome its limitations. Ideally, 
the photography process should be de­composed into a measurement phase (with a high-dynamic-range output), 
and a post-process phase that, among other things, man­ages the contrast. This post-process could be 
automatic or user­controlled, as part of the camera or on a computer, but it should take advantage of 
the wide range of available intensity to perform appropriate contrast reduction. In this paper, we introduce 
a fast and robust operator that takes a high-dynamic-range image as input, and compresses the contrast 
while preserving the details of the original image, as introduced by Tumblin [1999]. Our operator is 
based on a two-scale decomposi­tion of the image into a base layer (large-scale features) and a detail 
layer (Fig. 2). Only the base layer has its contrast reduced, thereby preserving the detail. In order 
to perform a fast decomposition into these two layers, and to avoid halo artifacts, we present a fast 
and robust edge-preserving .lter. 1.1 Overview The primary focus of this paper is the development of 
a fast and robust edge-preserving .lter that is, a .lter that blurs the small variations of a signal 
(noise or texture detail) but preserves the large discontinuities (edges). Our application is unusual 
however, in that the noise (detail) is the important information in the signal and must therefore be 
preserved. We build on bilateral .ltering, a non-linear .lter introduced by Tomasi et al. [1998]. It 
derives from Gaussian blur, but it prevents blurring across edges by decreasing the weight of pixels 
when the intensity difference is too large. As it is a fast alternative to the use of anisotropic diffusion, 
which has proven to be a valuable tool in a variety of areas of computer graphics, e.g. [McCool 1999; 
Desbrun et al. 2000], the potential applications of this technique extend beyond the scope of contrast 
reduction. This paper makes the following contributions: Bilateral .ltering and robust statistics: We 
recast bilateral .lter­ing in the framework of robust statistics, which is concerned with estimators 
that are insensitive to outliers. Bilateral .ltering is an estimator that considers values across edges 
to be outliers. This al­lows us to provide a wide theoretical context for bilateral .ltering, and to 
relate it to anisotropic diffusion. Fast bilateral .ltering: We present two acceleration techniques: 
we linearize bilateral .ltering, which allows us to use FFT and fast convolution, and we downsample the 
key operations. Uncertainty: We compute the uncertainty of the output of the .l­ter, which permits the 
correction of doubtful values. Contrast reduction: We use bilateral .ltering for the display of high-dynamic-range 
images. The method is fast, stable, and re­quires no setting of parameters.  2 Review of local tone 
mapping Tone mapping operators can be classi.ed into global and local techniques [Tumblin 1999; Ferwerda 
1998; DiCarlo and Wandell 2000]. Because they use the same mapping function for all pixels, most global 
techniques do not directly address contrast reduction. A limited solution is proposed by Schlick [1994] 
and Tumblin et al. [1999], who use S-shaped functions inspired from photography, thus preserving some 
details in the highlights and shadows. Unfor­tunately, contrast is severely reduced in these areas. Some 
authors propose to interactively vary the mapping according to the region of interest attended by the 
user [Tumblin et al. 1999], potentially using graphics hardware [Cohen et al. 2001]. A notable exception 
is the global histogram adjustment by Ward-Larson et al. [1997]. They disregard the empty portions of 
the histogram, which results in ef.cient contrast reduction. However, the limitations due to the global 
nature of the technique become obvious when the input exhibits a uniform histogram (see e.g. the example 
by DiCarlo and Wandell [2000]). In contrast, local operators use a mapping that varies spatially depending 
on the neighborhood of a pixel. This exploits the fact that human vision is sensitive mainly to local 
contrast. Most local tone-mapping techniques use a decomposition of the image into different layers or 
scales (with the exception of Socol­insky, who uses a variational technique [2000]). The contrast is 
reduced differently for each scale, and the .nal image is a recom­position of the various scales after 
contrast reduction. The major pitfall of local methods is the presence of haloing artifacts. When dealing 
with high-dynamic-range images, haloing issues become even more critical. In 8-bit images, the contrast 
at the edges is lim­ited to roughly two orders of magnitude, which directly limits the strength of halos. 
Chiu et al. vary a gain according to a low-pass version of the im­age [1993], which results in pronounced 
halos. Schlick had similar problems when he tried to vary his mapping spatially [1994]. Job­son et al. 
reduce halos by applying a similar technique at multiple scales [1997]. Pattanaik et al. use a multiscale 
decomposition of the image according to comprehensive psychophysically-derived .lter banks [1998]. To 
date, this method seems to be the most faithful to human vision, however, it may still present halos. 
DiCarlo et al. propose to use robust statistical estimators to im­prove current techniques [2000], although 
they do not provide a detailed description. Our method follows in the same spirit and fo­cuses on the 
development of a fast and practical method. Tumblin et al. [1999] propose an operator for synthetic images 
that takes advantage of the ability of the human visual system to decompose a scene into intrinsic layers 
, such as re.ectance and illumination [Barrow and Tenenbaum 1978]. Because vision is sen­sitive mainly 
to the re.ectance layers, they reduce contrast only in the illumination layer. This technique is unfortunately 
applicable only when the characteristics of the 3D scene are known. As we will see, our work can be seen 
as an extension to photographs. Our two-scale decomposition is very related to the texture-illuminance 
decoupling technique by Oh et al. [2001]. Recently, Tumblin and Turk built on anisotropic diffusion to 
decompose an image using a new low-curvature image simpli.er (LCIS) [Tumblin 1999; Tumblin and Turk 1999]. 
Their method can extract exquisite details from high-contrast images. Unfortunately, the solution of 
their partial differential equation is a slow iterative process. Moreover, the coef.cients of their diffusion 
equation must be adapted to each image, which makes this method more dif.­cult to use, and the extension 
to animated sequences unclear. We build upon a different edge-preserving .lter that is easier to con­trol 
and more amenable to acceleration. We will also deal with two problems mentioned by Tumblin et al.: the 
small remaining halos localized around the edges, and the need for a leakage .xer to completely stop 
diffusion at discontinuities.  3 Edge-preserving .ltering In this section, we review important edge-preserving-smoothing 
techniques, e.g. [Saint-Marc et al. 1991]. 3.1 Anisotropic diffusion Anisotropic diffusion [Perona and 
Malik 1990] is inspired by an interpretation of Gaussian blur as a heat conduction partial differ­ential 
equation (PDE): .I ...IThat is, the intensity I of each . .t pixel is seen as heat and is propagated 
over time to its 4 neighbors according to the heat spatial variation. Perona and Malik introduced an 
edge-stopping function g that varies the conductance according to the image gradient. This pre­vents 
heat .ow across edges: .I .div.g....I....I.. (1) .t They propose two expressions for the edge-stopping 
function g(x): 12.s2. e..x g1.x.. and g2.x..(2) . x2 1 . s2 where s is a scale parameter in the intensity 
domain that speci.es what gradient intensity should stop diffusion. The discrete Perona-Malik diffusion 
equation governing the value Is at pixel s is then Ist.1 .Ist . .. g.Ipt .Ist ..Ipt .Ist .. (3) 4 p.neighb4.s. 
where t describes discrete time steps, and neighb4.s.is the 4­neighborhood of pixel s. . is a scalar 
that determines the rate of diffusion. Although anisotropic diffusion is a popular tool for edge­preserving 
.ltering, its discrete diffusion nature makes it a slow process. Moreover, the results depend on the 
stopping time, since the diffusion converges to a uniform image.  3.2 Robust anisotropic diffusion Black 
et al. [1998] recast anisotropic diffusion in the framework of robust statistics. Our analysis of bilateral 
.ltering is inspired by their work. The .eld of robust statistics develops estimators that are robust 
to outliers or deviation to the theoretical distribution [Huber 1981; Hampel et al. 1986]. Black et al. 
[1998] show that anisotropic diffusion can be seen as the estimate of a value Is at each pixel s that 
is an estimate of its 4-neighbors, which minimizes an energy over the whole image: min .. ..Ip .Is.. 
(4) s.. p.neighb4.s. where . is the whole image, and . is an error norm (e.g. quadratic). Eq. 4 can be 
solved by gradient descent for each pixel: Ist.1 .Ist . .. ..Ip .Is.. (5) 4 p.neighb4 .s. where . is 
the derivative of ., and t is a discrete time variable. . is proportional to the so-called in.uence function 
that characterizes the in.uence of a sample on the estimate. For example, a least-square estimate is 
obtained by using ..x.. x2, and the corresponding in.uence function is linear, thus resulting in the 
mean estimator (Fig. 4, left). As a result, values far from the mean have a considerable in.uence on 
the estimate. In contrast, an in.uence function such as the Lorentzian error norm, given in Fig. 3 and 
plotted in Fig. 4, gives much less weight to outliers and is there­fore more robust. In the plot of ., 
we see that the in.uence function is redescending [Black et al. 1998; Huber 1981]1. Robust norms and 
in.uence functions depend on a parameter s that provides the notion of scale in the intensity domain, 
and controls where the in­.uence function becomes redescending, and thus which values are considered 
outliers. Black et al. note that Eq. 5 is similar to Eq. 3 govern­ing anisotropic diffusion, and that 
by de.ning g.x....x..x, anisotropic diffusion is reduced to a robust estimator. They also show that the 
g1 function proposed by Perona et al. is equivalent to the Lorentzian error norm plotted in Fig. 4 and 
given in Fig. 3. This analogy allows them to discuss desirable properties of edge­stopping functions. 
In particular, they show that Tukey s biweight function (Fig. 3) yields more robust results, because 
it completely stops diffusion across edges: The in.uence of outliers is null, as shown in Fig. 5, as 
opposed to the Lorentzian error norm that slowly goes to zero towards in.nity. This also solves the termination 
prob­lem, since diffusion then converges to a piecewise-uniform image. 1Some authors reserve the term 
redescending for function that vanish after a certain value [Hampel et al. 1986]. Huber Lorentz gs.x.. 
. 1 s .x..s 1 .x. . otherwise s gs.x.. 2 2.x2 s2 s. . 2 Tukey Gauss gs.x.. .1 2 .1 ..x.s. 2 . 2 .x..s 
0. otherwise s . . 5 gs.x..e . x2 2s2 s Figure 3: Robust edge-stopping functions. Note that . can be 
found by multiplying g by x, and . by integration of .. The value of s has to be modi.ed accordingly 
to use a consistent scale across estimators, as indicated below the Lorentz and Tukey functions.  4 
2 3 y1 y2 2 1 12 x 1 1 0 0 2 1 12 2 1 12 x 2 x  Least square ..x...x.Lorentz ..x...x. Figure 4: Least-square 
vs. Lorentzian error norm (after [Black et al. 1998]).   2 1 12 2 1 12 x x g.x. ..x. ..x. Figure 5: 
Tukey s biweight (after [Black et al. 1998]).  3.3 Bilateral .ltering Bilateral .ltering was developed 
by Tomasi and Manduchi as an alternative to anisotropic diffusion [1998]. It is a non-linear .lter where 
the output is a weighted average of the input. They start with standard Gaussian .ltering with a spatial 
kernel f (Fig. 6). However, the weight of a pixel depends also on a function g in the intensity domain, 
which decreases the weight of pixels with large intensity differences. We note that g is an edge-stopping 
function similar to that of Perona et al. [1990]. The output of the bilateral .lter for a pixel s is 
then: 1 Js .f .p .s.g.Ip .Is.Ip(6) . k.s. . p.. where k.s.is a normalization term: k.s... f .p .s.g.Ip 
.Is.. (7) p.. In practice, they use a Gaussian for f in the spatial domain, and a Gaussian for g in the 
intensity domain. Therefore, the value at a pixel s is in.uenced mainly by pixel that are close spatially 
and that have a similar intensity (Fig. 6). This is easy to extend to color images, and any metric g 
on pixels can be used (e.g. CIE-LAB). Barash proposes a link between anisotropic diffusion and bilat­eral 
.ltering [2001]. He uses an extended de.nition of intensity that includes spatial coordinates. This permits 
the extension of bilateral .ltering to perform feature enhancement. Unfortunately,  input spatial kernel 
f in.uence g in the intensity weight f .g output domain for the central pixel for the central pixel Figure 
6: Bilateral .ltering. Colors are used only to convey shape. the extended de.nition of intensity is not 
quite natural. Elad also discusses the relation between bilateral .ltering, anisotropic diffu­sion, and 
robust statistics, but he address the question from a linear­algebra point of view [to appear]. In this 
paper, we propose a dif­ferent uni.ed viewpoint based on robust statistics that extends the work by Black 
et al. [1998].  4 Edge-preserving smoothing as robust statistical estimation In their paper, Tomasi 
et al. only outlined the principle of bilat­eral .lters, and they then focused on the results obtained 
using two Gaussians. In this section, we provide a principled study of the properties of this family 
of .lters. In particular, we show that bilat­eral .ltering is a robust statistical estimator, which allows 
us to put empirical results into a wider theoretical context. 4.1 A uni.ed viewpoint on bilateral .ltering 
and 0­order anisotropic diffusion In order to establish a link to bilateral .ltering, we present a differ­ent 
interpretation of discrete anisotropic .ltering. In Eq. 3, Ipt .It is s used as the derivative of It 
in one direction. However, this can also be seen simply as the 0-order difference between the two pixel 
in­tensities. The edge-stopping function can thus be seen as preventing diffusion between pixels with 
large intensity differences. The two formulations are equivalent from a practical standpoint, but Black 
et al. s variational interpretation [1998] is more faithful to Perona and Malik s diffusion analogy, 
while our 0-order interpretation is more natural in terms of robust statistics. In particular, we can 
extend the 0-order anisotropic diffusion to a larger spatial support: Ist.1 .Ist ... f .p .s.g.Ipt .Ist 
..Ipt .Ist .. (8) p.. where f is a spatial weighting function (typically a Gaussian), . is the whole 
image,and t is still a discrete time variable. The anisotropic diffusion of Perona et al., which we now 
call local diffusion, corresponds to an f that is zero except at the 4 neigh­bors. Eq. 8 de.nes a robust 
statistical estimator of the class of M-estimators (generalized maximum likelihood estimator) [Ham­pel 
et al. 1986; Huber 1981]. In the case where the conductance g is uniform (isotropic .lter­ing) and where 
f is a Gaussian, Eq. 8 performs a Gaussian blur for each iteration, which is equivalent to several iterations 
of the heat­.ow simulation. It can thus be seen as a way to trade the number of iterations for a larger 
spatial support. However, in the case of anisotropic diffusion, it has the additional property of propagating 
heat across ridges. Indeed, if the image is white with a black line in the middle, local anisotropic 
diffusion does not propagate energy between the two connected components, while extended diffusion does. 
Depending on the application, this property will be either bene.cial or deleterious. In the case of tone 
mapping, for exam­ple, the notion of connectedness is not important, as only spatial neighborhoods matter. 
We now come to the robust statistical interpretation of bilateral .ltering. Eq. 6 de.nes an estimator 
based on a weighted average of the data. It is therefore a W -estimator [Hampel et al. 1986]. The iterative 
formulation is an instance of iteratively reweighted least squares. This taxonomy is extremely important 
because it was shown that M-estimators and W-estimators are essentially equiv­alent and solve the same 
energy minimization problem [Hampel et al. 1986], p. 116: min ....Is .Ip. (9) s.. p.. or for each pixel 
s: ...Is .Ip..0. (10) p.. where . is the derivative of .. As shown by Black et al. [1998] for anisotropic 
diffusion, and as is true also for bilateral .ltering, it suf.ces to de.ne ..x..g.x..x to .nd the original 
formulations. In fact the second edge-stopping function g2 in Eq. 2 de.ned by Perona et al. [1990] corresponds 
to the Gaussian in.uence function used for bilateral .ltering [Tomasi and Manduchi 1998]. As a con­sequence 
of this uni.ed viewpoint, all the studies on edge-stopping functions for anisotropic diffusion can be 
applied to bilateral .lter­ing. Eqs. 9 and 10 are not strictly equivalent because of local min­ima of 
the energy. Depending on the application, this can be de­sirable or undesirable. In the former case, 
the use of a very robust estimator, such as the median, to initialize an iterative process is recommended. 
In the case of tone mapping or texture-illuminance decoupling, however, we want to .nd the local minimum 
closest to the initial pixel value. It was noted by Tomasi et al. [1998] that bilateral .ltering usu­ally 
requires only one iteration. Hence it belongs to the class of one-step W-estimators,or w-estimators, 
which have been shown to be particularly ef.cient. The existence of local minima is however a very important 
issue, and the use of an initial median estimator is highly recommended. In contrast, Oh. et al. use 
a simple Gaussian blur [2001], which deserves further study. Now that we have shown that 0-order anisotropic 
diffusion and bilateral .ltering belong to the same family of estimators, we can compare them. They both 
respect causality: No maximum or mini­mum can be created, only removed. However, anisotropic diffusion 
is adiabatic (energy-preserving), while bilateral .ltering is not. To see this, consider the energy exchange 
between two pixels p and s. In the diffusion case, the energy . f .p .s.g.Ipt .Ist ..Ipt .Ist ..ow­ing 
from p to s is the opposite of the energy from s to p because the expression is symmetric (provided that 
g and f are symmet­ric). In contrast, in bilateral .ltering, the normalization factor 1.k is different 
for the two pixels, resulting in an asymmetric energy .ow. Energy preservation can be crucial for some 
applications, e.g. [Rushmeier and Ward 1994], but it is not for tone mapping or re­.ectance extraction. 
In contrast to anisotropic diffusion, bilateral .ltering does not rely on shock formation, so it is not 
prone to stairstepping artifacts. The output of bilateral .ltering on a gradient input is smooth. This 
point is mostly due to the non-iterative nature of the .lter and de­serves further exploration. 4.2 Robust 
estimators   x x g.x. ..x. ..x. Figure 7: Huber s minimax (after [Black et al. 1998]). Fig. 8 plots 
a variety of robust in.uence functions, and their For­mulas are given in Fig. 3. When the in.uence function 
is mono­tonic, there is no local minimum problem, and estimators always converge to a global maximum. 
Most robust estimators have a shape as shown on the left: The function increases, then decreases, and 
potentially goes to zero if it has a .nite rejection point. These plots can be very helpful in understanding 
how an esti­mator deals with outliers. For example, we can see that the Huber minimax gives constant 
in.uence to outliers, and that the Lorentz estimator gives them more importance than, say, the Gaussian 
esti­mator. The Tukey biweight is the only purely redescending function we show. Outliers are thus completely 
ignored.  Figure 8: Comparison of in.uence functions. We anticipate the results of our technique and 
show in Fig. 9 the output of a robust bilateral .lter using these different . functions (or their g equivalent 
in Eq. 6). We can see that larger in.uences of outliers result in estimates that are more blurred and 
further from the input pixels. In what follows, we use the Gaussian or Tukey in­.uence function, because 
they are more robust to outliers and better preserve edges.  5 Ef.cient Bilateral Filtering Now that 
we have provided a theoretical framework for bilateral .l­tering, we will next deal with its speed. A 
direct implementation of  Huber Lorentz Gaussian Tukey Figure 9: Comparison of the 4 estimators for 
the log of intensity of the foggy scene of Fig 15. The false-colored output is normalized to the log 
of the min and max of the input. bilateral .ltering might require O.n2 .time, where n is the number of 
pixels in the image. In this section, we dramatically accelerate bilateral .ltering using two strategies: 
a piecewise-linear approxi­mation in the intensity domain, and a sub-sampling in the spatial domain. 
We then present a technique that detects and .xes pixels where the bilateral .lter cannot obtain a good 
estimate due to lack of data. 5.1 Piecewise-linear bilateral .ltering A convolution such as Gaussian 
.ltering can be greatly accelerated using Fast Fourier Transform. A O.n2 .convolution in the primal becomes 
a O.n.multiplication in the frequency domain. Since the discrete FFT and its inverse have cost O.nlog 
n., there is a gain of one order of magnitude. Unfortunately, this strategy cannot be applied directly 
to bilat­eral .ltering, because it is not a convolution: The .lter is signal­dependent because of the 
edge-stopping function g.Ip .Is..How­ever consider Eq. 6 for a .xed pixel s. It is equivalent to the 
convolu­tion of the function HIs : p .g.Ip .Is.Ip by the kernel f . Similarly, the normalization factor 
k is the convolution of GIs : p .g.Ip .Is. by f . That is, the only dependency on pixel s is the value 
Is in g. Our acceleration strategy is thus as follows: We discretize the set of possible signal intensities 
into NB SEGMENTvalues .ij ., and compute a linear .lter for each such value: Jj 1 ij s .. f .p .s.g.Ip 
..Ip kj .s. p.. 1 Hj (11) .. f .p .s.p kj .s. p.. and kj.s... f .p .s.g.Ip .ij. p.. (12) .. f .p .s.Gj.p.. 
p.. The .nal output of the .lter for a pixel s is then a linear interpo­lation between the output Jsj 
of the two closest values ij of Is. This corresponds to a piecewise-linear approximation of the original 
bi­lateral .lter (note however that it is a linearization of the whole functional, not of the in.uence 
function). The pseudocode is given in Fig. 10. Fig. 11 shows the speed-up we obtain depending on the 
size of the spatial kernel. Quickly, the piecewise-linear version outper­forms the brute-force implementation, 
due to the use of FFT con­volution. The formal analysis of error remains to be performed, but no artifact 
was noticeable for segments up to the size of the scale sr. This could be further accelerated when the 
distribution of inten­sities is not uniform spatially. We can subdivide the image into sub-images, and 
if the difference between the max and min of the PiecewiseBilateral (Image I, spatial kernel fss , intensity 
in.uence gsr ) J=0 /* set the output to zero */ for j=0..NB SEGMENTS ij = minI+j .(max(I)-min(I))/NB 
SEGMENTS Gj =gsr (I -ij ) /* evaluate gsr at each pixel */ Kj=Gj .fss /* normalization factor */ Hj=Gj 
.I /* compute H for each pixel */ H.j =Hj .fss Jj=H.j/Kj /* normalize */ J=J+Jj .InterpolationWeight(I, 
ij ) Figure 10: Pseudo code of the piecewise-linear acceleration of bi­lateral .ltering. Operations 
with upper cases such as Gj=gsr (I, ij) denote computation on all pixels of the image. .denotes the con­volution, 
while .is simply the per-pixel multiplication. Interpola­tionWeight is the hat interpolation weight for 
linear interpolation. In practice, we use NB SEGMENT=(max(I)-min(I))/sr.  FastBilateral (Image I, spatial 
kernel fss , intensity in.uence gsr , downsampling factor z) J=0 /*set the full-scale output to zero 
*/ I =downsample ( I, z ) f . =downsample ( fss ,z ) ss .z for j=0..NB SEGMENTS ij = minI+j .(max(I)-min(I))/NB 
SEGMENTS G.j =gsr (I -ij ) /* evaluate gsr at each pixel */ K.j=G.j ./* normalization factor */ f . ss 
.z H.j=G.j H .I /* compute H for each pixel */ ..j=H.j .f . ss .z J.j =H..j/K.j /* normalize */ Jj=upsample(J.j 
,z) J=J+Jj .InterpolationWeight(I, ij ) Figure 12: Pseudo code of the downsampled piecewise-linear ac­celeration 
of bilateral .ltering. Parts at the full resolution are in green, while downsampled operations are in 
blue, and downsam­pled images are denoted with a prime. intensity is more reduced in the sub-images than 
in the whole im­age, fewer segments can be used. This solution has however not been implemented yet. 
 5.2 Subsampling To further accelerate bilateral .ltering, we note that all operations in Fig. 10 except 
the .nal interpolation aim at low-pass .ltering. We can thus safely use a downsampled version of the 
image with little quality loss. However, the .nal interpolation must be performed using the full-scale 
image, otherwise edges would not be respected, resulting in visible artifacts. Fig. 12 shows the new 
algorithm. We use nearest-neighbor downsampling, because it does not modify the histogram. The acceleration 
we obtain is plotted in Fig. 13 for an example. While a formal study of error/acceleration remains to 
be done, we did not notice any visible artifact up to downsampling factor of 10 to 25. At this resolution, 
the cost of the upsampling and linear interpolation outweighs the .ltering op­erations, and no further 
acceleration is gained by more aggressive downsampling. Figure 13: Speed-up due to downsampling for 
17 segments and a 576x768 image. The value for the full-scale .ltering is 173 sec. 5.3 Uncertainty As 
noted by Tumblin et al. [Tumblin 1999; Tumblin and Turk 1999], edge-preserving contrast reduction can 
still encounter small halo artifacts for antialiased edges or due to .are around high­contrast edges. 
We noticed similar problems on some synthetic as well as real images. We propose an explanation in terms 
of signal/noise ratio. These small halos correspond to pixels where there is not enough information in 
the neighborhood to decouple the large-scale and the small-scale features. Indeed, the values at the 
edges span the whole range between the upper and the lower values, and there are very few pixels in the 
zone of proper data of the in.uence function. We thus compute a statistical estimator with very little 
data, and the variance is quite high. Fortunately, bilateral .ltering provides a direct measure of this 
uncertainty: The normalization factor k inEq.6 isthesum of the in.uence of each pixel. We can therefore 
use it to detect dubious pixels that need to be .xed. In practice, we use the log of this value because 
it better extracts uncertain pixels. The .xing strategy we use is then simple. We compute a low­pass 
version J of the output J of the bilateral .lter, using a small Gaussian kernel (2 pixels in practice), 
and we assign to a pixel the value of a linear interpolation between J and J depending on the log of 
the uncertainty k.  6 Contrast reduction We now describe how bilateral .ltering can be used for contrast 
re­duction. We note that our method is not strictly a tone reproduction operator, in the sense of Tumblin 
and Rushmeier s [1993], since it does not attempt to imitate human vision. Building on previous approaches, 
our contrast reduction is based on a multiscale decomposition e.g. [Jobson et al. 1997; Pattanaik et 
al. 1998; Tumblin and Turk 1999]. However, we only use a two­scale decomposition, where the base image 
is computed using bilateral .ltering, and the detail layer is the division of the input intensity by 
the base layer. Fig. 2 illustrates the general approach. The base layer has its contrast reduced, while 
the magnitude of the detail layer is unchanged, thus preserving detail. Following Tumblin et al. [Tumblin 
1999; Tumblin and Turk 1999], we compress the range of the base layer using a scale factor in the log 
domain. We compute this scale factor such that the whole range of the base layer is compressed to a user-controllable 
base contrast. In practice, a base contrast of 5 worked well for all our examples, but in some situations 
where lights sources are visible, one might want to vary this setting. Our treatment of color is simple. 
We perform contrast reduction on the intensity of pixels and recompose color after contrast reduc­tion 
[Schlick 1994; Tumblin 1999; Tumblin and Turk 1999]. We perform our calculations on the logs of pixel 
intensities, because pixel differences then correspond directly to contrast, and because it yields a 
more uniform treatment of the whole range. Our approach is faithful to the original idea by Chiu et al. 
[1993], albeit using a robust .lter instead of their low-pass .lter. It can also be viewed as the decomposition 
of the image into intrinsic layers of re.ectance and illuminance [Oh et al. 2001], followed by an appro­priate 
contrast reduction of the illuminance (or base) layer [Tumblin et al. 1999]. For the .ltering phase, 
we experimented with the various in­.uence functions discussed in Section 4.2. As expected, the Hu­ber 
minimax estimator decreases the strength of halos compared to standard Gaussian blur, but does not eliminate 
them. Moreover, the results vary with the size of the spatial kernel. The Lorentz function performed 
better, but only the Gaussian and Tukey s biweight were able to accurately decompose the image. With 
both functions, the scale ss of the spatial kernel had little in.uence on the result. This is important 
since it allows us to keep ss constant to a value of 2% of the image size. The value sr .0.4 performed 
consistently well for all our ex­periments. Again, this property is quite important because the user 
does not have to set a complex parameter. The signi.cance of this value might come from two complementary 
origins, which are still areas of future research. First, it might be due to characteristics of the local 
sensitivity of the human visual system. Perhaps beyond this value, we notice no difference. Second, it 
might be related to the physical range of possible re.ectance values, between a perfect re.ector and 
a black material. As a conclusion, the only user-controlled parameters of our method are the overall 
brightness and the base contrast. While the automatic values perform very well, we found it useful to 
provide these intuitive degrees of freedom to allow the user a control over the look of the image. The 
base contrast provides a very intuitive alternative to the contrast/brightness setting of image-editing 
soft­ware. It controls the overall appearance of the image, while still preserving the .ne details. 6.1 
Implementation and results We have implemented our technique using a .oating point repre­sentation of 
images, and the Intel image processing library for the convolutions. We have tested it on a variety of 
synthetic and real images, as shown in the color plates. All the examples reproduced in the paper use 
the Gaussian in.uence function, but the results with Tukey s biweight are not different. The technique 
is extremely fast, as can be seen in Fig. 14. We have tested it on an upsam­pled 10Mpixel image with 
contrast of more than 1:100,000, and the computation took only 6s on a 2GHz Pentium 4. In particular, 
due to our acceleration techniques, the running time grows sub-linearly. This is a dramatic speed-up 
compared to previous methods. Our technique can address some of the most challenging pho­tographic situations, 
such as interior lighting or sunset photos, and produces very compelling images. In our experiments, 
Tumblin and Turk s operator [1999] appears to better preserve .ne details, while our technique better 
preserves the overall photorealistic appearance (Figs. 21 and 22). Image resolution # segments z timing 
(s) Grove D 710 * 480 15 4 0.33 Memorial 512 * 768 11 4 0.31 Hotel room 750 * 487 13 4 0.31 Vine 710 
* 480 10 4 0.23 Fog 1130 * 751 12 8 0.45 Grove C 709 * 480 14 4 0.30 Window 2K*1.3K 10 16 2.73 Interior 
2K*1.3K 19 16 2.19 Interior*2 2.6K * 4K 19 24 6.03 Figure 14: Results of our new technique. Timings on 
a 2GHz P4.   7 Discussion This paper opens several avenues of future research related to edge­preserving 
.ltering and contrast reduction. The uni.ed viewpoint on bilateral .ltering and anisotropic diffusion 
offers some interest­ing possibilities. The robust statistical framework we have intro­duced suggests 
the application of bilateral .ltering to a variety of graphics areas where energy preservation is not 
a major concern. The treatment of uncertainty deserves more attention. The cor­rection scheme based on 
a Gaussian blur by a small kernel works well in the cases we have tested, but a more formal analysis 
is needed. Other approaches might involve the use of a different range scale sr. In terms of contrast 
reduction, future work includes the develop­ment of a more principled .xing method for uncertain values, 
and the use of a more elaborate compression function for the base layer, e.g. [Tumblin et al. 1999; Larson 
et al. 1997]. White balance is an important issue for indoor scenes that also exhibit outdoor portions, 
as can be seen in Fig. 23. A strategy similar to Pattanaik et al. s op­erator [Pattanaik et al. 1998] 
should be developed. The inclusion of perceptual aspects is a logical step. The main dif.culty stems 
from the complex interaction between local adaptation and gaze move­ments. The extension to animated 
sequences is an exciting topic. Initial experiments are very encouraging. Finally, contrast reduction 
is only one example of pictorial tech­niques to cope with the limitations of the medium [Durand 2002]. 
We believe that these techniques are crucial aspects of the digital photography and video revolution, 
and will facilitate the creation of effective and compelling pictures.  Acknowledgments We would like 
to thank Byong Mok Oh for his help with the radi­ance maps and the bibliography; he and Ray Jones also 
provided crucial proofreading. Thanks to Paul Debevec and Jack Tumblin for allowing us to use their radiance 
maps. Thanks to the reviewers for their careful comments. This research was supported by NSF grants CCR-0072690 
and EIA-9802220, and by a gift from Pixar Animation Studios.  References ADAMS, A. 1995. The Camera+The 
Negative+The Print. Little Brown and Co. BARASH, D. 2001. A fundamental relationship between bilateral 
.ltering, adaptive smoothing and the nonlinear diffusion equation. IEEE PAMI. in press. BARROW, H., AND 
TENENBAUM, J. 1978. Recovering intrinsic scene characteristics from images. In Computer Vision Systems. 
Academic Press, New York, 3 26. BLACK, M., SAPIRO, G., MARIMONT, D., AND HEEGER, D. 1998. Robust anisotropic 
diffusion. IEEE Trans. Image Processing 7, 3 (Mar.), 421 432. CHIU, K., HERF, M., SHIRLEY,P., SWAMY, 
S., WANG, C., AND ZIMMERMAN, K. 1993. Spatially nonuniform scaling functions for high contrast images. 
In Proc. Graphics Interface, 245 253. COHEN, J., TCHOU, C., HAWKINS,T., AND DEBEVEC, P. 2001. Real-time 
high­dynamic range texture mapping. In Rendering Techniques 2001: 12th Eurographics Workshop on Rendering, 
Eurographics, 313 320. DEBEVEC, P. E., AND MALIK, J. 1997. Recovering high dynamic range radiance maps 
from photographs. In Proceedings of SIGGRAPH 97, ACM SIGGRAPH / Addison Wesley, Los Angeles, California, 
Computer Graphics Proceedings, An­nual Conference Series, 369 378. DESBRUN, M., MEYER, M., SCHR ¨ ODER,P., 
AND BARR, A. H. 2000. Anisotropic feature-preserving denoising of height .elds and bivariate data. In 
Graphics Inter­face, 145 152. DICARLO, J., AND WANDELL, B. 2000. Rendering high dynamic range images. 
Pro­ceedings of the SPIE: Image Sensors 3965, 392 401. DURAND, F. 2002. An invitation to discuss computer 
depiction. In Proc. NPAR 02. ELAD, M. to appear. On the bilateral .lter and ways to improve it. IEEE 
Trans. on Image Processing. FERWERDA, J. 1998. Fundamentals of spatial vision. In Applications of visual 
percep­tion in computer graphics. Siggraph 98 Course Notes. HAMPEL, F. R., RONCHETTI, E. M., ROUSSEEUW, 
P. J., AND STAHEL, W. A. 1986. Robust Statistics: The Approach Based on In.uence Functions. Wiley, New 
York. HUBER, P. J. 1981. Robust Statistics. John Wiley and Sons, New York. JOBSON,RAHMAN, AND WOODELL. 
1997. A multi-scale retinex for bridging the gap between color images and the human observation of scenes. 
IEEE Trans. on Image Processing: Special Issue on Color Processing 6 (July), 965 976. LARSON,G. W., RUSHMEIER, 
H., AND PIATKO, C. 1997. A visibility matching tone reproduction operator for high dynamic range scenes. 
IEEE Transactions on Visualization and Computer Graphics 3, 4 (October -December), 291 306. MADDEN, B. 
1993. Extended intensity range imaging. Tech. rep., U. of Pennsylvania, GRASP Laboratory. MCCOOL, M. 
1999. Anisotropic diffusion for monte carlo noise reduction. ACM Trans. on Graphics 18, 2, 171 194. MITSUNAGA,T., 
AND NAYAR, S. K. 2000. High dynamic range imaging: Spatially varying pixel exposures. In IEEE CVPR, 472 
479. OH, B. M., CHEN, M., DORSEY, J., AND DURAND, F. 2001. Image-based modeling and photo editing. In 
Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference 
Series, 433 442. PATTANAIK, S. N., FERWERDA, J. A., FAIRCHILD, M. D., AND GREENBERG,D. P. 1998. A multiscale 
model of adaptation and spatial vision for realistic image dis­ play. In Proceedings of SIGGRAPH 98, 
ACM SIGGRAPH / Addison Wesley, Or­lando, Florida, Computer Graphics Proceedings, Annual Conference Series, 
287 298. PERONA,P., AND MALIK, J. 1990. Scale-space and edge detection using anisotropic diffusion. IEEE 
PAMI 12, 7, 629 639. RUDMAN, T. 2001. The Photographer s Master Printing Course. Focal Press. RUSHMEIER, 
H. E., AND WARD, G. J. 1994. Energy preserving non-linear .lters. In Proceedings of SIGGRAPH 94, ACM 
SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 131 
138. SAINT-MARC,P., CHEN, J., AND MEDIONI, G., 1991. Adaptive smoothing: a gen­eral tool for early vision. 
SCHECHNER,Y. Y., AND NAYAR, S. K. 2001. Generalized mosaicing. In Proc. IEEE CVPR, 17 24. SCHLICK, C. 
1994. Quantization techniques for visualization of high dynamic range pictures. 5th Eurographics Workshop 
on Rendering, 7 20. SOCOLINSKY, D. 2000. Dynamic range constraints in image fusion and visualization. 
In Proc. Signal and Image Processing. TOMASI, C., AND MANDUCHI, R. 1998. Bilateral .ltering for gray 
and color images. In Proc. IEEE Int. Conf. on Computer Vision, 836 846. TUMBLIN, J., AND RUSHMEIER, H. 
1993. Tone reproduction for realistic images. IEEE Comp. Graphics &#38; Applications 13, 6, 42 48. TUMBLIN, 
J., AND TURK, G. 1999. Lcis: A boundary hierarchy for detail-preserving contrast reduction. In Proceedings 
of SIGGRAPH 99, ACM SIGGRAPH / Addi­son Wesley Longman, Los Angeles, California, Computer Graphics Proceedings, 
Annual Conference Series, 83 90. TUMBLIN, J., HODGINS, J., AND GUENTER, B. 1999. Two methods for display 
of high contrast images. ACM Trans. on Graphics 18, 1, 56 94. TUMBLIN, J. 1999. Three methods of detail-preserving 
contrast reduction for dis­played images. PhD thesis, College of Computing Georgia Inst. of Technology. 
WARD, G. J. 1994. The radiance lighting simulation and rendering system. In Pro­ceedings of SIGGRAPH 
94, ACM SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 
459 472. YANG, D., GAMAL, A. E., FOWLER, B., AND TIAN, H. 1999. A 640x512 cmos image sensor with ultrawide 
dynamic range .oating-point pixel-level adc. IEEE Journal of Solid State Circuits 34, 12 (Dec.), 1821 
1834.   without with uncertainty .x uncertainty Figure 19: Zoom of Fig. 17. The haloing artifacts 
in the vertical highlight and in the lamp are dramatically reduced. The noise is due to the sensor. 
  User-optimized gamma correction only on the intensity Histogram adjustment tering. The rightmost 
frame is the color-coded base layer. Radiance [Larson et al. 1997] map courtesy of Paul Debevec, USC 
[Debevec and Malik 1997].  LCIS. Image reprinted by permission, Figure 23: Window scene. The rightmost 
image shows the color­copyright ccoded base layer. .1999 Jack Tumblin [Tumblin and Turk 1999]  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566575</article_id>
		<sort_key>267</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Photographic tone reproduction for digital images]]></title>
		<page_from>267</page_from>
		<page_to>276</page_to>
		<doi_number>10.1145/566570.566575</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566575</url>
		<abstract>
			<par><![CDATA[A classic photographic task is the mapping of the potentially high dynamic range of real world luminances to the low dynamic range of the photographic print. This tone reproduction problem is also faced by computer graphics practitioners who map digital images to a low dynamic range print or screen. The work presented in this paper leverages the time-tested techniques of photographic practice to develop a new tone reproduction operator. In particular, we use and extend the techniques developed by Ansel Adams to deal with digital images. The resulting algorithm is simple and produces good results for a wide variety of images.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dynamic range]]></kw>
			<kw><![CDATA[tone reproduction]]></kw>
			<kw><![CDATA[zone system]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.10</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14119574</person_id>
				<author_profile_id><![CDATA[81100331006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14085738</person_id>
				<author_profile_id><![CDATA[81100218834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48024661</person_id>
				<author_profile_id><![CDATA[81100449948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382439</person_id>
				<author_profile_id><![CDATA[81100459651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferwerda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADAMS, A. 1980. The camera. The Ansel Adams Photography series. Little, Brown and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ADAMS, A. 1981. The negative. The Ansel Adams Photography series. Little, Brown and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ADAMS, A. 1983. The print. The Ansel Adams Photography series. Little, Brown and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLOMMAERT, F. J. J., AND MARTENS, J.-B. 1990. An object-oriented model for brightness perception. Spatial Vision 5, 1, 15-41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BURT, P. J., AND ADELSON, E. H. 1983. A multiresolution spline with application to image mosaics. ACM Transactions on Graphics 2, 4, 217-236.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHIU, K., HERF, M., SHIRLEY, P., SWAMY, S., WANG, C., AND ZIMMERMAN, K. 1993. Spatially nonuniform scaling functions for high contrast images. In Proceedings of Graphics Interface '93, 245-253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732299</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., TCHOU, C., HAWKINS, T., AND DEBEVEC, P. 2001. Real-Time high dynamic range texture mapping. In Rendering techniques 2001, S. J. Gortler and K. Myszkowski, Eds., 313-320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. E., AND MALIK, J. 1997. Recovering high dynamic range radiance maps from photographs. In SIGGRAPH 97 Conference Proceedings, Addison Wesley, T. Whitted, Ed., Annual Conference Series, ACM SIGGRAPH, 369-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732137</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DURAND, F., AND DORSEY, J. 2000. Interactive tone mapping. In Eurographics Workshop on Rendering, 219-230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FAIRCHILD, M. D. 1998. Color appearance models. Addison-Wesley, Reading, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237262</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FERWERDA, J. A., PATTANAIK, S., SHIRLEY, P., AND GREENBERG, D. P. 1996. A model of visual adaptation for realistic image synthesis. In SIGGRAPH 96 Conference Proceedings, Addison Wesley, H. Rushmeier, Ed., Annual Conference Series, ACM SIGGRAPH, 249-258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258813</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GEIGEL, J., AND MUSGRAVE, F. K. 1997. A model for simulating the photographic development process on digital images. In SIGGRAPH 97 Conference Proceedings, Addison Wesley, T. Whitted, Ed., Annual Conference Series, ACM SIGGRAPH, 135-142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GOVE, A., GROSSBERG, S., AND MINGOLLA, E. 1995. Brightness perception, illusory contours, and corticogeniculate feedback. Visual Neuroscience 12, 1027-1052.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GRAVES, C. 1997. The zone system for 35mm photographers, second ed. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1046881</ref_obj_id>
				<ref_obj_pid>1046876</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HANSEN, T., BARATOFF, G., AND NEUMANN, H. 2000. A simple cell model with dominating opponent inhibition for robust contrast detection. Kognitionswissenschaft 9, 93-100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HOLM, J. 1996. Photographics tone and colour reproduction goals. In CIE Expert Symposium '96 on Colour Standards for Image Technology, 51-56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[JERNIGAN, M. E., AND MCLEAN, G. F. 1992. Lateral inhibition and image processing. In Non-linear vision: determination of neural receptive fields, function, and networks, R. B. Pinter and B. Nabet, Eds. CRC Press, ch. 17, 451-462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[JOHNSON, C. 1999. The practical zone system. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LAND, E. H., AND MCCANN, J. J. 1971. Lightness and retinex theory. J. Opt. Soc. Am. 63, 1, 1-11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LONDON, B., AND UPTON, J. 1998. Photography, sixth ed. Longman.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MARR, D., AND HILDRETH, E. C. 1980. Theory of edge detection. Proceedings of the Royal Society of London, B 207, 187-217.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095712</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MARR, D. 1982. Vision, a computational investigation into the human representation and processing of visual information. W H Freeman and Company, San Fransisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MATKOVIC, K., NEUMANN, L., AND PURGATHOFER, W. 1997. A survey of tone mapping techniques. In 13th Spring Conference on Computer Graphics, W. Stra&#223;er, Ed., 163-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MCNAMARA, A., CHALMERS, A., AND TROSCIANKO, T. 2000. STAR: Visual perception in realistic image synthesis. In Eurographics 2000 STAR reports, Eurographics, Interlaken, Switzerland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[MCNAMARA, A. 2001. Visual perception in realistic image synthesis. Computer Graphics Forum 20, 4 (December), 211-224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[MILLER, N. J., NGAI, P. Y., AND MILLER, D. D. 1984. The application of computer graphics in lighting design. Journal of the IES 14 (October), 6-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[MITCHELL, E. N. 1984. Photographic Science. John Wiley and Sons, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[OPPENHEIM, A. V., SCHAFER, R., AND STOCKHAM, T. 1968. Nonlinear filtering of multiplied and convolved signals. Proceedings of the IEEE 56, 8, 1264-1291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PARDO, A., AND SAPIRO, G. 2001. Visualization of high dynamic range images. Tech. Rep. 1753, Institute for Mathematics and its Applications, University of Minnesota.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[PATTANAIK, S. N., FERWERDA, J. A., FAIRCHILD, M. D., AND GREENBERG, D. P. 1998. A multiscale model of adaptation and spatial vision for realistic image display. In SIGGRAPH 98 Conference Proceedings, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, 287-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344810</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[PATTANAIK, S. N., TUMBLIN, J., YEE, H., , AND GREENBERG, D. P. 2000. Time-dependent visual adaptation for fast realistic display. In SIGGRAPH 2000 Conference Proceedings, Addison Wesley, K. Akeley, Ed., Annual Conference Series, ACM SIGGRAPH, 47-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[PELI, E. 1990. Contrast in complex images. J. Opt. Soc. Am. A 7, 10 (October), 2032-2040.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[PESSOA, L., MINGOLLA, E., AND NEUMANN, H. 1995. A contrast- and luminance-driven multiscale network model of brightness perception. Vision Research 35, 15, 2201-2223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[RAHMAN, Z., JOBSON, D. J., AND WOODELL, G. A. 1996. A multiscale retinex for color rendition and dynamic range compression. In SPIE Proceedings: Applications of Digital Image Processing XIX, vol. 2847.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[RAHMAN, Z., WOODELL, G. A., AND JOBSON, D. J. 1997. A comparison of the multiscale retinex with other image enhancement techniques. In IS&T's 50th Annual Conference: A Celebration of All Imaging, vol. 50, 426-431.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618848</ref_obj_id>
				<ref_obj_pid>616072</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[REINHARD, E., ASHIKHMIN, M., GOOCH, B., AND SHIRLEY, P. 2001. Color transfer between images. IEEE Computer Graphics and Applications 21 (September/October), 34-41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[SCHEEL, A., STAMMINGER, M., AND SEIDEL, H.-P. 2000. Tone reproduction for interactive walkthroughs. Computer Graphics Forum 19, 3 (August), 301-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[SCHLICK, C. 1994. Quantization techniques for the visualization of high dynamic range pictures. In Photorealistic Rendering Techniques, Springer-Verlag Berlin Heidelberg New York, P. Shirley, G. Sakas, and S. M&#252;ller, Eds., 7-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[STOCKHAM, T. 1972. Image processing in the context of a visual model. Proceedings of the IEEE 60, 7, 828-842.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[STROEBEL, L., COMPTON, J., CURRENT, I., AND ZAKIA, R. 2000. Basic photographic materials and processes, second ed. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND RUSHMEIER, H. 1991. Tone reproduction for realistic computer generated images. Tech. Rep. GIT-GVU-91-13, Graphics, Visualization, and Useability Center, Georgia Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND RUSHMEIER, H. 1993. Tone reproduction for computer generated images. IEEE Computer Graphics and Applications 13, 6 (November), 42-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311544</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND TURK, G. 1999. LCIS: A boundary hierarchy for detail-preserving contrast reduction. In Siggraph 1999, Computer Graphics Proceedings, Addison Wesley Longman, Los Angeles, A. Rockwood, Ed., Annual Conference Series, 83-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300783</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., HODGINS, J. K., AND GUENTER, B. K. 1999. Two methods for display of high contrast images. ACM Transactions on Graphics 18 (1), 56-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>912623</ref_obj_id>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. 1985. The Realistic Presentation of Synthetic Images: Image Processing in Computer Graphics. PhD thesis, University of California at Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[WARD, G., RUSHMEIER, H., AND PIATKO, C. 1997. A visibility matching tone reproduction operator for high dynamic range scenes. IEEE Transactions on Visualization and Computer Graphics 3, 4 (December).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[WARD LARSON, G., AND SHAKESPEARE, R. A. 1998. Rendering with Radiance. Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>180934</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[WARD, G. 1994. A contrast-based scalefactor for luminance display. In Graphics Gems IV, P. Heckbert, Ed. Academic Press, Boston, 415-421.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[WHITE, M., ZAKIA, R., AND LORENZ, P. 1984. The new zone system manual. Morgan & Morgan, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[WOODS, J. C. 1993. The zone system craftbook. McGraw Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Photographic Tone Reproduction for Digital Images Erik Reinhard Michael Stark Peter Shirley James Ferwerda 
University of Utah University of Utah University of Utah Cornell University Abstract A classic photographic 
task is the mapping of the potentially high dynamic range of real world luminances to the low dynamic 
range of the photographic print. This tone reproduction problem is also faced by computer graphics practitioners 
who map digital images to a low dynamic range print or screen. The work presented in this pa­per leverages 
the time-tested techniques of photographic practice to develop a new tone reproduction operator. In particular, 
we use and extend the techniques developed by Ansel Adams to deal with dig­ital images. The resulting 
algorithm is simple and produces good results for a wide variety of images. CR Categories: I.4.10 [Computing 
Methodologies]: Image Pro­cessing and Computer Vision Image Representation Keywords: Tone reproduction, 
dynamic range, Zone System. 1 Introduction The range of light we experience in the real world is vast, 
spanning approximately ten orders of absolute range from star-lit scenes to sun-lit snow, and over four 
orders of dynamic range from shad­ows to highlights in a single scene. However, the range of light we 
can reproduce on our print and screen display devices spans at best about two orders of absolute dynamic 
range. This discrep­ancy leads to the tone reproduction problem: how should we map measured/simulated 
scene luminances to display luminances and produce a satisfactory image? A great deal of work has been 
done on the tone reproduction problem [Matkovic et al. 1997; McNamara et al. 2000; McNamara 2001]. Most 
of this work has used an explicit perceptual model to control the operator [Upstill 1985; Tumblin and 
Rushmeier 1993; Ward 1994; Ferwerda et al. 1996; Ward et al. 1997; Tumblin et al. 1999]. Such methods 
have been extended to dynamic and interac­tive settings [Ferwerda et al. 1996; Durand and Dorsey 2000; 
Pat­tanaik et al. 2000; Scheel et al. 2000; Cohen et al. 2001]. Other work has focused on the dynamic 
range compression problem by spatially varying the mapping from scene luminances to display lu­minances 
while preserving local contrast [Oppenheim et al. 1968; Stockham 1972; Chiu et al. 1993; Schlick 1994; 
Tumblin and Turk 1999]. Finally, computational models of the human visual system can also guide such 
spatially-varying maps [Rahman et al. 1996; Rahman et al. 1997; Pattanaik et al. 1998]. Copyright &#38;#169; 
2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use is granted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 Radiance map courtesy of Cornell Program of Computer Graphics Figure 1: A high dynamic range image 
cannot be displayed directly without losing visible detail using linear scaling (top). Our new algorithm 
(bottom) is designed to overcome these problems. Using perceptual models is a sound approach to the tone 
repro­duction problem, and could lead to effective hands-off algorithms, but there are two problems with 
current models. First, current mod­els often introduce artifacts such as ringing or visible clamping 
(see Section 4). Second, visual appearance depends on more than simply matching contrast and/or brightness; 
scene content, image medium, and viewing conditions must often be considered [Fairchild 1998]. To avoid 
these problems, we turn to photographic practices for in­spiration. This has led us to develop a tone 
reproduction technique designed for a wide variety of images, including those having a very high dynamic 
range (e.g., Figure 1). 2 Background The tone reproduction problem was .rst de.ned by photographers. 
Often their goal is to produce realistic renderings of captured scenes, and they have to produce such 
renderings while facing the  Figure 2: A photographer uses the Zone System to anticipate po­tential 
print problems. Figure 3: A normal-key map for a high-key scene (for example con­taining snow) results 
in an unsatisfactory image (left). A high-key map solves the problem (right). limitations presented by 
slides or prints on photographic papers. Many common practices were developed over the 150 years of pho­tographic 
practice [London and Upton 1998]. At the same time there were a host of quantitative measurements of 
media response characteristics by developers [Stroebel et al. 2000]. However, there was usually a disconnect 
between the artistic and technical aspects of photographic practice, so it was very dif.cult to produce 
satis­factory images without a great deal of experience. Ansel Adams attempted to bridge this gap with 
an approach he called the Zone System [Adams 1980; Adams 1981; Adams 1983] which was .rst developed in 
the 1940s and later popularized by Minor White [White et al. 1984]. It is a system of practical sensit­ometry 
, where the photographer uses measured information in the .eld to improve the chances of producing a 
good .nal print. The Zone System is still widely used more than .fty years after its in­ception [Woods 
1993; Graves 1997; Johnson 1999]. Therefore, we believe it is useful as a basis for addressing the tone 
reproduction problem. Before discussing how the Zone System is applied, we .rst summarize some relevant 
terminology. Zone: A zone is de.ned as a Roman numeral associated with an approximate luminance range 
in a scene as well as an approxi­mate re.ectance of a print. There are eleven print zones, rang­ing from 
pure black (zone 0) to pure white (zone X), each doubling in intensity, and a potentially much larger 
number of scene zones (Figure 4). Darkest Brightest textured textured shadow Dynamic range = 15 scene 
zones highlight  Print zones Figure 4: The mapping from scene zones to print zones. Scene zones at 
either extreme will map to pure black (zone 0) or white (zone X) if the dynamic range of the scene is 
eleven zones or more. Middle-grey: This is the subjective middle brightness region of the scene, which 
is typically mapped to print zone V. Dynamic range: In computer graphics the dynamic range of a scene 
is expressed as the ratio of the highest scene luminance to the lowest scene luminance. Photographers 
are more inter­ested in the ratio of the highest and lowest luminance regions where detail is visible. 
This can be viewed as a subjective measure of dynamic range. Because zones relate logarithmi­cally to 
scene luminances, dynamic range can be expressed as the difference between highest and lowest distinguishable 
scene zones (Figure 4). Key: The key of a scene indicates whether it is subjectively light, normal, or 
dark. A white-painted room would be high-key, and a dim stable would be low-key. Dodging-and-burning: 
This is a printing technique where some light is withheld from a portion of the print during develop­ment 
(dodging), or more light is added to that region (burn­ing). This will lighten or darken that region 
in the .nal print relative to what it would be if the same development were used for all portions of 
the print. In traditional photography this technique is applied using a small wand or a piece of pa­per 
with a hole cut out. A crucial part of the Zone System is its methodology for predicting how scene luminances 
will map to a set of print zones. The pho­tographer .rst takes a luminance reading of a surface he perceives 
as a middle-grey (Figure 2 top). In a typical situation this will be mapped to zone V, which corresponds 
to the 18% re.ectance of the print. For high-key scenes the middle-grey will be one of the darker regions, 
whereas in low-key scenes this will be one of the lighter re­gions. This choice is an artistic one, although 
an 18% grey-card is often used to make this selection process more mechanical (Fig­ure 3). Next the photographer 
takes luminance readings of both light and dark regions to determine the dynamic range of the scene (Fig­ure 
2 bottom). If the dynamic range of the scene does not exceed nine zones, an appropriate choice of middle 
grey can ensure that all textured detail is captured in the .nal print. For a dynamic range of more than 
nine zones, some areas will be mapped to pure black or white with a standard development process. Sometimes 
such loss of detail is desirable, such as a very bright object being mapped to pure white (see [Adams 
1983], p. 51). For regions where loss of detail is objectionable, the photographer can resort to dodging-and­burning 
which will locally change the development process. The above procedure indicates that the photographic 
process is dif.cult to automate. For example, determining that an adobe build­ing is high-key would be 
very dif.cult without some knowledge about the adobe s true re.ectance. Only knowledge of the geometry 
and light inter-re.ections would allow one to know the difference between luminance ratios of a dark-dyed 
adobe house and a normal adobe house. However, the Zone System provides the photogra­pher with a small 
set of subjective controls. These controls form the basis for our tone reproduction algorithm described 
in the next section. The challenges faced in tone reproduction for rendered or cap­tured digital images 
are largely the same as those faced in conven­tional photography. The main difference is that digital 
images are in a sense perfect negatives, so no luminance information has been lost due to the limitations 
of the .lm process. This is a blessing in that detail is available in all luminance regions. On the other 
hand, this calls for a more extreme dynamic range reduction, which could in principle be handled by an 
extension of the dodging-and-burning process. We address this issue in the next section. 3 Algorithm 
The Zone System summarized in the last section is used to develop a new tone mapping algorithm for digital 
images, such as those cre­ated by rendering algorithms (e.g., [Ward Larson and Shakespeare 1998]) or 
captured using high dynamic range photography [De­bevec and Malik 1997]. We are not trying to closely 
mimic the actual photographic process [Geigel and Musgrave 1997], but in­stead use the basic conceptual 
framework of the Zone System to manage choices in tone reproduction. We .rst apply a scaling that is 
analogous to setting exposure in a camera. Then, if necessary, we apply automatic dodging-and-burning 
to accomplish dynamic range compression. 3.1 Initial luminance mapping We .rst show how to set the tonal 
range of the output image based on the scene s key value. Like many tone reproduction meth­ods [Tumblin 
and Rushmeier 1993; Ward 1994; Holm 1996], we view the log-average luminance as a useful approximation 
to the key of the scene. This quantity L¯w is computed by:  1¯ Lw = explog (d + Lw(x, y))(1) N x,y 
where Lw(x, y) is the world luminance for pixel (x, y), N is the total number of pixels in the image 
and d is a small value to avoid the singularity that occurs if black pixels are present in the image. 
If the scene has normal-key we would like to map this to middle-grey of the displayed image, or 0.18 
on a scale from zero to one. This suggests the equation: a L(x, y)= Lw(x, y) (2) ¯ Lw where L(x, y) 
is a scaled luminance and a =0.18. For low-key or high-key images we allow the user to map the log average 
to different values of a. We typically vary a from 0.18 up to 0.36 and 0.72 andvaryitdownto 0.09,and 
0.045. An example of varying is given in Figure 5. In the remainder of this paper we call the value of 
parameter a the key value , because it relates to the key of the image after applying the above scaling. 
The main problem with Equation 2 is that many scenes have pre­dominantly a normal dynamic range, but 
have a few high luminance regions near highlights or in the sky. In traditional photography this issue 
is dealt with by compression of both high and low lumi­nances. However, modern photography has abandoned 
these s ­shaped transfer curves in favor of curves that compress mainly the  Radiance map courtesy of 
Paul Debevec Figure 5: The linear scaling applied to the input luminance allows the user to steer the 
.nal appearance of the tone-mapped image. The dynamic range of the image is 7 zones. high luminances 
[Mitchell 1984; Stroebel et al. 2000]. A simple tone mapping operator with these characteristics is given 
by: L(x, y) Ld(x, y)= . (3) 1+ L(x, y) Note that high luminances are scaled by approximately 1/L, while 
low luminances are scaled by 1. The denominator causes a graceful blend between these two scalings. This 
formulation is guaranteed to bring all luminances within displayable range. However, as men­tioned in 
the previous section, this is not always desirable. Equa­tion 3 can be extended to allow high luminances 
to burn out in a controllable fashion: ( ) Ld(x, y)= L(x, y) 1+ L(x,y) L2 white (4) 1+ L(x, y) where 
Lwhite is the smallest luminance that will be mapped to pure white. This function is a blend between 
Equation 3 and a linear mapping. It is shown for various values of Lwhite in Figure 6. If Lwhite value 
is set to the maximum luminance in the scene Lmax or higher, no burn-out will occur. If it is set to 
in.nity, then the function reverts to Equation 3. By default we set Lwhite to the maximum luminance in 
the scene. If this default is applied to scenes that have a low dynamic range (i.e., Lmax < 1), the effect 
is a subtle contrast enhancement, as can be seen in Figure 7. The results of this function for higher 
dynamic range images is shown in the left images of Figure 8. For many high dynamic range images, the 
compression provided by this technique appears to be suf.cient to preserve detail in low contrast areas, 
while compress­ing high luminances to a displayable range. However, for very high dynamic range images 
important detail is still lost. For these im­ages a local tone reproduction algorithm that applies dodging-and­burning 
is needed (right images of Figure 8). 3.2 Automatic dodging-and-burning In traditional dodging-and-burning, 
all portions of the print poten­tially receive a different exposure time from the negative, bringing 
up selected dark regions or bringing down selected light re­gions to avoid loss of detail [Adams 1983]. 
With digital images we have the potential to extend this idea to deal with very high dynamic range images. 
We can think of this as choosing a key value for ev­ery pixel, which is equivalent to specifying a local 
a in Equation 2. L = 0.5 1.0 1.5 3 8 white d 012345 World luminance (L) Figure 6: Display luminance 
as function of world luminance for a family of values for Lwhite. Figure 7: Left: low dynamic range 
input image (dynamic range is 4 zones). Right: the result of applying the operator given by Equation 
4. This serves a similar purpose to the local adaptation methods of the perceptually-driven tone mapping 
operators [Pattanaik et al. 1998; Tumblin et al. 1999]. Dodging-and-burning is typically applied over 
an entire region bounded by large contrasts. For example, a local region might cor­respond to a single 
dark tree on a light background [Adams 1983]. The size of a local region is estimated using a measure 
of local contrast, which is computed at multiple spatial scales [Peli 1990]. Such contrast measures frequently 
use a center-surround function at each spatial scale, often implemented by subtracting two Gaussian blurred 
images. A variety of such functions have been proposed, in­cluding [Land and McCann 1971; Marr and Hildreth 
1980; Blom­maert and Martens 1990; Peli 1990; Jernigan and McLean 1992; Gove et al. 1995; Pessoa et al. 
1995] and [Hansen et al. 2000]. After testing many of these variants, we chose a center-surround function 
derived from Blommaert s model for brightness perception [Blom­maert and Martens 1990] because it performed 
the best in our tests. This function is constructed using circularly symmetric Gaussian pro.les of the 
form: 22 1 x +y Ri(x, y, s)= exp- . (5) p(ais)2 (ais)2 These pro.les operate at different scales s and 
at different image positions (x, y). Analyzing an image using such pro.les amounts to convolving the 
image with these Gaussians, resulting in a re­sponse Vi as function of image location, scale and luminance 
dis­tribution L: Vi(x, y, s)=L(x, y). Ri(x, y, s). (6) This convolution can be computed directly in 
the spatial domain, or for improved ef.ciency can be evaluated by multiplication in the Fourier domain. 
The smallest Gaussian pro.le will be only slightly larger than one pixel and therefore the accuracy with 
which the above equation is evaluated, is important. We perform the integra­tion in terms of the error 
function to gain a high enough accuracy without having to resort to super-sampling. The center-surround 
function we use is de.ned by: V1(x, y, s)- V2(x, y, s) V (x, y, s)= (7) 2fa/s2 +V1(x, y, s)  Figure 
8: The simple operator of Equation 3 brings out suf.cient detail in the top image (dynamic range is 6 
zones), although ap­plying dodging-and-burning does not introduce artifacts. For the bottom image (dynamic 
range is 15 zones) dodging-and-burning is required to make the book s text visible. where center V1 and 
surround V2 responses are derived from Equa­tions 5 and 6. This constitutes a standard difference of 
Gaussians approach, normalized by 2fa/s2 +V1 for reasons explained below. The free parameters a and f 
are the key value and a sharpening parameter respectively. For computational convenience, we set the 
center size of the next higher scale to be the same as the surround of the current scale. Our choice 
of center-surround ratio is 1.6, which results in a difference of Gaussians model that closely resembles 
a Laplacian of Gaussian .lter [Marr 1982]. From our experiments, this ratio appears to pro­duce slightly 
better results over a wide range of images than other choices of center-surround ratio. However, this 
ratio can be altered by a small amount to optimize the center-surround mechanism for speci.c images. 
Equation 7 is computed for the sole purpose of establishing a measure of locality for each pixel, which 
amounts to .nding a scale sm of appropriate size. This scale may be different for each pixel, and the 
procedure for its selection is the key to the success of our dodging-and-burning technique. It is also 
a deviation from the orig­inal Blommaert model [Blommaert and Martens 1990]. The area to be considered 
local is in principle the largest area around a given pixel where no large contrast changes occur. To 
compute the size of this area, Equation 7 is evaluated at different scales s. Note that V1(x, y, s)provides 
a local average of the luminance around (x, y) roughly in a disc of radius s. The same is true for V2(x, 
y, s)al­though it operates over a larger area at the same scale s.The val­ues of V1 and V2 are expected 
to be very similar in areas of small luminance gradients, but will differ in high contrast regions. To 
choose the largest neighborhood around a pixel with fairly even lu­minances, we threshold V to select 
the corresponding scale sm. Starting at the lowest scale, we seek the .rst scale sm where: |V (x, y, 
sm)| <E (8) is true. Here E is the threshold. The V1 in the denominator of Equa­  Radiance map courtesy 
of Paul Debevec Figure 9: An example of scale selection. The top image shows cen­ter and surround at 
different sizes. The lower images show the re­sults of particular choices of scale selection. If scales 
are chosen too small, detail is lost. On the other hand, if scales are chosen too large, dark rings around 
luminance steps will form. tion 7 makes thresholding V independent of absolute luminance level, while 
the 2fa/s2 term prevents V from becoming too large when V approaches zero. Given a judiciously chosen 
scale for a given pixel, we observe that V1(x, y, sm) may serve as a local average for that pixel. Hence, 
the global tone reproduction operator of Equation 3 can be con­verted into a local operator by replacing 
L with V1 in the denomi­nator: L(x, y) Ld(x, y)= (9) 1+ V1(x, y, sm(x, y)) This function constitutes 
our local dodging-and-burning operator. The luminance of a dark pixel in a relatively bright region will 
sat­isfy L<V1, so this operator will decrease the display luminance Ld, thereby increasing the contrast 
at that pixel. This is akin to pho­tographic dodging . Similarly, a pixel in a relatively dark region 
will be compressed less, and is thus burned . In either case the pixel s contrast relative to the surrounding 
area is increased. For this reason, the above scale selection method is of crucial impor­tance, as illustrated 
in the example of Figure 9. If sm is too small, then V1 is close to the luminance L and the local operator 
reduces to our global operator (s1 in Figure 9). On the other hand, choosing sm too large causes dark 
rings to form around bright areas (s3 in the same .gure), while choosing the scale as outlined above 
causes the right amount of detail and contrast enhancement without intro­ducing unwanted artifacts (s2 
in Figure 9). Using a larger scale sm tends to increase contrast and enhance edges. The value of the 
threshold E in Equation 8, as well as the choice of f in Equation 7, serve as edge enhancement parameters 
and work by manipulating the scale that would be chosen for each pixel. Decreasing E forces the appropriate 
scale sm to be larger. Increasing f also tends to select a slightly larger scale sm, but only at small 
scales due to the division of f by s 2 . An example of the effect of varying f is given in Figure 10. 
A further observation is that because V1 tends to be smaller than L for very bright pixels, our local 
operator is not guaranteed to keep the display luminance Ld below 1. Thus, for extremely bright areas 
some burn-out may occur and this is the reason we clip the display luminance to 1 afterwards. As noted 
in section 2, a small amount of burn-out may be desirable to make light sources such as the sun look 
very bright. In summary, by automatically selecting an appropriate neigh­borhood for each pixel we effectively 
implement a pixel-by-pixel dodging and burning technique as applied in photography [Adams 1983]. These 
techniques locally change the exposure of a .lm, and so darken or brighten certain areas in the .nal 
print.  4 Results We implemented our algorithm in C++ and obtained the luminance values from the input 
R, G and B triplets with L =0.27R + 0.67G +0.06B. The convolutions of Equation 5 were computed using 
a Fast Fourier Transform (FFT). Because Gaussians are sepa­rable, these convolutions can also be ef.ciently 
computed in image space. This is easier to implement than an FFT, but it is somewhat slower for large 
images. Because of the normalization by V1, our method is insensitive to edge artifacts normally associated 
with the computation of an FFT. The key value setting is determined on a per image basis, while unless 
noted otherwise, the parameter f is set to 8.0 for all the im­ages in this paper. Our new local operator 
uses Gaussian pro.les s at 8 discrete scales increasing with a factor of 1.6 from 1 pixel wide to 43 
pixels wide. For practical purposes we would like the Gaussian pro.le at the smallest scale to have 2 
standard deviations overlap with 1 pixel. This is achieved by setting the scaling param­ v eter a1 to 
1/22 0.35. The parameter a2 is 1.6 times as large. The threshold E used for scale selection was set 
to 0.05. We use images with a variety of dynamic ranges as indicated throughout this section. Note that 
we are using the photographic de.nition of dynamic range as presented in Section 2. This results in somewhat 
lower ranges than would be obtained if a conventional computer graphics measure of dynamic range were 
used. However, we believe the photographic de.nition is more predictive of how challenging the tone reproduction 
of a given image is. In the absence of well-tested quantitative methods to compare tone mapping operators, 
we compare our results to a representative set of tone reproduction techniques for digital images. In 
this sec­tion we brie.y introduce each of the operators and show images of them in the next section. 
Speci.cally, we compare our new operator of Equation 9 with the following. Stockham s homomorphic .ltering 
Using the observation that lighting variation occurs mainly in low frequencies and hu­mans are more aware 
of albedo variations, this method op­erates by downplaying low frequencies and enhancing high frequencies 
[Oppenheim et al. 1968; Stockham 1972]. Tumblin-Rushmeier s brightness matching operator . A model of 
brightness perception is used to drive this global operator. Scale too small (s ) 1 2 Scale too large 
(s ) 3  We use the 1999 formulation [Tumblin et al. 1999] as we have found it produces much better subjective 
results to the earlier versions [Tumblin and Rushmeier 1991; Tumblin and Rush­meier 1993]. Chiu s local 
scaling A linear scaling that varies continuously is used to preserve local contrast with heuristic dodging-and­burning 
used to avoid burn-out [Chiu et al. 1993]. Ward s contrast scale factor A global multiplier is used that 
aims to maintain visibility thresholds [Ward 1994]. Ferwerda s adaptation model This operator alters 
contrast, color saturation and spatial frequency content based on psy­chophysical data [Ferwerda et al. 
1996]. We have used the photopic portion of their algorithm. Ward s histogram adjustment method This 
method uses an im­age s histogram to implicitly segment the image so that sep­arate scaling algorithms 
can be used in different luminance zones. Visibility thresholds drive the processing [Ward et al. 1997]. 
The model incorporates human contrast and color sen­sitivity, glare and spatial acuity, although for 
a fair comparison we did not use these features. Schlick s rational sigmoid This is a family of simple 
and fast methods using rational sigmoid curves and a set of tunable parameters [Schlick 1994]. Pattanaik 
s local adaptation model Both threshold and supra­threshold vision is considered in this multi-scale 
model of lo­cal adaptation [Pattanaik et al. 1998]. Chromatic adaptation is also included. Note that 
the goals of most of these operators are different from our goal of producing a subjectively satisfactory 
image. However, we compare their results with ours because all of the above methods do produce subjectively 
pleasing images for many inputs. There are comparisons possible with many other techniques that are out­side 
the scope of this evaluation. In particular, we do not compare our results with the .rst perceptually-driven 
works [Miller et al. 1984; Upstill 1985] because they are not widely used in graphics and are similar 
to works we do compare with [Ward 1994; Ferw­erda et al. 1996; Tumblin et al. 1999]. We also do not compare 
with the multiscale-Retinex work because it is reminiscent of Pattanaik s local adaptation model, while 
being aimed at much lower contrast reductions of about 5:1 [Rahman et al. 1996]. Holm has a com­plete 
implementation of the Zone System for digital cameras [Holm 1996], but his contrast reduction is also 
too low for our purposes. Radiance map and top image courtesy of Cornell Program of Computer Graphics 
 Next, we do not compare with the layering method because it re­quires albedo information in addition 
to luminances [Tumblin et al. 1999]. Finally, we consider some work to be visualization methods for digital 
images rather than true tone mapping operators. These are the LCIS .lter which consciously allows visible 
artifacts in ex­change for visualizing detail [Tumblin and Turk 1999], the mouse­driven foveal adaptation 
method [Tumblin et al. 1999] and Pardo s multi-image visualization technique [Pardo and Sapiro 2001]. 
The format in which we compare the various methods is a knock-out race using progressively more dif.cult 
images. We take this approach to avoid an extremely large number of images. In Figure 11 eight different 
tone mapping operators are shown side by Rendering by Peter Shirley     side using the Cornell 
box high dynamic range image as input. The model is slightly different from the original Cornell box 
because we have placed a smaller light source underneath the ceiling of the box so that the ceiling receives 
a large quantity of direct illumination, a characteristic of many architectural environments. This image 
has little high frequency content and it is therefore easy to spot any de.ciencies in the tone mapping 
operators we have applied. In this and the following .gures, the operators are ordered roughly by their 
ability to bring the image within dynamic range. Using the Cornell box image (Figure 11), we eliminate 
those operators that darken the image too much and therefore we do not include the contrast based scaling 
factor and Chiu s algorithm in further tests. Similar to the Cornell box image is the Nave photograph, 
al­though this is a low-key image and the stained glass windows con­tain high frequency detail. From 
a photographic point of view, good tone mapping operators would show detail in the dark areas while still 
allowing the windows to be admired. The histogram adjust­ment algorithm achieves both goals, although 
halo-like artifacts are introduced around the bright window. Both the Tumblin-Rushmeier model and Ferwerda 
s visibility matching method fail to bring the church window within displayable range. The same is true 
for Stockham style .ltering and Schlick s method. The most dif.cult image to bring within displayable 
range is presented in Figures 1 and 13. Due to its large dynamic range, it presents problems for most 
tone reproduction operators. This im­age was .rst used for Pattanaik s local adaptation model [Pattanaik 
et al. 1998]. Because his operator includes color correction as well as dynamic range reduction, we have 
additionally color corrected our tone-mapped image (Figure 13) using the method presented in [Reinhard 
et al. 2001]. Pattanaik s local adaptation operator pro­duces visible artifacts around the light source 
in the desk image, while the new operator does not. The ef.ciency of both our new global (Equation 3, 
without dodging-and-burning) and local tone mapping operators (Equa­tion 9) is high. Timings obtained 
on a 1.8 GHz Pentium 4 PC are given in Table 1 for two different image sizes. While we have not counted 
any disk I/O, the timings for preprocessing as well as the main tone mapping algorithm are presented. 
The preprocessing for the local operator (Equation 9) consists of the mapping of the log average luminance 
to the key value, as well as all FFT calcu­lations. The total time for a 5122 image is 1.31 seconds for 
the local operator, which is close to interactive, while our global oper­ator (Equation 3) performs at 
a rate of 20 frames per second, which we consider real-time. Computation times for the 10242 images is 
around 4 times slower, which is according to expectation. We have also experimented with a fast approximation 
of the Gaussian convolution using a multiscale spline based ap­proach [Burt and Adelson 1983], which 
was .rst used in the con­text of tone reproduction by [Tumblin et al. 1999], and have found that the 
computation is about 3.7 times faster than our Fourier do­main implementation. This improved performance 
comes at the cost of some small artifacts introduced by the approximation, which can be successfully 
masked by the high frequency content of the photographs. If high frequencies are absent, some blocky 
artifacts become visible, as can be seen in Figure 14. On the other hand, just like its FFT based counter-part, 
this approximation manages to bring out the detail of the writing on the open book in this .gure as opposed 
to our global operator of Equation 3 (compare with the left image of Figure 8). As such, the local FFT 
based implementa­tion, the local spline based approximation and the global operator provide a useful 
trade-off between performance and quality, allow­ing any user to select the best operator given a speci.ed 
maximum run-time. Finally, to demonstrate that our method works well on a broad range of high dynamic 
range images, Figure 15 shows a selection of tone-mapped images using our new operator. It should be 
noted Accurate implementation Spline approximation Radiance map courtesy of Cornell Program of Computer 
Graphics Algorithm Preprocessing Tone Mapping Total Image size: 512 × 512 Local 1.23 0.08 1.31 Spline 
0.25 0.11 0.36 Global 0.02 0.03 0.05 Image size: 1024 × 1024 Local 5.24 0.33 5.57 Spline 1.00 0.47 1.47 
Global 0.70 0.11 0.18  Table 1: Timing in seconds for our global (Equation 3) and local (Equation 9) 
operators. The middle rows show the timing for the approximated Gaussian convolution using a multiscale 
spline ap­proach [Burt and Adelson 1983]. that most of the images in this .gure present serious challenges 
to other tonemapping operators. Interestingly, the area around the sun in the rendering of the landscape 
is problematic for any method that attempts to bring the maximum scene luminance within a dis­playable 
range without clamping. This is not the case for our oper­ator because it only brings textured regions 
within range, which is relatively simple because, excluding the sun, this scene only has a small range 
of luminances. A similar observation can be made for the image of the lamp on the table and the image 
with the streetlight behind the tree. 5 Summary Photographers aim to compress the dynamic range of a 
scene in a manner that creates a pleasing image. We have developed a rela­tively simple and fast tone 
reproduction algorithm for digital im­ages that borrows from 150 years of photographic experience. It 
is designed to follow their practices and is thus well-suited for ap­plications where creating subjectively 
satisfactory and essentially artifact-free images is the desired goal.  Acknowledgments Many researchers 
have made their high dynamic range images and/or their tone mapping software available, and without that 
help our comparisons would have been impossible. This work was sup­ported by NSF grants 89-20219, 95-23483, 
97-96136, 97-31859, 98-18344, 99-77218, 99-78099, EIA-8920219 and by the DOE AVTC/VIEWS.     Radiance 
maps courtesy of Paul Debevec Renderings by Peter Shirley Radiance map courtesy of Greg Ward  Radiance 
map courtesy of Cornell Program of Computer Graphics Photograph courtesy of Franz and Ineke Reinhard 
Radiance map courtesy of Jack Tumblin, Northwestern University Radiance map courtesy of Greg Ward  
 References ADAMS, A. 1980. The camera. The Ansel Adams Photography series. Little, Brown and Company. 
ADAMS, A. 1981. The negative. The Ansel Adams Photography series. Little, Brown and Company. ADAMS, A. 
1983. The print. The Ansel Adams Photography series. Little, Brown and Company. BLOMMAERT, F.J.J., AND 
MARTENS, J.-B. 1990. An object-oriented model for brightness perception. Spatial Vision 5, 1, 15 41. 
BURT,P. J., AND ADELSON, E. H. 1983. A multiresolution spline with application to image mosaics. ACM 
Transactions on Graphics 2, 4, 217 236. CHIU,K., HERF,M., SHIRLEY,P., SWAMY,S., WANG,C., AND ZIMMERMAN, 
K. 1993. Spatially nonuniform scaling functions for high contrast images. In Proceedings of Graphics 
Interface 93, 245 253. COHEN,J., TCHOU,C., HAWKINS,T., AND DEBEVEC, P. 2001. Real-Time high dynamic 
range texture mapping. In Rendering techniques 2001, S. J. Gortler and K. Myszkowski, Eds., 313 320. 
 DEBEVEC,P. E., AND MALIK, J. 1997. Recovering high dynamic range radiance maps from photographs. In 
SIGGRAPH 97 Conference Proceedings, Addison Wes­ley, T. Whitted, Ed., Annual Conference Series, ACM SIGGRAPH, 
369 378. DURAND,F., AND DORSEY, J. 2000. Interactive tone mapping. In Eurographics Workshop on Rendering, 
219 230. FAIRCHILD, M. D. 1998. Color appearance models. Addison-Wesley, Reading, MA. FERWERDA,J.A., 
PATTANAIK,S., SHIRLEY,P., AND GREENBERG, D. P. 1996. A model of visual adaptation for realistic image 
synthesis. In SIGGRAPH 96 Confer­ence Proceedings, Addison Wesley, H. Rushmeier, Ed., Annual Conference 
Series, ACM SIGGRAPH, 249 258. GEIGEL,J., AND MUSGRAVE, F. K. 1997. A model for simulating the photographic 
development process on digital images. In SIGGRAPH 97 Conference Proceedings, Addison Wesley, T. Whitted, 
Ed., Annual Conference Series, ACM SIGGRAPH, 135 142. GOVE,A., GROSSBERG,S., AND MINGOLLA, E. 1995. Brightness 
perception, illusory contours, and corticogeniculate feedback. Visual Neuroscience 12, 1027 1052. GRAVES, 
C. 1997. The zone system for 35mm photographers, second ed. Focal Press. HANSEN,T., BARATOFF,G., AND 
NEUMANN, H. 2000. A simple cell model with dominating opponent inhibition for robust contrast detection. 
Kognitionswis­senschaft 9, 93 100. HOLM, J. 1996. Photographics tone and colour reproduction goals. In 
CIE Expert Symposium 96 on Colour Standards for Image Technology, 51 56. JERNIGAN,M. E., AND MCLEAN, 
G. F. 1992. Lateral inhibition and image process­ing. In Non-linear vision: determination of neural receptive 
.elds, function, and networks, R. B. Pinter and B. Nabet, Eds. CRC Press, ch. 17, 451 462. JOHNSON, C. 
1999. The practical zone system. Focal Press. LAND,E. H., AND MCCANN, J. J. 1971. Lightness and retinex 
theory. J. Opt. Soc. Am. 63, 1, 1 11. LONDON,B., AND UPTON, J. 1998. Photography, sixth ed. Longman. 
MARR,D., AND HILDRETH, E. C. 1980. Theory of edge detection. Proceedings of the Royal Society of London, 
B 207, 187 217. MARR, D. 1982. Vision, a computational investigation into the human representation and 
processing of visual information. W H Freeman and Company, San Fransisco. MATKOVIC,K., NEUMANN,L., AND 
PURGATHOFER, W. 1997. A survey of tone mapping techniques. In 13th Spring Conference on Computer Graphics, 
W. Straßer, Ed., 163 170. MCNAMARA,A., CHALMERS,A., AND TROSCIANKO, T. 2000. STAR: Visual perception 
in realistic image synthesis. In Eurographics 2000 STAR reports,Euro­graphics, Interlaken, Switzerland. 
MCNAMARA, A. 2001. Visual perception in realistic image synthesis. Computer Graphics Forum 20, 4 (December), 
211 224. MILLER,N. J., NGAI,P. Y., AND MILLER, D. D. 1984. The application of computer graphics in lighting 
design. Journal of the IES 14 (October), 6 26. MITCHELL, E. N. 1984. Photographic Science. John Wiley 
and Sons, New York. OPPENHEIM,A. V., SCHAFER,R., AND STOCKHAM, T. 1968. Nonlinear .ltering of multiplied 
and convolved signals. Proceedings of the IEEE 56, 8, 1264 1291. PARDO,A., AND SAPIRO, G. 2001. Visualization 
of high dynamic range images. Tech. Rep. 1753, Institute for Mathematics and its Applications, University 
of Min­nesota. PATTANAIK,S. N., FERWERDA,J. A., FAIRCHILD,M. D., AND GREENBERG,D. P. 1998. A multiscale 
model of adaptation and spatial vision for realistic image dis­play. In SIGGRAPH 98 Conference Proceedings, 
Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, 287 298. PATTANAIK,S.N., TUMBLIN,J., 
YEE,H., , AND GREENBERG, D. P. 2000. Time­dependent visual adaptation for fast realistic display. In 
SIGGRAPH 2000 Con­ference Proceedings, Addison Wesley, K. Akeley, Ed., Annual Conference Series, ACM 
SIGGRAPH, 47 54. PELI, E. 1990. Contrast in complex images. J. Opt. Soc. Am. A 7, 10 (October), 2032 
2040. PESSOA,L., MINGOLLA,E., AND NEUMANN, H. 1995. A contrast-and luminance­driven multiscale network 
model of brightness perception. Vision Research 35, 15, 2201 2223. RAHMAN,Z., JOBSON,D. J., AND WOODELL, 
G. A. 1996. A multiscale retinex for color rendition and dynamic range compression. In SPIE Proceedings: 
Appli­cations of Digital Image Processing XIX, vol. 2847. RAHMAN,Z., WOODELL,G. A., AND JOBSON, D. J. 
1997. A comparison of the multiscale retinex with other image enhancement techniques. In IS&#38;T s 50th 
Annual Conference: A Celebration of All Imaging, vol. 50, 426 431. REINHARD,E., ASHIKHMIN,M., GOOCH,B., 
AND SHIRLEY, P. 2001. Color transfer between images. IEEE Computer Graphics and Applications 21 (Septem­ber/October), 
34 41. SCHEEL,A., STAMMINGER,M., AND SEIDEL, H.-P. 2000. Tone reproduction for interactive walkthroughs. 
Computer Graphics Forum 19, 3 (August), 301 312. SCHLICK, C. 1994. Quantization techniques for the visualization 
of high dynamic range pictures. In Photorealistic Rendering Techniques, Springer-Verlag Berlin Heidelberg 
New York, P. Shirley, G. Sakas, and S. M¨ uller, Eds., 7 20. STOCKHAM, T. 1972. Image processing in 
the context of a visual model. Proceedings of the IEEE 60, 7, 828 842. STROEBEL,L., COMPTON,J., CURRENT,I., 
AND ZAKIA, R. 2000. Basic photo­graphic materials and processes, second ed. Focal Press. TUMBLIN,J., 
AND RUSHMEIER, H. 1991. Tone reproduction for realistic computer generated images. Tech. Rep. GIT-GVU-91-13, 
Graphics, Visualization, and Use­ability Center, Georgia Institute of Technology. TUMBLIN,J., AND RUSHMEIER, 
H. 1993. Tone reproduction for computer generated images. IEEE Computer Graphics and Applications 13, 
6 (November), 42 48. TUMBLIN,J., AND TURK,G.1999.LCIS:Aboundaryhierarchyfordetail-preserving contrast 
reduction. In Siggraph 1999, Computer Graphics Proceedings, Addison Wesley Longman, Los Angeles, A. Rockwood, 
Ed., Annual Conference Series, 83 90. TUMBLIN,J., HODGINS,J. K., AND GUENTER, B. K. 1999. Two methods 
for display of high contrast images. ACM Transactions on Graphics 18 (1), 56 94. UPSTILL, S. 1985. The 
Realistic Presentation of Synthetic Images: Image Processing in Computer Graphics. PhD thesis, University 
of California at Berkeley. WARD,G., RUSHMEIER,H., AND PIATKO, C. 1997. A visibility matching tone reproduction 
operator for high dynamic range scenes. IEEE Transactions on Visu­alization and Computer Graphics 3, 
4 (December). WARD LARSON,G., AND SHAKESPEARE, R. A. 1998. Rendering with Radiance. Morgan Kaufmann Publishers. 
WARD, G. 1994. A contrast-based scalefactor for luminance display. In Graphics Gems IV, P. Heckbert, 
Ed. Academic Press, Boston, 415 421. WHITE,M., ZAKIA,R., AND LORENZ, P. 1984. The new zone system manual. 
Morgan &#38; Morgan, Inc. WOODS, J. C. 1993. The zone system craftbook. McGraw Hill. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566576</article_id>
		<sort_key>277</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Transferring color to greyscale images]]></title>
		<page_from>277</page_from>
		<page_to>280</page_to>
		<doi_number>10.1145/566570.566576</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566576</url>
		<abstract>
			<par><![CDATA[We introduce a general technique for "colorizing" greyscale images by transferring color between a source, color image and a destination, greyscale image. Although the general problem of adding chromatic values to a greyscale image has no exact, objective solution, the current approach attempts to provide a method to help minimize the amount of human labor required for this task. Rather than choosing RGB colors from a palette to color individual components, we transfer the entire color "mood" of the source to the target image by matching luminance and texture information between the images. We choose to transfer only chromatic information and retain the original luminance values of the target image. Further, the procedure is enhanced by allowing the user to match areas of the two images with rectangular swatches. We show that this simple technique can be successfully applied to a variety of images and video, provided that texture and luminance are sufficiently distinct. The images generated demonstrate the potential and utility of our technique in a diverse set of application domains.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[color]]></kw>
			<kw><![CDATA[image processing]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
			<kw><![CDATA[video]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Color</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382472</person_id>
				<author_profile_id><![CDATA[81100282873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomihisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Welsh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Visual Computing, SUNY, Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14210394</person_id>
				<author_profile_id><![CDATA[81100609852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ashikhmin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Visual Computing, SUNY, Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14098811</person_id>
				<author_profile_id><![CDATA[81452605314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Klaus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mueller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Visual Computing, SUNY, Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A. AND FREEMAN, W. T., 2001. Image Quilting for Texture Synthesis and Transfer, In Proceedings of ACM SIGGRAPH 2001, 341-346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A. AND LEUNG, T. K., 1999. Texture Synthesis by Non-parametric sampling. 7th IEEE International Conference on Computer Vision, 1999, 1033-1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>22881</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[GONZALEZ, R. C. AND WINTZ, P., 1987. Digital Image Processing, Addison-Wesley Publishing, Reading MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., JACOBS, C., OLIVER, N., CURLESS, B., SALESIN., D., 2001. Image Analogies, In Proceedings of ACM SIGGRAPH 2002, 341-346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[SILBERG, J., 1998. Cinesite Press Article, http://www.cinesite.com/core/press/articles/1998/10_00_98-team.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[IOANNIS, P., 1991. Digital Image Processing Algorithms, Prentis Hall, 58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130597</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[PRATT, W. K. 1991. Digital Image Processing, John Wiley & Sons, 311.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618848</ref_obj_id>
				<ref_obj_pid>616072</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[REINHARD, E. ASHIKHMIN, M., GOOCH B. AND SHIRLEY, P., 2001. Color Transfer between Images, IEEE Computer Graphics and Applications, September/October 2001, 34-40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601699</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[ROGOWITZ, B. E. AND KALVIN, A. D., 2001. The "Which Blair Project": A Quick Visual Method for Evaluating Perceptual Color Maps, Proceedings of IEEE Visualization 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[RUDERMAN, D. L., CRONIN, T. W. AND CHIAO, C. C., 1998. Statistics of Cone Responses to Natural Images: Implications for Visual Coding, J. Optical Soc. of America, vol 15, no. 8, 2036-2045.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Transferring Color to Greyscale Images Tomihisa Welsh Michael Ashikhmin Klaus Mueller Center for Visual 
Computing, Computer Science Department, SUNY at Stony Brook*  Figure 1: Colors are transferred to the 
second image without user intervention. Source image courtesy &#38;#169; Ian Britton -FreeFoto.com Abstract 
We introduce a general technique for colorizing greyscale images by transferring color between a source, 
color image and a destination, greyscale image. Although the general problem of adding chromatic values 
to a greyscale image has no exact, objec­tive solution, the current approach attempts to provide a method 
to help minimize the amount of human labor required for this task. Rather than choosing RGB colors from 
a palette to color individual components, we transfer the entire color mood of the source to the target 
image by matching luminance and texture information between the images. We choose to transfer only chromatic 
infor­mation and retain the original luminance values of the target image. Further, the procedure is 
enhanced by allowing the user to match areas of the two images with rectangular swatches. We show that 
this simple technique can be successfully applied to a variety of images and video, provided that texture 
and luminance are suffi­ciently distinct. The images generated demonstrate the potential and utility 
of our technique in a diverse set of application domains. Keywords: Image Processing, Color, Texture 
Synthesis, Video 1 Introduction Color can be added to greyscale images in order to increase the visual 
appeal of images such as old black and white photos, classic movies or scientific illustrations. In addition, 
the information con­tent of some scientific images can be perceptually enhanced with color by exploiting 
variations in chromaticity as well as luminance. The task of colorizing a greyscale image involves assigning 
three-dimensional (RGB) pixel values to an image which varies along only one dimension (luminance or 
intensity). Since different colors may have the same luminance value but vary in hue or satu­ration, 
the problem of colorizing greyscale images has no inher­ently correct solution. Due to these ambiguities, 
human interaction usually plays a large role in the colorization process. *{tfwelsh,ash,mueller}@cs.sunysb.edu, 
http://www.cs.sunysb.edu/~tfwelsh/colorize Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Even in the case of pseudocoloring, [Gonzalez and Wintz 
1987] where the mapping of luminance values to color values is auto­matic, the choice of the colormap 
is commonly determined by human decision. Since most colorization software used in the movie industry 
is proprietary, detailed technical documents describing the process are generally not publicly available. 
However, a few web articles describe software in which humans must meticulously hand-color each of the 
individual image regions. For example, one software package is described in which the image is first 
polygonalized so the user can color individual components much like a coloring book. Then the system 
tracks polygons between frames and trans­fers colors in order to reduce the number of frames that the 
user must color manually [Silberg 1998]. Alternatively, photographs can be colorized using photo-editing 
software to manually or auto­matically select components of a scene. The area is then painted over with 
a color selected from a palette using a low opacity level. There also exist a number of applications 
for the use of color in information visualization. For example, Gonzalez and Wintz [1987] describe a 
simple approach for pseudocoloring greyscale images of luggage acquired by X-ray equipment at an airport. 
The method uses separate transformations for each color channel which results in coloring objects with 
the density of explosives in bright orange and other objects with a blue tone. Further, color can be 
added to a range of scientific images for illustrative and educa­tional purposes. In medicine, image 
modalities which only acquire greyscale images such Magnetic Resonance Imaging (MRI), X-ray and Computerized 
Tomography (CT) images can be enhanced with color for presentations and demonstrations. Pseudocoloring 
is a common technique for adding color to greyscale images such as X-ray, MRI, scanning electron micros­copy 
(SEM) and other imaging modalities in which color informa­tion does not exist. Pratt [1991] describes 
this method as an image enhancement technique because it can be used to enhance the detectability of 
detail within the image. In its most basic form, pseudocoloring is a transformation T [Pitas 1993], such 
that, (, ) = T(fxy)) where fxy) is the original greyscale image c xy (, (, and c xy) is the resulting 
color vector for the three RGB color (, channels. A simplified example of this method is the application 
of an arbitrary color map to the data where a single, global color vec­tor is assigned to each greyscale 
value. The strength of this approach is that it does not alter the information content of the original 
data since no extra information is introduced. For example, in an pseudo-colored MRI image there will 
be a one-to-one corre­spondence between each density value and color value, even though the color choice 
is arbitrary. However, by using a colormap which does not increase monotonically in luminance, pseudocol­ored 
images may introduce perceptual distortions. Studies have found a strong correlation of the perceived 
naturalness of face images and the degree to which the luminance values increase monotonically in the 
colormap [Rogowitz and Kalvin 2001]. Our concept of transferring color from one image to another is inspired 
by work by Reinhard et al. [2001] in which color is transferred between two color images. In their work, 
colors from a source image are transferred to a second colored image using a simple but surprisingly 
successful procedure. The basic method matches the three-dimensional distribution of color values between 
the images and then transforms the color distribution of the target image to match the distribution of 
the source image. Further, swatches can be employed to match similar areas between the two images. In 
this study, the greyscale image is represented by a one­dimensional distribution, hence only the luminance 
channels can be matched between the two images. Because a single luminance value could represent entirely 
different parts of an image, the sta­tistics within the pixel s neighborhood are used to guide the match­ing 
process. Once a pixel is matched, the color information is transferred but the original luminance value 
is retained. In difficult cases, a few swatches can be used to aid the matching process between the source 
and the target image. After color is transferred between the source and the target swatches, the final 
colors are assigned to each pixel in the greyscale image by matching each greyscale image pixel to a 
pixel in the target swatches using the L2 distance metric. Thus, each pixel match is determined by matching 
it only to other pixels within the same image. We have found that this simple procedure works well for 
a wide range of image types. Further, these methods are easily extended to video. Here the col­orization 
procedure is first applied to a single frame in the video sequence. The other frames in the scene are 
then assigned a color using the original frame s colorized swatches. We have found that for scene frames 
in which the objects do not change dramatically, the colorization procedure works surprisingly well. 
 2 Color Transfer Algorithm In this section, we describe the general algorithm for transferring color; 
the basic idea is then extended to use swatches. The general procedure for color transfer requires a 
few simple steps. First each image is converted into the laß color space. We use jittered sam­pling to 
select a small subset of pixels in the color image as sam­ples. Next, we go through each pixel in the 
greyscale image in scan-line order and select the best matching sample in the color image using neighborhood 
statistics. The best match is determined by using a weighted average of pixel luminance and the neighbor­hood 
statistics. The chromaticity values (a,ß channels)ofthe best matching pixel are then transferred to the 
greyscale image to form the final image. Color transfer using swatches involves the same global image 
matching procedure but only between the source and target swatches. The colorized pixels in the target 
swatch regions are then used as the source pixels for the color transfer to the remaining non-colorized 
pixels using a texture synthesis approach. More specific details and justification are provided below. 
2.1 Global Image Matching Both color (source) and greyscale (target) RGB images are con­verted to the 
decorrelated laß space [Ruderman et al. 1998] for subsequent analysis. laß space was developed to minimize 
corre­lation between the three coordinate axes of the color space. The color space provides three decorrelated, 
principal channels corre­sponding to an achromatic luminance channel (l) and two chro­matic channels 
a and ß, which roughly correspond to yellow-blue and red-green opponent channels. Thus, changes made 
in one color channel should minimally affect values in the other channels. The reason the laß color space 
is selected in the current procedure is because it provides a decorrelated achromatic channel for color 
images. This allows us to selectively transfer the chromatic a and ß channels from the color image to 
the greyscale image without cross-channel artifacts. The transformation procedure follows directly from 
Reinhard et al. [2001]. In order to transfer chromaticity values from the source to the target, each 
pixel in the greyscale image must be matched to a pixel in the color image. The comparison is based on 
the lumi­nance value and neighborhood statistics of that pixel. The lumi­nance value is determined by 
the l channel in laß space. In order to account for global differences in luminance between the two images 
we perform luminance remapping [Hertzmann et al. 2001] to linearly shift and scale the luminance histogram 
of the source image to fit the histogram of the target image. This helps create a better correspondence 
in the luminance range between the two images but does not alter the luminance values of the target image. 
The neighborhood statistics are precomputed over the image and consist of the standard deviation of the 
luminance values of the pixel neighborhood. We have found that a neighborhood size of 5x5 pixels works 
well for most images. For some problematic images we use a larger neighborhood size. Since most of the 
visually significant variation between pixel values is attributed to luminance differences, we can limit 
the num­ber of samples we use as source color pixels and still obtain a sig­nificant range of color variation 
in the image. This allows us to reduce the number of comparisons made for each pixel in the grey­scale 
image and decrease computation time. We have found that approximately 200 samples taken on a randomly 
jittered grid is sufficient. Then for each pixel in the greyscale image in scan-line order the best matching 
color sample is selected based on the weighted average of luminance (50%) and standard deviation (50%). 
We have also included the neighborhood mean and varied the ratio of these weights but have not found 
significant differ­ences in the results. Once the best matching pixel is found, the a and ß chromaticity 
values are transferred to the target pixel while the original luminance value is retained. This automatic, 
global matching procedure works reasonably well on images when corresponding color regions between the 
two images also correspond in luminance values. However, regions in the target image which do not have 
a close luminance value to an appropriate structure in the source image will not appear correct. Figure 
2c shows the results of transferring color using the global image matching procedure. Here, the sky and 
trees match reason­ably well between the images, but the road in the target does not match to the road 
in the source. Figure 2: The two variations of the algorithm. (a) Source color image. (b) Result of 
basic, global algorithm applied (no swatches). (c) Greyscale image with swatch colors transferred from 
Figure 2a. (d) Result using swatches. 2.2 Swatches In order to allow more user interaction in the color 
transfer proce­dure and to improve results, swatches are used between corre­sponding regions in the two 
images. Figures 2a,b,d demonstrate the basic idea. The first step is to use the general procedure described 
above to transfer color, but now only between the corre­sponding swatches. This allows the user to selectively 
transfer col­ors between the source and target swatches. We also expect the results to be good for individual 
swatches because there should be less overlap of luminance levels between different color regions within 
the same swatch. We perform luminance remapping as in the global procedure but only between corresponding 
swatches. Again, we use random jittered sampling with approximately 50 samples per swatch. The second 
step is similar to texture synthesis algorithms [Efros and Leung 1999; Efros and Freeman 2001] in which 
the L2 distance is used to find texture matches. We define the error distance E using the L2 metric between 
neighborhood Ng in the greyscale image and neighborhood Ns in the colorized swatch as: ENg Ns)= . [Ip 
()]2 (, () Sp p .N where I is the greyscale image, S is the luminance channel of the colorized swatch 
and p are the pixels in these neighborhoods. Note, at this stage we no longer search the color image 
for texture matches but only search for matches within the colorized swatches in the target image. The 
advantage of the approach is that in the first stage we transfer colors to the swatches selectively which 
prevents pixels with similar neighborhood statistics but from the wrong part of the image from corrupting 
the target swatch colors. It also allows the user to transfer colors from any part of image to a select 
region even if the two corresponding regions vary largely from one another in texture and luminance levels. 
Sec­ondly, since we expect there to be more texture coherence within an image than between two different 
images, we expect pixels which are similar in texture to the colorized target swatches to be colorized 
similarly. 2.3 Video Colorization of video can be automated using the colorization pro­cedure described 
above. To colorize all of the frames in a scene, we first transfer color from a source color image to 
a single target frame. Then every frame in the video sequence can be colorized using the same colorized 
target swatches used in the single frame. If a single frame is successfully colorized using these procedures, 
then frames which consist of the same objects as that single frame will be colorized similarly. Three 
sample clips are provided on the video proceedings. 3. Results Figure 3 showcases the final results of 
the algorithm applied to a variety of image domains. Figures 3a-c shows the results of col­orizing foliage, 
face and landscape photographs. The technique works well on scenes where the image is divided into distinct 
lumi­nance clusters or where each of the regions has distinct textures. In general, however, the current 
technique does not work very well with faces. Although colors are transferred well into the swatches, 
the L2 distance is not always a sufficient measure in classifying the difference between skin and lips 
and sometimes clothes and hair. Figures 3d-f demonstrate the use of the algorithm with differ­ent types 
of scientific data. Although this technique is not intended for clinical diagnosis with medical image 
data, it might be used in anatomical illustrations or to enhance scientific presentations. Although we 
show that the algorithm works well in a number of image domains, we do not claim that the technique will 
work on most images. It should be clear that when one considers only a small neighborhood size around 
a pixel it is often impossible to determine whether that neighborhood belongs to one texture or another. 
However, by using high resolution images and larger neighborhoods we can obtain improved results. Further, 
we believe that more images can be colorized using the basic method provided but with better texture 
classification methods at the expense of simplicity and computation time. The running time of the algorithm 
for one image can range from 15 seconds to 4 minutes on a Pentium III 900 Mhz CPU using optimized MATLAB 
code. Running time will vary depending on the number of samples used for comparison, the number of swatches, 
neighborhood size and the size of the images. Most images can be colorized reasonably well in under a 
minute. 4. Conclusions In this paper we have formulated a new, general, fast, and user­friendly approach 
to the problem of colorizing greyscale images. While standard methods accomplish this task by assigning 
pixel colors via a global color palette, our technique empowers the user to first select a suitable color 
image and then transfer the color mood of this image to the greylevel image at hand. We have inten­tionally 
kept the basic technique simple and general by not requir­ing registration between the images or incorporating 
spatial information. Our technique can be made applicable to a larger class of images by adding a small 
amount of user guidance. In this mode, the user first transfers the desired color moods from a set of 
specified swatch regions in the color image to a set of correspond­ing swatch regions in the greyscale 
image. Then, in the second and final stage of the colorization process, the colorized swatches are employed, 
using a texture synthesis-like method, to colorize the remaining pixels in the greyscale image. Currently, 
the L2 distance is used to measure texture similarity within the image. In the future we believe the 
technique can be substantially improved by using a more sophisticated measure of texture similarity. 
Our technique of employing an example color image to col­orize a greylevel image is particularly attractive 
in light of the growing sophistication of internet image search engines and the emergence of centralized 
and indexable image collections which can be used to easily locate suitable color images. Finally, one 
could also utilize a database of basis texture swatches for the initial color transfer in the user-guided 
stage of the colorization process. Acknowledgements This work was supported by NSF Career Grant ACI-0093157. 
   References EFROS,A.A.AND FREEMAN, W. T., 2001. Image Quilting for Texture Synthesis and Transfer, 
In Proceedings of ACM SIGGRAPH 2001, 341-346. EFROS,A.A.AND LEUNG, T. K., 1999. Texture Synthesis by 
Non-parametric sam­ pling. 7th IEEE International Conference on Computer Vision, 1999, 1033-1038. GONZALEZ,R.C. 
AND WINTZ, P., 1987. Digital Image Processing, Addison-Wesley Publishing, Reading MA. HERTZMANN,A., JACOBS,C., 
OLIVER,N., CURLESS,B., SALESIN., D., 2001. Image Analogies, In Proceedings of ACM SIGGRAPH 2002, 341-346. 
SILBERG, J., 1998. Cinesite Press Article, http://www.cinesite.com/core/press/articles/ 1998/10_00_98-team.html 
IOANNIS, P., 1991. Digital Image Processing Algorithms, Prentis Hall, 58. PRATT, W. K. 1991. Digital 
Image Processing, John Wiley &#38; Sons, 311. REINHARD,E.ASHIKHMIN,M., GOOCH B. AND SHIRLEY, P., 2001. 
Color Transfer between Images, IEEE Computer Graphics and Applications, September/October 2001, 34-40. 
ROGOWITZ,B.E. AND KALVIN, A. D., 2001. The Which Blair Project : A Quick Visual Method for Evaluating 
Perceptual Color Maps, Proceedings of IEEE Visu­ alization 2001. RUDERMAN,D. L., CRONIN,T.W. AND CHIAO, 
C. C., 1998. Statistics of Cone Responses to Natural Images: Implications for Visual Coding, J. Optical 
Soc. of America, vol 15, no. 8, 2036-2045. (a) Foliage image mapped using a swatch for the bark and the 
leaves. Source image courtesy of Adam +  = Superchi. Target image courtesy of http:// philip.greenspun.com. 
 (b) The results of col­orizing a photograph of a face. Four swatches were used for the hair, skin, shirt 
and back­ +  = ground with a 11x11 neighborhood size for the L2 metric. Images courtesy of Lela. (c) 
The results of col­orizing an Ansel Adam s photograph. A total of 3 swatches were used. Source image 
 +  = courtesy of Paul Kien­itz. (d) A Landsat 7 satel­lite image (converted to greyscale) was col­orized 
with another Landsat satellite image +  = using the global match­ing procedure. (e) One slice of a 
col­orized MRI volume. We used a color cryo­section from the Visi­ble Human Project +  = dataset and 
two swatches. By using swatches, we could avoid transferring the colorofthe blue gel. (f) A scanning 
elec­tron microscopy (SEM) image colorized with a photograph of an ant using the global matching procedure. 
 + =  Figure 3: The results of applying the algorithm to photographs and scientific datasets. The first 
column contains the original color image, the second column contains the target greyscale image and the 
final column is the result of the colorization procedure. Note, the images have been scaled to fit the 
page. Swatches have been included as colored rectangles on the source and target images. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566577</section_id>
		<sort_key>281</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling and simulation]]></section_title>
		<section_page_from>281</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P382462</person_id>
				<author_profile_id><![CDATA[81100558762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Fedwik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566578</article_id>
		<sort_key>281</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[CHARMS]]></title>
		<subtitle><![CDATA[a simple framework for adaptive simulation]]></subtitle>
		<page_from>281</page_from>
		<page_to>290</page_to>
		<doi_number>10.1145/566570.566578</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566578</url>
		<abstract>
			<par><![CDATA[Finite element solvers are a basic component of simulation applications; they are common in computer graphics, engineering, and medical simulations. Although <i>adaptive</i> solvers can be of great value in reducing the often high computational cost of simulations they are not employed broadly. Indeed, building adaptive solvers can be a daunting task especially for 3D finite elements. In this paper we are introducing a new approach to produce <i>conforming, hierarchical, adaptive refinement methods</i> (CHARMS). The basic principle of our approach is to refine basis functions, not elements. This removes a number of implementation headaches associated with other approaches and is a general technique independent of domain dimension (here 2D and 3D), element type (e.g., triangle, quad, tetrahedron, hexahedron), and basis function order (piece-wise linear, higher order B-splines, Loop subdivision, etc.). The (un-)refinement algorithms are simple and require little in terms of data structure support. We demonstrate the versatility of our new approach through 2D and 3D examples, including medical applications and thin-shell animations.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[adaptive computation]]></kw>
			<kw><![CDATA[basis function]]></kw>
			<kw><![CDATA[multiresolution]]></kw>
			<kw><![CDATA[refinement relation]]></kw>
			<kw><![CDATA[subdivision]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor>Finite element methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Engineering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor>Multigrid and multilevel methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003718</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations in finite fields</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003729</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Partial differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40026357</person_id>
				<author_profile_id><![CDATA[81320489894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eitan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grinspun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382457</person_id>
				<author_profile_id><![CDATA[81100427343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Petr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krysl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCSD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>154090</ref_obj_id>
				<ref_obj_pid>154074</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALPERT, B. K. 1993. A Class of Bases in L2 for the Sparse Representation of Integral Operators. SIAM Journal on Mathematical Analysis 24, 246-262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>587304</ref_obj_id>
				<ref_obj_pid>587158</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ARNOLD, D. N., MUKHERJEE, A., AND POULY, L. 2001. Locally Adapted Tetrahedra Meshes using Bisection. SIAM Journal on Scientific Computing 22, 2, 431-448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[AZAR, F. S., METAXAS, D. N., AND SCHNALL, M. D. 2001. A Deformable Finite Element Model of the Breast for Predicting Mechanical Deformations under External Perturbations. Academic Radiology 8, 10, 965-975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., WARREN, J., AND XU, G. 2002. A Smooth Subdivision Scheme for Hexahedral Meshes. The Visual Computer. to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BANK, R., AND XU, J. 1996. An Algorithm for Coarsening Unstructured Meshes. Numerische Mathematik 73, 1, 1-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BEY, J. 2000. Simiplicial Grid Refinement: On Freudenthal's Algorithm and the Optimal Number of Congruence Classes. Numerische Mathematik 85, 1-29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CATMULL, E., AND CLARK, J. 1978. Recursively generated B-spline surfaces on arbitrary topological meshes. CAD 1O, 6, 350-355.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CELNIKER, G., AND GOSSARD, D. 1991. Deformable Curve and Surface Finite Elements for Free-Form Shape Design. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4, 257-266]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CIRAK, F., AND ORTIZ, M. 2001. Fully c1-conforming subdivision elements for finite deformation thin-shell analysis. IJNME 51, 7, 813-833.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[CIRAK, F., ORTIZ, M., AND SCHR&#214;DER, P. 2000. Subdivision surfaces: A new paradigm for thin-shell finite-element analysis. IJNME 47, 12, 2039-2072.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>372589</ref_obj_id>
				<ref_obj_pid>372584</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[COHEN, A., DEVORE, R., AND DAHMEN, W. 2001. Adaptive Wavelet Methods for Elliptic Operator Equations: Convergence Rates. Mathematics of Computation 70, 233, 27-75.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383262</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[DEBUNNE, G., DESBRUN, M., CANI, M.-P., AND BARR, A. H. 2001. Dynamic Real-Time Deformations Using Space & Time Adaptive Sampling. Proceedings of SIGGRAPH 2001, 31-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[DESLAURIERS, G., AND DUBUC, S. 1989. Symmetric iterative interpolation processes. Constructive Approximation 5, 1, 49-68.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[DOO, D., AND SABIN, M. 1978. Analysis of the behaviour of recursive division surfaces near extraordinary points. CAD 10, 6, 356-360.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[DYN, N., LEVIN, D., AND GREGORY, J. A. 1990. A Butterfly Subdivision Scheme for Surface Interpolation with Tension Control. ACM TOG 9, 2 (April), 160-169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378512</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[FORSEY, D. R., AND BARTELS, R. H. 1988. Hierarchical B-Spline Refinement. Computer Graphics (Proceedings of SIGGRAPH 88) 22, 4, 205-212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND FEDKIW, R. 2001. Practical Animation of Liquids. Proceedings of SIGGRAPH 2001, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199410</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., AND COHEN, M. F. 1995. Hierarchical and Variational Geometric Modeling with Wavelets. 1995 Symposium on Interactive 3D Graphics, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166146</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., SCHR&#214;DER, P., COHEN, M. F., AND HANRAHAN, P. 1993. Wavelet Radiosity. Proceedings of SIGGRAPH 93, 221-230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74335</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[GOURRET, J.-P., THALMANN, N. M., AND THALMANN, D. 1989. Simulation of Object and Human Skin Deformations in a Grasping Task. Computer Graphics (Proceedings of SIGGRAPH 89) 23, 3, 21-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[HALSTEAD, M., KASS, M., AND DEROSE, T. 1993. Efficient, Fair Interpolation Using Catmull-Clark Surfaces. Proceedings of SIGGRAPH 93, 35-44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. 1996. Progressive Meshes. Proceedings of SIGGRAPH 96, 99-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>350448</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[HOUSE, D. H., AND BREEN, D. E., Eds. 2000. Coth Modeling and Animation. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[JOHNSON, C. 2001. Adaptive finite element and local regularization methods for the inverse ECG problem. In Inverse Problems in Electrocardiology, WIT Press, P. Johnston, Ed.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[KAGAN, P., AND FISCHER, A. 2000. Integrated mechanically based CAE system using B-Spline finite elements. CAD 32, 8-9, 539-552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344835</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L. 2000. &radic;3 Subdivision. Proceedings of SIGGRAPH 2000, 103-112.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[KOBER, C., AND MULLER-HANNEMANN, M. 2000. Hexahedral Mesh Generation for the Simulation of the Human Mandible. In Proceedings of the 9th International Meshing Roundtable, Sandia National Laboratories, 423-434.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[KRAFT, R. 1997. Adaptive and linearly independent multilevel B-splines. In Surface Fitting and Multiresolution Methods, A. L. M&#233;haut&#233;, C. Rabut, and L. L. Schumaker, Eds., vol. 2. Vanderbilt University Press, 209-218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[KRYSL, P., GRINSPUN, E., AND SCHR&#214;DER, P. 2002. Natural hierarchical refinement for finite element methods. To appear; IJNME, http://hogwarts.ucsd.edu/~pkrysl/pubs/adref.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>862169</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[LANGTANGEN, H. P. 1999. Computational Partial Differential Equations. Springer Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614376</ref_obj_id>
				<ref_obj_pid>614267</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[LEE, S., WOLBERG, G., AND SHIN, S. Y. 1997. Scattered Data Interpolation with Multilevel B-Splines. IEEE TVCG 3, 3, 228-244.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[LOOP, C. 1987. Smooth Subdivision Surfaces Based on Triangles. Master's thesis, University of Utah, Department of Mathematics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. 1997. Multiresolution analysis for surfaces of arbitrary topological type. ACM TOG 16, 1, 34-73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267106</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[MANDAL, C., QIN, H., AND VEMURI, B. C. 1997. Dynamic Smooth Subdivision Surfaces for Data Visualization. In IEEE Visualization '97, 371-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134085</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[METAXAS, D., AND TERZOPOULOS, D. 1992. Dynamic deformation of solid primitives with constraints. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2, 309-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical Modeling and Animation of Brittle Fracture. In Proceedings of SIGGRAPH 99, 137-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[RIVARA, M., AND INOSTROZA, P. 1997. Using Longest-side Bisection Techniques for the Automatic Refinement of Delaunay Triangulations. IJNME 40, 4, 581-597.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2246205</ref_obj_id>
				<ref_obj_pid>2246196</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[STAM, J. 2001. On Subdivision Schemes Generalizing Uniform B-Spline Surfaces of Arbitrary Degree. CAGD 18, 5 (June), 383-396.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[STRANG, G., AND FIX, G. 1973. An Analysis of the Finite Element Method. Wellesley-Cambridge Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[STRANG, G., AND NGUYEN, T. 1996. Wavelets and Filter Banks. Wellesley-Cambridge Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319691</ref_obj_id>
				<ref_obj_pid>2318971</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[STRELA, V., HELLER, P. N., STRANG, G., TOPIWALA, P., AND HEIL, C. 1999. The Application of Multiwavelet Filterbanks to Image Processing. IEEE TIP 8, 4, 548-563.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>176580</ref_obj_id>
				<ref_obj_pid>176579</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND QIN, H. 1994. Dynamic NURBS with Geometric Constraints for Interactive Sculpting. ACM TOG 13, 2, 103-136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 1987. Elastically Deformable Models. Computer Graphics (Proceedings of SIGGRAPH 87) 21, 4, 205-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[TIMOSHENKO, S., AND WOINOWSKY-KRIEGER, S. 1959. Theory of Plates and Shells. McGraw-Hill Book Company Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2246206</ref_obj_id>
				<ref_obj_pid>2246196</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[VELHO, L., AND ZORIN, D. 2001. 4-8 Subdivision. CAGD 18, 5 (June), 397-427.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>370388</ref_obj_id>
				<ref_obj_pid>370049</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[WARFIELD, S. K., FERRANT, M., GALLEZ, X., NABAVl, A., JOLESZ, F. A., AND KIKINIS, R. 2000. Real-time biomechanical simulation of volumetric brain deformation for image guided neurosurgery. In SC 2000: High performance networking and computing conference, vol. 230, 1-16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[WELCH, W., AND WITKIN, A. 1992. Variational Surface Modeling. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2, 157-166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[WU, X., DOWNES, M. S., GOKTEKIN, T., AND TENDICK, F. 2001. Adaptive Nonlinear Finite Elements for Deformable Body Simulation Using Dynamic Progressive Meshes. Computer Graphics Forum 20, 3, 349-358.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>11169</ref_obj_id>
				<ref_obj_pid>11166</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[YSERENTANT, H. 1986. On the multilevel splitting of finite-element spaces. Numerische Mathematik 49, 4, 379-412.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[ZIENKIEWICZ, O. C., AND TAYLOR, R. L. 2000. The finite element method: The basis, 5 ed., vol. 1. Butterworth and Heinemann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., AND SCHR&#214;DER, P., Eds. 2000. Subdivision for Modeling and Animation. Course Notes. ACM Siggraph.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2246207</ref_obj_id>
				<ref_obj_pid>2246196</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., AND SCHR&#214;DER, P. 2001. A Unified Framework for Primal/Dual Quadrilateral Subdivision Schemes. CAGD 18, 5, 429-454.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SCHR&#214;DER, P., AND SWELDENS, W. 1996. Interpolating Subdivision for Meshes with Arbitrary Topology. Proceedings of SIGGRAPH 96, 189-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>891574</ref_obj_id>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D. 2000. Smoothness of Subdivision on Irregular Meshes. Constructive Approximation 16, 3, 359-397.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CHARMS: A Simple Framework for Adaptive Simulation Eitan Grinspun Petr Krysl Peter Schr¨oder Caltech 
UCSD Caltech Abstract Finite element solvers are a basic component of simulation appli­cations; they 
are common in computer graphics, engineering, and medical simulations. Although adaptive solvers can 
be of great value in reducing the often high computational cost of simulations they are not employed 
broadly. Indeed, building adaptive solvers can be a daunting task especially for 3D .nite elements. In 
this paper we are introducing a new approach to produce conforming, hierarchical, adaptive re.nement 
methods (CHARMS). The basic principle of our approach is to re.ne basis functions, not elements. This 
removes a number of implementation headaches associated with other approaches and is a general technique 
independent of domain dimension (here 2D and 3D), element type (e.g., triangle, quad, tetrahedron, hexahedron), 
and basis function order (piece­wise linear, higher order B-splines, Loop subdivision, etc.). The (un-)re.nement 
algorithms are simple and require little in terms of data structure support. We demonstrate the versatility 
of our new approach through 2D and 3D examples, including medical applica­tions and thin-shell animations. 
CR Categories: G.1.8 [Partial Differential Equations]: Finite element meth­ods, Multigrid and multilevel 
methods; G.1.2 [Approximation]: Wavelets, Spline and piecewise polynomial approximation, Linear approximation; 
I.3.5 [Computational Geometry and Object Modeling]: Physically based modeling, Splines; I.3.7 [Three-Dimensional 
Graphics and Realism]: Animation; J.2 [PHYSICAL SCIENCES AND ENGINEERING]: Engineering General Terms: 
Algorithms, Design, Experimentation, Performance Additional Keywords: Adaptive Computation, Re.nement 
Relation, Basis Func­tion, Subdivision, Multiresolution 1 Introduction Many applications of computer 
graphics require the modeling of physical phenomena with high visual or numerical accuracy. Ex­amples 
include the simulation of cloth [House and Breen 2000], water [Foster and Fedkiw 2001], human tissue 
[Wu et al. 2001] and engineering artifacts [Kagan and Fischer 2000], among many oth­ ers. Typically the 
underlying formulations require the solution of partial differential equations (PDEs). Such equations 
are also at the base of many geometric modeling [Celniker and Gossard 1991] and optimization problems 
[Lee et al. 1997]. Most often the continuous equations are discretized with the .nite element (FE) or 
.nite differ­ence (FD) method before a (non-)linear solver can be used to com­pute an approximate solution 
to the original problem. For exam­ple, Terzopoulos and coworkers described methods to model many physical 
effects for purposes of realistic animation [1987]. Their Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Figure 1: Examples run within 
the CHARMS framework demon­strating a variety of simulations which bene.t from our general adaptive solver 
framework: surgery simulation, modelling cloth pillows, in.ating a metal-foil balloon. For details see 
Section 4. discretization was mostly based on simple, uniform FD approxima­tions. Later Metaxas and Terzopoulos 
employed FE methods since they are more robust, accurate, and come with more mathematical machinery [1992]. 
For this reason, human tissue simulations have long employed FE methods (e.g., [Gourret et al. 1989; 
Azar et al. 2001] and references therein). For better performance it is highly desirable to construct 
adap­tive discretizations, allocating resources where they can be most pro.tably used. Building such 
adaptive discretizations robustly is generally very dif.cult for FD methods and very little theoretical 
guidance exists. For FE methods many different approaches ex­ist. They all rely on the basic principle 
that the resolution of the domain discretization ( mesh ) should be adjusted based on local error estimators. 
For example, Debunne et al. superimposed tetra­hedral meshes at different resolutions and used heuristic 
interpola­tion operators to transfer quantities between the disparate meshes as required by an error 
criterion [2001]. Empirically this worked well for real-time soft-body deformation, though there exists 
no mathematical analysis of the method. A strategy based on precom­puted progressive meshes (PM) [Hoppe 
1996] was used by Wu et al. [2001] for surface based FE simulations. Since the PM is con­ structed in 
a pre-process it is unclear how well it can help adapt to the online simulation. O Brien and Hodgins 
followed a more tradi­tional approach by splitting tetrahedra in their simulation of brittle fracture 
(mostly to accommodate propagating cracks) [1999]. Such re.nement algorithms and their associated unre.nement 
operators have the advantage that they come with well established theory [Co­ hen et al. 2001] and result 
in nested meshes, and by implication nested approximation spaces. Since the latter is very useful for 
many multi-resolution techniques we have adopted re.nement as our basic strategy. Typical mesh re.nement 
algorithms approach the problem of re­.nement as one of splitting elements in isolation. Unfortunately 
this leads to a lack of compatibility ( cracks ); to deal with this issue one may: (1) snap T-vertices 
to the neighboring edge; (2) use Lagrange multipliers or penalty methods to numerically enforce compatibility; 
or (3) split additional elements through insertion of conforming edges as in red/green triangulations 
or bisection al­gorithms (the technique used by O Brien and Hodgins [1999], for example). Each one of 
these approaches works, but none is ideal. For example, penalty methods lead to stiff equations with 
their asso­ciated numerical problems, while red/green triangulations are very cumbersome to implement 
in 3D because of the many cases in­volved [Bey 2000]. As a result various different, specialized al­gorithms 
exist for different element types such as triangles [Bank and Xu 1996; Rivara and Inostroza 1997], tetrahedra 
[Arnold et al. 2001] and hexahedra [Langtangen 1999]. This lack of a general approach at times coupled 
with daunt­ing implementation complexity (especially in 3D) has no doubt contributed to the fact that 
sophisticated adaptive solvers are not broadly used in computer graphics applications or general engi­neering 
design. The situation is further complicated by the need of many computer graphics applications for higher 
order ( smooth ) elements. For example, [Celniker and Gossard 1991] used higher order FEs for surface 
modeling with physical forces and geometric constraints (see also [Halstead et al. 1993] and [Mandal 
et al. 1997] who used Catmull-Clark subdivision surfaces and [Terzopoulos and Qin 1994] who used NURBS). 
None of these employed adaptiv­ ity in their solvers. In fact, for basis functions such as B-splines 
or those induced by subdivision, elements cannot be re.ned indi­vidually without losing nestedness of 
approximation spaces. For example, Welch and Witkin, who used tensor product cubic B­splines as their 
constrained geometric modeling primitive, encoun­tered this dif.culty [Welch and Witkin 1992]. To enlarge 
their FE solution space they added .ner level basis functions, reminiscent of hierarchical splines [Forsey 
and Bartels 1988], instead of re.n­ ing individual elements. Later, Gortler and Cohen used cubic B­spline 
wavelets to selectively increase the solution space for their constrained variational sculpting environment 
[Gortler and Cohen 1995]. Contributions The use of hierarchical splines and wavelets in a FE solver framework 
are specialized instances of a general class of conforming, hierarchical, adaptive re.nement methods, 
for short CHARMS, which we introduce here. Instead of re.ning elements, CHARMS are based on the re.nement 
of basis functions. From an approximation theory point of view, this is a simple statement, but it has 
a number of very important and highly practical consequences. Our adaptive solver framework requires 
only that the basis func­tions are re.nable. It makes no assumptions as to (1) the dimen­sion of the 
domain; (2) the actual element types, be they triangles, quadrilaterals, tetrahedra, hexahedra, or more 
general domains; (3) the approximation order; and (4) the connectivity of the support of the basis functions. 
The approach is always globally compati­ble without requiring any particular enforcement of this fact. 
Con­sequently, all the usual implementation headaches associated with maintaining compatibility are entirely 
eliminated. What does need to be managed are tesselations of the overlap between basis func­tions, possibly 
living at very different levels of the re.nement hi­erarchy. However, we will show that very short and 
simple algo­rithms, based on simple invariants, keep track of these interactions in a guaranteed fashion. 
While we were originally motivated by the need to .nd a re.nement strategy for higher order basis functions, 
CHARMS has signi.cant advantages even when only piecewise lin­ear basis functions are used. A single 
programmer implemented and debugged our basic CHARMS algorithm for 1D problems within a day (using an 
ex­isting non-adaptive FE solver). Extending the implementation to 2D and 3D problems took another day. 
This forcefully attests to the simplicity and generality of the underlying framework. We demon­strate 
the versatility of our approach by applying it to several dif­ferent FE simulations, involving both surface 
and volume settings (Fig. 1). 2 Motivation In this section we consider a very simple example to elucidate 
the difference between .nite element and basis function re.nement, before we describe the general (un-)re.nement 
algorithms in Sec­tion 3. To serve our exposition we will examine a boundary value problem. Initial value 
problems, which unfold in time from given initial conditions, are very common in animation applications, 
and are also accommodated by our framework. 2.1 Piecewise Linear Approximation in 1D As a canonical example 
of a second order boundary value problem consider Laplace s equation with prescribed displacements at 
the boundary -.u(x)=0 ,u|.O =1 ,x . O . Rd . (1) A FE method typically solves the weak form of this equation, 
se­lecting from the trial space the solution U which satis.es a(U, v)=VU ·Vv dx =0 O for all v in some 
test space. We write the solution U = g + u as a sum of the function g that satis.es the inhomogeneous 
essential boundary condition, and of the trial function u that satis.es the ho­mogeneous boundary condition 
u|.O =0. In the Galerkin method, which we adopt in this discussion, these test and trial spaces coin­cide. 
Since the bilinear form a(·, ·) contains only .rst derivatives, we may approximate the solution using 
piecewise linear basis functions for both spaces. The domain is discretized into a disjoint union of 
elements of .nite extent. Each such element has an associated linear function (e.g., Fig. 2 for d =1). 
This results in a linear system Ku = b where the stiffness matrix entries kij describe the interaction 
of de­grees of freedom (DOFs) at vertex i and j under the action of a(·, ·); the right hand side b incorporates 
the inhomogeneous boundary conditions; and u is the unknown vector of DOFs. Up to now we have made no 
reference to the number of spa­tial dimensions. We turn our attention to the 1D case (d=1), and consider 
afterwards d> 1. We shall discuss the discretization from two perspectives, which we will refer to as 
the (.nite) element point of view and the basis (function) point of view respectively (Fig. 2). Finite 
Elements In the element point of view, the approxima­tion function is described by its restriction onto 
each element. Basis Functions In the basis point of view, the approximation function is chosen from the 
space spanned by the basis functions. We now consider adaptive re.nement and observe that the strate­gies 
suggested by the alternative points of view are quite different. Element Re.nement In the most simple 
scheme, we bisect an element to re.ne, and merge a pair of elements to unre.ne. In bi­secting an element, 
the linear function over the element is replaced by a piecewise linear function comprised of linear segments 
over the left and right subelements. The solution remains unchanged if the introduced node is the mean 
of its neighbors. This style of re­.nement is very attractive since it is entirely local: each element 
can be processed independently of its neighbors (Fig. 3, left). (right) points of view using linear 
B-splines. In the element point of view, the solution is described over each element as a linear function 
interpolating the function values at the endpoints of the element. In the basis point of view, the solution 
is written as a lin­ear combination of the linear B-spline functions associated with the mesh nodes. 
Basis Re.nement Alternatively, we may reduce the error by enlarging the approximation space with additional 
basis functions. To re.ne, we augment the approximation space with .ner (more localized spatially) functions; 
conversely to unre.ne we eliminate the introduced functions. One possibility is to add a dilated basis 
function in the middle of an element to affect the exact same change as in the element bisection approach 
(Fig. 3, middle). The solution remains unchanged if the coef.cient of the introduced function is zero. 
We refer to such detail or odd coef.cients in deliberate anal­ogy with the use of these terms in the 
subdivision literature [Zorin and Schr¨oder 2000]. Bases constructed in this fashion are exactly the 
classical hierarchical bases of the FE literature [Yserentant 1986]. Note that in this setup there may 
be entries in the stiffness matrix corresponding to basis functions with quite different re.ne­ment levels 
j. Figure 3: Comparison of element re.nement (left), and basis re.ne­ment in hierarchical (middle) or 
quasi-hierarchical fashion (right). For linear B-splines, each hierarchical introduction of a .ner odd 
basis function (middle) affects the same change as element bisec­tion (left). Quasi-hierarchical re.nement 
(right) uses the re.nement relation between the coarse basis function and its dilates. Alternatively 
we may take advantage of the fact that the hat func­tion observes a re.nement relation: it can be written 
as the sum of three dilated hats (Fig. 3, right). We may replace one of the ba­sis functions by three 
dilated versions. Once again with appropri­ately chosen coef.cients the solution is unaltered. To distinguish 
this approach we will later refer to it as quasi-hierarchical (Fig. 3, right). Here too we will have 
entries in the stiffness matrix which correspond to basis functions from different levels. In practice 
the disparity between levels will not be high since coarser functions are entirely replaced by .ner functions, 
not just augmented, as in the hierarchical basis re.nement method. To further illustrate this, consider 
uniformly re.ning the original coarse-level basis: the re­sulting hierarchical basis consists of both 
coarse-and .ner-level functions, whereas the corresponding quasi-hierarchical basis con­sists entirely 
of .ner-level functions, i.e., it has a .at, not hierarchi­cal structure. 2.2 Higher Order Approximation 
in 1D Because piecewise linear functions were suf.cient for the dis­cretization of the weak form of Laplace 
s equations, we have seen very few differences between the element and basis points of view, except when 
considering adaptive re.nement strategies. This will now change as we consider a fourth order elliptic 
problem, the bi­laplacian with prescribed displacements and normal derivatives at the boundary, .u .2 
u(x)=0 ,u|.O =1 , |.O =0 ,x . O . Rd . (2) .n This kind of functional is often used in geometric modeling 
ap­plications (e.g., [Gortler and Cohen 1995] and references therein). Its weak form involves second 
derivatives, favoring the use of basis functions which are C1 (more precisely, they must be in H2 [Strang 
and Fix 1973]). One of the advantages of the element point of view was that each element could be considered 
in isolation from its neighbors. To maintain this property and satisfy the C1 condition a natural approach 
is to raise the order of the local polynomial over each el­ement. The natural choice that maintains symmetry 
is the Hermite cubic interpolant (Fig. 4). Two DOFs, displacement and derivative, are now associated 
with each vertex. Note that in using Hermite interpolants the dimension of our solution space has doubled 
and non-displacement DOFs were introduced these are quite unnatu­ral in applications which care about 
displacements, not derivatives. As an alternative basis we can use quadratic B-splines (Fig. 4). They 
satisfy the C1 requirement, require only displacement DOFs, and lead to smaller linear systems. Perhaps 
most importantly, we will see in Section 2.4 that in the bivariate, arbitrary topology set­ ting, Hermite 
interpolation becomes considerably more cumber­some, while generalizations of B-splines such as subdivision 
meth­ods continue to work with no dif.culties. We again compare the two perspectives for adaptive re.nement, 
and learn that basis re.nement applies where element re.nement does not. Element Re.nement Using Hermite 
cubic splines it is easy to re.ne a given element through bisection. A new vertex with associ­ated value 
and derivative coef.cients is introduced in the middle of the element and the single cubic over the parent 
element becomes a pair of C1 cubics over the two child elements. This re.nement can be performed without 
regard to neighboring elements. For quadratic (and higher order) B-splines re.nement of an ele­ment in 
isolation, i.e., without regard to its neighbors, is not pos­sible. The reason for this is that a given 
B-spline of degree two or higher overlaps more than two elements. Figure 4: Basis functions of cubic 
Hermites (left) and quadratic B­splines (right) give rise to C1 approximations. The Hermite ba­sis functions 
are centered at nodes and supported over adjacent elements hence allow either element or basis re.nement, 
but they require non-displacement DOFs and do not easily generalize to higher dimensions. The B-spline 
basis functions have larger sup­port hence allow only basis re.nement. Basis Re.nement Hermite basis 
functions admit basis re.ne­ment and the picture is substantially the same as in the hat function case. 
Quadratic (and higher order) B-splines, which do not admit isolated element re.nement, do admit basis 
re.nement since they all observe a re.nement relation. 2.3 Piecewise Linear Approximation in 2D In the 
2D setting, we .nd new differences between the element and basis perspectives that were not apparent 
in 1D. Again we may ap­proximate the solution to Laplace s equation (1) using a piecewise linear, i.e., 
C0 function, but this time over a triangulation of the do­main. The DOFs live at the vertices and de.ne 
a linear interpolant over each triangle. As before, we view the discretization alternating between the 
element and basis points of view. The element point of view de.nes u(x) by its restriction over each 
element, whereas the basis function point of view de.nes u(x) as a linear combination of basis functions, 
each of which spans several elements. Once more we compare the two perspectives for adaptive re.ne­ment, 
and shed new light on the simplicity of basis re.nement: Element Re.nement One possibility is to quadrisect 
only those triangles that are too big. A new problem appears that did not reveal itself in the 1D setting: 
this approach produces a mesh with T-vertices, i.e., incompatibly placed nodes (Fig. 5). Such nodes are 
problematic since they introduce discontinuities. Intro­duction of conforming edges ( red/green triangulations) 
can .x these incompatibilities [Bey 2000]. Alternatively one may use bi­ section of the longest edge 
instead of quadrisection [Rivara and In­ ostroza 1997]. This approach is limited to simplices only and 
be­ comes cumbersome in higher dimensions [Arnold et al. 2001]. Figure 5: Re.nement of an element in 
isolation produces T-vertices, or incompatibilities with adjacent elements. In the case of 2D trian­gulations 
(left) incompatibilities may be addressed by introducing conforming edges. In other settings, e.g., quadrilateral 
meshes, 3D tetrahedral meshes or hexahedral meshes (right), the analogy to insertion of conforming edges 
is more involved. Basis re.nement never leads to such incompatibilities. Basis Re.nement Alternatively, 
we may augment the approxi­mation space with .ner, more compactly supported functions. Con­sider re.ning 
the original mesh globally via triangle quadrisection, which preserves all the existing vertices and 
introduces new vertices on the edge midpoints. Every node in this .ner mesh associates to a (.ner) nodal 
basis function supported by its (.ner) incident tri­angles. We may now augment our original approximation 
space (induced by the coarser triangulation) with any of the nodal basis functions of the .ner mesh. 
As such, the result is simply an ex­panded linear combination with additional functions. With this ap­proach 
compatibility is automatic; we don t deal with problematic T-vertices. If we augment the current basis 
with only odd .ner basis func­tions, we generate a hierarchical basis. If instead we replace a coarser 
function with all .ner (even and odd) functions of its re­.nement relation, we generate a quasi-hierarchical 
basis. 2.4 Higher Order Approximation in 2D Consider the bilaplacian (2) in 2D; this kind of functional 
appears in thin-plate and thin-shell problems. As in Section 2.2 its discretiza­ tion is built of C1 
basis functions; here the element point of view has a serious handicap. Building polynomials over each 
element and requiring that they match up globally with C1 continuity leads to high order and cumbersome 
Hermite interpolation problems. On the other hand, constructing basis functions over arbitrary triangu­lations 
using, for example, Loop s [1987] subdivision scheme is quite easy and well understood. Such basis functions 
are supported on more than a 1-ring of triangles. Consequently, locally re.ning the triangulation induces 
a new function space which does not in general span (a superset of) the original space. In the basis 
point of view, the original space is augmented, thus the original span is preserved. Summary and Preview 
Element re.nement becomes more cumbersome or even impossible as the number of dimensions or approximation 
order is increased. In contrast, basis re.nement ap­plies uniformly to any re.nable function space. What 
are needed in the basis re.nement strategy are ef.cient data structures and algorithms to (1) keep track 
of non-zero entries in the stiffness matrices and (2) manage a tesselation of the domain suitable for 
evaluation of the associated integrals. In traditional, piecewise-linear elements, non-zero entries in 
the stiffness matrix are trivially identi.ed with the edges of the FE mesh. When using higher order B-splines 
or subdivision basis func­tions their enlarged support implies that there are further interac­tions, 
which must be identi.ed and managed. Additionally, interac­tions induced between active members of the 
re.nement hierarchy lead to inter-level interactions. Similarly, for numerical integra­tion, the cells 
of the FE mesh are a suitable tesselation when using piecewise linear elements, while for the basis re.nement 
methods suitable tesselations must be explicitly constructed. Some of these issues were confronted by 
earlier researchers who wished to enrich cubic B-spline tensor product surfaces with .ner functions in 
selected regions. This was done by enforcing buffer regions of control points which were not allowed 
to move [Forsey and Bartels 1988; Welch and Witkin 1992] or through explicit wavelets which were resolved 
into B-splines based on the re.ne­ment relation [Gortler and Cohen 1995]. These earlier approaches are 
specialized instances of CHARMS and we now proceed to the general algorithms which rely solely on re.nability. 
  3 Algorithms To establish a context for our discussion, we put down a framework that might be used 
in an animation application to adaptively solve a nonlinear initial value problem using basis re.nement: 
IntegratePDE 1 While t<tend 2 predict: measure error and construct sets B+ and B- 3 adapt: 4 B := B.B+\B- 
5 maintain basis: remove redundant functions from B 6 solve: Rt(ut)= 0 7 t := t +.t Each simulation step 
has three stages: predict, adapt and solve. First, an oracle predicts which regions of the domain require 
more (resp. less) resolution, and constructs a set of basis functions to be introduced to (resp. removed 
from) the approximation space (line 2). Next, the approximation space is adapted: the set of active basis 
functions is updated (line 4), functions redundant to the basis are removed (line 5). The removal of 
redundant functions ensures that the set B is linearly independent; in certain settings this is impor­tant 
for numerical stability, in others this step may be skipped. The solution at time t is found by solving 
a system of linear or nonlinear equations (line 6). For a nonlinear system Rt(·) we linearize and solve 
with Newton s method; therefore, the Jacobian matrix Kt and the load term bt need to be assembled. Note 
that the structure of Kt depends on which basis functions are active. The framework above is one of many 
that could be adopted; all will have an adaptation stage, and our discussion focuses on laying out de.nitions 
and then algorithms for managing the data structures which represent the approximation space B and quantities 
depend­ing on B, e.g., K.  3.1 Re.nable Functions In order to formally describe our algorithms we need 
to .x a num­ ber of ideas, foremost amongst them the notion of a re.nable func­ tion. Traditionally, 
the theory of such functions (e.g., Strang and Nguyen [1996]) is pursued in the regular Euclidian setting, 
i.e., as functions from Rd to R, coupled with a regular tesselation. In this case functions are linear 
combinations of their own dilates. Exam­ ples from 1D include B-splines and Deslauriers-Dubuc interpolat­ 
ing functions [1989]. In contrast, we are interested in a more gen­ eral formulation: consider arbitrary 
topology surfaces and subsets of R3; in both settings the domain will in general not admit regular tesselations. 
We need a broader context: the theory and algorithms of subdivision provide such a framework [Lounsbery 
et al. 1997; Zorin 2000; Zorin and Schr¨oder 2000]. In this case the .ner level functions are not all 
strict dilates of a coarser level function, but the subdivision stencils still supply the basic ingredients 
for the re.ne­ ment relation we need. Another setting of importance to us is that of elements with locally 
supported polynomials up to some desired order. The elements may be split into .ner elements each carry­ 
ing dilations of the coarser polynomials. The associated theory is that of multi-scaling function re.nement 
[Strela et al. 1999; Alpert 1993]. Such bases have been used with great success in wavelet radiosity 
simulations [Gortler et al. 1993]. Our algorithms will cover both cases. To simplify the exposition we 
use surface subdivision as the canonical example, but will en­sure that more general approaches such 
as volume subdivision, and multi-scaling functions de.ned relative to elements, are captured as well. 
 3.2 Building Blocks For our purposes a mesh consists of sets of topological entities together with 
the usual incidence relations: vertices, V = {vi}; edges, E = {ej }; faces, F = {fk}; and (in 3D) cells, 
C = {cl}. We assume that the incidence relations de.ne a manifold (with boundary). Typical examples include 
triangle, quad, tetrahedra, and hexahedra meshes. The term element refers to faces in the bivariate and 
cells in the trivariate setting. The mesh carries coef.cients associated with basis functions. These 
coef.cients may describe the geometric shape, e.g., (x, y) . R2 or (x, y, z) . R3 or functions de.ned 
over the shape such as displacement, density, force, etc. Coef.cients may live at any of the topological 
quantities. Most of the time coef.cients will be as­sociated with vertices; some schemes have coef.cients 
associated with elements. Similarly, polynomials over individual elements will often result in coef.cients 
associated with elements. A topological re.nement operator describes how topological en­tities are split 
and a .ner mesh constructed with them. In develop­ing our theory, we consider global re.nement (all entities 
are split); in practice we implement adaptive re.nement as lazy evaluation of a conceptually global and 
in.nite re.nement hierarchy. Most topo­logical re.nement operators split elements or vertices (Fig. 6). 
Less typical (but accommodated here) are 4-8 [Velho and Zorin 2001] v and 3 [Kobbelt 2000] schemes. A 
coef.cient re.nement operator associated with a given topo­logical re.nement operator describes how the 
coef.cients from the coarser mesh are used to compute coef.cients of the .ner mesh. Figure 6: Examples 
of topological re.nement operators: quadri­section for quadrilaterals and triangles. We assume that these 
operators are linear, .nitely supported, of lo­cal de.nition, and depend only on connectivity. A subdivision 
scheme is a pairing of topological-and coef.cient­re.nement operators. Examples of common subdivision 
schemes include linear splines over triangles or tetrahedra; bilinear or trilin­ear tensor product splines 
over quadrilaterals and hexahedra; Doo-Sabin [1978], Catmull-Clark [1978] and their higher order [Zorin 
and Schr¨oder 2001; Stam 2001] and 3D [Bajaj et al. 2002] gen­ eralizations; Loop [1987], Butter.y [Dyn 
et al. 1990; Zorin et al. v 1996], and 3 [Kobbelt 2000] schemes for triangles. In the case of primal 
subdivision schemes, i.e., those with coef.cients at vertices and splitting of faces/cells as their topological 
re.nement opera­tor, we distinguish between even and odd coef.cients. The former correspond to vertices 
that the .ner mesh inherits from the coarser mesh, while the latter correspond to newly created vertices. 
A basis function is the limit of repeated subdivision beginning with a single coef.cient set to unity 
and all others set to zero. In this way a basis function is associated in a natural way with each entity 
carrying a coef.cient, such as vertices in the case of lin­ear splines (both triangles and tetrahedra) 
or Loop s scheme, and faces in schemes such as Doo-Sabin1 or Alpert s multi-scaling func­tions [1993]. 
A re.nement relation is observed by all functions de.ned through subdivision. It states that a basis 
function from a coarser level can be written as a linear combination of basis functions from the next 
.ner level (j)(j+1) (j+1) f(x)=af(x) (3) i ikk k where j indicates the level of re.nement (j =0 corresponding 
to the original, coarsest mesh), and i, respectively k index the basis (j+1) functions at a given level. 
The coef.cients aik can be found by starting with a single 1 at position i on level j, applying a single 
subdivision step and reading off all non-zero coef.cients. Note that (j+1) the aik generally depend on 
i, but for stationary schemes they do not depend on j. Since we assume that the subdivision scheme is 
(j+1) .nitely supported only a .nite number of aik will be non-zero. In the case of multi-scaling functions 
we will have matrix valued (j+1) aik . The children of a basis function are given by (j)(j+1) (j+1) C(f)= 
{f |a=0}, i kik while the parents follow from the adjoint relation (j)(j-1) (j)(j-1) C (f)= {f|f.C(f)}. 
i kik The natural support set, S(fi (j)), of a basis function is the minimal set of elements at level 
j, which contain the parametric support of the basis function. For example, linear splines, fi (j) are 
supported on the triangles (tetrahedra) incident to vi at mesh re.nement level j; a Loop basis function 
has the 2-ring of triangles surrounding 1This is not the usual way Doo-Sabin (or other dual schemes) 
are de­scribed, but our description can be mapped to the standard view by dualiz­ing the mesh [Zorin 
and Schr¨oder 2001]. From a FE point of view this turns out to be more natural as it ensures that elements 
from .ner levels are strict subsets of elements from coarser levels. the given vertex as its natural 
support set (Fig. 7); and a Doo-Sabin basis function, which is centered at an element in our dualized 
view, has a natural support set containing the element and all elements that share an edge or vertex 
with it. The adjoint, S (ejl ), returns the set of basis functions whose natural support contains the 
element elj . The descendants of an element, D(eij ), are all elements at levels >j which have non-zero 
intersection (in the parametric domain) with the given element. The ancestor relation is de.ned through 
the adjoint, D (eij ). Figure 7: Examples of natural support sets. Left to right: linear splines, Loop 
basis, bilinear spline, and Catmull-Clark basis. 3.3 Putting It All Together Synopsis of Theory Given 
a coarsest-level mesh, consider the in.nite sequence of meshes generated by some subdivision scheme. 
Coef.cient i on mesh level j associates to the basis func­tion fi (j)(x). To describe the current approximation 
space, we choose a .nite subset B..i,j f(ij)(x) of all available basis func­tions; B is the set of active 
basis functions. The span of B must always include the span of the coarsest-level basis .if(0)(x). Re­ 
. i .nement enlarges the space, span Bold. span {Bnew}; unre.ne­ment reduces it. See also our companion 
paper [Krysl et al. 2002]. Figure 8: Illustrative example of the data structures. Shown in bold are 
a pair of active basis functions on mesh levels 0 and 1. The as­ (0) (1) sociated data structures are: 
B = {f0 ,f2 }, E = {e00,e21,e13}, (0) (1) (0) S(f)= {e00,e10}, S(f)= {e21,e13}, Bs(e00)= {f}, 02 0 (1) 
(0) Ba(e00)= Ø, Bs(e21)= {f}, Ba(e21)= {f}, Bs(e31)= 20 (1) (0) {f2 }, Ba(e13)= {f0 }. Data Structures 
Our algorithm maintains B, as well as an asso­ciated set of active integration elements E; recall that 
element refers to a face in 2D and a cell in 3D. The set of active basis functions is useful for error 
indicators, for example, which iterate over currently active basis functions to see which must be re.ned 
and which can be unre.ned (deactivated) (line 2 of IntegratePDE). The set of ac­tive integration cells 
is required to evaluate the weak-form integrals, e.g., to compute stiffness matrix entries. These sets 
are initialized (0) = {e0 as B = {fi } and E i }, i.e., all basis functions (resp. integra­tion cells) 
at the coarsest level. For each integration cell e .E we need to keep track of the active functions whose 
natural support sets overlap it: those from the same level Bs(e) and those from ancestor levels Ba(e) 
(Fig. 8). Initially Ba(e)= Ø and Bs(e)= S (e) for e .E. Algorithms To evaluate the stiffness matrix, 
we need to be able to compute the action of the operator on pairs of basis functions. Traditionally this 
is done by iterating over all active elements, com­puting local interactions and accumulating these into 
the global stiffness matrix K. With the data structures described above, we have all necessary tools 
at hand to effect this computation: ComputeStiffness(E) 1 ForEach e .E do 2 ForEach f . Bs(e) do 3 kff+= 
Integrate(f,f,e) 4 ForEach . . Bs(e) \{f} do 5 kf.+= Integrate(f,.,e) 6 k.f+= Integrate(.,f,e) 7 ForEach 
. . Ba(e) do 8 kf.+= Integrate(f,.,e) 9 k.f+= Integrate(.,f,e) Here we used += (and later .= and \=) 
in C-language fashion to indicate a binary operation with the result assigned to the left hand side. 
ComputeStiffness considers interactions between every pair of overlapping basis functions at the coarsest 
level that captures the interaction: if coarser function fc overlaps .ner function ff , we evaluate the 
bilinear form over cells in the natural support set of ff which also support fc: {e | e .S(ff ) .D (e) 
nS(fc)= Ø}. With this approach every interaction is considered exactly once, at a suf.ciently .ne resolution. 
To implement this approach, we it­erate over each active cell (line 1), and consider only interactions 
between every same-level active function (line 2) and every active function either on the same level 
(lines 3-6) or ancestral level (lines 7-9). For symmetric K appropriate calls to Integrate can be omit­ted. 
In practice, we do not call ComputeStiffness every time the basis B is adapted, rather we make incremental 
modi.cations to K. In some settings it may not be possible or desirable to express the integrand as a 
bilinear form, e.g., explicit dynamics simulators may directly compute non-linear forces. In this case, 
the use of a stiffness matrix is abandoned and integration is carried out over a set of tile cells that 
has the following properties: (a) the set tiles the integration domain and (b) each tile cell is at least 
as .ne as every active cell that overlaps it. Given the set of active cells one can construct this tile 
set from scratch but it is preferable to maintain it incrementally. During the course of the solution 
process, basis functions are (de)activated (lines 4-5 of IntegratePDE) and the data structures described 
above must be updated. When a basis function f is acti­vated B and E as well as Bs and Ba must be updated: 
Activate(f) 1 B.= {f}2 ForEach e .S(f) do 3 (e) .= {f} Bs 4 // upon activation initialize ancestor list 
 5 If e/.E then Ba(e) .= Ancestor(e); E.= {e} fI 6 // add to ancestor lists of active descendants 7 
ForEach . . (D(e) nE) do Ba(.) .= {f} Ancestor(e) 1 . := Ø 2 ForEach . .D (e) nE do 3 . .= Bs(.) . Ba(.) 
4 return . Activate .rst augments the set of active functions (line 1), and then iterates over each cell 
in the natural support set of f (lines 2-7). Since f is active, it belongs in the table of same-level 
active func­tions of every supporting cell (line 3). Furthermore since f is active its supporting cells 
are active (line 5): they are activated (if inactive) by adding them to the set of active cells and initializing 
their table of ancestral active-functions. Note here the call to Ancestor(e), which returns all active 
coarser-level basis-functions whose natural support set overlaps e. Finally, all active descendants of 
the sup­porting cell also support f, hence we update their tables of ancestral active-functions (line 
7). Conversely, to deactivate a basis function f we proceed as fol­lows: Deactivate(f) 1 B\= {f}2 ForEach 
e .S(f) do 3 (e) \= {f} Bs 4 // deactivate element? 5 If Bs(e)= Ø then E\= {e} 6 // update ancestor 
lists of active descendants 7 ForEach . .D(e) nE do Ba(.) \= {f} As before, we .rst update the set of 
active functions (line 1) and then iterate over the supporting cells (lines 2-7). Since f has be­come 
inactive, it is removed from the table of same-level active­functions of every supporting cell e (line 
3) and from the table of ancestral active-functions of every active descendant of e (line 7). Furthermore 
if the supporting cell is left with an empty active­function table then it is deactivated (line 5). Assuming 
that an appropriate error estimator is at hand we can consider a wide variety of adaptive solver strategies 
built on top of Activate. Here we present two re.nement strategies, hierarchi­cal and quasi-hierarchical, 
as applications of Activate and Deacti­vate (there are other attractive strategies, e.g., selectively 
activat­ing individual functions). The re.nement algorithms take some ac­tive basis function f .B, modify 
the basis and update the vec­tor of DOFs u. Similarly, the unre.nement algorithms take some previously-re.ned 
basis function f .B. HierarchicalRe.ne(f) 1 ForEach . .C(f) do 2 If ./.B. Odd(.) then Activate(.); u. 
:= 0 fI HierarchicalUnre.ne(f) 1 ForEach . .C(f) do 2 If ./.B. Odd(.) then Deactivate(.)  QuasiHierarchicalRe.ne(f) 
1 Deactivate(f) 2 ForEach . .C(f) do 3 If ./.B then Activate(.); u. := 0 fI 4 u.+= af,.uf QuasiHierarchicalUnre.ne(f) 
1 Activate(f) ; initialize uf 2 ForEach . .C(f) do 3 If ./.B then Deactivate(.) Here u. is the coef.cient 
associated with ., and af,. is the weight of . in the re.nement relation of f (Eqn. 3). Re.nement is 
loss­ less, whereas unre.nement must be lossy except in the special case that the current approximation 
lies inside the new unre.ned approximation space (as noted in the literature, unre.nement error can be 
hidden using interpolation techniques). Line 1 of Quasi-HierarchicalUnre.ne initializes uf by projecting 
the current ap­proximation into the unre.ned space, e.g., by choosing the uf that (for some given norm 
I ·I) minimizes In certain settings, it is important that the active functions are linearly independent. 
This is the case, for example, in classical FE applications, as a linear dependency in the basis leads 
to a singular stiffness matrix. If only hierarchical re.nement is applied then the active set is always 
a proper basis. If other re.nement strategies are used (e.g., quasi-hierarchical, and selective (de)activation 
of indi­vidual functions) then maintaining a proper basis requires special care. Our companion paper 
[Krysl et al. 2002] treats the speci.c case of CHARMS applied to classical FEs, i.e., a setting in which 
basis functions are supported on a 1-ring. There we present ef.­cient algorithms for maintaining linear 
independence of the active set B during (un)re.nement. In more general settings, approaches such as those 
used by Kraft may be adopted [1997]. Finally, in some settings, such as our explicit time-integration 
of non-linear thin-shells (Section 4), we observe that the solution process remains well-behaved even 
without linear independence of the active set. With this, all the basic elements are in place to build 
simula­tors. What remains to be added are standard solvers for the result­ing (non-)linear algebraic 
systems, error estimators appropriate for the equation to be solved, and a quadrature routine.   4 
Example Applications Here we show that CHARMS can be pro.tably applied to many ap­plication domains including 
animation, modeling, engineering, and medical simulation/visualization. To that end, we present exam­ple 
applications covering different types of elements, in 2D and 3D settings, with different basis functions, 
using hierarchical as well as quasi-hierarchical re.nement. Although we have implemented these examples, 
our aim here is to provide a survey of the applica­tions; to that end we have omitted details including 
problem speci.c error estimators. These are best left to the original literature. The 2D examples employ 
subdivision basis functions to simu­late thin .exible structures including a balloon, a metallic cylinder, 
and a pillow. The 3D examples employ linear tetrahedra and tri­linear hexahedra to address bio-medical 
problems: (1) brain vol­ume deformation during surgery; (2) stress distribution in a human mandible; 
and (3) potential .elds in the human thorax for electro­cardiography (ECG) modeling. 4.1 Non-Linear 
Mechanics of Thin-Shells The thin-shell equations describe the behavior of thin .exible struc­tures. 
Examples include aluminium cans, cloth, Mylar, and paper among others. The underlying PDEs, based on 
the classic Kirch­hoff Love theory [Timoshenko and Woinowsky-Krieger 1959], de­ scribe the mechanical 
response of the surface to external forces in terms of the .rst and second fundamental forms of the original 
and deformed surfaces. Thin-shells are closely related to thin-plates, which are useful for variational 
geometric modeling and intuitive direct manipulation of surfaces. Thin-plate equations assume that the 
undeformed geometry is .at: the resulting equations are easier to solve but cannot capture subtleties 
of the nonlinear dynamic be­havior of more complex shapes (Figs. 1 and below). Thin-shell equations accommodate 
arbitrary initial con.gurations and cap­ture nonlinearities important for accurate modeling of stability 
phe­nomena, e.g., complex wrinkling patterns, buckling and crushing (Figs. 9 and 11). Subdivision bases 
are ideal for discretizing thin­ shell PDEs. For example, Loop basis functions (a) naturally satisfy 
the H2 smoothness requirement of these fourth order PDEs; (b) are controlled by displacements (not derivative 
quantities); and (c) easily model arbitrary topology. Cirak introduced the discretiza­  uff(x) - u..(x) 
..C(f)  . tion of thin-shells using Loop basis functions and presented non­adaptive simulations [2000; 
2001]. Adaptivity is essential for ef­ .ciently modeling complex material phenomena such as wrinkling 
and buckling; such simulations were the original motivation behind  Figure 9: Thin-shell simulation 
of in.ating metal-foil balloon (left); red spheres represent active basis functions (right). Note the 
con­centration of .ner basis functions near wrinkles and folds. the development of CHARMS. Here we present 
one static and two dynamic simulations that demonstrate the application of CHARMS to thin-shells using 
Loop basis functions; the accompanying movie (.lename: CHARMS.mov) is on the Conference Proceedings CD-ROM 
and DVD-ROM. In.ating Balloon We simulated the dynamic behavior of a rapidly in.ating metal-foil balloon 
(Fig. 9). The initial .at con.g­ uration has 50 nodes, and the fully-in.ated con.guration has 1000 active 
nodes. We applied internal pressure to the balloon and used quasi-hierarchical re.nement over the course 
of the 5ms simulated in.ation. Figure 10 shows the distribution of active nodes and cells near the end 
of the simulation; note the sparsity at the .nest levels. Non-adaptive approaches require a very .ne 
grid throughout this simulation, in contrast our adaptive approach begins with a coarse mesh and adds 
only necessary detail. Poking Balloon We poked the in.ated balloon with a .nger and used quasi-hierarchical 
re.nement as well as unre.nement to adapt the basis near the contact region. Pillow Using the balloon-in.ation 
technique we modeled a pil­low (Fig. 1). Starting with two rectangular pieces of fabric, we applied internal 
pressure and solved for the equilibrium state. The adapted solution captures the .ne wrinkles of the 
fabric. The pillow uses a thicker material (cloth) than the balloon (metal-foil), thus it forms characteristically 
different wrinkling patterns. Crushing Cylinder We animated the dynamic behavior of an aluminium cylinder 
under compression (Fig. 11). The crushing was applied as follows: the bottom rim of the cylinder was 
.xed; the vertical velocity (only) of the top rim was prescribed using a linear ramp. The .nal animation 
shows the rapid buckling patterns in slow-motion.  4.2 Volume Deformation as Surgery Aid Surgeons plan 
a brain operation based on landmarks from a time­consuming, pre-operative, high-resolution volume scan 
of the pa­tient [War.eld et al. 2000]. After opening the skull, the surgeons may acquire additional low-resolution 
volume scans, which show the deformation of the brain boundary surface, e.g., collapsing un­der gravity. 
However, these rapid scans do not encode landmarks. War.eld uses physical simulation with tetrahedral 
.nite elements to infer the volume deformation from the position of the brain bound­ary [2000]. He maps 
the high-resolution scan via the computed volume deformation, and shows surgeons the shifted landmarks. 
CHARMS adapts the discretization to maintain high accuracy. We modeled the volumetric deformation of 
the brain following the removal of cancerous tissue in the left hemisphere. Our mate­rial model is an 
isotropic elastic continuum [Zienkiewicz and Taylor 2000]; as the deformations are small we adopted linearized 
equa­ tions of equilibrium. The initial model has 2,898 nodes (5,526 DOFs) and 9,318 tetra­hedral elements. 
We .rst solve for the coarse displacement .eld, and then re.ne quasi-hierarchically to 64,905 DOFs, aiming 
for error equidistribution. Our error metric is the strain energy den­sity. Figure 12 shows the initial 
and re.ned meshes side by side. For comparison, a uniformly .ner mesh with the same precision as the 
.nest regions of the adapted grid would involve approximately 300,000 DOFs. Solving the volume deformation 
problem for the re.ned mesh takes 38s on a 600MHz PIII laptop with 256MB: with a two-or four-CPU PC our 
simulation is fast enough for actual surgical interventions. Figure 13 shows the re.ned FE model viewed 
in the caudal di­ rection (left). The cavity after resection is visible in this view. Note that very 
little re.nement is introduced next to the cavity itself. The deformation of the brain due to sagging 
under gravity is visualized in Fig. 1 (color coded displacement amplitude with zero: red and maximum: 
purple), where the skull has been included as a visual aid.  4.3 Stress Distribution in Human Mandible 
Numerical simulations are widely used in biomechanics to visual­ize response of skeletal structures to 
mechanical loads, as planning aid for operative treatments, design of implants, and exploration of ostheosynthesis 
methods [Kober and Muller-Hannemann 2000].  Here we present an adaptive simulation of the response 
of the human mandible to the pressure involved in biting on a hard object. The internal structure of 
the bone is very complex, but for the pur­pose of this simulation we consider the bone to be homogeneous 
and isotropic. The initial model is a coarse approximation of the geometry of the human mandible. Figure 
14 shows the original and re.ned FE model. CHARMS re.nement is achieved through octasection of each cube 
in the [-1, 1]3 reference con.guration with odd vertices placed at edge, face, and cell barycenters. 
The initial mesh con­sists of 304 hexahedral, trilinear cells (1,700 DOFs; Figure 14, left). The hierarchically 
re.ned model, based on strain-energy error in­dication, captures the stress concentration immediately 
underneath the pressure point and in the thinner extremities of the mandible. It has approximately 4,200 
DOFs (Fig. 14, right). We also ran this simulation with quasi-hierarchical re.nement with practically 
iden­tical results. Figure 15 shows a close-up of the re.ned hierarchical model. Active basis functions 
are shown as green dots, and are sup­ported on the cells sharing that vertex. The re.ned basis consists 
of functions on three levels in the mesh hierarchy.  4.4 Potential Field in the Human Torso Inverse 
problems, in which heart surface potentials are determined from measurements on the outer surfaces of 
the upper torso, are of particular importance in computer-assisted electrocardiography (ECG). An adaptive 
procedure for this problem has been outlined  Figure 15: Mandible FE model with a hierarchical basis. 
Green dots indicate nodes associated with active basis functions. Top row: quadrature cells from all 
levels with active nodes; cells supporting basis functions on level 1 are colored blue. Bottom row: cells 
sup­porting basis functions on level 2 and 3 are respectively colored purple and tan. Note that cells 
at different levels overlap, and the basis functions active on the .ner levels vanish along the internal 
boundaries of their supporting cells, thereby guaranteeing compat­ibility. Figure 16: ECG .eld analysis 
example. On the left the initial grid; on the right the quasi-hierarchically re.ned grid (four levels). 
Red balls indicate active basis functions, cells of different colors indi­cate the support of basis function 
on different levels. Initial surface grid courtesy of the Center for Scienti.c Computing and Imaging, 
University of Utah. by Johnson [2001]. Here we show how CHARMS can be applied to a subproblem of inverse 
ECG, the adaptive solution of the gen­eralized Laplace equation. Figure 16 (left) shows the initial grid 
with 900 nodes; (mid­ dle) a three-level quasi-hierarchically re.ned grid with 8,500 nodes; (right) the 
computed .eld of a dipole located on the epicardial sur­face is visualized through isopotential surfaces 
(assuming isotropic, homogeneous conductivity).  5 Conclusion and Future Work CHARMS provide a simple 
framework for the construction of adaptive solvers for PDEs with applications in computer graphics, engineering, 
and bio-medical computing. The methods exploit re­.nability of basis functions and use it as a literal 
prescription for adaptive enrichment of approximation spaces used in Galerkin dis­cretizations of PDEs. 
On the theory side, CHARMS provide no surprises; the approach can reproduce many well studied adaptive 
approximation spaces of interest. The main advantage of CHARMS lies in the implemen­tation ease. Traditional, 
element based, adaptive re.nement strate­gies are dif.cult to implement, especially in 3D. Some bases, 
such as higher order B-splines or subdivision surfaces do not even sup­port elementwise re.nement. In 
contrast, the algorithmic issues of compatibility are entirely circumvented in CHARMS, leading to rapid 
development and ef.cient management of adaptive solvers. For animations, the re.nability property also 
avoids troublesome popping artifacts during re.nement. In future work we hope to explore hierarchical 
solvers. In prin­ciple all the machinery to apply multigrid and wavelet precondi­tioning techniques are 
in place, because of our use of re.nement relations. Acknowledgment This work was supported in part by 
NSF (DMS-9874082, ACI-9721349, DMS-9872890, ACI-9982273), the DOE (W-7405-ENG-48/B341492), Intel, Alias|Wavefront, 
Pixar, Microsoft, the Packard Foundation, and the Hellman Fellow­ship 2001 (PK). Special thanks to Mathieu 
Desbrun, Steven Schkolne, Sylvain Jaume, Christopher Malek, Mika Nystroem, Patrick Mullen, Jeff Boltz, 
Mark Meyer, Ilja Friedel, Joe Kiniry, Andrei Khodakovsky, Nathan Litke, and Zo¨ e Wood.  References 
ALPERT, B. K. 1993. A Class of Bases in L2 for the Sparse Representation of Integral Operators. SIAM 
Journal on Mathematical Analysis 24, 246 262. ARNOLD, D. N., MUKHERJEE, A., AND POULY, L. 2001. Locally 
Adapted Tetrahe­ dra Meshes using Bisection. SIAM Journal on Scienti.c Computing 22, 2, 431 448. AZAR, 
F. S., METAXAS, D. N., AND SCHNALL, M. D. 2001. A Deformable Fi­ nite Element Model of the Breast for 
Predicting Mechanical Deformations under External Perturbations. Academic Radiology 8, 10, 965 975. BAJAJ, 
C., WARREN, J., AND XU, G. 2002. A Smooth Subdivision Scheme for Hexahedral Meshes. The Visual Computer. 
to appear. BANK, R., AND XU, J. 1996. An Algorithm for Coarsening Unstructured Meshes. Numerische Mathematik 
73, 1, 1 36. BEY, J. 2000. Simiplicial Grid Re.nement: On Freudenthal s Algorithm and the Optimal Number 
of Congruence Classes. Numerische Mathematik 85, 1 29. CATMULL, E., AND CLARK, J. 1978. Recursively generated 
B-spline surfaces on arbitrary topological meshes. CAD 10, 6, 350 355. CELNIKER, G., AND GOSSARD, D. 
1991. Deformable Curve and Surface Finite Elements for Free-Form Shape Design. Computer Graphics (Proceedings 
of SIG-GRAPH 91) 25, 4, 257 266. CIRAK, F., AND ORTIZ, M. 2001. Fully c 1-conforming subdivision elements 
for .nite deformation thin-shell analysis. IJNME 51, 7, 813 833. CIRAK, F., ORTIZ, M., AND SCHR ODER¨ 
, P. 2000. Subdivision surfaces: A new paradigm for thin-shell .nite-element analysis. IJNME 47, 12, 
2039 2072. COHEN, A., DEVORE, R., AND DAHMEN, W. 2001. Adaptive Wavelet Methods for Elliptic Operator 
Equations: Convergence Rates. Mathematics of Computation 70, 233, 27 75. DEBUNNE, G., DESBRUN, M., CANI, 
M.-P., AND BARR, A. H. 2001. Dynamic Real-Time Deformations Using Space &#38; Time Adaptive Sampling. 
Proceedings of SIGGRAPH 2001, 31 36. DESLAURIERS, G., AND DUBUC, S. 1989. Symmetric iterative interpolation 
pro­cesses. Constructive Approximation 5, 1, 49 68. DOO, D., AND SABIN, M. 1978. Analysis of the behaviour 
of recursive division surfaces near extraordinary points. CAD 10, 6, 356 360. DYN, N., LEVIN, D., AND 
GREGORY, J. A. 1990. A Butter.y Subdivision Scheme for Surface Interpolation with Tension Control. ACM 
TOG 9, 2 (April), 160 169. FORSEY, D. R., AND BARTELS, R. H. 1988. Hierarchical B-Spline Re.nement. Computer 
Graphics (Proceedings of SIGGRAPH 88) 22, 4, 205 212. FOSTER, N., AND FEDKIW, R. 2001. Practical Animation 
of Liquids. Proceedings of SIGGRAPH 2001, 23 30. GORTLER, S. J., AND COHEN, M. F. 1995. Hierarchical 
and Variational Geometric Modeling with Wavelets. 1995 Symposium on Interactive 3D Graphics, 35 42. GORTLER, 
S. J., SCHR ¨ ODER, P., COHEN, M. F., AND HANRAHAN, P. 1993. Wavelet Radiosity. Proceedings of SIGGRAPH 
93, 221 230. GOURRET, J.-P., THALMANN, N. M., AND THALMANN, D. 1989. Simulation of Object and Human Skin 
Deformations in a Grasping Task. Computer Graphics (Proceedings of SIGGRAPH 89) 23, 3, 21 30. HALSTEAD, 
M., KASS, M., AND DEROSE, T. 1993. Ef.cient, Fair Interpolation Using Catmull-Clark Surfaces. Proceedings 
of SIGGRAPH 93, 35 44. HOPPE, H. 1996. Progressive Meshes. Proceedings of SIGGRAPH 96, 99 108. HOUSE, 
D. H., AND BREEN, D. E., Eds. 2000. Coth Modeling and Animation. A.K. Peters. JOHNSON, C. 2001. Adaptive 
.nite element and local regularization methods for the inverse ECG problem. In Inverse Problems in Electrocardiology, 
WIT Press, P. Johnston, Ed. KAGAN, P., AND FISCHER, A. 2000. Integrated mechanically based CAE system 
using B-Spline .nite elements. CAD 32, 8-9, 539 552. v KOBBELT, L. 2000. 3 Subdivision. Proceedings of 
SIGGRAPH 2000, 103 112. KOBER, C., AND MULLER-HANNEMANN, M. 2000. Hexahedral Mesh Generation for the 
Simulation of the Human Mandible. In Proceedings of the 9th International Meshing Roundtable, Sandia 
National Laboratories, 423 434. KRAFT, R. 1997. Adaptive and linearly independent multilevel B-splines. 
In Surface Fitting and Multiresolution Methods, A. L. M´e, C. Rabut, and L. L. Schu­ ehaut´maker, Eds., 
vol. 2. Vanderbilt University Press, 209 218. KRYSL, P., GRINSPUN, E., AND SCHR ¨ P. 2002. Natural ODER, 
hi­erarchical re.nement for .nite element methods. To appear; IJNME, http://hogwarts.ucsd.edu/ pkrysl/pubs/adref.pdf 
. LANGTANGEN, H. P. 1999. Computational Partial Differential Equations. Springer Verlag. LEE, S., WOLBERG, 
G., AND SHIN, S. Y. 1997. Scattered Data Interpolation with Multilevel B-Splines. IEEE TVCG 3, 3, 228 
244. LOOP, C. 1987. Smooth Subdivision Surfaces Based on Triangles. Master s thesis, University of Utah, 
Department of Mathematics. LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. 1997. Multiresolution analysis 
for surfaces of arbitrary topological type. ACM TOG 16, 1, 34 73. MANDAL, C., QIN, H., AND VEMURI, B. 
C. 1997. Dynamic Smooth Subdivision Surfaces for Data Visualization. In IEEE Visualization 97, 371 378. 
METAXAS, D., AND TERZOPOULOS, D. 1992. Dynamic deformation of solid primi­ tives with constraints. Computer 
Graphics (Proceedings of SIGGRAPH 92) 26, 2, 309 312. O BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical 
Modeling and Animation of Brittle Fracture. In Proceedings of SIGGRAPH 99, 137 146. RIVARA, M., AND INOSTROZA, 
P. 1997. Using Longest-side Bisection Techniques for the Automatic Re.nement of Delaunay Triangulations. 
IJNME 40, 4, 581 597. STAM, J. 2001. On Subdivision Schemes Generalizing Uniform B-Spline Surfaces of 
Arbitrary Degree. CAGD 18, 5 (June), 383 396. STRANG, G., AND FIX, G. 1973. An Analysis of the Finite 
Element Method. Wellesley-Cambridge Press. STRANG, G., AND NGUYEN, T. 1996. Wavelets and Filter Banks. 
Wellesley-Cambridge Press. STRELA, V., HELLER, P. N., STRANG, G., TOPIWALA, P., AND HEIL, C. 1999. The 
Application of Multiwavelet Filterbanks to Image Processing. IEEE TIP 8, 4, 548 563. TERZOPOULOS, D., 
AND QIN, H. 1994. Dynamic NURBS with Geometric Con­ straints for Interactive Sculpting. ACM TOG 13, 2, 
103 136. TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 1987. Elastically Deformable Models. 
Computer Graphics (Proceedings of SIGGRAPH 87) 21, 4, 205 214. TIMOSHENKO, S., AND WOINOWSKY-KRIEGER, 
S. 1959. Theory of Plates and Shells. McGraw-Hill Book Company Inc. VELHO, L., AND ZORIN, D. 2001. 4-8 
Subdivision. CAGD 18, 5 (June), 397 427. WARFIELD, S. K., FERRANT, M., GALLEZ, X., NABAVI, A., JOLESZ, 
F. A., AND KIKINIS, R. 2000. Real-time biomechanical simulation of volumetric brain defor­mation for 
image guided neurosurgery. In SC 2000: High performance networking and computing conference, vol. 230, 
1 16. WELCH, W., AND WITKIN, A. 1992. Variational Surface Modeling. Computer Graphics (Proceedings of 
SIGGRAPH 92) 26, 2, 157 166. WU, X., DOWNES, M. S., GOKTEKIN, T., AND TENDICK, F. 2001. Adaptive Nonlinear 
Finite Elements for Deformable Body Simulation Using Dynamic Pro­ gressive Meshes. Computer Graphics 
Forum 20, 3, 349 358. YSERENTANT, H. 1986. On the multilevel splitting of .nite-element spaces. Nu­merische 
Mathematik 49, 4, 379 412. ZIENKIEWICZ, O. C., AND TAYLOR, R. L. 2000. The .nite element method: The 
basis, 5 ed., vol. 1. Butterworth and Heinemann. ZORIN, D., AND SCHR ¨ ODER, P., Eds. 2000. Subdivision 
for Modeling and Animation. Course Notes. ACM Siggraph. ZORIN, D., AND SCHR ¨ ODER, P. 2001. A Uni.ed 
Framework for Primal/Dual Quadri­ lateral Subdivision Schemes. CAGD 18, 5, 429 454. ZORIN, D., SCHR ¨ 
ODER, P., AND SWELDENS, W. 1996. Interpolating Subdivision for Meshes with Arbitrary Topology. Proceedings 
of SIGGRAPH 96, 189 192. ZORIN, D. 2000. Smoothness of Subdivision on Irregular Meshes. Constructive 
Approximation 16, 3, 359 397. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566579</article_id>
		<sort_key>291</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Graphical modeling and animation of ductile fracture]]></title>
		<page_from>291</page_from>
		<page_to>294</page_to>
		<doi_number>10.1145/566570.566579</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566579</url>
		<abstract>
			<par><![CDATA[In this paper, we describe a method for realistically animating ductile fracture in common solid materials such as plastics and metals. The effects that characterize ductile fracture occur due to interaction between plastic yielding and the fracture process. By modeling this interaction, our ductile fracture method can generate realistic motion for a much wider range of materials than could be realized with a purely brittle model. This method directly extends our prior work on brittle fracture [O'Brien and Hodgins, SIGGRAPH 99]. We show that adapting that method to ductile as well as brittle materials requires only a simple to implement modification that is computationally inexpensive. This paper describes this modification and presents results demonstrating some of the effects that may be realized with it.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation techniques]]></kw>
			<kw><![CDATA[cracking]]></kw>
			<kw><![CDATA[deformation]]></kw>
			<kw><![CDATA[ductile fracture]]></kw>
			<kw><![CDATA[dynamics]]></kw>
			<kw><![CDATA[finite element method]]></kw>
			<kw><![CDATA[fracture]]></kw>
			<kw><![CDATA[physically based modeling]]></kw>
			<kw><![CDATA[plasticity]]></kw>
			<kw><![CDATA[simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P133156</person_id>
				<author_profile_id><![CDATA[81100311781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382416</person_id>
				<author_profile_id><![CDATA[81100355860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Bargteil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14028670</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ANDERSON, T. L. 1995. Fracture Mechanics: Fundamentals and Applications, second ed. CRC Press, Boca Raton. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DUNCAN, J. 2001. More war. Cinefex 86 (July), 64-97. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[FUNG, Y. C. 1965. Foundations of Solid Mechanics. Prentice-Hall, Englewood Cliffs, N.J. 1, 2, 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[FUNG, Y. C. 1969. A First Course in Continuum Mechanics. Prentice-Hall, Englewood Cliffs, N.J. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HAN, W., AND REDDY, B. D. 1999. Plasticity: Mathematical Theory and Numerical Analysis. Interdisciplinary Applied Mathematics. Springer-Verlag, New York. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351688</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[MAZARAK, O., MARTINS, C., AND AMANATIDES, J. 1999. Animating exploding objects. In the proceedings of Graphics Interface '99. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[MERRIAM-WEBSTER, Ed. 1998. Merriam---Webster's Collegiate Dictionary, 10th ed. International Thomson Publishing, Springfield, Mass. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351686</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[NEFF, M., AND FIUME, E. 1999. A visual model for blast waves and fracture. In the proceedings of Graphics Interface '99. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>115248</ref_obj_id>
				<ref_obj_pid>115244</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[NORTON, A., TURK, G., BACON, B., GERTH, J., AND SWEENEY, P. 1991. Animation of fracture by physical modeling. The Visual Computer 7, 210-217. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical modeling and animation of brittle fracture. In the proceedings of ACM SIGGRAPH 99, Computer Graphics Proceedings, Annual Conference Series, 137-146. 1, 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[SMITH, J., WITKIN, A., AND BARAFF, D. 2001. Fast and controllable simulation of the shattering of brittle objects. Computer Graphics Forum 20, 2, 81-91. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND FLEISCHER, K. 1988. Deformable models. The Visual Computer 4, 306-331. 1, 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling inelastic deformation: Viscoelasticity, plasticity, fracture. In the proceedings of ACM SIGGRAPH 88, Computer Graphics Proceedings, Annual Conference Series, 269-278. 1, 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344801</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[YNGVE, G. D., O'BRIEN, J. F., AND HODGINS, J. K. 2000. Animating explosions. In the proceedings of ACM SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, 29-36. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphical Modeling and Animation of Ductile Fracture James F. O Brien Adam W. Bargteil Jessica K. Hodgins 
University of California, Berkeley University of California, Berkeley Carnegie Mellon University Abstract 
In this paper, we describe a method for realistically animating duc­tile fracture in common solid materials 
such as plastics and metals. The effects that characterize ductile fracture occur due to interac­tion 
between plastic yielding and the fracture process. By modeling this interaction, our ductile fracture 
method can generate realistic motion for a much wider range of materials than could be real­ized with 
a purely brittle model. This method directly extends our prior work on brittle fracture [O Brien and 
Hodgins, SIGGRAPH 99]. We show that adapting that method to ductile as well as brit­tle materials requires 
only a simple to implement modi.cation that is computationally inexpensive. This paper describes this 
modi.­cation and presents results demonstrating some of the effects that may be realized with it. CR 
Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling Physically based modeling; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; I.6.8 [Simulation and Modeling]: 
Types of Simulation Animation Keywords: Animation techniques, physically based modeling, simulation, 
dynamics, fracture, cracking, deformation, .nite ele­ment method, ductile fracture, plasticity. 1 Introduction 
As techniques for generating photorealistic computer rendered im­ages have improved, the use of physically 
based animation to gen­erate special effects in .lm, television, and games has become in­creasingly common. 
Physically based animation techniques have proven to be particularly useful for violent or destructive 
effects that would be impractical or expensive to achieve using other meth­ods. For example, when creating 
effects for the .lm Pearl Har­bor, Industrial Light and Magic made extensive use of simulation methods 
for modeling the destruction of ships, planes, and other structures [Duncan, 2001]. Animating objects 
as they break, crack, tear, or in general frac­ture appears to be an obvious place where physically based 
mod­eling should be useful, particularly if the object is expensive, ir­replaceable, or if breaking it 
would be hazardous. However even the most general of current techniques for animating fracture are limited 
to modeling only brittle materials. The term brittle does not mean that a material is fragile. It means 
that the material experiences only elastic deformation before frac­ture. Few real materials are truly 
brittle. In contrast, ductile ma­terials behave elastically up to a point and then experience some E-mail: 
job@cs.berkeley.edu, adamb@cs.berkeley.edu, jkh@cs.cmu.edu Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00  Figure 1: Four hollow balls 
that have been dropped onto a hard surface. The ball in (a) .attens out and visibly demonstrates plastic 
yielding. The other three do not show an appreciable amount of plastic deformation, but the manner in 
which they split and tear, as opposed to shattering, arises from of the interaction between plastic yielding 
and the fracture process. amount of plastic deformation before fracture. When brittle materi­als fracture, 
they shatter. However, ductile materials demonstrate a much wider range of fracture behaviors. (See .gures 
1 and 3.) This wider range of behaviors arises due to the interaction of plastic en­ergy absorption with 
the fracture process. This paper describes a method suitable for modeling ductile frac­ture in common 
solid materials such as plastics or metals. This method directly extends our prior technique presented 
in [O Brien and Hodgins, 1999] for modeling brittle fracture. Adapting that technique to ductile as well 
as brittle materials requires only a sim­ple to implement and computationally inexpensive modi.cation. 
This extension dramatically expands the range of materials that may be modeled. For the sake of brevity, 
this paper describes only this modi.cation and presents results demonstrating some of the effects that 
may be realized with it. 2 Related Work The primary contribution of this paper is extending [O Brien 
and Hodgins, 1999] to include ductile fracture by adding a plastic­ity model to the underlying .nite-element 
method. The plastic­ity model we describe is not novel. It consists of the von Mises yield criterion, 
simple kinematic work hardening, and a .nite yield limit [Fung, 1965]. This plasticity model is similar 
to the one used in [Terzopoulos and Fleischer, 1988a] and [Terzopoulos and Fleis­ cher, 1988b]. The primary 
differences between their model and the one presented here are that this model realistically preserves 
vol­ume and it includes a second elastic regime once a limit on the amount of plastic .ow has been exceeded. 
Although ductile fracture has not been widely addressed in the graphics literature, several other graphics 
researchers have investi­gated brittle fracture. In [Terzopoulos and Fleischer, 1988a] and [Terzopoulos 
and Fleischer, 1988b] a .nite differencing scheme was used to model tearing sheets of cloth-like material. 
Work by [Norton et al., 1991] used a mass/spring system to model a breaking teapot. Fracture in the context 
of explosions was explored by [Mazarak et al., 1999], [Neff and Fiume, 1999], and [Yngve et al., 2000]. 
Most recently [Smith et al., 2001] used constraint­ based methods for modeling brittle fracture. Outside 
the graphics literature, both brittle and ductile fracture have been investigated extensively. A comprehensive 
review of this work can be found in [Anderson, 1995]. 3 Ductile vs. Brittle Fracture The common usage 
of the terms elastic and brittle differs substan­ tially from their technical meanings. For example, 
elastic is often used incorrectly as a synonym for .exible, and the term brittle as a synonym for fragile 
[Merriam-Webster, 1998]. The technically cor­ rect de.nition of an elastic material refers to a material 
that returns to its original con.guration when deforming forces have been re­ moved. The ratio between 
the magnitude of a force and the amount of deformation it induces, that is how easily the material deforms, 
is the compliance of the material and it is irrelevant to whether or not the material is elastic. Although 
no real material is perfectly elas­ tic, both natural rubber and common glass are examples of nearly 
elastic materials. Rubber s elastic behavior is obvious while glass appears to be rigid. A brittle material 
is simply one that behaves elastically up until the point where it fractures. In contrast to an elastic 
material, a plastic material will not re­ turn to its original con.guration once deforming forces have 
been removed. When a material, such as lead, bends and then holds its new shape, it demonstrates plastic 
behavior. As previously stated, real materials do not behave perfectly elastically. Real materials can 
be deformed only to a limited extent before they will no longer return to their original con.guration. 
This limit is known as the ma­ terial s elastic limit or yield point. When the elastic limit has been 
exceeded, the material enters a plastic regime and begins to experi­ ence plastic .ow. Eventually, at 
the failure threshold, it fractures. The terms brittle and ductile relate to the relative values of the 
elastic limit and failure threshold. If the failure threshold nearly coincides with the elastic limit, 
then the material will experience only negligible plastic deformation before fracture. The term brit­ 
tle refers to such a material. In contrast, for a ductile material the failure threshold is signi.cantly 
larger than the elastic limit so that as the material deforms it experiences an elastic regime, followed 
a plastic regime, and then .nally fracture. The signi.cance of the distinction between ductile and brittle 
materials arises because elastic deformation stores energy whereas plastic deformation dissipates it. 
When a brittle material is de­ formed to its failure threshold, the majority of the energy used to deform 
it has been stored as elastic potential. When fracture oc­ curs, the energy is released and it tends 
to drive the fracture further into the material. Thus, even though a large or small force may be required 
to start a crack in a brittle material (depending on its toughness), once the crack is started only a 
small amount of energy is required to push it further. In contrast, a ductile material requires signi.cantly 
more work to propagate a crack because energy is be­ ing absorbed by plastic deformation. As a result, 
brittle materials tend to shatter, whereas ductile ones tend to tear. In general the underlying causes 
of plasticity are fairly compli­ cated and they give rise to a number of phenomena. For example, the 
energy absorbed by plastic deformation does not simply vanish and it may result in effects such as fatigue 
weakening. However for the purposes of animating failure events that occur over relatively short periods 
of time, the most signi.cant effect of plasticity is how it directly effects fracture propagation, and 
the methods discussed here focus on modeling those effects ef.ciently. Additional infor­ mation about 
mathematical models of deformation and plasticity can be found in [Fung, 1965; Fung, 1969] and [Han and 
Reddy, 1999]. Additional information concerning both brittle and ductile fracture may be found in [Anderson, 
1995]. 4 Modeling Ductility The dynamic fracture propagation technique described in [O Brien and Hodgins, 
1999] models the fracture process using a simple tetrahedral .nite-element method, rules for fracture 
initiation and propagation, and procedures for automatic remeshing as a crack advances. The quality of 
the results produced with that method is suf.cient for graphical applications, and the only limitation 
that makes it unsuitable for modeling fracture in ductile materials is that the continuum model does 
not account for plastic deformation. Extending that model to account for plasticity may be accom­plished 
by simply rede.ning the strain metric used to compute ele­ment stresses. This change has only a local 
impact on the fracture algorithm, and so we will not repeat the details of the method which appear in 
[O Brien and Hodgins, 1999]. Instead we describe only the modi.cations that should be made to the algorithm: 
 The elastic strain, e de.ned in section 4.1 of this paper, takes the place of the total strain, , when 
computing the elastic stress.  A routine for updating the plastic strain, described in sec­tion 4.2 
of this paper, must be called during every integration step.  Even though this extension requires only 
incremental modi.cations to the previous method, it signi.cantly extends the range of materi­als that 
may be realistically modeled. Furthermore, as our examples will demonstrate, small amounts of plastic 
yielding can dramati­cally effect the overall appearance of fracture patterns in a material, even though 
the plastic deformation itself cannot be observed di­rectly. We feel that the signi.cant relationship 
between plasticity and the appearance of fracture in most materials makes modeling plasticity a required 
component of any general system for animat­ing fracture. 4.1 Decomposing Strain The .rst step towards 
modeling plastic deformation requires sepa­rating the strain into two components: p e =+ (1) where is 
the total strain, p is the strain due to plastic defor­mation, and e is the strain due to elastic deformation. 
The total strain is a purely geometric measure, it indicates how much the local shape of an object has 
changed from some initial reference con.guration and it may be computed from the material s current con.guration. 
(See [O Brien and Hodgins, 1999] for computation of Green s strain.) The plastic strain re.ects how the 
material s rest shape has been permanently distorted and it is part of the material s state. Initially, 
the plastic strain is zero1 and it will evolve accord­ing to am update rule as the simulation progresses. 
Because the total and plastic strains are known at any given time, equation (1) may be used to compute 
the elastic strain. 4.2 Plastic Update The algorithm for modeling the evolution of the plastic strain 
con­sists of a yield condition that must be met before plastic deforma­tion occurs and a rule for computing 
plastic .ow once the yield criterion has been met. We employ von Mises s yield criterion for the condition 
under which plastic .ow will begin [Fung, 1965]. Our method for updating the plastic strain assumes that 
the rate of plas­tic .ow in the material is close enough to its rate of deformation so that plastic .ow 
can be updated instantaneously. This assumption precludes modeling phenomena such as creep and relaxation, 
but 1 A non-zero initial value for the plastic strain could be used to model an object that has already 
experienced plastic deformation. (a)(b)(c)  Plastic Limit, Elastic limit, Zero strain point Plastic 
strain, Current total strain deviation, Figure 2: These diagrams illustrate the behavior of the plasticity 
model. (a) Elastic deformation. (b) and (c) Plastic deformation. (d) Limit of plastic yield. (See explanation 
in the text.) under most circumstances these phenomena do not signi.cantly ef­fect fracture behavior. 
We also ignore the weakening of a material due to repeated plastic deformation known as fatigue. While 
fa­tigue often plays a signi.cant role in the failure of mechanisms and structures, a previously fatigued 
object may be modeled by locally adjusting its toughness and plastic limits. The von Mises yield criterion 
is based on the deviation of the elastic strain given by ' e Tr ( e ) =- (2) 3 where Tr (·) is the 
trace of a matrix andis the identity matrix. By averaging out the sum of the diagonal terms, the elastic 
strain deviation re.ects only the portion of the elastic strain that is due to shape distortion and it 
excludes dilation. Excluding dilation makes the plastic deformation insensitive to hydrostatic pressure 
and will prevent the material from changing its volume which would gener­ate unnatural behavior. The 
yield criterion compares the magnitude of the elastic strain deviation (Frobenius norm) to a material 
constant, 11 : 11 < || ' || . (3) Together equations (2) and (3) de.ne the von Mises yield crite­ rion 
[Fung, 1965]. If this condition is met then plastic deformation will occur. We compute the base change 
in plastic deformation ac­cording to: p || ' || - 11 ' = . (4) ||' || A limit on the total amount of 
plastic deformation that can be with­stood by the material, 12 , is enforced by updating the plastic 
strain at every time-step according to: p p p 12 :=(+ )min1, p . (5) ||+ p || The behavior of this 
plasticity model is illustrated by .gure 2. The image plane represents a two-dimensional projection of 
the .ve-dimensional space of strain deviations.2 The plastic strain be­haves as if it were being dragged 
by the total strain using a rope of length 11 . The difference between the plastic strain s and the total 
2 For three-dimensional objects, strain is a 3 × 3 symmetric tensor with nine components. Because of 
symmetry, only six of these components are independent. Equation (2) removes one degree of freedom, leaving 
.ve. Figure 3: Images showing the results of simulating a set of eight thin walls with different material 
parameters as they are each struck by a heavy projectile. A purely brittle material is shown in the top­left. 
The others images demonstrate how varying the plasticity of the material can produce a range of effects. 
strain s locations represents the current elastic strain. A barrier at radius 12 restricts the motion 
of the plastic strain, but not the total strain. An elastic force (stress) attracts the total strain 
to the plas­tic strain, but not the plastic strain to the total strain. As shown in .gure 2.c, the plastic 
deformation will depend on the history of the total strain s movement.  5 Results and Discussion Figure 
3 shows a set of thin walls that have been struck by a heavy weight. The walls are clamped at the bottom, 
and they experi­ence collision forces due to contacts with the ground plane, the weight, and self-collisions. 
The top-left image in .gure 3 with (11 = 12 =0) shows the behavior of a purely brittle material. The 
other images in .gure 3 show some examples that demonstrate the effects of different plastic parameter 
values. In the left column 11 has been varied while 12 was held .xed. The right column demon­strates 
the result of varying 12 while 11 was held .xed. Some of the images, such as the bottom-right with (11 
=0.001,12 =0.486), demonstrate obvious amounts of plastic yielding. However, plastic­ity also plays a 
signi.cant role in the images where plastic yielding is not obviously visible. For example, (11 =0.001,12 
=0.162) shows only a small part of the wall being torn away largely intact, and (11 =0.001,12 =0.006) 
shows the wall breaking into several large pieces. Both of these behaviors demonstrate how the fracture 
 Figure 4: A solid cylinder that experiences ductile fracture when it is pulled apart. Figure 5: A thin 
sheet that has been torn apart. process can be affected by otherwise unnoticeable amounts of plas­tic 
deformation. The proceedings DVD contains animations that further illustrate the behaviors depicted in 
.gure 3 as well as the behaviors shown in the other .gures. Figure 4 shows a solid cylinder tearing as 
it is pulled and twisted apart. Figures 5 and 7 show the ductile fracture that results when other objects 
are ripped apart. One way to assess the realism of an animation technique is by comparing it with the 
real world. Figure 6 shows a real clay slab that has been struck by a spherical projectile and a simulated 
slab of plastic material that has also be struck by a spherical projectile. Although the two images have 
obvious differences, the holes left by the projectiles demonstrate qualitative similarities. While modifying 
the computation of the element stresses to use the elastic strain instead of the total strain requires 
only minor changes to an existing code, the change may also have an effect on the integration scheme. 
Our implementation uses an explicit in­tegrator that takes adaptive time steps. The step size is determined 
by monitoring the total energy to ensure that the system is not go­ing unstable. We compared the size 
of steps taken when simulating a purely elastic material to those taken when simulating a material that 
was identical except that the plasticity code had been enabled. During periods when collisions were occurring, 
both simulations took similar-sized integration steps. At other times, however, the average step size 
for the plastic material was approximately twice that of the purely elastic one. This result is not surprising 
because plastic deformation absorbs energy implying that it should tend to help stabilize the system, 
but it is only a single test on a single set of parameters and further tests would need to be done before 
any more general statement could be made. The deformation model we implemented allows a regime of elas­tic 
deformation, followed by a plastic regime, and then possibly followed by a second elastic regime. While 
this model suf.ces for many materials, other materials, such as woven fabrics, may go through multiple 
cycles of elastic and plastic behavior. We have also worked only with a linear relationship between elastic 
strain and stress. While a linear model adequately describes many materi­als, other materials such as 
biological tissues demonstrate distinctly non-linear elastic behavior. Developing adequate graphical 
models for these types of materials remains an area for future work.  Acknowledgments The authors thank 
Cindy Grimm for the use of the model shown in .gure 7, and the members of the Berkeley Graphics Group 
for  Figure 6: A comparison showing (left) a real clay slab that has been punctured by a spherical projectile 
and (right) a similar result generated with our method. their helpful criticism and comments. This work 
was supported in part by Pixar Animation Studios, Intel Corporation, Sony Com­puter Entertainment America, 
and a research grant from the Okawa Foundation.  References ANDERSON, T. L. 1995. Fracture Mechanics: 
Fundamentals and Applications, sec­ond ed. CRC Press, Boca Raton. 2 DUNCAN, J. 2001. More war. Cinefex 
86 (July), 64 97. 1 FUNG, Y. C. 1965. Foundations of Solid Mechanics. Prentice-Hall, Englewood Cliffs, 
N.J. 1,2,3 FUNG, Y. C. 1969. A First Course in Continuum Mechanics. Prentice-Hall, Engle­wood Cliffs, 
N.J. 2 HAN, W., AND REDDY, B. D. 1999. Plasticity: Mathematical Theory and Numerical Analysis. Interdisciplinary 
Applied Mathematics. Springer-Verlag, New York. 2 MAZARAK, O., MARTINS, C., AND AMANATIDES, J. 1999. 
Animating exploding objects. In the proceedings of Graphics Interface 99.2 MERRIAM-WEBSTER, Ed. 1998. 
Merriam Webster s Collegiate Dictionary, 10th ed. International Thomson Publishing, Spring.eld, Mass. 
2 NEFF, M., AND FIUME, E. 1999. A visual model for blast waves and fracture. In the proceedings of Graphics 
Interface 99.2 NORTON, A., TURK, G., BACON, B., GERTH, J., AND SWEENEY, P. 1991. Anima­tion of fracture 
by physical modeling. The Visual Computer 7, 210 217. 2 O BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical 
modeling and animation of brittle fracture. In the proceedings of ACM SIGGRAPH 99, Computer Graphics 
Proceedings, Annual Conference Series, 137 146. 1, 2 SMITH, J., WITKIN, A., AND BARAFF, D. 2001. Fast 
and controllable simulation of the shattering of brittle objects. Computer Graphics Forum 20, 2, 81 91. 
2 TERZOPOULOS, D., AND FLEISCHER, K. 1988. Deformable models. The Visual Computer 4, 306 331. 1, 2 TERZOPOULOS, 
D., AND FLEISCHER, K. 1988. Modeling inelastic deformation: Viscoelasticity, plasticity, fracture. In 
the proceedings of ACM SIGGRAPH 88, Computer Graphics Proceedings, Annual Conference Series, 269 278. 
1, 2 YNGVE, G. D., O BRIEN, J. F., AND HODGINS, J. K. 2000. Animating explosions. In the proceedings 
of ACM SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, 29 36. 2  Figure 7: A 
cartoon character being dismembered by a red torture device.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566580</article_id>
		<sort_key>295</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Creating models of truss structures with optimization]]></title>
		<page_from>295</page_from>
		<page_to>301</page_to>
		<doi_number>10.1145/566570.566580</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566580</url>
		<abstract>
			<par><![CDATA[We present a method for designing truss structures, a common and complex category of buildings, using non-linear optimization. Truss structures are ubiquitous in the industrialized world, appearing as bridges, towers, roof supports and building exoskeletons, yet are complex enough that modeling them by hand is time consuming and tedious. We represent trusses as a set of rigid bars connected by pin joints, which may change location during optimization. By including the location of the joints as well as the strength of individual beams in our design variables, we can simultaneously optimize the geometry and the mass of structures. We present the details of our technique together with examples illustrating its use, including comparisons with real structures.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[constrained optimization]]></kw>
			<kw><![CDATA[nonlinear optimization]]></kw>
			<kw><![CDATA[physically based modeling]]></kw>
			<kw><![CDATA[truss structures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Constrained optimization</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Nonlinear programming</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14040519</person_id>
				<author_profile_id><![CDATA[81100084732]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39024867</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39046080</person_id>
				<author_profile_id><![CDATA[81100505460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Irving]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oppenheim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18516</person_id>
				<author_profile_id><![CDATA[81100295587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CHAPMAN, C., SAITOU, K., AND JAKIELA, M. 1993. Genetic algorithms as an approach to configuration and topology design. In Advances in Design Automation, vol. 65, ASME, 485-498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>16558</ref_obj_id>
				<ref_obj_pid>16551</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HAFTKA, R. T., AND GRANDHI, R. V. 1986. Structural shape optimization---a survey. Computer Methods in Applied Mechanics and Engineering 57, 91-106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HARRISS, J. 1975. The Tallest Tower --- Eiffel and the Belle Epoque. Houghton Mifflin.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[HEMP, W. 1973. Optimum Structures. Clarendon.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HEYMAN, J. 1956. Design of beams and frames for minimum material consumption. Quarterly of Applied Mathematics 8, 373-381.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1212487</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HIBBELER, R. 1998. Structural Analysis, fourth ed. Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[KIRSCH, U. 1989. Optimal topologies of structures. Applied Mechanics Reviews 42, 8, 223-238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[MACCALLUM, C., AND HANNA, R. 1997. Deflect: A computer aided learning package for teaching structural design. In Proceedings of Education in Computer Aided Architectural Design in Europe.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[MICHELL, A. 1904. The limits of economy of material in frame structures. Philosophical Magazine 8, 589-597.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[PARISH, Y. I. H., AND M&#220;LLER, P. 2001. Procedural modeling of cities. In Proceedings of SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 301-308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[PEDERSON, P. 1992. Topology optimization of three dimensional trusses. In Topology Designs of Structures, NATO ASI Series --- NATO Advanced Research Workshop, Kluwer Academic Publishers, 19-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[PICCOLOTTO, M., AND RIO, O. 1995. Design education with computers. In Proceedings of ACADIA 95: Computing in Design, 285-299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[POPOV, E. 1998. Engineering Mechanics of Solids. Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ, P. 1993. Modeling and visualization of biological structures. In Proceeding of Graphics Interface, 128-137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[REDDY, G., AND CAGAN, J. 1995. An improved shape annealing algorithm for truss topology. ASME Journal of Mechanical Design 117, 2A, 315-321.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>22735</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SPANIER, J., AND OLDHAM, K. 1987. An Atlas of Functions. Hemisphere.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[SPILLERS, W. 1975. Iterative Structural Design. North Holland Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[TOPPING, B. 1983. Shape optimization of skeletal structures: A review. Journal of Structural Engineering 109, 1933-1951.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[VANDERPLAATS, G., AND MOSES, F. 1977. Automated optimal geometry design of structures. Journal of the Structural Division of the American Society of Civil Engineers 98, ST3 (March).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[WRIGHT, M., AND GILL, P. 1981. Practical Optimization. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Models of Truss Structures with Optimization Jeffrey Smith Jessica Hodgins Irving Oppenheim 
Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University Andrew Witkin Pixar 
Animation Studios Abstract We present a method for designing truss structures, a common and complex 
category of buildings, using non-linear optimization. Truss structures are ubiquitous in the industrialized 
world, appear­ing as bridges, towers, roof supports and building exoskeletons, yet are complex enough 
that modeling them by hand is time consuming and tedious. We represent trusses as a set of rigid bars 
connected by pin joints, which may change location during optimization. By including the location of 
the joints as well as the strength of individ­ual beams in our design variables, we can simultaneously 
optimize the geometry and the mass of structures. We present the details of our technique together with 
examples illustrating its use, including comparisons with real structures. CR Categories: I.3.5 [Computer 
Graphics]: Computational Ge­ometry and Object Modeling Physically based modeling; G.1.6 [Numerical Analysis]: 
Optimization Nonlinear programming; G.1.6 [Numerical Analysis]: Optimization Constrained optimiza­tion 
Keywords: Physically based modeling, truss structures, con­strained optimization, nonlinear optimization 
 1 Introduction A recurring challenge in the .eld of computer graphics is the cre­ ation of realistic 
models of complex man-made structures. The standard solution to this problem is to build these models 
by hand, but this approach is time consuming and, where reference images are not available, can be dif.cult 
to reconcile with a demand for visual realism. Our paper presents a method, based on practices in the 
.eld of structural engineering, to quickly create novel and physically realistic truss structures such 
as bridges and towers, us­ ing simple optimization techniques and a minimum of user effort. Truss structures 
is a broad category of man-made structures, including bridges (Figure 1), water towers, cranes, roof 
support trusses (Figure 10), building exoskeletons (Figure 2), and tempo­ rary construction frameworks. 
Trusses derive their utility and dis­ tinctive look from their simple construction: rod elements (beams) 
 {jeffrey|jkh}@cs.cmu.edu, ijo@andrew.cmu.edu, aw@pixar.com Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Figure 1: A cantilever bridge 
generated by our software, compared with the Homestead bridge in Pittsburgh, Pennsylvania. which exert 
only axial forces, connected concentrically with welded or bolted joints. These utilitarian structures 
are ubiquitous in the industrialized world and can be extremely complex and thus dif.cult to model. For 
example, the Eiffel Tower, perhaps the most famous truss struc­ture in the world, contains over 15,000 
girders connected at over 30,000 points [Harriss 1975] and even simpler structures, such as railroad 
bridges, routinely contain hundreds of members of varying lengths. Consequently, modeling of these structures 
by hand can be dif.cult and tedious, and an automated method of generating them is desirable. 1.1 Background 
Very little has been published in the graphics literature on the problem of the automatic generation 
of man-made structures. While signi.cant and successful work has been done in recre­ating natural structures 
such as plants, the trunks and roots of trees, and corals and sponges (summarized in the review paper 
by Prusinkiewicz [1993]), these studies emphasize visual plausi­bility and morphogenetic realism over 
structural optimality. Parish and M¨uller recently described a system to generate cityscapes us­ing L-systems 
[Parish and M¨uller 2001], but this research did not address the issue of generating individual buildings 
for particular purposes or optimality conditons. Computer-aided analysis of sim­ple truss structures, 
coupled with graphic displays of de.ection or changing stresses, has been used for educational purposes 
within the structural engineering [MacCallum and Hanna 1997] and archi­tecture [Piccolotto and Rio 1995] 
communities, but these systems are not intended for the design of optimal structures. In the .eld of 
structural engineering, the use of numerical opti­mization techniques to aid design dates back to at 
least 1956 when linear programming was used to optimize frame structures based on plastic design theory 
[Heyman 1956]. Since then, extensive research has been done in the .eld of structural synthesis, as it 
is sometimes called, although its penetration into industry has been limited [Topping 1983; Haftka and 
Grandhi 1986]. Tech­niques in the structural engineering literature generally fall into three broad categories: 
geometry optimization, topology optimiza­tion, and cross-sectional optimization (also known as size opti­mization 
) [Kirsch 1989]. Cross-sectional optimization, the most heavily researched of these three techniques, 
assumes a .xed topology and geometry (the number of beams and joints, their connectivity, and locations) 
and .nds the shape of the beams that will best, either in terms of mass or stiffness, support a given 
set of loads. The parameters of the struc­ture that are changed during optimization, called the design 
vari­ables, are properties that affect the cross-sectional area of a beam such as, for the common case 
of tubular elements, the radius and thickness of each tube. An example of this technique in practice 
is the design of the beams that are used to build utility transmission towers [Vanderplaats and Moses 
1977], where savings of only a few hundred dollars in material costs, when multiplied by the thousands 
of towers needed for a new transmission route, can be a substantial gain. Topology optimization addresses 
the issues that size optimiza­tion ignores; it is concerned with the number and connectivity of the beams 
and joints, rather than their individual shape. Because struc­ture topology is most easily represented 
by discrete variables, nu­merical techniques used for topology optimization are quite differ­ent from 
those used for continuous size and geometry optimization problems. Prior approaches to this problem have 
included genetic programming [Chapman et al. 1993], simulated annealing [Reddy and Cagan 1995], and ground 
structure methods wherein a highly connected grid of pin-joints is optimized by removing members based 
on stress limits [Hemp 1973; Pederson 1992]. A review of these discrete parameter optimization problems 
in structural engi­neering can be found in Kirsch [1989]. The third category of structural optimization, 
geometry opti­mization, lies between the extremes of size and topology optimiza­tion. The goal of geometry 
optimization is to re.ne the position, strength and, to some extent, the topology of a truss structure. 
Be­cause these problems are highly non-linear, geometry optimization does not have as lengthy a history 
as size and topology optimiza­tion [Topping 1983]. A common approach, called multi-level de­sign, frames 
the problem as an iterative process wherein the con­tinuous design variables are optimized in one pass, 
and then the topology is changed on a second pass [Spillers 1975]. It is from this literature that we 
draw the inspiration for our work. For civil and mechanical engineers, the ultimate goal of struc­tural 
optimization is a highly accurate modeling of reality. Thus, common to all these techniques, no matter 
how different their im­plementations, is a desire for strict physical accuracy. In the .eld Figure 2: 
A cooling tower at a steel mill created by our software compared with an existing tower. From left to 
right: a real cooling tower, our synthesized tower, and the same model with the obstacle constraints 
shown. of computer graphics, however, we are often just as concerned with the speed of a solution and 
its visual impact as with its accuracy. For example, although important to structural engineers, optimiz­ing 
the cross-sections of the members used to build a bridge would be considered wasted effort in the typical 
computer graphics appli­cation, as the subtle differences between different shapes of beams are hardly 
noticeable from cinematic distances. Thus, rather than being concerned only with our model s approximation 
to reality, we are interested in optimizing the geometry and topology of truss structures with the goals 
of speed, user control and physical real­ism.  2 Representing Truss Structures Truss structures consist 
of rigid beams, pin-connected at joints, ex­erting axial forces only. This simple form allows us to represent 
trusses as a connected set of three-dimensional particles where ev­ery beam has exactly two end-points, 
and joints can accommodate any number of beams. In our model, the pin-joints are classi.ed into three 
types: free joints, loads, and anchors. Anchors are points where beams are joined to the earth, and thus 
are always in force balance. Loads are points at which external loads are being applied, e.g. the weight 
of vehicles on a bridge. Lastly, free joints are pin joints where beams connect but which are not in 
contact with the earth and have no external loads. 2.1 Constructing the Model Before solving for an optimal 
truss structure, we must have a clear idea of what purpose we want the structure to serve. For example, 
a bridge must support some minimum weight along its span, the Eif­fel Tower must support observation 
decks, and roof trusses need to support the roo.ng material. We model these support requirements as loads, 
which are placed by the user. Although most structural loads are continuous (e.g. a planar roadbed), 
appoximating loading as a set of discrete load-points is standard practice within the civil and structural 
engineering disciplines [Hibbeler 1998]. In addition to having external loads, every truss structure 
must also be supported at one or more points by the ground. For real structures, the location of these 
anchors is in.uenced by topogra­phy, geology, and the economics of a particular site, but for our modeling 
purposes their positions are speci.ed by the user. After placement of the anchors and loads, a rich set 
of free joints is automatically added and highly connected to all three sets of Figure 4: A typical railroad 
bridge and similar truss bridge designed by our software.  joints (see Figure 3). Speci.cally, our software 
generates free joints on a regular three-dimensional grid de.ned by the locations and spacings of the 
load and anchor points. Currently, our user inter­face asks the user to provide the number of vertical 
layers of free joints and whether these joints are initially placed above or below the loads (for example, 
they are placed above the road in Figure 3). In addition to this rectilinear placement, we also experimented 
with random placement of the free joints, distributing them in a spherical or cubic volume surrounding 
the loads and anchors. We found that random placement did not affect the quality of the .nal results, 
but could greatly increase the time needed for convergence. After generating the free joints, the software 
automatically makes connections between all three sets of joints, usually connect­ing each joint to its 
nearest neighbors using a simple O(N2) algo­rithm. Note that during optimization, beams may change strength 
and position, but new beams cannot be added. In this sense, we are using a ground structure technique, 
as described in Hemp [1973]. The initial structure does not need to be practical or even stable; it is 
merely used as a starting point for the optimization problem.  3 Optimizing Truss Structures The most 
important property of any structure, truss or not, is that it be stable; i.e. not fall down. For a truss 
structure to be considered stable, none of the joints can be out of force balance. Because our model 
consists of rigid beams exerting axial forces only, we can describe the forces acting on any joint i 
as: Bi ?lj Fi(?. )= ?gmi + . j 1= ?lj .j (1) where . j is the workless force being exerted by beam j, 
. is the vector of these forces for all beams, ?g is the gravity vector, mi is the mass of joint i, Bi 
is the number of beams attached to joint i, and lj is the vector pointing from one end of beam j to the 
other (the direction of this vector is not important as long as it is consistent). Given an objective 
function G (usually the total mass, but per­haps containing other terms), we can optimize a truss structure 
sub­ject to stability constraints by solving the following problem: min G(?q) (2) s.t. ?Fi(?q)= 0 i = 
1...NJ where NJ is the number of joints and ?q is the vector of design vari­ ables. If we wish to do 
simple cross-sectional optimization, this design vector is merely?. . If we wish to solve the more interesting 
geometry optimization problem, we also include the positions of all the free joints in ?q (see Section 
3.2 for more details). In order to avoid physically meaningless solutions, we should also constrain the 
maximum force that any member can exert: .j = .max j = 1...NB (3) where NB is the total number of beams. 
We constrain the absolute value of .j because the sign of the workless force will be positive when the 
beam is under compression and negative when the beam is under tension. In equation 1, we approximate 
the mass of a joint mi as half the masses of the beams that connect to it plus, in the case of load joints, 
whatever external loads may be applied at that joint. Although in reality the mass of a truss structure 
(exclusive of the externally ap­plied loads) is in its beams rather than its pin joints, this lumping 
approximation is standard practice in structural engineering and is considered valid as long as the overall 
structure is signi.cantly larger than any component member [Hibbeler 1998]. This assump­tion reduces 
the number of force balance constraints by a factor of two or more, depending on the connectivity of 
the structure, as well as allowing us to model members as ideal rigid beams. 3.1 Mass Functions For a 
given structural material, the mass of a beam is a function of its shape (length and cross-section). 
Under tension, the required cross-sectional area of a truss member scales linearly with the force the 
member exerts [Popov 1998]. Therefore, the volume of a beam under tension will be a linear function of 
length and force and, as­suming a constant material density, so will the mass mT : mT = -kT . j ?lj (4) 
where kT is a scaling factor determined by the density and tensile strength of the material being modeled, 
?lj is the length of beam j, and .j is the workless force it exerts (note that . j here will be negative 
because the beam is in tension). Figure 5: A depiction of Euler buckling under a compressive load. Under 
compression, long slender beams are subject to a mode of failure known as Euler buckling, wherein compressive 
forces can cause a beam to bend out of true and ultimately fail (see Figure 5). The maximum axial compressive 
force (FE ) that can be supported by a beam before it undergoes Euler buckling is governed by the following 
equation: p2EIp2Er2A FE == (5) ?lj 2 ?lj 2 where ?lj is the length of beam j, I is its area moment of 
inertia, and A is its cross-sectional area. E is the Young s Modulus of the material being modeled and 
r is the radius of gyration, which de­scribes the way in which the area of a cross-section is distributed 
around its centroidal axis. Because the cross-sectional area of a member is proportional to the square 
of r, we can rewrite equation 5 in terms of A and r as FE ?lj 2 A2 . (6) p2E Because we wish to use beams 
with minimum mass, . j (for a given beam j) will be equal to FE and our approximation of the mass function 
under compression is mC = .Aj ?lj = kC. j ?lj 2 (7) where . is the density of the material, ?lj is the 
length of beam j, and . j is the workless force being exerted by the beam. kC is a scaling factor determined 
by . and the constants in equation 6. Assuming the structures are made of steel I-beams, and using units 
of meters and kilograms, we use the values of 5 × 10-6 for kT and 1.5×10-5 for kC in equations 4 and 
7 respectively. Because we plan to do continuous optimization, we wish to avoid any disconti­nuities 
in the mass function and thus we use a nonlinear blending function between mT and mC centered around 
. j = 0 to smooth the transition. 3.2 Cross-Sectional and Geometry Optimization Assuming that the objective 
function G(?q) in equation 2 is merely a sum of the masses of the joints, and that the vector of design 
variables ?q consists only of?. , we can use the equations developed in the last section to perform a 
simple version of size optimization. Solving this non-linear, constrained optimization problem will give 
us, for a .xed geometry, the minimum mass structure that is strong enough to support its own weight in 
addition to the user-speci.ed external loads. We are interested, however, in the more useful geometry 
opti­mization problem, where both the strengths of the beams and the geometry of the overall structure 
can be changed. To allow the si­multaneous optimization of the sizing and geometry variables, we add 
the positions of the free joints to the vector of design variables, ?q. We do not add the anchor or load 
positions to the design vector because the locations of these two types of joints are set by the user. 
Adding these variables to the optimization problem does not change the form of the equations we have 
derived, but for numer­ical stability we now also constrain the lengths of all the beams to be above 
some small value: min G(?q) s.t. ?Fi(?q)= 0 i = 1...NJ (8) . j = .max j = 1...NB ?j = 1...NB lj = lmin 
where lmin was set to 0.1 meters in the examples reported here. Be­cause the optimization algorithm we 
use (sequential quadratic pro­gramming, described in detail in Gill [1981]) handles inequality constraints 
ef.ciently, these length constraints add minimal cost to the solution of the problem. Similar to the 
methods discussed in Pederson [1992], we use a multilevel design algorithm consisting of two steps. First, 
we solve the optimization problem described in equation 8. Having found a feasible (if not yet globally 
optimal) structure, the system then merges any pairs of joints that are connected to one another by a 
beam that is at the minimum allowable length, because these two joints are now essentially operating 
as one. The system could also, if we wanted, eliminate beams that are exerting little force (i.e. those 
with small .j ), as they are not actively helping to sup­port the loads. However, we prefer to leave 
such useless beams in the model so as to leave open more topology options for future iterations. After 
this topology-cleaning step, the results are examined by the user and, if they are not satisfactory for 
either mass or aesthetic reasons, the optimization is run again using this new structure as the starting 
point. In practice, we have found that a single iteration almost always gave us the structures we desired, 
and never did it take more than three or four iterations of the complete cycle to yield an appealing 
.nal result.  3.3 Objective and Constraint Functions Although the procedure outlined above generates 
good results, there are many situations in which we want a more sophisticated modeling of the physics 
or more control over the .nal results. Con­strained optimization techniques allow us to add intuitive 
control knobs to the system very easily. For example, in addition to constraints on the minimum length 
of beams, we can also impose constraints on the maximum length. These constraints imitate the real-world 
dif.culty of manufacturing and shipping long beams. (Due to state regulations on truck .at­beds, girders 
over 48 feet are not easily shipped in North Amer­ica.) Other changes or additions we have made to the 
objective and constraint functions include: minimizing the total length of beams (rather than mass), 
preferentially using tensile members (cables) over compressive members, and symmetry constraints, which 
cou­ple the position of certain joints to each other in order to derive symmetric forms. Another particularly 
useful class of constraint functions are ob­stacle avoidance constraints, which forbid the placement 
of joints or beams within certain volumes. In this paper, we have used two different types of obstacle 
constraints: one-sided planar and spheri­cal constraints. One-sided planar constraints are used to keep 
joints and beams in some particular half-volume of space; for example to keep the truss-work below the 
deck of the bridge shown in Figure 7. Implementation of this constraint is simple: given a point on the 
plane ?r and a normal ?n pointing to the volume that joints are al­lowed to be in, we constrain the distance 
from the free joints ?pi to this plane with NJ new constraints: (?pi -?r) ·?n = 0 i = 1...NJ (9)  Figure 
6: From left to right: initial structure, tripod solution, derrick solution. Similarly, to keep the beams 
and joints outside of a spherical vol­ume, we add a constraint on each beam that the distance between 
the center of this sphere and the beam (a line segment) must be be greater than or equal to some radius 
R. This distance formula may be found in geometry textbooks, such as Spanier [1987]. For opti­mization 
purposes, we approximated the gradients of this function with a .nite-difference method. The primary 
use of obstacle constraints is to allow the user to sculpt the .nal structure intuitively while preserving 
realism, but they can also serve to produce novel structures by creating local optima. Figure 6 shows, 
from left to right, an initial structure, in­feasible because it violates the obstacle avoidance constraint, 
and two designs produced as solutions from slightly different (random) initial guesses for ?. . In each 
image, the red sphere is the volume to be avoided, the green sphere at the top is the load that must 
be supported, and the cylinders are the beams, colored cyan or tan de­pending on whether they are in 
compression or tension. The an­chors are located at the three points where the structure touches the 
ground. The middle, tripod solution hangs the mass on a tensile member (a cable) from the apex of a pyramid, 
and the derrick so­lution on the right supports the mass in a much more complex way (this solution has 
a mass about 3 times that of the tripod). Both of these solutions are valid and both exist as real designs 
for simple winches and cranes. Although it is a general concern that nonlinear optimizations can become 
trapped in sub-optimal local solutions, in our experience this has not been a problem. When, as in the 
above example, the system produces a locally optimal design, we have found that a few additional iterations 
of our algorithm are suf.cient to .nd a much better optimum.  4 Results We have described a simple, 
physically motivated model for the rapid design and optimization of models of truss structures. The following 
examples illustrate the output of this work and demon­strate the realistic and novel results that can 
be generated. 4.1 Bridges Some of the most frequently seen truss structures are bridges. Strong and easy 
to build, truss bridges appear in a variety of shapes and sizes, depending on their use. A common type 
of truss bridge, called a Warren truss, is shown in Figure 4 with a photo of a real railroad bridge. 
The volume above the deck (the surface along which vehicles pass) was kept clear in this example by using 
con­straints to limit the movement of the free joints to vertical planes. The initial guess to generate 
this bridge was created automat­ically from thirty user-speci.ed points (the loads and anchors), shown 
in Figure 3. From this description of the problem, the system automatically added 22 free joints (one 
above each load point) and connected each of these to their eight nearest neighbors, resulting in a problem 
with 228 variables (22 free points and 163 beams). The Figure 8: A perspective and side view of a through-deck 
cantilever bridge. .nal bridge design consists of 48 joints and 144 beams, some of the particles and 
members having merged or been eliminated during the topology-cleaning step. Similar procedures were used 
to generate the initial guesses for all of our results. Using this same initial structure, but with constraints 
that no ma­terial may be placed above the deck, we generated a second bridge, shown in Figure 7. Note 
that the trusswork under the deck has con­verged to a single, thick spine. This spine is more conservative 
of materials than the rectilinear trusswork in Figure 4, but in the earlier case the constraints to keep 
the joints in vertical planes prevented it from arriving at this solution. Another type of bridge, a 
cantilever truss, is shown in Figure 1. As with the bridge shown in Figure 7, the cantilever bridge was 
constrained to have no material above the deck, and the joints were further constrained to move in vertical 
planes only. However, the addition of a third set of anchor joints in the middle of the span has signi.cantly 
in.uenced the .nal design of this problem. This bridge is shown with a real bridge of the same design: 
the Homestead High Level Bridge in Pittsburgh, Pennsylvania. The bridge in Figure 8 was generated with 
the same starting point and the same objective function as that in Figure 1, but without the clear deck 
and vertical-plane constraints. Removal of these constraints has allowed the structure to converge to 
a signi.cantly different solution, called a through-deck geometry. 4.2 Ei.el Tower A tall tower, similar 
to the upper two-thirds of the Eiffel Tower is shown in Figure 9. This tower was optimized from an initial 
rectilinear set of joints and beams, automatically generated from eight user-speci.ed points (the four 
anchor sites and a four loads at the top). We concentrated on the top two-thirds of the Tower because 
the design of the bottom third is dominated by aesthetic demands. Similarly, the observation decks, also 
ornamental, were not sythesized. Figure 9: Our trusswork tower, compared with a detail of the Eiffel 
Tower. Because they are ornamental and not structural, the obser­vation decks are not included in our 
tower.  4.3 Roof Trusses The frameworks used to support the roofs of buildings are perhaps the most 
common truss constructions. We have generated three dif­ferent types of roof trusses for two different 
roof pitches. In Fig­ure 10 we show each category of truss (cambered Fink, composite Warren, and Scissors) 
in its own column, at the top of which is an illustration of a real example. For a given pitch, all three 
types of trusses were generated from the the same initial geometry. The vari­ation in the results is 
due to different objective functions: total mass (cambered Fink and Scissors) and total length of beams 
(composite Warren), and different roof mass (the Scissors trusses have a roof that weighs twice as much 
as the other two types of trusses). 4.4 Michell Truss The Michell Truss is a well-known minimum-weight 
planar truss designed to support a single load with anchors placed on a circle in the same plane Although 
impractical because of the varying lengths and curved beams needed for an optimal solution, the Michell 
truss has been a topic of study and a standard problem for structural optimization work for nearly a 
century. We have reproduced the Michell truss with our system, starting from a grid-like initial guess 
and arriving at a solution very close to the analytical optimum (Fig­ure 11). 4.5 Timing Information 
Sequential quadratic programming, relying as it does on the itera­tive solution of quadratic sub-problems, 
is a robust and fast method for optimizing non-linear equations. Even with complicated prob­lems containing 
thousands of variables and non-linear constraints the total time to optimize any of the above examples 
from auto­generated initial guesses varied between tens of seconds and less than .fteen minutes on a 
275MHz R10000 SGI Octane. Specifying the anchor and load points and the locations of obstacles (if any) 
rarely took more than a few minutes and with better user-interface design this time could be signi.cantly 
reduced. For comparison, we timed an expert user of Maya as he constructed duplicates of the cooling 
tower (Figure 2), the Warren truss bridge (Figure 4) and the tower shown in Figure 9 from source photographs 
of real structures. We found that modeling these structures at a comparable level of detail by hand took 
an hour for the cooling tower, an hour and a half for the bridge and almost three hours for the Eiffel 
Tower. Although informal, this experiment showed that our method has the potential to speed up the construction 
of models of truss structures enormously, while simultaneously guaranteeing physical realism.  5 Summary 
and Discussion We have described a system for representing and optimizing trusses, a common and visually 
complex category of man-made structures. By representing the joints of the truss as movable points, and 
the links between them as scalable beams, we have framed the design as a non-linear optimization problem, 
which allows the use of powerful numerical techniques. Furthermore, by altering the mass functions of 
the beams, the objective function, and the con­straints, we can alter the design process and easily generate 
a vari­ety of interesting structures. Other than the location of anchors and loads, the factor that we 
found to have the largest effect on the .nal results was the use of obstacle avoidance constraints. Placement 
of these constraints is a powerful way of encouraging the production of certain shapes, such as the volume 
inside a cooling tower or the clear traf.c deck on a railroad bridge. Not surprisingly, we also found 
that the number of free joints added during the initial model construction could affect the look of the 
.nal structure. However, this effect was largely one of increas­ing the detail of the truss-work, rather 
than fundamentally chang­ing the .nal shape. The initial positions of the free joints, however, made 
little difference to the .nal designs, although they could affect the amount of time required for the 
optimization. In cases where the initial positions did make a difference, it was generally the result 
of constraints (such as obstacle avoidance) creating a barrier to the movement of free joints during 
optimization and thus creating local minima. In our experience, however, a few additional iterations 
of the algorithm were suf.cient to get out of these local minima and .nd a global solution. We also found 
that beyond some minimum number of beams per joint (three or four), additional connections to more distant 
joints had a negligible effect on the .nal designs. We attribute this lack of impact to two factors. 
First, because we are minimizing the to­tal mass of the structure (or occasionally only the total length 
of all members), shorter beams connected to closer joints will be used more readily than longer ones, 
especially if they are under compres­sion. Secondly, by allowing unused beams to fade away as the force 
they exert drops to zero, initial structures with many beams per joint can become equivalent to structures 
with fewer beams per joint, allowing both more and less complex initial structures to con­verge to the 
same answer. Although successful at capturing the geometric and topological complexity of truss structures, 
our work does not account for all the details of true truss design. For example, a better objective function 
would calculate the actual cost of construction, including variables such as connection costs (the cost 
to attach beams to a joint) and the cost of anchors, which varies depending on terrain and the force 
they must transmit to the ground. Nor does our model explicitly include stress limits in the materials 
being modeled (al­though the constants kT and kC in equations 4 and 7 are an im­plicit approximation). 
Similarly, a more complex column formula than the simpli.ed Euler buckling formula, such as those described 
in Popov[1998], might capture more nuances of real design. True structural engineering must also take 
into account an envelope of possible load forces acting on a structure, not a single set as we have implemented. 
In each of these cases, our approximations were made not for technical reasons (for example, load envelopes 
could be handled with multi-objective optimization techniques, and stress limits with more inequality 
constraints), but rather because the vi­sual detail they add is not commensurate with the added complexity 
and expense of the solution.  Eventually, we would like to be able to include more abstract aesthetic 
criteria in the objective function. Our current system in­corporates the concepts of minimal mass and 
symmetry (via con­straints), but many elements of compelling design are based on less easily quanti.able 
concepts such as harmony, the visual weight of a structure, and use of familiar geometric forms. Because 
our technique is fast enough for user guidance, implementing even a crude approximation to these qualitative 
architectural ideals would allow users more .exibility in design and the ability to create more imaginative 
structures while still guaranteeing their physical real­ism.  Acknowledgements We would like to thank 
Joel Heires for helping us with the Maya modelling tests. The photo of the Homestead Hilevel Bridge in 
Figure 1 is Copyright 2002 Pittsburgh Post-Gazette Archives. All rights reserved. Reprinted with permission. 
The photo of the steel­mill cooling tower in Figure 2 is Copyright 2002 Bernd and Hilla Becher. All rights 
reserved. The photo of the bridge in Figure 4 is Copyright 2002 Bruce S. Cridlebaugh. All rights reserved. 
 References CHAPMAN, C., SAITOU, K., AND JAKIELA, M. 1993. Genetic algorithms as an approach to con.guration 
and topology design. In Advances in Design Automation, vol. 65, ASME, 485 498. HARRISS, J. 1975. The 
Tallest Tower Eiffel and the Belle Epoque. Houghton Mif.in. HEMP, W. 1973. Optimum Structures. Clarendon. 
HEYMAN, J. 1956. Design of beams and frames for minimum material consumption. Quarterly of Applied Mathematics 
8, 373 381. HIBBELER, R. 1998. Structural Analysis, fourth ed. Prentice Hall. KIRSCH, U. 1989. Optimal 
topologies of structures. Applied Mechanics Reviews 42, 8, 223 238. MACCALLUM, C., AND HANNA, R. 1997. 
De.ect: A computer aided learning package for teaching structural design. In Proceedings of Education 
in Computer Aided Architectural Design in Europe. MICHELL, A. 1904. The limits of economy of material 
in frame structures. Philo­sophical Magazine 8, 589 597. PARISH, Y. I. H., AND M¨ ULLER, P. 2001. Procedural 
modeling of cities. In Proceed­ings of SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference 
Series, 301 308. PEDERSON, P. 1992. Topology optimization of three dimensional trusses. In Topology Designs 
of Structures, NATO ASI Series NATO Advanced Research Workshop, Kluwer Academic Publishers, 19 30. PICCOLOTTO, 
M., AND RIO, O. 1995. Design education with computers. In Pro­ceedings of ACADIA 95: Computing in Design, 
285 299. POPOV, E. 1998. Engineering Mechanics of Solids. Prentice Hall. PRUSINKIEWICZ, P. 1993. Modeling 
and visualization of biological structures. In Proceeding of Graphics Interface, 128 137. REDDY, G., 
AND CAGAN, J. 1995. An improved shape annealing algorithm for truss topology. ASME Journal of Mechanical 
Design 117, 2A, 315 321. SPANIER, J., AND OLDHAM, K. 1987. An Atlas of Functions. Hemisphere. SPILLERS, 
W. 1975. Iterative Structural Design. North Holland Publishing Co. TOPPING, B. 1983. Shape optimization 
of skeletal structures: A review. Journal of Structural Engineering 109, 1933 1951. VANDERPLAATS, G., 
AND MOSES, F. 1977. Automated optimal geometry design of structures. Journal of the Structural Division 
of the American Society of Civil Engineers 98, ST3 (March). WRIGHT, M., AND GILL, P. 1981. Practical 
Optimization. Academic Press. HAFTKA, R. T., AND GRANDHI, R. V. 1986. Structural shape optimization a 
survey. Computer Methods in Applied Mechanics and Engineering 57, 91 106.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566581</article_id>
		<sort_key>302</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A procedural approach to authoring solid models]]></title>
		<page_from>302</page_from>
		<page_to>311</page_to>
		<doi_number>10.1145/566570.566581</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566581</url>
		<abstract>
			<par><![CDATA[We present a procedural approach to authoring layered, solid models. Using a simple scripting language, we define the internal structure of a volume from one or more input meshes. Sculpting and simulation operators are applied within the context of the language to shape and modify the model. Our framework treats simulation as a modeling operator rather than simply as a tool for animation, thereby suggesting a new paradigm for modeling as well as a new level of abstraction for interacting with simulation environments.Capturing real-world effects with standard modeling techniques is extremely challenging. Our key contribution is a concise procedural approach for seamlessly building and modifying complex solid geometry. We present an implementation of our language using a flexible tetrahedral representation. We show a variety of complex objects modeled in our system using tools that interface with finite element method and particle system simulations.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[signed-distance function]]></kw>
			<kw><![CDATA[tetrahedral representation]]></kw>
			<kw><![CDATA[volumetric modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP45026777</person_id>
				<author_profile_id><![CDATA[81100560718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cutler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P150906</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14058906</person_id>
				<author_profile_id><![CDATA[81100137780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Leonard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48032022</person_id>
				<author_profile_id><![CDATA[81100615490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P362259</person_id>
				<author_profile_id><![CDATA[81100626574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jagnow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADZHIEV, V., CARTWRIGHT, R., FAUSETT, E., OSSIPOV, A., PASKO, A., AND SAVCHENKO, V. 1999. HyperFun Project: A framework for collaborative multi-dimensional F-rep modeling. In Proceedings of Implicit Surfaces '99, 59-69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ANDERSON, H. L., Ed. 1989. A Physicist's Desk Reference, 2nd ed. American Institute of Physics, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAKER, T. J. 1989. Automatic mesh generation for complex three-dimensional regions using a constrained delaunay triangulation. Engineering with Computers, 5, 161-175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218462</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J., AND FERGUSON, K. 1995. Polygonization of non-manifold implicit surfaces. In Proceedings of ACM SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, 309-316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>180923</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J, 1994. An implicit surface polygonizer. In Graphics Gems IV. Academic Press, Boston, 324-349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375222</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CIGNONI, P., COSTANZA, D., MONTANI, C., ROCCHINI, C., AND SCOPIGNO, R. 2000. Simplification of tetrahedral meshes with accurate error evaluation. In IEEE Visualization 2000, 85-92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L. 1984. Shade trees. In Computer Graphics (Proceedings of ACM SIGGRAPH 84), 18(3), 223-231.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97900</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[COQUILLART, S. 1990. Extended free-form deformation: A sculpturing tool for 3d geometric modeling. In Computer Graphics (Proceedings of SIGGRAPH 90), 24(4), 187-196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237280</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DORSEY, J., PEDERSEN, H. K., AND HANRAHAN, P. M. 1996. Flow and changes in appearance. In Proceedings of ACM SIGGRAPH 96, Computer Graphics Proceedings, Annual Conference Series, 411-420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DORSEY, J., EDELMAN, A., LEGAKIS, J., JENSEN, H. W., AND PEDERSEN, H. K. 1999. Modeling and rendering of weathered stone. In Proceedings of ACM SIGGRAPH 99, Computer Graphics Proceedings, Annual Conference Series, 225-234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551861</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[EBERT, D. S., MUSGRAVE, F. K., PEACHEY, D., PERLIN, K., AND WORLEY, S. 1998. Texturing & Modeling, 2nd ed. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FLEISCHMANN, P., KOSIK, R., HAINDL, B., AND SLBERHERR, S. 1999. Simple examples to illustrate specific finite element mesh requirements. In Proceedings of the 8th International Meshing Roundtable, 241-246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[FREITAG, L. A., AND OLLIVIER-GOOCH, C. 1997. Tetrahedral mesh improvement using swapping and smoothing. International Journal for Numerical Methods in Engineering, vol. 40, 3979-4002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[FRISKEN, S. F., PERRY, R. N., ROCKWOOD, A. P., AND JONES, T. R. 2000. Adaptively sampled distance fields: A general representation of shape for computer graphics. In Proceedings of ACM SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, 249-254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. S. 1997. Surface simplification using quadric error metrics. In Proceedings of ACM SIGGRAPH 97, Computer Graphics Proceedings, Annual Conference Series, 209-216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND LAWSON, J. 1990. A language for shading and lighting calculations. In Computer Graphics (Proceedings of ACM SIGGRAPH 90), 24(4), 289-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. 1996. Progressive meshes. In Proceedings of ACM SIGGRAPH 96, Computer Graphics Proceedings, Annual Conference Series, 99-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383293</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LEGAKIS, J., DORSEY, J., AND GORTLER, S. J. 2001. Feature-based cellular texturing for architectural models. In Proceedings of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 309-316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LOHNER, R. 1988. Generation of three-dimensional unstructured grids by the advancing front method. In International Journal for Numerical Methods in Fluids, vol. 8, 1135-1149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LORENSEN, W. E., AND CLINE, H. E. 1987. Marching cubes: A high resolution 3d surface construction algorithm. In Computer Graphics (Proceedings of ACM SIGGRAPH 87), 21(4), 163-169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MIZUNO, S., OKADA, M., AND ICHIRO TORIWAKI, J. 1998. Virtual sculpting and virtual woodcut printing. The Visual Computer, 14(2), 39-51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776361</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M&#220;LLER, M., DORSEY, J., MCMILLAN, L., AND JAGNOW, R. 2001. Real-time simulation of deformation and fracture of stiff materials. In Proceedings of Eurographics Workshop on Animation and Simulation 2001, 113-124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267064</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M., AND SUNG, J. 1997. Interval volume tetrahedrization. In IEEE Visualization '97, 221-228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375281</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[NOORUDDIN, F. S., AND TURK, G. 2000. Interior/exterior classification of polygonal models. In IEEE Visualization 2000, 415-422.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical modeling and animation of brittle fracture. In Proceedings of ACM SIGGRAPH 99, Computer Graphics Proceedings, Annual Conference Series, 137-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[PARISH, Y. I. H., AND M&#220;LLER, P. 2001. Procedural modeling of cities. In Proceedings of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 301-308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617722</ref_obj_id>
				<ref_obj_pid>616021</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PAYNE, B. A., AND TOGA, A. W. 1992. Distance field manipulation of surface models. IEEE Computer Graphics & Applications, 12(1), 65-71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND HOFFERT, E. M. 1989. Hypertexture. In Computer Graphics (Proceedings of ACM SIGGRAPH 89), 23(3), 253-262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1985. An image synthesizer. In Computer Graphics (Proceedings of ACM SIGGRAPH 85), 19(3), 287-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378503</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ, P., LINDENMAYER, A., AND HANAN, J. 1988. Developmental models of herbaceous plants for computer imagery purposes. In Computer Graphics (Proceedings of ACM SIGGRAPH 88), 22(4), 141-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[RAVIV, A., AND ELBER, G. 2000. Three-dimensional freeform sculpting via zero sets of scalar trivariate functions. Computer-Aided Design, 32(8-9), 513-526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[RICCI, A. 1973. A constructive geometry for computer graphics. The Computer Journal, 16(2), 157-160.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325510</ref_obj_id>
				<ref_obj_pid>325509</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[SETHIAN, J. A. 1999. Level Set Methods and Fast Marching Methods, 2nd ed. Cambridge University Press, Cambridge, United Kingdom.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>276894</ref_obj_id>
				<ref_obj_pid>276884</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[SHEWCHUK, J. R. 1998. Tetrahedral mesh generation by delaunay refinement. In Proceedings of the 14th Annual Symposium on Computational Geometry, 86-95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288328</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[STAADT, O. G., AND GROSS, M. H. 1998. Progressive tetrahedralizations. In IEEE Visualization '98, 397-402.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614434</ref_obj_id>
				<ref_obj_pid>614275</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[TROTTS, I. J., HAMANN, B., AND JOY, K. I. 1999. Simplification of tetrahedral meshes with error bounds. IEEE Transactions on Visualization and Computer Graphics, 5(3), 224-237.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>534134</ref_obj_id>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. 1990. The Renderman Companion: A Programmer's Guide to Realistic Computer Graphics. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199430</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[WANG, S. W., AND KAUFMAN, A. E. 1995. Volume sculpting. In Symposium on Interactive 3D Graphics, ACM Press, 151-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[WYVILL, B., MCPHEETERS, C., AND WYVILL, G. 1986. Data structure for soft objects. The Visual Computer, 2(4), 227-234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[WYVILL, B., GUY, A., AND GALIN, E. 1999. Extending the CSG tree. Warping, blending and boolean operations in an implicit surface modeling system. Computer Graphics Forum, 18(2), 149-158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[YERRY, M. A., AND SHEPHARD, M. S. 1984. Automatic three-dimensional mesh generation by the modified octree technique. International Journal For Numerical Methods in Engineering, 20, 1965-1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Procedural Approach to Authoring Solid Models Barbara Cutler Julie Dorsey Leonard McMillan Matthias 
M¨Robert Jagnow uller Laboratory for Computer Science Massachusetts Institute of Technology  Abstract 
We present a procedural approach to authoring layered, solid mod­els. Using a simple scripting language, 
we de.ne the internal struc­ture of a volume from one or more input meshes. Sculpting and simulation 
operators are applied within the context of the language to shape and modify the model. Our framework 
treats simulation as a modeling operator rather than simply as a tool for animation, thereby suggesting 
a new paradigm for modeling as well as a new level of abstraction for interacting with simulation environments. 
Capturing real-world effects with standard modeling techniques is extremely challenging. Our key contribution 
is a concise pro­cedural approach for seamlessly building and modifying complex solid geometry. We present 
an implementation of our language us­ing a .exible tetrahedral representation. We show a variety of com­plex 
objects modeled in our system using tools that interface with .nite element method and particle system 
simulations. Additional Keywords: volumetric modeling, signed-distance function, tetrahedral representation. 
 1 Introduction Geometric models are a fundamental component in any graphics system. While there has 
been tremendous progress in the area of rendering over the past three decades, creating and acquiring 
high .delity geometric models remains a challenging and tedious pro­cess.1 Models are generally designed 
with high-end rendering in mind and can be dif.cult to modify and manipulate. Furthermore, as animation 
and simulation techniques become increasingly so­phisticated and widely available, there is an increasing 
demand for models suitable for these purposes as well. Today s model generation tools are primitive in 
that they gen­erally lack a formal speci.cation framework. This stands in stark contrast to commonly 
available rendering systems, such as Render-Man, in which lighting, materials, objects, and even shading 
are speci.ed procedurally [Hanrahan and Lawson 1990; Upstill 1990]. In this paper, we introduce a procedural 
modeling approach for authoring layered, solid models. We are especially interested in generating models 
that are suitable for both rendering and physical 1The widespread use of the same small set of models, 
such as the Stan­ford bunny and the Utah teapot, attests to these dif.culties. Copyright &#38;#169; 2002 
by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use is granted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
simulation. Just as computer graphics rendering systems provide a framework for light transport simulation, 
we envision an analogous framework for physical processes and other operators that modify and shape geometry. 
There are many reasons to consider a procedural approach to sur­face creation and modi.cation. A concise 
speci.cation framework permits different simulation techniques for example, ray tracing, radiosity, 
.nite element method (FEM), and simpli.ed spring-mass models to be applied and compared. In addition, 
complicated processes can be described algorithmically. A procedural de.nition can be used as an intermediate 
format for capturing, editing, and replaying interactive editing sessions. It also provides a high-level 
abstraction, permitting a variety of different representations for example, meshes and implicit functions 
 to coexist in the same environment, regardless of the underlying simulation system. Pro­cedural models 
are advantageous in that they can be incrementally edited and re.ned based on artistic needs. Finally, 
powerful simu­lation tools, such as FEM or particle systems, can be embedded as modeling operators within 
such a procedural framework. 1.1 Related Work Within traditional modeling systems, complex models are 
created by applying a variety of operations, such as constructive solid ge­ometry(CSG) and freeform deformations, 
to a vast array of geomet­ric primitives [Coquillart 1990; Payne and Toga 1992; Wyvill et al. 1999; Adzhiev 
et al. 1999]. In the hands of a talented artist, these systems produce intricate geometric models, but 
the process is ex­tremely labor intensive. The range of tools available for specifying and editing shapes 
is also very limited. Surface representations can be locally deformed by simply modifying surface control 
points; however, tools for shaping geometry are rarely physically based, and the underlying geometry 
generally lacks information about the internal physical properties of the model, which would be necessary 
for creating complex deformations. In addition, such deformations can create self-intersections that 
are dif.cult to detect or prevent. Furthermore, performing topological changes to a model, such as drilling 
a hole through it, can be challenging using a surface de­scription alone. Another approach to creating 
models involves interactive sculpt­ing, in which the user modi.es a solid material with a tool [Wang 
and Kaufman 1995; Mizuno et al. 1998; Raviv and Elber 2000; Frisken et al. 2000]. Such systems are typically 
based on sampled volumetric representations (voxels or octrees), which can be costly to store and render 
interactively. Additionally, performing deforma­tions within a grid-based representation requires shifting 
data over cell boundaries, which can be expensive and lossy. Unlike surfaces, which are merely hollow 
shells, volumetric representations can cap­ture the internal material structure of a model. One of the 
main bene.ts of volumetric representations is that they support robust sculpting operations and simulations 
[Dorsey et al. 1999; O Brien and Hodgins 1999]. However, volumetric models often lack visual .delity 
because a high resolution volume is necessary to represent a complex model. 3D digitizing has emerged 
as a popular technique for acquiring complex models, such as sculptures or mechanical parts, which would 
be dif.cult or impossible to create with interactive tech­niques. While such digitizers are useful for 
acquiring surface shape and appearance properties, they do not capture the internal structure of the 
geometry, which is often necessary for animation or simula­tion. Procedural modeling techniques have 
proved to be valuable in several speci.c domains of computer graphics [Ebert et al. 1998]. Examples include 
plant modeling [Prusinkiewicz et al. 1988], solid texturing [Perlin 1985; Perlin and Hoffert 1989], displacement 
maps [Cook 1984], cellular texturing [Legakis et al. 2001], and ur­ban modeling [Parish and M¨uller 2001]. 
One of the dif.culties of procedural modeling is that the various techniques are domain spe­ci.c. Additionally 
it can be dif.cult to precisely control the gener­ation process to create a speci.c model. In our approach, 
we use a surface model as a starting point and use procedural techniques to generate a solid model. This 
provides a framework for the cre­ation of a rich class of models, which are suitable for rendering and 
simulation. 1.2 Overview Our procedural framework provides a controlled, systematic way to specify the 
geometric and material properties of a solid model and to vary these attributes as a function of time. 
We have devel­oped a simple scripting language for authoring complex volumetric models and we show examples 
of its use. In our language, models are .rst initialized and then modi.ed with a palette of physically 
inspired simulation operations. Model initialization is presented in Section 2 and the de.nition and 
use of simulation tools is described in Section 3. In Section 4 we present an implementation of the lan­guage 
using layered tetrahedral models, which we used to create the examples discussed in Section 5.  2 Model 
Speci.cation Many real-world objects are composed of layers: architectural framing, insulation and siding; 
the skeleton, muscles, and skin of an animal; or the peel of a fruit. Building a physically-realistic 
model of any of these objects requires a description of the boundaries be­tween materials and the variations 
within each material. Such a model could be created by an artist, but the process would be time­consuming. 
The data could be obtained through tomography or dis­section approaches, but this can be inaccurate or 
destructive. Our modeling language is based on the observation that often the inter­nal structure of 
an object can be inferred from a representation of its primary interface. Our basic building block is 
the layered vol­ume. Within our framework, volumes can be combined, and layer composition can be controlled 
procedurally. Through a series of examples based on a simple model of a chocolate candy, we show that 
our language provides a natural and expressive way to construct volumetric models. We include frag­ments 
of code from the scripts used to generate the images in this paper. As a convention in our examples, 
we use all capital letters to indicate user-de.ned functions and materials. A simpli.ed gram­mar for 
the language appears in the appendix. 2.1 Layers of Material To construct a volume with an interesting 
internal structure, we build layers of material from the primary surface. In our .rst ex­ample, we begin 
with a simple candy-shaped surface mesh and add two layers of material to the exterior and one layer 
to the interior (Figure 1). Figure 1: A chocolate candy created with two layers exterior to the original 
surface and one layer to .ll the interior. The outermost layer has a procedural de.nition to create stripes 
of chocolate. STRIPED_CANDY = volume { distance_field = surface_mesh { file = candy.obj } layers = { 
 interior_layer { material = CHOCOLATE thickness = fill } exterior_layer { material = WHITE_CHOCOLATE 
thickness = 0.10 } exterior_layer { material = STRIPED_CHOCOLATE thickness = 0.05 }}} Each layer has 
a material type and thickness. The type and thickness can be uniform or vary procedurally, which we discuss 
later. The thickness keyword fillcan be used with a well-de.ned closed mesh to describe an interior layer 
that is thick enough to .ll the remaining interior space. The material keyword nothingcan be used to 
describe a layer of air with no volumetric properties. 2.2 Procedural Layer and Material De.nitions 
Materials are de.ned by a list of rendering and simulation parame­ters. We have a small library of built-in 
materials; additional mate­rials can be de.ned within the script .le as shown below. Default values are 
assigned to any unspeci.ed parameters. CHOCOLATE = material { color = { 0.31 0.17 0.15 } density = 1100 
/* kg/m 3 */ etc. } A layer need not be composed of a uniform material. The user can procedurally de.ne 
a continuous variation of properties, such as wood grain or concrete particles, within a single material. 
This information can be used by a simulation and during rendering. Al­ternatively, a procedure can be 
used to subdivide the layer into dis­tinct materials. Below is the speci.cation used to create the striped 
layer of chocolate on the candy. Material* STRIPED_CHOCOLATE(Vec3f &#38;p) { if (p.y() < 0.2) return 
Lookup("WHITE_CHOCOLATE"); if ((p.x() > -0.9 &#38;&#38; p.x() < -0.7) || (p.x() > -0.5 &#38;&#38; p.x() 
< -0.3) || (p.x() > -0.1 &#38;&#38; p.x() < 0.1) || (p.x() > 0.3 &#38;&#38; p.x() < 0.5) || (p.x() > 
0.7 &#38;&#38; p.x() < 0.9)) return Lookup("CHOCOLATE"); return Lookup("WHITE_CHOCOLATE"); } Within 
a script, the user can de.ne arbitrary C++ functions with default values for optional parameters. These 
functions are com­piled and linked at runtime to interface with the core system. The Lookup function 
gives access to script .le variable assignments. Function calls consist of the function name and a list 
of name = value pairs within curly braces. The arguments may appear out of order, or be left unspeci.ed 
if optional. 2.3 Volume Speci.cation Many objects we would like to model are more complicated than simply 
layers of material constructed from a primary interface. Of­ten these objects can be easily described 
as a collection of overlap­ping shapes. In our language, we use the precedence construct to combine volumes. 
In the example below, precedence is used to .rst create the volume for the almond, and then de.ne the 
candy shape around the almond (Figure 2a). Subsequent shapes could be de.ned to .ll the remaining unoccupied 
space. ALMOND_CANDY = precedence { volume_1 = volume { distance_field = surface_mesh { file = almond.obj 
layers = { interior_layer { material = NUT thickness = fill }}} volume_2 = volume { distance_field 
= surface_mesh { file = candy.obj } layers = { interior_layer { material = CHOCOLATE thickness = fill 
} exterior_layer { material = WHITE_CHOCOLATE thickness = 0.10 } exterior_layer { material = STRIPED_CHOCOLATE 
thickness = 0.05 }}}} The use of the precedence operator is particularly interesting when the surface 
meshes intersect. In Figure 2b the almond shape is larger and rotated so that it protrudes from the original 
candy surface and beyond the additional layers of material. However, the user may instead wish the outer 
layers to be wrapped around the protruding almond as shown in Figure 2c. To do this, we use a volume 
as the primary shape for a new volume. First, we use prece­dence to combine the almond shape with the 
interior layer of the chocolate. Then, we extract the outermost interface of the volume to use as the 
initializing surface for the second volume that adds the exterior layers of chocolate. ALMOND_CANDY_2 
= volume { distance_field = from_volume_surface { volume = precedence { volume_1 = volume { distance_field 
= surface_mesh { file = almond.obj } layers = { interior_layer { material = NUT thickness = fill }}} 
volume_2 = volume { distance_field = surface_mesh { file = candy.obj } layers = { interior_layer { material 
= CHOCOLATE thickness = fill }}}}} layers = { exterior_layer { material = WHITE_CHOCOLATE thickness = 
0.10 } exterior_layer { material = STRIPED_CHOCOLATE thickness = 0.05 }}}  2.4 Signed Distance Field 
Signed distance .elds are a natural choice for describing and imple­menting the layers and volumes in 
our language. A signed distance .eld is a continuous scalar function de.ned throughout a volume, which 
can be used to compute offset isosurfaces while elegantly handling changes in topology and preventing 
self-intersection of the interfaces. In most cases, we initialize the distance .eld from a surface mesh 
using the method described in Section 4.2. Alter­natively, we can create the .eld from an implicit surface 
or other Figure 2: Specifying the interaction of two meshes allows many other possibilities: a) simple 
precedence to create the candy around an almond, b) &#38; c) precedence with intersecting meshes, and 
d) union of the candy and almond meshes. function. The layers of a volume are implemented as ranges 
of dis­tance values. Often the desired distance .eld is most easily described by com­bining distance 
.elds using simple operators such as scaling, union (minimum), intersection (maximum), and subtraction 
[Ricci 1973; Frisken et al. 2000]. To demonstrate distance .eld composition, we use the union operator 
to combine the candy and almond surface meshes to produce the volume shown in Figure 2d. The zero iso­surface 
of the resulting shape lies between the chocolate and white chocolate layers. UNION_CANDY = volume { 
distance_field = union { distance_field_1 = surface_mesh { file = almond.obj } distance_field_2 = surface_mesh 
{ file = candy.obj }} layers = { interior_layer { material = CHOCOLATE thickness = 0.2 } interior_layer 
{ material = PINK_FROSTING thickness = fill } exterior_layer { material = WHITE_CHOCOLATE thickness 
= 0.15 }}} Usually, a distance .eld is simply a Euclidean measurement from each point to the original 
surface. Layers de.ned within this type of distance .eld will have uniform thickness within each layer. 
How­ever, it is often natural to describe layers that are thicker or thinner according to some pattern. 
To create interesting internal structures that have varying layer thicknesses, we can de.ne non-Euclidean 
distance metrics by modifying the interface velocity. The spacing between isosurfaces in a distance .eld 
is greater where the velocity is higher. The user may de.ne a pattern of increased velocity by painting 
on the surface as shown in Figure 3a. Alternatively, the interface velocity can be de.ned procedurally: 
in Figure 3b the ve­locity is set by a random turbulence function, resulting in a bumpy appearance, and 
in Figure 3c a short procedure creates a diagonal swirl. The velocity can also be computed using visibility, 
accessi­bility, etc. Interface velocity is implemented per distance .eld, and all lay­ers within that 
.eld have a thickness pattern based on that velocity. Nesting volume speci.cations allows us to create 
a model with lay­ers having different thickness patterns. For example, in Figure 3d we build a layer 
of bumpy frosting from a turbulent velocity .eld followed by a layer of foil wrapper with a diagonal 
pattern. This type of speci.cation is common enough to warrant a syntactic sugar construct, which desugars 
velocities speci.ed per layer into nested volume speci.cations. LUMPY_CANDY = volume { distance_field 
= surface_mesh { file = candy.obj } layers = { exterior_layer { material = PINK_FROSTING thickness = 
0.1 velocity = BUMPY } exterior_layer { material = FOIL_WRAPPER thickness = 0.05 velocity = DIAGONAL 
}}} is equivalent to: LUMPY_CANDY = volume { distance_field = from_volume_surface { volume = volume { 
distance_field = surface_mesh { file = candy.obj velocity = BUMPY } layers = { exterior_layer { material 
= PINK_FROSTING thickness = 0.1 }}} velocity = DIAGONAL } layers = { exterior_layer { material = FOIL_WRAPPER 
 thickness = 0.05 }}}  3 Operations In the previous section we discussed how our language is used to 
initialize a volumetric model. The advantages of these models become apparent when visualized and modi.ed 
in complex ways. Many simulation techniques have been developed for sculpting and weathering [Dorsey 
et al. 1996; Dorsey et al. 1999; O Brien and Hodgins 1999]. We have incorporated implementations of a 
few of these techniques into our system and provide user control of these tools through our language. 
The user is able to develop additional tools based on these packages or link to other simulation libraries. 
3.1 Usability through Abstraction One of the main obstacles the user must overcome in using a sim­ulation 
package is determining proper values for the numerous pa­rameters needed to control the system. Different 
implementations of the same simulation technique may require different sets of pa­rameters. The .rst 
goal of our tool interface is to provide abstrac­tion and standardization so the user of the tool can 
apply operations to the model without studying the details of the implementation. A simple interface 
between each simulation package and our system is established and a set of sample tools is created. Each 
tool de.nes default values for standard parameters such as position, orientation, size, and affected 
materials and calls one or more simulation pack­ages. Using the sample tools as a guide, the user can 
create new tools. We have linked our system to a .exible FEM simulation. We apply a distribution of forces 
to our model and the system computes Figure 3: By modifying the interface velocity of the distance .eld, 
we can create layers with non-uniform thickness: a) painted ve­locity, b) turbulent velocity for a bumpy 
appearance c) procedurally created diagonal stripes, and d) a diagonal layer on top of the bumpy layer. 
Images e and f are cross-sections of c and d respectively. the appropriate deformations and fractures. 
We can also control which materials are affected by the simulation; no other materials will be modi.ed. 
Below we de.ne a simple tool which applies a single hammer-like force to the model. void HAMMER(Model 
*model, Vec3f position = Vec3f(0,0,0), Vec3f orientation = Vec3f(1,0,0), float magnitude = 1.0, float 
size = 1.0, List<Material*> *affects = NULL) { Vec3f force = orientation; force *= magnitude; AppliedArea 
*a = GaussSphere(position,size); FEM(model,a,force,affects); } Below is an example use of this tool. 
HAMMER { model = BRONZE_CAT position = { 1.08 0.79 0.29 } orientation = { -0.32 -0.26 -0.91 } affects 
= { FIRED_CLAY }}  3.2 De.ning Simulation Behavior The power of a language for tool de.nition extends 
beyond copying and modifying existing tools. The language facilitates the speci.­cation of new types 
of behavior for the simulation. Particle systems have been used in many different applications to create 
a variety of effects that span a wide range of physical accuracy. The complexity of a particle system 
simulation depends on the de.nition of particle motion, interaction, and effects. Below we present the 
de.nition of a tool used to wash dirt from a statue. void WASH(Model *model, int num_particles = 10000, 
 float particle_life = 1) { Function *initialize = VerticalFall; Function *motion = Lookup("CLINGING"); 
 Function *action = Lookup("REMOVE_DIRT"); ParticleSystem(model,num_particles,particle_life, initialize,motion,action); 
} The particle motion and action functions de.ned below each take two arguments: the particle to move, 
and the model with which it interacts. Motion functions that compute interactions between particles would 
also need the list of all particles as an argument. void REMOVE_DIRT(Model *m, Particle *p) { Vec3f clean_color 
= Vec3f(1,1,1); List<Vertex*> vlist; float radius = 0.1; m->CollectVertices(vlist,p->pos(),radius); 
 for (int i = 0; i < vlist.numElements(); i++) { Vertex *v = vlist.getElement(i); v->BlendColor(clean_color, 
 p->pos(),radius); }} void CLINGING(Model *m, Particle *p) { Vec3f n; m->NormalAt(p->pos(),n); if (n.dot(Gravity) 
> cos(p->FallingAngle())) p->Drip(m,Gravity); else p->MoveAlongMesh(m,Gravity); } In the CLINGINGmotion 
function, smaller values for the falling angle result in .ow that behaves with greater surface tension. 
 3.3 Interactive Sculpting Choosing the appropriate position, orientation, and radius for the types of 
tools described above can be tedious for complex mod­els. Our language can also be used as an intermediate 
format for an interactive sculpting program. A simpli.ed version of the volumet­ric model can be sculpted 
interactively and the actions saved. The logged actions can be edited by hand or simply appended to a 
script .le that is run of.ine on the high resolution model.  4 Volumetric Representation Our scripting 
language was designed to provide great freedom in model speci.cation, independent of the underlying implementation 
of the volume data structures. In our implementation we use tetra­hedral meshes to represent volumetric 
models. In this section we discuss some speci.cs of this implementation. Additionally, the system could 
maintain and convert between other volumetric repre­sentations that are more advantageous for certain 
operations. 4.1 Tetrahedral Mesh Our volumetric representation consists of a set of tetrahedra, where 
each tetrahedron stores pointers to its four vertices and the four neighbors sharing its faces. Generally, 
neighbors are tetrahedra, but those tetrahedra with a face on the visible interface have a triangle neighbor 
that stores rendering information such as vertex normals and texture coordinates. The list of visible 
interface triangles forms a watertight mesh and is used for interactive display and of.ine ren­dering. 
Each tetrahedron stores its material type and any additional sub-tetrahedron material variations. We 
can also ef.ciently extract the set of faces that de.ne the interfaces between different materi­als. 
These faces are necessary to accurately render refraction and translucency for non-opaque materials. 
We have chosen a tetrahedral mesh because it offers many advan­tages in this application over other volumetric 
techniques, such as voxels or octree-based volumes [Wang and Kaufman 1995; Frisken et al. 2000]. With 
a tetrahedral mesh, we have a simple correla­tion between volume and surface, and the corresponding triangle 
mesh is easy to render on graphics hardware. The visible and inte­rior interfaces can be represented 
at variable resolutions and model sharp creases in the geometry accurately. The data structure is in­herently 
adaptive, allowing more tetrahedra in areas of high detail. Tetrahedral meshes are a simple extension 
of triangle meshes, and their geometric properties, such as simpli.cation and subdivision, are well understood. 
Finally, many popular simulation techniques such as FEM are designed to work on tetrahedral meshes. Axis­aligned 
volumetric techniques such as voxels or octree-based dis­tance .elds are poorly suited to handle operations 
that deform or fracture the model. 4.2 Evaluating the Signed Distance Field We synthesize tetrahedral 
models from triangle meshes by evalu­ating the signed distance .eld (discussed in Section 2.4) on a uni­form 
3D grid. The system determines a default grid based on the bounding box of the function or surface mesh, 
but it can be overrid­den by the user in the script .le. We compute the distance value at each grid point 
using the Fast Marching Level Set method described by Sethian [1999], which elegantly avoids self-intersections 
when computing isosurfaces. Given surface S, a signed distance function fS is de.ned as fol­lows: for 
any point p in R3 , the magnitude of fS (p) is the distance from p to the closest point on S, and the 
sign of fS (p) is negative if p lies in the interior volume of S and positive if it lies outside. We 
initialize a band of known vertices near the original surface by iter­ating over the faces in the surface 
mesh and rasterizing each face F into the volume grid. For all grid points p near F , we update fS (p) 
iff |fF (p)| < |fS (p)|. To compute fF (p), the signed dis­tance from point p to F , we .nd point p . 
on F closest to p. Then, |fF (p)| = .p - p . . and the sign of fF (p) is obtained as the sign of (p . 
- p) · n, where n is the surface normal at p . . To make this scheme robust, if p . lies on a vertex 
or edge of F , the normal n must be obtained by averaging the normals of adja­cent faces.2 After all 
faces have been rasterized, the function fS is de.ned in the proximity of S. We propagate the distance 
of each known vertex to its neighbors, which are then marked trial. The trial vertices are stored in 
a priority queue by magnitude, and start­ing with the smallest distance, they are marked known and propa­gated 
to their neighbors until the necessary layer thickness has been de.ned. 4.3 Tetrahedral Mesh Generation 
Once the signed distance function has been initialized, we use a standard method for creating tetrahedral 
meshes a structured method based on an axis-aligned grid or octree [Yerry and Shep­hard 1984; Wyvill 
et al. 1986; Lorensen and Cline 1987; Bloo­menthal 1994]. The octree method is robust and simpler to 
imple­ment than unstructured methods such as advancing front and De­launay methods [Lohner 1988; Baker 
1989]. Unstructured methods produce a mesh independent of object orientation and attempt to match the 
vertices and faces of the original mesh. In our applica­tion, we handle large scanned meshes and matching 
the surface is 2Nooruddin and Turk [2000] present an alternative approach for obtain­ing the signed distance 
.eld which does not require a watertight mesh. usually unnecessary and even undesirable. Both structured 
and un­structured mesh generation techniques are usually used in conjunc­tion with mesh simpli.cation 
and optimization, which we discuss in Section 4.4. First each cubic grid cell is divided into .ve tetrahedral 
cells, alternating the orientation of the central tetrahedron so that diag­onals match on neighboring 
cubic cells. We chose not to use the six-tetrahedron decomposition because it results in more tetrahe­dra 
and requires interpolation along the long diagonal of the cube, which can lead to additional artifacts 
on material interfaces. We illustrate our technique with a 2D example. Each tetrahedral cell is then 
divided into tetrahedra of the appro­priate materials, similar to Nielson et al. [1997]. If the distance 
val­ues of all four vertices of a tetrahedral cell are within the range for a single layer, one tetrahedron 
of that material is created. If the ver­tices are within different layer ranges, we split the tetrahedral 
cell into two cells by splitting one of its edges at an interface crossing (neighboring tetrahedral cells 
sharing that edge are also split) and recurse. A protocol for ordering edge splits based on vertex and 
interface identi.ers guarantees a proper mesh with matching tetra­hedral faces. This algorithm places 
no constraints on the thickness of layers or the number of interface crossings allowed per tetrahe­dral 
cell. Using precedence to combine volume descriptions (Section 2.3) can introduce non-manifold interfaces 
[Bloomenthal and Ferguson 1995]. If a tetrahedral cell is not assigned material by the .rst volume description 
in a precedence operation, we proceed to the next volume description. When we split a tetrahedral cell, 
we also split tetrahedra assigned by previous volume descriptions that share the edge to be split. This 
operation prevents T-junctions at non­manifold interface intersections and is performed ef.ciently using 
a hash table of all tetrahedral edges. After the volume has been tetrahedralized, tetrahedra for layers 
with procedural descriptions (Section 2.2) are subdivided as neces­sary to correctly assign materials. 
 4.4 Simpli.cation of Models for Simulation The structured mesh generation technique described in Section 
4.3 produces a large number of tetrahedra and poorly shaped tetrahe­dra [Shewchuk 1998] when an interface 
passes very close to the grid points. Many simulation techniques require tetrahedra to be well-proportioned, 
which is often measured by the minimum solid angle [Fleischmann et al. 1999]. We have several methods 
to reduce the overall number of tetrahedra and improve their shape. To obtain a high resolution interface, 
we require a high resolu­tion grid; however, if a material layer is thick relative to the grid, this 
leads to extraneous tetrahedra within the layer. An adaptive octree approach dramatically reduces the 
initial number of tetra­hedra produced, as illustrated in Figure 4. Similarly to Frisken et Figure 4: 
The mesh on the left was created from a uniform distance .eld. The mesh on the right was created from 
the same distance .eld after adaptive re.nement, resulting in less than half as many tetrahedra. The 
meshes have similar interface quality. Simpli.ca­tion can be used to further reduce the size of the model. 
 al. [2000], we compute the signed distance .eld on a uniform grid, then collapse grid cells that are 
accurately represented by interpo­lation or do not contain an interface crossing. We restrict the grid 
cell collapses such that cells sharing faces are no more than one level different in the octree. This 
restriction bounds the minimum solid angle of intermediate tetrahedral cells. Additionally, the user 
can specify that certain interfaces must be represented at a higher resolution and with more accuracy. 
After the initial tetrahedralization, we use a combination of sim­pli.cation and mesh improvement techniques 
[Hoppe 1996; Staadt and Gross 1998; Trotts et al. 1999; Cignoni et al. 2000]. We found it dif.cult to 
de.ne an appropriate edge collapse weighting func­tion (used in the Progressive Mesh techniques) that 
simultaneously solved our goals. Our solution is similar to the mesh improvement strategy described by 
Freitag and Ollivier-Gooch [1997] and has been ef.cient and effective in practice. First, we compute 
a quality metric (ranging from 0 to 10) for each tetrahedron t, which can vary depending on the exact 
require­ments of the simulation we plan to run. The equations below reward tetrahedra that are close 
to equilateral (minimum solid angle .0.55 steradians) and have volume close to the ideal volume (total 
model volume / desired tetrahedral count). We use a = 0.7. Quality(t) = a * A(t) + (1 - a) * V(t) A(t) 
= 10 * min (1, J2 * min solid angle(t)) V(t) = 10 * min (1, volume(t) ideal volume ) We target the removal 
or improvement of low-quality tetrahedra while maintaining the visible and interior interfaces (using, 
e.g., quadric error [Garland and Heckbert 1997] or volume preserva­tion). Our simpli.cation strategy 
is outlined in the following psue­docode. for q = 0 to 10 T = { all tetrahedra with Quality(t) < q } 
for each t in T try these actions: 3 -2, 2 -3, and 2 -2 tetrahedral .ips  half edge collapses  move 
each vertex to the average of its neighbors  We choose not to perform an action if the interface is 
unacceptably degraded, or if the minimum quality of the affected tetrahedra after the action is lower 
than the minimum quality before the action. If a  Figure 5: Lost wax casting. Adapted from Hodges [1970]. 
stopping criterion (such as a desired number of tetrahedra) has not been met, the interface requirements 
are reduced and the process is repeated.  5 Results In this section we present three illustrative examples 
from our sys­tem. We describe our artistic intentions for each model based on its environment and history. 
5.1 Lost Wax Casting The lost wax casting process is a common technique for creating bronze statues (Figure 
5). A roughly-shaped clay core is covered with malleable wax, in which the shape and details of the .nal 
sculpture are formed. When the wax sculpture is .nished, a thick layer of clay is spread over the wax. 
The model is slowly heated to allow the wax to drip from the clay mold and then the mold is .red in a 
kiln. Molten bronze is poured into the hardened clay mold. Finally, when cool, the brittle clay is chipped 
away to reveal the bronze statue. The original cat surface has sharp edges and areas of high cur­vature, 
but the outer clay layer does not contain such detail. In the physical process, the artist applies a 
thicker layer of clay to the concave portions of the model. To model this process, we use the convex 
hull of the original surface as a second mesh. BRONZE_CAT = precedence { volume_1 = volume { distance_field 
= surface_mesh { file = cat.obj } layers = { interior_layer { material = BRONZE thickness = 1 } interior_layer 
{ material = FIRED_CLAY thickness = fill }}} volume_2 = volume { distance_field = surface_mesh { file 
= cat_hull.obj } layers = { interior_layer { material = FIRED_CLAY thickness = fill } exterior_layer 
{ material = FIRED_CLAY thickness = 2.5 }}}}  Figure 6: A sequence of images from our bronze statue 
simulation. The outer layer of .red clay is broken away using a hammer tool. A polish tool is used to 
clean and shine the model. We used the hammer tool to break away the outer layer of clay, by specifying 
that only .red clay tetrahedra are affected. The tool is used repeatedly on different portions of the 
model. We also de­signed a polish tool to clean and shine the statue. This tool performs a CSG subtraction 
operation to remove clay left on or around the model. Subtraction is implemented in our system by subdivision 
and tetrahedron removal. The polish tool also increases the shini­ness of nearby tetrahedra, by blending 
with the SHINY BRONZE material. void POLISH(Model *model, Vec3f position = Vec3f(0,0,0), float size = 
1) { AppliedArea *a = Sphere(position, size); List<Material*> affects(Lookup("FIRED_CLAY")); CSG_Subtract(model,a,affects); 
List<Tetra*> lst; model->CollectTetras(lst,position,size); for (int i = 0; i < lst.numElements(); i++) 
{ Tetra *t = lst.getElement(i); t->BlendMaterial(Lookup("SHINY_BRONZE"), position,size); }} We interactively 
sculpted a model of approximately 100,000 tetrahedra, and replayed the operations on a model with 300,000 
tetrahedra [M¨uller et al. 2001]. A sequence from this simulation is shown in Figure 6. 5.2 Displaced 
Brick Paving In our next example, we model a tree in an urban setting surrounded by brick paving. As 
the tree grows, the roots push upward, shifting the bricks. Here is the script we used to produce the 
initial model. URBAN_TREE = precedence { volume_1 = volume { distance_field = union { distance_field_1 
= TRUNK distance_field_2 = 2D_EXTRUDE { file = roots.ppm }} layers = { interior_layer { material = TREE 
thickness = fill }}} volume_2 = volume { distance_field = GROUND_PLANE layers = { interior_layer { 
material = BRICK_PAVING thickness = 0.075 } interior_layer { material = DIRT thickness = 1.00 }}}} We 
created an abstract tree model using our language: the trunk is represented with an implicit function 
for a cylinder plus turbu­lence, and the roots are procedurally created from a simple 2D sketch. The 
brick paving is created with a procedural de.nition similar to the striped chocolate de.nition in Section 
2.2. The simpli.ed model has approximately 200,000 tetrahedra. To displace the brick paving around the 
tree, we created a tool to translate upward the vertices of all tree tetrahedra. The FEM system is used 
to solve for the static equilibrium positions of the remaining vertices. The results are shown in Figure 
7. The bricks maintain their rectilinear shape because the brick material has a large value Figure 7: 
We simulate tree growth by translating all tree vertices upward and deforming the dirt and bricks around 
the roots. for the elasticity parameter; the dirt between and beneath the bricks deforms easily because 
of its relatively smaller value. Appropriate values for these materials can be obtained from standard 
references [Anderson 1989]. 5.3 Weathered Statue In Figure 8, we show the layering of weathering effects 
on a gar­goyle statue mounted on the exterior of a building. Gargoyles are subjected to interesting .ow 
patterns because they were originally used as decorative downspouts to direct rainwater away from build­ing 
foundations. Long term exposure causes a variety of effects on exterior architectural details including 
discoloration, weakening, erosion, biological growth, and fracture due to the freeze/thaw cy­cle. The 
model shown here was created from a scanned mesh as one layer of stone. We use several tools built on 
our particle system that use differ­ent procedures for particle motion and action. First, we apply an 
even layer of dirt to the model and use the wash tool to remove dirt according to rain .ow. The FEM hammer 
tool is used to break off the ear and a corner of the wing. Next, an erosion tool moves particles toward 
exposed areas of the mesh where a small sphere of material is removed. Finally, we apply a biological 
growth tool similar to the wash tool, but with minimal particle motion, resulting in lichen-colored discoloration 
on the top-facing surfaces. Below is the script used to modify the model, which after simpli.cation con­tained 
approximately 500,000 tetrahedra. DIRT { model = GARGOYLE color = { 0.5 0.5 0.5 }} WASH { model = GARGOYLE 
 num_particles = 200000 particle_life = 1.0 } HAMMER { model = GARGOYLE position = { -0.78 1.22 0.77 
} orientation = { -0.23 -0.47 0.85 }} HAMMER { model = GARGOYLE position = { -2.53 1.03 1.06 } orientation 
= { 0.56 -0.19 -0.80 }} ERODE { model = GARGOYLE num_particles = 2000 } LICHEN { model = GARGOYLE num_particles 
= 40000 }  6 Discussion and Future Work We have presented a procedural framework for specifying layered 
solid models and applying a series of simulation operations to them. Our approach allows complex volumetric 
models to be constructed from existing triangle meshes as well as from implicit functions and distance 
.elds. These different modeling approaches are han­dled seamlessly within our high-level framework. These 
models can then be easily modi.ed using procedural simulation tools. Ours is one of the .rst modeling 
systems where simulation is treated as a sculpting tool rather than merely for animation, and we think 
this approach has tremendous potential. In general, it pro­vides both a higher level of abstraction for, 
and a convenient inter­face to, existing simulation environments. Our scripting language is also valuable 
as an intermediate .le representation for capturing the history of interactive sculpting operations. 
Our system has been used to successfully construct models for a wide range of rendering, simulation, 
and animation applications. We have built small models, with a few hundred tetrahedra, for use in real-time 
animation research [M¨ uller et al. 2001], as well as large models with millions of tetrahedra for off-line 
weathering and ero­sion simulations. In fact, models at either resolution can be con­structed from essentially 
the same script. In the future, we plan to expand our language to incorporate new modeling and simulation 
tools. We would like to alternate between the various phases of modeling and simulation more readily. 
We would also like to add better procedural support for volume gener­ation, perhaps incorporating support 
for materials, such as cement­based products, which have intricate internal structures. Overall, we believe 
that a procedural interface between model­ing and simulation is an important tool for our community. 
With our prototype framework, we have experienced a dramatic increase in modeling productivity and .exibility, 
smoothed transitions of mod­els between simulation and rendering applications, and provided access to 
complex simulation systems to novice users. 7 Acknowledgments We would like to thank Hugues Hoppe for 
helpful discussions, Justin Legakis for the use of his rendering software, and Stephen Duck for the architectural 
model in the gargoyle renderings. This Figure 8: A sequence of renderings from the gargoyle simulation: 
work was supported by NSF grants CCR-9988535, CCR-0072690, the initial model made of fresh white stone; 
a layer of dirt is applied and EIA-9802220 and by a gift from Pixar Animation Studios. and partially 
washed away by rain; fracture removes the gargoyle s ear and wing, erosion affects top surfaces; biological 
growth. A Modeling Language Type grammar script : ( assignment | operation )* assignment : identi.er 
= value operation : function { assignment* } value : integer | .oat | string | material | layer | distance 
.eld | volume | function |{ value* } distance .eld : function: Vec3f . .oat substance : material | function: 
Vec3f . material | nothing thickness : .oat | fill velocity : .oat | function: Vec3f . .oat  Selected 
built-in functions material material (string name,.oat density = 1.0,etc. ); layer interior layer (substance 
material, thickness thickness = 1.0 ); layer exterior layer (substance material, thickness thickness 
= 1.0 ); distance .eld surface mesh (string file, velocity velocity = 1.0 ); distance .eld union (distance 
.eld distance field 1, distance .eld distance field 2, velocity velocity = 1.0 ); distance .eld from 
volume surface (volume volume, velocity velocity = 1.0); volume load (string file); volume volume (distance 
.eld distance field,layers layers); volume precedence (volume volume 1,volume volume 2);  References 
ADZHIEV, V., CARTWRIGHT, R., FAUSETT, E., OSSIPOV, A., PASKO, A., AND SAVCHENKO, V. 1999. HyperFun Project: 
A framework for collaborative multi­dimensional F-rep modeling. In Proceedings of Implicit Surfaces 99, 
59 69. ANDERSON, H. L., Ed. 1989. A Physicist s Desk Reference, 2nd ed. American Institute of Physics, 
New York. BAKER, T. J. 1989. Automatic mesh generation for complex three-dimensional re­gions using a 
constrained delaunay triangulation. Engineering with Computers, 5, 161 175. BLOOMENTHAL, J., AND FERGUSON, 
K. 1995. Polygonization of non-manifold implicit surfaces. In Proceedings of ACM SIGGRAPH 95, Computer 
Graphics Proceedings, Annual Conference Series, 309 316. BLOOMENTHAL, J. 1994. An implicit surface polygonizer. 
In Graphics Gems IV. Academic Press, Boston, 324 349. CIGNONI, P., COSTANZA, D., MONTANI, C., ROCCHINI, 
C., AND SCOPIGNO, R. 2000. Simpli.cation of tetrahedral meshes with accurate error evaluation. In IEEE 
Visualization 2000, 85 92. COOK, R. L. 1984. Shade trees. In Computer Graphics (Proceedings of ACM SIG-GRAPH 
84), 18(3), 223 231. COQUILLART, S. 1990. Extended free-form deformation: A sculpturing tool for 3d geometric 
modeling. In Computer Graphics (Proceedings of SIGGRAPH 90), 24(4), 187 196. DORSEY, J., PEDERSEN, H. 
K., AND HANRAHAN, P. M. 1996. Flow and changes in appearance. In Proceedings of ACM SIGGRAPH 96, Computer 
Graphics Proceed­ings, Annual Conference Series, 411 420. DORSEY, J., EDELMAN, A., LEGAKIS, J., JENSEN, 
H. W., AND PEDERSEN, H. K. 1999. Modeling and rendering of weathered stone. In Proceedings of ACM SIG-GRAPH 
99, Computer Graphics Proceedings, Annual Conference Series, 225 234. EBERT, D. S., MUSGRAVE, F. K., 
PEACHEY, D., PERLIN, K., AND WORLEY, S. 1998. Texturing &#38; Modeling, 2nd ed. Academic Press. FLEISCHMANN, 
P., KOSIK, R., HAINDL, B., AND SLBERHERR, S. 1999. Simple examples to illustrate speci.c .nite element 
mesh requirements. In Proceedings of the 8th International Meshing Roundtable, 241 246. FREITAG, L. A., 
AND OLLIVIER-GOOCH, C. 1997. Tetrahedral mesh improvement using swapping and smoothing. International 
Journal for Numerical Methods in Engineering, vol. 40, 3979 4002. FRISKEN, S. F., PERRY, R. N., ROCKWOOD, 
A. P., AND JONES, T. R. 2000. Adaptively sampled distance .elds: A general representation of shape for 
com­puter graphics. In Proceedings of ACM SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference 
Series, 249 254. GARLAND, M., AND HECKBERT, P. S. 1997. Surface simpli.cation using quadric error metrics. 
In Proceedings of ACM SIGGRAPH 97, Computer Graphics Pro­ceedings, Annual Conference Series, 209 216. 
HANRAHAN, P., AND LAWSON, J. 1990. A language for shading and lighting cal­culations. In Computer Graphics 
(Proceedings of ACM SIGGRAPH 90), 24(4), 289 298. HOPPE, H. 1996. Progressive meshes. In Proceedings 
of ACM SIGGRAPH 96, Computer Graphics Proceedings, Annual Conference Series, 99 108. LEGAKIS, J., DORSEY, 
J., AND GORTLER, S. J. 2001. Feature-based cellular textur­ing for architectural models. In Proceedings 
of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 309 316. LOHNER, R. 1988. 
Generation of three-dimensional unstructured grids by the advanc­ing front method. In International Journal 
for Numerical Methods in Fluids, vol. 8, 1135 1149. LORENSEN, W. E., AND CLINE, H. E. 1987. Marching 
cubes: A high resolution 3d surface construction algorithm. In Computer Graphics (Proceedings of ACM 
SIGGRAPH 87), 21(4), 163 169. MIZUNO, S., OKADA, M., AND ICHIRO TORIWAKI, J. 1998. Virtual sculpting 
and virtual woodcut printing. The Visual Computer, 14(2), 39 51. MULLER¨ , M., DORSEY, J., MCMILLAN, 
L., AND JAGNOW, R. 2001. Real-time simulation of deformation and fracture of stiff materials. In Proceedings 
of Euro­graphics Workshop on Animation and Simulation 2001, 113 124. NIELSON, G. M., AND SUNG, J. 1997. 
Interval volume tetrahedrization. In IEEE Visualization 97, 221 228. NOORUDDIN, F. S., AND TURK, G. 2000. 
Interior/exterior classi.cation of polygonal models. In IEEE Visualization 2000, 415 422. O BRIEN, J. 
F., AND HODGINS, J. K. 1999. Graphical modeling and animation of brittle fracture. In Proceedings of 
ACM SIGGRAPH 99, Computer Graphics Proceedings, Annual Conference Series, 137 146. PARISH, Y. I. H., 
AND M¨ ULLER, P. 2001. Procedural modeling of cities. In Proceed­ings of ACM SIGGRAPH 2001, Computer 
Graphics Proceedings, Annual Confer­ence Series, 301 308. PAYNE, B. A., AND TOGA, A. W. 1992. Distance 
.eld manipulation of surface models. IEEE Computer Graphics &#38; Applications, 12(1), 65 71. PERLIN, 
K., AND HOFFERT, E. M. 1989. Hypertexture. In Computer Graphics (Proceedings of ACM SIGGRAPH 89), 23(3), 
253 262. PERLIN, K. 1985. An image synthesizer. In Computer Graphics (Proceedings of ACM SIGGRAPH 85), 
19(3), 287 296. PRUSINKIEWICZ, P., LINDENMAYER, A., AND HANAN, J. 1988. Developmental models of herbaceous 
plants for computer imagery purposes. In Computer Graph­ics (Proceedings of ACM SIGGRAPH 88), 22(4), 
141 150. RAVIV, A., AND ELBER, G. 2000. Three-dimensional freeform sculpting via zero sets of scalar 
trivariate functions. Computer-Aided Design, 32(8-9), 513 526. RICCI, A. 1973. A constructive geometry 
for computer graphics. The Computer Journal, 16(2), 157 160. SETHIAN, J. A. 1999. Level Set Methods and 
Fast Marching Methods, 2nd ed. Cam­bridge University Press, Cambridge, United Kingdom. SHEWCHUK, J. R. 
1998. Tetrahedral mesh generation by delaunay re.nement. In Proceedings of the 14th Annual Symposium 
on Computational Geometry, 86 95. STAADT, O. G., AND GROSS, M. H. 1998. Progressive tetrahedralizations. 
In IEEE Visualization 98, 397 402. TROTTS, I. J., HAMANN, B., AND JOY, K. I. 1999. Simpli.cation of tetrahedral 
meshes with error bounds. IEEE Transactions on Visualization and Computer Graphics, 5(3), 224 237. UPSTILL, 
S. 1990. The Renderman Companion : A Programmer s Guide to Realistic Computer Graphics. Addison-Wesley. 
WANG, S. W., AND KAUFMAN, A. E. 1995. Volume sculpting. In Symposium on Interactive 3D Graphics, ACM 
Press, 151 156. WYVILL, B., MCPHEETERS, C., AND WYVILL, G. 1986. Data structure for soft objects. The 
Visual Computer, 2(4), 227 234. WYVILL, B., GUY, A., AND GALIN, E. 1999. Extending the CSG tree. Warping, 
blending and boolean operations in an implicit surface modeling system. Computer Graphics Forum, 18(2), 
149 158. YERRY, M. A., AND SHEPHARD, M. S. 1984. Automatic three-dimensional mesh generation by the modi.ed 
octree technique. International Journal For Numerical Methods in Engineering, 20, 1965 1990.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566582</section_id>
		<sort_key>312</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Geometry]]></section_title>
		<section_page_from>312</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14079289</person_id>
				<author_profile_id><![CDATA[81100199891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanspeter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfister]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Lab]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566583</article_id>
		<sort_key>312</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Cut-and-paste editing of multiresolution surfaces]]></title>
		<page_from>312</page_from>
		<page_to>321</page_to>
		<doi_number>10.1145/566570.566583</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566583</url>
		<abstract>
			<par><![CDATA[Cutting and pasting to combine different elements into a common structure are widely used operations that have been successfully adapted to many media types. Surface design could also benefit from the availability of a general, robust, and efficient cut-and-paste tool, especially during the initial stages of design when a large space of alternatives needs to be explored. Techniques to support cut-and-paste operations for surfaces have been proposed in the past, but have been of limited usefulness due to constraints on the type of shapes supported and the lack of real-time interaction. In this paper, we describe a set of algorithms based on multiresolution subdivision surfaces that perform at interactive rates and enable intuitive cut-and-paste operations.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P109318</person_id>
				<author_profile_id><![CDATA[81100014491]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henning]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biermann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382437</person_id>
				<author_profile_id><![CDATA[81100389759]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ioana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P83787</person_id>
				<author_profile_id><![CDATA[81100373633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fausto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bernardini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39037916</person_id>
				<author_profile_id><![CDATA[81100328351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zorin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[www.paraform.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[www.geomagic.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. Barghiel, R. Bartels, and D. Forsey. Pasting spline surfaces. In Mathematical Methods for Curves and Surfaces: Ulvik, Norway, pages 31-40. Vanderbilt University Press, 1994. Available at ftp://cgl.uwaterloo.ca/pub/users/rhbartel/Paste.ps.gz.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383280</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[H. Biermann, D. Kristjansson, and D. Zorin. Approximate boolean operations on free-form solids. In Proceedings of SIGGRAPH 01, pages 185-194, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883415</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Biermann, I. Martin, D. Zorin, and F. Bernardini. Sharp features on multiresolution subdivision surfaces. In Proceedings of Pacific Graphics 2001, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. 10(6):350-355, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266794</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[L. K. Y. Chan, S. Mann, and R. Bartels. World space surface pasting. In W. Davis, M. Mantei, and V. Klassen, editors, Proceedings of Graphics Interface, pages 146-154, May 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Conrad and S. Mann. Better pasting via quasi-interpolation. In P.-J. Laurent, P. Sablonni&#232;re, and L. L. Schumaker, editors, Curve and Surface Design: Saint-Malo, 1999, pages 27-36, Nashville, TN, 2000. Vanderbilt University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution analysis of arbitrary meshes. Proceedings of SIGGRAPH 95, pages 173-182, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248308</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. S. Floater. Parametrization and smooth approximation of surface triangulations. Computer Aided Geometric Design, 14(3):231-250, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>327790</ref_obj_id>
				<ref_obj_pid>327763</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L. Freitag, M. Jones, and P. Plassmann. A parallel algorithm for mesh smoothing. SIAM J. Sci. Comput., 20(6):2023-2040 (electronic), 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566589</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[X. Gu, S. Gortler, and H. Hoppe. Geometry images. In Proceedings of SIGGRAPH 02, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, A. Khodakovsky, and P. Schr&#246;oder. Hybrid meshes. submitted, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311577</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, W. Sweldens, and P. Schr&#246;der. Multiresolution signal processing for meshes. In Proceedings of SIGGRAPH 99, pages 325-334, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344831</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, K. Vidimce, W. Sweldens, and P. Schrder. Normal meshes. In Proceedings of SIGGRAPH 00, pages 95-102, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. Kimmel and J. A. Sethian. Computing geodesic paths on manifolds. Proc. Natl. Acad. Sci. USA, 95(15):8431-8435 (electronic), 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>246133</ref_obj_id>
				<ref_obj_pid>246121</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt. A variational approach to subdivision. Comput. Aided Geom. Design, 13(8):743-761, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280831</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt, S. Campagna, J. Vorsatz, and H.-P. Seidel. Interactive multiresolution modeling on arbitrary meshes. In Proceedings of SIGGRAPH 98, pages 105-114, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[L. P. Kobbelt. Discrete fairing and variational subdivision for freeform surface design. The Visual Computer, 16(3-4):142-150, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Leroy. Fitting smooth surfaces to dense polygon meshes. In Proceedings of SIGGRAPH 96, pages 313-324, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351673</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[S. Kuriyama and T. Kaneko. Discrete parameterization for deforming arbitrary meshes. In Proceedings of Graphics Interface '99, pages 132-139, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344829</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. Lee, H. Moreton, and H. Hoppe. Displaced subdivision surfaces. In Proceedings of SIGGRAPH 00, pages 85-94, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[A. W. F. Lee, W. Sweldens, P. Schr&#246;der, L. Cowsar, and D. Dobkin. Maps: Multiresolution adaptive parameterization of surfaces. In Proceedings of SIGGRAPH 98, pages 95-104, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776358</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H. Lee, L. Kim, M. Meyer, and M. Desbrun. Meshes on fire. In EG Workshop on Computer Animation and Simulation, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280930</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[B. L&#233;vy and J.-L. Mallet. Non-distorted texture mapping for sheared triangulated meshes. In M. Cohen, editor, Proceedings of SIGGRAPH 98, Computer Graphics Proceedings, Annual Conference Series, pages 343-352. Addison Wesley, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601721</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[N. Litke, A. Levin, and P. Schr&#246;der. Fitting subdivision surfaces. In Proceedings of IEEE Visualization 2001, pages 319-324, October 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[C. Loop. Smooth subdivision surfaces based on triangles. Master's thesis, University of Utah, Department of Mathematics, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[M. Lounsbery, T. DeRose, and J. Warren. Multiresolution analysis for surfaces of arbitrary topological type. Transactions on Graphics, 16(1):34-73, January 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[M. Ma. The direct manipulation of pasted surfaces. Master's thesis, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1, 2000. Available on WWW as ftp://cs-archive.uwaterloo.ca/cs-archive/CS-2000-15/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, H. Yahia, and A. Verroust. Interactive texture mapping. In Proceedings of SIGGRAPH 93, pages 27-34, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>270133</ref_obj_id>
				<ref_obj_pid>270122</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[B. Oberknapp and K. Polthier. An algorithm for discrete constant mean curvature surfaces. In Visualization and mathematics (Berlin-Dahlem, 1995), pages 141-161. Springer, Berlin, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[H. K&#248;hling Pedersen. Decorating implicit surfaces. In Proceedings of SIGGRAPH 95, pages 291-300, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237268</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[H. K&#248;hling Pedersen. A framework for interactive texturing operations on curved surfaces. In Proceedings of SIGGRAPH 96, pages 295-302, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[U. Pinkall and K. Polthier. Computing discrete minimal surfaces and their conjugates. Experiment. Math., 2(1):15-36, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[K. Polthier and M. Schmies. Straightest geodesics on polyhedral surfaces. In H. C. Hege and K. Polthier, editors, Mathematical Visualization. Springer Verlag, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344987</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[E. Praun, A. Finkelstein, and H. Hoppe. Lapped textures. In Proceedings of SIGGRAPH 00, pages 465-470, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383277</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[E. Praun, W. Sweldens, and P. Schr&#246;der. Consistent mesh parameterizations. In Proceedings of ACM SIGGRAPH 01, pages 179-184, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[K. Pulli and M. Lounsbery. Hierarchical editing and rendering of subdivision surfaces. Technical Report UW-CSE-97-04-07, Dept. of CS&E, University of Washington, Seattle, WA, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>871284</ref_obj_id>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[A. Sheffer and E. de Sturler. Surface parameterization for meshing by triangulation flattening. In Proc. 9th International Meshing Roundtable, pages 161-172, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280945</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[J. Stam. Exact evaluation of catmull-clark subdivision surfaces at arbitrary parameter values. In Proceedings of SIGGRAPH 98, pages 395-404, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[H. Suzuki, Y. Sakurai, T. Kanai, and F. Kimura. Interactive mesh dragging with an adaptive remeshing technique. The Visual Computer, 16(3-4):159-176, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[C. L. F. Tsang. Animated surface pasting. Master's thesis, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1, 1998. Available at ftp://cs-archive.uwaterloo.ca/cs-archive/CS-98-19/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[F. W. Warner. Foundations of differentiable manifolds and Lie groups. Springer-Verlag, New York, 1983. Corrected reprint of the 1971 edition.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237254</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[D. Zorin, P. Schr&#246;der, and W. Sweldens. Interpolating subdivision for meshes with arbitrary topology. In Proceedings of SIGGRAPH 96, pages 189-192, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[D. Zorin, P. Schr&#246;der, and W. Sweldens. Interactive multiresolution mesh editing. In Proceedings of SIGGRAPH 97, pages 259-268, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cut-and-Paste Editing of Multiresolution Surfaces Henning Biermannt, Ioana Martint, Fausto Bernardinit, 
Denis Zorint tMedia Research Laboratory New York University Abstract Cutting and pasting to combine 
different elements into a common structure are widely used operations that have been successfully adapted 
to many media types. Surface design could also bene.t from the availability of a general, robust, and 
ef.cient cut-and­paste tool, especially during the initial stages of design when a large space of alternatives 
needs to be explored. Techniques to sup­port cut-and-paste operations for surfaces have been proposed 
in the past, but have been of limited usefulness due to constraints on the type of shapes supported and 
the lack of real-time interaction. In this paper, we describe a set of algorithms based on multiresolu­tion 
subdivision surfaces that perform at interactive rates and enable intuitive cut-and-paste operations. 
 1 Introduction Pasting and blending of images are among the most common op­erations implemented by image 
manipulation systems. Such oper­ations are a natural way to build complex images out of individual pieces 
coming from different sources. For example, photographs can be easily combined with hand-drawn and computer-generated 
images. In contrast, pasting and blending tools are hardly available for surfaces. Most geometric modeling 
systems expect the user to manipulate control points of NURBS, individual mesh vertices and polygons, 
or use conventional, higher-level operations such as vol­ume deformations and boolean operations. In 
an image processing system, vertex and control point manipulation would be equivalent to painting an 
image pixel-by-pixel. While it may be useful to have access to such low-level operations in certain cases, 
most image manipulations are done using higher-level tools. In this paper we describe a technique for 
interactive cut-and­paste editing of surfaces, an important instance of a natural oper­ation on a surface 
(see Figure 1 for an example). The algorithms we propose enable a number of useful design scenarios which 
are dif.cult to perform using existing technology. For example, in the design of automobile body parts, 
it is common to work in parallel on a digital mock-up and on a clay model. Using the cut-and-paste technique, 
a designer can paste a logo obtained by 3D scanning onto a digitally-modeled surface, import features 
from a library of prede.ned shapes, or copy parts of a design from a different project. The basic idea 
of pasting is quite simple. The user selects an area of interest on the source surface. Both the source 
and the tar­get surfaces are separated into base and detail, such that the detail surface represents 
a vector offset over the base surface. Next, the Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00  Figure 1: An example of a pasting operation. user speci.es 
a location and an orientation on the target surface where the source feature is to be pasted and interactively 
adjusts the position, orientation, and size of the pasted feature. The main questions we address in this 
paper are: How to separate a surface into base and detail?  How to identify an area on the target surface 
where the feature should be pasted and how to establish the necessary mappings be­tween the source and 
the target?  How to implement the process ef.ciently to allow for interactive pasting of complex features? 
 We use multiresolution subdivision surfaces as our underlying representation [28, 44]. The actual computer 
representation is a semi-regular control mesh for the surface and most operations are performed on this 
mesh. The associated limit surface is used for computing quantities such as tangents and normals, as 
well as for additional re.nement when necessary for antialiasing. This is sim­ilar to pixel representations 
of images: when images are scaled or rotated, they are typically assumed to be sampled representations 
of smoothly varying continuous images (e.g., obtained by cubic inter­polation). The regular and hierarchical 
structure of this surface represen­tation makes it possible to perform operations on detailed surfaces 
at interactive rates as discussed in Section 3.2. In many ways, us­ing this representation makes surface 
manipulation similar to image manipulation: almost everywhere the connectivity of the mesh ap­proximating 
the surface is locally regular. At the same time, many common problems speci.c to geometry have to be 
addressed: the lack of a common parameterization domain for separate surfaces, the lack of a unique best 
parameterization domain for the surfaces, the separation of surface features. While our algorithms can 
be ap­plied to a broad class of surfaces, there are limitations on the source and target geometry for 
which the pasting paradigm is appropriate (see Section 9 for a discussion).  2 Previous Work The concept 
of surface pasting was introduced in the work of Bar­tels, Mann and co-workers in the context of hierarchical 
splines [3, 7, 8, 29, 42]. We are using similar ideas, but most of the techni­cal details are different. 
Most importantly, we consider more gen­eral surface types and we do not assume that separate detail and 
base surfaces are given: they need to be extracted from the input surfaces. Moving existing features 
on a mesh was explored by Suzuki et al. [41]. The advantage of their approach is that no resampling of 
the repositioned feature is performed. However, continuous remeshing is required, which limits the complexity 
of the objects and features that can be handled. Issues such as pasting features between sur­faces and 
separation into base and detail surfaces are not considered by these authors. The task of base/detail 
separation is similar to the construction of displaced subdivision surfaces [22]. One of the elements 
of our approach, i.e., mesh smoothing to extract a base surface, was de­scribed by Kobbelt et al. [18] 
in the more general context of arbi­trary meshes. An alternative approach was proposed by Guskov et al.[14]. 
We discuss the advantages and disadvantages of restricting the class of surfaces to semi-regular meshes 
in Section 3.2. Part of our construction of base surfaces is closely related to the work of Kobbelt et 
al. on variational subdivision [19, 17]. It also draws upon the work of Polthier et al. [34, 31]. Parameterization 
techniques are important in many geometric modeling and texturing applications and a variety of algorithms 
have been proposed, including general parameterization methods [10, 11] for reparameterization (i.e., 
changing connectivity to semi­regular) [9, 15, 20, 23] and texture mapping [25, 36, 30]. In [21], Kuriyama 
and Koneko use local parameterizations to add offsets to a surface. The work of Pedersen [32, 33] on 
interactively placing textures on implicit surfaces is also relevant as it requires dynamic reparameterization 
of surface areas similar to pasting. However, the problem of parameterizing a surface area over a plane 
with minimal visual distortion is far from solved. As ex­plained in Section 6, until recently no algorithms 
combining sev­eral crucial properties for our application were available. We use a variation of the remarkable 
algorithm by Sheffer and Sturler [39] which satis.es our requirements.  3 Pasting Surfaces We begin 
with a formalized description of pasting operations on surfaces. At this point we discuss continuous 
surfaces and map­pings without considering their discrete representations. This framework applies to 
a wide class of manifold surfaces, ranging from splines to implicit surfaces. Precise descriptions of 
all basic mathematical concepts that we use can be found in any standard textbook (e.g., [43]). 3.1 Formulation 
of the Problem For simplicity, we restrict our attention to parts of surfaces param­eterized over planar 
domains: MCR2. Furthermore, we assume that these parameterizations are suf.ciently smooth. Given two 
sur­faces (M1,f1)and (M2,f2), where f1and f2are their parameter­izations, we would like to paste a feature 
from one surface to the other (see Figure 1). Such an operation requires separating each surface into 
two parts: the base surface and the detail surface. The goal is to replace the detail part of the second 
surface with the detail part of the .rst. The key question is how to transfer correctly the details from 
one surface to the other. Base and detail surfaces. The base surface b(x)is typically a smoothed or .attened 
version of the original surface (we discuss appropriate choices in Section 5). The detail surface d(x)can 
be de.ned as f(x)-b(x). However, to ensure that the offset direction is at least invariant with respect 
to rigid transformations of the base, it must be represented in a local frame. The local frame is a triple 
of vectors (n, 1b, 2b), including the normal and two tangents (two partial derivatives of the parameterization). 
It is convenient to think about these derivatives together as a map Db(differential of b) which maps 
vectors in the plane to vectors in the tangent plane of the surface. The detail surface is thus de.ned 
by the triple . t1 dn,d,dt2, which can also be thought of as a scalar displacement along the normal dnand 
a tangential displacement in parametric coordinates dt=(dt1,dt2). The equation relating the original 
surface, the base, and the details can be written as: f(x)=b(x)+ Db(x)dt(x)+n.(x)dn(x), where xis a point 
in the domain. Surface pasting. With both surfaces separated into base and detail parts, we can formulate 
a precise de.nition of pasting. All quanti­ties with index 1 refer to the source surface from which we 
extract the details and all quantities with index 2 refer to the target surface on which the details 
are pasted. Suppose the part of the surface we want to paste is de.ned over G1CM1. Let pbe a map from 
G1to M2, which de.nes how the surface is pasted. We discuss separately how pis chosen (Sec­tion 6). The 
result of a simple pasting operation is a new surface coin­ciding with f2outside p(G1), which has the 
same base as f2but for which the details are taken from the source surface: (n fpasted pdtdnÆp-1 =b2+Db2D1+n..1 
where all functions are evaluated at a point x2Ep(G1), and Æ denotes function composition. Note that 
we use the composition of differentials Db2ÆDpto transform the tangential component of details. This 
establishes the natural map between the local frames on the source and target sur­faces. Figure 2 illustrates 
the different maps involved. p = p2-1T p1 Figure 2: A diagram of surface maps involved in pasting. Using 
this formulation, there are two main choices to be made: the separation of both source and target surfaces 
into base and detail and the de.nition of a pasting mapping p, identifying the domain G1with a part of 
the domain M2. The map phas to satisfy two conditions: it has to be one-to-one and it should minimize 
distortion of the mapped feature. An impor­tant consideration is whether the mapping pfrom the domain 
of one surface to the domain of the other surface is constructed directly or by using an intermediate 
planar domain. We favor the latter ap­proach as it considerably simpli.es three tasks: making sure that 
the mapping is visually smooth, minimizing distortion, and resam­pling the source over the target sampling 
pattern. To explain our choices, we need to be more speci.c about the surface representa­tion we are 
using.  3.2 Multiresolution Subdivision Surfaces The representation that we use was introduced in various 
forms in [28, 38, 45]. Subdivision de.nes a smooth surface recursively as the limit of a sequence of 
meshes.1 Each .ner mesh is obtained from a coarse mesh by using a set of .xed re.nement rules, e.g., 
Loop [27] or Catmull-Clark [6] subdivision rules. In our implementation we use Catmull-Clark rules. Multiresolution 
surfaces extend subdivi­sion surfaces by introducing details at each level. Each time a .ner mesh is 
computed, it is obtained by adding detail offsets to the sub­divided coarse mesh. If we are given a semi-regular 
mesh, i.e., a mesh with subdivision connectivity, we can easily convert it to a multiresolution surface 
if we de.ne a smoothing operation to com­pute vertices on a coarse level from a .ner level. The details 
are then computed as differences between levels (see Section 5). An aspect of multiresolution surfaces 
important for modi.cation operations is that details are represented in local coordinate frames, which 
are computed from the coarser level. This is analogous to representing the detail surface in the frame 
computed from the base surface. For our purposes, it is important to interpret the multiresolution surface 
as a function on a domain. A multiresolution subdivision surface can be naturally viewed as a function 
on the initial mesh as shown in Figure 3. Figure 3: Natural parameterization of the subdivision surface. 
Each time we apply the subdivision rules to compute the .ner control mesh we also apply midpoint subdivision 
to a copy of the initial control mesh. As we repeatedly subdivide, we get a mapping from a denser and 
denser subset of the control polygon to the control points of a .ner and .ner control mesh. In the limit 
we get a map from the control polygon to the surface. Advantages and disadvantages of the representation. 
The main reason behind our choice of representation is ef.ciency. There are a number of reasons why semi-regular 
meshes allow for highly ef­.cient algorithms:  Connectivity information only needs to be stored for 
the coars­est level. Geometric data is stored in a regular and space-coherent manner. Both factors are 
important for computer architectures for which bad cache behavior results in poor performance. In addition, 
regularly sampled patches can be rendered very ef.ciently.  Our meshes have a built-in natural hierarchy 
that can be exploited by numerical solvers. These are used, for example, to de.ne a fam­ily of smooth 
surfaces for base surface selection by hierarchical .tting and for parameterization, when an initial 
approximation of the .ne level solution can be obtained by solving the system on a coarse level, followed 
by re.ning the solution by subdivision.  Compact representation for smooth surfaces: for example, the 
original vase in Figure 1 (left) is completely de.ned by the initial mesh and the smooth surface can 
be recomputed on-the-.y. When details are added, additional re.nement is needed only in the re­gions 
with details.  1To be more accurate, we should say that the limit surface is the point­wise limit of 
a sequence of piecewise linear functions de.ned on the initial control mesh. Local frames can be computed 
in a simple, fast, and reliable way, consistently across resolution levels (i.e., re.ning the mesh for 
the base surface does not change the frames computed for the surface at existing vertices). Our experiments 
with solvers that take advantage of the regular structure and generic solvers that use a sparse matrix 
representation suitable for arbitrary meshes show that the former yield speedups by a factor of 2 to 
4. Furthermore, using a hierarchical solver for an arbitrary mesh would require building a hierarchy 
by simpli.cation, a step entirely omitted in our construction. The main disadvantage of representing 
surfaces using semi­ regular meshes is having to convert surfaces represented by arbi­ trary meshes to 
this format. Fortunately, considerably progress has been made in this area [20, 23, 15] and commercial 
software (e.g., Paraform [1], Geomagic Studio [2]) typically includes such conver­ sion tools. All of 
the scanned models used in this paper were con­ verted to semi-regular meshes using Geomagic Studio. 
We believe that, in all cases when surface data is extensively modi.ed, conver­ sion is the best approach, 
as reparameterization is almost inevitable if the surface is texture-mapped. Gu et al. provide a detailed 
study of the bene.ts of a conversion to a similar representation (i.e., the geometric image [12]).  
3.3 Pasting with an Intermediate Plane A direct construction of the pasting mapping pis dif.cult to achieve 
ef.ciently. Visual smoothness and minimization of distortion are typically obtained by minimizing appropriate 
functionals. In the case of a direct mapping of the source region to the target surface domain the values 
of the mapping are not a part of any af.ne space. Indeed, the domain of the surface is a collection of 
faces of the coarse-level control mesh, so each point needs to be characterized as (i,u,v)where iis the 
face id, and (u,v)are coordinates within the face. Unless the whole surface can be reparameterized on 
a plane, there is no simple way to compute linear combinations of two arbitrary points (e.g., the midpoint 
of the interval connecting the points), which makes the application of most common com­putational techniques 
very dif.cult. Even a simple operation such as computing angles of a triangle given three vertices becomes 
a complicated task, an important consideration for the angle-based .attening technique we consider. To 
avoid these dif.culties, we parameterize the corresponding areas of the source and target over the plane. 
The idea is to map each surface onto the plane as isometrically as possible and then align the two planar 
parameterizations, using a linear transforma­ tion to compensate for the .rst-order distortion. In this 
case, the pasting map is restricted to a simple class of maps (i.e., linear trans­ formations T, see 
Figure 2), but new parameterizations p 1and p 2 are constructed for the parts of surfaces of interest 
for every pasting operation. There is a similarity between the idea of our approach and the method of 
Praun et al. [37] for establishing correspondences be­ tween different meshes. In [37] the correspondence 
is established by reparameterizing each mesh on the same base domain. Given our disk topology assumption, 
we can use the plane as the common domain. Our approach has two main disadvantages. First, it makes it 
dif­ .cult to generalize our technique to pasting regions with topology different from that of a subset 
of a plane (e.g., pasting all details from one sphere to another). Second, it may result in higher distor­ 
tion than a direct mapping from one surface to the other. The higher the Gaussian curvature of the base 
surface is, the more likely it is that additional distortion is introduced. A direct mapping method similar 
to the one used in [4] might produce better results in this case, but it would make pasting of complex 
surfaces at interactive rates dif.cult, if at all possible. feature feature selection base surface parameterization 
target region source and target final result finding parameterizations Figure 4: Main steps of the pasting 
algorithm, row-wise from top left: (a) selected feature on the source surface, (b) base source sur­face, 
(c) source parameterization onto the plane, (d) target region .nding by geodesic walking, (e) source 
(black) and target (red) pa­rameterizations superimposed in the plane, (f) source feature pasted onto 
the target surface.  4 Overview of the Algorithm The main steps of our algorithm are illustrated in 
Figure 4: 1. The user marks a region on the source surface and optionally speci.es a spine. A spine of 
a region is a collection of curves which capture the general shape of the region. It approximates the 
medial axis of the region and it can be used by the system for mapping the source to the target (see 
also Figure 8). 2. The details are separated from the base for the source surface. The user interactively 
selects a base surface from a continuous range interpolating between a zero level given by a membrane 
surface and the actual surface (Section 5). 3. The source region is parameterized over the plane (Section 
6). 4. The boundary of the source region is parameterized by dis­tance and direction from the spine 
and a covering by disks is computed. 5. The user positions at least one point of the spine on the target, 
and speci.es an orientation. 6. A target region for pasting is determined on the surface using geodesic 
disks (Section 7). 7. The target area is mapped to a common plane with the source and the source is 
resampled over the target sampling pattern (Section 8). 8. The resulting surface is computed by blending 
the target base surface, the source surface resampled details, and the target details. The user can specify 
different blending modes.   5 Separating Base Surface from Detail An important step in the pasting 
process is the de.nition of which features of the source surface region constitute details that the user 
wants to paste over the target surface, as opposed to the larger-scale surface shape that should be ignored. 
Separating the base surface from the details depends on the semantics of the operation and has to be 
user-guided. For example, one may want to extract only the texture-like geometry of the skin on the nose 
of a head model, or to paste the entire nose onto a different model. Different choices for base-detail 
separation result in different pasting effects (Figure 6). Our approach is to provide a continuum of 
base surface choices guided by a single parameter which can be thought of as the .atness of the base 
surface. A natural way to obtain a smooth base surface given our multiresolution data representation 
is to remove or reduce the multiresolution details present in the multiresolution hierarchy on the .ner 
levels. The degree to which this approach works de­pends on the way the coarser levels were obtained 
when the hierar­chy was constructed. By comparing several approaches (Taubin s smoothing, quasi-interpolation, 
and .tting), we found that .tting works best for pasting. Least-Squares Fitting. The .tting procedure 
minimizes a func­tional that measures how well the smooth surface .ts the vertices of the original mesh 
subdivided to the .nest level M. While .tting of subdivision surfaces is not new (e.g., [26]), there 
appears to be no detailed description of it in the literature and we present it here for completeness. 
The minimization problem for level mof the smooth surface hierarchy can be stated as: M-m2 m.mIIpw -OsMpm]wII(1) 
p wEV. where the minimum is computed over all possible choices of con­trol points pmfor the smooth mesh, 
VMis the set of vertices of the .nest-level mesh, pMare the corresponding control points, sM-m is the 
subdivision matrix for M-msubdivision steps, and O]w means that the resulting smooth surface is evaluated 
at parameter values corresponding to vertices Wof the control mesh. The min­imization problem is equivalent 
to .nding solutions for the linear system ATAx=ATb, with A=sM-m, b=pMand x=pm, and can be solved by using 
the Conjugate Gradient method. To apply this method, the only operations needed aside from linear combinations 
of vectors and dot products, are matrix-vector multi­plications for the sM-mmatrix and its transpose. 
As the matrix is obtained by iterative application of the subdivision matrix, there is no need to represent 
or store it explicitly: applying Acorresponds to the application of M-msubdivision steps. Applying ATto 
a vector can be interpreted in similar terms. More speci.cally, as shown in Figure 5, the mask for each 
vertex von level m-1con­tains all vertices on level mwhich are affected by vwhen subdivi­sion is performed. 
If vertex vhas coef.cient ain the subdivision rule used to compute the control point for vertex W, then 
vertex W has coef.cient ain the transpose averaging rule for v. ß2 Figure 5: The mask for local averaging 
used for transpose subdi­vision. Coef.cients aiand fiare the Catmull-Clark vertex rule coef.cients for 
corresponding vertices. I=1-na-nf, where n is the valence of the central vertex and aand fare the vertex 
rule coef.cients. Once the sequence of levels is computed, a continuum of base surfaces can be obtained 
by interpolation as shown in Figure 6. The user can select one interactively by moving a slider.  source 
base surface choice target result of pasting the source feature Figure 6: Depending on the choice of 
base surface, different scales of shape details are transferred to the target. An alternative approach 
to .tting is to use the quasi-interpolation approach of Litke et al.[26]. This approach is somewhat faster, 
but results in larger errors. We use the more accurate .tting approach as it does not constitute a bottleneck 
in our system. Boundary constraints. The technique previously described ex­plains how to produce smoother 
approximations of the surface globally. This approach is quite fast as the base surfaces on different 
levels can be precomputed and only interpolation is required after that. However, when we separate the 
feature from the surface, we need a base surface only near the feature. Even more importantly, in most 
cases the details should gradually decay in magnitude as we approach the boundary of the feature. To 
adapt the global base sur­faces to our needs, we use the following simple blending approach: the local 
base surface is computed as a blend of the source surface and a global base surface. The region in the 
interior of the feature is assigned alpha values 1 and all vertices outside the region are given values 
0. Next, relaxation is applied for values in the interior while keeping the values outside constant. 
The amount of relaxation is user-controllable and allows to change the way features blend into the target. 
The resulting alpha values are used to interpolate be­tween the global base and the source surface (Figure 
11). Minimal base surface. Base surfaces de.ned by .tting and blend­ing cannot be .atter in the area 
of the feature than the base surface obtained by .tting on the coarsest level. This might be not appro­priate 
for some applications where it is necessary to retain more of the feature shape (e.g., Figure 14). In 
such cases, it is best to de­.ne the base surface as a smooth, relatively .at surface that .lls the hole 
remaining after the feature is cut off. To obtain such a surface, we optimize the membrane energy of 
the surface inside the feature curve while constraining its boundary to remain .xed. We use a multigrid-type 
approach [19], which is a natural choice in the con­text of our multiresolution representation. Similarly, 
the transition between the feature and the base is handled by assigning alpha val­ues. This allows us 
to extend the range of possible base surfaces beyond the coarsest-level .tted surface. As a result the 
user has a choice of base surfaces varying from the minimal surface spanning the outline of the feature, 
all the way to the original surface.  6 Parameterization Once we have separated the details from the 
base surface, we need to .nd a map from the source base surface to the target, to be able to transfer 
the details. As it was discussed in Section 3.3, we construct the map in two steps. First, we map the 
source surface to the plane. Second, we determine the region on the target surface where the feature 
will be pasted and we parameterize it onto the same plane. Parameterization is needed both for the source 
and target base surfaces. The type of surface patches that we need to parameter­ ize is relatively uncommon: 
while the surface is likely to be quite smooth, the shape of the patch can be relatively complex. The 
pa­ rameterization we construct should satisfy the following require­ ments: The parameterization region 
should not be chosen a priori. The need for this can be seen from the following simple example: the outline 
of a feature selected on the plane base surface can be ar­bitrarily complex, however the parameterization 
should not be dif­ferent from the surface itself. Any algorithm that requires a .xed domain is not likely 
to perform well in this situation.  The parameterization should be guaranteed to be one-to-one. As we 
need to resample, for each vertex on the target we need to iden­tify a unique position on the source. 
This means that at least the map from the source to the plane has to be one-to-one.  The parameterization 
should minimize a reasonable measure of distortion. Ideally, for developable surfaces it should be an 
isom­etry up to a scale factor. The algorithm that we use does not ex­plicitly minimize a measure, but 
it appears to produce results with close to minimal shape distortion, as discussed below. It tends to 
produce better results than all other algorithms that we have tried in situations relevant for us.  
Most of the existing parameterization algorithms do not deter­mine the domain automatically: it is either 
determined using a heuristic approach or it has to be prescribed by the user. The pa­rameterization described 
in [36] allows for free boundary evolu­tion, but it requires a vector .eld de.ned over the surface and 
it does not provide a one-to-one guarantee. Until recently, algorithms that guaranteed a one-to-one parameterization 
required convex do­mains, like the many variations of Floater s algorithm ([10]). We use the algorithm 
of Sheffer and Sturler [39] which best meets our requirements. Angle-based .attening. For a given mesh 
a parameterization is de.ned by specifying the positions (parametric coordinates) of all vertices of 
the mesh in the plane. Without the loss of generality, we can assume a triangular mesh (we use quad subdivision 
surfaces, but each quad can be easily split into two triangles). The idea of angle-based .attening is 
to compute the parametric coordinates of the vertices indirectly: .rst, all angles are computed using 
an opti­mization procedure, then the planar mesh is reconstructed by .xing the length of one of the edges. 
The reason for computing the angles rather than vertex positions directly is that the one-to-one condition 
can be easily enforced and aspect ratios can be controlled explicitly. The disadvantage is that the reconstruction 
procedure is relatively unstable, as positions of vertices depend sequentially on each other. However, 
we found that for the relatively small numbers of trian­gles that we use (at most thousands), this is 
never a problem. Next we describe the formulation of the optimization problem for angles mostly following 
[39]. Let tdenote a triangle of the mesh, Tthe set of all triangles; let vbe a vertex and Vthe set of 
all vertices in mesh. If vis a vertex of tthen the angle atis the corresponding angle in the triangle 
t. Let N(v)be the set of all triangles sharing a vertex v. The target value for angle atis de.ned  
as follows: =27a/a, i.e., ideally all angles at a tssEN( )s vertex should be rescaled by the same amount, 
so that their sum is equal to 27. The resulting functional is 2 F(a)=Wt(at - t), t, Et where the weights 
are chosen to be 1/( t)2 . Minimization of this functional needs to be constrained for the results to 
correspond to a valid planar triangulation. The necessary next(v) at prev(v) at notation consistency 
constraint violation Figure 7: The image on the right shows the situation prevented by the constraint 
94(t)in the parameterization algorithm. constraints are as follows: (a) the angles should stay above 
some minimal value E; (b) the sum of all angles at a vertex should be 27; (c) the sum of all angles of 
each triangle should be 7; (d) each 1-neighborhood of a vertex should be consistent. This means that 
if we reconstruct a neighborhood triangle-by-triangle going around the vertex, the last edge of the last 
triangle should coincide with the .rst edge of the .rst triangle as shown in Figure 7. The mathematical 
expressions for these four constraints are: 91(v,t)=at -E?0;92(v)=at -27=0; tEN() 93(t)=at -7=0 Et prev(t) 
t 94(v)=-1=0. ItEN() s.manext() s.ma t The inequality constraints are best enforced in an iterative pro­cedure 
for minimizing the functional by rejecting values that vio­late the constraints. The equality constraints 
are included into the functional by means of Lagrange multipliers: L(a)=F(a)+ A(v)92(v)+/(t)93(v)+94(t). 
t This is a nonlinear optimization problem which is solved using Newton s method: xnH1=xn -H(L)-1'L, 
where 'Land H(L)are the gradient and the hessian of L, and xis the vector of all angles and Lagrange 
multipliers. At each step, we need to solve a linear system to invert H(L). In [39] a direct solver is 
used. We observe that the system does not change much from one iteration of the Newton method to the 
next. Furthermore, for smooth surfaces it is likely that the initial guess for the angles is quite close 
to the solution (e.g. for the developable surface it is already a solution). This indicates that iterative 
solvers are likely to perform quite well. Although the system is symmetric, it is not positive de.nite, 
and the Conjugate Gradient method cannot be used. However, the Con­jugate Residuals method applies. For 
fast convergence rates, pre­conditioning is required, i.e., an approximate sparse inverse of the matrix 
needs to be computed. To avoid an expensive preconditioner computation, we add a small negative constant 
equal to the inverse of the number of triangles on the diagonal which makes it possi­ble to avoid pivoting 
when calculating an incomplete factorization (ILU) preconditioner. Our experience was that a small number 
of iterations of the solver were suf.cient to obtain a reasonable param­eterization. It should be noted 
that, while the constraints guarantee that the resulting mesh is one-to-one locally (no .ipped triangles), 
the boundary of the image may self-intersect and globally the map is still not one-to-one. A technique 
for eliminating such self intersec­tions is described in [39].  7 Determining a Target Region Before 
pasting can be performed, an area on the target surface cor­responding to the feature has to be identi.ed 
and parameterized. It is a chicken-and-egg problem: to determine the region covered by the pasted feature, 
we need to map it to the target; however, mapping the feature to the target requires parameterizing the 
corre­sponding part of the target surface over the plane. As parameteriz­ing the whole target is generally 
not an option, we use the following approach: we observe that initially we need to identify only an ap­proximate 
boundary region where the feature will .t, rather than to establish a one-to-one mapping of the interior. 
Once the region is identi.ed, it can be parameterized over the plane and a mapping is computed as the 
composition of the two parameterizations. The algorithm that we use for identifying the region proceeds 
in several steps: .rst, we represent the boundary of the source re­gion in a generalized radial form, 
constructing line segments (pla­nar geodesics) connecting the spine to the boundary. Then we map the 
one-dimensional spine to the target, and use geodesics on the target to map the boundary points to the 
target. Finally, we connect the points on the target and .ll in the interior region (Figure 8). The computation 
of geodesics passing through a point is a central tool in the algorithm and is discussed in greater detail. 
Parameterizing the source boundary. The user has the option to draw a curve on the surface, possibly 
with several branches, which serves as the spine of the feature. It is our main intention to help the 
system map the feature to the target surface with the least distortion. If the user does not de.ne a 
spine, a single point (the centroid of the boundary of the feature) is automatically selected to serve 
as the spine. The following algorithm is used to parameterize the source boundary. First, the spine is 
mapped to a curve in the plane by the parameterization. Let o,...m-1be equispaced points on the spine 
in the parametric domain. The number of points can be ad­justed to trade speed for quality. For each 
vertex Wjon the boundary of the source parameteriza­tion .nd the closest point i. Let nibe the number 
of points closest to the point i, djbe the distance from ito Wjand Ijbe the an­gle between the direction 
from ito Wiand the spine. If the spine consists of a single point, an arbitrary .xed direction is used 
as the direction of the spine. The boundary of the source region can be characterized by the set of triples 
(i,dj,Ij), where i=0...m-1, and j=0...ni -1. This collection of triples can be thought of as a discrete 
parameter­ization of the boundary with respect to the spine generalizing the radial parameterization. 
In the case of a single-point spine, this is just the radial parameterization. Mapping the spine to the 
target. Mapping the spine to the tar­get is straightforward: the user speci.es an initial position and 
ori­entation for a point on the spine. The other points on the spine are obtained sequentially by walking 
as follows. Suppose the posi­tions T(o)...T(i)are known. If the angle between the intervals (i-1,i)and 
(i,iH1)is fi, then the next point on the spine is obtained by walking on the target surface at an angle 
fito the pre­vious segment for a distance equal to Ii,iH1I. Finding the target region. Once the positions 
of all points T(o)...T(m-1)are found on the target, we .nd the posi­tions of each boundary point T(Wj)using 
the corresponding triple (i,dj,Ij). Speci.cally, we walk starting from ialong a geodesic direction forming 
the angle Ijon the target for a distance djto obtain T(Wj). Once all the points on the boundary are found, 
they need to be connected. We do this by using a plane which passes through the two points T(Wj)and T(Wj 
H1)and the normal at one of the points. If both normals happen to be aligned with the direction between 
the points, an additional boundary point is inserted midway between them and a corresponding radial representation 
is generated for it by adding an extra geodesic path. We traverse the triangles along the intersection 
of the plane with the surface starting from T(Wj)in the direction of T(Wj H1). There are three possible 
outcomes: either we reach T(WjH1)(both points are on the same continuous segment of the plane surface 
intersec­tion), we return to T(Wj), or we reach a boundary. In the last two cases, we add a new point 
on the boundary of the source region and we repeat the procedure for each pair of points. Once all sequential 
points T(Wj)on the target surface are con­nected, we use a .ll algorithm to mark the complete region. 
radial parametrization selected feature target region finding spine parameterization region mapped to 
a target Figure 8: Finding the target region. It is important to note that the algorithm may produce 
an area which is not topologically equivalent to a disk. For example, any large enough region mapped 
to a sphere can cover the entire sphere. The algorithm described above should be followed by a test check­ing 
the topology of the resulting area. This can be done by comput­ing the genus assuming that there is a 
face attached to the boundary loop of the region. The genus computed in this way should be zero, and 
the region should have exactly one boundary loop. If the test fails, pasting at this scale is not possible. 
The user should decrease the scale for the pasted feature or place it at a different location. Target 
parameterization. Once the target is determined, it is mapped to the plane. Generally, we use the same 
relatively ex­pensive angle-based .attening algorithm for target parameteriza­tion each time a pasting 
operation is performed. This allows us to achieve maximum .exibility in feature placement and lowest 
dis­tortion. While this approach still permits interactive manipulation rates, the frame rate is much 
better if a larger area of the target can be parameterized and the feature is moved inside this area. 
In this case, the most expensive part, i.e., target area .nding and reparam­eterization is completely 
excluded, and only resampling has to be done at most steps. Geodesic walking. One of the key ingredients 
of the algorithm for determining the target region is the algorithm for computing a geodesic emanating 
from a given point in a speci.ed direction. While a number of algorithms for this or similar problems 
have been proposed [35, 16, 24], our application has speci.c require­ments. We need the algorithm to 
be fast, as the target region has to be found at interactive rates. This makes it dif.cult to use methods 
based on front propagation. Even more importantly, we need a continuity property. Note that termination 
of the algorithm for .nding the target region depends on our ability to make the distance between points 
T(Wj)on the target arbitrarily small by increasing the density of the points Wj on the source boundary. 
Such continuity means that as we decrease the angle between two outgoing geodesics for a point, the distance 
between their endpoints can be made arbitrarily small. It is known however that straightest geodesics 
on meshes may violate this con­dition ( the saddle point problem ).  Accuracy of the result is of secondary 
importance, as the mapping process is approximate. Also the swallow tail problem , i.e., the fact that 
geodesics may intersect near an elliptic point, is not rele­vant for us as we only determine the the 
boundary of the region and we do not construct a one-to-one map.  saddle surface straightest geodesics 
n1 ni n0 pi geodesic construction our geodesics Figure 9: Comparison of straightest geodesics and our 
geodesics. Note the empty regions for the straightest geodesics: no matter how densely the directions 
are sampled, no geodesic passes through a part of the region. Our procedure is based on the fact that 
the geodesic 9(t)is al­ways a locally normal curve, i.e., its second derivative 9..(t)is pointing along 
the normal to the surface. By interpolating the nor­mals, we approximate a smooth surface with continuously 
changing normal. The elementary step remains going from triangle to trian­gle, but the angles are computed 
differently. Suppose we start at a point piat the edge .oof a triangle Ti, and voand v1are the vertices 
of the edge .o. Let noand n1be the normals at the vertices voand v1. We compute the normal ni at the 
point pilinearly interpolating between normals noand n1 and renormalizing. If noand n1point in opposite 
directions, ar­bitrary choice is made. Suppose there is an initial direction vector tide.ned at pi. The 
procedure described next de.nes the direc­tion vector at a sequence of points pjof the discrete geodesic 
to be perpendicular to the interpolated normal vector at the point (if the initial one is not, we project 
it to the plane perpendicular to the normal). To obtain the point piH1and the new direction tiH1at that 
point we perform the following steps: 1. Intersect the plane spanned by niand tiwith Titoget adi­rection 
.(ti). If the plane coincides with the plane of Ti, ti itself is used. 2. Intersect the line along .(ti)in 
the triangle Tiwith its edges to get the point piH1. Suppose the intersected edge is .1with endpoints 
v1and v2. The next triangle TiH1is the triangle across the edge .1.  3. Compute the normal niH1at piH1by 
linearly interpolating bewtween n1and n2at vertices v1and v2. Project the di­rection .(ti)onto the plane 
perpendicular to niH1to obtain tiH1.If .(ti)is parallel to niH1, we use the average of the projections 
obtained for two small perturbations of position of the point piH1. It can be veri.ed that this procedure 
satis.es the continuity re­quirement if the mesh approximating the surface is smooth enough, i.e., the 
projection of the ring of triangles around any vertex onto the plane perpendicular to the normal is one-to-one. 
 8 Mapping and Resampling Once the mappings from the source and target to the plane are es­tablished, 
their planar images are aligned using the point and ori­entation correspondences speci.ed by the user 
when the target area was chosen. The .nal step in the pasting algorithm is resampling and combining the 
details from the source with the details and base surface of the target. For every vertex vof the parameterization 
of the target which is inside the parameterization domain of the source, we .nd the corresponding quad 
of the source parameterization. Next, u,vco­ordinates are computed in this quad and the source is evaluated. 
Evaluation can be done in two ways: for fast resampling, the values of the source at the vertices of 
the quad are interpolated. For higher quality, subdivision surface evaluation [40] should be used. This 
is similar to using bilinear .lters for fast image editing and bi-cubic .ltering for a higher-quality 
.nal result. Adaptive re.nement and sampling. The further away the geom­etry of the feature is from a 
displacement map, the less suitable pasting for surface operations is. However, in some cases it is de­sirable 
to use the pasting paradigm to place objects which cannot be reparameterized over the plane without considerable 
distortion (Figure 14 left). In other cases, the resolution of the source surface is substantially higher 
than the resolution of the target. In these cases, uniform sampling of the target is not adequate and 
a form of adaptivity is needed. Hybrid meshes [13] offer the maximal de­gree of .exibility, as it is 
possible to perform irregular re.nement in some spots and align mesh edges exactly with pasted feature 
edges. We use a more conventional approach where only regular re.ne­ment of individual faces is allowed. 
However, rather than quadri­secting individual faces recursively according to a criterion, we es­timate 
the local density of the source samples over a target face. We use this estimate to directly compute 
the subdivision level required for a given face and we re.ne faces to that level uniformly. 9 Results 
A number of models created using our system are shown in Fig­ures 12 to 14. Figure 12 shows how details 
from a scanned object are pasted on a simple vase model. In this case, the target object itself serves 
as the base surface. Similarly, Figure 13 demonstrates how details from a scanned model can be combined 
with a differ­ent computer-generated model. Figure 14 (right) demonstrates how a medium scale detail 
can be pasted on a surface while preserv­ing small-scale surface details. Figure 14 (left) shows examples 
of feature manipulation on the surface. In all cases the operations were performed interactively, but 
the frame rate varied greatly depending on the complexity of the fea­ture, the complexity of the target 
region, and the sampling density in the target region. If the target is a simple smooth object, a large 
area can be parameterized at once without signi.cant distortion and no dynamic parameterization is required. 
In such cases, suf.ciently complex models permit manipulation at high frame rates. How­ever, if no large 
region can be parameterized without distortion, the frame rate varies in the range 5-0.5 frames per second. 
 Limitations of the approach. The principal limitations of our ap­proach include: The algorithm fails 
to produce a valid surface when the identi.ed target region is not homeomorphic to a disk. This may occur, 
for example, if it completely covers a handle.  The approach is useful for transferring features from 
one surface to another when the curvature of the chosen target base surface does not deviate radically 
from the curvature of the source base surface at corresponding points. While the algorithm will produce 
a valid surface for any situation when the identi.ed target region has disc topology, when the target 
and source base surfaces are radically different the resulting surface may exhibit distortion of features 
and self-intersections.  The resulting surfaces may exhibit geometric aliasing near sharp features as 
the sampling pattern of the target is used to resample the source. Possible straightforward solutions 
include adaptive re­.nement near sharp features which does not eliminate the problem but reduces the 
scale of artifacts and smoothing which eliminates the artifacts at the expense of detail. A more promising 
approach is mentioned in Section 10.  Except for the .rst one, all of the above limitations are soft 
in the sense that the algorithms we have described still produce a formally valid result. 10 Conclusion 
and Future Work We have described an approach to surface editing that can be ex­ tended in many ways. 
One can imagine a variety of blending modes, combinations of pasting and texture generation, as well 
as other enhancements. One of the important advantages of the ap­ proach is that the structure of the 
target mesh is not changed by pasting (except for possible adaptive re.nement). This means that the complexity 
of the object is not likely to increase quickly each time a feature is added as in the case of boolean 
operations. This is also a disadvantage, as pasting features with complex shapes may result in strong 
mesh distortion. While applicable to a broad range of surfaces, pasting is primar­ ily intended for displacement-map-like 
features. In its current im­ plementation, the further away a feature is from a displacement map, the 
more likely self-intersections are to appear especially when a feature is pasted on a highly curved surface. 
We believe that the applicability of the approach can be extended if hierarchal pasting is used, i.e., 
the feature is decomposed into details and each level is pasted onto the previous. In this case, more 
complex fea­ tures can be pasted more robustly. Many CAD models have sharp creases. While a multiresolution 
surface can approximate sharp creases arbitrarily well, the approx­ imate creases are never perfectly 
sharp and often exhibit aliasing. Furthermore, using details on all levels to introduce a simple cor­ 
ner is wasteful. The representation can be extended [5] to introduce such features by tagging some of 
the edges but without changing connectivity. Note that the parameterization in this approach (Fig­ ure 
10) must conform to the sharp feature. An important future enhancement of our system is the ability to 
paste sharp features.  selected feature initial alpha values pasting without blending smooth alpha values 
pasting with blending     Acknowledgements. The authors thank the staff and students of NYU Media 
Research Lab for their help. Special thanks go to Xin Zhang for his help with writing and debugging parts 
of the code. This work was partially supported by funds from the NYU Center for Advanced Technology, 
IBM Faculty Partnership award, Sloan Foundation Fellowship, NSF award ACI-9978147, CCR-9900528, CCR-0093390, 
and NYU Dean s fellowship.   References [1] www.paraform.com. [2] www.geomagic.com. [3] C. Barghiel, 
R. Bartels, and D. Forsey. Pasting spline surfaces. In Mathematical Methods for Curves and Surfaces: 
Ulvik, Nor­way, pages 31 40. Vanderbilt University Press, 1994. Available at ftp://cgl.uwaterloo.ca/pub/users/rhbartel/Paste.ps.gz. 
[4] H. Biermann, D. Kristjansson, and D. Zorin. Approximate boolean operations on free-form solids. In 
Proceedings of SIGGRAPH 01, pages 185 194, August 2001. [5] H. Biermann, I. Martin, D. Zorin, and F. 
Bernardini. Sharp features on multires­olution subdivision surfaces. In Proceedings of Paci.c Graphics 
2001, 2001. [6] E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological 
meshes. 10(6):350 355, 1978. [7] L. K. Y. Chan, S. Mann, and R. Bartels. World space surface pasting. 
In W. Davis, M. Mantei, and V. Klassen, editors, Proceedings of Graphics Interface, pages 146 154, May 
1997. [8] B. Conrad and S. Mann. Better pasting via quasi-interpolation. In P.-J. Laurent, P. Sablonni`ere, 
and L. L. Schumaker, editors, Curve and Surface Design: Saint-Malo, 1999, pages 27 36, Nashville, TN, 
2000. Vanderbilt University Press. [9] M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. 
Stuetzle. Multiresolution analysis of arbitrary meshes. Proceedings of SIGGRAPH 95, pages 173 182, August 
1995. [10] M. S. Floater. Parametrization and smooth approximation of surface triangula­tions. Computer 
Aided Geometric Design, 14(3):231 250, 1997. [11] L. Freitag, M. Jones, and P. Plassmann. A parallel 
algorithm for mesh smoothing. SIAM J. Sci. Comput., 20(6):2023 2040 (electronic), 1999. [12] X. Gu, S. 
Gortler, and H. Hoppe. Geometry images. In Proceedings of SIG-GRAPH 02, July 2002. [13] I. Guskov, A. 
Khodakovsky, and P. Schr¨ooder. Hybrid meshes. submitted, 2001. [14] I. Guskov, W. Sweldens, and P. Schr¨oder. 
Multiresolution signal processing for meshes. In Proceedings of SIGGRAPH 99, pages 325 334, August 1999. 
[15] I. Guskov, K. Vidimce, W. Sweldens, and P. Schrder. Normal meshes. In Pro­ceedings of SIGGRAPH 00, 
pages 95 102, July 2000. [16] R. Kimmel and J. A. Sethian. Computing geodesic paths on manifolds. Proc. 
Natl. Acad. Sci. USA, 95(15):8431 8435 (electronic), 1998. [17] L. Kobbelt. A variational approach to 
subdivision. Comput. Aided Geom. Design, 13(8):743 761, 1996. [18] L. Kobbelt, S. Campagna, J. Vorsatz, 
and H.-P. Seidel. Interactive multi­resolution modeling on arbitrary meshes. In Proceedings of SIGGRAPH 
98, pages 105 114, July 1998. [19] L. P. Kobbelt. Discrete fairing and variational subdivision for freeform 
surface design. The Visual Computer, 16(3-4):142 150, 2000. [20] V. Krishnamurthy and M. Levoy. Fitting 
smooth surfaces to dense polygon meshes. In Proceedings of SIGGRAPH 96, pages 313 324, August 1996. [21] 
S. Kuriyama and T. Kaneko. Discrete parameterization for deforming arbitrary meshes. In Proceedings of 
Graphics Interface 99, pages 132 139, June 1999. [22] A. Lee, H. Moreton, and H. Hoppe. Displaced subdivision 
surfaces. In Proceed­ings of SIGGRAPH 00, pages 85 94, July 2000. [23] A. W. F. Lee, W. Sweldens, P. 
Schr¨oder, L. Cowsar, and D. Dobkin. Maps: Mul­tiresolution adaptive parameterization of surfaces. In 
Proceedings of SIGGRAPH 98, pages 95 104, July 1998. [24] H. Lee, L. Kim, M. Meyer, and M. Desbrun. Meshes 
on .re. In EG Workshop on Computer Animation and Simulation, 2001. [25] B. L´evy and J.-L. Mallet. Non-distorted 
texture mapping for sheared triangulated meshes. In M. Cohen, editor, Proceedings of SIGGRAPH 98, Computer 
Graphics Proceedings, Annual Conference Series, pages 343 352. Addison Wesley, July 1998. [26] N. Litke, 
A. Levin, and P. Schr¨oder. Fitting subdivision surfaces. In Proceedings of IEEE Visualization 2001, 
pages 319 324, October 2001. [27] C. Loop. Smooth subdivision surfaces based on triangles. Master s thesis, 
Uni­versity of Utah, Department of Mathematics, 1987. [28] M. Lounsbery, T. DeRose, and J. Warren. Multiresolution 
analysis for surfaces of arbitrary topological type. Transactions on Graphics, 16(1):34 73, January 1997. 
[29] M. Ma. The direct manipulation of pasted surfaces. Master s thesis, University of Waterloo, Waterloo, 
Ontario, Canada N2L 3G1, 2000. Available on WWW as ftp://cs-archive.uwaterloo.ca/cs-archive/CS-2000-15/. 
[30] J. Maillot, H. Yahia, and A. Verroust. Interactive texture mapping. In Proceed­ings of SIGGRAPH 
93, pages 27 34, August 1993. [31] B. Oberknapp and K. Polthier. An algorithm for discrete constant mean 
curvature surfaces. In Visualization and mathematics (Berlin-Dahlem, 1995), pages 141 161. Springer, 
Berlin, 1997. [32] H. Køhling Pedersen. Decorating implicit surfaces. In Proceedings of SIG-GRAPH 95, 
pages 291 300, August 1995. [33] H. Køhling Pedersen. A framework for interactive texturing operations 
on curved surfaces. In Proceedings of SIGGRAPH 96, pages 295 302, August 1996. [34] U. Pinkall and K. 
Polthier. Computing discrete minimal surfaces and their con­jugates. Experiment. Math., 2(1):15 36, 1993. 
[35] K. Polthier and M. Schmies. Straightest geodesics on polyhedral surfaces. In H.C. Hege and K. Polthier, 
editors, Mathematical Visualization. Springer Verlag, 1998. [36] E. Praun, A. Finkelstein, and H. Hoppe. 
Lapped textures. In Proceedings of SIGGRAPH 00, pages 465 470, July 2000. [37] E. Praun, W. Sweldens, 
and P. Schr¨oder. Consistent mesh parameterizations. In Proceedings of ACM SIGGRAPH 01, pages 179 184, 
August 2001. [38] K. Pulli and M. Lounsbery. Hierarchical editing and rendering of subdivision surfaces. 
Technical Report UW-CSE-97-04-07, Dept. of CS&#38;E, University of Washington, Seattle, WA, 1997. [39] 
A. Sheffer and E. de Sturler. Surface parameterization for meshing by triangula­tion .attening. In Proc. 
9th International Meshing Roundtable, pages 161 172, 2000. [40] J. Stam. Exact evaluation of catmull-clark 
subdivision surfaces at arbitrary pa­rameter values. In Proceedings of SIGGRAPH 98, pages 395 404, July 
1998. [41] H. Suzuki, Y. Sakurai, T. Kanai, and F. Kimura. Interactive mesh dragging with an adaptive 
remeshing technique. The Visual Computer, 16(3-4):159 176, 2000. [42] C. L. F. Tsang. Animated surface 
pasting. Master s thesis, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1, 1998. Available 
at ftp://cs-archive.uwaterloo.ca/cs-archive/CS-98-19/. [43] F. W. Warner. Foundations of differentiable 
manifolds and Lie groups. Springer-Verlag, New York, 1983. Corrected reprint of the 1971 edition. [44] 
D. Zorin, P. Schr¨oder, and W. Sweldens. Interpolating subdivision for meshes with arbitrary topology. 
In Proceedings of SIGGRAPH 96, pages 189 192, Au­gust 1996. [45] D.Zorin,P.Schr¨oder,andW.Sweldens.Interactivemultiresolutionmeshediting. 
In Proceedings of SIGGRAPH 97, pages 259 268, August 1997. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566584</article_id>
		<sort_key>322</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Pointshop 3D]]></title>
		<subtitle><![CDATA[an interactive system for point-based surface editing]]></subtitle>
		<page_from>322</page_from>
		<page_to>329</page_to>
		<doi_number>10.1145/566570.566584</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566584</url>
		<abstract>
			<par><![CDATA[We present a system for interactive shape and appearance editing of 3D point-sampled geometry. By generalizing conventional 2D pixel editors, our system supports a great variety of different interaction techniques to alter shape and appearance of 3D point models, including cleaning, texturing, sculpting, carving, filtering, and resampling. One key ingredient of our framework is a novel concept for interactive point cloud parameterization allowing for distortion minimal and aliasing-free texture mapping. A second one is a dynamic, adaptive resampling method which builds upon a continuous reconstruction of the model surface and its attributes. These techniques allow us to transfer the full functionality of 2D image editing operations to the irregular 3D point setting. Our system reads, processes, and writes point-sampled models without intermediate tesselation. It is intended to complement existing low cost 3D scanners and point rendering pipelines for efficient 3D content creation.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D content creation]]></kw>
			<kw><![CDATA[parameterization]]></kw>
			<kw><![CDATA[point-based graphics]]></kw>
			<kw><![CDATA[surface painting]]></kw>
			<kw><![CDATA[surface sculpting]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP37026031</person_id>
				<author_profile_id><![CDATA[81100289561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zwicker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40028797</person_id>
				<author_profile_id><![CDATA[81100582775]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pauly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382454</person_id>
				<author_profile_id><![CDATA[81100609274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knoll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40025406</person_id>
				<author_profile_id><![CDATA[81100260276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>199429</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGRAWALA, M., BEERS, A. C., AND LEVOY, M. 1995. 3d painting on scanned surfaces. pages 145-150. ISBN 0-89791-736-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601673</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALEXA, M. 2001. Point set surfaces. In IEEE Visualization.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ALIAS WAVEFRONT 2001. Maya. http://www.aliaswavefront.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ARIUS3D 2001. pointstream. http://www.pointstream.net.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>95535</ref_obj_id>
				<ref_obj_pid>95508</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[ASHBY, MANTEUFFEL, AND SAYLOR 1990. A taxonomy for conjugate gradient methods. J. Numer. Anal., 27:1542-1568.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B. AND LEVOY, M. 1996. A volumetric method for building complex models from range images. In SIGGRAPH 96, pages 303-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[EYETRONICS 2001. ShapeSnatcher. http://www.eyetronics.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248308</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FLOATER, M. S. 1997. Parametrization and smooth approximation of surface triangulations. Comp. Aided Geom. Design, 14:231-250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>373171</ref_obj_id>
				<ref_obj_pid>373164</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FLOATER, M. S. AND REIMERS, M. 2001. Meshless parameterization and surface reconstruction. Comp. Aided Geom. Design, 18:77-92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383308</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[LEVY, B. 2001. Constrained texture mapping for polygonal meshes. In SIGGRAPH 2001, pages 417-424. Los Angeles, CA, August 12-17, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280930</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[LEVY, B. AND MALLET, J.-L. 1998. Non-distorted texture mapping for sheared triangulated meshes. In SIGGRAPH 98, pages 343-352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383310</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[OH, B. M., CHEN, M., DORSEY, J., AND DURAND, F. 2001. Image-based modeling and photo editing. In SIGGRAPH 2001, pages 433-442.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[O'NEILL, B. 1966. Elementary Differential Geometry. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383301</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PAULY, M. AND GROSS, M. 2001. Spectral processing of point-sampled geometry. In SIGGRAPH 01, pages 379-386.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PEDERSEN, H. K. 1995. Decorating implicit surfaces. In SIGGRAPH 1995, pages 291-300. Los Angeles, CA, August 6-11, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383264</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PERRY, R. N. AND FRISKEN, S. F. 2001. Kizamu: A system for sculpting digital characters. In SIGGRAPH 2001, pages 47-56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344936</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[PFISTER, H., ZWICKER, M., VANBAAR, J., AND GROSS, M. 2000. Surfels: Surface elements as rendering primitives. In SIGGRAPH 2000, pages 335-342. New Orleans, LA, July 23-28, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[RIGHT HEMISPHERE 2001. DeepPaint3D. http://www.us.deeppaint3d.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[RUSINKIEWICZ, S. AND LEVOY, M. 2000. QSplat: A multiresolution point rendering system for large meshes. In SIGGRAPH 2000, pages 343-352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134037</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SZELISKI, R. AND TONNESEN, D. 1992. Surface modeling with oriented particle systems. In SIGGRAPH 92, pages 185-194. July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[WELCH, W. AND WITKIN, A. 1994. Free-form shape design using triangulated surfaces. In SIGGRAPH 94, pages 247-256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383300</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[ZWICKER, M., PFISTER, H., VANBAAR, J., AND GROSS, M. 2001. Surface splatting. In SIGGRAPH 2001, pages 371-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pointshop 3D: An Interactive System for Point-Based Surface Editing Matthias Zwicker Mark Pauly Oliver 
Knoll Markus Gross ETH Zürich  Figure 1: 3D content creation: Scanning of a physical model (left). Editing 
of the point-sampled object: Carving (middle), texturing (right). Abstract We present a system for interactive 
shape and appearance editing of 3D point-sampled geometry. By generalizing conventional 2D pixel editors, 
our system supports a great variety of different inter­action techniques to alter shape and appearance 
of 3D point mod­els, including cleaning, texturing, sculpting, carving, filtering, and resampling. One 
key ingredient of our framework is a novel con­cept for interactive point cloud parameterization allowing 
for dis­tortion minimal and aliasing-free texture mapping. A second one is a dynamic, adaptive resampling 
method which builds upon a con­tinuous reconstruction of the model surface and its attributes. These 
techniques allow us to transfer the full functionality of 2D image editing operations to the irregular 
3D point setting. Our sys­tem reads, processes, and writes point-sampled models without intermediate 
tesselation. It is intended to complement existing low cost 3D scanners and point rendering pipelines 
for efficient 3D content creation. Keywords: 3D Content Creation, Point-Based Graphics, Surface Painting, 
Surface Sculpting, Texture Mapping, Parameterization 1 INTRODUCTION When 2D digital photography became 
instrumental, it immediately created the need to efficiently edit and to interactively improve the quality 
of digital images. Hence, considerable effort has been devoted to the development of such systems, both 
for the private and for the professional user of digital cameras. This conventional photo editing software 
includes a variety of individual tools rang­ing from simple artifact removal or paint brushes to highly 
special- Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make 
digital or hard copies of part or all of this work for personal or classroom use is granted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for components of this work owned by others than 
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 ized image effect filters. The most popular package is undoubtedly Adobe 
s Photoshop, providing a set of powerful tools for user guided alteration of 2D image data. In recent 
years advances in 3D digital photography spawned scanning systems that acquire both geometry and appearance 
of real-world objects. A major application for such 3D range cameras is for instance the ready creation 
of 3D internet content for e-com­merce applications. However, the process of 3D model production is often 
quite tedious and requires a variety of different techniques including registration of raw scans, resampling, 
filtering, sculpt­ing, or re-texturing. The early stages of processing of 3D photos frequently produce 
3D point clouds, which are most often con­verted into triangle meshes for further modeling. In this paper 
we present an interactive 3D photo editing system which is entirely based on points. It takes an irregular 
point-sampled model as an input, provides a set of tools to edit geometry and appearance of the model, 
and produces a point-sampled object as an output. Conceptually, 2D photo editing systems are based on 
pixels as the major image primitive. As a consequence, all editing tools operate on subsets of image 
pixels, often making heavy use of adjacency and parameterization. Despite the multilayered structure 
of an image, the regular sampling lattice makes many pixel opera­tions simple and efficient. Furthermore, 
pixel processing is mostly carried out on color or transparency channels changing appearance attributes 
of the image only. Geometry is typically less important. If at all, range layers are manipulated by converting 
them into intensity fields. In this work we generalize 2D photo editing to make it amena­ble to 3D photography. 
While existing 3D geometry-oriented mod­eling, painting or sculpting systems are either based on polynomials 
[Alias Wavefront 2001], triangle meshes [Agrawala et al. 1995, Right Hemisphere 2001], implicits [Pedersen 
1995, Perry and Frisken 2001], or images [Oh et al. 2001], our approach is completely different in spirit. 
It is purely founded on irregular 3D points as powerful and versatile 3D image primitives. By gen­eralizing 
2D image pixels towards 3D surface pixels (surfels [Szeliski and Tonnesen 1992, Pfister et al. 2000]) 
we combine the functionality of 3D geometry based sculpting with the simplicity and effectiveness of 
2D image based photo editing. Point samples provide an abstraction of geometry and appear­ance, since 
they discretize position and texture on the object sur­face. However, as opposed to triangle primitives, 
they do not store information about local surface connectivity. Unlike 2D image pixels, the absence of 
local topology in combination with the irreg­ularity of the sampling pattern poses great challenges to 
the design of 3D photo editing tools. We found that the two key ingredients for such tools are interactive 
parameterization and dynamic resa­mpling. For instance, distortion minimal re-texturing or surface carving 
both demand a flexible parameterization of the point cloud. In addition, points discretize geometry and 
appearance attributes at the same rate in object space. Thus, fine-grain surface detail embossing of 
an existing object with a high resolution depth map can lead to heavy aliasing and requires a dynamic 
adaptation of the sampling rate. In the following, we will present a set of methods to solve the problems 
stated above and integrate them into a versatile system. Specifically, our paper makes the following 
contributions: Interactive parameterization (Section 3): By extending prior work on triangle meshes [Levy 
2001] we designed a novel method for distortion minimal parameterization of point clouds. The algo­rithm 
allows for constraints and enables users to interactively adapt the parameterization to input changes. 
A multigrid approach accomplishes robust and efficient computation. Dynamic resampling (Section 4): As 
a prerequisite, changes of the sampling rate demand a continuous reconstruction of the model surface 
and of its attributes. To this end, we introduce a novel surface representation based on a parameterized 
scattered data approximation. In addition, we propose a method which dynamically adapts the number of 
samples to properly represent fine geometric or appearance details. We combined our sampling strategy 
with existing texture antialiasing techniques for point­sampled geometry [Zwicker et al. 2001]. Editing 
framework (Section 5): Our system provides a unified conceptual framework to edit 3D models. It supports 
a great vari­ety of individual tools to alter the geometry and appearance of irregular point-sampled 
geometry. The scope of possible opera­tions goes well beyond the functionality of conventional 2D photo 
editing systems. We implemented re-texturing, sculpting, emboss­ing, and filtering, however, new effect 
filters can be added very easily. Overall, our system combines the efficiency of 2D photo editing with 
the functionality of 3D sculpting systems. Pointshop 3D is not intended to be a point-based modeling 
sys­tem. As such, editing of the surface geometry is confined to nor­mal displacements and to moderate 
changes of the surface structure only. It is rather designed to complement low cost scan­ning devices 
[Eyetronics 2001] and point-based 3D viewers [Rusinkiewicz and Levoy 2000, Pfister et al. 2000, Arius3D 
2001], yielding a powerful pipeline for efficient 3D content creation and display. Pointshop 3D explores 
the usability of point primitives for surface editing and constitutes an alternative to conventional 
polygonal mesh or splines based approaches. Since our algorithms are based on k -nearest neighbor search, 
input data with a substantial amount of noise or highly irregular sampling distribution, e.g., as obtained 
from multiple merged range scans, can lead to instabilities. In these cases, the raw scans have to be 
resampled to a clean point cloud using standard meth­ods [Curless and Levoy 1996], also allowing the 
computation of accurate normals. 2 SYSTEM OVERVIEW Our editing framework originates from the motivation 
to provide a wide range of editing and processing techniques for point-sampled 3D surfaces, similar to 
those found in common photo editing tools for 2D images. To give an overview of our system we will first 
describe a typical photo editing operation on an abstract level. Then we will explain how these concepts 
can be transferred to sur­face editing, commenting on the fundamental differences between images and 
surfaces. This will serve as a motivation for the tech­niques and algorithms described in the following 
sections. We also introduce an operator notation for general editing operations that will be used throughout 
the paper. A 2D image I can be considered a discrete sample of a continu­ous image function containing 
image attributes such as color or transparency. Implicitly, the discrete image I always represents the 
continuous image, and image editing operations are performed directly on the discrete image. The continuous 
function can be computed using a reconstruction operator whenever necessary. We describe a general image 
editing operation as a function of an original imageI and a brush image B . The brush image is used as 
a general tool to modify the original image. Depending on the considered operation, it may be interpreted 
as a paint brush or a discrete filter, for example. The editing operation involves the fol­lowing steps: 
First, we need to specify a parameter mapping F that aligns the imageI with the brush B . For example,F 
can be defined as the translation that maps the pixel at the current mouse position to the center of 
B . Next, we have to establish a common sampling grid for I and B , such that there is a one-to-one corre­spondence 
between the discrete samples. This requires a resam­pling operation . that first reconstructs the continuous 
image function and then samples this function on the common grid. Finally, the editing operator . combines 
the image samples with the brush samples using the one-to-one correspondence estab­lished before. We 
thus obtain the resulting discrete imageI' as a concatenation of the operators described above: ..F( 
I ).B ). (1) I'= ( (), () Our goal is now to generalize the operator framework of Equation (1) to irregular 
point-sampled surfaces, as illustrated in Figure 2. Formally, we do this by replacing the discrete imageI 
by a point-based surface S . Hence, we represent a 3D object as a set of irregular samplesS = {} of its 
surface. Since the samples sisi are a direct extension of image pixels, we will also call them sur­fels 
[Szeliski and Tonnesen 1992, Pfister et al. 2000]. As summa­rized in Table 1, each surfel stores appearance 
attributes, including color, transparency, or material attributes, and shape attributes, such as position 
and normal. Let us now consider what effects the transition from image to surface has on the individual 
terms of Equation (1). Parameterization F. For photo editing, the parameter map­ping Fis usually specified 
by a simple, global 2D to 2D affine mapping, i.e., a combination of translation, scaling, and rotation. 
Mapping a manifold surface onto a 2D domain is much more involved, however. In our system, the user interactively 
selects subsets, or patches, of S that are parameterized, as described in Section 3. In general, such 
a mapping leads to distortions that can­Figure 2: Overview of the operator framework for point-based 
sur­face editing. Table 1: Attributes of a surface sample si . ATTRIBUTE ABBREVIATION Position Normal 
Color Transparency Material properties xi ni ci ai mi same simple interface that specifies a tool by 
a set of bitmaps and few additional parameters. For example, a sculpting tool is defined by a 2D displacement 
map, an alpha mask and an intrusion depth. 3 PARAMETERIZATION In our system, the user interactively 
selects a subsetS of the sur­face S , which we call a patch. We compute a parameterization of the patchF:S 
.01]×01] that assigns parameter coordi­ [, [, nates ui to each pointsi inS and then apply the editing 
operation on the parameterized patch. The user chooses between two types of interaction schemes to select 
a patch and compute the parame­terization: A selection interaction for large patches, described in Sections 
3.1 and 3.2, and a brush interaction for small patches pre­sented in Section 3.3. 3.1 Selection Interaction 
In this interaction scheme, the user triggers each step in the evalua­tion of Equation (1) separately. 
First, she marks an arbitrary sur­face patch using a dedicated selection tool and specifies a set of 
feature points. In a next step, she initiates a constrained minimum distortion parameterization algorithm 
that uses the feature points, as described in Section 3.2. Then she typically performs a series of editing 
operations on the parameterized patch, such as filtering or texture mapping. This process is illustrated 
in Figure 3. not be avoided completely. In Section 3.2, we present an efficient method that automatically 
minimizes these distortions, and at the same time lets the user intuitively control the mapping. Resampling 
.. Images are usually sampled on a regular grid, hence signal processing methods can be applied directly 
for resam­pling. However, the sampling distribution of surfaces is in general irregular, requiring alternative 
methods for reconstruction and sampling. We apply a scattered data approximation approach for reconstructing 
a continuous function from the samples, as described in Section 4. We also present a technique for resampling 
our modified surface function onto irregular point clouds in Section 4.2. A great benefit of our system 
is that it supports adap­tive sampling, i.e., works on a dynamic structure. This allows us to concentrate 
more samples in regions of high textural or geometric detail, while smooth parts can be represented by 
fewer samples. Editing .. Once the parameterization is established and resam­pling has been performed, 
all computations take place on discrete samples in the 2D parameter domain. Hence we can apply the full 
functionality of photo editing systems for texturing and texture fil­tering. However, since we are dealing 
with texture and geometry, the scope of operations is much broader. Additional editing opera­tors include 
sculpting, geometry filtering and simplification. As will be described in Section 5, all of these tools 
are based on the a) b)c) Figure 3: Selection interaction: a) Patch selection and feature points. b) Texture 
map with feature points. c) Texture mapping. 3.2 Minimum Distortion Parameterization We describe a novel 
algorithm for computing minimum distortion parameterizations of point-based objects. Our approach is 
based on an objective function, similar to Levy s method for polygonal meshes [Levy 2001]. However, we 
then derive a discrete formula­tion for surfaces represented by scattered points without requiring any 
tesselation. We solve the resulting linear least squares problem efficiently using a multilevel approach 
by hierarchical clustering of points. Objective Function. Let us denote a continuous parameterized surface 
patch by XS . The patch is defined by a one-to-one map­pingX: 01]×01]. .IR3 which for each [, [, XS pointT 
T uv) in 01 ]×01]represents a point x = (,, u = (,[,[,xyz) on the surface: The mappingX describes a parameterization 
of the surface, withU = X 1 its inverse. Our method computes a parameterization that optimally adapts 
to the geometry of the surface, i.e., mini­mizes metric distortions. Additionally, the user is able to 
specify a set M of point correspondences between points on the surface xj and points in the parameter 
domain pj , j .M , to control the map­ping. This can be expressed as the following objective function: 
x u() u 01[, ] 01[, ]× X u().. = y u() .x .XS = (2) z u() ()= {() xj}+ e.udu , (3) CXX pj()   . 2 
. j .M . 2 . 2 . () () (4) where .u= . X.,r . d., . .u ...r 2 . . . . cos()and X.,r = X u + () . r . 
. (5) u . . . sin() The first term in (3) represents the fitting error as the sum of the squared deviations 
from the user specified data points. The second term measures the smoothness, or distortion, of the parameteriza­tion. 
At each surface point,.u integrates the squared curvature () of the parameterization in each radial direction 
using a local polar () .u is zero, the parameterization reparameterization X.,r . If() u atu is a so 
called polar geodesic map, which preserves arc length in each radial direction [O'Neill 1966, Welch and 
Witkin 1994]. With the parameter e, the user additionally controls the relative weight of the data fitting 
error and the smoothness constraint. The desired parameterization X can be obtained by computing the 
minimum of the functional (3). We now describe how to set up and minimize (3) in the discrete case. Discrete 
formulation. Given a set of distinct points {}on xithe surface, our goal is to assign to each point xi 
a point ui in the parameter domain, such that the objective function is minimized. In other words, we 
are solving for the unknown discrete mapping U:xi .ui and hence we reformulate (3) by substituting the 
unknownU for X . Moreover, we assume that the parameteriza­tion is piecewise linear, thus the second 
derivative of U is not defined at the pointsxi in general. As an approximation, we dis­cretize the smoothness 
criterion by computing at each pointxi the squared difference of the first derivatives along a set of 
normal sections. This yields the following objective function C U: () n .().(). 2 2 . U xiU xi CU= . 
{ uj }+e.. ----.v--j --- -- - ----- ,(6) () pj . -----------. . .v j . j .Mi =1 j .Ni where n is the 
number of points in the patch,Ni specifies the set of normal sections, and vj and v j are unit vectors 
on the surface given by the normal section. Directional Derivatives. We compute the directional deriva­tives 
.U xi/.vj and .()/( )v ()( ) U xi. jin (6) as illustrated in Figure 4: At each point xi , we collect 
a set Ni = {i1 ik}con­taining the indices of its k nearest neighbors, typically k = 9. For each neighbor 
xj , j .Ni , we determine the planeP defining the normal section, which is given by the normal ni atxi 
and the vec­tor vj = xi xj . We then choose the two points xaand xß, , , such that the angles betweenva 
= xa xi and aß.Ni vß = xß xi and the planeP are minimal, while the angles between vj and va, and between 
vj and vßare bigger than 90 degrees. Otherwise, the normal section crosses the boundary of the xß n i 
 xj plane defining normal section Figure 4: Computing directional derivatives using normal sections. 
patch, hence we ignore it. This procedure is sufficient to handle patches with boundaries. Next, we compute 
the direction v j of the intersection line of the planeP and the plane given by xi , xa, and xß(see Figure 
4). Assuming a piecewise linear mapping U between xi and xj , the directional derivative at xi alongvj 
is simply ui uj . ()= -- ------------. (7) U xi .vj vj Likewise, we compute the derivative along v j 
as described in [Levy 2001, Levy and Mallet 1998] by assuming a piecewise lin­ear mapping on the triangle 
defined by the points xi,,xaxß. This leads to a linear expression of the form . ()=+ +, (8) U xiaiuiaaua 
aßuß .v j where the coefficientsai,,aaaßare determined by the points xi,,xaxß, as presented in detail 
in [Levy and Mallet 1998]. In contrast to Floater s shape preserving weights [Floater and Reimers 2001, 
Floater 1997], our method can be used as an extrap­olator, since we do not enforce the coefficients of 
(8) to be a con­vex combination. As a consequence, we do not have to specify a convex boundary. Still, 
our method has the reproduction property: If all points lie in a plane and at least three or more points 
obeying an affine mapping are given as fitting constraints, the resulting parameterization will be an 
affine mapping, too. Moreover, we do not need to construct a local triangulation at each point as in 
[Floater and Reimers 2001] to establish our constraints. Note that the parameterization is not guaranteed 
to be bijective. It is rather left to the user to select a suitable patch and appropriate point cor­respondences 
to obtain the desired mapping. Multigrid Least Squares Solver. The discrete objective function of (6) 
is now a sum of squared linear relations of the gen­eral form n . 2. 0 aji , . C U= ...bj . () ui = 
b Au 2 , (9) . 0 aji , j . . i =1 T whereu is a vector of all unknownsui = (ui,vi)and the coeffi­cients 
result from (7) and (8). We compute this linear least aji , squares problem using normal equations and 
conjugate gradient methods [Ashby et al. 1990]. The convergence of such iterative solvers can be further 
accelerated by efficient multilevel tech­niques. To this end, we designed a hierarchical strategy as 
illus­trated in Figure 5. In a top-down pass, we contract the system by recursively clustering the unknowns 
ui . The clustering is driven by the spatial proximity of the corresponding surface points xi , and each 
cluster yields one unknown on the current level. In a bot­tom-up pass, we solve (9) starting with the 
coarsest level. The solution is then prolonged by assigning it as an initial value to the next higher 
resolution level. This process is repeated recursively up to the original resolution. ui     Figure 
5: Multilevel scheme to solve Equation (9). Figure 3 depicts an example of our parameterization technique. 
We first apply the minimum distortion parameterization on a com­plex surface patch, controlling the mapping 
by specifying a set of corresponding feature points. Finally, we perform a texture map­ping operation. 
3.3 Brush Interaction Here, the user has to select a brush image and an editing operation first. Then 
he moves the brush over the surface while continuously triggering painting events. Each painting event 
corresponds to one evaluation of Equation (1), which is performed as follows (see Figure 6): We center 
the brush image at the user defined location on the surface, typically at the current mouse position. 
We then align the brush with the tangent plane at this point and orthogo­nally project the surface pointsxi 
onto the brush image plane. For each point having its projection inside the brush image, we assign the 
resulting coordinates in the brush image plane as its parameter coordinates, yielding = .(), where .denotes 
the orthogo­ ui xi nal projection. This process combines the patch selection and parameterization step 
using a simple projection. It is therefore suit­able only for small patches with limited curvature. Finally 
we apply the pre-selected editing operation, as illustrated in Figure 6c. Figure 6: Brush interaction: 
a) Aligning the brush tool to the tan­gent plane. b) Patch selection and parameterization by orthogonal 
projection. c) Editing operation, e.g. texture mapping.  4 RESAMPLING Given a set of sample points S 
representing some parameterized, continuous surface XS , the resampling operator strives to generate 
a new sampleS'= .S of the same surface. The challenge of () this operation is to minimize information 
loss while avoiding sam­pling artifacts. Resampling consists of two separate steps: First, the reconstruction 
step should provide a smooth, accurate approxi­mation of the continuous surface XS , i.e., of all its 
shape and appearance attributes as in Table 1. To avoid aliasing, the actual sampling step should then 
properly band-limit XS before evaluat­ing it at the new sampling locations. 4.1 Reconstruction As described 
in Section 3, during a typical editing session the user repeatedly modifies the surface parameterization. 
Hence a para­mount objective of the surface reconstruction procedure is that it can be quickly recomputed 
under changes of the parameterization. We therefore apply the following approach consisting of two stages: 
first, we perform a local surface fitting step, followed by a parameter matching step. The local fitting 
procedure is indepen­dent of the global parameterization computed in Section 3, hence it can be performed 
as a preprocess. The parameter matching step then uses the global parameterization as a common frame 
of refer­ence for the local fits, and blends them to a smooth surface. We present two alternative techniques 
to perform the matching step: For general patches being parameterized using our method from Section 3.2, 
we develop an optimization based technique. For the parameterization by projection approach described 
in Section 3.3, we use a more efficient, analogous matching by projection tech­nique. Local Surface Fitting. 
At each point xi , we compute a local approximation of the surface using its k nearest neighbors, denoted 
by the index set Ni . This requires a local parameterization of the neighbors, which we compute by projecting 
the points xj , j .Ni onto tangent plane at xi . We denote the local parameter i coordinates of thexj 
by tj . As a local surface approximation, we i ti compute polynomial fitting functions f()using a scheme 
simi­lar to [Welch and Witkin 1994]. However, it turned out that for our purpose a linear fit, i.e., 
the tangent plane, is sufficient. We then ri ti compute a reconstruction kernel() that will be used to 
blend the local fits. It can be interpreted as a weight indicating the confi­dence that the fitting function 
accurately represents the surface. Currently, we use radially symmetric Gaussians centered at tii . Their 
variance sis determined by computing the radiusdi of the i i smallest circle containing the tj , where 
j .Ni . Typically, we choose k = 9 and s= 1.5 · . i di Parameter Matching by Optimization. This step 
brings all local parameterizations into one common frame of reference. For i each local parameterization, 
we look for a mapping .i: t.u i such that the local parameter coordinates tj of a point xj match its 
patch parameter coordinates computed in Section 3.2, hence iuj = .(), j . In our method, we restrict 
the mappings . uji tj .Ni i to be affine and compute them by minimizing 2 ( .()), (10) . uji tjj .Ni 
which is again a linear least squares problem. Since both the local parameterization and the patch parameterization 
are smooth in a neighborhood , local affine mappings .provide a sufficient Ni i approximation and have 
lead to good results in our system. Instead of applying this two-step approach with a local fit fol­lowed 
by parameter matching, we could also directly compute the fitting functions in the patch parameter domain. 
However, our scheme is more efficient when the global parameterization changes often. We then have to 
recompute the parameter matching only, instead of recomputing the fitting functions and the reconstruction 
kernels. Parameter Matching by Projection. In Section 3.3, we compute the parameter coordinates ui of 
a point xi as = .(), where . denotes an orthogonal projection. In the ui xi same way, we can then project 
the fitting functionsfi to the patch i ii parameter domain, i.e., u = .f), thus .()= ( () (().f). i ti 
ti t Inverting this projection amounts to ray-tracing the fitting func­tions. For linear basis functions, 
this can be implemented by an efficient scan conversion. Blending the Fitting Functions. After establishing 
the map­pings .i from the local parameterizations to the patch parameter domain, we define fitting functions 
fuand reconstruction ker­ i() i nels ru in the patch parameter domain as i()= f( ()) i() fui .i t i 
and ru= f(()). We now obtain a continuous surface func­ i() i ri ttionXS u as the weighted sum () nn 
. ... . ... ()= furu/ ru. (11) u () i() i() XS .. i ... . . ... i =1 i =1 of fitting functions f i and 
reconstruction kernels r i . Our approach is similar in spirit to the construction of point set surfaces 
introduced in [Alexa 2001], in that both methods use local parameterizations and polynomials to approximate 
the surface. However, instead of implicitly defining the surface by a projection operator, we blend the 
local approximations using a global param­eterization. In our system, we use linear fitting functions 
for the surface position, and constants for the other surface attributes listed in Table 1. We illustrate 
curve reconstruction using constant and lin­ear fitting functions in Figure 7. In Figure 7a, the constant 
fitting functions simply reproduce the data points, whereas in 7b, linear fitting functions result in 
straight lines aligned with the tangential directions at the data points. Clearly, linear functions lead 
to a more accurate approximation of the data points (Figure 7c). 16 516 858 12 12 8 8 4 4 2 5 101520 
5 101520 a) b) 123 4567811.5 constant fits linear fits 0.8 1 0.6 0.4 0.5 0.2 0 0 12345678 0.2 0.4 
0.6 0.8 1 c) d) Figure 7: Curve reconstruction: a) Reconstruction with constant fitting functions. b) 
Reconstruction with linear fitting functions. c) Absolute error at the data points. d) The reconstruction 
functions in the parameter domain.  4.2 Sampling The actual sampling includes two aspects: First we 
need to find a suitable resampling operator . that specifies the location of the new samples. Then, we 
evaluate the surface function according to the new sampling distribution. Resampling Operators. We provide 
three resampling opera­tors specifying different resampling distributions, i.e. sets of new sampling 
locations {} in the patch parameter domain: ui Brush Resampling .S . In this method, we use the original 
surface points as the resampling grid. Hence we have to resample the brush, yielding a new sample of 
the brush = .(). Resa- BS SB mpling the surface conceptually results in the same sample S = .(), therefore 
it is not necessary to perform this opera- SS tion. The advantage of this method is that we do not have 
to insert any new surface points, and there is no information loss inS due to resampling. Surface Resampling 
.B . In many operations, such as tex­ture mapping, we want to resample the surface at the sampling dis­tribution 
of the brush B that represents the texture, avoiding any loss in texture quality. Hence we generate a 
new sample of the sur­face = .BS. Since the brush is often sampled on a regular SB () grid, the evaluation 
of the surface function can be optimized using incremental calculations, e.g. for evaluating the Gaussian 
weight functions and polynomial fitting functions [Zwicker et al. 2001]. Adaptive Resampling .A . If 
the sampling density of the sur­face or the brush varies significantly, it occurs that in some areas 
in a patch the surface sampling distribution is finer, and in others the brush sampling density. In this 
case, both operators. and . SB fail to preserve detail of either the brush or the surface. Therefore 
we propose a simple adaptive resampling operator .A that locally decides whether to use samples of S 
or of B . The decision is based on the comparison of the radii of the Gaussian reconstruc­tion functions 
inS and B , since these radii directly correspond to the local sampling density. Band-Limiting the Surface 
Function. The goal of this pro­cess is to avoid aliasing artifacts when evaluating the surface func­tion 
at the resampling grid. This can be achieved by properly band­limiting the continuous function before 
sampling. In a regular sig­nal processing framework, band-limiting is performed by convolv­ing the function 
with a suitable low-pass filter. Our approach is inspired by signal processing, approximating this procedure 
with irregular sampling distributions, however. Given a resampling operator. specifying a set of new 
sampling locations {}, we uifirst compute corresponding new reconstruction kernels uas ri() described 
in Section 4.1. To sample the surface attributes of a point sj , we approximate the convolution of the 
surface function XS with the reconstruction function rj and evaluate it at uj : . n .. n . . ... = f().()/ 
.() ( (), (12) sj ujujujXS .ri)uj .. ii . .. i . . ... i =1 i =1 where .()= u.ri() ur i() u is also called 
a resampling filter. i With Gaussian weight functions, the resampling filter can be com­puted explicitly 
[Zwicker et al. 2001]. Note that this resampling procedure is applied to all surface attributes, such 
as color, posi­tion, normal, etc.   5 SURFACE EDITING The resampling method of Section 4 provides samples 
of the sur­S. (()B..B with identi­ face = .FS )and of the brush = () cal sampling distribution. We can 
thus combine the two by applying an editing operator directly on the discrete coefficients. Note that 
bothS. and B.represent all the surfel attributes of < summarize timings of the multilevel solver for 
different patches xi bi di ... xi ' , (13) with varying sizes on this object, recorded on a Pentium 
IV at 2 GHz. The data for the textured patch depicted in Figure 10 is shown in the third row of Table 
2. wherebi is the base point on the reference plane and n the plane normal (see Figure 8). Carving operations 
can also be applied to Table 2: Timings of the multilevel solver: Number of unknowns, time to setup the 
least squares system, time to compute the initial rough surfaces (see Figure 9d), where normal displacements 
fail solution, time to update when one feature point is modified. due to the strong variations of the 
surface normals. UNKNOWNS SETUP INIT UPDATE 58170 6.9 sec. 2.2 sec. 1.8 sec. 107394 14.9 sec. 8.3 sec. 
6.6 sec. 215628 26.3 sec. 12.1 sec. 7.6 sec. brush image normal displacement carving After texturing, 
we additionally embossed a displacement map Table 1. Depending on the intended functionality, an editing 
opera­tor will then manipulate a subset of these surface attributes, such as texture or material properties. 
In the following we will describe some of the editing operators that we have implemented in our sys­tem. 
A prime will denote the manipulated attributes, e.g., xi ' describes the position of a surfel of the 
edited surface. Quantities that stem from the brushB. are marked with a bar, e.g.,ci is the color of 
a brush sample. All other variables are part of the surface function S. . Painting. Painting operations 
modify surface attributes by alpha-blending corresponding surface and brush coefficients. For example, 
the surface texture can be altered by applying the paint­ing operator on the color values, i.e., ci ' 
= ai · ci + (1 ai)· ci , where ai is an alpha value specified in the brush function (see Figure 9a). 
Similarly, painting can be applied to other attributes such as transparency or material properties. Sculpting. 
We have implemented two variations of sculpting operations that modify the geometry of the surface. The 
first option is to apply normal displacements to the surfel positions, i.e., xi ' = xi + di · ni , where 
di is a displacement coefficient given in the brush function. As illustrated in Figure 9c, this type 
of editing operation is particularly suitable for embossing or engraving. On the other hand, the carving 
operation is motivated by the way art­ists work when sculpting with clay or stone. It implements a chisel 
stroke that removes parts of the surface in the fashion of a CSG-type intersection. The editing tool 
is defined with respect to a reference plane that is specified by the surface normal of the touching 
point and an intrusion depth. The new surfel position is then given by a) b) d) Figure 9: Editing operations: 
a) Texturing with alpha blending. b) Texture filtering. c) Normal displacement. d) Carving on a rough 
surface.  6 RESULTS We have implemented a point-based surface editing system featur­ing the techniques 
described in the previous sections. Figure 10 depicts an example of a constrained texture mapping operation 
with 10 feature points on a model with 218k points. In Table 2 we Filtering. Filtering is a special 
kind of editing operation that modifies the samples of the original model using a user-specified filter 
function f . First we apply the filter function to S. yielding f S. = fS.) , then we ( combine filtered 
and original attributes using the brush function for alpha blending. As an example, con­ f f sider texture 
filtering, i.e., ci ' = a· ci + (1 a) · ci , whereci is the filtered color value (illustrated in Figure 
9b). The filter func­tion is usually implemented as a discrete convolution. We can therefore implement 
arbitrary discrete linear filters by simply choosing the appropriate kernel grid. Filters can be applied 
to any attribute associated with a surfel, e.g. color, normal or distance from the reference plane for 
geometric offset filtering. Note that filtering with large kernels can be implemented efficiently in 
the spectral domain, similar to [Pauly and Gross 2001]. on the model as shown in Figure 11a. To produce 
Figure 11b, we started with a sphere with 114k points and then applied the texture from the moon surface 
shown in Figure 11c. We resampled the sphere at the resolution of the texture, which is 700 × 700 pixels. 
Finally, we applied the displacement map depicted in Figure 11d. Our system includes a splat renderer 
similar to [Zwicker et al. 2001]. On a Pentium IV at 2.0 GHz, it renders approximately 500k antialiased 
splats per second at an output resolution of 512 × 512 pixels.  7 CONCLUSIONS AND FUTURE WORK We presented 
a versatile system for efficient 3D appearance and shape editing of point-based models. The key ingredients 
of our editor comprise a flexible and powerful point cloud parameteriza­tion and a dynamic resampling 
scheme based on a continuous reconstruction of the model surface. Although geometry editing is limited 
to normal displacement, we currently support a broad range of editing operators for efficient 3D content 
creation. The achievable effects go well beyond the conventional 2D photo edit­ing functionality and 
new, more sophisticated editing operators can be added very easily. Future work will be devoted to extending 
our system towards more general modeling operations. We will also investigate high quality rendering 
of parameterized point-based surfaces using a ray tracing approach.   Acknowledgements We would like 
to thank Martin Roth for proofreading the paper. References AGRAWALA, M., BEERS, A. C., AND LEVOY, M. 
1995. 3d painting on scanned surfaces. pages 145 150. ISBN 0-89791-736-7. ALEXA, M. 2001. Point set surfaces. 
In IEEE Visualization. ALIAS WAVEFRONT 2001. Maya. http://www.aliaswavefront.com. ARIUS3D 2001. pointstream. 
http://www.pointstream.net. ASHBY, MANTEUFFEL, AND SAYLOR 1990. A taxonomy for conjugate gra­ dient methods. 
J. Numer. Anal., 27:1542 1568. CURLESS, B. AND LEVOY, M. 1996. A volumetric method for building com­ 
plex models from range images. In SIGGRAPH 96, pages 303 312. EYETRONICS 2001. ShapeSnatcher. http://www.eyetronics.com. 
FLOATER, M. S. 1997. Parametrization and smooth approximation of sur­face triangulations. Comp. Aided 
Geom. Design, 14:231 250. FLOATER, M. S. AND REIMERS, M. 2001. Meshless parameterization and surface 
reconstruction. Comp. Aided Geom. Design, 18:77 92. LEVY, B. 2001. Constrained texture mapping for polygonal 
meshes. In SIG-GRAPH 2001, pages 417 424. Los Angeles, CA, August 12-17, 2001. LEVY, B. AND MALLET, J.-L. 
1998. Non-distorted texture mapping for sheared triangulated meshes. In SIGGRAPH 98, pages 343 352. OH, 
B. M., CHEN, M., DORSEY, J., AND DURAND, F. 2001. Image-based modeling and photo editing. In SIGGRAPH 
2001, pages 433 442. O NEILL, B. 1966. Elementary Differential Geometry. Academic Press. PAULY, M. AND 
GROSS, M. 2001. Spectral processing of point-sampled geometry. In SIGGRAPH 01, pages 379 386. PEDERSEN, 
H. K. 1995. Decorating implicit surfaces. In SIGGRAPH 1995, pages 291 300. Los Angeles, CA, August 6-11, 
1995. PERRY, R. N. AND FRISKEN, S. F. 2001. Kizamu: A system for sculpting digital characters. In SIGGRAPH 
2001, pages 47 56. PFISTER, H., ZWICKER, M., VANBAAR, J., AND GROSS, M. 2000. Surfels: Surface elements 
as rendering primitives. In SIGGRAPH 2000, pages 335 342. New Orleans, LA, July 23-28, 2000. RIGHT HEMISPHERE 
2001. DeepPaint3D. http://www.us.deeppaint3d.com. RUSINKIEWICZ, S. AND LEVOY, M. 2000. QSplat: A multiresolution 
point rendering system for large meshes. In SIGGRAPH 2000, pages 343 352. SZELISKI, R. AND TONNESEN, 
D. 1992. Surface modeling with oriented par­ticle systems. In SIGGRAPH 92, pages 185 194. July 1992. 
WELCH, W. AND WITKIN, A. 1994. Free-form shape design using triangu­lated surfaces. In SIGGRAPH 94, pages 
247 256. ZWICKER, M., PFISTER, H., VANBAAR, J., AND GROSS, M. 2001. Surface splatting. In SIGGRAPH 2001, 
pages 371 378.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566585</article_id>
		<sort_key>330</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Level set surface editing operators]]></title>
		<page_from>330</page_from>
		<page_to>338</page_to>
		<doi_number>10.1145/566570.566585</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566585</url>
		<abstract>
			<par><![CDATA[We present a level set framework for implementing editing operators for surfaces. Level set models are deformable implicit surfaces where the deformation of the surface is controlled by a speed function in the level set partial differential equation. In this paper we define a collection of speed functions that produce a set of surface editing operators. The speed functions describe the velocity at each point on the evolving surface in the direction of the surface normal. All of the information needed to deform a surface is encapsulated in the speed function, providing a simple, unified computational framework. The user combines pre-defined building blocks to create the desired speed function. The surface editing operators are quickly computed and may be applied both regionally and globally. The level set framework offers several advantages. 1) By construction, self-intersection cannot occur, which guarantees the generation of physically-realizable, simple, closed surfaces. 2) Level set models easily change topological genus, and 3) are free of the edge connectivity and mesh quality problems associated with mesh models. We present five examples of surface editing operators: blending, smoothing, sharpening, openings/closings and embossing. We demonstrate their effectiveness on several scanned objects and scan-converted models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[deformations]]></kw>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[implicit surfaces]]></kw>
			<kw><![CDATA[shape blending]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics editors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39051212</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39023663</person_id>
				<author_profile_id><![CDATA[81100023224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Breen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P249754</person_id>
				<author_profile_id><![CDATA[81100483298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ross]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Whitaker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14034821</person_id>
				<author_profile_id><![CDATA[81100070192]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>205143</ref_obj_id>
				<ref_obj_pid>205127</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADALSTEINSSON, D., AND SETHIAN, J. 1995. A fast level set method for propagating interfaces. Journal of Computational Physics 118, 269-277.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280947</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AMENTA, N., BERN, M., AND KAMVYSSELIS, M. 1998. A new voronoi-based surface reconstruction algorithm. In Proc. SIGGRAPH '98, 415-421.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218424</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., BERNARDINI, F., AND XU, G. 1995. Automatic reconstruction of surfaces and scalar fields from 3D scans. In Proc. SIGGRAPH '95, 109-118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BARR, A. 1981. Superquadrics and angle-preserving transformations. IEEE Computer Graphics and Applications 1, 1, 11-23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383280</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BIERMANN, H., KRISTJANSSON, D., AND ZORIN, D. 2001. Approximate Boolean operations on free-form solids. In Proc. SIGGRAPH 2001, 185-194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>549676</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J., ET AL., Eds. 1997. Introduction to Implicit Surfaces. Morgan Kaufmann, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>339361</ref_obj_id>
				<ref_obj_pid>339355</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BOUGUET, J.-Y., AND PERONA, P. 1999. 3D photography using shadow in dual space geometry. International Journal of Computer Vision 35, 2 (Nov/Dev), 129-149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614490</ref_obj_id>
				<ref_obj_pid>614282</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BREEN, D., AND WHITAKER, R. 2001. A level set approach for the metamorphosis of solid models. IEEE Trans. on Visualization and Computer Graphics 7, 2, 173-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[BREEN, D., MAUCH, S., AND WHITAKER, R. 2000. 3D scan conversion of CSG models into distance, closest-point and colour volumes. In Volume Graphics, M. Chen, A. Kaufman, and R. Yagel, Eds. Springer, London, 135-158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[COHEN, E., RIESENFELD, R., AND ELBER, G. 2001. Geometric Modeling with Splines. AK Peters, Natick, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., AND LEVOY, M. 1996. A volumetric method for building complex models from range images. In Proc. SIGGRAPH '96, 303-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., AND CANI, M.-P. 1998. Active implicit surface for animation. In Graphics Interface, 143-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218456</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., AND GASCUEL, M. 1995. Animating soft substances with implicit surfaces. In Proc. SIGGRAPH 95 Conference, 287-290.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311576</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., MEYER, M., SCHR&#214;DER, P., AND BARR, A. 1999. Implicit fairing of irregular meshes using diffusion and curvature flow. In Proc. SIGGRAPH '99, 317-324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[DO CARMO, M. 1976. Differential Geometry of Curves and Surfaces. Prentice-Hall, Englewood Cliffs, NJ.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>156635</ref_obj_id>
				<ref_obj_pid>174462</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[EDELSBRUNNER, H., AND M&#220;CKE, E. 1994. Three-dimensional alpha shapes. ACM Trans. on Graphics 13, 1, 43-72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[EVANS, L., AND SPRUCK, J. 1991. Motion of level sets by mean curvature, I. Journal of Differential Geometry 33, 635-681.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND FEDKIW, R. 2001. Practical animation of liquids. In Proc. SIGGRAPH 2001, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[FRISKEN, S., PERRY, R., ROCKWOOD, A., AND JONES, T. 2000. Adaptively sampled distance fields: A general representation of shape for computer graphics. In SIGGRAPH 2000 Proceedings, 249-254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122747</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[GALYEAN, T., AND HUGHES, J. 1991. Sculpting: An interactive volumetric modeling technique. In Proc. SIGGRAPH '91, 267-274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74803</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[HOFFMANN, C. 1989. Geometric and Solid Modeling. Morgan Kaufmann, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280831</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L., CAMPAGNA, S., VORSATZ, J., AND SEIDEL, H.-P. 1998. Interactive multi-resolution modeling on arbitrary meshes. In Proc. SIGGRAPH '98, 105-114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383265</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L. P., BOTSCH, M., SCHWANECKE, U., AND SEIDEL, H.-P. 2001. Feature sensitive surface extraction from volume data. In Proc. SIGGRAPH 2001, 57-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15904</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LAIDLAW, D., TRUMBORE, W., AND HUGHES, J. 1986. Constructive solid geometry for polyhedral objects. In Proc. SIGGRAPH '86, 161-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[LORENSEN, W., AND CLINE, H. 1987. Marching Cubes: A high resolution 3D surface construction algorithm. In Proc. SIGGRAPH '87, 163-169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>200867</ref_obj_id>
				<ref_obj_pid>200862</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[MALLADI, R., SETHIAN, J., AND VEMURI, B. 1995. Shape modeling with front propagation: A level set approach. IEEE Trans. on Pattern Analysis and Machine Intelligence 17, 2, 158-175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319223</ref_obj_id>
				<ref_obj_pid>2318937</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[MARAGOS, P. 1996. Differential morphology and image processing. IEEE Trans. on Image Processing 5, 6 (June), 922-937.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>512581</ref_obj_id>
				<ref_obj_pid>512576</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[OSHER, S., AND FEDKIW, R. 2001. Level set methods: An overview and some recent results. Journal of Computational Physics 169, 475-502.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[OSHER, S., AND SETHIAN, J. 1988. Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations. Journal of Computational Physics 79, 12-49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>338913</ref_obj_id>
				<ref_obj_pid>338852</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[PENG, D., MERRIMAN, B., OSHER, S., ZHAO, H.-K., AND KANG, M. 1999. A PDE-based fast local level set method. Journal of Computational Physics 155, 410-438.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383264</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[PERRY, R., AND FRISKEN, S. 2001. Kizamu: A system for sculpting digital characters. In Proc. SIGGRAPH 2001, 47-56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[REQUICHA, A., AND VOELCKER, H. 1985. Boolean operations in solid modeling: Boundary evaluation and merging algorithms. Proceedings of the IEEE 73, 1, 30-44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>142312</ref_obj_id>
				<ref_obj_pid>142273</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[RUDIN, L., OSHER, S., AND FATEMI, C. 1992. Nonlinear total variation based noise removal algorithms. Physica D 60, 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[SAPIRO, G., KIMMEL, R., SHAKED, D., KIMIA, B., AND BRUCKSTEIN, A. 1993. Implementing continuous-scale morphology via curve evolution. Pattern Recognition 26, 9, 1363-1372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>365759</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[SAPIRO, G. 2001. Geometric Partial Differential Equations and Image Analysis. Cambridge University Press, Cambridge, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1098652</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[SERRA, J. 1982. Image Analysis and Mathematical Morphology. Academic Press, London.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[SETHIAN, J. 1996. A fast marching level set method for monotonically advancing fronts. In Proceedings of the National Academy of Science, vol. 93, 1591-1595.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325510</ref_obj_id>
				<ref_obj_pid>325509</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[SETHIAN, J. 1999. Level Set Methods and Fast Marching Methods, second ed. Cambridge University Press, Cambridge, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G. 1995. A signal processing approach to fair surface design. In Proc. SIGGRAPH '95, 351-358.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[TSITSIKLIS, J. 1995. Efficient algorithms for globally optimal trajectories. IEEE Trans. on Automatic Control 40, 9, 1528-1538.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617911</ref_obj_id>
				<ref_obj_pid>616032</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[WANG, S., AND KAUFMAN, A. 1994. Volume-sampled 3D modeling. IEEE Computer Graphics and Applications 14, 5 (September), 26-32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199430</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[WANG, S., AND KAUFMAN, A. 1995. Volume sculpting. In Proc. Symposium on Interactive 3D Graphics, ACM SIGGRAPH, 151-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[WELCH, W., AND WITKIN, A. 1994. Free-form shape design using triangulated surfaces. In Proc. SIGGRAPH '94, 247-256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[WHITAKER, R., AND XUE, X. 2001. Variable-conductance, level-set curvature for image denoising. In Proc. IEEE International Conference on Image Processing, 142-145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386459</ref_obj_id>
				<ref_obj_pid>2386438</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[WHITAKER, R., BREEN, D., MUSETH, K., AND SONI, N. 2001. Segmentation of biological datasets using a level-set framework. In Volume Graphics 2001, M. Chen and A. Kaufman, Eds. Springer, Vienna, 249-263.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>299682</ref_obj_id>
				<ref_obj_pid>299660</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[WHITAKER, R. 1998. A level-set approach to 3D reconstruction from range data. International Journal of Computer Vision 29, 3, 203-231.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[WYVILL, B., GALIN, E., AND GUY, A. 1999. Extending the CSG tree. warping, blending and Boolean operations in an implicit surface modeling system. Computer Graphics Forum 18, 2 (June), 149-158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835639</ref_obj_id>
				<ref_obj_pid>832286</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[ZHAO, H.-K., OSHER, S., AND FEDKIW, R. 2001. Fast surface reconstruction using the level set method. In Proc. 1st IEEE Workshop on Variational and Level Set Methods, 194-202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Level Set Surface Editing Operators .... Ken Museth David E. Breen Ross T. Whitaker Alan H. Barr *Computer 
Science Department .School of Computing California Institute of Technology University of Utah Figure 
1: Surfaces edited with level set operators. Left: A damaged Greek bust model is repaired with a new 
nose, chin and sharpened hair. Right: A new model is constructed from models of a grif.n and dragon (small 
.gures), producing a two-headed, winged dragon. Abstract We present a level set framework for implementing 
editing oper­ators for surfaces. Level set models are deformable implicit sur­faces where the deformation 
of the surface is controlled by a speed function in the level set partial differential equation. In this 
paper we de.ne a collection of speed functions that produce a set of sur­face editing operators. The 
speed functions describe the velocity at each point on the evolving surface in the direction of the sur­face 
normal. All of the information needed to deform a surface is encapsulated in the speed function, providing 
a simple, uni.ed computational framework. The user combines pre-de.ned building blocks to create the 
desired speed function. The surface editing op­erators are quickly computed and may be applied both regionally 
and globally. The level set framework offers several advantages. 1) By construction, self-intersection 
cannot occur, which guarantees the generation of physically-realizable, simple, closed surfaces. 2) Level 
set models easily change topological genus, and 3) are free of the edge connectivity and mesh quality 
problems associated with mesh models. We present .ve examples of surface editing opera­tors: blending, 
smoothing, sharpening, openings/closings and em­bossing. We demonstrate their effectiveness on several 
scanned ob­jects and scan-converted models. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry 
and Object Modeling Surface and object representations; I.3.4 [Computer Graphics]: Graphics Utilities 
Graphics Editors; Keywords: Deformations, geometric modeling, implicit surfaces, shape blending. Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 1 Introduction The creation of complex models for such applications as movie special effects, graphic 
arts, and computer-aided design can be a time-consuming, tedious, and error-prone process. One of the 
so­lutions to the model creation problem is 3D photography [Bouguet and Perona 1999], i.e. scanning a 
3D object directly into a digital representation. However, the scanned model is rarely in a .nal de­sired 
form. The scanning process is imperfect and introduces errors and artifacts, or the object itself may 
be .awed. 3D scans can be converted to polygonal and parametric surface meshes [Edelsbrunner and M¨ucke 
1994; Bajaj et al. 1995; Amenta et al. 1998]. Many algorithms and systems for editing these polyg­onal 
and parametric surfaces have been developed [Cohen et al. 2001], but surface mesh editing has its limitations 
and must ad­dress several dif.cult issues. For example, it is dif.cult to guar­antee that a mesh model 
will not self-intersect when performing a local editing operation based on the movement of vertices or 
con­trol points, producing non-physical, invalid results. See Figure 2. If self-intersection occurs, 
it must be .xed as a post-process. Also, when merging two mesh models the process of clipping individual 
polygons and patches may produce errors when the elements are small and/or thin, or if the elements are 
almost parallel. In addition while it is not impossible to change the genus of a surface mesh model [Biermann 
et al. 2001], it is certainly dif.cult and requires signi.cant effort to maintain the consistency/validity 
of the under­lying vertex/edge connectivity structure. 1.1 New Surface Editing Operators In order to 
overcome these dif.culties we present a level set ap­proach to implementing operators for locally and 
globally editing closed surfaces. Level set models are deformable implicit surfaces that have a volumetric 
representation [Osher and Sethian 1988]. They are de.ned as an iso-surface, i.e. a level set, of some 
im­plicit function .. The surface is deformed by solving a partial dif­ferential equation (PDE) on a 
regular sampling of ., i.e. a vol­ume dataset. To date level set methods have not been developed for 
adaptive grids, a limitation of current implementations, but not of the mathematics. It should be emphasized 
that level set methods do not manipulate an explicit closed form representation of ., but No offset LS 
offset Mesh offset Figure 2: (left) A cross-section of the teapot model near the spout. (middle) No 
self-intersection occurs, by construction, when per­forming a level set (LS) offset, i.e. dilation, of 
the surface. (right) Self-intersections may occur when offsetting a mesh model. only a sampling of it. 
Level set methods provide the techniques needed to change the voxel values of the volume in a way that 
de­forms the embedded iso-surface to meet a user-de.ned goal. The user controls the deformation of the 
level set surface by de.ning a speed function .............), the speed of the level set at point .in 
the direction of the normal to the surface at .. Therefore all the information needed to deform a level 
set model may be encapsu­lated in a single speed function ....., providing a simple, uni.ed computational 
framework. We have developed a number of surface editing operators within the level set framework by 
de.ning a collection of new level set speed functions. The cut-and-paste operator (Section 5.1) gives 
the user the ability to copy, remove and merge level set models (using volumetric CSG operations) and 
automatically blends the intersec­tion regions (See Section 5.2). Our smoothing operator allows a user 
to de.ne a region of interest and smooths the enclosed surface to a user-de.ned curvature value. See 
Section 5.3. We have also de­veloped a point-attraction operator. See Section 5.4. Here, a region­ally 
constrained portion of a level set surface is attracted to a single point. By de.ning line segments, 
curves, polygons, patches and 3D objects as densely sampled point sets, the single point attraction operator 
may be combined to produce a more general surface em­bossing operator. As noted by others, the opening 
and closing mor­phological operators may be implemented in a level set framework [Sapiro et al. 1993; 
Maragos 1996]. We have also found them useful for performing global blending (closing) and smoothing 
(opening) on level set models. Since all of the operators accept and produce the same volumetric representation 
of closed surfaces, the operators may be applied repeatedly to produce a series of surface editing op­erations. 
See Figure 11. 1.2 Bene.ts and Issues Performing surface editing operations within a level set framework 
provides several advantages and bene.ts. Many types of surfaces may be imported into the framework as 
a distance volume, a vol­ume dataset that stores the signed shortest distance to the surface at each 
voxel. This allows a number of different types of surfaces to be modi.ed with a single, powerful procedure. 
By construc­tion, the framework always produces non-self-intersecting surfaces that represent physically-realizable 
objects, an important issue in computer-aided design. Level set models easily change topologi­cal genus, 
and are free of the edge connectivity and mesh quality problems associated with deforming and modifying 
mesh models. Additionally, some reconstruction algorithms produce volumetric models [Curless and Levoy 
1996; Whitaker 1998; Zhao et al. 2001] and volumetric scanning systems are increasingly being employed 
in a number of diverse .elds. Therefore volumetric models are be­coming more prevalent and there is a 
need to develop powerful edit­ing operators that act on these types of models directly. There are implementation 
issues to be addressed when using level set models. Given their volumetric representation, one may be 
concerned about the amount of computation time and memory needed to process level set models. Techniques 
have been devel­oped to limit level set computations to only a narrow band around the level set of interest 
[Adalsteinsson and Sethian 1995; Whitaker 1998; Peng et al. 1999] making the computational complexity 
pro­portional to the surface area of the model. We have also developed computational techniques that 
allow us to perform the narrow band calculations only in a portion of the volume where the level set 
is actually moving. Additionally, fast marching methods have been developed to rapidly evaluate the level 
set equation under certain circumstances [Tsitsiklis 1995; Sethian 1996]. Memory usage has not been an 
issue when generating the results in this paper. The memory needed for our results (512 MB) is available 
on standard workstations and PCs. We have implemented our operators in an interactive environment that 
allows us to easily edit a number of complex surfaces. Additionally, concerns have been raised that volume-based 
models cannot represent .ne or sharp features. Re­cent advances [Frisken et al. 2000; Kobbelt et al. 
2001] have shown that is is possible to model these kinds of structures with volume datasets, without 
excessively sampling the whole volume. These advances will also be available for our operators once adaptive 
level set methods, an active research area, are developed. 1.3 Contributions The major contributions 
of our work are the following. . The introduction of a uni.ed approach to surface editing within a level 
set framework. Editing operators de.ned by speed functions.  Results produced by solving a PDE.  . 
The de.nition of level set speed functions that implement blending, smoothing and embossing surface editing 
operators. Blending is automatic and is constrained to only occur within a user-speci.ed distance to 
an arbitrarily com­plex intersection curve.  Smoothing and embossing are constrained to occur within 
a user-speci.ed region.  The user speci.es the local geometric properties of the resulting surface modi.cations. 
 The user speci.es if material should be added and/or removed during editing operations.  The new techniques 
used to localize level set calculations. . In Appendix B we present a numerically-stable curvature measure 
for level set surfaces.  2 Previous Work Three areas of research are closely related to our level set 
sur­face editing work; volumetric sculpting, mesh-based surface edit­ing/fairing and implicit modeling. 
Volumetric sculpting provides methods for directly manipulating the voxels of a volumetric model. CSG 
Boolean operations [Hoffmann 1989; Wang and Kaufman 1994] are commonly found in volume sculpting systems, 
providing a straightforward way to create complex solid objects by combining simpler primitives. One 
of the .rst volume sculpting systems is pre­sented in [Galyean and Hughes 1991]. [Wang and Kaufman 1995] 
improved on this work by introducing tools for carving and saw­ing. More recently [Perry and Frisken 
2001] implemented a volu­metric sculpting system based on Adaptive Distance Fields (ADF) [Frisken et 
al. 2000], allowing for volumetric models with adaptive resolution. Performing CSG operations on mesh 
models is a long-standing area of research [Requicha and Voelcker 1985; Laidlaw et al. 1986]. Figure 
3: Our level set surface editing operators (red) .t into a larger editing framework. The pipeline consists 
of: input models (blue), pre-processing (yellow), CSG operations (orange), local LS operators (red), 
global LS operators (purple) and rendering (green). Recently CSG operations were developed for multi-resolution 
sub­division surfaces by [Biermann et al. 2001], but this work did not address the problem of blending 
or smoothing the sharp features of­ten produced by the operations. However, the smoothing of meshes has 
been studied on several occasions [Welch and Witkin 1994; Taubin 1995; Kobbelt et al. 1998]. [Desbrun 
et al. 1999] have de­veloped a method for fairing irregular meshes using diffusion and curvature .ow, 
demonstrating that mean-curvature based .ow pro­duces the best results for smoothing. There exists a 
large body of surface editing work based on im­plicit models [Bloomenthal et al. 1997]. This approach 
uses im­plicit surface representations of analytic primitives or skeletal off­sets. The implicit modeling 
work most closely related to ours is found in [Wyvill et al. 1999]. They describe techniques for per­forming 
blending, warping and boolean operations on skeletal im­plicit surfaces. [Desbrun and Gascuel 1995] address 
the converse problem of preventing unwanted blending between implicit primi­tives, as well as maintaining 
a constant volume during deformation. Level set methods have been successfully applied in computer graphics, 
computer vision and visualization [Sethian 1999; Sapiro 2001], for example medical image segmentation 
[Malladi et al. 1995; Whitaker et al. 2001], shape morphing [Desbrun and Cani 1998; Breen and Whitaker 
2001], 3D reconstruction [Whitaker 1998; Zhao et al. 2001], and recently for the animation of liquids 
[Foster and Fedkiw 2001]. Our work stands apart from previous work in several ways. We have not developed 
volumetric modeling tools. Our editing opera­tors act on surfaces that happen to have an underlying volumetric 
representation, but are based on the mathematics of deforming im­plicit surfaces. Our editing operators 
share several of the capabil­ities of mesh-based tools, but are not hampered by the dif.culties of maintaining 
vertex/edge information. Since level set models are not tied to any speci.c implicit basis functions, 
they easily represent complex models to within the resolution of the sampling. Our work is the .rst to 
utilize level set methods to perform user-controlled editing of complex geometric models. 3 Overview 
of the Editing Pipeline The level set surface editing operators should be viewed as com­ponents of a 
larger modeling framework. The pipeline for this framework is presented in Figure 3. The red components 
contain the level set speed functions that we have developed for localized surface editing. The remaining 
components contain the data and operations needed for level set modeling, input models (blue), pre­processing 
(yellow), CSG operations (orange), global LS operators (purple) and rendering (green). The pipeline provides 
the context for the details of our speed functions. 3.1 Input and Output Models We are able to import 
a wide variety of closed geometric models into the level set environment. We represent a level set model 
as an iso-surface embedded in a distance volume. Frequently we only store distance information in a narrow 
band of voxels surround­ing the level set surface. As illustrated in Figure 3 we have devel­oped and 
collected a suite of scan conversion methods for convert­ing polygonal meshes, CSG models [Breen et al. 
2000], implicit primitives, and NURBS surfaces into distance volumes. Addition­ally many types of scanning 
processes produce volumetric models directly, e.g. MRI, CT and laser range scan reconstruction. These 
models may be brought into our level set environment as is or with minimal pre-processing. We frequently 
segment these models with another level set technique [Whitaker et al. 2001], and then apply Sethian 
s Fast Marching Method [Sethian 1996] to convert the re­sults into distance volumes. The models utilized 
in this paper and their original form are listed in Table 1. Table 1: Native representations of the input 
models and dimensions of the corresponding scan converted distance volumes. Model Representation Dimensions 
Dragon Grif.n Greek bust Human head Utah teapot Supertoroid volumetric reconstruction volumetric reconstruction 
polygonal reconstruction polygonal reconstruction NURBS surface implicit primitive ..................... 
............... In the .nal stage of the pipeline we can either volume render the surface directly or 
render a polygonal mesh extracted from the volume. While there are numerous techniques available for 
both approaches, we found extracting and rendering Marching Cubes meshes [Lorensen and Cline 1987] to 
be satisfactory.  4 Level Set Surface Modeling The Level Set Method, .rst presented in [Osher and Sethian 
1988], is a mathematical tool for modeling surface deformations. A de­formable (i.e. time-dependent) 
surface is implicitly represented as an iso-surface of a time-varying scalar function, ............ A 
detailed description of level set models is presented in Appendix A. 4.1 LS Speed Function Building Blocks 
Given the de.nition..................................(1) the fundamental level set equation, Eq. (12), 
can be rewritten as .................................(2) where ......and .............are the velocity 
and normal vectors at .on the surface. We assume a positive-inside/negative­outside sign convention for 
..........., i.e. .points outwards. Eq. (1) introduces the speed function ., which is a user-de.ned scalar 
function that can depend on any number of variables including .,., .and its derivatives evaluated at 
., as well as a variety of ex­ternal data inputs. .....is a signed scalar function that de.nes the motion 
(i.e. speed) of the level set surface in the direction of the local normal .at .. The speed function 
is usually based on a set of geometric mea­sures of the implicit level set surface and data inputs. The 
chal­lenge when working with level set methods is determining how to combine the building blocks to produce 
a local motion that creates a desired global or regional behavior of the surface. The general the ROI 
functions, cf. Eq. (4). tance to a surface, cf. Eq. (6b). , of dis­tance to point set, cf. Eq. (6a). 
ric measure, , cf. Eq. (7). Figure 4: Graph of region-of-in.uence (ROI) functions used to de­.ne the 
speed functions for our local level set operations, cf. Eq. (3). structure for the speed functions used 
in our surface editing opera­tors is ...........................................(3) where .....is a distance-based 
cut-off function that depends on a distance measure .to a geometric structure .. .......is a cut-off 
function that controls the contribution of .......to the speed func­tion. .......is a function that depends 
on geometric measures .de­rived from the level set surface, e.g. curvature. Thus, .......acts as a region-of-in.uence 
function that regionally constrains the LS calculation. .......is a .lter of the geometric measure and 
.......pro­vides the geometric contribution of the level set surface. In general .is de.ned as zero, 
.rst, or second order measures of the LS sur­face. 4.2 Regionally Constraining LS Deformations Most 
of our surface operators may be applied locally in a small user-de.ned region on the edited surface. 
In order to regionally re­strict the deformation during the level set computation, a technique is needed 
for driving the value of .....to zero outside of the region. This is accomplished in three steps. The 
.rst step involves de.ning the region of in.uence (ROI), i.e. the region where .....should be non-zero. 
This is done by either the user interactively placing a 3D object around the region, or by automatically 
calculating a region from properties of the surface. Both cases involve de.ning a ge­ometric structure 
that we refer to as a region-of-in.uence (ROI) primitive . The nature of these primitives will vary for 
the differ­ent LS operations and will be explicitly de.ned in Section 5. The second step consists of 
calculating a distance measure to the ROI primitive. The .nal step involves de.ning a function that smoothly 
approaches zero at the boundary of the ROI. We de.ne a region-of-in.uence function .......in Eq. (3), 
where .is a distance measure from a point on the level set surface to the ROI primitive .. The functional 
behavior of .......clearly depends on the speci.c ROI primitive, ., but we found the following piece­wise 
polynomial function to be useful as a common speed function building block: . .. for............for ... 
.........(4).. ................for .....for ....... ......and its derivatives are continuous and relatively 
inexpensive to compute. See Figure 4(a). Other continuous equations with the same basic shape would also 
be satisfactory. We then de.ne ..........................................(5) ................... where 
.......and .......are user-de.ned parameters that de.ne the limits and sharpness of the cut-off. Let 
us .nally de.ne the follow­ing region-of-in.uence functions ...............................(6a) ....... 
.................... (6b) for a point set, ., and a closed surface, .. In Eq. (6a) .denotes the distance 
from a point on the level set surface to the closest point in the point set .. In Eq. (6b) .denotes a 
signed distance measure from a point on the level set surface to the implicit surface .. The signed distance 
measure does not nec­essarily have to be Euclidean distance -just a monotonic distance measure following 
the positive-inside/negative-outside convention. Note that ........is one when the shortest distance, 
., to the point set is smaller than ......., and decays smoothly to zero as .increases to ......, after 
which it is zero. ........, on the other hand, is zero everywhere outside, as well as on, the surface 
.(....), but one inside when the distance measure .is larger than ........ An additional bene.t of the 
region-of-in.uence functions is that they de.ne the portion of the volume where the surface cannot move. 
We use this information to determine what voxels should be updated during the level set deformation, 
signi.cantly lowering the amount of computation needed when performing editing opera­tions. This technique 
allows our operators to be rapidly computed when modifying large models. 4.3 Limiting Geometric Property 
Values We calculate a number of geometric properties from the level set surface. The zero order geometric 
property that we utilize is short­est distance from the level set surface to some ROI primitive. The 
.rst order property is the surface normal, .............. Second order information includes a variety 
of curvature measures of the LS surface. In Appendix B we outline a new numerical approach to deriving 
the mean, Gaussian and principle curvatures of a level set surface. Our scheme has numerical advantages 
relative to tra­ditional central .nite difference schemes for computing the second order derivatives. 
We found mean curvature to be the most useful second order measure [Evans and Spruck 1991] for our application. 
Another desirable feature of our operators is that they allow the user to control the geometric properties 
of surface in the region be­ing edited. This feature is implemented with another cut-off func­tion, ....., 
within the level set speed function. .....allows the user to slow and then stop the level set deformation 
as a particular sur­face property approaches a user-speci.ed value. We reuse the cut­off function, Eq. 
(5), de.ned in the previous section, as a building block for ..... ........ We de.ne... ........................for 
........ (7)...................................for ... .. where ............................... The 
four parameters ....., .........., ................., and ......de.ne respectively the upper and lower 
bounds of .......... the cut-off function, see Figure 4(d).  4.4 Constraining the Direction of LS Motions 
Another important feature of the level set framework is its ability to control the direction of the level 
set deformation. We are able Figure 5: Left: Positioning the (red) wing model on the dragon model. Middle: 
The models are pasted together (CSG union operation), producing sharp, undesirable creases, a portion 
of which is expanded in the box. Right: Same region after automatic blending based on mean curvature. 
The blending is constrained to only move outwards. The models are rendered with .at-shading to highlight 
the details of the  surface structure. to restrict the motion of the surface to only add or remove material 
during the level set editing operations. At any point the level set surface can only move in the direction 
of the local surface normal. Hence, we can simply rede.ne the speed function as min.......to remove material 
(inward motion only) and max.......to add mate­rial (outward motion only). In the case of curvature driven 
speed functions this produces min/max .ows [Sethian 1999]. Of course no restriction on the direction 
of the motion need be imposed.  5 De.nition of Surface Editing Operators Given the building blocks 
described in the previous section, the level set surface editing operators outlined in Figure 3 may be 
de­.ned. We begin by de.ning the well-known CSG operations that are essential to most editing systems. 
We then de.ne the new level set speed functions that implement our surface editing operators by combining 
the geometric measures with the region-of-in.uence and cut-off functions. 5.1 CSG Operations Since level 
set models are volumetric, the constructive solid geom­etry (CSG) [Hoffmann 1989] operations of union, 
difference and intersection may be applied to them. This provides a straightfor­ward approach to implementing 
copy, cut and paste operations on level set surfaces. In our level set framework, with a positive­inside/negative-outside 
sign convention for the distance volumes, these are implemented as min/max operations [Wang and Kauf­man 
1994] on the voxel values as summarized in Table 2. Any two closed surfaces represented as signed distance 
volumes can be used as either the main edited model or the cut/copy primitive. In our editing system 
the user is able to arbitrarily scale, translate and rotate the models before a CSG operation is performed. 
Table 2: Implementation of CSG operations on two level set mod­els, .and ., represented by distance volumes 
...and ...with positive inside and negative outside values. Action CSG Operation Implementation Copy 
Intersection, .....Min Paste Union, .....Max............. Cut Difference, .....Min........... 5.2 Automatic 
Localized LS Blending The surface models produced by the CSG paste operation typically contain sharp 
and sometimes jagged creases at the intersection of the two surfaces. We can dramatically improve this 
region of the surface by applying an automatic localized blending. The method is automatic because it 
only requires the user to specify a few pa­rameter values. It is localized because the blending operator 
is only applied near the surface intersection region. One possible solution to localizing the blending 
is to perform the deformation in regions near both of the input surfaces. However, this naive approach 
would result in blending the two surfaces in all regions of space where the surfaces come within a user-speci.ed 
distance of each other, creating unwanted blends. A better solution, and the one we use, involves de.ning 
the region of in.uence based on the distance to the intersection curve shared by both input surfaces. 
A sampled representation of this curve is the set of voxels that contains a zero distance value (within 
some sub-voxel value .) to both surfaces. We have found this approximate representation of the intersection 
curve as a point set to be suf.cient for de.ning a shortest distance.for the region-of-in.uence function, 
......., cf. Eq. (3). Repre­senting the intersection curve by a point set allows the curve to take an 
arbitrary form -it can even be composed of multiple curve seg­ments without introducing any complications 
to the computational scheme. The blending operator moves the surface in a direction that min­imizes a 
curvature measure, ., on the level set surface. This is obtained by making the speed function, ., Eq. 
(3), proportional to ., leading to the following blending speed function: ..........................................(8) 
where is a user-de.ned positive scalar that controls the rate of .. convergence of the LS calculation, 
.......is de.ned in Eq. (6a) where .is the shortest distance from the level set surface to the intersection 
curve point set, and .......is given by Eq. (7) where .is one of the curvatures de.ne in Appendix B. 
Through the functions..and .the user has full control over the region of in.uence of the blending (......and 
......) and the upper and lower curvature................. values of the blend (........, ........and 
........., ...). Furthermore we can control if the blend adds or removes material, or both as described 
in Section 4.4. Automatic blending is demonstrated in Figure 5. A wing model is positioned relative to 
a dragon model. The two models are pasted together and automatic mean curvature-based blending is applied 
to smooth the creased intersection region. 5.3 Localized LS Smoothing/Sharpening The smoothing operator 
smooths the level set surface in a user­speci.ed region. This is accomplished by enclosing the region 
of interest by a geometric primitive. The region-of-in.uence prim­Figure 6: Regionally constrained smoothing. 
Left: Laser scan re­construction with unwanted, pointed artifacts in the eye. Middle: De.ning the region 
to be smoothed with a (red) superellipsoid. Right: Smoothing the surface within the superellipsoid. The 
sur­face is constrained to only move inwards. itive can be any closed surface for which we have signed 
in­side/outside information, e.g. a level set surface or an implicit prim­itive. We use superellipsoids 
[Barr 1981] as a convenient ROI prim­itive, a .exible implicit primitive de.ned by two shape parameters. 
The surface is locally smoothed by applying motions in a direction that reduces the local curvature. 
This is accomplished by moving the level set surface in the direction of the local normal with a speed 
that is proportional to the curvature. Therefore the speed function for the smoothing operator is .............................................(9) 
Here .denotes the signed value of the monotonic inside/outside function of the ROI primitive .evaluated 
at .. As before, .......en­sures that the speed function smoothly goes to zero as .approaches the boundary 
of the ROI primitive. .......limits the value of the surface s curvature within the ROI primitive. Figure 
6 demonstrates our smoothing operator applied to a laser scan reconstruction. Unwanted artifacts are 
removed from an eye by .rst placing a red superellipsoid around the region of interest. A smoothing operator 
constrained to only remove material is ap­plied and the spiky artifacts are removed. Figure 7 demonstrates 
our smoothing operator applied to a preliminary 3D scan conver­sion of the Utah teapot. Unwanted artifacts 
are removed from the region where the spout meets the body of the teapot by .rst placing a superellipsoid 
around the region of interest. A smoothing oper­ator constrained to only add material is applied and 
the crevices are removed. In our .nal, arti.cial smoothing example in Figure 8 a complex structure is 
completely smoothed away. This example illustrates that changes of topological genus and number of discon­nected 
components are easily handled within a level set framework during smoothing. Figure 8: Changes in topological 
genus and the number of discon­nected components are easily handled within a level set framework during 
smoothing. The superellipsoid de.nes the portion of the surface to be smoothed. The surface is constrained 
to move only inwards. Figure 9: Left: Three types of single point attractions/repulsions using different 
ROI primitives and values. Right: Utah teapot embossed with 7862 points sampling the SIGGRAPH 2002 logo. 
We obtain a sharpening operator by simply inverting the sign of .in Eq. (9) and applying an upper cut-off 
to the curvature in .....in order to maintain numerical stability. The sharpening operator has been applied 
to the hair of the Greek bust in Figure 1. 5.4 Point Set Attraction and Embossing We have developed 
an operator that attracts and repels the surface towards and away from a point set. These point sets 
can be sam­ples of lines, curves, planes, patches and other geometric shapes, e.g. text. By placing the 
point sets near the surface, we are able to emboss the surface with the shape of the point set. Similar 
to the smoothing operator, the user encloses the region to be embossed with a ROI primitive e.g. a superellipsoid. 
The region-of-interest function for this operator is ........, Eq. (6b). First, assume that all of the 
attraction points are located outside the LS surface. ..denotes the closest attraction point to ., a 
point on the LS surface. Our operator only allows the LS surface to move towards ..if the unit vector, 
........................, is pointing in the same direction as the local surface normal .. Hence, the 
speed function should only be non-zero when ............... Since the sign of ......is reversed if ..is 
instead located inside the LS sur­face we simply require ..sign......................to be positive for 
any closest attraction point .... This amounts to having only posi­tive cut-off values for ........ Finally 
we let ...............since this will guarantee that the LS surface will stop once it reaches ... The 
following speed function implements the point set attraction operator: ......................................sign.................................... 
(10) where .is a signed distance measure to a ROI primitive evaluated at .on the LS surface, and ..is 
the closest point in the set to .. The shape of the primitive and the values of the four positive parameters 
in Eq. (7) de.ne the footprint and sharpness of the embossing. See Figure 9, left. Point repulsion is 
obtained by making negative. . Note that Eq. (10) is just one example of many possible point set attraction 
speed functions. In Figure 9, right, the Utah teapot is embossed with 7862 points that have been acquired 
by scanning an image of the SIGGRAPH 2002 logo and warping the points to .t the shape of the teapot. 
 5.5 Global Morphological Operators The new level set operators presented above were designed to lo­cally 
deform a level set surface. However, if the user wishes to perform a global smoothing of a level set 
surface, it is preferable to use an operator other than .......... For a global smoothing the level set 
propagation is computed on the whole volume, which can be slow for large volumes. However, in this case 
morphological opening and closing operators [Serra 1982] offer faster alternatives Figure 7: (left) Scan 
conversion errors near the teapot spout. (middle) Placing a (red) superellipsoid around the errors. (right) 
The errors are smoothed away in 15 seconds. The surface is constrained to only move outwards. Operation 
Objects sub-volume Time Paste Blend Smooth Opening Emboss wing on dragon wing on dragon teapot spout 
human head single point ................................................................ 33 sec. 98 sec. 
15 sec. 22 sec. 1.5 sec.  to global smoothing of level set surfaces. While we are not the .rst to explore 
morphological operators within a level set framework [Sapiro et al. 1993; Maragos 1996], we have implemented 
them and .nd them useful. Morphological openings and closings consist of two fundamental operators, dilations 
...and erosions .... Dilation creates an offset surface a distance .outwards from the original surface, 
and erosion creates an offset surface a distance .inwards from the original surface. The morphological 
opening operator ... is an erosion followed by a dilation, i.e. ........., which removes small pieces 
or thin appendages. A closing is de.ned as........, and closes small gaps or holes within objects. Mor­phological 
operators may be implemented by solving a special form ......of the level set equation, the Eikonal equation, 
........, up to a certain time ., utilizing Sethian s Fast Marching Method [Sethian 1996]. The value 
of .controls the offset distance from the original surface, ......... Figure 10 contains a model from 
a laser scan reconstruction that has been smoothed with an opening operator with .equal to 3. model is 
loaded, and the wing is positioned, pasted and blended onto it. A mirror copy of the wing model is created. 
It is also posi­tioned, pasted and blended onto the other side of the double-headed dragon. We then added 
a loop onto the dragon s back as if designing a bracelet charm. This is accomplished by positioning, 
pasting, and blending a scan-converted supertoroid, producing the .nal model seen in the bottom right. 
The Greek bust model was repaired by copying the nose from the human head model of Figure 10, and pasting 
and blending the copied model onto the broken nose. A piece from the right side of the bust was copied, 
mirrored, pasted and blended onto the left side of her face. Local smoothing operators were applied to 
various por­tions of her cheeks to clean minor cracks. Finally, the sharpening operator was applied within 
a user-de.ned region around her hair. Table 3: Typical operator execution times on a R10K 250MHz MIPS 
processor. .....................  5.6 Editing Session Details Figure 11 contains a series of screen 
shots taken of our level set modeling program while constructing the two-headed winged dragon. The .rst 
shows the original dragon model loaded into the system. A cylindrical primitive is placed around its 
head and it is cut off. The model of the head is duplicated and the two heads are positioned relative 
to each other. Once the user is satis.ed with their orientation, they are pasted together and an automatic 
blending is performed at the intersection seam. The combined double head model is positioned over the 
cropped neck of the dragon body. The double head is pasted and blended onto the body. The grif.n model 
is loaded into the LS modeling system. A primitive is placed around one of its wings. The portion of 
the model within the primitive is copied, being stored in a buffer. Several cutting operations are used 
to trim the wing model (not shown). The double-headed dragon Table 4: Parameters used in examples. ............and 
............are only used during sharpening. Their values are 0.8 and 0.9. No upper limit is placed on 
.in the other examples. ...................... ............ Example Wing Blending Eye Smoothing Spout 
Smoothing Hair Sharpening Teapot Embossing 7 0.9 0.9 0.9 0.9 9 1 1 1 1 0.04 0.04 0.1 0.01 0.8 0.06 0.07 
0.13 0.013 0.9  6 Conclusion and Future Work We have presented an approach to implementing surface 
editing operators within a level set framework. By developing a new set of level set speed functions 
automatic blending, localized smooth­ing and embossing may be performed on level set models. Addi­tionally 
we have implemented morphological and volumetric CSG operators to .ll out our modeling environment. All 
of the infor­mation needed to deform a level set surface is encapsulated in the speed function, providing 
a simple, uni.ed computational frame­work. The level set framework offers several advantages. By con­struction, 
self-intersection cannot occur, which guarantees the gen­eration of physically-realizable, simple, closed 
surfaces. Addition­ally, level set models easily change topological genus, and are free of the edge connectivity 
and mesh quality problems associated with mesh models.  Several issues still must be addressed to improve 
our work. Cur­rently level set implementations are based on uniform samplings of space, a fact that effectively 
limits the resolution of the objects that can be modeled. The development of adaptive level set methods 
would allow our operators to be applied to adaptive distance .elds. It is possible to shorten the time 
needed to edit level set surfaces. Incrementally updating the mesh used to view the edited surface, utilizing 
direct volume rendering hardware, parallelizing the level set computations, and exploring multiresolution 
volumetric repre­sentations will lead to editing operations that require only a fraction of a second, 
instead of tens of seconds. We have presented .ve example level set surface editing oper­ators. Given 
the generality and .exibility of our framework many more can be developed. We intend to explore operators 
that uti­lize Gaussian and principal curvature, extend embossing to work directly with lines, curves 
and solid objects, and ones that may be utilized for general surface manipulations, such as dragging, 
warp­ing, and sweeping.  7 Acknowledgements We would like to thank Mathieu Desbrun for his helpful suggestions, 
and Katrine Museth and Cici Koenig for helping with the .gures. The Greek bust and human head models 
were provided by Cyberware Inc. The dragon and grif.n models were provided by the Stanford Computer Graphics 
Labo­ratory. The teapot model was provided by the University of Utah s Geomet­ric Design and Computation 
Group. This work was .nancially supported by National Science Foundation grants ASC-89-20219, ACI-9982273 
and ACI-0083287. References ADALSTEINSSON, D., AND SETHIAN, J. 1995. A fast level set method for propagat­ing 
interfaces. Journal of Computational Physics 118, 269 277. AMENTA, N., BERN, M., AND KAMVYSSELIS, M. 
1998. A new voronoi-based surface reconstruction algorithm. In Proc. SIGGRAPH 98, 415 421. BAJAJ, C.,BERNARDINI, 
F., AND XU,G.1995.Automaticreconstructionofsurfaces and scalar .elds from 3D scans. In Proc. SIGGRAPH 
95, 109 118. BARR, A. 1981. Superquadrics and angle-preserving transformations. IEEE Computer Graphics 
and Applications 1, 1, 11 23. BIERMANN, H., KRISTJANSSON, D., AND ZORIN, D. 2001. Approximate Boolean 
operations on free-form solids. In Proc. SIGGRAPH 2001, 185 194. BLOOMENTHAL, J., ET AL., Eds. 1997. 
Introduction to Implicit Surfaces. Morgan Kaufmann, San Francisco. BOUGUET, J.-Y., AND PERONA, P. 1999. 
3D photography using shadow in dual space geometry. International Journal of Computer Vision 35, 2 (Nov/Dev), 
129 149. BREEN, D., AND WHITAKER, R. 2001. A level set approach for the metamorphosis of solid models. 
IEEE Trans. on Visualization and Computer Graphics 7, 2, 173 192. BREEN, D., MAUCH, S., AND WHITAKER, 
R. 2000. 3D scan conversion of CSG models into distance, closest-point and colour volumes. In Volume 
Graphics, M. Chen, A. Kaufman, and R. Yagel, Eds. Springer, London, 135 158. COHEN, E., RIESENFELD, R., 
AND ELBER, G. 2001. Geometric Modeling with Splines. AK Peters, Natick, MA. CURLESS, B., AND LEVOY, M. 
1996. A volumetric method for building complex models from range images. In Proc. SIGGRAPH 96, 303 312. 
DESBRUN, M., AND CANI, M.-P. 1998. Active implicit surface for animation. In Graphics Interface, 143 
150. DESBRUN, M., AND GASCUEL, M. 1995. Animating soft substances with implicit surfaces. In Proc. SIGGRAPH 
95 Conference, 287 290. DESBRUN, M., MEYER, M., SCHR ¨ ODER, P., AND BARR, A. 1999. Implicit fairing 
of irregular meshes using diffusion and curvature .ow. In Proc. SIGGRAPH 99, 317 324. DO CARMO, M. 1976. 
Differential Geometry of Curves and Surfaces. Prentice-Hall, Englewood Cliffs, NJ. EDELSBRUNNER, H., 
AND M¨ UCKE, E. 1994. Three-dimensional alpha shapes. ACM Trans. on Graphics 13, 1, 43 72. EVANS, L., 
AND SPRUCK, J. 1991. Motion of level sets by mean curvature, I. Journal of Differential Geometry 33, 
635 681. FOSTER, N., AND FEDKIW, R. 2001. Practical animation of liquids. In Proc. SIG-GRAPH 2001, 23 
30. FRISKEN, S., PERRY, R., ROCKWOOD, A., AND JONES, T. 2000. Adaptively sam­pled distance .elds: A general 
representation of shape for computer graphics. In SIGGRAPH 2000 Proceedings, 249 254. GALYEAN, T., AND 
HUGHES, J. 1991. Sculpting: An interactive volumetric modeling technique. In Proc. SIGGRAPH 91, 267 274. 
HOFFMANN, C. 1989. Geometric and Solid Modeling. Morgan Kaufmann, San Francisco. KOBBELT, L., CAMPAGNA, 
S., VORSATZ, J., AND SEIDEL, H.-P. 1998. Interactive multi-resolution modeling on arbitrary meshes. In 
Proc. SIGGRAPH 98, 105 114. KOBBELT, L. P., BOTSCH, M., SCHWANECKE, U., AND SEIDEL, H.-P. 2001. Fea­ture 
sensitive surface extraction from volume data. In Proc. SIGGRAPH 2001, 57 66. LAIDLAW, D., TRUMBORE, 
W., AND HUGHES, J. 1986. Constructive solid geometry for polyhedral objects. In Proc. SIGGRAPH 86, 161 
170. LORENSEN, W., AND CLINE, H. 1987. Marching Cubes: A high resolution 3D surface construction algorithm. 
In Proc. SIGGRAPH 87, 163 169. MALLADI, R., SETHIAN, J., AND VEMURI, B. 1995. Shape modeling with front 
propagation: A level set approach. IEEE Trans. on Pattern Analysis and Machine Intelligence 17, 2, 158 
175. MARAGOS, P. 1996. Differential morphology and image processing. IEEE Trans. on Image Processing 
5, 6 (June), 922 937. OSHER, S., AND FEDKIW, R. 2001. Level set methods: An overview and some recent 
results. Journal of Computational Physics 169, 475 502. OSHER, S., AND SETHIAN, J. 1988. Fronts propagating 
with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations. Journal of Computa­tional 
Physics 79, 12 49. PENG, D., MERRIMAN, B., OSHER, S., ZHAO, H.-K., AND KANG, M. 1999. A PDE-based fast 
local level set method. Journal of Computational Physics 155, 410 438. PERRY, R., AND FRISKEN, S. 2001. 
Kizamu: A system for sculpting digital charac­ters. In Proc. SIGGRAPH 2001, 47 56. REQUICHA, A., AND 
VOELCKER, H. 1985. Boolean operations in solid modeling: Boundary evaluation and merging algorithms. 
Proceedings of the IEEE 73, 1, 30 44. RUDIN, L., OSHER, S., AND FATEMI, C. 1992. Nonlinear total variation 
based noise removal algorithms. Physica D 60, 259 268. SAPIRO, G., KIMMEL, R., SHAKED, D., KIMIA, B., 
AND BRUCKSTEIN, A. 1993. Implementing continuous-scale morphology via curve evolution. Pattern Recogni­tion 
26, 9, 1363 1372. SAPIRO, G. 2001. Geometric Partial Differential Equations and Image Analysis. Cambridge 
University Press, Cambridge, UK. SERRA, J. 1982. Image Analysis and Mathematical Morphology. Academic 
Press, London. SETHIAN, J. 1996. A fast marching level set method for monotonically advancing fronts. 
In Proceedings of the National Academy of Science, vol. 93, 1591 1595. SETHIAN, J. 1999. Level Set Methods 
and Fast Marching Methods, second ed. Cam­bridge University Press, Cambridge, UK. TAUBIN, G. 1995. A 
signal processing approach to fair surface design. In Proc. SIGGRAPH 95, 351 358. TSITSIKLIS, J. 1995. 
Ef.cient algorithms for globally optimal trajectories. IEEE Trans. on Automatic Control 40, 9, 1528 1538. 
WANG, S., AND KAUFMAN, A. 1994. Volume-sampled 3D modeling. IEEE Com­puter Graphics and Applications 
14, 5 (September), 26 32. WANG, S., AND KAUFMAN, A. 1995. Volume sculpting. In Proc. Symposium on Interactive 
3D Graphics, ACM SIGGRAPH, 151 156. WELCH, W., AND WITKIN, A. 1994. Free-form shape design using triangulated 
surfaces. In Proc. SIGGRAPH 94, 247 256. WHITAKER, R., AND XUE, X. 2001. Variable-conductance, level-set 
curvature for image denoising. In Proc. IEEE International Conference on Image Processing, 142 145. WHITAKER, 
R., BREEN, D., MUSETH, K., AND SONI, N. 2001. Segmentation of biological datasets using a level-set framework. 
In Volume Graphics 2001, M. Chen and A. Kaufman, Eds. Springer, Vienna, 249 263. WHITAKER, R. 1998. A 
level-set approach to 3D reconstruction from range data. International Journal of Computer Vision 29, 
3, 203 231. WYVILL, B., GALIN, E., AND GUY, A. 1999. Extending the CSG tree. warping, blending and Boolean 
operations in an implicit surface modeling system. Computer Graphics Forum 18, 2 (June), 149 158. ZHAO, 
H.-K., OSHER, S., AND FEDKIW, R. 2001. Fast surface reconstruction using the level set method. In Proc. 
1st IEEE Workshop on Variational and Level Set Methods, 194 202. A Level Set Models A deformable (i.e. 
time-dependent) surface, ......., is implicitly represented as an iso-surface of a time-varying scalar 
function, ..........., embedded in 3D, i.e. ...............................................(11) where 
.....is the iso-value, .......is time, and .............is a point in space on the iso-surface. It might 
seem inef.cient to implicitly represent a surface with a 3D scalar function; however the higher dimensionality 
of the representation provides one of the major advantages of the LS method: the .exible handling of 
changes in the topology of the deformable surface. This implies that LS surfaces can easily represent 
complicated surface shapes that can, form holes, split to form multiple objects, or merge with other 
objects to form a single structure. The fundamental level set equation of motion for .................is 
derived by differentiating both sides of Eq. (11) with respect to time ., and applying the chain rule 
giving: . ................. (12). where ......denotes the speed vectors of the level set surface. A number 
of numerical techniques by [Osher and Sethian 1988; Adalsteinsson and Sethian 1995] make the initial 
value problem of Eq. (12) computationally feasible. A complete discussion of the details of the level 
set method is beyond the scope of this paper. We instead refer the interested reader to [Sethian 1999; 
Osher and Fedkiw 2001]. However, we will brie.y mention two of the most important techniques: the .rst 
is the so called up-wind scheme which addresses the problem of overshooting when trying to solve Eq. 
(12) by a simple .nite forward difference scheme. The second is related to the fact that one is typically 
only interested in a single solution to Eq. (12), say the .....level set. This implies that the evaluation 
of .is important only in the vicinity of that level set. This forms the basis for narrow-band schemes 
[Adalsteinsson and Sethian 1995; Whitaker 1998; Peng et al. 1999] that solve Eq. (12) in a narrow band 
of voxels containing the surface. The up-wind scheme makes the level set method numerically robust, and 
the narrow-band scheme makes its computational complexity proportional to the level set s surface area 
rather than the size of the volume in which it is embedded. B Curvature of Level Set Surfaces The principle 
curvatures and principle directions are the eigenvalues and eigenvectors of the shape matrix [do Carmo 
1976]. For an implicit surface, the shape matrix is the derivative of the normalized gradient (surface 
nor­mals) projected onto the tangent plane of the surface. If we let the normals be ................, 
the derivative of this is the .....matrix ............ .............(13) The projection of this derivative 
matrix onto the tangent plane gives the shape matrix [do Carmo 1976] ................., where .is the 
ex­terior product. The eigenvalues of the matrix .are ........and zero, and the eigenvectors are the 
principle directions and the normal, respectively. Because the third eigenvalue is zero, we can compute 
........and various differential invariants directly from the invariants of .. Thus the weighted curvature 
.ow is computing from .using the identities ............,................, and ............... The principle 
curvatures are cal-.culated by solving the quadratic ...................... In many cir­cumstances, the 
curvature term, which is a kind of directional diffusion that does not suffer from overshooting, can 
be computed directly from .rst-and second-order derivatives of .using central difference schemes. However, 
we have found that central differences do introduce instabilities when com­puting .ows that rely on quantities 
other than the mean curvature. Therefore we use the method of differences of normals [Rudin et al. 1992; 
Whitaker and Xue 2001] in lieu of central differences. The strategy is to compute nor­malized gradients 
at staggered grid points and take the difference of these . staggered normals to get centrally located 
approximations to . The shape matrix .is computed with gradient estimates from central differences. The 
resulting curvatures are treated as speed functions (motion in the normal di­rection), and the associated 
gradient magnitude is computed using the up­wind scheme.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566586</article_id>
		<sort_key>339</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Dual contouring of hermite data]]></title>
		<page_from>339</page_from>
		<page_to>346</page_to>
		<doi_number>10.1145/566570.566586</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566586</url>
		<abstract>
			<par><![CDATA[This paper describes a new method for contouring a signed grid whose edges are tagged by Hermite data (i.e; exact intersection points and normals). This method avoids the need to explicitly identify and process "features" as required in previous Hermite contouring methods. Using a new, numerically stable representation for quadratic error functions, we develop an octree-based method for simplifying contours produced by this method. We next extend our contouring method to these simpli&pound;ed octrees. This new method imposes no constraints on the octree (such as being a restricted octree) and requires no "crack patching". We conclude with a simple test for preserving the topology of the contour during simplification.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[contouring]]></kw>
			<kw><![CDATA[crack prevention]]></kw>
			<kw><![CDATA[implicit functions]]></kw>
			<kw><![CDATA[polyhedral simplification]]></kw>
			<kw><![CDATA[quadratic error functions]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35024051</person_id>
				<author_profile_id><![CDATA[81100098726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rice University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382434</person_id>
				<author_profile_id><![CDATA[81100603350]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Losasso]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rice University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382463</person_id>
				<author_profile_id><![CDATA[81100603187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schaefer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rice University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14210854</person_id>
				<author_profile_id><![CDATA[81100611449]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rice University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>218462</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J., AND FERGUSON, K. 1995. Polygonization of non-manifold implicit surfaces. In Proceedings of SIGGRAPH 95, ACM SIGGRAPH / Addison Wesley, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Series, 309-316. ISBN 0-201-84776-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>55285</ref_obj_id>
				<ref_obj_pid>55279</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J. 1988. Polygonization of implicit surfaces. Computer Aided Geometric Design 5, 4, 341-356.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375270</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BONNELL, K. S., SCHIKORE, D. R., JOY, K. I., DUCHAINEAU, M., AND HAMANN, B. 2000. Constructing material interfaces from data sets with volume-fraction information. In IEEE Visualization 2000, 367-372. ISBN 0-7803-6478-3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CIGNONI, P., GANOVELLI, F., MONTANI, C., AND SCOPIGNO, R. 2000. Reconstruction of topologically correct and adaptive trilinear isosurfaces. 399-418.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FOLEY, J., VAN DAM, A., FEINER, S., AND HUGHES, J. 1995. Computer Graphics: Principles and Practice. Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FRISKEN, S. F., PERRY, R. N., ROCKWOOD, A. P., AND JONES, T. R. 2000. Adaptively sampled distance fields: A general representation of shape for computer graphics. In Proceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, 249-254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288280</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. S. 1998. Simplifying surfaces with color and texture using quadric error metrics. In IEEE Visualization '98, IEEE, 263-270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375250</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GERSTNER, T., AND PAJAROLA, R. 2000. Topology preserving and controlled topology simplifying multiresolution isosurface extraction. In IEEE Visualization 2000, 259-266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288142</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GIBSON, S. F. F. 1998. Using distance maps for accurate surface reconstruction in sampled volumes. In 1998 Volume Visualization Symposium, IEEE, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GOLUB, G. A., AND VAN LOAN, C. F. 1989. Matrix Computations, second ed. The Johns Hopkins University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>780990</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GUSKOV, I., AND WOOD, Z. 2001. Topological noise removal. In Graphics Interface 2001, 19-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383265</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[KOBBELT, L. P., BOTSCH, M., SCHWANECKE, U., AND SEIDEL, H.-P. 2001. Feature-sensitive surface extraction from volume data. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 57-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344912</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LINDSTROM, P. 2000. Out-of-core simplification of large polygonal models. In Proceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, 259-262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614330</ref_obj_id>
				<ref_obj_pid>614261</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LIVNAT, Y., SHEN, H.-W., AND JOHNSON, C. R. 1996. A near optimal isosurface extraction algorithm using the span space. 73-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383264</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PERRY, R. N., AND FRISKEN, S. F. 2001. Kizamu: A system for sculpting digital characters. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 47-56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J., AND BORRELL, P. 1993. Multi-resolution 3d approximation for rendering complex scenes. In Modeling in Computer Graphics, 455-465.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245628</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[SHEKHAR, R., FAYYAD, E., YAGEL, R., AND CORNHILL, J. F. 1996. Octree-based decimation of marching cubes surfaces. In IEEE Visualization '96, IEEE, 335-344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258868</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[STANDER, B. T., AND HART, J. C. 1997. Guaranteeing the topology of an implicit surface polygonization for interactive modeling. In Proceedings of SIGGRAPH 97, ACM SIGGRAPH / Addison Wesley, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Series, 279-286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WESTERMANN, R., KOBBELT, L., AND ERTL, T. 1999. Real-time exploration of regular volume data by adaptive reconstruction of isosurfaces. 100-111.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130882</ref_obj_id>
				<ref_obj_pid>130881</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[WILHELMS, J., AND GELDER, A. V. 1992. Octrees for faster isosurface generation. 201-227.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375254</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[WOOD, Z. J., DESBRUN, M., SCHR&#214;DER, P., AND BREEN, D. 2000. Semi-regular mesh extraction from volumes. In IEEE Visualization 2000, 275-282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dual Contouring of Hermite Data Tao Ju, Frank Losasso, Scott Schaefer, Joe Warren Rice University* 
 Figure 1: A temple undergoing destructive modi£cations. Both models were generated by dual contouring 
a signed octree whose edges contain Hermite data. The modified model on the right was computed from the 
lefthand model in real-time. Abstract This paper describes a new method for contouring a signed grid 
whose edges are tagged by Hermite data (i.e; exact intersection points and normals). This method avoids 
the need to explicitly iden­tify and process features as required in previous Hermite contour­ing methods. 
Using a new, numerically stable representation for quadratic error functions, we develop an octree-based 
method for simplifying contours produced by this method. We next extend our contouring method to these 
simpli£ed octrees. This new method imposes no constraints on the octree (such as being a restricted oc­tree) 
and requires no crack patching . We conclude with a simple test for preserving the topology of the contour 
during simplifica­tion. CR Categories: I.3.5 [Computation Geometry and Object Model­ing]: CSG Curve, 
surface, solid and object representations Keywords: implicit functions, contouring, crack prevention, 
quadratic error functions, polyhedral simplification * e-mail: {jutao,losasso,sschaefe,jwarren}@rice.edu 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 1 Introduction In the spring of 2001, 17 students in an advanced computer graphics class set out 
on a semester-long group project to develop a cutting­edge computer game. One of the primary goals for 
the game was to incorporate technology that allowed real-time modification of the game geometry. (In 
gaming terminology, such geometry is referred to as destructible .) The geometric engine for the resulting 
game was based on implicit modeling with the environment modeled as the zero contour of a 3D grid of 
scalar values. Our choice of this representation was guided by the fact that CSG operations are par­ticularly 
simple to implement for implicit models. Although this game was a relative success, we noted that our 
implicit approach to modeling had several disadvantages. During a post-project review, we identified 
several problems: Due to the use of a uniform grid, we were restricted to rela­tively small grid sizes. 
In particular, the final game could only process environments defined using grids of size 64 3 due to 
the requirement that the game run in real-time.  The resulting environment lacked the sharp edges found 
in most polyhedral models. Although we could simulate a small class of shapes such as rooms and hallways 
by cleverly ma­nipulating the sign field, the resulting environment was geo­metrically simple in comparison 
to those modeled using BSP trees.  The polyhedral meshes produced by contouring often con­tained large 
flat regions tiled by numerous small polygons. The tiling of these flat regions trivially inflated the 
number of polygons in our model and often overwhelmed the graphics card used for the game.  In preparation 
for the next version of the gaming class, the in­structor and three members of the class (the authors) 
decided to pur­sue a yearlong project to rewrite the game engine to address these deficiencies. In particular, 
we focused on adapting three pieces of recently developed modeling technology for our program. Each of 
these pieces addresses one of the problems: First, we use an octree in place of a 3D uniform grid. In 
partic­ular, our octree is inspired by those used in Adaptive Distance Fields [Frisken et al. 2000; Perry 
and Frisken 2001] in which signs are maintained at corners of cubes in the octree.  At the leaves of 
the octree, we tag those edges with sign changes by exact intersection points and their normals from 
the contour. This choice is inspired by the Extended March­ing Cubes method of [Kobbelt et al. 2001]. 
Adding normals allows this method to exactly reproduce a wide class of poly­hedral shapes as well as 
curve or sharp edges on the contour.  Third, we use these normals to de£ne a quadratic error func­tion 
(QEF) for each leaf of the octree. These QEFs are then used in an octree-based polyhedral simplification 
method sim­ilar to that of [Lindstrom 2000]. Our method uses the added information specified by the signs 
attached to the corners of cubes in the octree to preserve the topology of this contour during simplification. 
 The resulting representation is an octree whose leaf cubes have signs at their corners with exact intersections 
and normals tagging edges that exhibit sign changes. (See the upper left portion of figure 2 for an example). 
Interior nodes in the octree contain QEFs used during simplification. This representation can accurately 
approxi­mate implicit shapes as well as parametric shapes such as subdivi­sion surfaces. (These parametric 
shapes are imported as polygonal approximations and scan converted into a signed octree.) The adap­tive 
structure of the octree allows for real-time approximate CSG operations and simplification of the resulting 
shapes. Given that we are building on several pieces of previous work, we should make clear our original 
contributions in this paper. First, we propose a new method for contouring a 3D grid of Hermite data 
that avoids the need to explicitly identify and process features as done in the Extended Marching Cubes 
method. After extend­ing this contouring method to the case of multiple materials, we demonstrate how 
to model textured contours. We also introduce a new, numerically stable representation for quadratic 
error functions that we use in a standard octree-based method for simplifying these contours and their 
textured regions. We then develop a version of our contouring method for simplified octrees that imposes 
no con­straints on the octree (such as being a restricted octree) and requires no crack patching . We 
conclude with a simple new test for pre­serving the topology of both the contour and its textured regions 
during simplification.  2 Dual contouring on uniform grids Although our ultimate goal is to develop 
a simple contouring method that is suitable for octrees, we first consider various meth­ods for contouring 
signed uniform grids. The upper left portion of figure 2 shows a typical example of a signed uniform 
grid. Those edges of the grid that exhibit a sign change are tagged by Hermite data consisting of exact 
intersection points and normals from the contour. This Hermite data can be computed directly from the 
im­plicit definition of the contour or by scan converting a closed polyg­onal mesh. Figure 2: A signed 
grid with edges tagged by Hermite data (up­per left), its Marching Cubes contour (upper right), its Extended 
Marching Cubes contour (lower left), and its dual contour (lower right). 2.1 Previous contouring methods 
Cube-based methods such as the Marching Cubes (MC) algorithm and its variants generate one or more polygons 
for each cube in the grid that intersects the contour. Typically, these methods generate one polygon 
for each portion of the contour that interest a particu­lar cube with the vertices of these polygons 
being positioned at the intersection of the contour with the edges of the cube. The upper right portion 
of figure 2 shows a 2D example of the MC contour generated from the signed grid to its left. The left-hand 
side of fig­ure 3 shows a 3D example of a sphere generated as the zero contour 222 of the function f 
[x, y, z]= 1 - x- y- z. This contour consists of a collection of polygons that approximate the restriction 
of the contour to individual cubes in the grid. Dual methods such as the SurfaceNets algorithm of [Gibson 
1998] generate one vertex lying on or near the contour for each cube that intersects the contour. For 
each edge in the grid that exhibits a sign change, the vertices associated with the four cubes that con­tain 
the edge are joined to form a quad. The result is a continuous polygonal surface that approximates the 
contour. The right-hand side of figure 3 shows an example of the same sphere contoured using the SurfaceNets 
method. Note that the polygonal mesh pro­duced by the SurfaceNets method is dual to the mesh produced 
by MC in the standard topological sense: vertices of the SurfaceNets mesh correspond to faces of the 
MC mesh and vice versa. Dual methods typically deliver polygonal meshes with better aspect ra­tios since 
the vertices of the mesh are free to move inside the cube as opposed to being restricted to edges of 
the grid as in cube-based methods. 1 1Note that other methods such as [Wood et al. 2000] contour without 
respect to the underlying fine grid. We focus our attention on grid-based Figure 3: A sphere contoured 
using the Marching Cubes method (left) and the SurfaceNets method (right). The Extended Marching Cubes 
(EMC) method is a hybrid be­tween a cube-based method and a dual method. The EMC method detects the presence 
of sharp features inside a cube by examin­ing normals associated with the intersection points on the 
edges of the cube. Those cubes whose normals lie inside a user-specified cone are deemed to be featureless. 
In this case, the EMC method generates a polygon(s) using standard MC. For those cubes that do contain 
a feature, the method generates a vertex positioned at the minimizer of the quadratic function E[x]= 
.(ni · (x - pi))2 (1) i where the pairs pi, ni correspond to the intersections (and unit nor­mals) of 
the contour with the edges of the cube. Once this vertex has been positioned, the method generates a 
triangle fan to the edges on the boundary of the cube. Finally, if two adjacent cubes both con­tain feature 
vertices, then the pair of triangles generated by the fan to their common face has its common edge flipped 
to form a feature edge. The lower left portion of figure 2 shows a 2D example of the contour generated 
by EMC. 2.2 Dual contouring of Hermite data The main advantage of the EMC method is that it uses Hermite 
data and QEFs in positioning the vertices associated with cubes that contain features. This Hermite approach 
can generate contours that contain both sharp vertices and sharp edges. One drawback of this method is 
the need to explicitly test for such features and to then perform some type of special processing in 
these cases. As an alternative to the EMC method, we propose the following dual contouring method for 
Hermite data: 1. For each cube that exhibits a sign change, generate a vertex positioned at the minimizer 
of the quadratic function of equa­tion 1. 2. For each edge that exhibits a sign change, generate a quad 
connecting the minimizing vertices of the four cubes contain­ing the edge.  This method is an interesting 
hybrid of the EMC method and the SurfaceNets method. It uses the EMC method s feature ver­tex rule for 
positioning all vertices of the contour while using the SurfaceNets method to determine the connectivity 
of these vertices. (Note that the SurfaceNets method uses a completely different rule methods like the 
ones above since this grid structure is the basis of our fast CSG operations. Figure 4: A mechanical 
part generated by dual contouring Hermite data on a 643 grid. for positioning vertices on the contour.) 
By using QEFs to position all of the vertices of the contour, this method avoids the need to explicit 
test for features. Vertices on the contour are simply posi­tioned to be consistent with the normals associated 
with the data. The lower right portion of figure 2 shows a 2D example of the dual contour generated by 
the Hermite data in the upper left portion of the figure. Figure 4 shows a 3D example of a mechanical 
part modeled by dual contouring Hermite data on a 643 grid. The left image shows a smooth shaded version 
of the part while the right image shows the polygonal mesh produced by dual contouring. The intersection 
points and normals for the model were generated from a closed subdivision surface. A sign field denoting 
the inside/outside of the model was computed using a standard scan conversion algorithm as described 
in [Foley et al. 1995]. 2.3 Representing and minimizing QEFs At this point, we should make a few comments 
concerning how we represent and minimize quadratic error functions. The function E[x] of equation 1 is 
constructed from a collection of intersection points pi and normals ni. This function E[x] can be expressed 
as the inner product (Ax - b)T (Ax - b) where A is a matrix whose rows are the normals ni and b is a 
vector whose entries are ni · pi. Typically, the quadratic function E[x] is expanded into the form E[x]= 
xT AT Ax - 2xT AT b + bT b (2) where the matrix AT A is a symmetric 3× 3 matrix, AT b is a column vector 
of length three and bT b is a scalar. The advantage of this ex­pansion is that only the matrices AT A, 
AT b and bT b need be stored (10 floats), as opposed to storing the matrices A and b. Further­more, a 
minimizing value x for E[x] can be computed by solving the normal equations AT Ax = AT b. One drawback 
of this representation is that it is numerically un­stable. For example, consider computing the value 
of E[x] in float­ing point arithmetic when the intersection points and normals used in constructing E[x] 
are sampled from a flat area. For a grid of size 2563 (as in figure 1), the magnitude of bT b can be 
on the order of 106. Since floats are only accurate to six decimal digits, if E[x] is evaluated at points 
on the original flat area (where E[x] should be zero), the resulting value has an error on the order 
of 1. One possible solution to this problem is to use double precision numbers instead of floats in representing 
AT A, AT b and bT b. Us­ ing doubles, the value of E[x] in our flat example now has an error of 10-6. 
Of course, the main drawback of using doubles in place of floats is that the space require to store a 
QEF is doubled. For our application, we found this solution to be problematic since our program tended 
to be space bound as opposed to being time bound. (See the last section for details.) An alternative 
representation for QEFs that delivers the accuracy of doubles while using only floats is based on the 
QR decompo­sition [Golub and Van Loan 1989]. If (Ab) is the matrix formed by appending the column vector 
b to the matrix A, the idea behind this decomposition is to commpute an orthogonal matrix Q whose product 
with (Ab) is an upper triangular matrix of the form . xxxx . 0 xxx . A b . 00 xx . 0 r . . = . . (3) 
000 x . 00 . 0000 ... ...... ... ... ... Here, A is an upper triangular 3 × 3 matrix, b is a column vector 
of length 3 and r is a scalar. This matrix Q can be expressed as the product of a sequence of Givens 
rotations where each rotation zeroes a single entry in the lower part of (Ab). Since any orthogonal matrix 
Q satis£es the relation QTQ = I, E[x] can be rewritten as (Ax - b)T (Ax - b)=(Ax - b)T QT Q(Ax - b) =(QAx 
- Qb)T (QAx - Qb) =( b )T ( b )+ r2 Ax - Ax - To evaluate E[x] in this form, we compute the product of 
the vector 2 Ax - b with itself and then add r. Returning to our previous flat example, we note that 
b has entries on the order of 103 and therefore Ax - b has entries that are on the order of 10-3 when 
x is chosen from the flat regions. Therefore, the computed value of E[x] will be on the order of 10-6. 
If A is non-singular, the minimizing x can be computed by solv­ing Ax = b using back substitution. However, 
during dual con­ touring, A is often computed from noisy normals that are nearly coplanar. In this case, 
the matrix A is nearly singular. As a result, the minimizing x may lie far outside the defining cube. 
To solve this problem, we compute the SVD decomposition of A and form its pseudo-inverse by truncating 
its small singular values as done in [Kobbelt et al. 2001; Lindstrom 2000]. Based on experimenta­tion, 
we typically truncate those singular values with a magnitude of less than 0.1. Using the resulting pseudo-inverse, 
we then ap­proximately solve A x = b while minimizing the distance of x to the centroid of the intersection 
points pi.  2.4 Modeling textured contours In figure 3, both contouring algorithms produced surfaces 
that bounded the transition from negative (empty) space to positive (solid) space. In a realistic environment, 
solids are not composed of a single homogeneous material. In practice, solids are composed of a collection 
of materials; each of which induces a region with a dis­tinct texture on the contour. Figure 5 shows 
an example of a cube consisting of two materials, a gold material formed by extruding a Chinese character 
through the cube and a red material forming the remaining portion of the cube. Note that the gold material 
is a true solid (and not a surface texture) since the gold character extends all the way through the 
cube (as evidenced by the cube after a spherical cut on the right). This partition of solids into distinct 
materials can be modeled implicitly by replacing the signs - and + (corresponding to empty Figure 5: 
A solid cube undergoing a sequence of CSG operations. Figure 6: The dual contour for a three-index grid 
(left), treating the two solid (dark) indices as single index(right) and solid space) by a material index. 
In this representation, each grid point has an index corresponding to a distinct material.(See [Bloomenthal 
and Ferguson 1995; Bonnell et al. 2000] for exam­ples of similar approaches.) Figure 6 shows a 2D grid 
with three distinct indices; the gray and black grid points denote distinct solid materials while white 
grid points denote empty space. As before, edges that exhibit index changes are also tagged by exact 
intersec­tion points and normals. During contouring, this Hermite data is used in equation 1 to define 
a QEF associated with each cube in the grid. Next, for each edge that exhibits a index change, dual contour­ing 
generates a quad connecting the minimizers of the QEFs for the four cubes containing the edge. The left 
portion of figure 6 shows the dual contour that separates the three materials. If the viewer is restrict 
to empty space, we can optimize this contouring method for solids that consist several different materials. 
In particular, quads generated by solid/solid edges are not visible from empty space. The remaining quads 
correspond to solid/empty edges and can be textured using the material properties of the solid endpoint 
of the edge 2. In figure 5, the underlying grid contains three material; empty space, gold material, 
and red material. The resulting dual contour consists of red quads generated by red/empty edges and gold 
quads generated by gold/empty edges. Red/gold edges do not generate quads. 2Each material has an associated 
black box de£ned during the mate­rial s addition to the model that converts 3D geometric coordinates 
into 2D texture coordinates. When three or more materials meet inside a single cube, dual contouring 
places the minimizing vertex at or near their intersection point. This positioning allows the outlines 
of letters and characters embossed on a surface to be reproduced very accurately. (The right­hand portion 
of figure 12 shows a close-up example of this effect.) Cube-based contouring methods constrain the vertices 
of the con­tour to lie on the edges of the 3D grid making this effect difficult to achieve. The move 
to the multi-material case allows for several interest­ing variations on the CSG operations used in the 
two-material case. In place of the standard CSG operations, we use a single operation Add that overwrites 
a portion of the existing model with a new ma­terial. Subtractive operations such as the spherical cut 
in the upper right of figure 5 can be represented as adding a sphere of empty space to the model. Another 
useful variant of Add is the operation Replace that overwrites only the solid portion of the model. This 
operation can also be used to simulate texturing a portion of the contour. For example, the lower images 
in figure 5 show a portion of the solid replaced by a blue material and then subsequently cut by a sphere. 
 3 Adaptive dual contouring The previous algorithm for dual contouring has the obvious disad­vantage 
of being formulated for uniform grids. In practice, most of a uniform grid is devoted to storing homogeneous 
cubes (i.e; cubes whose vertices all have the same sign). Only a small fraction of the cubes are heterogeneous 
and thus, intersect the contour. One way to avoid this waste of space is to replace the uniform grid 
by an octree. In this section, we describe an adaptive version dual con­touring based on simplifying 
an octree whose leaves contain QEFs. This method is essentially an adaptive variant of a uniform simpli­fication 
method due to [Lindstrom 2000]. Our method has three steps: Generate a signed octree whose homogeneous 
leaves are max­imally collapsed.  Construct a QEF for each heterogeneous leaf and simplify the octree 
using these QEFs.  Recursively generate polygons for this simplified octree.  Note that our method 
also differs from Lindstrom s method in that we generate polygons from the signed octree instead of collapsing 
polygons in an existing mesh. Generating the signed octree in the first step is relatively straight­forward. 
For implicit or polygonal models, this octree can be con­structed recursively in a top-down manner by 
spatially partitioning the models. For signed data on a uniform grid, this octree can be generated in 
a bottom-up manner by recursively collapsing homo­geneous regions. The next two subsections examine the 
second and third parts of this process in more detail. (Note that the adaptive method described here 
works for multi-material case without mod­ification.) 3.1 Octree simpli.cation using QEFs Our approach 
to the second step of the adaptive method is to con­struct a QEF associated with each heterogeneous leaf 
using equa­tion 1. Note that the residual associated with the minimizer of this QEF estimates how well 
the minimizing vertex approximates the original geometry [Garland and Heckbert 1998]. Our approach to 
simplifying the resulting octree is to form QEFs at interior nodes of the octree by adding the QEFs associated 
with the leaves of the subtree rooted by the node. Those interior nodes whose QEFs have a residual less 
than a given tolerance are collapsed into leaves. Figure 7: Closeups of two polygonal approximations 
to the temple computed using the standard (left) and QR (right) representation for QEFs. The only modification 
that we make to this method is to change our internal representation for QEFs to take advantage of the 
QR decomposition discussed in the previous section. To this end, we represent a QEF in terms of 10 floats 
corresponding to the entries of the upper triangular matrix of equation 3. Adding two QEFs cor­responds 
to merging the rows of their two upper triangular matrices to form a single 8 × 4 matrix of the form 
. x x x x 0 x x x . 0 0 x x . . 0 0 0 x . . x x x x . 0 x x x . . 0 0 x x . 0 0 0 x and then performing 
a sequence of Givens rotations to bring the matrix back into upper triangular form of equation 3. Due 
to the or­thogonality of the Givens rotations, the QEF for the merged system is the sum of QEFs associated 
with the unmerged systems. Note that bringing the merged system back into upper triangular form is slower 
(around 150 arithmetic operations) than simply adding 10 floats as done in the standard representation. 
However, the im­proved stability of the representation leads to better simpli£cations. Figure 7 gives 
a concrete illustration of the advantage of the QR representation s stability. The meshes in this figure 
show two sim­plifications of the temple from figure 1 to an error of 0.014; the left mesh was computed 
using the standard representation for QEFs, the right mesh was computed using the QR representation for 
QEFs. (To give a sense of scale, the temple was defined over a 256 3 unit grid.) Due to the numerical 
error introduced by the instability of the standard representation, the mesh on the left contains 78K 
polygons while the mesh on the right has 36K polygons. 3.2 Polygon generation for simpli.ed octrees 
Given this simplified octree, our next task is to modify the poly­gon generation phase of dual contouring 
appropriately. For cube­based methods, this problem of generating contours from octrees has been extensively 
studied [Bloomenthal 1988; Wilhelms and Gelder 1992; Livnat et al. 1996; Shekhar et al. 1996; Westermann 
et al. 1999; Frisken et al. 2000; Cignoni et al. 2000]. Typically, these methods restrict the octree 
to have neighboring leaves that differ by at most one level (i.e; restricted octrees) and usually per­form 
some type of crack repair to ensure a closed contour. [Perry and Frisken 2001] describes a variant of 
the SurfaceNets algorithm for signed octrees based on enumerating the edges associated with leaves of 
the octree. In particular, For each edge that exhibits a sign change, generate all trian­gles that connect 
the vertices associated with any three distinct cubes containing the edge. To avoid generating redundant 
triangles, this method culls some of the generated triangles based on the relative positions of their 
corresponding edges inside a leaf cube. In the final mesh, each edge corresponds to either one or two 
triangles (a quad) on the resulting contour. The main disadvantage of this method (as acknowledged by 
the authors) is that it occasionally yields contours with cracks. The authors identify these problem 
configurations and avoid them by performing extra subdivision on the octree. We propose a simpler rule 
for dual contouring signed octrees that avoids this need for extra subdivision. The rule is based on 
the observation that only those edges of leaf cubes that do not properly contain an edge of a neighboring 
leaf should generate a polygon. We refer to such edges as the minimal edges of the octree. Thus, our 
rule for polygon generation is For each minimal edge that exhibits a sign change, generate a polygon 
connecting the minimizing vertices of cubes that contain the edge. This rule has the property that it 
always produces a closed polyg­onal mesh for any simplified octree. In particular, every edge in the 
mesh is contained by an even number of polygons. To prove this fact, we observe that edges in the dual 
contour are generated by pairs of face-adjacent leaf cubes. The minimal edges tiling the boundary of 
their common square face always exhibit an even num­ber of sign changes since the boundary of the square 
is a closed curve. Therefore, the rule always generates an even number of poly­gons containing the edge. 
For example, the common square face always consists of four consecutive edges in the uniform case. This 
chain of four edges can exhibit either two or four sign changes and consequently generate two or four 
polygons containing the com­mon edge. (It is possible to construct signed octrees that generate dual 
contours with 6 or more polygons share a common edge.) Fig­ure 8 shows three simplified approximations 
to the mechanical part. Note that the rightmost mesh has undergone a topology change. This rule generates 
triangles instead of quads in transitional areas of the octree where a single coarse cube is face-adjacent 
to four fine cubes. Minimal edges in the middle of the shared coarse face are contained by only three 
cubes and generate triangles that form a transition between coarse quads and fine quads. Figure 7 shows 
many examples of such transition triangles produced by contouring a simplified octree. Figure 9: Recursive 
functions faceProc (black) and edgeProc (gray) used in enumerating pairs of leaf squares that contain 
a com­mon edge. Note that the Perry/Frisken rule enumerates edges in the octree and then locates those 
cubes that contain the edge. This neighbor finding entails either walking up and down the octree or explicitly 
maintaining links between neighboring cubes. Instead of enumer­ating edges and trying to find neighbors, 
we propose a recursive method for enumerating those sets of cubes that contain a common minimal edge. 
For the sake of simplicity, we explain this enumera­tion method for quadtrees while noting that a similar 
method works for octrees. The key to this enumeration procedure are two recursive func­tions faceProc[q] 
and edgeProc[q1,q2]. Given an interior node q in the quadtree, faceProc[q] recursively calls itself on 
the four children of q as well as calling edgeProc on all four pairs of edge­adjacent children of q. 
Given a pair of edge-adjacent interior nodes q1 and q2, edgeProc[q1,q2] recursively calls itself on the 
two pairs of edge-adjacent children spanning the common edge between q1 and q2. Figure 9 depicts the 
mutually recursive structure of these two functions. The recursive calls to edgeProc[q1,q2] terminate 
when both q1 and q2 are leaves of the quadtree. At this point, the call to edgeProc has all of the information 
necessary to generate the seg­ment associated with the minimal edge shared by q1 and q2. Note the running 
time of this method is linear in the size of the quadtree since there is one call to faceProc for each 
square in the quadtree and one call to edgeProc for each edge in the quadtree. Contouring octrees requires 
three functions cellProc[q] , faceProc[q1,q2] and edgeProc[q1,q2,q3,q4] . The func­tion cellProc spawns 
eight calls to cellProc, twelve calls to faceProc and six calls to edgeProc. faceProc spawns four calls 
to faceProc and four calls to edgeProc. Finally, edgeProc spawns two calls to edgeProc. The recursive 
calls to edgeProc[q1,q2,q3,q4] terminate at minimal edges of the oc­trees where all of the qi s are leaves. 
 4 Simpli.cation with topology safety Simpli£cation methods such as [Rossignac and Borrell 1993; Lind­strom 
2000] have the property that the topological connectivity of the polygonal mesh may change during simplification. 
More so­phisticated methods such as [Stander and Hart 1997; Gerstner and Pajarola 2000; Wood et al. 2000; 
Guskov and Wood 2001] were developed to maintain the connectivity of the mesh during simpli­fication. 
Unfortunately, in our setting, not only can the topolog­ical connectivity of the contour change during 
simplification, but also the connectivity of its textured regions. The left side of fig­ure 12 shows 
an example of a simplification in which distinct parts of the Chinese character have merged making it 
difficult to recog­nize. While these topological changes are not always undesirable, we wish to have 
the option of maintaining the topological connec­tivity of the contour and its textured regions during 
simpli£cation. Speci£cally, given an interior node in the octree whose eight chil­dren are leaves, we 
desire a test based on the signs (or indices) at the corners of these leaves that guarantees that the 
topological con­nectivity of the dual contour and its textured regions is preserved during collapse of 
the node. 4.1 The two-signed case Consider a coarse cube consisting of eight leaf cubes. The signs at 
the corners of the eight leaf cubes define a 3 × 3 × 3 grid whose corners defined a 2 × 2 × 2 coarse 
grid. Our goal is to develop a test for determining whether the dual contour generated by this fine grid 
is topologically equivalent to the dual contour generated by the coarse grid 3. Before presenting the 
test, we recall that a d-dimensional con­tour is locally a manifold if it is topologically equivalent 
to a d­dimensional disc. Since a cube has twelve edges, dual contouring can generate up to twelve polygons 
that meet at the central vertex associated with the cube. For most common sign configurations on the 
cube, these polygons define a manifold at this vertex. How­ever, there exist sign con£gurations for which 
the dual contour is non-manifold. (These configurations correspond to the ambigu­ous sign con£gurations 
in standard cube-based methods.) Given this de£nition, the safety test has three checks: 1. Test whether 
the dual contour for the coarse cube is a mani­fold. If not, stop. 2. Test whether the dual contour 
for each individual fine cube is a manifold. If not, stop. 3. Test whether the fine contour is topologically 
equivalent to the coarse contour on each of the sub-faces of the coarse cube. If not, stop; otherwise 
safely collapse.  The first two checks restrict the simplification process to mani­fold dual contours. 
(Note that the second check can be dropped if the fine leaf cubes are themselves the results of a previous 
collapse.) In practice, this restriction is acceptable since most fine resolution contours are manifold 
with non-manifold contours usually arising due to unsafe simplification. For the first two checks, [Gerstner 
and Pajarola 2000] describe a simple test for determining whether the contour associated with a single 
cube is a manifold. The idea is to repeatedly collapse the edges of the cube whose corners have the same 
sign to a single vertex. Now, the contour associated with the cube is manifold if and only if the result 
of this reduction is a single edge. The result of this test can be pre-computed for all possible sign 
configurations associated with a single cube and stored in a table of size 28. The third check tests 
topological equivalence of the coarse and fine contours as follows: First, the method checks for topological 
equivalence on the edges of the coarse cube. Next, the method checks for topological equivalence on the 
faces of the coarse cube. Finally, the method checks for equivalence on the interior of the coarse cube. 
These checks can be implemented as a sequence of sign comparisons on the 3 × 3 × 3 grid of signs. The 
sign in the middle of a coarse edge must agree with the sign of at least one of the edge s two endpoints. 
 The sign in the middle of a coarse face must agree with the sign of at least one of the face s four 
corners.  The sign in the middle of a coarse cube must agree with the sign of at least one of the cube 
s eight corners.  3Two shapes are topologically equivalent if they can be deformed into each other by 
a continuous, invertible mapping. Figure 10 shows three signed quadtrees that are candidates for simplification. 
The dual contour for the left quadtree has two dis­tinct connected components. In this case, the first 
check rejects the simplification as unsafe since the contour for the collapsed quadtree is non-manifold. 
The dual contour for the middle quadtree also has two distinct components. In this case, the third check 
rejects the simplification since the left edge of the quadtree cannot be safely simplified. The signs 
for the rightmost quadtree satisfy all three checks and therefore the quadtree can be safely simplified. 
The proof of correctness for these sign checks is based on estab­lishing topological equivalence for 
subfaces of the coarse cube in order of increasing dimension. The right mesh in figure 8 shows an example 
of a simplified version of the mechanical part that has un­dergone a topology change that disconnects 
the mesh. The middle mesh in figure 8 shows an example of the part after safe simplifica­tion with the 
topology checks preventing further unsafe simplifica­tion. 4.2 The multi-material case One nice feature 
of the contouring and simplification methods dis­cussed in the previous sections is that these methods 
handle the case of multiple materials without any extra difficulty. Luckily, the safety test described 
in the previous subsection also generalizes to the contours of multi-material regions with one small 
change. An apparent difficulty is that the contours of multi-material regions are inherently non-manifold 
in the two-material sense. For exam­ple, figure 11 shows three examples of dual contours separating the 
three materials. Two of the contours have a vertex where three ma­terials meet. Note that if we consider 
the boundary of each mate­rial s region separately, we can still classify whether this portion of the 
dual contour is a manifold. Specifically, a multi-material dual contour is a quasi-manifold if the boundary 
of each material s re­gion is a manifold. In the two-material case, being a quasi-manifold is equivalent 
to being a manifold. Now, the multi-material safety test determines whether it is topo­logically safe 
to simplify dual contours that are quasi-manifolds. As before, this restriction is not particularly problematic 
since most portions of a multi-material contour are quasi-manifold. This new test again consists of three 
phases and is identical to the two­material test with the exception that we replace the first and second 
checks for whether the contour inside a single cube is a manifold by an equivalent test for whether the 
contour is a quasi-manifold. The index tests in phase three remain unchanged. In analogy with the manifold 
case, the quasi-manifold test for a multi-material cube involves collapsing each edge of the cube whose 
endpoints have the same index. Now, the dual contour asso­ciated with the cube is a quasi-manifold if 
and only if the collapsed edge graph is a simplex (i.e; a point, a segment, a triangle or a tetra­hedron). 
As in the two-sign case, the values of this function can be pre-computed and stored in a lookup table 
of size 48. (If a cube has 5 or more distinct indices, its edge graph cannot collapse to a simplex.) 
The correctness of this test can be verified by selecting an index on the cube and treating all of the 
remaining indices as being equivalent. Since the resulting edge graph collapses to a segment, the portion 
of the contour corresponding to the chosen index is a manifold.  Figure 11 shows two multi-material 
quadtrees that are candidates for simplification. The left quadtree is rejected since the third check 
fails on the bottom edge of the quadtree. The middle quadtree passes all three checks and collapses to 
the quadtree on the right. Note that the contour for this collapsed quadtree is a quasi-manifold since 
the collapsed edge graph for this square is a triangle. Figure 12 shows two simplified versions of the 
Chinese cube from figure 5. The left version has been simplified without any type of topological safety. 
Note that the disjoint components of the Chinese character have fused together. The right version has 
been safely simplified using the multi-sign test with separate regions of the character remaining distinct 
after simplification.  5 Results The current version of our geometric program runs on a consumer­grade 
PC equipped with a GeForce 3 video card. The program per­forms adaptive dual contouring on an indexed 
octree. The table below shows the number of quads generated by our method for var­ious examples after 
simplification to an error tolerance of 0.01. (All grids have unit spacing.) The time field represents 
the sum of the times to simplify the initial octree (with topological safety) and then generate polygons 
from the simplified octree. The CSG operations (spheres of radius 6) in figure 1 took approximately 30 
milliseconds to compute. model # quads time (MS) space (MB) part 643 2578 44 1.3 Chinese cube 1283 1646 
636 32 temple 2563 39201 3586 156 david 5123 143533 2948 91 Acknowledgements We would like to thank 
Danny Sorenson for his suggestion to use the QR decomposition to represent QEFs. We would also like to 
thank Marc Levoy and the Michelangelo Project for their gracious permission to use the scanned version 
of Michelangelo s David. References BLOOMENTHAL, J., AND FERGUSON, K. 1995. Polygonization of non-manifold 
implicit surfaces. In Proceedings of SIGGRAPH 95, ACM SIGGRAPH / Addison Wesley, Los Angeles, California, 
Computer Graphics Proceedings, Annual Confer­ence Series, 309 316. ISBN 0-201-84776-0. BLOOMENTHAL, J. 
1988. Polygonization of implicit surfaces. Computer Aided Geometric Design 5, 4, 341 356. BONNELL, K. 
S., SCHIKORE, D. R., JOY, K. I., DUCHAINEAU, M., AND HAMANN, B. 2000. Constructing material interfaces 
from data sets with volume-fraction information. In IEEE Visualization 2000, 367 372. ISBN 0-7803-6478-3. 
CIGNONI, P., GANOVELLI, F., MONTANI, C., AND SCOPIGNO, R. 2000. Recon­struction of topologically correct 
and adaptive trilinear isosurfaces. 399 418. FOLEY, J., VAN DAM, A., FEINER, S., AND HUGHES, J. 1995. 
Computer Graphics: Principles and Practice. Addison Wesley. FRISKEN, S. F., PERRY, R. N., ROCKWOOD, A. 
P., AND JONES, T. R. 2000. Adap­tively sampled distance fields: A general representation of shape for 
computer graphics. In Proceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, 
Computer Graphics Proceedings, Annual Conference Series, 249 254. GARLAND, M., AND HECKBERT, P. S. 1998. 
Simplifying surfaces with color and texture using quadric error metrics. In IEEE Visualization 98, IEEE, 
263 270. GERSTNER, T., AND PAJAROLA, R. 2000. Topology preserving and controlled topol­ogy simplifying 
multiresolution isosurface extraction. In IEEE Visualization 2000, 259 266. GIBSON, S. F. F. 1998. Using 
distance maps for accurate surface reconstruction in sampled volumes. In 1998 Volume Visualization Symposium, 
IEEE, 23 30. GOLUB, G. A., AND VAN LOAN, C. F. 1989. Matrix Computations, second ed. The Johns Hopkins 
University Press. GUSKOV, I., AND WOOD, Z. 2001. Topological noise removal. In Graphics Interface 2001, 
19 26. KOBBELT, L. P., BOTSCH, M., SCHWANECKE, U., AND SEIDEL, H.-P. 2001. Feature-sensitive surface 
extraction from volume data. In Proceedings of SIG-GRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics 
Proceedings, Annual Conference Series, 57 66. LINDSTROM, P. 2000. Out-of-core simplification of large 
polygonal models. In Pro­ceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, 
Computer Graphics Proceedings, Annual Conference Series, 259 262. LIVNAT, Y., SHEN, H.-W., AND JOHNSON, 
C. R. 1996. A near optimal isosurface extraction algorithm using the span space. 73 84. PERRY, R. N., 
AND FRISKEN, S. F. 2001. Kizamu: A system for sculpting digital characters. In Proceedings of SIGGRAPH 
2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 47 56. ROSSIGNAC, 
J., AND BORRELL, P. 1993. Multi-resolution 3d approximation for rendering complex scenes. In Modeling 
in Computer Graphics, 455 465. SHEKHAR, R., FAYYAD, E., YAGEL, R., AND CORNHILL, J. F. 1996. Octree-based 
decimation of marching cubes surfaces. In IEEE Visualization 96, IEEE, 335 344. STANDER, B. T., AND HART, 
J. C. 1997. Guaranteeing the topology of an implicit surface polygonization for interactive modeling. 
In Proceedings of SIGGRAPH 97, ACM SIGGRAPH / Addison Wesley, Los Angeles, California, Computer Graphics 
Proceedings, Annual Conference Series, 279 286. WESTERMANN, R., KOBBELT, L., AND ERTL, T. 1999. Real-time 
exploration of regular volume data by adaptive reconstruction of isosurfaces. 100 111. WILHELMS, J., 
AND GELDER, A. V. 1992. Octrees for faster isosurface generation. 201 227. WOOD, Z. J., DESBRUN, M., 
SCHR ¨ ODER, P., AND BREEN, D. 2000. Semi-regular mesh extraction from volumes. In IEEE Visualization 
2000, 275 282.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566587</section_id>
		<sort_key>347</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Parameterization and meshes]]></section_title>
		<section_page_from>347</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39036867</person_id>
				<author_profile_id><![CDATA[81100305012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566588</article_id>
		<sort_key>347</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Interactive geometry remeshing]]></title>
		<page_from>347</page_from>
		<page_to>354</page_to>
		<doi_number>10.1145/566570.566588</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566588</url>
		<abstract>
			<par><![CDATA[We present a novel technique, both flexible and efficient, for interactive remeshing of irregular geometry. First, the original (arbitrary genus) mesh is substituted by a series of 2D maps in parameter space. Using these maps, our algorithm is then able to take advantage of established signal processing and halftoning tools that offer real-time interaction and intricate control. The user can easily combine these maps to create a control map --- a map which controls the sampling density over the surface patch. This map is then sampled at interactive rates allowing the user to easily design a tailored resampling. Once this sampling is complete, a Delaunay triangulation and fast optimization are performed to perfect the final mesh.As a result, our remeshing technique is extremely versatile and general, being able to produce arbitrarily complex meshes with a variety of properties including: uniformity, regularity, semi-regularity, curvature sensitive resampling, and feature preservation. We provide a high level of control over the sampling distribution allowing the user to interactively custom design the mesh based on their requirements thereby increasing their productivity in creating a wide variety of meshes.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39033302</person_id>
				<author_profile_id><![CDATA[81100223488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alliez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC / INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31048103</person_id>
				<author_profile_id><![CDATA[81100569027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14026403</person_id>
				<author_profile_id><![CDATA[81100041821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Desbrun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[www.cgal.org: Computational Geometry Algorithms Library.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BOROUCHAKI, H. Geometric Surface Mesh. In 2nd International Conference on Integrated and Manufacturing in Mechanical Engineering (may 1998), pp.343-350.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>249284</ref_obj_id>
				<ref_obj_pid>249274</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BOROUCHAKI, H., GEORGE, P. L., HECHT, F., LAUG, P., AND SALTEL, E. Delaunay Mesh Generation Governed by Metric Specifications. Finite Elements in Analysis and Design 25 (1997), pp.61-83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOROUCHAKI, H., HECHT, F., AND FREY, P. J. Mesh Gradation Control. In Proceedings of 6th International Meshing Roundtable, Sandia National Labs (oct 1997), pp. 131-141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BOSSEN, F., AND HECKBERT, P. A Pliant Method for Anisotropic Mesh Generation. In 5th Intl. Meshing Roundtable (oct 1996), pp.63-76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BOTSCH, M., AND KOBBELT, L. Resampling Feature and Blend Regions in Polygonal Meshes for Surface Anti-Aliasing. In Eurographics proceedings (sep 2001), pp.402-410.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BOTSCH, M., R&#214;SSL, C., AND KOBBELT, L. Feature Sensitive Sampling for Interactive Remeshing. In Vision, Modeling and Visualization proceedings (2000), pp. 129-136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DE COUGNY, H. L., AND SHEPHARD, M. S. Surface Meshing Using Vertex Insertion. In Proceedings of 5th International Meshing Roundtable, Sandia National Labs (oct 1996), pp.243-256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., MEYER, M., AND ALLIEZ, P. Intrinsic parameterizations of surface meshes. In Proceedings of Eurographics (2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>570841</ref_obj_id>
				<ref_obj_pid>570828</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DYN, N., HORMANN, K., S.-J. KIM, AND LEVIN, D. Optimizing 3D Triangulations using Discrete Curvature Analysis. Mathematical methods for curves and surfaces, Oslo 2000 (2001), pp. 135-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In Proceedings of SIGGRAPH (1995), pp. 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>513430</ref_obj_id>
				<ref_obj_pid>513400</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[ERIKSON, J., AND HAR-PELED, S. Optimally cutting a surface into a disk. In Proceedings of the 18th Annual ACM Symposium on Computational Geometry (2002). to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[FREY, P. J. About Surface Remeshing. In Proceedings of the 9th Int. Meshing Roundtable (2000), pp. 123-136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288280</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. Simplifying Surfaces with Color and Texture using Quadric Error Metrics. In IEEE Visualization Conference Proceedings (1998), pp.263-269.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364345</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., WILLMOTT, A., AND HECKBERT, P. Hierarchical Face Clustering on Polygonal Surfaces. In ACM Symposium on Interactive 3D Graphics (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[GEORGE, P. L., AND BOROUCHAKI, H., Eds. Delaunay Triangulation and Meshing Application to Finite Elements. HERMES, Paris, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>524240</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[GRAY, A., Ed. Modern Differential Geometry of Curves and Surfaces. Second edition. CRC Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218475</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[GRIMM, C. M., AND HUGHES, J. F. Modeling Surfaces of Arbitrary Topology using Manifolds. In Proceedings of SIGGRAPH (1995), pp.359-368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566589</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[GU, X., GORTLER, S., AND HOPPE, H. Geometry Images. In Proceedings of SIGGRAPH (2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311577</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[GUSKOV, I., SWELDENS, W., AND SCHR&#214;DER, P. Multiresolution Signal Processing for Meshes. In Proceedings of SIGGRAPH (1999), pp. 325-334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344831</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[GUSKOV, I., VIDIMCE, K., SWELDENS, W., ANDSCHR&#214;DER, P. Normal Meshes. In Proceedings of SIGGRAPH (2000), pp.95-102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[HORMANN, K., LABSIK, U., AND GREINER, G. Remeshing Triangulated Surfaces with Optimal Parameterizations. Computer-Aided Design 33 (2001), pp.779-788.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378630</ref_obj_id>
				<ref_obj_pid>378583</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[LAZARUS, F., POCCHIOLA, M., VEGTER, G., AND VERROUST, A. Computing a Canonical Polygonal Schema of an Orientable Triangulated Surface. In Proceedings of 17th Annu. ACM Sympos. Comput. Geom. (2001), pp.80-89.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LEE, A. W. F., SWELDENS, W., SCHR&#214;DER, P., COWSAR, L., AND DOBKIN, D. MAPS: Multiresolution Adaptive Parameterization of Surfaces. In Proceedings of SIGGRAPH (1998), pp.95-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383308</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L&#201;VY, B. Constrained Texture Mapping for Polygonal Meshes. In Proceedings of SIGGRAPH (2001), pp.417-424.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566590</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[L&#201;VY, B., PETITJEAN, S., RAY, N., AND MAILLOT, J. Least Squares Conformal Maps for Automatic Texture Atlas Generation. In Proceedings of SIGGRAPH (2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288288</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[LINDSTROM, P., AND TURK, G. Fast and Memory Efficient Polygonal Simplification. In IEEE Visualization Proceedings (1998), pp. 279-286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>629034</ref_obj_id>
				<ref_obj_pid>628896</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[MEYER, M., DESBRUN, M., SCHR&#214;DER, P., AND BARR, A. H. Discrete Differential-Geometry Operators for Triangulated 2-Manifolds, 2002. http://multires.caltech.edu/pubs/diffGeoOps.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383326</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[OSTROMOUKHOV, V. A Simple and Efficient Error-Diffusion Algorithm. In Proceedings of SIGGRAPH (2001), pp.567-572.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[P. V&#201;RON, J.-C. L. Static Polyhedron Simplification using Error Measurements. Computer-Aided Design 29(4) (1997), pp.287-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383301</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[PAULY, M., AND GROSS, M. Spectral Processing of Point-Sampled Geometry. In Proceedings of SIGGRAPH (2001), pp.379-386.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[PINKALL, U., AND POLTHIER, K. Computing Discrete Minimal Surfaces and Conjugates. Experimental Mathematics 2(1) (1993), pp. 15-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[RASSINEUX, A., VILLON, P., SAVIGNAT, J.-M., AND STAB, O. Surface Remeshing by Local Hermite Diffuse Interpolation. International Journal for numerical methods in Engineering 49 (2000), pp.31-49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>884118</ref_obj_id>
				<ref_obj_pid>882487</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[SHEFFER, A. Spanning Tree Seams for Reducing Parameterization Distortion of Triangulated Surfaces. In Proceedings of Shape Modeling International (2002). to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>673287</ref_obj_id>
				<ref_obj_pid>645908</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[SHEWCHUK, J. R. Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. In Proceedings of the First workshop on Applied Computational Geometry, Philadelphia, Pennsylvania (1996), pp. 123-133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>179026</ref_obj_id>
				<ref_obj_pid>179015</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[SIMPSON, R. B. Anisotropic Mesh Transformations and Optimal Error Control. Appl. Num. Math. 14(1-3) (1994), pp. 183-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[TRISTANO, J. R., OWEN, S. J., AND CANANN, S. A. Advancing Front Surface Mesh Generation in Parametric Space Using a Riemannian Surface Definition. In Proceedings of 7th International Meshing Roundtable, Sandia National Labs (oct 1998), pp.429-445.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Re-Tiling Polygonal Surfaces. In Proceedings of SIGGRAPH (1992), pp.55-64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[ULICHNEY, R. A. Dithering with Blue Noise. In Proceedings of the IEEE (1988), vol. 76(1), pp.56-79.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[VAN DAMME, R., AND ABOUL, L. Tight Triangulations. Mathematical Methods for Curves and Surfaces (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[VORSATZ, J., R&#214;SSL, C., KOBBELT, L., AND SEIDEL, H.-P. Feature Sensitive Remeshing. In Eurographics proceedings (sep 2001), pp. 393-401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Geometry Remeshing Pierre Alliez Mark Meyer Mathieu Desbrun Figure 1: A brief overview 
of our remeshing process: The input surface patch (top left) is .rst parameterized; Then geometric quantities 
are computed over the parameterization and stored in several 2D maps; These maps are combined to produce 
a control map, indicating the desired sampling distribution; The control map is then sampled using a 
halftoning technique, and the samples are triangulated, optimized and .nally output as a new 3D mesh. 
A few examples of the various types of meshes our system can produce are shown (top, from left to right): 
uniform, increased sampling on higher curvature, the next with a smoother gradation, regular quads, and 
semi-regular triangles. After an initial pre-processing stage (~1s), each of these meshes was produced 
in less than 2 seconds on a low-end PC. Abstract We present a novel technique, both .exible and ef.cient, 
for inter­active remeshing of irregular geometry. First, the original (arbitrary genus) mesh is substituted 
by a series of 2D maps in parameter space. Using these maps, our algorithm is then able to take advan­tage 
of established signal processing and halftoning tools that offer real-time interaction and intricate 
control. The user can easily com­bine these maps to create a control map a map which controls the sampling 
density over the surface patch. This map is then sampled at interactive rates allowing the user to easily 
design a tailored re­sampling. Once this sampling is complete, a Delaunay triangulation and fast optimization 
are performed to perfect the .nal mesh. As a result, our remeshing technique is extremely versatile and 
general, being able to produce arbitrarily complex meshes with a variety of properties including: uniformity, 
regularity, semi­regularity, curvature sensitive resampling, and feature preservation. We provide a high 
level of control over the sampling distribution allowing the user to interactively custom design the 
mesh based on their requirements thereby increasing their productivity in creating a wide variety of 
meshes. Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make 
digital or hard copies of part or all of this work for personal or classroom use is granted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for components of this work owned by others than 
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 1 Introduction As 3D geometry becomes a prevalent media, a proliferation 
of meshes are readily available, coming from a variety of sources in­cluding 3D scanners, modeling software, 
and output from computer vision algorithms. Although these meshes capture geometry accu­rately, their 
sampling quality is usually far from ideal for subse­quent applications. For instance, these (sometimes 
highly) irreg­ular meshes are not appropriate for computations using Finite El­ements, or for rapid, 
textured display on low-end computers. In­stead, meshes with nearly-equilateral triangles, a smooth gradation 
of sample density depending on curvatures, or even uniform sam­pling are preferable inputs to most existing 
geometry processing algorithms. Remeshing, i.e., modifying the sampling and connec­tivity of a geometry 
to generate a new mesh, is therefore a funda­mental step for ef.cient mesh processing. We propose a precise 
and .exible remeshing technique for arbi­trary geometry. Unlike previous techniques, we offer a high 
level of control over the sampling quality of the output mesh, as well as an unprecedented speed of execution. 
We will show that our remeshing engine can accurately generate any tailored sampling at interactive rates, 
and, if necessary, quickly optimize the quality of the resulting mesh, allowing the user to easily design 
a resampled geometry conforming to her requirements. 1.1 Background Although studied in Computer Graphics 
for obvious reasons, sur­face remeshing has also received a lot of attention from various non-CG .elds 
interested in mesh generation mainly Computational Fluid Dynamics, Finite Element Methods, and Computational 
Ge­ometry. However, the diverging goals resulted in vastly different, non-overlapping solutions as we 
now brie.y review. Mesh Generation Community Since the emphasis is gener­ally on numerical accuracy, 
most of the tools developed in the non-CG communities focus on mesh quality. Remeshing procedures often 
use a parameter space to impose quantitative mesh properties such as local triangle sizes and shapes 
[8, 37, 16]. Others sim­ply perform mesh simpli.cation [30] or edge operations and vertex shifting [2] 
to conform to a global mesh property. However, most techniques heavily rely on mesh optimization [13, 
33] to satisfy common requirements like equal angles for FE computations [3] or smooth gradation [4]; 
accuracy is therefore obtained at the price of rather slow computations. Computer Graphics Community 
In contrast to the quality requirements of the other .elds, CG work has focused mainly on ef­.ciency. 
The majority of previous work has proposed semi-regular remeshing techniques [24, 20, 21, 22], based 
on an initial phase of simpli.cation which could be used in itself for remeshing [14, 27] since it performs 
the aforementioned edge operations and vertex shifting. A noticeable body of work has also been recently 
pro­posed to accurately remesh sharp features [41, 6]. However, none of these methods can offer .exibility 
on the quality of the remesh­ing obtained, since issues such as area distortion or triangle shape distortion 
are not even considered: tailored output can only be pro­duced through extensive trial-and-error by a 
patient user. A controllable mesh re-tiling technique was proposed by Turk [38] to resample an input 
mesh using properties such as uni­ formity or curvature-based density, allowing a much more precise design 
of the output meshes. However, the algorithm requires the propagation of particles on the original mesh 
and a global relax­ation of their positions until convergence, requiring heavy computa­tion. Similarly, 
Bossen and Heckbert [5] proposed a 2D anisotropic mesh generation involving vertex insertions, vertex 
removals, and iterative relaxation. Again, output meshes conforming to various requirements can be generated 
but only after signi.cant computa­tional effort. Our goal is thus to attain accuracy, .exibility, and 
ef­.ciency for resampling, as none of the techniques described above can offer such a combination. 1.2 
Contributions &#38; Overview Our main contributions over previous remeshing techniques are in terms of 
ef.ciency as simple meshes can now be processed in real or interactive time through a novel resampling 
stage followed by an output-sensitive remeshing algorithm, and .exibility as we of­fer complete and precise 
control over the sampling rate and quality anywhere on the geometry. These two critical properties are 
ob­tained through the use of parameterization and conventional image processing tools such as .ltering, 
transfer functions and error dif­fusion, in order to compute near-optimal resamplings in a matter of 
milliseconds. Previous approaches often worked directly on the mesh, resulting in either slow performance 
or little control over the remeshing quality. The structure of this paper follows closely the overall 
algorith­mic pipeline depicted at the bottom of Figure 1. We .rst describe the atlas of parameterization 
and geometry analysis we perform on the input mesh in Section 2, in order to generate a catalog of 2D 
maps as an alternate representation for the input mesh. We detail how these resulting maps are processed 
ef.ciently using standard signal processing tools to create a near-optimal resampling of the input mesh 
in Section 3. A .nal, rapid phase of optimization can then be performed to get accurate results as described 
in Section 4. Finally, we present a number of results to demonstrate the wide range of possible resamplings 
we can interactively obtain in Sec­tion 5, before concluding in Section 6.  2 Geometry Analysis In 
this section, we explain in detail how we build a complete set of maps from the raw, input geometry. 
This will construct an alterna­tive representation of the surface and all of its intrinsic properties 
in the form of convenient 2D images, which are easy to process. We demonstrate how simple and ef.cient 
this process is when graphics hardware is used appropriately. We show how to create a small set of tiling 
patches from a closed object of arbitrary genus, then give details on how to compute the geometry maps 
from these surface patches by .attening them onto isomorphic planar triangulations. 2.1 Creation of an 
Atlas of Parameterization The .rst processing stage undergone by the input mesh consists in splitting 
the surface into disk-like patches, creating an atlas of pa­rameterization [18]. A number of existing 
clustering algorithms such as [15, 31, 26] could be used successfully to achieve such a partition. Unfortunately, 
they do not produce smooth patch bound­aries on the geometry as demonstrated in Figure 4, and therefore 
lead to poor-quality stitching across the remeshed patches. Note that one could also make some cuts in 
the geometry to turn it into a single patch, as often proposed in the last two years [23, 12, 34, 19]. 
All of these methods are valid ways to deal with arbitrary genus surfaces, and the resampling technique 
presented in this paper is mostly independent of the cutting/unfolding method chosen. In the remainder 
of this paper, we use a variant of the mesh par­titioning proposed by Eck et al. [11] (later improved 
by Guskov et al. [21]), that computes approximate Vorono¨i diagrams as an ini­tial non-smooth partitioning 
of the mesh into genus-0 patches. This procedure, which we will extend in Section 2.5 to generate area­ 
balanced patches, automatically produces a series of tiling patches from input meshes of arbitrary genus. 
 2.2 Parameterization The second stage is to map each individual surface patch to an iso­morphic planar 
triangulation. This operation, called parameteriza­tion, also has many solutions readily available ([11, 
25, 26, 9] to name a few). Although most parameterization techniques would be adequate, one that guarantees 
visual smoothness of isoparamet­ric lines and preserves the conformal structure of the input mesh is 
most preferable. We thus strongly advocate for the conformal parameterization as de.ned in [32, 11] since 
it behaves extremely well even on irregular triangulations [9]. This technique requires solving a simple, 
sparse linear system with coef.cients based on the geometry of the mesh, and is usually handled in a 
matter of sec­onds using a Conjugate Gradient solver with good preconditioning. We .x the boundary to 
be a square (see Figure 2) or any convenient rectangular region so that our maps can be ef.ciently stored 
and processed as regular .oating point images. ture mapping of a checker-board. Notice the inevitable 
area dis­tortion on the nose, which we will automatically compensate for during the resampling process 
(see Section 3.1).  2.3 Geometry Maps Once a parameterization has been found, we compute several scalar 
maps to serve as a complete substitute for the input geometry. This will allow us to work almost solely 
on the 2D images instead of on the original 3D mesh. Catalog of Maps For our application, we have identi.ed 
the following geometrical values as being relevant: . Area distortion map MA: since no discrete parameterization 
can (in general) preserve the area of every triangle, we need a piecewise constant scalar map indicating 
how each triangle has been shrunk or expanded during the parameterization. This is easily computed using 
the ratio A3D/A2D of each triangle s surface area in 3D and its corresponding area in the 2D parameterization. 
Note that this map will compensate for any area distortion inevitably introduced by the parameterization 
(as depicted in Figure 2). . Curvature maps MK and MH : since any differential quantity on a smooth surface 
can be expressed as a (possibly nonlinear) com­bination of three invariants: area A, Gaussian curvature 
K, and mean curvature H [17], we compute both a Gaussian curvature and a mean curvature map (in addition 
to the previously mentioned area distortion map). We use the discrete differential operators described 
in [28] to compute the mean and Gaussian curvatures at each vertex of the input mesh, though any reliable 
approximation of curvatures on piecewise-linear surfaces can be used. These two maps can then be combined 
to obtain other useful curvature maps: for instance, one can compute maps of minimum curvature .1, maximum 
curva­ture .2, or total curvature .21 + .22 by simple per-pixel operations on those two basic maps. Additional 
data, such as curvature tensors could also be computed on the surface and stored in maps, but we do not 
make use of them in this work; . Embedding Map Mx: we also need the position x =(x, y, z) of each vertex, 
describing the exact geometry of the surface in 3D. These three maps (one per component) will provide 
a very ef.cient way of computing the mapping between a value u =(ux,uy) on the parameterization and its 
associated 3D point on the input mesh x =(x, y, z); . Face Index Map Mindex : we also construct a face 
index map by as­signing a color to each triangle in the parameterization correspond­ing to its face index 
in the mesh, as done by Botsch et al. [7]. Such a map turns out to be ef.cient for locating in constant 
time the trian­gle in which a given parametric value lies, saving potentially costly searches. . Additional 
Maps: .nally, any attribute (normal, texture, color, etc.) can also be mapped onto the parameterization 
to complete the cat­alog of maps.  Figure 3: Examples (in inverse mode for better visualization) of 
ge­ometry maps for the mask in Figure 2. A. MH , the mean curva­ture map computed according to [28]. 
B. MA, the area map; the nose has been compressed during the .attening process, while ar­eas nearby the 
corners have been stretched. C. Sampling control map, using a per-pixel multiplication: A · B. Hardware-Assisted 
Map Generation Piecewise-constant maps representing area distortions, face indices or per-face normals 
are ef.ciently generated using hardware accelerated OpenGL com­mands. Each .oating-point or integer value 
is separated into the R, G, B, A color channels (similar to [7]), and all the triangles are ren­ dered 
using OpenGL .at shaded triangle primitives in a back buffer. We assign a depth proportional to the surface 
area of each triangle to reduce the aliasing of small triangles in the map. For linearly interpolated 
maps representing curvature, positions, per-vertex normals or attributes, we use the face index map and 
standard barycentric coordinates to compute the linear interpola­tion between the vertices in the parametric 
space. Note that the map creation could be simpli.ed and optimized even further in the near future as 
soon as graphics boards implement full 32-bit .oating point buffers for rendering (several OpenGL 2.0 
proposals already require this feature). Nonetheless, generating the maps using cur­rent graphics hardware 
speeds up the map creation by two orders of magnitude compared to a na¨ive pixel-by-pixel implementation, 
and takes less than 100 ms for large meshes with thousands of trian­gles. Figure 3 depicts both a curvature 
and an area map, as well as a compositing of the two.  2.4 Features and Constraints In addition to the 
geometry maps, we sometimes need to de.ne speci.c features and/or constraints that the user wishes to 
enforce during the remeshing process. Typically, we want sharp features (present in mechanical parts 
for instance, see top left of Figure 6) to be preserved. Similarly, some particular points of the input 
surface may need to be constrained to become vertices of the remeshed version, for animation purposes 
for example. Features We .rst assume that feature edges are either extracted using a simple dihedral 
angle thresholding, or directly input by the user by tagging existing input edges or creating arbitrary 
piecewise­linear feature curves. From this set of feature edges (Figure 6, top middle) we classify vertices 
by their number of adjacent fea­ture edges, leading to two categories: we call crease vertices any vertex 
connected to exactly two feature edges, and corner vertices all the other vertices, connected to one 
or more than two feature edges. These feature edges are then chained together into a fea­ture graph. 
This is very similar to the feature skeleton composed of backbones as introduced by Kobbelt et al.in 
a series of papers concerning geometry resampling and feature remeshing [7, 41, 6] (see Figure 6, top 
right, for an example). This feature graph re­quires little memory and can be computed in a straightforward 
way. We should note the following details that need to be addressed dur­ing the implementation: i) the 
graph can have cycles, ii) each patch boundary or cutting path is also added to the feature graph as 
a closed cycle (as being either a sharp, boundary or seaming back­bone), iii) some features may meet 
at corners living on the bound­ary, and iv) a crease vertex should be classi.ed as a corner if an important 
change of direction is detected along the feature. The latter corresponds to a feature in.exion point 
and is a rare occur­rence. Once the feature graph has been properly constructed, the speci.ed piecewise 
linear features will be exactly preserved by our remeshing technique as explained in Section 3.2. Constraints 
We also allow the user to de.ne a list of (u,v) val­ues for which (s)he desires to get corresponding 
vertices in the out­put mesh. These values can be de.ned by the user by simply click­ing on the input 
mesh. We save a list of all the constraints for later use during resampling. 2.5 Making the Atlas Area-Balanced 
As mentioned in Section 2.1, we mostly use an existing technique to construct the atlas of parameterization. 
We, however, make use of our novel maps to improve this procedure. Eck [11] proposed to smooth patch 
boundaries iteratively by mapping two adjacent patches onto a 2×1 rectangular region using the discrete 
conformal mapping discussed in Section 2.2, and then re-de.ning the bound­ ary between the two patches 
as the middle isoline in the parame­terization (see Figure 4), which guarantees smoothness. However, 
this relaxation has a major inconvenience: it is slippery since the Figure 4: Area-balanced atlas. 
From left to right: geometry of a Bunny ear; conformal parameterization and resulting area distor­tion 
visualized through a texture mapping of a checkerboard; face clustering obtained using [15]; partitioning 
obtained by simple bi­ section [11, 21]; the conformal parameterization, with the two me­ dians; area-balanced 
and smooth partitioning, using the median line of its area map MA (computed in 50 ms). parameterization 
does not have any guarantee on area distortion, the middle isoline often splits the two patches into 
patches of two very different sizes, with a tendency to slip away from very curved features. As depicted 
on Figure 4, this often leads to patches with highly variable surface areas (compare the left and right 
areas after splitting) and with large parameterization distortion (note that one patch contains the entire 
ear, while the other is relatively .at). Instead, we propose to construct the area distortion map of 
the 2×1 mapping as described in the previous section, and use it to .nd a good splitting line that creates 
equal sized patches. This is done by .nding the median vertical line such that the sum of all pixel values 
on one side of the line is equal to the sum of the pixel values on the other side. Since a single sweep 
of the picture is suf.cient to .nd the median, this operation takes little time about 50ms for a 512 
× 512 image. As demonstrated in Figure 4, this change in the original algorithm signi.cantly enhances 
the quality of the par­titioning, as no slipping occurs and each patch has the same surface area. Note 
that the dividing line is smooth thanks to the angle­preserving parameterization (i.e., a straight line 
in parametric space corresponds to a smooth line on the surface). Once the partitioning is done, we can 
compute the maps for each of the created patches as aforementioned. We use a lazy evaluation, computing 
a map only if needed to save both memory and time. We show in the next section the main contribution 
of this paper, i.e., how these maps alone are used to resample the surface geometry at interactive rate. 
  3 Realtime Geometry Resampling Now that the input geometry has been preprocessed and replaced by an 
equivalent series of maps, we can use these maps to design a proper resampling. In this section we propose 
a realtime technique to resample the geometry. This is achieved in two stages: .rst, the user designs 
a control map by combining different geometry maps to de.ne the desired density of samples; then a simple 
halftoning technique is used to discretize this map and generate the exact, re­quested number of vertices. 
We show that this resampling is near optimal, and only a quick optimization will be needed to obtain 
a high quality mesh as output. 3.1 Designing the Control Map To allow for a vast range of possible remeshings, 
we let the user de­ sign a control map that denotes the vertex density for the remeshing. Area Map as 
Sampling Space Resampling the parameteriza­tion uniformly would not result in a regular 3D resampling 
of the geometry, due to the area distortion introduced during .attening. However, the area map MA does 
indicate the density of sampling needed on the parameterization to obtain a uniform sampling on the surface 
itself. The area map is therefore the sampling space we will use as reference sampling density. Modulating 
the Sampling Density The .nal control map is obtained by multiplying the sampling space map by the impor­tance 
map a map denoting the desired sampling density across the patch. Many different maps can be used to 
tailor the sampling to the user s requirements, though we have mainly used curvature related maps in 
this work. To demonstrate the diversity of possi­ble remeshing, we mention some canonical examples of 
importance maps that we have tried: . constant, we will obtain a uniform vertex density on the 3D surface 
(see Figure 8), . related to an estimation of curvature using MK and MH , we will adapt the sampling 
rate to the local curvature (see Figure 11), . any user-de.ned map, we will obtain a map with user speci.ed 
sam­pling (useful for animation and displacement maps). See Figure 9 for such an example. The resulting 
map is then rescaled to the unit interval, and inverted (x . 1 - x) so that darker areas on the picture 
correspond to re­gions which require higher sampling. A simple example is depicted  Figure 5: Sampling 
of the map from Figure 3(C) using error diffu­ sion with various numbers of requested samples (40 ms 
each). in Figure 3(C), where the area map is modulated with a mean cur­ vature map (very light (white) 
areas correspond to .at and/or highly stretched regions of the mesh due to the .attening, and require 
few samples).  3.2 Halftoning the Control Map Once the control map has been decided upon, we need to 
resample it with a local density of vertices in accordance with the control map, and with the exact number 
of samples the user requests. In other words, we need to transform the control map into a binary image, 
indicating the presence or absence of a vertex on the parameteriza­tion. In essence, our problem is directly 
related to the technique of halftoning grey-level images. Halftoning has been carefully stud­ied for 
decades [39] and is still an active research .eld [29], mainly trying to improve the quality of dithering 
and printing. Different methods have been proposed to sample a continuous image with an adequate density, 
and to best statistically simulate an optimal blue noise signal in a single rasterization pass [39]. 
Discretizing the Control Map We use a recent error diffusion algorithm developed by Ostromoukhov [29], 
which samples an im­ age using a serpentine rasterization (left to right on even lines, right to left 
on odd lines) with near-optimal quality. We add the following modi.cations to suit our purposes: . while 
the original technique works on 8-bit images, we use 32-bit images to increase the range of densities; 
 . to avoid the well-known dead zone problem in error diffusion (large empty areas at the start of an 
error diffusion), we concatenate a vertically .ipped copy of the control map above the control map and 
perform the halftoning for the total image, retaining only the bottom half of the image as the result; 
 . we also test for features and constraints (see Section 2.4), forcing a pixel to be black if it falls 
on one of the constraints, or forcing a pixel to be white if it falls on one of the features (as they 
will be sampled separately). The error diffusion accommodates for these forced selections by diffusing 
the error into nearby pixels. The user simply chooses a given number of samples (which will be the .nal 
number of vertices) since an exact number of vertices can easily be reached by a simple linear scaling 
of the intensity of the control map [29] that preserves the ratio between the number of black pixels 
(i.e., number of samples) and the image area. Note that the size of the maps determines the maximum number 
of samples (there cannot be more samples than there are pixels in the map). Therefore, we allow the user 
to select an appropriate image size having enough space for the sampler to work properly (though the 
choice of image size can easily be made automatically if desired). Such a technique turns out to be extremely 
ef.cient: a 512×512 image is sampled in only 40 ms on a 1 GHz PIII. Examples of error diffusion are given 
in Figure 5.  Discretizing the feature graph A separate 1D error diffusion is performed along the boundaries 
and features in order to guarantee a consistent mesh density between the boundary and inner regions, 
as well as good feature preservation. After the initial sampling, we: i) gather all the pixels of the 
feature graph in a 1D array using Bresenham s line algorithm, ii) normalize their intensity according 
to the following law: x . 1 - (1 - x) (intuitively, the square root appears since if we want the fraction 
x of the samples to be v black in 2D, it means we need the fraction x of the samples to be black in any 
1D cross-section), iii) apply a 1D error diffusion, and .nally iv) put the resulting samples into the 
sampled image. This guarantees an adequate feature sampling conforming to the control map, as demonstrated 
in Figure 8. The seams across patches are dealt with similarly to ensure an easy stitching.  3.3 User 
Control Since our resampler runs at interactive rates, we can provide the user with a preview of the 
new mesh and allow for realtime edit­ing of the control map to tailor the sampling to speci.c needs. 
An extremely powerful feature of our map based technique is that we can take advantage of many well-known 
signal processing tools for images. As a consequence, we can offer a multitude of tools still with realtime 
performance; for example: o Transfer Function -Besides combinations obtained from .ltering, scaling 
and shifting of the maps, we found it particularly useful to allow editing of a general transfer function 
over the importance map, or even direct editing of the importance map itself. For in­stance, a simple 
gamma function f(x, .)= x . over the curvature map gives the user control over the sampling with respect 
to the curvature. The user can also use pass-band .lters or even a general transfer function to produce 
meshes with arbitrary sampling. No­tice that the generality of this approach allows our system to sim­ulate 
virtually any remeshing by choosing the maps and transfer functions appropriately (such as the L2-optimal 
sampling derived in [36]). o Smooth gradation [3] of the vertex density can be achieved by low pass 
.ltering of the importance map, using an optimized Gaussian .lter routine. Changes over the global size 
of the .lter kernel allow a .ne and interactive tuning of the gradation. Note that in the ideal case, 
the local size of the .lter kernel should be driven by the area map, making it a non-linear diffusion 
of the importance map. o Minimum Sampling -A guaranteed minimum density of samples can be obtained by 
shifting the intensity of the importance map so that its minimum corresponds to the requested minimum 
sampling (i.e., a minimum grey level).  Interactive Preview The error diffusion is fast enough (40 ms 
including the transfer function computation) to provide a real-time feedback of the sampling. Additionally, 
we provide the option of using the dithered map as a texture directly on the 3D origi­nal model since 
we already have the (u, v) parameterization. The samples thus appear on the mesh instantaneously, leading 
to a good preview of the current sampling.  4 Mesh Creation and Optimization At this point, we are 
already able to interactively produce a resam­pling of an input mesh with a density proven to be statistically 
in agreement with the user s request. However, connectivity has not yet been computed. Additionally, 
the halftoning implies quantized positions for the vertices. Therefore, we now explain how to gen­erate 
an initial connectivity and how a post-process optimization can greatly improve both connectivity and 
geometry in mere sec­onds. We emphasize that, contrary to [5] and most other remeshing techniques, we 
neither add, nor remove any vertex during the opti­mization since, in essence, the blue noise property 
already spreads just enough vertices everywhere. Consequently, the optimization is extremely ef.cient 
and consists of only a few edge swaps and local vertex displacements. 4.1 Mesh Creation Once the control 
map has been sampled, we perform a 2D con­strained Delaunay triangulation [1, 35] over the points sampled 
in the parametric space. Constrained edges correspond to an ordered list of points sampled using 1D error 
diffusion along backbones of the feature skeleton (see Section 2.4), as can be seen in Figure 6, bottom 
middle. The vertex coordinates are then mapped into 3D Figure 6: Simple example of features: the feature 
edges (in red) are chained together to create the feature graph; a 1D error diffu­sion is then performed 
along the graph followed by a constrained Delaunay triangulation of the whole sampling; after a constrained 
mesh optimization, the feature edges are perfectly preserved, while blended in the new mesh. using the 
face index map (see Section 2.3) and barycentric coordi­ nates within the triangle to .nd the accurate 
3D position1. The con­ strained Delaunay triangulation and the reprojection onto the orig­inal 2-manifold 
typically take a total of 200 ms for 3000 vertices generated. Notice that the connectivity generated 
by a Delaunay triangulation in the parameter plane may not be the most relevant one. However, since all 
triangulations with a given number of ver­tices are all isomorphic to each other through edge swapping, 
we use this triangulation as an initial guess , and will perform con­nectivity optimization as necessary. 
 4.2 Connectivity Optimization For a .xed set of vertices obtained by resampling, the connectiv­ity can 
be arbitrarily modi.ed by simple edge swapping. Many optimizations can be easily implemented (see, for 
instance, tight­est triangulation [40], minimum curvature [10]). We also used the following two simpler 
criteria: Regularity Edge swaps can be performed in order to favor va­lence 6 for interior vertices, 
and valence 4 on boundary vertices. This is implemented by randomly picking a non-feature edge and performing 
an edge swap only if it reduces the valence dispersion. A few additional constraints can be added in 
order to prevent face .ipping in the parameterization, or large geometric distortions for instance. Note 
that we can also balance the valences on both sides of each inner backbone. The rib effect [6] can therefore 
be ob­ tained by forcing exactly two neighbors on each side of a sharp edge whenever possible, as demonstrated 
in Figure 8. Face Aspect Ratio Similarly, edge swaps can be performed to improve the aspect ratio of 
the triangles. In practice, we swap an edge between two triangles if it improves their surface area/perimeter2 
ratio (computed in 3D). This simple test often re­sults in dramatic improvements, since the connectivity 
is now de­pendent on the embedding, and not solely on the parameterization. 4.3 Geometry Optimization 
In addition to the connectivity optimization, we also perform a small geometry optimization to improve 
the geometric quality of the mesh. We perform a weighted Laplacian .ow in the parameter­ization by moving 
every vertex p that does not belong to the feature p graph by: .p =.twi(qi - p) i.N(p) where .t is a 
step chosen suf.ciently small (e.g. 0.1), N (p) is the set of adjacent vertex indices to vertex p, and 
qi corresponds to the ith adjacent vertex to p. 1Although using Mx would be faster, it is usually not 
accurate enough for small maps, and could therefore result in small noise in the reprojection. Figure 
7: Top: Left, Delaunay triangulation over the sampling. Right, after connectivity and geometry optimization. 
Middle, com­parison of valence dispersion. Bottom: Left, Delaunay triangula­tion of a sampling performed 
upon the area map (leading to uni­form mesh) of a mushroom-shape model. Middle, after minimiza­tion of 
local area dispersion. Right, the remeshed model. Note the uniformity obtained despite the strong area 
distortion due to the .attening process. Depending on the choice of remeshing that the user made when 
selecting the control map, we perform an adequate optimization by choosing the weights wi so as to minimize 
an appropriate quan­tity. For example, if the users require a uniformly resampled mesh, we can minimize 
the local area disper­sion by using the following weighting: 3D 3D (A·cot(ai)+A·cot(ßi)) ii-1 wi = . 
n , j=1 A3jD where ai and ßi are the opposite angles in the parameterization as depicted, and A3D i and 
A3D i-1 are the 3D face areas to the left and right of pqi. This novel weighting has the quality of 
inducing no changes if the triangles are already of equal sizes, while producing a Laplacian smoothing 
([28]) to iteratively improve the quality otherwise. The result of such an optimization can be seen in 
the bottom of Figure 8 for instance. The area distortion minimization is only a particular instance of 
the more general mesh optimization we offer. The area terms in the previous weights can be substituted 
by other values, based on the control map used. For a curvature-based map for in­stance, we replace the 
area terms by integrals of the control map over the associated triangles. Indeed, a single pass over 
the control map suf.ces to collect the integral of the map over each triangle. These integrals, measuring 
the amount of curvature (or amount of anything the control map measures) contained in a triangle, are 
therefore appropriate weighting values if one wants to guarantee a triangulation adapted to the control 
map. This ef.cient smoothing generally happens in a matter of seconds, leading for instance to the results 
on Figure 11. 4.4 Combined Optimization Our system can create a variety of optimizations by alternating 
be­tween connectivity and geometry optimization stages. For instance, uniform meshes can be obtained 
by alternating edge swaps favoring regularity with geometry optimization iterations minimizing area dispersion 
(see Figure 7 bottom). If the user wishes to create the rib effect [6], she can simply alternate edge 
swaps which favor regularity and a univariate Laplacian smoothing of the feature ver­tices (Figure 6, 
bottom right). Additional results are given in the following section.  5 Remeshing Results Our current 
implementation is written in C++ using a sparse ma­trix structure, biconjugate gradient and SSOR preconditioning 
for computing the conformal parameterization. All operations on the maps are performed using a standard 
image processing library, us­ing OpenGL hardware whenever possible. The sampling previews use standard 
OpenGL texture mapping. All result timings are given for a 1 GHz PIII with 256 MBytes of memory. Figure 
8 illus­ trates uniform remeshing of the fandisk at various resolutions us­ing a 800 × 800 control map. 
Note how the 1D error diffusion performs well all the way from 200 vertices to higher complex­ity along 
the backbones of the feature skeleton. The conformal mapping is performed in 3.1s using SSOR with over-relaxation, 
and all the maps are computed in 1.2s total, while each sampling is done at an interactive rate in 160ms. 
For the 2.5k vertex ver­sion the constrained Delaunay triangulation [35] takes 190ms, and the optimization 
stage takes 5s overall. The .nal 3D mapping takes 250ms. Note that our goal of sampling at interactive 
rates is achieved, greatly increasing the user s productivity and work.ow. Figure 8: Uniform remeshing 
of the fandisk. Top: conformal pa­rameterization, and sampling obtained by error diffusion with 2.5k 
vertices with superimposed feature skeleton. Middle: result of con­strained Delaunay triangulation before 
and after uniformity opti­mization. Bottom: several uniform remeshings with 0.2, 0.6, 1.4, 2.5 and 50k 
vertices respectively. Note the excellent behavior of the 1D error diffusion along the backbones, leading 
to consistent density between sharp edges and planar areas. Figure 9: Left: Semi-regular remeshing of 
a foot model. Right: Mesh created by pasting an image on the importance map (useful for animations and 
displacement maps). Figure 10 illustrates an example of uniform geometry remeshing of the MaxPlanck model 
using a 3 patch atlas. The original mesh (23kV) is uniformly remeshed to the requested 8.3kV. Notice 
that the .nal, remeshed model shows no signs that it was created us­ing 3 independent patches. In Figure 
11 the MaxPlanck model is remeshed with various transfer functions over the curvature map. The .rst example 
is uniform (i.e., .at transfer function) with 15kV, and the three following examples are generated using 
a progres­sively increasing gamma function over the curvature map. All inter­mediate meshes ranging from 
uniform to adapted sampling can be obtained easily just by increasing the gamma or other custom trans­fer 
functions. Figure 9(left) shows a semi-regular remeshing of the foot model by applying regular subdivision 
in parametric space over a uniform base mesh. Figure 9(right) illustrates a custom-tailored sampling. 
 6 Conclusions and future work We have demonstrated a novel, versatile technique for interactive geometry 
resampling that allows a very .ne and easy control over the desired quality of the mesh. We substitute 
the original geome­try by one or more 2D maps on which numerous operations such as halftoning and integration 
can be performed in real-time. Once an initial, near-optimal resampling has been designed, a fast optimiza­tion 
is performed to perfect the resulting mesh. We allow the user to custom design the mesh based on their 
requirements at interactive rates thereby increasing their productivity in creating a wide variety of 
meshes. Many additional features can be added to our framework. We are investigating error diffusion 
in a quadtree data structure (to avoid the possibly large memory requirement of our current approach), 
anisotropic remeshing (possibly using ellipse packing) on a tensor control map of the principal curvatures, 
and hierarchical solving to accelerate the possibly slow parameterization stage. Finally, we plan to 
use our remeshing engine for other projects such as com­pression (how to remesh a surface to obtain the 
best rate/distortion tradeoff), as well as better geometric approximation. Acknowledgements The work 
reported here was supported in part by the IMSC NSF Engineering Research Center (EEC-9529152), by the 
ECG project of the EU No IST-2000-26473, and by a NSF CAREER award (CCR-0133983). Special thanks to Victor 
Ostromoukhov for sharing his insights about error diffusion techniques, Yiying Tong for coding help with 
matrix preconditioning, and the SIGdraft reviewers for helpful comments. Models are courtesy of Hugues 
Hoppe, Leif Kobbelt and Peter Schr¨oder.  References [1] www.cgal.org: Computational Geometry Algorithms 
Library. [2] BOROUCHAKI, H. Geometric Surface Mesh. In 2nd International Con­ference on Integrated and 
Manufacturing in Mechanical Engineering (may 1998), pp.343 350. [3] BOROUCHAKI, H., GEORGE, P. L., HECHT, 
F., LAUG, P., AND SALTEL, E. Delaunay Mesh Generation Governed by Metric Speci.cations. Finite Elements 
in Analysis and Design 25 (1997), pp.61 83. [4] BOROUCHAKI, H., HECHT, F., AND FREY, P. J. Mesh Gradation 
Con­trol. In Proceedings of 6th International Meshing Roundtable, Sandia National Labs (oct 1997), pp.131 
141. [5] BOSSEN, F., AND HECKBERT, P. A Pliant Method for Anisotropic Mesh Generation. In 5th Intl. Meshing 
Roundtable (oct 1996), pp.63 76. [6] BOTSCH, M., AND KOBBELT, L. Resampling Feature and Blend Regions 
in Polygonal Meshes for Surface Anti-Aliasing. In Eurographics pro­ceedings (sep 2001), pp.402 410. [7] 
BOTSCH, M., R ¨ OSSL, C., AND KOBBELT, L. Feature Sensitive Sampling for Interactive Remeshing. In Vision, 
Modeling and Visualization pro­ ceedings (2000), pp.129 136. [8] DE COUGNY, H. L., AND SHEPHARD, M. S. 
Surface Meshing Us­ing Vertex Insertion. In Proceedings of 5th International Meshing Roundtable, Sandia 
National Labs (oct 1996), pp.243 256. [9] DESBRUN, M., MEYER, M., AND ALLIEZ, P. Intrinsic parameterizations 
of surface meshes. In Proceedings of Eurographics (2002). [10] DYN, N., HORMANN, K., S.-J. KIM, AND LEVIN, 
D. Optimizing 3D Trian­gulations using Discrete Curvature Analysis. Mathematical methods for curves and 
surfaces, Oslo 2000 (2001), pp.135 146. [11] ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, 
M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In Pro­ceedings of SIGGRAPH (1995), 
pp.173 182. [12] ERIKSON, J., AND HAR-PELED, S. Optimally cutting a surface into a disk. In Proceedings 
of the 18th Annual ACM Symposium on Compu­tational Geometry (2002). to appear. [13] FREY, P. J. About 
Surface Remeshing. In Proceedings of the 9th Int. Meshing Roundtable (2000), pp.123 136. [14] GARLAND, 
M., AND HECKBERT, P. Simplifying Surfaces with Color and Texture using Quadric Error Metrics. In IEEE 
Visualization Confer­ence Proceedings (1998), pp.263 269. [15] GARLAND, M., WILLMOTT, A., AND HECKBERT, 
P. Hierarchical Face Clustering on Polygonal Surfaces. In ACM Symposium on Interactive 3D Graphics (2001). 
[16] GEORGE, P. L., AND BOROUCHAKI, H., Eds. Delaunay Triangulation and Meshing Application to Finite 
Elements. HERMES, Paris, 1998. [17] GRAY, A., Ed. Modern Differential Geometry of Curves and Surfaces. 
Second edition. CRC Press, 1998. [18] GRIMM, C. M., AND HUGHES, J. F. Modeling Surfaces of Arbitrary 
Topology using Manifolds. In Proceedings of SIGGRAPH (1995), pp.359 368. [19] GU, X., GORTLER, S., AND 
HOPPE, H. Geometry Images. In Proceed­ings of SIGGRAPH (2002). [20] GUSKOV, I., SWELDENS, W., AND SCHR 
¨ Multiresolution Sig-ODER, P. nal Processing for Meshes. In Proceedings of SIGGRAPH (1999), pp.325 334. 
[21] GUSKOV, I., VIDIMCE, K., SWELDENS, W., AND SCHR ¨ NormalODER, P. Meshes. In Proceedings of SIGGRAPH 
(2000), pp.95 102. [22] HORMANN, K., LABSIK, U., AND GREINER, G. Remeshing Triangulated Surfaces with 
Optimal Parameterizations. Computer-Aided Design 33 (2001), pp.779 788. [23] LAZARUS, F., POCCHIOLA, 
M., VEGTER, G., AND VERROUST, A. Com­puting a Canonical Polygonal Schema of an Orientable Triangulated 
Surface. In Proceedings of 17th Annu. ACM Sympos. Comput. Geom. (2001), pp.80 89. [24] LEE, A. W. F., 
SWELDENS, W., SCHR ¨ODER, P., COWSAR, L., AND DOBKIN, D. MAPS: Multiresolution Adaptive Parameterization 
of Surfaces. In Proceedings of SIGGRAPH (1998), pp.95 104. [25] LEVY, B. In´ Constrained Texture Mapping 
for Polygonal Meshes. Proceedings of SIGGRAPH (2001), pp.417 424. ´ formal Maps for Automatic Texture 
Atlas Generation. In Proceedings of SIGGRAPH (2002). [26] LEVY, B., PETITJEAN, S., RAY, N., AND MAILLOT, 
J. Least Squares Con­ [27] LINDSTROM, P., AND TURK, G. Fast and Memory Ef.cient Polygonal Simpli.cation. 
In IEEE Visualization Proceedings (1998), pp. 279 286. [28] MEYER, M., DESBRUN, M., SCHR ¨ DiscreteODER, 
P., AND BARR, A. H. Differential-Geometry Operators for Triangulated 2-Manifolds, 2002. http://multires.caltech.edu/pubs/diffGeoOps.pdf. 
[29] OSTROMOUKHOV, V. A Simple and Ef.cient Error-Diffusion Algo­rithm. In Proceedings of SIGGRAPH (2001), 
pp.567 572.  ´ surements. Computer-Aided Design 29(4) (1997), pp.287 298. [30] P. V ERON, J.-C. L. 
Static Polyhedron Simpli.cation using Error Mea­ [31] PAULY, M., AND GROSS, M. Spectral Processing of 
Point-Sampled Ge­ometry. In Proceedings of SIGGRAPH (2001), pp.379 386. [32] PINKALL, U., AND POLTHIER, 
K. Computing Discrete Minimal Surfaces and Conjugates. Experimental Mathematics 2(1) (1993), pp.15 36. 
[33] RASSINEUX, A., VILLON, P., SAVIGNAT, J.-M., AND STAB, O. Surface Remeshing by Local Hermite Diffuse 
Interpolation. International Journal for numerical methods in Engineering 49 (2000), pp.31 49. [34] SHEFFER, 
A. Spanning Tree Seams for Reducing Parameterization Dis­tortion of Triangulated Surfaces. In Proceedings 
of Shape Modeling International (2002). to appear. [35] SHEWCHUK, J. R. Triangle: Engineering a 2D Quality 
Mesh Gener­ator and Delaunay Triangulator. In Proceedings of the First work­shop on Applied Computational 
Geometry, Philadelphia, Pennsylva­nia (1996), pp.123 133. [36] SIMPSON, R. B. Anisotropic Mesh Transformations 
and Optimal Error Control. Appl. Num. Math. 14(1-3) (1994), pp.183 198. [37] TRISTANO, J. R., OWEN, S. 
J., AND CANANN, S. A. Advancing Front Surface Mesh Generation in Parametric Space Using a Riemannian 
Surface De.nition. In Proceedings of 7th International Meshing Roundtable, Sandia National Labs (oct 
1998), pp.429 445. [38] TURK, G. Re-Tiling Polygonal Surfaces. In Proceedings of SIG-GRAPH (1992), pp.55 
64. [39] ULICHNEY, R. A. Dithering with Blue Noise. In Proceedings of the IEEE (1988), vol. 76(1), pp.56 
79. [40] VAN DAMME, R., AND ABOUL, L. Tight Triangulations. Mathematical Methods for Curves and Surfaces 
(1995). [41] VORSATZ, J., R ¨ OSSL, C., KOBBELT, L., AND SEIDEL, H.-P. Feature Sen­sitive Remeshing. 
In Eurographics proceedings (sep 2001), pp.393 401.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566589</article_id>
		<sort_key>355</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Geometry images]]></title>
		<page_from>355</page_from>
		<page_to>361</page_to>
		<doi_number>10.1145/566570.566589</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566589</url>
		<abstract>
			<par><![CDATA[Surface geometry is often modeled with irregular triangle meshes. The process of remeshing refers to approximating such geometry using a mesh with (semi)-regular connectivity, which has advantages for many graphics applications. However, current techniques for remeshing arbitrary surfaces create only <i>semi-regular</i> meshes. The original mesh is typically decomposed into a set of disk-like charts, onto which the geometry is parametrized and sampled. In this paper, we propose to remesh an arbitrary surface onto a <i>completely regular</i> structure we call a <i>geometry image.</i> It captures geometry as a simple 2D array of quantized points. Surface signals like normals and colors are stored in similar 2D arrays using the same implicit surface parametrization --- texture coordinates are absent. To create a geometry image, we cut an arbitrary mesh along a network of edge paths, and parametrize the resulting single chart onto a square. Geometry images can be encoded using traditional image compression algorithms, such as wavelet-based coders.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[remeshing]]></kw>
			<kw><![CDATA[surface parametrization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP77028616</person_id>
				<author_profile_id><![CDATA[81455605505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xianfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269895</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DAVIS, G. Wavelet Image Compression Construction Kit. http://www.geoffdavis.net/dartmouth/wavelet/wavelet.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DEY, T. K., AND SCHIPPER, H. A new technique to compute polygonal schema for 2-manifolds with application to null-homotopy detection. Discrete and Computational Geometry 14 (1995), 93-110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. In SIGGRAPH 95, pp. 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>513430</ref_obj_id>
				<ref_obj_pid>513400</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ERICKSON, J., AND HAR-PELED, S. Cutting a surface into a disk. ACM SoCG 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134031</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FERGUSON, H., ROCKWOOD, A., AND COX, J. Topological design of sculptured surfaces. In SIGGRAPH 92, pp. 149-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248308</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FLOATER, M. Parametrization and smooth approximation of surface triangulations. CAGD 14, 3 (1997), 231-250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344831</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GUSKOV, I., VIDIMCE, K., SWELDENS, W., AND SCHR&#214;DER, P. Normal Meshes. In SIGGRAPH 2000, pp. 95-102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614459</ref_obj_id>
				<ref_obj_pid>614278</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HAKER, S., ANGENENT, S., TANNENBAUM, A., KIKINIS, R., SAPIRO, G., AND HALLE, M. Conformal Surface Parameterization for Texture Mapping. IEEE TVCG 6, 2 (2000), 181-189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Progressive Meshes. In SIGGRAPH 96, pp. 99-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344922</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[KHODAKOVSKY, A., SCHR&#214;DER, P., AND SWELDENS, W. Progressive Geometry Compression. In SIGGRAPH 2000, pp. 271-278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378630</ref_obj_id>
				<ref_obj_pid>378583</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[LAZARUS, F., POCCHIOLA, M., VEGTER, G., AND VERROUST, A. Computing a Canonical Polygonal Schema of an Orientable Triangulated Surface. In ACM SoCG 2001, pp. 80-89.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344829</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LEE, A., MORETON, H., AND HOPPE, H. Displaced Subdivision Surfaces. In SIGGRAPH 2000, pp. 85-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LEE, A., SWELDENS, W., SCHR&#214;DER, P., COWSAR, L., AND DOBKIN, D. MAPS: Multiresolution Adaptive Parameterization of Surfaces. In SIGGRAPH 98, pp. 95-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237750</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M., DEROSE, T., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. ACM TOG 16, 1 (January 1997), 34-73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MUNKRES, J. Topology. Prentice Hall, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344990</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PIPONI, D., AND BORSHUKOV, G. D. Seamless Texture Mapping of Subdivision Surfaces by Model Pelting and Texture Blending. In SIGGRAPH 2000, pp. 471-478.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581909</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[SANDER, P., GORTLER, S., SNYDER, J., AND HOPPE, H. Signal-Specialized Parametrization. Microsoft Research MSR-TR-2002-27 (January 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383307</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SANDER, P., SNYDER, J., GORTLER, S., AND HOPPE, H. Texture Mapping Progressive Meshes. In SIGGRAPH 2001, pp. 409-416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>884118</ref_obj_id>
				<ref_obj_pid>882487</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SHEFFER, A. Spanning Tree Seams for Reducing Parameterization Distortion of Triangulated Surfaces. Shape Modelling International (2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G., AND ROSSIGNAC, J. Geometric compression through topological surgery. ACM TOG 17, 2 (1998), 84-115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>98546</ref_obj_id>
				<ref_obj_pid>98524</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[VEGTER, G., AND YAP, C. K. Computational complexity of combinatorial surfaces. In ACM SoCG 1990, pp. 102-111.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[VORSATZ, J., R&#214;SSL, C., KOBBELT, L., AND SEIDEL, H.-P. Feature Sensitive Remeshing. Computer Graphics Forum 20, 3 (2001), 393-401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[WOOD, Z., HOPPE, H., DESBRUN, M., AND SCHR&#214;DER, P. Isosurface topology simplification. Microsoft Research MSR-TR-2002-28 (January 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258863</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., SCHR&#214;DER, P., AND SWELDENS, W. Interactive multiresolution mesh editing. In SIGGRAPH 97, pp. 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Geometry Images Xianfeng Gu Steven J. Gortler Hugues Hoppe Harvard University Harvard University Microsoft 
Research Abstract Surface geometry is often modeled with irregular triangle meshes. The process of remeshing 
refers to approximating such geometry using a mesh with (semi)-regular connectivity, which has advan­tages 
for many graphics applications. However, current techniques for remeshing arbitrary surfaces create only 
semi-regular meshes. The original mesh is typically decomposed into a set of disk-like charts, onto which 
the geometry is parametrized and sampled. In this paper, we propose to remesh an arbitrary surface onto 
a com­pletely regular structure we call a geometry image. It captures ge­ometry as a simple 2D array 
of quantized points. Surface signals like normals and colors are stored in similar 2D arrays using the 
same implicit surface parametrization texture coordinates are ab­sent. To create a geometry image, we 
cut an arbitrary mesh along a network of edge paths, and parametrize the resulting single chart onto 
a square. Geometry images can be encoded using traditional image compression algorithms, such as wavelet-based 
coders. Keywords: remeshing, surface parametrization. 1 INTRODUCTION Surface geometry is often modeled 
with irregular triangle meshes. The process of remeshing refers to approximating such geometry using 
a mesh with (semi)-regular connectivity (e.g. [3, 13]). Resampling geometry onto a regular structure 
offers a number of bene.ts. Compression is improved since the connectivity of the samples is implicit. 
Moreover, remeshing can reduce the non-uniformity of the geometric samples in the tangential surface 
directions, thus reducing overall entropy [10]. The regularity of sample neighborhoods helps in applying 
signal­processing operations and in creating hierarchical representations for multiresolution viewing 
and editing [14, 24]. However, current techniques for remeshing arbitrary surfaces create only semi-regular 
meshes. The original mesh is typically decomposed into a set of disk-like charts, onto which the geometry 
is parametrized and sampled. Although the sampling on each chart follows regular subdivision, the chart 
domains form an irregular network over the surface. This irregular domain network complicates processing, 
particularly for operations that require accessing data across neighboring charts. In contrast, texture 
data is typically represented in a completely regular fashion, as a (possibly compressed) 2D array of 
[r, g, b] values. This distinction, among others, causes geometry and textures to be treated and represented 
quite differently by current graphics hardware. Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 In this paper, we propose to remesh an arbitrary surface 
onto a completely regular structure we call a geometry image. It cap­tures geometry as a simple n × n 
array of [x, y, z] values. Other surface attributes, such as normals and colors, are stored as addi­tional 
square images, sharing the same do­main as the geometry. Because the geome­try and attributes share the 
same parametriza­tion, the parametrization itself is implicit texture coordinates are absent. Moreover, 
this parametrization fully utilizes the texture domain (with no wasted space). Geometry images can be 
encoded using traditional image compression algorithm, such as wavelet­based coders. Also, geometry images 
are ideally suited for hard­ware rendering. They may be transmitted to the graphics pipeline in a compressed 
form just like texture images. And, they eliminate expensive pointer-based structures such as indexed 
vertex lists. Of course, arbitrary surfaces cannot generally be mapped directly onto a square image domain, 
because their topology can differ from that of a disk. The basic idea in our approach is to slice open 
the mesh along an appropriate set of cut paths, to allow the unfolding of the mesh onto a disk-like surface. 
The vertices and edges along the cut paths are represented redundantly (typically twice) along the boundary 
of this disk. Next, we parametrize this cut surface onto the square domain of the image, and sample the 
geometry at the 2D grid samples. Representing surfaces as geometry images presents challenges: A cut 
must be found that opens the mesh into a topological disk, and that also permits a good parametrization 
of the surface within this disk. We describe an effective, automatic method for cutting arbitrary 2-manifold 
meshes (possibly with boundaries).  The image boundary must be parametrized such that the reconstructed 
surface matches exactly along the cut, to avoid cracks. Traditional texture mapping is more forgiving 
in this respect, in that color discontinuities at boundaries are less noticeable.  The parametrization 
must evenly distribute image samples over the surface, since undersampling would lead to geometric blurring. 
We do not make a technical contribution in this area, but simply apply the geometric-stretch parametrization 
of [18, 17].  Straightforward lossy compression of the geometry image may introduce tears along the 
surface cut. We allow fusing of the cut by encoding the cut topology as a small data sideband.  Geometry 
images have the following limitations: They cannot represent non-manifold geometry.  Unwrapping an 
entire mesh as a single chart can create parametrizations with greater distortion and less uniform sam­pling 
than can be achieved with multiple local charts, particu­larly for surfaces of high genus.  In this 
paper, we describe an automatic system for converting arbitrary meshes into geometry images and associated 
attribute maps (Figure 1). We demonstrate that they form a practical and elegant representation for a 
variety of graphical models (Figure 7).  (a) Original mesh with cut (b) Geometry image 257×257 (c) 
Geometry reconstructed (d) Geometry reconstructed 70K faces; genus 0 (b*) Compr. to 1.5KB (not shown) 
entirely from b entirely from b* (e) Geometry of d topology-fused (f) Normal-map image 512×512 (g) Geometry 
of c (h) Geometry of e using sideband data (f*) Compr. to 24KB (not shown) normal-mapped using f normal-mapped 
using f* Figure 1: Creation, compression, and rendering of a geometry image. Images b* and f* (not shown) 
are compressed using an image wavelet­coder. Geometry image is 12-bit [x, y, z] visualized as [r, g, 
b]. Normal-map image is 8-bit [nx, ny, nz] visualized as [r, g, b]. 2 PREVIOUS WORK There exist several 
schemes for semi-regular remeshing of arbitrary surfaces. Eck et al. [3] achieve remeshing by cutting 
a mesh into multiple charts using a Voronoi-like decomposition. Each chart is parametrized using a harmonic 
map, sampled using a regular triangular subdivision pattern, and compressed using a triangular wavelet 
construction [14]. Khodakovsky et al. [10] use the MAPS scheme [13] to partition the mesh into charts 
and create the chart parametrizations. They obtain impressive compression results using zero-tree coding 
of local-frame wavelet coef.cients. Lee et al. [12] create a multi-chart domain using mesh simpli.cation. 
They de.ne a subdivision surface over this domain and .t it to the original surface. The .t residual 
is expressed as a semi-regular scalar displacement map over the smooth subdivision surface. Guskov el 
al. [7] use a MAPS-like approach to create multiple charts. These charts are recursively subdivided, 
and newly introduced vertices are expressed using displacements from the previous mesh, mostly as scalar 
displacements. In our setting, previous semi-regular remeshing approaches can be viewed as representing 
a surface as a collection of abutting geometry images. The crux of our contribution is to represent the 
entire surface as a single geometry image, by cutting the surface and sampling it using a completely 
regular quad grid. We optimize the creation of the cut to allow for a good parametrization.  3 CREATION 
OF GEOMETRY IMAGES From a 2-manifold triangle mesh M, we create a geometry image consisting of an n × 
n array of [x, y, z] data values. If we plan to render using normal mapping, we also create another 2D 
array of normal values [nx, ny, nz]. (See Figure 1.) Our approach is to cut the mesh M to form a new 
mesh Ml that has the topology of a disk (Figure 2). The cut p is speci.ed as a set of edges in M. To 
create Ml, we split each non-boundary edge in p into two boundary edges to form the opened cut pl. This 
directed loop of edges pl is the boundary of Ml. We say that two edges in pl are mates if they result 
from the splitting of an edge in p. A vertex v with valence k in p is replicated as k vertices in pl. 
Vertices in p that have valence k = 2 in the cut are called cut-nodes. (We still refer to these as cut-nodes 
when replicated in pl.) A cut­path is the set of boundary edges and vertices between two ordered cut-nodes 
in the loop pl. Each cut-path has a mate de.ned by the mates of its edges (unless its edges were boundary 
edges in p). Let D be the domain unit square for the geometry image. The parametrization 1 is a piecewise 
linear map from the unit square D to Ml, de.ned by associating domain coordinates (s, t) with each mesh 
vertex in Ml. The domain D has a rectilinear n×n grid, where grid points have coordinates (i/(n-1), j/(n-1)) 
with i, j =0.. n-1. We evaluate 1 at the grid points to sample the mesh geometry, as well as any other 
surface attributes (e.g. color, skinning weights, radiance transfer coef.cients). The geometry image 
samples are used to reconstruct an approxi­mation of M. In this work, we use linear basis functions (triangles) 
to de.ne the reconstruction interpolant for geometry. Our goal is to .nd a good cut p and parametrization 
1, such that this reconstruc­tion is a good approximation of M for moderate sampling rates. Approach 
overview Our strategy for .nding a good cut p and parametrization 1 is as follows. We .rst .nd a topologically 
suf.cient cut, and create an initial parametrization using this cut. We use information from the parametrization 
to improve the cut, and reparametrize based on the new cut. This process of cutting and reparametrizing 
is iterated until the parametrization no longer improves. To aid in the exposition, we .rst describe 
how a parametrization is found given any cut p (Section 3.1). We then describe how the space of cuts 
is explored (Section 3.2). 3.1 Parametrization For now, assume that we are given a cut p. To create a 
parametrization, we .rst .x a mapping between the opened cut p l and the boundary of the unit square 
D. Next, we solve for a map of Ml onto D that is consistent with these boundary conditions. We now describe 
these two steps in more detail. Boundary parametrization In order to avoid cracks in the reconstructed 
geometry, it is necessary that each cut-node in p l be exactly sampled in the remesh. This implies that 
we must map cut­nodes to grid points on the boundary of D. (Other vertices in p l are not constrained 
to lie on grid points.) In addition, cut-path mates must be sampled at identical surface points to avoid 
cracks, which requires that cut-path mates be allocated the same length on the boundary of D. To accomplish 
this, we allocate for each cut-path an amount of the boundary proportional to its length in p l . This 
allocation is then rounded to an integer multiple of 1/(n - 1). If due to rounding we have over-or under-allocated 
the boundary, we redistribute the residual to the various cut-paths in units of 1/(n-1), making sure 
to treat cut-path mates identically. Note that an n×n geometry image can represent a surface with genus 
at most n. To avoid degeneracies, we must enforce two more constraints. First, no triangle in Ml can 
have all its three vertices mapped to one of the four sides of the square, for it would become parametrically 
degenerate. If such a triangle arises, we split the triangle by introducing new vertices at the midpoints 
of its non-boundary edge(s), and split neighboring triangles so as to avoid T-junctions. Second, as we 
lay out p l along the boundary of D we must break any edge that spans one of the four corners of D. Otherwise 
a single boundary edge in Ml would map to an L shape in D. The edge is broken by introducing a vertex 
at the domain corner, thus splitting its adjacent triangle into two. To enforce topological consistency 
across the cut, the same procedure is applied to its mate edge. Finally, we .nd that placing a valence-1 
cut-node at a corner of D results in poor geometric behavior, so if this occurs we rotate the boundary 
parametrization. Interior parametrization Having .xed the boundary of the parametrization, we now solve 
for its interior. When creating a parametrization, there are numerous metrics that can be used to measure 
its quality, e.g. [3, 6, 8, 13, 16]. For our application, an ideal metric would be some measure of surface 
accuracy after sampling and reconstruction. As shown by the analysis in [17], the L2 geometric-stretch 
metric introduced in [18] is in fact an approximation of this ideal measure. Geometric stretch measures 
the amount of spacing that occurs on the surface when the parameter domain is uniformly sampled. Thus, 
minimizing geometric stretch tends to uniformly distribute samples on the surface. In [17], the stretch 
metric is shown to be related to signal-approximation error (SAE) the difference between a signal de.ned 
on the surface and its reconstruction from a discrete grid sampling. Speci.cally, the stretch metric 
corresponds to the .rst-order Taylor expansion of SAE under the assumption of locally constant reconstruction. 
In our context, the signal is the geometry itself, and therefore geometric stretch can be seen as a predictor 
of geometric reconstruction error. In Section 5, we show the advantage of using a geometric-stretch parametrization 
over the Floater shape-preserving parametrization. We compute a geometric-stretch parametrization using 
the hierarchical optimization algorithm described in [17]. First, the interior of Ml is simpli.ed to 
form a progressive mesh representation [9]. The few interior vertices in the resulting base mesh are 
optimized within D by brute-force. Then, we apply vertex splits from the progressive mesh to successively 
re.ne the mesh. For each inserted vertex, we optimize the parametrization of its neighborhood to minimize 
stretch using a local, non-linear optimization algorithm.  3.2 Cutting We now describe how we automatically 
.nd a good cut p for M. Starting with a surface of arbitrary genus, we .rst .nd an initial cut that opens 
M into a disk. Given the resulting topological disk, we use a novel algorithm to augment the cut in order 
to improve the subsequent parametrization and reconstruction quality. Initial cut It is well known that 
any closed surface can be opened into a topological disk (called a polygonal schema) by cutting along 
an appropriate set of edges [15]. Such a cut was used in [5] as part of a geometric modeling system for 
creating smooth surfaces. Piponi and Borshukov [16] describe an interactive system allowing a user to 
manually cut a genus-zero manifold into a single chart using a tree of edge cuts. The computational complexity 
of optimally cutting a mesh of arbitrary genus into a disk is studied in [4]. Algorithms for .nding special 
kinds of cuts (those that form reduced and canonically reduced polygonal schemata) are described in [2, 
11, 21]. Our algorithm, which is most similar to that of [2], works as follows. If the mesh has boundaries, 
let B be the set of original boundary edges. This set remains frozen throughout the algorithm, and is 
always a subset of the .nal cut p. After removing a single seed triangle from the mesh, we apply two 
phases. In the .rst phase we repeatedly identify an edge e . B adjacent to exactly one triangle, and 
remove both the edge and the triangle. Note that the two remaining edges of the triangle are left in 
the simplicial complex, even if they are dangling. In order to obtain a result of minimal radius , we 
order triangle removals according to their geodesic distance from the seed triangle. When this .rst phase 
terminates, we have removed a topological disk that includes all of the faces of the mesh. Thus, the 
remaining vertices (which is in fact all of them), and the remaining edges must form a topological cut 
p of M. At this point, p consists of a set of connected loops along with some unnecessary trees of edges 
(and is similar to the construction of [20]). In a second phase, we repeatedly identify a vertex adjacent 
to exactly one edge (i.e. a dangling edge), and remove both the vertex and the edge. This second phase 
terminates when all the edge trees have been trimmed away, leaving just the connected loops. Since the 
resulting cut p may be serrated (it is not made up of shortest paths), we straighten each cut-path in 
p by computing a constrained shortest path that connects its two adjacent cut-nodes and stays within 
a neighborhood of the original cut-path. For the case of a closed mesh of genus 0, the resulting p will 
consist of a single vertex, since it has no loops. Because our parametrization requires that we map p 
l onto a square, we add back to p two adjacent mesh edges. Iterated cut augmentation Through experiment, 
we have found that to obtain ef.cient geometry images, it is important for p to pass through the various 
extrema of M. For example, in the hand model a good cut should pass through its .ve .ngers (see Figure 
2). Therefore our goal is to .nd these extrema and augment the cut so that it passes through them. A 
similar subproblem is investigated by Sheffer [19], who classi.es extrema as vertices with high (discrete) 
curvature. Unfortunately, this type of local method will not be able to .nd protrusions with widely distributed 
curvature. Our approach to .nding extrema is to search for mesh re­gions that behave poorly (have large 
geometric stretch) under a parametrization using the current cut. Speci.cally, we map the ver­tices of 
p l to the unit circle C, spaced according to their edge lengths over the surface. (We use the unit circle 
at this point instead of the unit square in order to avoid boundary constraints.) The cut mesh Ml is 
parametrized into the interior of C using the shape-preserving parametrization of Floater [6]. Given 
the resulting map we identify the triangle with maximum geometric stretch, and pick one of its vertices 
as an extremal vertex.  ···  ···  (a) (b) (c) (d) (e)  Figure 2: Columns (a d) show iterations 
of the cut improvement algorithm. Upper images show the mesh M with the current cut p (blue except red 
where occluded). Bottom images show the Floater parametrization (over circle) of the corresponding Ml, 
together with the shortest path to an extremal point, which will be added to p. Column (e) shows the 
.nal cut p and the geometric-stretch parametrization (over square). The intuition for this method is 
that any protrusion of the mesh experiences high geometric stretch under a Floater parametrization. For 
instance, it can be shown that when parametrizing a tube closed at its top and open at its base, a triangle 
at a height h from the base has geometric stretch exponential in h, reaching a maximum at the tube apex. 
It is important to use the Floater parametrization for protrusion detection, since the geometric-stretch 
parametrization would evenly distribute stretch, thus hiding the extrema. Having identi.ed an extremal 
point, we .nd the shortest path from it to the current boundary of Ml (measuring distance on the mesh), 
and add this path to p. This maintains the invariant that p is a valid cut of M. We repeatedly apply 
this augmentation process, as shown in Figure 2. To determine when to stop, we run our geometric-stretch 
parametrization algorithm (Section 3.1) after each cut, and stop if the geometric stretch increases. 
As a further improvement in the case of genus-zero meshes, when we .nd the .rst extremal point, we discard 
the original cut, which was based on an arbitrary random seed point, and replace the cut with a pair 
of adjacent edges at this extremum. Cutting summary This pseudocode summarizes our algorithm: function 
Cut and parametrize(mesh M) Remove seed triangle. while there remains an edge e adjacent to only one 
triangle t Remove e and t. while there remains a vertex v adjacent to only one edge e Remove v and e. 
Cut p := remaining edges and vertices. if only a single vertex remains in p then Add back two adjacent 
edges to p. Straighten each cut-path in p. Param 1 := geometric-stretch parametrization using p. repeat 
f := Floater parametrization using p. t := triangle with maximal stretch under f . s := shortest path 
on M from t to p. p l := p + s. 1l := geometric-stretch parametrization using p l . if stretch(1l) > 
stretch(1) break. p := p l; 1 := 1l .  3.3 Topological sideband A geometry image is a parametric sampling 
of the topological disk Ml . Its reconstruction looks like M because its boundary vertices coincide geometrically. 
For some applications though, it is important to be able to fuse the boundary of D so that it has the 
original topology of p. This fusing could be achieved by searching for geometric correspondences on the 
image boundary, but this process might be error-prone, particularly if the geometry image undergoes lossy 
compression. Since the necessary topological cut information is extremely compact, we record it into 
a sideband signal as follows. We associate a pair of labels e.g. {a, a} to each cut-path and its mate. 
We then store the string of labels corresponding to the sequence of cut-paths on the boundary of Ml, 
e.g. ababcc. From this string, we can recover the topology of the cut, i.e. the valence k of each cut-node 
in p and the ordering of the cut-nodes along p l . We also store for each cut-path a its discretized 
length on the boundary of domain D, and we store the starting boundary location of the .rst cut-path. 
From this topological and parametric information, we can later establish the correspondence of all boundary 
grid vertices. The size of this sideband information is O(q log n) bits, where q is the number of cut-paths 
and n is the sampling rate over D. For our models, q ranges between 3 and 10, and the sideband is approximately 
12 bytes long.  4 APPLICATIONS Rendering To render geometry images on current hardware, we span each 
2×2 quad of grid points using two triangles, by splitting along the shorter of the two diagonals. Level-of-detail 
rendering is implemented by mip-mapping the geometry image, as shown in Figure 3. In order to avoid cracks 
at multiple levels of details, we use geometry images of size (2j +1) × (2j + 1), and minify using simple 
sub-sampling. Also, the boundary mapping 1 of Section 3.1 is constructed to place cut­nodes to grid-points 
of the lowest intended resolution (65×65 for all of our examples). Unlike [16], our boundary samples 
coincide exactly across the cut so we need no special boundary treatment, even for mip-mapping. For hardware 
that implements normal mapping, we also create a normal map using the exact same parametrization 1. Usually, 
 Report cut p and parametrization 1. we sample the normals into an image of higher resolution than the 
  Original mesh (342K faces) Geometry image (257×257) Mip-mapped (129×129) Mip-mapped (65×65) Figure 
3: Mip-mapping a geometry image. As in all examples, the boundary parametrization is constructed for 
a 65×65 domain grid. geometry since the normal-map signal tends to be more detailed. During rendering, 
the normal-map signal is rasterized over the tri­angles by hardware texture-mapping, using bilinear reconstruction 
of each quad in the normal map. (Texture coordinates at the vertices are assigned the range [(0. 5)/nl 
, ...,(nl -0. 5)/nl] where nl is the texture resolution, for correspondence with the texture samples.) 
Because geometry images have the same regular structure as texture images, one can envision hardware 
that would use bilinear (or even bicubic) basis functions to reconstruct the geometry. Moreover, the 
rendering process should be inherently simpler than with traditional texture mapping. The attribute samples 
can be accessed in scan order rather than backward-mapped through random-access texture coordinates. 
Also, the attribute samples have a regular correspondence with the geometry samples, and therefore do 
not require general tri-linear interpolation lookup. Both view-frustum and backface culling could be 
implemented in a uni.ed setting by constructing hierarchies on the geometry image and the normal image 
respectively. Compression and Decompression For compression we use the image-compression coder provided 
by Davis [1]. For decompres­sion, we decode the wavelet coef.cients to recreate an n×n grid of [x, y, 
z] values. Our wavelet decoder produces .oating-point coor­dinate values as output. Quantizing these 
values to 12-bit integers provides suf.cient resolution for our models. Since this wavelet coding is 
lossy, cut-path mates may be recon­structed differently, leading to cracks in the mesh (see Figure 1d). 
To address this problem, we also record and losslessly compress the topological sideband (Section 3.3). 
During decompression, we use this topological information to geometrically fuse the cut. We .rst determine 
the equivalence classes of boundary grid points. Most boundary grid point are paired up with a single 
other grid point, while grid points that sample a cut-node are grouped with k - 1 other grid points, 
where k is the valence of the cut-node in p.We average together the [x, y, z] values of equivalent grid 
points, and replace their data with this common average. We record the vector displacement added due 
to this averaging for later error diffusion. This simple averaging scheme gives rise to a continuous 
surface, but can lead to unsightly steps in the reconstructed geometry near the cut. In order to smooth 
these steps, we apply a simple error dif­fusion technique, spreading the displacements towards the center 
of the square. The result of this fusing process is shown in Figure 1e.  5 RESULTS We have run our system 
on a number of high-resolution models, with and without boundaries. Uncompressed examples are shown in 
Figure 7. These required about an hour to convert of.ine. The conversion bottleneck is the sequence of 
parametrizations in the iterated cut augmentation process. Currently, we set the geometry image resolution 
n manually (most often n = 257), but this parameter could be set automatically to achieve a desired accuracy. 
 Figure 5: Example artifacts in the Buddha geometry image: aliasing (jaggedness) near sharp features, 
and regions of high anisotropy. Geometry images tend to be relatively smooth, and therefore pro­vide 
opportunity for compression. Even simple image compressors will de.ne basis functions that span the whole 
surface, and there­fore allow high compression ratios. Figure 4 shows rate-distortion curves when using 
the image wavelet-coder of [1]. These curves measure the reconstruction accuracy for various compression 
rates applied to the geometry image. Error is measured as Peak Signal to Noise Ratio PSNR = 20 log10(peak/d), 
where peak is the bound­ing box diagonal and d is the symmetric rms Hausdorff error (ge­ometric distance) 
between the original mesh and the reconstructed geometry. The blue curves show results for wavelet-compressed 
geometry image created using a geometric-stretch parametrization and two different sampling rates. The 
green curve corresponds to a geometry image formed using the same cut, but with a Floater parametrization, 
and is noticeably less ef.cient. For comparison, the red curve is the result of the compression scheme 
described in [10], which is more ef.cient by about 3dB. Reconstructions from compressed geometry images 
are shown in Figure 6.   (a)49KB (b)12KB (c)3KB (d) 49 KB Figure 6: (a c) Surfaces reconstructed from 
a 257×257 geometry image under increasing levels of wavelet compression. (d) Reconstructed from a 257×257 
Floater-parametrized geometry image. All models are .at-shaded. 6 SUMMARY AND DISCUSSION We have introduced 
geometry images, a completely regular repre­sentation for approximating the geometry of an irregular 
mesh. Ge­ometry images can be easily rendered and compressed using current hardware and software. Due 
to their simplicity, we envision that ge­ometry images may inspire new hardware rendering approaches. 
We have found that we can create ef.cient geometry images on a wide variety of models. However, models 
of high genus can be problematic. Such models may require long cuts to open up all the topological handles. 
In that case, much of the surface lies near the cut boundary, making it dif.cult to create a parametrization 
without signi.cant geometric stretch and poor resampling. Figure 5 shows examples of trouble areas in 
the remeshing of the Buddha model. Our genus-6 Buddha model was obtained by .ltering out tiny topological 
handles from a genus-104 scanned model [23]; working directly on the genus-104 surface would have been 
impossible. In general, remeshing techniques can have dif.culty capturing sharp surface features accurately 
at low sampling rates. In semi­regular remeshing, one technique to improve accuracy is to make the chart 
boundaries correspond with the most signi.cant features, so that the subdivided domain edges follow these 
features [13]. Another technique is feature-sensitive remeshing [22], which warps the parametrization 
as a post-process to align the remesh edges with the sharp surface features. When creating our geometry 
images, adding a pass of feature-sensitive remeshing could improve reconstruction results for meshes 
with sharp geometry. Since we used off-the-shelf compression code, we did not explore the extra savings 
that could be obtained using local-frame detail representation [10]. Adding this to our system may improve 
compression ef.ciencies.  ACKNOWLEDGEMENTS We gratefully thank Zo¨e Wood for providing the topologically 
sim­pli.ed dragon and Buddha, Pedro Sander for the parametrization code, and Stanford University and 
Cyberware for models. REFERENCES [1] DAVIS, G. Wavelet Image Compression Construction Kit. http://www.geoffdavis.net/dartmouth/wavelet/wavelet.html. 
[2] DEY, T. K., AND SCHIPPER, H. A new technique to compute polygonal schema for 2-manifolds with application 
to null-homotopy detection. Discrete and Computational Geometry 14 (1995), 93 110. [3] ECK, M., DEROSE,T., 
DUCHAMP,T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. 
In SIGGRAPH 95, pp. 173 182. [4] ERICKSON, J., AND HAR-PELED, S. Cutting a surface into a disk. ACM SoCG 
2002. [5] FERGUSON, H., ROCKWOOD, A., AND COX, J. Topological design of sculptured surfaces. In SIGGRAPH 
92, pp. 149 156. [6] FLOATER, M. Parametrization and smooth approximation of surface triangulations. 
CAGD 14, 3 (1997), 231 250. [7] GUSKOV,I., VIDIMCE, K., SWELDENS,W., AND SCHR¨ ODER,P. Normal Meshes. 
In SIGGRAPH 2000, pp. 95 102. [8] HAKER, S., ANGENENT, S., TANNENBAUM, A., KIKINIS,R., SAPIRO, G., AND 
HALLE, M. Conformal Surface Parameterization for Texture Mapping. IEEE TVCG 6, 2 (2000), 181 189. [9] 
HOPPE, H. Progressive Meshes. In SIGGRAPH 96, pp. 99 108. [10] KHODAKOVSKY, A., SCHR¨ ODER,P., AND SWELDENS, 
W. Progres­sive Geometry Compression. In SIGGRAPH 2000, pp. 271 278. [11] LAZARUS,F., POCCHIOLA, M., 
VEGTER, G., AND VERROUST, A. Computing a Canonical Polygonal Schema of an Orientable Triangulated Surface. 
In ACM SoCG 2001, pp. 80 89. [12] LEE, A., MORETON, H., AND HOPPE, H. Displaced Subdivision Surfaces. 
In SIGGRAPH 2000, pp. 85 94. [13] LEE, ODER,P., ANDA., SWELDENS,W., SCHR¨COWSAR,L., DOBKIN, D. MAPS: 
Multiresolution Adaptive Parameterization of Surfaces. In SIGGRAPH 98, pp. 95 104. [14] LOUNSBERY, M., 
DEROSE,T., AND WARREN, J. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. ACM TOG 
16, 1 (January 1997), 34 73. [15] MUNKRES,J. Topology. Prentice Hall, 2000. [16] PIPONI, D., AND BORSHUKOV, 
G. D. Seamless Texture Mapping of Subdivision Surfaces by Model Pelting and Texture Blending. In SIGGRAPH 
2000, pp. 471 478. [17] SANDER,P., GORTLER, S., SNYDER, J., AND HOPPE, H. Signal-Specialized Parametrization. 
Microsoft Research MSR-TR-2002-27 (January 2002). [18] SANDER,P., SNYDER, J., GORTLER, S., AND HOPPE, 
H. Texture Mapping Progressive Meshes. In SIGGRAPH 2001, pp. 409 416. [19] SHEFFER, A. Spanning Tree 
Seams for Reducing Parameterization Distortion of Triangulated Surfaces. Shape Modelling International 
(2002). [20] TAUBIN, G., AND ROSSIGNAC, J. Geometric compression through topological surgery. ACM TOG 
17, 2 (1998), 84 115. [21] VEGTER, G., AND YAP, C. K. Computational complexity of combinatorial surfaces. 
In ACM SoCG 1990, pp. 102 111. [22] VORSATZ, R ¨ KOBBELT,L., SEIDEL,J., OSSL,C., AND H.-P. Feature Sensitive 
Remeshing. Computer Graphics Forum 20,3 (2001), 393 401. [23] WOOD,Z., H., M., AND SCHR¨HOPPE, DESBRUN, 
ODER,P. Isosurface topology simpli.cation. Microsoft Research MSR-TR­2002-28 (January 2002). [24] ZORIN, 
D., SCHR¨ AND SWELDENS, W. ODER,P., Interactive multiresolution mesh editing. In SIGGRAPH 97, pp. 259 
268. Original (500K faces; genus 1) Original (500K faces; genus 6) Original (47K faces; genus 3) Original 
(480K faces; genus 0) Geometry image (257x257) Geometry image (257x257) Geometry image (129x129) Geometry 
image (257x257) Reconstruction (PSNR=66.8) Reconstruction (PSNR=64.9) Reconstruction (PSNR=75.2) Reconstruction 
(PSNR=78.6) Normal map (512x512) Normal map (512x512) Normal map (256x256) Normal map (512x512) Normal-mapped 
reconstruction Normal-mapped reconstruction Normal-mapped reconstruction Normal-mapped reconstruction 
Figure 7: Examples: original meshes with cut, geometry images and their reconstructions, and use of normal-mapping. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566590</article_id>
		<sort_key>362</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Least squares conformal maps for automatic texture atlas generation]]></title>
		<page_from>362</page_from>
		<page_to>371</page_to>
		<doi_number>10.1145/566570.566590</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566590</url>
		<abstract>
			<par><![CDATA[A Texture Atlas is an efficient color representation for 3D Paint Systems. The model to be textured is decomposed into charts homeomorphic to discs, each chart is parameterized, and the unfolded charts are packed in texture space. Existing texture atlas methods for triangulated surfaces suffer from several limitations, requiring them to generate a large number of small charts with simple borders. The discontinuities between the charts cause artifacts, and make it difficult to paint large areas with regular patterns.In this paper, our main contribution is a new quasi-conformal parameterization method, based on a least-squares approximation of the Cauchy-Riemann equations. The so-defined objective function minimizes angle deformations, and we prove the following properties: the minimum is unique, independent of a similarity in texture space, independent of the resolution of the mesh and cannot generate triangle flips. The function is numerically well behaved and can therefore be very efficiently minimized. Our approach is robust, and can parameterize large charts with complex borders.We also introduce segmentation methods to decompose the model into charts with natural shapes, and a new packing algorithm to gather them in texture space. We demonstrate our approach applied to paint both scanned and modeled data sets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[paint systems]]></kw>
			<kw><![CDATA[polygonal modeling]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Geometric correction</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP36024720</person_id>
				<author_profile_id><![CDATA[81100154829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[L&#233;vy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISA (Inria Lorraine and CNRS), France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P272850</person_id>
				<author_profile_id><![CDATA[81100234821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Petitjean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISA (Inria Lorraine and CNRS), France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14191756</person_id>
				<author_profile_id><![CDATA[81100552186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ray]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISA (Inria Lorraine and CNRS), France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382438</person_id>
				<author_profile_id><![CDATA[81100269040]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[J&#233;rome]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maillot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISA (Inria Lorraine and CNRS), France and Alias|Wavefront]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>199429</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Agrawala, A. Beers, and M. Levoy. 3D painting on scanned surfaces. In Proc. 1995 Symposium on Interactive 3D Graphics, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266967</ref_obj_id>
				<ref_obj_pid>266956</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Y. Azar and L. Epstein. On 2d packing. J. of Algorithms, (25):290-310, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288224</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[P. Cigogni, C. Montani, C. Rocchini, and R. Scopino. A general method for recovering attributes values on simplified meshes. In Proc. of IEEE Visualization Conf., pages 59-66. ACM Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution analysis of arbitrary meshes. In SIGGRAPH 95 Conf. Proc., pages 173-182. Addison Wesley, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Eells and L. Lemaire. Another report on harmonic maps. Bull. London Math. Soc., 20:385-524, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248308</ref_obj_id>
				<ref_obj_pid>248299</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Floater. Parametrization and smooth approximation of surface triangulations. Computer Aided Geometric Design, 14(3):231-250, April 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344831</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[I. Guskov, K. Vidimce, W. Sweldens, and P. Schr&#246;der. Normal meshes. In SIGGRAPH 00 Conf. Proc., pages 95-102. ACM Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614459</ref_obj_id>
				<ref_obj_pid>614278</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. Haker, S. Angenent, A. Tannenbaum, R. Kikinis, G. Sapiro, and M. Halle. Conformal surface parameterization for texture mapping. IEEE Transactions on Visualization and Computer Graphics, 6(2):181-189, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[P. Hanrahan and P. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. In SIGGRAPH 90 Conf. Proc., pages 215-223. Addison Wesley, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[K. Hormann and G. Greiner. MIPS: An efficient global parametrization method. In P.-J. Laurent, P. Sablonni&#232;re, and L. Schumaker, editors, Curve and Surface Design: Saint-Malo 1999, pages 153-162. Vanderbilt University Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601716</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Hubeli and M. Gross. Multiresolution features extraction from unstructured meshes. In Proc. of IEEE Visualization Conf., 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>710069</ref_obj_id>
				<ref_obj_pid>646922</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Hurdal, P. Bowers, K. Stephenson, D. Sumners, K. Rehms, K. Schaper, and D. Rottenberg. Quasi-conformally flat mapping the human cerebellum. In Proc. of MICCAI'99, volume 1679 of Lecture Notes in Computer Science, pages 279-286. Springer-Verlag, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364404</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[T. Igarashi and D. Cosgrove. Adaptive unwrapping for interactive texture painting. In Symp. on Interactive 3D Graphics, pages 209-216. ACM, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Levoy. Fitting smooth surfaces to dense polygon meshes. In SIGGRAPH 96 Conf. Proc., pages 313-324. Addison Wesley, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>304025</ref_obj_id>
				<ref_obj_pid>304012</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[F. Lazarus and A. Verroust. Level set diagrams of polyhedral objects. In Proc. of Solid Modeling and Applications, pages 130-140. ACM Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280828</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Lee, W. Sweldens, P. Schr&#246;der, L. Cowsat, and D. Dobkin. MAPS: Multiresolution adaptive parameterization of surfaces. In SIGGRAPH 98 Conf. Proc., pages 95-104. Addison Wesley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383308</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[B. L&#233;vy. Constrained texture mapping for polygonal meshes. In SIGGRAPH 01 Conf. Proc., pages 417-424. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280930</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[B. L&#233;vy and J.-L. Mallet. Non-distorted texture mapping for sheared triangulated meshes. In SIGGRAPH 98 Conf. Proc., pages 343-352. Addison Wesley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>696466</ref_obj_id>
				<ref_obj_pid>646504</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[P. Lienhardt. Extension of the notion of map and subdivisions of a 3D space. In Proc. of 5th Symp. on Theo. Aspects in Comp. Sci., pages 301-311, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, H. Yahia, and A. Verroust. Interactive texture mapping. In SIGGRAPH 93 Conf. Proc., pages 27-34. Addison Wesley, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>316661</ref_obj_id>
				<ref_obj_pid>316660</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[V. Milenkovic. Rotational polygon containment and minimum enclosure using only robust 2D constructions. Computational Geometry, 13(1):3-19, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>225094</ref_obj_id>
				<ref_obj_pid>224841</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[H. Murata, K. Fujiyoshi, S. Nakatake, and Y. Kajitani. Rectangle-packing-based module placement. In Proc. of ICCAD, pages 472-479. IEEE, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[H. Pedersen. Decorating implicit surfaces. In SIGGRAPH 95 Conf. Proc., pages 291-300. Addison Wesley, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[U. Pinkall and K. Polthier. Computing discrete minimal surfaces and their conjugates. Experimental Math., 2(15), 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344990</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[D. Piponi and G. Borshukov. Seamless texture mapping of subdivision surfaces by model pelting and texture blending. In SIGGRAPH 00 Conf. Proc., pages 471-478. ACM Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344987</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[E. Praun, A. Finkelstein, and H. Hoppe. Lapped textures. In SIGGRAPH 00 Conf. Proc., pages 465-470. ACM Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383307</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[P. Sander, J. Snyder, S. Gortler, and H. Hoppe. Texture mapping progressive meshes. In SIGGRAPH 01 Conf. Proc., pages 409-416. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A. Sheffer and E. de Sturler. Param. of faceted surfaces for meshing using angle-based flattening. Engineering with Computers, 17(3):326-337, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617692</ref_obj_id>
				<ref_obj_pid>616019</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Y. Shinagawa, T. Kunii, and Y.-L. Kergosien. Surface coding based on Morse theory. IEEE Computer Graphics and Applications, 11(5):66-78, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[W. Tutte. Convex representation of graphs. In Proc. London Math. Soc., volume 10, 1960.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614523</ref_obj_id>
				<ref_obj_pid>614286</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[G. Zigelman, R. Kimmel, and N. Kiryati. Texture mapping using surface flattening via multi-dimensional scaling. IEEE Transactions on Vis. and C.G., 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Least Squares Conformal Maps for Automatic Texture Atlas Generation Bruno Lévy Sylvain Petitjean Nicolas 
Ray Jérome Maillot* ISA (Inria Lorraine and CNRS), France  Abstract A Texture Atlas is an ef.cient color 
representation for 3D Paint Sys­tems. The model to be textured is decomposed into charts home­omorphic 
to discs, each chart is parameterized, and the unfolded charts are packed in texture space. Existing 
texture atlas methods for triangulated surfaces suffer from several limitations, requiring them to generate 
a large number of small charts with simple bor­ders. The discontinuities between the charts cause artifacts, 
and make it dif.cult to paint large areas with regular patterns. In this paper, our main contribution 
is a new quasi-conformal pa­rameterization method, based on a least-squares approximation of the Cauchy-Riemann 
equations. The so-de.ned objective function minimizes angle deformations, and we prove the following 
proper­ties: the minimum is unique, independent of a similarity in texture space, independent of the 
resolution of the mesh and cannot gener­ate triangle .ips. The function is numerically well behaved and 
can therefore be very ef.ciently minimized. Our approach is robust, and can parameterize large charts 
with complex borders. We also introduce segmentation methods to decompose the model into charts with 
natural shapes, and a new packing algorithm to gather them in texture space. We demonstrate our approach 
ap­plied to paint both scanned and modeled data sets. CR Categories: I.3.3 [Computer Graphics] Picture/Image 
Gen­eration; I.3.5 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, shading, shadowing 
and texture; I.4.3 [Image processing]: Enhancement Geometric Correction, Texture Keywords: Texture Mapping, 
Paint Systems, Polygonal Modeling *Alias|Wavefront Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 1 INTRODUCTION A 3D paint system makes it possible to 
enhance the visual appear­ance of a 3D model by interactively adding details to it (colors, bump maps 
. . . ). If the discretization of the surface is .ne enough, it is possible to directly paint its vertices 
[1]. However, in most cases, the desired precision for the colors is .ner than the geomet­ric details 
of the model. Assuming that the surface to be painted is provided with a parameterization, it is possible 
to use texture map­ping to store colors in parameter space [9]. Parametric surfaces (such as NURBS) have 
a natural parameterization. For other repre­sentations, such as polygonal surfaces, .nding a parameterization 
is non-trivial. To decorate polygonal models with regular patterns, the lapped textures approach [26] 
can be applied: local overlapping parameterizations are used to repeatedly map a small texture swatch 
onto a model. A texture atlas is a more general representation (see, e.g., [13, 20, 23]). The model to 
be textured is partitioned into a set of parts homeomorphic to discs, referred to as charts, and each 
of them is provided with a parameterization. A texture atlas can be easily represented by standard .le 
formats and displayed using standard texture mapping hardware. When used in a 3D paint sys­tem, a texture 
atlas should meet the following requirements: the chart boundaries should be chosen to minimize texture 
arti­facts,  the sampling of texture space should be as uniform as possible,  the atlas should make 
an optimal use of texture space.  The generation of a texture atlas can be decomposed into the following 
steps: 1. Segmentation: The model is partitioned into a set of charts. 2. Parameterization: Each chart 
is unfolded , i.e. put in corre­spondence with a subset of R2 . 3. Packing: The charts are gathered 
in texture space.  The remainder of this section presents the existing methods for these three steps, 
and their limitations with respect to the require­ments mentioned above. We then introduce a new texture 
atlas gen­eration method, meeting these requirements by creating charts with natural shapes, thus reducing 
texture artifacts. 1.1 Previous Work Segmentation into charts. In [14] and [23], the model is interac­ 
tively partitioned by the user. To perform automatic segmentation, Maillot et al. [20] group the facets 
by their normals. Several multi­resolution methods [7, 16] decompose the model into charts corre­ sponding 
to the simplices of the base complex. In [27], Sander et al. use a region-growing approach to segmentation, 
merging charts according to both planarity and compactness criteria. All these ap­proaches are designed 
to produce charts that can be treated by ex­isting parameterization methods, which are limited to charts 
with convex borders. For this reason, a large number of charts is gen­erated, which introduces many discontinuities 
when constructing a texture atlas. Chart parameterization. Discrete Harmonic Map, described by Eck et 
al. [4], are the most widely used. They are approximations of Continuous Harmonic Maps [5], minimizing 
a metric dispersion criterion. Pinkal and Polthier [24] have shown the link between this criterion and 
another one named conformality, and have expressed both in terms of Dirichlet energy. Haker et al. [8] 
describe a similar method in the speci.c case of a surface triangulation homeomor­phic to a sphere. The 
theory on graph embedding has been studied by Tutte [30], where Barycentric Maps are introduced. The 
bijectivity of the so­de.ned parameterization is mathematically guaranteed. Floater [6] proposes speci.c 
weights improving the quality of the mapping, in terms of area deformations and conformality. In [18], 
a method is proposed to take additional constraints into account. In all the methods mentioned above, 
since conformality is ex­pressed as an indirect coupling between the parameters, boundary conditions 
are required, i.e. boundary nodes need to be .xed on a convex border in parameter space. Other expressions 
of confor­mality, such as the non-linear MIPS method [10], make it possi­ ble to overcome this problem, 
and let the boundary nodes be free to move. However, this latter method requires a time-consuming non-linear 
optimization, and may get stuck in a local minima of the non-linear function. In [12], Hurdal et al. 
propose a method based on circle packings, which are certain con.gurations of cir­cles with speci.ed 
pattern of tangencies known to provide a way to approximate a conformal mapping. Building circle packings 
is however quite expensive. The approach proposed in [28] consists in solving for the angles in parameter 
space. It results in a highly constrained optimization problem. Other methods [17, 25, 31] can also extrapolate 
the border, but do not guarantee the absence of tri­angle .ips and require interaction with the user. 
We introduce here a conformal mapping method, offering more guarantees, ef.ciency and robustness than 
those approaches. In the case of texture mapping, not only the bijectivity of the parameterization should 
be ensured, but also its ability to make an optimum use of texture memory, and to accurately represent 
a sig­nal stored in texture space. Sander et. al. [27] describe an approach to minimize both a texture 
stretch criterion, and texture deviation between level of details. Since their approach is independent 
from the initial parameterization method, it can be applied to optimize the sampling of the parameterizations 
constructed by our method. Charts packing in texture space. Finding the optimal packing of the charts 
in texture space is known as the bin packing problem. It has been studied by several authors, such as 
Milenkovic (see, e.g., [21]), but the resulting algorithms take a huge amount of time since the problem 
is NP-complete. To speed up these computa­tions, several heuristics have been proposed in the computer 
graph­ics community. In the case of individual triangles, such a method is described by several authors 
(see, e.g., [3]). In the general case of charts, Sander et al. [27] propose an approach to pack the minimal 
area bounding rectangles of the charts. In our case, since the charts can have arbitrarily shaped borders, 
the bounding rectangle can be far away from the boundary of the charts. Therefore, a lot of texture space 
can be wasted. For this reason, we propose a more accurate packing algorithm that can handle the complex 
charts created by our segmentation and parameterization methods.  1.2 Overview The paper is organized 
as follows. Since it is our main contribu­tion, we will start by introducing Least Squares Conformal 
Maps (LSCMs), a new optimization-based parameterization method with the following properties (see Section 
2 and Figure 1): Our criterion minimizes angle deformations and non-uniform scalings. It can be ef.ciently 
minimized by classical NA meth­ods, and does not require a complex algorithm such as the ones used in 
[12] and in [28].  We prove the existence and uniqueness of the minimum of this criterion. Therefore, 
the solver cannot get stuck in a local minimum, in contrast with non-linear methods [10, 25, 27, 31] 
where this property is not guaranteed.  The borders of the charts do not need to be .xed, as with most 
of the existing methods [4, 6, 18]. Therefore, large charts with arbitrarily shaped borders can be parameterized. 
 We prove that the orientation of the triangles is preserved, which means that no triangle .ip can occur. 
However, as in [28], over­ laps may appear, when the boundary of the surface self-intersects in texture 
space. Such con.gurations are automatically detected, and the concerned charts are subdivided. This problem 
was sel­dom encountered in our experiments (note that as with classical methods [4, 6], if all the border 
nodes are .xed on a convex poly­gon, no overlap can occur).  We prove that the result is independent 
of the resolution of the mesh. This type of property may be usefull to reduce texture deviation when 
parameterizing different level of details of the same object.  In Section 3, we present a new segmentation 
method to decom­pose the model into charts. Thanks to the additional .exibility of­fered by LSCMs, it 
is possible to create large charts correspond­ing to meaningful geometric entities, such as biological 
features of characters and animals. The required number of charts is dramati­cally reduced, together 
with the artifacts caused by the discontinu­ities between the charts. Moreover, these large charts facilitate 
the use of regular patterns in a 3D paint system. Section 4 presents our method to pack the charts in 
texture space. Since our segmentation method can create charts with complex bor­ders, we pack the charts 
more accurately than with bounding rect­angles, as in previous approaches. Our method is inspired by 
the strategy used by a Tetris player. The paper concludes with some results, on both scanned and modeled 
meshes.  2 LEAST SQUARES CONFORMAL MAPS In this section, we focus on the problem of parameterizing a 
chart homeomorphic to a disc. It will then be shown how to decompose the model into a set of charts, 
and how to pack these charts in texture space. 2.1 Notations scalars are denoted by normal characters 
x, y, u, v,  vectors are denoted by bold characters x =(x, y),  complex numbers are denoted by capitals 
U =(u + iv),  vectors of complex numbers are denoted by bold capitals U,  maps and matrices are denoted 
by cursive fonts U, X .  Figure 1: The body of the scanned horse is a test case for the robustness 
of the method; it is a single very large chart of 72,438 triangles, with a complex border. A: Resulting 
iso-parameter curves; B: The corresponding unfolded surface, where the border has been automatically 
extrapolated; C: These cuts make the surface equivalent to a disc; D: The parameterization is robust, 
and not affected by the large triangles in the circled area (caused by shadow zones appearing during 
the scanning process). 2.2 Conformal Maps In this section, we quickly introduce the notion of conformal 
map. We will present further a new way to approximate the conformality criterion and the mathematical 
properties of this approximation. Figure 2: In a conformal map, the tangent vectors to the iso-u and 
to the iso-v curves are orthogonal and have the same length. As shown in Figure 2, an application X mapping 
a (u, v) domain to a surface is said to be conformal if for each (u, v), the tangent vectors to the iso-u 
and iso-v curves passing through X (u, v) are orthogonal and have the same norm, which can be written 
as: .X .X N(u, v) × (u, v)= (u, v), (1) .u .v where N(u, v) denotes the unit normal to the surface. In 
other words, a conformal map is locally isotropic, i.e. maps an elemen­tary circle of the (u, v) domain 
to an elementary circle of the sur­face. It is possible to rewrite Equation 1 using differential operators, 
such as Laplace-Beltrami, as done in [24] and in [8], which results in the well known cotangent weighting 
coef.cients (see e.g. [4]). The (u, v) parameters are then found to be the solution of two sep­arate 
linear systems, one for u and one for v. The relation between u and v is indirectly taken into account 
by the right hand sides of the two systems. For this reason, this type of method requires the border 
to be .xed on a convex polygon. The MIPS method [10] does not have this restriction, and expresses conformality 
as a re­lation linking the coef.cients of the metric tensor. However, the resulting equations are non-linear. 
Another approach has been de­scribed in [28], based on the remark that the criterion de.ning a conformal 
mapping should be independent of a translation, rotation and scaling in parameter space (i.e. a similarity). 
The unknowns are the angles at the corners of the triangles. This requires a time­consuming constrained 
optimization method. Rather than discretizing the Laplace operator at the vertices of the triangulation, 
we instead take the dual path of considering the conformality condition on the triangles of the surface. 
Using the fact that a similarity can be represented by the product of complex numbers, we show how to 
turn the conformality problem into an unconstrained quadratic minimization problem. The u and v pa­rameters 
are linked by a single global equation. This direct cou­pling of the u and v parameters makes it possible 
to ef.ciently pa­rameterize large charts with complex borders, as shown in Figure 1. In this example, 
the cuts have been done manually (Figure 1-C), to create a large test case for the robustness of the 
method. (It will be shown in Section 3 how to automatically cut a model into charts homeomorphic to discs.) 
Riemann s theorem states that for any surface S homeomorphic to a disc, it is possible to .nd a parameterization 
of the surface sat­isfying Equation 1. However, since we want to use the resulting parameterization for 
texture mapping, we add the constraint that the edges of the triangulation should be mapped to straight 
lines, and the mapping should vary linearly in each triangle. With this additional constraint, it is 
not always possible to satisfy the confor­mality condition. For this reason, we will minimize the violation 
of Riemann s condition in the least squares sense. 2.3 Conformality in a Triangulation Consider now 
a triangulation G = {[1 ...n], T , (pj )1.j.n}, where [1 ...n],n . 3, corresponds to the vertices, where 
T is a set of n . triangles represented by triples of vertices, and where pj . R3 denotes the geometric 
location at the vertex j. We sup­pose that each triangle is provided with a local orthonormal basis, 
where (x1,y1), (x2,y2), (x3,y3) are the coordinates of its vertices in this basis (i.e., the normal is 
along the z-axis). The local bases of two triangles sharing an edge are consistently oriented. We now 
consider the restriction of X to a triangle T and apply the conformality criterion to the inverse map 
U :(x, y) .. (u, v) (i.e. the coordinates of the points are given and we want their pa­rameterization). 
In the local frame of the triangle, Equation 1 be­ comes .X .u - i .X .v = 0, where X has been written 
using complex numbers, i.e. X = x+iy. By the theorem on the derivatives of inverse functions, this implies 
that .U .U + i =0, (2) .x .y where U = u + iv. (This is a concise formulation of the Cauchy-Riemann 
equations.) Since this equation cannot in general be strictly enforced, we minimize the violation of 
the conformality condition in the least squares sense, which de.nes the criterion C: C(T )= .U .U + i 
.x .y 2 dA = .U .U + i .x .y 2 AT ,  T where AT is the area of the triangle and the notation |z| stands 
for the modulus of the complex number z. Summing over the whole triangulation, the criterion to minimize 
is then C(T )= C(T ). T .T  2.4 Gradient in a Triangle Our goal is now to associate with each vertex 
j a complex num­ber Uj such that the Cauchy-Riemann equation is satis.ed (in the least squares sense) 
in each triangle. To this aim, let us rewrite the criterion C(T ), assuming the mapping U varies linearly 
in T . We consider a triangle {(x1,y1), (x2,y2), (x3,y3)} of R2, with scalars u1,u2,u3 associated with 
its vertices. We have: Figure 3: Our LSCM parameterization is insensitive to the resolution of the mesh. 
The iso-parameter curves obtained on a coarse mesh (Figure A) and on a .ne one (Figure B) are identical, 
and remain stable when the resolution varies within a mesh (circled zone in Figures C and D). For the 
optimization problem to have a non-trivial solution, some of the Ui s must be set to a priori values. 
Let us decompose the vec­tor U as (U. f , U. p )., where Uf is the vector of free coordinates of U (the 
variables of the optimization problem) and Up is the vector of pinned coordinates of U, of length p (p 
. n). Along the same .u/.x 1 lines, M can be decomposed in block matrices as u1y2 - y3 y3 - y1 y1 - 
y2 u2 = , x3 - x2 x1 - x3 x2 - x1 .u/.y dT u3 M =(Mf Mp) , where dT =(x1y2 - y1x2)+(x2y3 - y2x3)+(x3y1 
- y3x1) is twice the area of the triangle. The two components of the gradient can be gathered in a com­plex 
number: .u .u i + i =(W1 W2 W3)(u1 u2 u3). , .x .y dT where Mf is a n . × (n - p) matrix and Mp is a 
n . × p matrix. Now, Equation 3 can be rewritten as C(U)= U * M * MU = .MU.2 = .Mf Uf + MpUp.2 , where 
the notation .v.2 stands for the inner product < v, v > (v stands for the conjugate of v). Rewriting 
the objective function with only real matrices and vec­ tors yields where W1 =(x3 - x2)+ i(y3 - y2),C(x)= 
.Ax - b.2 , (4) W2 =(x1 - x3)+ i(y1 - y3), with W3 =(x2 - x1)+ i(y2 - y1). .. M1 M1 U1 -M2 -M2 The Cauchy-Riemann 
equation (Equation 2) can be rewritten as f f ppp A = , b = - , M2 M1 M2 M1 U2 follows: ff ppp .U .U 
i . where the superscripts 1 and 2 stand respectively for the real and + i =(W1 W2 W3)(U1 U2 U3)=0, 
 .x .y dT imaginary part, .v. stands this time for the traditional L2-norm of a vector with real coordinates 
and x =(U1 f . , Uf 2 . ). is the vector where Uj = uj + ivj . The objective function thus reduces to 
 of unknowns. Note that A is a 2n . ×2(n - p) matrix, b is a vector of R2n . and C(U =(U1,...,Un ).)= 
 with x is a vector of R2(n-p) (the ui and vi coordinates of the vertices C(T ), in parameter space 
that are allowed to move freely). T .T (Wj1,T Wj2,T Wj3,T )(Uj1 Uj2 Uj3 ) 2 , 1 C(T )= dT  2.6 Properties 
where triangle T has vertices indexed by j1,j2,j3. (We have mul­tiplied C(T ) by a factor of 2 to simplify 
the expression.) 2.5 Least Squares Conformal Maps C(U) is quadratic in the complex numbers U1,...,Un, 
so can be written down as C(U)= U * CU, (3) where C is a Hermitian symmetric n × n matrix and the notation 
U * stands for the Hermitian (complex) conjugate of U. C is an instance of a Hermitian Gram matrix, i.e. 
it can be written as C = M * M, where M =(mij ) is the sparse n . × n matrix (rows are indexed by triangles, 
columns are indexed by vertices) whose coef.cient is The above minimization problem has several fundamental 
proper­ties which are proved in the appendix: The matrix A has full rank when the number of pinned vertices, 
 i.e. p, is larger than or equal to 2.  As a consequence, the minimization problem has a unique solu­tion 
when p . 2, given by x =(A.A)-1A.b. The best value for p is 2, since in this case the mapping U can be 
fully conformal if the surface is developable (i.e. the minimum of the objective  function is zero). 
In our experiments, we have pinned the two vertices maximizing the length of the shorted path between 
them (i.e. the graph diameter).  The solution to the minimization problem is invariant by a simi­larity 
in texture space.  The solution to the minimization problem is independent of the resolution of the 
mesh. This property is illustrated in Figure 3.  In texture space, all the triangles are consistently 
oriented if the  Wj,Ti v if vertex j belongs to triangle Ti, pinned vertices are chosen on the boundary 
of T . In other words, dTi mij = triangle .ips cannot occur. otherwise. 0  Figure 4: A: Result of the 
features detection algorithm; B: The distance_to_seed function is not an optimal choice for driving our 
chart growing process; C: The distance_to_features function shows iso-contours with more natural shapes; 
D: Result of our segmentation algorithm, driven by the distance_to_features function.  3 SEGMENTATION 
The segmentation algorithm decomposes the model into a set of charts. The design of the algorithm aims 
at meeting the following two requirements: 1. charts boundaries should be positioned in such a way that 
most of the discontinuities between the charts will be located in zones where they will not cause texture 
artifacts, 2. charts must be homeomorphic to discs, and it must be possible to parameterize them without 
introducing too much deformation.  For the .rst point, since the shading models depend on the nor­mal, 
zones of high curvatures cause sharp variations of lighting. In these zones, a texture artifact will 
not be noticeable, since it will be negligible compared to the shading variation. Thus, to minimize artifacts, 
we will design the segmentation algorithm in such a way as to avoid chart boundaries in .at zones. In 
other words, it is suit­able to generate large charts with most of their boundaries in high curvature 
zones. For the second point, we will present an automatic approach, mimicking the way a user manually 
segments a model. Basically, the user attempts to decompose the model into parts resembling cylinders. 
To detect such cylinders, we will use an approach in­spired by Morse theory, characterizing a function 
de.ned over the surface (see, e.g., [29]). The next two sections present a feature detection algorithm, 
which .nds curves corresponding to high curvature zones of the model, and a chart growing algorithm making 
them meet at these feature curves. Then, it will be shown how to validate the result, and subdivide the 
charts if needed. In the remainder of this section, we suppose that the surface is represented by a halfedge 
based data structure (see, e.g., [19]). 3.1 Detect Features The features detection phase can be outlined 
as follows: 1. Compute a sharpness criterion on the edges. We use here the second order differences (SOD), 
i.e. the angle between the nor­mals, as in [11]. It is also possible to use more elaborate criteria. 
 2. Choose a threshold t so that a certain proportion of the edges is .ltered out. In our examples, we 
kept 5 percent of the detected edges. 3. For each of the remaining edges, grow a feature curve by apply­ing 
Algorithm 1.  Algorithm 1 attempts to anticipate the best paths, and .lters out the small features caused 
by noise. Tagging the neighborhoods of the detected features avoids generating a large number of features 
in zones of high curvature. In our examples, the parameters are set expand_feature_curve(halfedge start) 
vector<halfedge> detected_feature for halfedge h .{ start, opposite(start) } halfedge h. . h do use depth-.rst 
search to .nd the string S of halfedges starting with h. and such that: two consecutive halfedges of 
S share a vertex  the length of S is . than max_string_length  sharpness(S) . sharpness(e) is maximum 
 e.S no halfedge of S goes backward (relative to h.) no halfedge of S is tagged as a feature neighbor 
h. . second item of S append h. to detected_feature while(sharpness(S) > max_string_length × t) end // 
for if (length(detected_feature)> min_feature_length) then tag the elements of detected_feature as features 
tag the halfedges in the neighborhood of detected_feature as feature neighbors end // if end // expand_feature_curve 
Algorithm 1: Features growing as follows: max_string_length =5, which controls the size of the discontinuities 
to be .lled, and min_feature_length = 15. 3.2 Expand Charts Once the sharp features have been detected, 
the charts can be cre­ated. Our method is a greedy algorithm, expanding all the charts simultaneously 
from a set of seeds. It is similar to the s-source Dijkstra algorithm used in [4] and to the region-growing 
paradigm used in computer vision. Since we want chart boundaries to meet at the level of features, the 
s-source algorithm is modi.ed as follows: To select the set of seeds, the intuitive idea is to reverse 
engi­neer the expected result. More precisely, we use the follow­ing method: a front is propagated from 
the borders and the fea­ture curves detected by the previous algorithm, to compute a distance_to_features 
function at each facet. Then, the seeds are found to be the local maxima of this distance_to_features 
function.  For closed surfaces without any detected feature, propagation is initialized from the two 
extremities of a diameter of the facets graph, as done in [15].  Our s-source propagation uses -distance_to_features 
as the priority function, rather than distance_to_seeds. The advan­tage of this approach is shown in 
Figure 4.   Figure 5: A: Our segmentation algorithm detects cylindrical shapes; B: An addi­tional 
cut is added to sock-shaped extremal cylinders. expand_charts priority_queue<halfedge> Heap sorted by 
dist(facet(halfedge)) set<edge> chart_boundaries initialized with all the edges of the surface // Initialize 
Heap foreach facet F where dist(F ) is a local maximum create a new chart with seed F add the halfedges 
of F to Heap end // foreach // Charts-growing phase while(Heap is not empty) halfedge h . e . Heap such 
that dist(e) is maximum remove h from Heap facet F . facet(h) facet Fopp . the opposite facet of F relative 
to h if ( chart(Fopp) is unde.ned ) then add Fopp to chart(F ) remove E from chart_boundaries remove 
non-extremal edges from chart_boundaries, // (i.e. edges that do not link two other chart boundary edges) 
add the halfedges of Fopp belonging to chart_boundaries to Heap elseif ( chart(Fopp) =.chart(F ) and 
max_dist(chart(F )) -dist(F )< e and max_dist(chart(Fopp)) -dist(F )< e ) then merge chart(F ) and chart(Fopp) 
 end // if end // while end // expand_charts Algorithm 2: Charts growing. Charts are merged if they 
meet at a small distance d<e from their seed. In our experiments, e = maxdist/4, where maxdist denotes 
the global maximum of distance_to_features. Our charts growing algorithm (Algorithm 2) uses the following 
data:  distance_to_feature is stored in each facet F , and denoted dist(F );  for each chart C, the 
scalar max_dist(C) denotes the maximum distance to features for all the facets of C;  The set of edges 
chart_boundaries represents the borders of all charts. It makes it possible for a patch to be its own 
neighbor while remaining a topological disc. As shown in Figure 5, our algorithm can detect cylindrical 
shapes, as the approach proposed in [15]. In our case, two con­.gurations can be distinguished:  The 
cylinder corresponds to an extremity, such as the .ngers of the dinosaur s wings (Figure 5-B). The corresponding 
charts have the shape of a sock. This con.guration is detected by com-  Figure 6: A: The dinosaur s 
head made of a single chart; B: Despite the absence of triangle .ips, overlaps may occur, caused by self-intersections 
of the border; C: They can be removed by subdividing the chart. puting the area/perimeter ratio. In this 
case, to facilitate the pa­rameterization phase, a cut is added by starting from the seed and cutting 
the edges along the steepest-descent path. The cylinder is non-extremal, and therefore non-capped. Algo­rithm 
2 generates suitable boundaries, without requiring any spe­cial treatment (the resulting chart is rolled 
around the cylinder). 3.3 Validate Charts The so-constructed charts are then parameterized using the 
method presented in Section 2. After that, the following two criteria are tested: As mentioned in Section 
2, no triangle .ip can occur, and the border can be extrapolated. However, since the border can be non-convex, 
a new class of overlaps can be encountered. They are caused by a self-intersection of the border, as 
in [28]. Such con.gurations can be ef.ciently detected by the hardware, by drawing the parameter space 
in stencil mode. The stencil pixels drawn more than once correspond to overlaps. If such overlaps are 
detected, the corresponding chart is subdivided, by cutting it along the edges on the border of the overlapped 
zone, as shown in Figure 6. Note that our segmentation algorithm would not generate a single chart for 
the dinosaur s head (see Figure 5). In practice, the overlap problem has seldom appeared in our exper­iments, 
and was caused by tiny loops formed by the border.  Our criterion respects angles very well, as shown 
in the results section. As far as areas are concerned, large charts with zones of high curvature may 
result in large area variations. To detect these problems, the minimum and maximum model area/texture 
area ratio is measured over the facets. If the max/min ratio is greater than a certain threshold, the 
concerned chart is split, by growing two charts from the facets corresponding to the minimum and the 
maximum. In the examples shown here, the threshold has been set to 2.   4 PACKING Once the model is 
decomposed into a set of parameterized charts, it is possible to create a texture atlas by merging all 
the (u, v) domains of the charts. Usually, only a limited amount of texture memory is available. It is 
then suitable to minimize the unused space. In other words, given a set of possibly non-convex polygons, 
we want to .nd a non-overlapping placement of the polygons in such a way that the enclosing rectangle 
is of minimum area. The so-obtained texture coordinates are then re-scaled to .t the size of the texture. 
The packing problem is known to be NP-complete (see, e.g., [21] and [22]). Approaches based on computational 
geometry show good performances in terms of minimization of lost area, but are not ef.cient enough for 
large and complex data sets. For this rea­son, several heuristics have been proposed in computer graphics. 
For instance, in the method proposed by Sander et al. [27], the bounding rectangles of the charts are 
packed. In our case, since Harmonic Maps LSCM stretch (before optim.) 3.2 3.5 stretch (after optim.) 
1.65 1.52  Table 1: Stretch optimization of the cow head data set  AB Figure 7: A: Our packing algorithm 
inserts the charts one by one, and maintains the horizon (in blue) during the process. Each chart (in 
green) is inserted at the position minimizing the wasted space (in black) between its bottom horizon 
(in pink) and the current horizon. The top horizon (in red) of the current chart is then used to update 
the horizon. B: Result on the dinosaur data set. the border may have an arbitrary shape, the bounding 
rectangle is not an accurate approximation. For this reason, we propose a different algorithm, that packs 
the charts directly rather than their bounding rectangles. As in [2], our algorithm is inspired by how 
a Tetris player would operate, but without approximating the charts by their bounding boxes: 1. Each 
chart is rescaled to make its area in (u, v) space equal to its area in (x, y, z) space. 2. The maximum 
diameter of the charts are oriented vertically and sorted in decreasing order.  .3.AsshowninFigure7,foreachchart 
,thetophorizon ChC.red)andbottomhorizon (inpink)iscomputed.Ratherthan h C (in being represented by their 
bounding rectangles, the charts are ..approximatedbytheareabetweenthetwocurves andhhC C As in classical 
approaches, in order to avoid unwanted blends caused by mip-mapping, an additional margin is added to 
the horizons. 4. The charts are inserted one by one, using the method described below. As shown in Figure 
7-A, the piecewise linear function h(u) rep­resenting the horizon is maintained by the algorithm. For 
each chart C, the uC coordinate at the lower left corner of C is cho­sen in such a way that the lost 
space (in black) between the bottom .  ter space is discretized into texels, it seems natural to represent 
the  .horizon hChorizonsbyarraysofdiscretizedvalues,withtextureresolution. This makes the algorithm 
much simpler than using piecewise linear functions. For a chart C, all discrete values for uC are tested. 
The algorithm performs well, and takes less than one second to process all the data sets we have tested. 
 5 RESULTS Figure 8: Data sets and associated texture space constructed by our method. We have applied 
our method to different data sets, comprising of C (in pink) and the current horizon h (in blue) is minimized 
(see Figure 7-A). Then, the horizon h is updated using .thetophorizon hC of the current chart (in red). 
Since the parame­ meshes created with a 3D modeler (using subdivision surfaces) and scanned meshes. The 
results are equivalent to those obtained with MIPS [10], but with mathematical guarantees, and can be 
more ef­ .ciently computed. As shown in Table 1, the stretch (see [27]) measured on the result is of 
the same order as when using stan­dard methods (e.g. [4, 6]). To optimize the mapping, it is easy to 
post-process the result of our method by the algorithm proposed in [27]. Since the border nodes are naturally 
positionned (rather than arbitrarily .xed on a convex polygon), the result can be better (see Table 1). 
Figure 8 shows some texture atlases. Note the presence of small charts, most of them corresponding to 
geometric details of the mod­ dinosaur skull bunny horse . vertices 14,669 16,949 34,834 48,485 . facets 
14,384 15,124 69,451 96,968 . charts 43 40 23 44 segmentation time (s) 8 17 30 43 parameterization time 
(s) 10 23 95 190 packing ratio (rectangles) 0.48 0.51 0.43 0.37 packing ratio (our algo.) 0.55 0.55 0.6 
0.58 stretch (before optim.) 2.9 2.5 1.16 1.14 stretch (after optim.) 1.26 1.55 1.14 1.12 Table 2: Statistics 
and timings. els (teeth, hoofs . . . ). This is not a problem for most paint systems,  Figure 9: Angle 
and area deformations histograms ( Horse data set). that can treat them properly. Some examples of textured 
models are shown in Figure 10. Table 2 shows the sizes of the data sets, the number of created charts, 
and the following statistics, obtained on a 1.3 GHz Pentium III (note that the timings for the packing 
algorithm are not included, since they are negligible): time to segment the model into charts;  time 
to parameterize the charts. Our LSCM criterion (Equation 4) is minimized using the CG (Conjugate Gradient) 
algorithm. The independence to resolution suggests that a multi-grid ap­proach would be even more ef.cient; 
 packing ratio obtained using an enclosing rectangle packing ap­proach [27] and our algorithm.  stretch 
measured before and after applying Sander et. al. s opti­mization method as a post-processing (see [27]). 
 The left histogram in Figure 9 shows the distribution of the an­ gles in degrees between u and v gradient 
vectors. The mapping is nearly conformal in each triangle (the differences of lengthes be­tween the u 
and v gradients we have measured are very near to zero). The right histogram shows the area deformations 
obtained with the Horse data set, before stretch optimization. This his­togram, showing texture area/model 
area ratios has been normal­ized, i.e. scaled in such a way that the mean value is mapped to 1. Note 
that since the mapping is nearly isotropic in each triangle, the L2 and L8 stretch histogram (not shown 
here) have exactly the same appearance as the area histogram. As can be seen, even though our LSCM criterion 
is not designed to punish area defor­mations, few facets are distorted, and can easily be .xed by post­processing 
using Sander et. al. s method. The resulting texture at­ lases combine the advantages of LSCM (few chart 
discontinuities) and stretch-optimized parameterization (uniform sampling).  CONCLUSION In this paper, 
we have presented a new automatic texture atlas gen­eration method for polygonal models. Overall, we 
have proposed a complete and mathematically valid solution to the parameterization of complex models 
which proved to be more ef.cient and robust than existing methods and available tools in real production 
envi­ronments. Our segmentation algorithm, driven by detected features and inspired by Morse theory, 
decomposes the model into charts with natural shapes, corresponding to meaningful geometric enti­ties. 
These two algorithms may have applications in other domains, such as re-meshing and data compression. 
We have successfully applied our technique to both scanned and synthetic data sets, mak­ing it possible 
to use existing 3D paint systems with them (Deep-Paint3D, Painter). In future works, to parameterize 
huge models, we will consider out-of-core algorithm, and analyze different nu­merical methods to minimize 
the LSCM criterion, including multi­grid approaches and pre-conditioned CG. Including the stretch cri­terion 
directly into the LSCM criterion is also another possible fu­ture direction of research. ACKNOWLEDGMENTS 
We want to thank the Graphite development team (http://www.loria.fr/ levy/graphite), espe­cially Ben 
Li and Bijendra Vishal. Thanks also to the reviewers for their comments and help in improving this paper. 
A PROPERTIES OF LSCMS The minimization problem of Section 2 has several interesting properties when the 
number p of pinned vertices in parameter space is suf.cient. In what follows, T is assumed to be homeomorphic 
to a disc. A.1 Full Rank We .rst show that the matrices Mf and A have full rank when p . 2 (p denotes 
the number of pinned vertices). For this, recall that a triangulation that is topologically a disc can 
be incrementally constructed with only two operations (cf. Fig­ure 11): the glue operation creates one 
new vertex and one new face, and the join operation creates one new face. Thus, incremen­tal construction 
creates at most as much vertices as faces. Since the simplest triangulation (one triangle) has one face 
and three vertices, we have that n . . n - 2 (where n denotes the number of vertices, and n . the number 
of triangles, as in the rest of the paper). We .rst show that the rank of Mf is n - p when p . 2. First 
note that since n . . n - 2, min (n . ,n - p)= n - p if p . 2 and the rank of Mf is at most n - p. We 
assume that T is incrementally constructed with glue and join operations and prove the result by induction 
on the size of Mf . We also assume, without loss of generality, that the p pinned vertices are concentrated 
in the initial triangulation. Let ni. ,ni - p be the dimensions of the matrix Mf (i) at step i. Observe 
that since T is a non-degenerate triangulation, none of the coef.cients Wj,Ti is zero. At step 0, the 
triangulation has n0 - p =1 vertices and n0 . . 1 triangles. M(0) f has a single column and, since T 
is a proper triangulation, some of its coef.cients are non-zero and it has rank 1= n0 - p. Assume that 
the property holds after step i. If step i +1 is a join, then the number of rows of Mf (i) grows by 1 
while the number of columns is unchanged, so the rank is ni+1 - p = ni - p. If step i +1 is a glue, a 
new vertex vi+1 and a new triangle T are added. (i+1) Let v1 and v2 be the other vertices of T . The 
new matrix Mf is as follows: . . 0 A.A is a square 2(n - p) × 2(n - p) matrix, it is thus invertible 
and the minimization problem has a unique solution (when p . 2) -1A. x =(A.A)b. The minimum of C(U) 
is zero when Ax = b, i.e. when A is invertible. Since it has full rank, this happens exactly when A is 
square, i.e. when n . = n - p. Using the fact that n . . n - 2, this implies that p =2. We conclude that 
the mapping U is fully conformal (barring self-intersections) exactly when p =2 and the triangulation 
T is built only with glue operations. A.3 Invariance by Similarity We now prove that if U is a solution 
to the minimization prob­lem, then zU + T is also a solution, for all z . C and T = (z . ,...,z .),z 
. . C. In other words, the problem is invariant by a similarity transformation. First note that the vector 
H = (1,..., 1). is trivially in the kernel of M, since W1 + W2 + W3 =0 in each triangle. Assume U is 
a solution of the problem. We get: C(zU + T)= zz C(U)+2zT * CU, = zz C(U)+2z(MT) * MU = zz C(U), because 
T = z .H is in the kernel of M. If C(U)=0, then C(zU + T)=0. A.4 Independence to Resolution We now show 
that if a given mesh is densi.ed , then the solution to the augmented optimization problem restricted 
to the vertices of the initial mesh is the same. We prove this result when a single triangle . (i) . 
M ..... ..... T is split into three triangles, but the proof generalizes easily to . a more general 
setting. So let v be the new vertex introduced in triangle T , i.e. as a linear combination of vertices 
v1, v2, v3: . f 0 Wv1,T Wv2,T Wi+1,T v 0 ··· 0 dT dT dT 33 It is now easy to see that its columns are 
linearly independent. In-v = aivi,ai =1,ai > 0. deed, assume there are complex numbers .j such that i=1 
i=1 ni+1 (i+1) Assume also for the sake of simplicity that none of v, v1, v2, v3 .j m=0, (5) j is pinned. 
Call Ti (i =1,..., 3) the triangle created that does not j=1 have vi as vertex. Then it is easy to see 
that dTi = aidT . (i+1) (i+1) Mf is an n . × (n - p) matrix. After insertion of v, the new where the 
mare the column vectors of M. If we jf matrix M+ f is (n . + 2) × (n +1 - p). Indeed, one vertex is 
added, look at the .rst ni coordinates of the column vectors, then Equa­ augmenting the number of columns 
by one, and three new triangles ni (i) 0,j =1,...,ni, since Mf (i) has full rank. Now the equa­ tion 
5 reduces to 0, which implies that .j = replace an old one, augmenting the number of rows by two. The 
.j m = j j=1 structure of these matrices is as follows: . . (i+1) tion linking the last coordinate of 
the vectors mj reduces to v 0 . . . Nf ..... ..... .ni+1 Wi+1,T /dT = 0, implying that .ni+1 =0. Thus 
the . . .. Nf .. , M+ f = 0 (i+1) Mf = columns of M are linearly independent and the matrix has . f 
 full rank. The result is proved. F 0 ··· 0 0 L P Since Mf has rank n - p, both M1 f and M2 f have rank 
n - p when p . 2. In turn, this implies that A has rank 2(n - p) when p . 2. where Nf is an (n .-1)×(n-p) 
matrix, F is 1×3, L is 3×3 and P v is 3 × 1. If the coef.cients of F are denoted by fj = Wj,T /dT , 
then it is easy to observe that the coef.cients of L =(lij ) and A.2 Single Minimum P =(pi) satisfy 
We now show that, when p . 2, C(U) has a unique minimum. First, notice that 1 1 fi. (6) lij = v aifj 
- aj fi ,pi = v ai ai .C = 2(A.Ax -A.b). .x The (n - p) × 1 solution to the initial problem is: Now, 
since the rank of the Gram matrix of A (i.e. A.A) is the same as the rank of A, A.A has rank 2(n - p) 
when p . 2. Since Uf =(M * f Mf )(-1)Mf * MpUp. Consider the (n +1 - p) × 1 solution to the augmented 
problem: References U+ * M+ M+ * M+U+ =(M+ )(-1)pp . (7) fff f Using the relations of Equation 6 and 
the fact that f1 +f2 +f3 =0, it suf.ces then to observe that U+ f = f ,Uv (U. ). is the (unique) solution 
to 7, where Uv = a1U1 + a2U2 + a3U3. In other words, the least squares conformal parameterization is 
un­changed at the old vertices and is the barycenter of the parameteri­zations of v1, v2 and v3 at the 
new vertex v.  A.5 Preserving Orientations We now sketch the proof that least squares conformal maps 
preserve orientations, i.e. there are no triangle .ips. As a preliminary, note .rst that if complex 
numbers Wi are as­sociated to vertices of a triangle as in Section 2.4, with vertices ordered counterclockwise, 
then .T = iW2W1 - W1W2 (8) is positive (and equal to 2dT ). We again assume that the triangulation T 
is incrementally con­structed with the glue and join operations. Denote the current tri­angulation by 
Ti. For the join operation, the result is trivial. We now prove the result when the current step is a 
glue. We use the notations of Figure 12. Let V and V . be the images of T and T . in parameter space. 
Let also Wj (resp. Wj .) be complex numbers attached to T (resp. T .) and Xj (resp. Xj . ) be complex 
numbers attached to V (resp. V .). Since the local bases of two triangles of T sharing an edge are consistently 
oriented, both .T as de.ned in Equation 8 and W .. . .T . = i 1W2 - W2.W1 are positive. If we assume 
that the unfolding of Ti has no triangle .ips, then we also have that .V > 0, where .V is de.ned as in 
Equation 8, replacing Wj by Xj . Now, writing down the equations de.ning U+ f =(U. f ,Uv). as in the 
previous section, we .nd that W1U1 + W2U2 + WvUv =0, (9) where U1,U2,Uv are the parameterizations of 
vertices v1, v2, v (U1,U2 being unchanged by addition of v). Using the fact that W1 . + W2 . + W3 . =0,X1 
. = U2 - Uv and X2 . = Uv - U1, Equation 9 rewrites as W2X1 - W1X2 =0. (10) Using Equation 10 and the 
de.nition in Equation 8, we have: .V . = iX1. X2 - X2. X1 , X1. X1 . . W .. .. X1. X1 . = i 1W2 - W2.W1 
= .T . > 0. W .W . W .W . 11 11 Thus, V . is consistently oriented and the glue operation does not produce 
a triangle .ip, proving the result. [1] M. Agrawala, A. Beers, and M. Levoy. 3D painting on scanned 
surfaces. In Proc. 1995 Symposium on Interactive 3D Graphics, 1995. [2] Y. Azar and L. Epstein. On 2d 
packing. J. of Algorithms, (25):290 310, 1997. [3] P. Cigogni, C. Montani, C. Rocchini, and R. Scopino. 
A general method for recovering attributes values on simpli.ed meshes. In Proc. of IEEE Visualization 
Conf., pages 59 66. ACM Press, 1998. [4] M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. 
Stuetzle. Multiresolution analysis of arbitrary meshes. In SIGGRAPH 95 Conf. Proc., pages 173 182. Addison 
Wesley, 1995. [5] J. Eells and L. Lemaire. Another report on harmonic maps. Bull. London Math. Soc., 
20:385 524, 1988. [6] M. Floater. Parametrization and smooth approximation of surface triangulations. 
Computer Aided Geometric Design, 14(3):231 250, April 1997. [7] I. Guskov, K. Vidimce, W. Sweldens, and 
P. Schröder. Normal meshes. In SIGGRAPH 00 Conf. Proc., pages 95 102. ACM Press, 2000. [8] S. Haker, 
S. Angenent, A. Tannenbaum, R. Kikinis, G. Sapiro, and M. Halle. Conformal surface parameterization for 
texture mapping. IEEE Transactions on Visualization and Computer Graphics, 6(2):181 189, 2000. [9] P. 
Hanrahan and P. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. In SIGGRAPH 90 Conf. Proc., 
pages 215 223. Addison Wesley, 1990. [10] K. Hormann and G. Greiner. MIPS: An ef.cient global parametrization 
method. In P.-J. Laurent, P. Sablonnière, and L. Schumaker, editors, Curve and Surface Design: Saint-Malo 
1999, pages 153 162. Vanderbilt University Press, 2000. [11] A. Hubeli and M. Gross. Multiresolution 
features extraction from unstructured meshes. In Proc. of IEEE Visualization Conf., 2001. [12] M. Hurdal, 
P. Bowers, K. Stephenson, D. Sumners, K. Rehms, K. Schaper, and D. Rottenberg. Quasi-conformally .at 
mapping the human cerebellum. In Proc. of MICCAI 99, volume 1679 of Lecture Notes in Computer Science, 
pages 279 286. Springer-Verlag, 1999. [13] T. Igarashi and D. Cosgrove. Adaptive unwrapping for interactive 
texture paint­ing. In Symp. on Interactive 3D Graphics, pages 209 216. ACM, 2001. [14] V. Krishnamurthy 
and M. Levoy. Fitting smooth surfaces to dense polygon meshes. In SIGGRAPH 96 Conf. Proc., pages 313 
324. Addison Wesley, 1996. [15] F. Lazarus and A. Verroust. Level set diagrams of polyhedral objects. 
In Proc. of Solid Modeling and Applications, pages 130 140. ACM Press, 1999. [16] A. Lee, W. Sweldens, 
P. Schröder, L. Cowsar, and D. Dobkin. MAPS: Multires­olution adaptive parameterization of surfaces. 
In SIGGRAPH 98 Conf. Proc., pages 95 104. Addison Wesley, 1998. [17] B. Lévy. Constrained texture mapping 
for polygonal meshes. In SIGGRAPH 01 Conf. Proc., pages 417 424. ACM Press, 2001. [18] B. Lévy and J.-L. 
Mallet. Non-distorted texture mapping for sheared triangulated meshes. In SIGGRAPH 98 Conf. Proc., pages 
343 352. Addison Wesley, 1998. [19] P. Lienhardt. Extension of the notion of map and subdivisions of 
a 3D space. In Proc. of 5th Symp. on Theo. Aspects in Comp. Sci., pages 301 311, 1988. [20] J. Maillot, 
H. Yahia, and A. Verroust. Interactive texture mapping. In SIGGRAPH 93 Conf. Proc., pages 27 34. Addison 
Wesley, 1993. [21] V. Milenkovic. Rotational polygon containment and minimum enclosure using only robust 
2D constructions. Computational Geometry, 13(1):3 19, 1999. [22] H. Murata, K. Fujiyoshi, S. Nakatake, 
and Y. Kajitani. Rectangle-packing-based module placement. In Proc. of ICCAD, pages 472 479. IEEE, 1995. 
[23] H. Pedersen. Decorating implicit surfaces. In SIGGRAPH 95 Conf. Proc., pages 291 300. Addison Wesley, 
1995. [24] U. Pinkall and K. Polthier. Computing discrete minimal surfaces and their con­jugates. Experimental 
Math., 2(15), 1993. [25] D. Piponi and G. Borshukov. Seamless texture mapping of subdivision surfaces 
by model pelting and texture blending. In SIGGRAPH 00 Conf. Proc., pages 471 478. ACM Press, 2000. [26] 
E. Praun, A. Finkelstein, and H. Hoppe. Lapped textures. In SIGGRAPH 00 Conf. Proc., pages 465 470. ACM 
Press, 2000. [27] P. Sander, J. Snyder, S. Gortler, and H. Hoppe. Texture mapping progressive meshes. 
In SIGGRAPH 01 Conf. Proc., pages 409 416. ACM Press, 2001. [28] A. Sheffer and E. de Sturler. Param. 
of faceted surfaces for meshing using angle­based .attening. Engineering with Computers, 17(3):326 337, 
2001. [29] Y. Shinagawa, T. Kunii, and Y.-L. Kergosien. Surface coding based on Morse theory. IEEE Computer 
Graphics and Applications, 11(5):66 78, 1991. [30] W. Tutte. Convex representation of graphs. In Proc. 
London Math. Soc., vol­ume 10, 1960. [31] G. Zigelman, R. Kimmel, and N. Kiryati. Texture mapping using 
surface .atten­ing via multi-dimensional scaling. IEEE Transactions on Vis. and C.G., 2001.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566591</article_id>
		<sort_key>372</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Progressive lossless compression of arbitrary simplicial complexes]]></title>
		<page_from>372</page_from>
		<page_to>379</page_to>
		<doi_number>10.1145/566570.566591</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566591</url>
		<abstract>
			<par><![CDATA[Efficient algorithms for compressing geometric data have been widely developed in the recent years, but they are mainly designed for closed polyhedral surfaces which are <i>manifold</i> or "nearly manifold". We propose here a <i>progressive</i> geometry compression scheme which can handle manifold models as well as "triangle soups" and 3D tetrahedral meshes. The method is lossless when the decompression is complete which is extremely important in some domains such as medical or finite element.While most existing methods enumerate the vertices of the mesh in an order depending on the connectivity, we use a kd-tree technique [Devillers and Gandoin 2000] which does not depend on the connectivity. Then we compute a compatible sequence of meshes which can be encoded using edge expansion [Hoppe et al. 1993] and vertex split [Popovi&#263; and Hoppe 1997].The main contributions of this paper are: the idea of using the kd-tree encoding of the geometry to drive the construction of a sequence of meshes, an improved coding of the edge expansion and vertex split since the vertices to split are implicitly defined, a prediction scheme which reduces the code for simplices incident to the split vertex, and a new generalization of the edge expansion operation to tetrahedral meshes.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[coding]]></kw>
			<kw><![CDATA[interactivity]]></kw>
			<kw><![CDATA[mesh compression]]></kw>
			<kw><![CDATA[non manifold meshes]]></kw>
			<kw><![CDATA[progressivity]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P317376</person_id>
				<author_profile_id><![CDATA[81100502461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre-Marie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gandoin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39050686</person_id>
				<author_profile_id><![CDATA[81100609522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Devillers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Geometrica, BP93, 06902 Sophia-Antipolis, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383281</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALLIEZ, P., AND DESBRUN, M. 2001. Progressive compression for lossless transmission of triangle meshes. In SIGGRAPH 2001 Conference Proc., 199-202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALLIEZ, P., AND DESBRUN, M. 2001. Valence-driven connectivity encoding for 3d meshes. In Eurographics 2001 Conference Proc., 480-489.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., CUTCHIN, S., PASCUCCI, V., AND ZHUANG, G. 1999. Error resilient streaming of compressed vrml. Tech. rep., University of Texas.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319426</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., PASCUCCI, V., AND ZHUANG, G. 1999. Progressive compression and transmission of arbitrary triangular meshes. In IEEE Visualization 99 Conference Proc., 307-316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>789628</ref_obj_id>
				<ref_obj_pid>789086</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., PASCUCCI, V., AND ZHUANG, G. 1999. Single resolution compression of arbitrary triangular meshes with properties. Computational Geometry: Theory and Applications, 247-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319358</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COHEN-OR, D., LEVIN, D., AND REMEZ, O. 1999. Progressive compression of arbitrary triangular meshes. In IEEE Visualization 99 Conference Proc., 67-72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DEERING, M. 1995. Geometry compression. In SIGGRAPH 95 Conference Proc., 13-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375262</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEVILLERS, O., AND GANDOIN, P.-M. 2000. Geometric compression for interactive transmission. In IEEE Visualization 2000 Conference Proc., 319-326.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245626</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[EVANS, F., SKIENA, S., AND VARSHNEY, A. 1996, Optimizing triangle strips for fast rendering. In IEEE Visualization 96 Conference Proc., 319-326.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>339645</ref_obj_id>
				<ref_obj_pid>339613</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GU&#201;ZIEC, A., BOSSEN, F., TAUBIN, G., AND SILVA, C. 1999. Efficient compression of non-manifold polygonal meshes. Comput. Geom. Theory Appl. 14, 137-166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280836</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GUMHOLD, S., AND STRASSER, W. 1998. Real time compression of triangle mesh connectivity. In SIGGRAPH 98 Conference Proc., 133-140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319356</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GUMHOLD, S., GUTHE, S., AND STRASSER, W. 1999. Tetrahedral mesh compression with the cut-border machine. In IEEE Visualization 99 Conference Proc., 91-98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897914</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S., AND GARLAND, M. 1997. Survey of polygonal surface simplification algorithms. Tech. rep., Carnegie Mellon University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., AND STUETZLE, W. 1993. Mesh optimization. In SIGGRAPH 93 Conference Proc., 1 9-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>305000</ref_obj_id>
				<ref_obj_pid>304893</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[ISENBURG, M., AND SNOEYINK, J. 1999. Mesh collapse compression. In Symposium on Computational Geometry, 419-420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897956</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[ISENBURG, M. 2000. Triangle fix er: Edge-based connectivity encoding. In 16th European Workshop on Computational Geometry Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344924</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KARNI, Z., AND GOTSMAN, C. 2000. Spectral compression of mesh geometry. In SIGGRAPH 2000 Conference Proc., 279-286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344922</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[KHODAKOVSKY, A., SCHR&#214;DER, P., AND SWELDENS, W. 2000. Progressive geometry compression. In SIGGRAPH 2000 Conference Proc., 271-278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[KING, D., AND ROSSIGNAC, J. 1999. Guaranteed 3.67v bit encoding of planar triangle graphs. In Canadian Conference on Computational Geometry Proc., 146-149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LI, J., AND KUO, C.-C. J. 1998. A dual graph approach to 3d triangular mesh compression. In IEEE International Conference on Image Processing Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614450</ref_obj_id>
				<ref_obj_pid>614277</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PAJAROLA, R., AND ROSSIGNAC, J. 2000. Compressed progressive meshes. IEEE Transactions on Visualization and Computer Graphics 6, 1 (January-March), 79-93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>793088</ref_obj_id>
				<ref_obj_pid>792759</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PAJAROLA, R., AND ROSSIGNAC, J. 2000. Squeeze: Fast and progressive decompression of triangle meshes. CGI 2000 Proc., 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319425</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PAJAROLA, R., ROSSIGNAC, J., AND SZYMCZAK, A. 1999. Implant sprays: Compression of progressive tetrahedral mesh connectivity. In IEEE Visualization 99 Conference Proc., 299-306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258852</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[POPOVI&#262;, J.,., AND HOPPE, H. 1997. Progressive simplicial complexes. In SIGGRAPH 97 Conference Proc., 217-224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J., AND BORREL, P. 1993, Geometric Modeling in Computer Graphics. Springer-Verlag, July, ch. Multi-Resolution 3D Approximations for Rendering Complex Scenes, 455-465.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>339644</ref_obj_id>
				<ref_obj_pid>339613</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J., AND SZYMCZAK, A. 1999. Wrap&zip: Linear decoding of planar triangle graphs. Computational Geometry: Theory and Applications, 119-135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614421</ref_obj_id>
				<ref_obj_pid>614273</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J. 1999. Edgebreaker: Connectivity compression for triangle meshes. IEEE Transactions on Visualization and Computer Graphics, 47-61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836090</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SCHMALSTIEG, D., AND SCHAUFLER, G. 1997. Smooth levels of detail. In IEEE Virtual Reality Annual International Symposium, 12-19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G., AND ROSSIGNAC, J. 1998. Geometric compression through topological surgery. ACM Transactions on Graphics 17, 2, 84-115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280834</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G., GU&#201;ZIEC, A., HORN, W., AND LAZARUS, F. 1998. Progressive forest split compression. In SIGGRAPH 98 Conference Proc., 123-132.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[TOUMA, C., AND GOTSMAN, C. 1998. Triangle mesh compression. In Graphics Interface 98 Conference Proc., 26-34,]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>214771</ref_obj_id>
				<ref_obj_pid>214762</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[WITTEN, I., NEAL, R., AND CLEARY, J. 1987. Arithmetic coding for data compression. Communications of the ACM 30, 6, 520-540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375225</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[YANG, C., MITRA, T., AND CHIUEH, T. 2000. On-the-fly rendering of losslessly compressed irregular volume data. In 11th IEEE Visualization Conference, 329-336.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Progressive Lossless Compression Of Arbitrary Simplicial Complexes Pierre-Marie Gandoin Olivier Devillers. 
quantized 3×5 bits,1% of raw data; 3×6 bits, 3%; 3×12 bits, 20%; lossless, 25 bits/vertex 3×6 bits, 7%; 
3×10 bits, 14%; 3×6 bits, 3%; 3×12 bits, 14%; 3×5 bits, 2%; 3×6 bits, 4%; 3×8 bits, 13%;3×12 bits, 25%; 
lossless, 15 bits/vertex lossless, 18 bits/vertex lossless, 26 bits/vertex Figure 1: Steps in the progressive 
decompression of various models. Abstract Ef.cient algorithms for compressing geometric data have been 
widely developed in the recent years, but they are mainly de­signed for closed polyhedral surfaces which 
are manifold or nearly manifold . We propose here a progressive geometry compression scheme which can 
handle manifold models as well as triangle soups and 3D tetrahedral meshes. The method is lossless when 
the decompression is complete which is extremely important in some domains such as medical or .nite element. 
While most existing methods enumerate the vertices of the mesh in an order depending on the connectivity, 
we use a kd-tree tech­nique [Devillers and Gandoin 2000] which does not depend on the connectivity. Then 
we compute a compatible sequence of meshes which can be encoded using edge expansion [Hoppe et al. 1993] 
and vertex split [Popovi´c and Hoppe 1997]. The main contributions of this paper are: the idea of using 
the kd-tree encoding of the geometry to drive the construction of a se­quence of meshes, an improved 
coding of the edge expansion and vertex split since the vertices to split are implicitly de.ned, a pre­diction 
scheme which reduces the code for simplices incident to the split vertex, and a new generalization of 
the edge expansion opera­tion to tetrahedral meshes. Keywords: Mesh Compression, Non manifold Meshes, 
Coding, Progressivity, Interactivity . INRIA Geometrica, BP93, 06902 Sophia-Antipolis, France. Pierre-Marie.Gandoin 
Olivier.Devillers@sophia.inria.fr Copyright &#38;#169; 2002 by the Association for Computing Machinery, 
Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for commercial advantage 
and that copies bear this notice and the full citation on the first page. Copyrights for components of 
this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 1 INTRODUCTION Compressing data manipulated by computers 
has always been, and stays, a crucial necessity since the amount of data grows as fast as the size of 
computer storage. After text, sound and images, the compression of geometric structures is a new challenge 
both for storage and for visualization and transmission over the network. For this latter application, 
we would like to design compression schemes that are progressive, where the information is organized 
such that a coarse model can be visualized before the transmission is complete. Most often, a geometric 
structure consists of a set of points, often referred to as the geometry (or vertex positions) and the 
connectiv­ity (or topology), composed of the adjacency relations between the vertices. The description 
is sometimes also completed by a list of attributes (normals, colors, textures). 1.1 Related Work 1.1.1 
Connectivity Driven Approach The main dif.culty in the design of a compression scheme is to achieve good 
compression rates for both geometry and connectivity. Regarding the single resolution (non progressive) 
geometry com­pression, which dates back to an article by Deering in 1995 using generalized triangle strips 
[Deering 1995], all the methods give pri­ority to the connectivity coding. The common intuitive idea 
is to describe a spanning tree of vertices and faces by reordering the ver­tices according to a deterministic 
strategy to traverse the mesh. This strategy constructs a sequence where each vertex is associated to 
a label (Rossignac s edge-breaker [1999]), or some other additional information such as the degree of 
the vertex (Touma and Gotsman s algorithm [1998]), describing the way this vertex is connected to the 
previous ones in the sequence. Thus, the order of enumeration of the vertices is imposed by the connectivity 
and the geometric part of the coder tries to get some additional gain, using differential cod­ing or 
positions prediction: instead of being speci.ed with absolute coordinates, the new vertex position can 
be expressed relatively to its predecessors, using a difference vector or some linear predictor. Most 
of the single-rate compression methods [Deering 1995; Evans et al. 1996; Taubin and Rossignac 1998; Gumhold 
and Strasser 1998; Touma and Gotsman 1998; Rossignac 1999; Rossignac and Szymczak 1999; King and Rossignac 
1999; Isenburg and Snoeyink 1999; Isenburg 2000; Li and Kuo 1998; Bajaj et al. 1999a; Bajaj et al. 1999c; 
Gumhold et al. 1999; Alliez and Desbrun 2001b] fol­low this framework and reach costs as low as 2 bits 
per vertex on average for the most ef.cient connectivity coders. Historically, progressive compression 
methods .nd their origin in mesh simpli.cation. The general idea is to create a decimation sequence from 
the original mesh using some canonical operator (vertex or face removal, edge collapse, vertex uni.cation) 
yielding a very coarse version of the mesh. Furthermore, this decimation sequence is driven by a criterion 
optimizing the choice of the deci­mated elements in order to maximize the rate/distortion ratio. Thus 
if the coarse model is transmitted, followed by a sequence of re­.nements describing .ner models, the 
client, by truncating the bit stream at any point, is guaranteed to obtain the best approximation of 
the original object. Unfortunately, such a hierarchical organiza­tion often leads to a signi.cant overhead 
cost in bit size. This is why simpli.cation algorithms were not initially used as compression methods 
(an overview of these algorithms can be found in the very complete survey of Garland and Heckbert [1997]). 
The .rst meth­ods for progressive geometric compression are extensions of single­rate methods [Taubin 
et al. 1998; Bajaj et al. 1999b]. By grouping the re.nement operations in batches, some techniques yield 
results relatively close to those of single resolution methods. For instance, the algorithm proposed 
by Pajarola and Rossignac [2000a; 2000b] uses vertex bit-marking instead of the costly explicit coding 
of the vertex indices. The two most ef.cient methods to our knowledge are based on the vertex removal 
operator, followed by a canonical retriangulation allowing the decoder to identify the patches result­ing 
from the deletions. The .rst one is due to Cohen-Or, Levin and Remez [1999], and uses triangle coloring 
to code the patches. This technique results in connectivity costs of around 6 bits per vertex on usual 
models, but the strip retriangulation produces in­termediate meshes whose visual quality is unsatisfactory. 
To avoid this problem, Alliez and Desbrun [2001a] propose to preserve the regularity of the mesh during 
the simpli.cation by maximizing the number of degree 6 vertices. To this aim, the algorithm alternates 
 the main decimation phase with a regularization phase where de­gree 3 vertices are removed. Besides 
the better quality of the tran­sitional meshes, this method compresses the connectivity of usual meshes 
(nearly manifold) down to 3.7 bits per vertex on average. As in single-rate methods, the geometry coding 
follows from the connectivity coding and is generally based on a linear local predic­tion. Some of these 
methods can handle non manifold meshes of small genus, either by coding explicitly the changes in topology 
or by stitching manifold patches [Gu´ eziec et al. 1999], which gen­erally induces important overcosts. 
It is also important to note that much lower bit-rates can be reached when the algorithm begins with 
a complete remeshing of the model to generate regularity and uniformity [Khodakovsky et al. 2000; Karni 
and Gotsman 2000], which is not admissible in many practical applications where data loss is prohibited. 
 1.1.2 Geometry Driven Approach Schmalstieg and Schau.er [1997], following Rossignac and Bor­rel [1993] 
have tackled the problem from a completely different point of view. They group vertices in clusters on 
a geometric ba­sis and merge them to construct different levels of details. How­ever, the main goal in 
their approach is to obtain continuity between coarse to .ne approximations of the object, and the achieved 
com­pression ratios are not competitive with the current state of the art. In a previous paper [Devillers 
and Gandoin 2000], we have adopted a similar approach: observing that the geometry is, bitwise, the most 
expensive part of a mesh, and that in many cases, the connec­tivity can be automatically reconstructed 
from the vertex positions, we designed an ef.cient, purely geometric coder, which constitutes the starting 
point of our present work. The algorithm, valid in any dimension, is based on a kd-tree decomposition 
by cell subdivision. Given n 2D points with integer coordinates on b bits, the starting cell is a rectangular 
bounding box of size 2b by 2b . The algorithm starts by encoding the total number of points n on an arbitrary 
.xed number of bits (32 for example). Then it starts the main loop, which consists in subdividing the 
current cell in two halves along the hor­izontal axis and then coding the number of points contained 
in one of them (the left one for example) on an optimal number of bits: if the parent cell contains p 
points, the number of points in the half­cell, which can take the p +1 values 0, 1,...,p, will be coded 
on log2 (p + 1) bits using arithmetic coding [Witten et al. 1987]. The number of points contained in 
the second half-cell does not have to be explicitly encoded since it can be deduced from the total number 
and the number transmitted for the .rst half-cell, after which each one of the two resulting cells is 
subdivided along the vertical axis according to the same rule. The process, depicted in Figure 2, iter­ates 
until there is no non-empty cell greater than 1 by 1. As shown on Figure 2 (in yellow), the corresponding 
coding sequence con­sists only of the numbers of points. The positions of these points are hidden in 
the order of the output. As the algorithm progresses, the cell size decreases and the transmitted data 
lead to a more accu­rate localization. The worst case for the algorithm has been proven to correspond 
to an uniform distribution of points in the bounding box. In the latter case, the gain is equal to n 
(log2 n - 2.402) bits. In practice, the method takes advantage of structured distributions containing 
variations of local density, which yields generally much better performances (see Section 2.5). The method 
works in any dimension and in the sequel we will use it for points in dimension 3. 32 bits 3 2.3 1.6 
2.3 2 1 1 1 1 2 1 log8 log4 log5 log2 log3 ... Figure 2: The geometry coder on a two-dimensional example. 
Regarding the compression of the connectivity, we propose two alternatives. The .rst one is to reconstruct 
the connectivity from the geometry, which is reasonable in special cases such as terrain models or densely 
sampled objects. The second possibility is an edge-based connectivity coder, but the proposed technique 
handles only edges and not higher dimensional faces. It is rather expensive and spend more than 12 bits 
per vertex for a triangular mesh, these bad performances restrict this technique to very speci.c applica­tions 
for sparse meshes with few edges.  1.2 Overview In this article, we present a new algorithm for connectivity 
coding, compatible with the kd-tree geometry coder described above. The general idea is to .rst run the 
geometry coder splitting the cells without taking the connectivity into account, then, when the full 
precision is reached, the connectivity of the model can be added and we can run the splitting process 
backwards, merging the cells and deducing connectivity between the cells of coarser models (Sec­tion 
2.1). Connectivity changes between successive models can be encoded by symbols inserted between the numbers 
of the code of Figure 2. The changes of connectivity can be described by two well-known decimation operators 
originally used in a surface simpli.cation context: edge expansion and vertex split. The edge expansion 
gen­erates short codes in locally manifold cases, while the more expen­sive (but more general) vertex 
split operator allows us to treat gen­eral models and even unconnected 3D objects like triangle soups 
. Compared to classical use of these two operators, we get a cheap description for two reasons: .rst, 
in our case, the vertex to be split is implicitly de.ned and does not need to be referenced explic­itly; 
the second reason is the use of prediction techniques which improve the compression of the connectivity 
by about 50% (Sec­tion 2.4). We get bit-rates of 8 bits per vertex for the connectivity in the non manifold 
cases and bit-rates as low as 3 bits per vertex for nearly manifold meshes usually handled by the geometric 
compres­sion community. In the manifold case, we are competitive with the most ef.cient published algorithms 
[Pajarola and Rossignac 2000b; Cohen-Or et al. 1999; Alliez and Desbrun 2001a], while we reach a continuity 
in the bit-rate with respect to the manifold/non manifold axis using a uni.ed encoder (Section 2.5). 
Furthermore, we show how the method can be extended to vol­umetric meshes. To this aim, we de.ne a new 
operator for edge expansion and propose an ef.cient encoding for it. This approach improves the best 
progressive method reported for tetrahedral com­pression [Pajarola et al. 1999] by 25% (Section 3). We 
.nally dis­cuss the possibility to adapt the method to polygonal meshes and conclude in Section 4.  
 2 THE CONNECTIVITY CODER 2.1 Principle Of The Algorithm Starting from the principle described in Section 
1.1.2, the key idea consists in de.ning a connectivity between the cells of the kd-tree to approximate 
the connectivity of the original 3D model. Then the geometric code is enriched: to the number of vertices 
in the .rst half of a split cell is appended a code which describes how the connectivity with other cells 
evolves during the split. There are now two different problems: on the one hand, we have to associate 
connectivity to coarse levels where the cells contain several points, on the other hand, the way the 
connectivity evolves has to be coded ef.ciently. Let our model be composed of a point set and a set of 
simplices (edges and triangles). If we consider some intermediate step of the construction of the kd-tree, 
we embed the connectivity of the model on the set of cells by creating edges and triangles between the 
cells if they exist in the original model between the points contained by these cells. This sequence 
of connectivities for the sequence of sets of cells is constructed in the .ne to coarse direction, going 
back through the subdivision process up to the biggest cell (the object s bounding box), using cell merging. 
After each merge, the informa­tion required by the decoder to restore the original connectivity is encoded. 
When two cells are merged, they are replaced by a parent cell. Moreover, each cell is identi.ed to its 
center-point, representative of all vertices contained in the cell. Therefore, merging two cells is equivalent 
to unifying the two vertices respectively representing them. Accordingly, in the following, we will use 
tools and vocab­ulary originating from progressive mesh simpli.cation. Basically, the vertex merging 
is performed by the two following decimating operators: edge collapse, originally de.ned by Hoppe et 
al. [1993] and widely used in surface simpli.cation (but also for compression pur­poses by Pajarola and 
Rossignac [2000a; 2000b]), will be used to merge two adjacent cells under some hypotheses. The two end­points 
of the edge are merged, which leads to the deletion of the two adjacent triangles (only one if the edge 
belongs to a mesh boundary) Figure 3: The edge collapse. Figure 4: The vertex uni.cation. degenerating 
in .at triangles (Figure 3). vertex uni.cation, as de.ned by Popovi´ c and Hoppe [1997], is a much more 
general operation that will allow us to merge any two cells even if they are not adjacent in the current 
connectivity; the result is non manifold in general (Figure 4). Each of these operators has a reverse 
operation: the edge expan­sion and the vertex split, and their ef.cient coding will be described in detail 
in Section 2.2. In the surface simpli.cation literature discussed previously, the algorithm usually has 
complete freedom to choose the mesh ele­ments on which the decimating operation is applied. This has 
two main consequences on the methods. On the one hand, it is possible to optimize the decimation in order 
to best approximate the orig­inal surface as long as possible. Thus to minimize the geometric distortion, 
a priority queue containing the mesh components is dy­namically maintained, and at each decimation, the 
item minimizing a proximity criterion to the original mesh (or sometimes to the pre­vious version of 
the mesh) is chosen. On the other hand, in order to let the decoder know which component have been deleted 
and must be restored, an additional code describing the index of the compo­nent among the whole current 
set must be output. In our case, on the contrary, the edge to be collapsed or the ver­tices to be uni.ed 
are implicitly speci.ed by the subdivision order of the geometric coder. The decoding algorithm uses 
this implicit de.nition and the cost of specifying the vertices to apply the op­erator is avoided. The 
connectivity coder simply generates a sym­bol identifying which of the two operators has been used, followed 
by the parameters detailing the way this operator has modi.ed the connectivity of the current set of 
cells. The next section shows how these parameters can be ef.ciently encoded. 2.2 Coding Of The Decimation 
Operators 2.2.1 Edge Collapse The edge collapse operator is very inexpensive in terms of coding, but 
can be applied only in a quite restrictive context: not only the vertices to merge have to be adjacent, 
but also the neighborhood of the contracted edge must be manifold and orientable. Under these hypotheses, 
an edge collapse results in the loss of the two adjacent faces, and the necessary information to code 
the reverse operation the edge expansion consists of the indices of two edges (VN2 and VN7 in Figure 
5) among the edges incident to the merged ver­tex V (VN1 to VN10 ). Actually, since the neighborhood 
of V is manifold and orientable, the two edges speci.ed unambiguously split the adjacent simplices in 
two subsets, one of which will be attached to V1 , the other to V2 . If the merged vertex has a degree 
d, the description of the two () edges to be expanded into triangles costs log22d . For an average degree 
equal to 6, and with arithmetic coding, this leads to a code size smaller than 4 bits. However, in the 
context of the cell subdivi­sion, an additional bit is necessary to complete the description. In fact, 
when a cell is halved, its center-point is split into two vertices whose positions are .xed to the centers 
of the sub-cells. Conse­quently, the decoder needs to know which vertex is connected to the red simplices 
(or equivalently, which one is connected to the blue simplices, see Figure 5). Figure 5: The edge expansion. 
In Section 2.4, we will see how simple prediction techniques for the two edges expanded into triangles 
can result in edge collapse coding using less than 3 bits per vertex on nearly manifold and regular enough 
meshes. 2.2.2 Vertex Uni.cation When the vertices to be merged are not adjacent, or when their neighborhood 
possesses a complex topology, a more general op­erator is used: the vertex uni.cation, introduced by 
Popovi´c and Hoppe [1997] in Progressive Simplicial Complexes, and whose re­verse operation is called 
generalized vertex split. Using this op­erator allows us to simplify but also, in the framework of this 
article, to ef.ciently compress any simplicial complex (i.e. any set containing simplices of dimension 
0 (points), 1 (edges), 2 (trian­gles), 3 (tetrahedra), and so on), which is much more general than the 
connected manifold case usually handled by previous geometric compression methods. The counterpart of 
this genericity is that without careful coding, the description of the generalized vertex split can be 
extremely ex­pensive. Indeed, the principle is to exhaustively detail the evolution of the simplices 
incident to the vertex V to be split. More precisely, when V1 and V2 are uni.ed into V , any simplex 
S incident to V1 or V2 is replaced by S' =(S \{V1 ,V2 }) U V (in the case where S' is obtained several 
times, only one occurrence remains). Con­sequently, the coding sequence associated to the uni.cation 
must provide the decoder with the information required to reconstruct the original simplex set. In order 
to do so, each simplex S' incident to V receives a symbol from 1 to 4 determining its evolution after 
V has been split: code 1: S' becomes S1 incident to V1 ;  code 2: S' becomes S2 incident to V2 ;  
code 3: S' becomes S1 incident to V1 and S2 incident to V2 ;  code 4: S' becomes S1 incident to V1 , 
S2 incident to V2 , and S of dimension dim(S' )+1 incident to V1 and V2 . (see Figure 6).  dim 0  
dim 1 split V V1 V2 V1 V2 V1 V2 V1 V2  code 1 code 2 code 3 code 4 dim 2 split   V1 V2 V1 V2 V1 
V2 V1 V2 Figure 6: Examples of generalized vertex split. If naively encoded, this description can lead 
to prohibitive costs of about 30 bits. Popovi´c and Hoppe propose to optimize the cod­ing by two means. 
First, they observe that the simplex codes are not independent. Their mutual interaction can be summarized 
by the following two rules: i) if a simplex S has code c E{1, 2}, all the simplices adjacent to S with 
dimension dim(S)+1 have code c; ii) if a simplex S has code 3, none of the simplices adjacent to S with 
dimension dim(S)+1 have code 4. By coding the sim­plices adjacent to a vertex to be split by ascending 
dimensions, and applying these rules and their contrapositives, the cost is reduced by 50% on average. 
The second optimization suggested by the au­thors is related to entropy coding. Given a simplex S of 
dimension d with possible codes c1 to ck , k : 4 (some codes can be known impossible accordingly to rules 
i) and ii)), the codes c1 to ck have not the same probability. Therefore, a statistical array is .lled 
dur­ing the decimation process, then the sequence is reversed and the .nal codes are optimally output: 
for each simplex, the probability distribution corresponding to its dimension, its potential codes, and 
its actual code, are sent to the arithmetic encoder. This yields a new gain of about 50% which lowers 
the .nal cost down to 8 bits per vertex for usual 3D objects.   2.3 Analysis And Features The decimation 
algorithm starts from the set of separated vertices with their original connectivity, and performs successive 
cell/vertex merging according to the .ne to coarse sequence of subdivisions generated by the geometric 
coder, until one vertex representative of the whole point set and centered in the bounding box is obtained. 
In this section, we give an experimental analysis of the percentage of vertex splits and edge expansions 
obtained in that context. Since n - 1 decimation operations are necessary to completely decimate a set 
of n points, the global cost of the connectivity coding in bits per vertex is almost: Cseparation + Ccollapse 
Pcollapse + Cunif Punif where Cseparation is the size of the header specifying which re­.nement operator 
is used, Ccollapse and Cunif are the respective costs of the re.nement descriptions, Pcollapse and Punif 
are the respective percentages of occurrences of the operators in the cod­ing sequence. The performance 
of the algorithm thus depends on Pcollapse and Punif ; Table 1 shows some statistics for typical mod­els 
(the last row gives the size of the separation code arithmetically encoded). In usual surface simpli.cation 
methods where the deci­mating items are explicitly speci.ed, they are chosen to give prior­ity to edge 
collapse on vertex uni.cation; here, we do not have this freedom, but we still observe that on nearly 
manifold models, the edge collapse is quite predominant in the decimation process (up to 96% of the operations). 
 VV1 V2 V2 models triceratops blob fandisk bunny horse average number of vertices 2832 8033 6475 35947 
19851 73138 edge collapse 76.4% 90.9% 96.0% 93.0% 92.4% 92.2% vertex uni.cation 23.6% 9.1% 4.0% 7.0% 
7.6% 7.8% separation cost 0.79 bit 0.44 bit 0.24 bit 0.37 bit 0.39 bit 0.39 bit Table 1: Decimation 
operators percentages. The cell subdivision principle that governs the algorithm makes it intrinsically 
well suited to progressive visualization purposes. As the decoding progresses, cell sizes decrease and 
the received data allow us to localize the points with more accuracy, and simultane­ously, to smooth 
the object by incorporating the new connectivity information. Therefore it is possible to visualize the 
set of points at any stage of the decoding, with smooth transitions between the suc­cessive versions 
using geomorphs. Moreover, the precision over the point coordinates is controlled since it is equal to 
half the current cell size. Besides progressivity, the advantage of the method is to pro­vide advanced 
interactivity to the user. For instance, the geometric coder does not impose an a priori quantization 
of the coordinates: the server can compress the points on as many bits as necessary to preserve the original 
.oating point precision of the model, and the client asks for re.nement data until he/she considers the 
accuracy suf.cient for his/her needs. Furthermore, since the cells are struc­tured in a kd-tree, it is 
possible, during decoding, to select one or more subsets of the scene and to re.ne them selectively. 
Hence an interactive navigation through a complex 3D scene is optimized in terms of quantity of data 
transmitted. Moreover, the connectivity coder can ef.ciently handle any sim­plicial complex in dimension 
d (see Section 3 for the volumetric meshes extension). In particular, it is important to note that any 
geometric structure that can be described as a set of edges (i.e. 1­dimensional simplices) is manageable 
by our algorithm. For in­stance, this can provide a way to progressively encode polygo­nal surface meshes 
(in this case however, a post-process phase is needed to reconstruct the polygons by searching the edge 
loops). Thus, the edge-based connectivity coder proposed in our previous paper [Devillers and Gandoin 
2000] appears as a particular case of our method. Figure 7 gives an example of connectivity code in­serted 
in the geometric code of Figure 2. The portion of code in the .gure starts with a geometric vertical 
split with 3 points on the left, followed by a code of edge expansion and the indices of the two incident 
edges (1 and 4) expanded in triangles; the three next horizontal geometrical split (001) involves cells 
with only one point and does not need connectivity code; the last code is a horizontal geometrical split 
with 2 points above followed by a code of gener­alized vertex split and the connectivity splitting code 
for the vertex, the edges and the incident triangles, the simplices are colored on the .gure accordingly 
to their splitting code (due to compatibility rules, only one triangle is really coded). 2 3 3 1 2 3 
4 5 edge expansion: 1, 4 generalized vertex split: 2 41 4 111 222 10 0 0 0 1 44 Figure 7: Geometry and 
connectivity coders.  2.4 Prediction For the encoding of the edge expansion of vertex split, we have 
consider all cases with the same probability, for example, in Fig­ure 5 the 10 points Ni are considered 
as possible vertices of the triangles incident to edge V1 V2 with the same probability. If we can compute 
more realistic probability, then the performance of the arithmetic coding are improved, the most probable 
Ni get short en­coding and the less probable larger one. In the end of this section, we compute such 
probabilities. Since the edge collapse operator is by far the most frequent (as shown by Table 1), we 
focused on the optimization of its descrip­tion. In the coding sequence, the description of connectivity 
fol­lows that of geometry, so that when the decoder is about to read the description of an edge expansion, 
it already knows the geometric position of the vertex V to be split, those of the resulting vertices 
V1 and V2 , and those of the neighbors Ni (see Figure 5). The idea is to exploit this geometric information 
in order to attempt to guess the two edges having to be expanded into faces to recover the orig­inal 
connectivity (the cut-edges). To do so, a score is assigned to each incident edge according to some criterion 
(de.ned later), then the scores are normalized and passed to the arithmetic coder; if the prediction 
scheme is reliable, the actual edges to be expanded ob­tain a high probability, and thus a short code. 
Among the numerous criteria we have tested, two stand out by their ef.ciency and their robustness. The 
.rst one is very simple and intuitive: the probabil­ity for the edge VNi to be one of the two cut-edges 
is de.ned as a linear function of |d(Ni,V1 ) - d(Ni,V2 )| (where d() is the Eu­clidean distance). For 
the second predictor, an interpolating plane of V , V1 , V2 and Ni is .rst computed, then a Delaunay-like 
crite­rion is applied to the projected points: the score of each edge VNi is inversely proportional to 
the radius of the circumscribed circle of {V1 ,V2 ,Ni} (see Figure 8). By linearly combining these two 
criteria, an average gain of up to 40% can be reached for the edge collapse coding sequence. As for the 
additional bit assigning the two edge subsets (white and blue edges in Figure 5) to the ver­tices V1 
and V2 , the gain is even more spectacular since a simple proximity criterion reduces the cost down to 
0.2 bit per operation. The prediction proceeds as follows: if B1 (resp. B2 ) denotes the barycenter of 
the neighbors of V in the .rst (resp. second) sub­set, the comparison of the expressions d(V1 ,B1 )+ 
d(V2 ,B2 ) and d(V1 ,B2 )+d(V2 ,B1 ) allows us to predict which subset is attached to V1 and which one 
to V2 , with an average reliability of 95% on the models of Table 1. N6 Figure 8: Delaunay (left) and 
proximity (right) criterions. For the vertex uni.cation, the same kind of proximity criterion can be 
used to predict the code of the simplices. In our implemen­tation, we choose to apply prediction schemes 
only for simplices whose code is 1 or 2, which are much more frequent than codes 3 and 4. We deduce the 
probabilities for a simplex of barycenter R to be attached to V1 (resp. V2 ) directly from the distance 
d(R, V1 ) (resp. d(R, V2 )) (see Figure 8). The probabilities are then passed to the arithmetic coder 
and yields a gain of 10 to 20% on the sole vertex uni.cation coding sequence. It must be noted that, 
as usually with prediction, the ef.ciency of all these methods strongly depends on the regularity of 
the mesh. 2.5 Results Table 2 presents some results of our algorithm compared to those of Pajarola and 
Rossignac [2000b], Cohen-Or, Levin and Re­mez [1999] and Alliez and Desbrun [2001a] (a row concerning 
the Touma and Gotsman [1998] single-rate method has been added as reference). For each model and each 
benchmarked algorithm, the .rst line gives the connectivity cost and the second one the geom­etry cost 
in bits per vertex. As shown by the last line, our method reaches progressivity with less than 5% overhead 
compared to the most ef.cient single resolution algorithms and compares well to other multi-resolution 
techniques. Generally speaking, the perfor­mance comparison between the various published works is made 
very delicate owing to the disparity of the tested models, as well as the disparity of quantizations 
on a given model. Since we have not implemented all the methods, Table 2 uses the 3D objects ap­pearing 
in the different articles. Regarding the quantization, all the models have 12 bits coordinates, except 
the fandisk whose vertex coordinates are coded on 10 bits. models vertex TG PR CLR AD our number 1998 
2000 2000 2001 algo triceratops 2832 2.2 7.4 5.8 5.9 6.0 20.0 21.0 20.4 25.5 19.2 blob 8033 1.7 5.9 7.6 
4.3 4.1 20.0 21.0 19.7 20.6 20.1 fandisk 6475 1.1 6.8 ? 5.0 2.9 9.0 15.0 12.3 12.1 bunny 35947 ? 7.0 
? 4.0 3.1 16.0 15.4 14.8 horse 19851 2.3 ? 5.7 4.6 3.9 17.0 15.4 16.2 16.4  average 73138 2.0 7.1 5.8 
4.4 3.5 16.5 16.9 17.0 16.3 15.7 total 18.5 24.0 22.8 20.7 19.2 Table 2: Results on manifold models in 
bits per vertex for connec­tivity (number above) and geometry (number below). As shown in Section 2.2.2, 
the vertex uni.cation operator makes our algorithm applicable to a much wider range of geometric struc­tures 
than the manifold triangulated surfaces. Table 3 gathers the results of our algorithm on 3D objects modeled 
with triangle soups (these objects are freely available on the 3DCafe web site: http://www.3dcafe.com/asp/meshes.asp). 
The connectivity and ge­ometry bit-rates are separated as previously, and the vertex coordi­nates have 
been quantized on 12 bits for all the models. Contrary to the objects tested in Table 2, the edge collapse 
occurs barely during the decimation process: it represents less than 5% of the operations. As a result, 
the average bit-rate of the connectivity goes up to 8 bits per vertex. models vertex results models vertex 
results number number aqua05 16784 8.5 grass14 29224 7.5 16.4 18.8 maple01 45499 8.2 skeleton 6103 11.4 
16.9 15.9 m tree1 17782 7.9 average 115392 8.2 16.0 17.1 Table 3: Results on triangle soups in bits per 
vertex for connectivity (number above) and geometry (number below). Figures 9 and 10 show the progressive 
decompression for a man­ifold surface (the triceratops) and a triangle soup (the tree, pre­sented in 
a global view and a zoomed in region). For the tricer­atops, the size of the compressed data goes from 
4% (for the .rst low precision version) to 25% (for the .nal lossless version) of the original data in 
their raw form (3 x 12 bits per vertex for the posi­tions plus 3 x log2 n bits per triangle for the connectivity). 
As for the tree, the compressed size spreads from 8% to 45% of the orig­inal raw size. We also draw the 
average positionning error of the vertices as the decompression evolves. Figures 1 and 12 show more examples 
of progressive decompression. size: 4% size: 7% 5 bits 6 bits size: 11% size: 14% 7 bits 8 bits size: 
17% size: 25% 9 bits 12 bits error 6 5 4 3 2 1 rate 0 10 20 100 Figure 9: Rate distorsion on the triceratops 
model.    3 VOLUMETRIC MESHES EXTENSION Volumetric meshes have been extensively used in .nite element 
for years, and are also more and more widespread in volume visualiza­tion. More speci.cally, tetrahedral 
meshes, which offer a direct and .exible way to interpolate numerical values in any point in space, have 
established themselves as the most natural and powerful tool for volume representation. Although the 
need for tetrahedral compression is clear (the con­nectivity being by far more expensive than for surface 
meshes), rel­atively few methods have been proposed up to now. Regarding the progressive coding, the 
only article tackling the progressive tetra­hedral mesh compression is due to Pajarola, Rossignac and 
Szym­czak [1999], and uses an edge collapse operator applied to succes­sive batches of independent edges. 
Besides the cost of the vertex split description (the implant), an additional bit per vertex and per 
batch is used to identify the vertices to be split. Thus the global cost depends on the number of independent 
edges collapsed in each batch. To avoid the appearance of non manifold regions during the simpli.cation 
process, some edge collapses are forbidden. As for the compression of the vertex positions, the authors 
suggest to ex­ploit the prediction techniques designed for triangular meshes. 3.1 Generalization Of The 
Decimation Operators The generalization of the vertex uni.cation operator described in Section 2.2.2 
is straightforward in any dimension. As a result, the progressive geometry and connectivity coding algorithms 
we pro­posed in the framework of triangular structures are easily applica­size: 2% size: 5% 5 bits 6 
bits size: 7% size: 9% 7 bits 8 bits size: 12% size: 20% 9 bits 12 bits error 6 5 4 3 2 1 rate 0 10 20 
100 Figure 10: Rate distorsion on the m tree1 model. ble to any simplicial complex in dimension d. However, 
without a low cost operator equivalent to the edge collapse described in Sec­tion 2.2.1, the performances 
regarding the connectivity compres­sion will not be competitive. More precisely, the average cost of 
a vertex uni.cation for tetrahedral meshes is 120 bits without op­timization, and around 40 bits by applying 
the two rules stated in Section 2.2.2 plus arithmetic coding. We suggest here a decimation operator equivalent 
to the edge collapse adapted to tetrahedral meshes. Similarly to the case of polyhedral surfaces, we 
improve the coding of special vertex uni­.cations that do not create topological changes in their neighbor­hood. 
More precisely, this 3D edge collapse can be used when the reverse operation ful.lls the following conditions: 
 the vertex V to be split has a code 4 (i.e. the split is an edge ex­pansion);  the set of triangles 
incident to V having a code 4 (i.e. the tri­angles generating a tetrahedron after the split) forms a 
manifold surface M , and V is not on its boundary;  the remaining edges (i.e. the edges adjacent to 
V that do not be­long to M) have a code c E{1, 2};  M separates the set of code 1 edges from the set 
of code 2 edges (see Figure 11).  When the conditions are satis.ed, the piece of code that allows to 
restore the neighborhoods of the vertices V1 and V2 before the uni.cation is composed of: i) the number 
of code 4 triangles adja­cent to V ; ii) the indices of these triangles among the set of triangles incident 
to V ; iii) the indices of the edges lying on the .rst side of M. As shown in Section 2.4, the cost can 
be lowered with the help of prediction techniques combined with arithmetic coding. On average, the .nal 
cost of this 3D equivalent of the edge collapse is about 20 bits. code 1 edges surface M (code 4 faces) 
V code 2 edges Figure 11: Good case for the 3D edge expansion. 3.2 Results As in the coding of triangular 
structures, the global cost depends essentially on the occurrence percentages of the two operators. For 
the Delaunay tetrahedralization of 10, 000 points points uniformly distributed in a sphere, the 3D edge 
collapse occurs in less than 45% of the vertex merging; we thus reach a bit-rate of 34 bits per vertex 
to encode the mesh connectivity and 35 bits per vertex for the ge­ometry if the point coordinates are 
quantized on 16 bits. We obtain a .nal compression rate of 15% if we compare with a direct storage with 
16 bits per coordinate and 4 × log2 n bits per tetrahedron. These results can be compared with those 
obtained by Pajarola, Rossignac and Szymczak [1999]for progressive tetrahedral com­pression. For a random 
Delaunay tetrahedralization containing 10, 000 vertices, their edge collapse operator, whose functionalities 
and coding are quite different from ours, yields a total cost around 45 bits per vertex for the connectivity 
(taking into account the base mesh plus the re.nement cost). Moreover, the progressivity is lim­ited 
since the base mesh cannot be arbitrary small (it represents about 1/4 of the total cost). The geometry 
coding is not addressed in this article, just like in Yang, Mitra and Chiueh work [2000], which tackle 
the progressive coding of tetrahedral meshes from a rendering point of view. We have also tested our 
algorithm on meshes coming from real applications: for a mesh of a Falcon business jet, (courtesy of 
Dassault-Aviation) with 10188 vertices and 54911 tetrahedra with coordinates on 16 bits, we obtain 41% 
of edge collapses and a bit­rate of 23 bits per vertex for the geometry and 25 bits per vertex for the 
connectivity.  4 CONCLUSION AND FUTURE WORK We have presented a new progressive connectivity coding 
algorithm based on surface simpli.cation techniques optimized for compres­sion purposes. This work is 
built on the kd-tree geometric coder [Devillers and Gandoin 2000], and as such, allows an ef.cient joint 
compression of the positions and the connectivity of the mesh. Be­sides the good compression rates, which 
are competitive with the most ef.cient progressive methods, the algorithm has the advantage of being 
applicable to any simplicial complex, including in partic­ular non manifold triangulations and triangle 
soups. Furthermore, the method can be extended to any dimension and yields in the case of tetrahedral 
meshes a cost reduction of about 25% compared with previous work. Future work will address the extension 
of the algorithm to polyg­onal meshes. Indeed, the method permits the compression of such geometric structures 
by describing them as a set of edges, and by reconstructing the polygons from this set by loop searching, 
but it should be more ef.cient to de.ne an edge collapse operator adapted to polygons with a size greater 
than 3. Also, we think the compres­sion ratios can be still improved, in particular by combining connec­tivity 
and geometry prediction as the re.nement process proceeds. Acknowledgments: Thanks to Pierre Alliez, 
Alain Dervieux, George Drettakis and Monique Teillaud for their help. Aqua Blob 4 bits, 2% 5 bits, 
4% 3 bits 4 bits 5 bits 6 bits 7 bits 8 bits 12 bits 0.3% 0.9% 2% 4% 7% 10% 23%, lossless Horse 7 bits, 
9% 12 bits, 21%, lossless 3 bits, 0.1% 4 bits,0.4% 5 bits, 1% 6 bits, 3% 7 bits, 6% 12 bits, 17%, lossless 
Figure 12: More examples of progressive decompression.  References ALLIEZ, P., AND DESBRUN, M. 2001. 
Progressive compression for loss­less transmission of triangle meshes. In SIGGRAPH 2001 Conference Proc., 
199 202. ALLIEZ, P., AND DESBRUN, M. 2001. Valence-driven connectivity encod­ing for 3d meshes. In Eurographics 
2001 Conference Proc., 480 489. BAJAJ, C., CUTCHIN, S., PASCUCCI, V., AND ZHUANG, G. 1999. Error resilient 
streaming of compressed vrml. Tech. rep., University of Texas. BAJAJ, C., PASCUCCI, V., AND ZHUANG, G. 
1999. Progressive compres­sion and transmission of arbitrary triangular meshes. In IEEE Visualiza­tion 
99 Conference Proc., 307 316. BAJAJ, C., PASCUCCI, V., AND ZHUANG, G. 1999. Single resolution com­pression 
of arbitrary triangular meshes with properties. Computational Geometry : Theory and Applications, 247 
296. COHEN-OR, D., LEVIN, D., AND REMEZ, O. 1999. Progressive compres­sion of arbitrary triangular meshes. 
In IEEE Visualization 99 Conference Proc., 67 72. DEERING, M. 1995. Geometry compression. In SIGGRAPH 
95 Conference Proc., 13 20. DEVILLERS, O., AND GANDOIN, P.-M. 2000. Geometric compression for interactive 
transmission. In IEEE Visualization 2000 Conference Proc., 319 326. EVANS, F., SKIENA, S., AND VARSHNEY, 
A. 1996. Optimizing triangle strips for fast rendering. In IEEE Visualization 96 Conference Proc., 319 
326. GU ´ EZIEC, A., BOSSEN, F., TAUBIN, G., AND SILVA, C. 1999. Ef.cient compression of non-manifold 
polygonal meshes. Comput. Geom. Theory Appl. 14, 137 166. GUMHOLD, S., AND STRASSER, W. 1998. Real time 
compression of triangle mesh connectivity. In SIGGRAPH 98 Conference Proc., 133 140. GUMHOLD, S., GUTHE, 
S., AND STRASSER, W. 1999. Tetrahedral mesh compression with the cut-border machine. In IEEE Visualization 
99 Con­ference Proc., 91 98. HECKBERT, P. S., AND GARLAND, M. 1997. Survey of polygonal surface simpli.cation 
algorithms. Tech. rep., Carnegie Mellon University. HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., 
AND STUET-ZLE, W. 1993. Mesh optimization. In SIGGRAPH 93 Conference Proc., 19 26. ISENBURG, M., AND 
SNOEYINK, J. 1999. Mesh collapse compression. In Symposium on Computational Geometry, 419 420. ISENBURG, 
M. 2000. Triangle .xer : Edge-based connectivity encoding. In 16th European Workshop on Computational 
Geometry Proc. KARNI, Z., AND GOTSMAN, C. 2000. Spectral compression of mesh geometry. In SIGGRAPH 2000 
Conference Proc., 279 286. KHODAKOVSKY, A., SCHR ODER¨ , P., AND SWELDENS, W. 2000. Pro­gressive geometry 
compression. In SIGGRAPH 2000 Conference Proc., 271 278. KING, D., AND ROSSIGNAC, J. 1999. Guaranteed 
3.67v bit encoding of planar triangle graphs. In Canadian Conference on Computational Geometry Proc., 
146 149. LI, J., AND KUO, C.-C. J. 1998. A dual graph approach to 3d trian­gular mesh compression. In 
IEEE International Conference on Image Processing Proc. PAJAROLA, R., AND ROSSIGNAC, J. 2000. Compressed 
progressive meshes. IEEE Transactions on Visualization and Computer Graphics 6, 1 (January March), 79 
93. PAJAROLA, R., AND ROSSIGNAC, J. 2000. Squeeze : Fast and progressive decompression of triangle meshes. 
CGI 2000 Proc., 173 182. PAJAROLA, R., ROSSIGNAC, J., AND SZYMCZAK, A. 1999. Implant sprays : Compression 
of progressive tetrahedral mesh connectivity. In IEEE Visualization 99 Conference Proc., 299 306. POPOVI 
C´, J., AND HOPPE, H. 1997. Progressive simplicial complexes. In SIGGRAPH 97 Conference Proc., 217 224. 
ROSSIGNAC, J., AND BORREL, P. 1993. Geometric Modeling in Computer Graphics. Springer-Verlag, July, ch. 
Multi-Resolution 3D Approxima­tions for Rendering Complex Scenes, 455 465. ROSSIGNAC, J., AND SZYMCZAK, 
A. 1999. Wrap&#38;zip : Linear decod­ing of planar triangle graphs. Computational Geometry : Theory and 
Applications, 119 135. ROSSIGNAC, J. 1999. Edgebreaker : Connectivity compression for triangle meshes. 
IEEE Transactions on Visualization and Computer Graphics, 47 61. SCHMALSTIEG, D., AND SCHAUFLER, G. 1997. 
Smooth levels of detail. In IEEE Virtual Reality Annual International Symposium, 12 19. TAUBIN, G., AND 
ROSSIGNAC, J. 1998. Geometric compression through topological surgery. ACM Transactions on Graphics 17, 
2, 84 115. TAUBIN, G., GUEZIEC´, A., HORN, W., AND LAZARUS, F. 1998. Pro­gressive forest split compression. 
In SIGGRAPH 98 Conference Proc., 123 132. TOUMA, C., AND GOTSMAN, C. 1998. Triangle mesh compression. 
In Graphics Interface 98 Conference Proc., 26 34. WITTEN, I., NEAL, R., AND CLEARY, J. 1987. Arithmetic 
coding for data compression. Communications of the ACM 30, 6, 520 540. YANG, C., MITRA, T., AND CHIUEH, 
T. 2000. On-the-.y rendering of losslessly compressed irregular volume data. In 11th IEEE Visualization 
Conference, 329 336.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566592</article_id>
		<sort_key>380</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Linear combination of transformations]]></title>
		<page_from>380</page_from>
		<page_to>387</page_to>
		<doi_number>10.1145/566570.566592</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566592</url>
		<abstract>
			<par><![CDATA[Geometric transformations are most commonly represented as square matrices in computer graphics. Following simple geometric arguments we derive a natural and geometrically meaningful definition of scalar multiples and a commutative addition of transformations based on the matrix representation, given that the matrices have no negative real eigenvalues. Together, these operations allow the linear combination of transformations. This provides the ability to create weighted combination of transformations, interpolate between transformations, and to construct or use arbitrary transformations in a structure similar to a basis of a vector space. These basic techniques are useful for synthesis and analysis of motions or animations. Animations through a set of key transformations are generated using standard techniques such as subdivision curves. For analysis and progressive compression a PCA can be applied to sequences of transformations. We describe an implementation of the techniques that enables an easy-to-use and transparent way of dealing with geometric transformations in graphics software. We compare and relate our approach to other techniques such as matrix decomposition and quaternion interpolation.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[exponential map]]></kw>
			<kw><![CDATA[linear space]]></kw>
			<kw><![CDATA[logarithm]]></kw>
			<kw><![CDATA[matrix exponential]]></kw>
			<kw><![CDATA[transformations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Spline and piecewise polynomial interpolation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14091648</person_id>
				<author_profile_id><![CDATA[81100235480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technische Universit&#228;t Darmstadt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALEXA, M., AND M&#220;LLER, W. 2000. Representing animations by principal components. Computer Graphics Forum 19, 3 (August), 411-418. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134086</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARR, A. H., CURRIN, B., GABRIEL, S., AND HUGHES, J. F. 1992. Smooth interpolation of orientations with angular velocity constraints using quaternions. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 313-320. ISBN 0-201-51585-7. Held in Chicago, Illinois.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>26962</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARTELS, R. H., BEATTY, J. C., AND BARSKY, B. A. 1985. An introduction to the use of splines in computer graphics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DENMAN, E. D., AND BEAVERS JR., A. N. 1976. The matrix sign function and computations in systems. Appl. Math. Comput. 2, 63-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DO CARMO, M. P. 1992. Riemannian Geometry. Birkh&#228;user Verlag, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618917</ref_obj_id>
				<ref_obj_pid>616077</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DORST, L., AND MANN, S. 2001. Geometric algebra: a computation framework for geometrical applications. submitted to IEEE Computer Graphics & Applications. available as http://www.cgl.uwaterloo.ca/~smann/Papers/CGA01.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DYN, N., LEVIN, D., AND GREGORY, J. 1987. A 4-point interpolatory subdivision scheme for curve design. Computer Aided Geometric Design 4, 4, 257-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GABRIEL, S. A., AND KAJIYA, J. T. 1985. Spline interpolation in curved space. In SIGGRAPH '85 State of the Art in Image Synthesis seminar notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GOLUB, G. H., AND VAN LOAN, C. F. 1989. Matrix Computations, second ed., vol. 3 of Johns Hopkins Series in the Mathematical Sciences. The Johns Hopkins University Press, Baltimore, MD, USA. Second edition.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781241</ref_obj_id>
				<ref_obj_pid>781238</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GRASSIA, F. S. 1998. Practical parameterization of rotations using the exponential map. Journal of Graphics Tools 3, 3, 29-48. ISSN 1086-7651.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HESTENES, D. 1991. The design of linear algebra and geometry. Acta Applicandae Mathematicae 23, 65-93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HIGHAM, N. J. 1997. Stable iterations for the matrix square root. Numerical Algorithms 15, 2, 227-242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>19572</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HORN, R. A., AND JOHNSON, C. A. 1991. Topics in Matrix Analysis. Cambridge University press, Cambridge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>174506</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HOSCHEK, J., AND LASSER, D. 1993. Fundamentals of computer aided geometric design. ISBN 1-56881-007-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[JOLLIFFE, I. T. 1986. Principal Component Analysis. Series in Statistics. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KENNEY, C., AND LAUB, A. J. 1989. Condition estimates for matrix functions. SIAM Journal on Matrix Analysis and Applications 10, 2, 191-209.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218486</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KIM, M.-J., SHIN, S. Y., AND KIM, M.-S. 1995. A general construction scheme for unit quaternion curves with simple high order derivatives. Proceedings of SIGGRAPH 95 (August), 369-376. ISBN 0-201-84776-0. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300533</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LENGYEL, J. E. 1999. Compression of time-dependent geometry. 1999 ACM Symposium on Interactive 3D Graphics (April), 89-96. ISBN 1-58113-082-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>339385</ref_obj_id>
				<ref_obj_pid>339343</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MARTHINSEN, A. 2000. Interpolation in Lie groups. SIAM Journal on Numerical Analysis 37, 1, 269-285.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MOLER, C. B., AND LOAN, C. F. V. 1978. Nineteen dubious ways to compute the matrix exponential. SIAM Review 20, 801-836.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>561828</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MURRAY, R. M., LI, Z., AND SASTRY, S. S. 1994. A Mathematical Introduction to Robotic Manipulation. CRC Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[NAEVE, A., AND ROCKWOOD, A. 2001. Geometric algebra. SIGGRAPH 2001 course #53.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>256160</ref_obj_id>
				<ref_obj_pid>256157</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PARK, F. C., AND RAVANI, B. 1997. Smooth invariant interpolation of rotations. ACM Transactions on Graphics 16, 3 (July), 277-295. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258870</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND BARR, A. H. 1997. Fast construction of accurate quaternion splines. Proceedings of SIGGRAPH 97 (August), 287-292. ISBN 0-89791-896-7. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>155324</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SHOEMAKE, K., AND DUFF, T. 1992. Matrix animation and polar decomposition. Graphics Interface '92 (May), 258-264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325242</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SHOEMAKE, K. 1985. Animating rotation with quaternion curves. Computer Graphics (Proceedings of SIGGRAPH 85) 19, 3 (July), 245-254. Held in San Francisco, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SHOEMAKE, K. 1991. Quaternions and 4x4 matrices. Graphics Gems II, 351-354. ISBN 0-12-064481-9. Held in Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[WEB3D CONSORTIUM. 1999. H-Anim. http://ece.uwaterloo.ca:80/~h-anim.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[ZEFRAN, M., AND KUMAR, V. 1998. Rigid body motion interpolation. Computer Aided Design 30, 3, 179-189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[ZEFRAN, M., KUMAR, V., AND CROKE, C. 1996. Choice of riemannian metrics for rigid body kinematics. In ASME 24th Biennial Mechanisms Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>925036</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[ZEFRAN, M. 1996. Continuous methods for motion planning. PhD-thesis, U. of Pennsylvania, Philadelphia, PA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[ZORIN, D., AND SCHR&#214;DER, P. 1999. Subdivision for modeling and animation. SIGGRAPH 1999 course # 47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Linear Combination of Transformations Marc Alexa Interactive Graphics Systems Group, Technische Universit¨at 
Darmstadt * Abstract Geometric transformations are most commonly represented as square matrices in computer 
graphics. Following simple geometric arguments we derive a natural and geometrically meaningful de.­nition 
of scalar multiples and a commutative addition of transfor­mations based on the matrix representation, 
given that the matrices have no negative real eigenvalues. Together, these operations allow the linear 
combination of transformations. This provides the abil­ity to create weighted combination of transformations, 
interpolate between transformations, and to construct or use arbitrary transfor­mations in a structure 
similar to a basis of a vector space. These basic techniques are useful for synthesis and analysis of 
motions or animations. Animations through a set of key transformations are generated using standard techniques 
such as subdivision curves. For analysis and progressive compression a PCA can be applied to sequences 
of transformations. We describe an implementation of the techniques that enables an easy-to-use and transparent 
way of dealing with geometric transformations in graphics software. We compare and relate our approach 
to other techniques such as matrix decomposition and quaternion interpolation. CR Categories: G.1.1 [Numerical 
Analysis]: Interpolation Spline and piecewise polynomial interpolation; I.3.5 [Computer Graphics]: Computational 
Geometry and Object Modeling Geometric Transformations; I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Animation; Keywords: transformations, linear space, matrix exponential and logarithm, 
exponential map 1 Introduction Geometric transformations are a fundamental concept of computer graphics. 
Transformations are typically represented as square real matrices and are applied by multiplying the 
matrix with a coor­dinate vector. Homogeneous coordinates help to represent addi­tive transformations 
(translations) and multiplicative transforma­tions (rotation, scaling, and shearing) as matrix multiplications. 
This representation is especially advantageous when several trans­formations have to be composed: Since 
the matrix product is asso­ciative all transformation matrices are multiplied and the concate­nation 
of the transformations is represented as a single matrix. *email:alexa@gris.informatik.tu-darmstadt.de 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 Figure 1: A two-dimensional cow space: Two transformations A and B, both of which include a rotation, 
a uniform scale, and a translation, form a two-dimensional space of transformations. In this space (0,0) 
is the identical transformation, (1,0) and (0,1) rep­resent the speci.ed transformations A and B. For 
the representation of motion it is necessary to interpolate from one given transformation to another. 
The common way in computer graphics for blending or interpolating transformations is due to the pioneering 
work of Shoemake [Shoemake 1985; Shoe­make 1991; Shoemake and Duff 1992]. The approach is to decom­pose 
the matrices into rotation and stretch using the polar decompo­sition and then representing the rotation 
using quaternions. Quater­nions are interpolated using SLERP and the stretch matrix might be interpolated 
in matrix space. Note, however, that the quaternion approach has drawbacks. We would expect that half 
of a trans­formation T applied twice would yield T . Yet this is not the case in general because the 
factorization uses the matrix product, which is not commutative. In addition, this factorization induces 
an order dependence when handling more than two transformations. Barr et al. [1992], following Gabriel 
&#38; Kajiya [1985], have for­mulated a de.nition of splines using variational techniques. This allows 
one to satisfy additional constraints on the curve. Later, Ra­mamoorthi &#38; Barr [1997] have drastically 
improved the computa­tional ef.ciency of the technique by .tting polynomials on the unit quaternion sphere. 
Kim et al. [1995] provide a general framework for unit quaternion splines. However, compared to the rich 
tool-box for splines in the euclidean space, quaternions splines are still dif.­cult to compute, both 
in terms of programming effort as well as in terms of computational effort. We identify as the main problem 
of matrix or quaternion repre­sentations that the standard operators are not commutative. In this work 
we will give geometrically meaningful de.nitions for scalar product and addition of transformations based 
on the matrix repre­sentation. We motivate the de.nitions geometrically. The de.ni­Figure 2: De.ning 
scalar multiples of transformations. Intuitively, half of a given transformation T should be so de.ned 
that applying it twice yields T . This behavior is expected for arbitrary parts of transformations. Consequently, 
scalar multiples are de.ned as powers of the transformation matrices.  tions lead to the use of an exponential 
map into the Lie group of geometric transformations. Once this connection is established we compare our 
de.nition to other approaches. The implementation of this approach uses a transform object that transparently 
offers scalar product and addition operators. This gives API users an easy-to­use, intuitive, and .exible 
tool whenever it is desirable to combine transforms rather than composing them.  2 Related work Our 
approach essentially uses interpolation in Lie groups by means of the exponential map [Marthinsen 2000]. 
Grassia has introduced this idea for 3D graphics to represent the group of rotations [Gras­sia 1998]. 
The group of rotations SO(3) and the group of rigid body motions SE(3) are commonly used for motion planning 
in the .eld of robotics. Park and Ravani compute interpolating splines for a set of rotations in SO(3) 
[Park and Ravani 1997]. They com­pare the groups SO(3) and SU(2) (the group of unit quaternions) in detail. 
One main advantage of using SO(3) for interpolation is bi­invariance, e.g. if two sets of rotations are 
connected with an af.ne mapping the resulting curves are connected by the same map. In our context, this 
property is naturally contained as part of linear­ity. Zefran analyzes SE(3) for general problems in 
motion planning (see [Zefran 1996] and the references therein). The main problem is that the respective 
spaces have non-Euclidean geometry and one has a choice of several reasonable metrics [do Carmo 1992; 
Zefran et al. 1996]. Once a metric is de.ned, variational methods are used to determine an interpolant 
[Zefran and Kumar 1998]. In our ap­proach we have rather traded the problem of de.ning the geomet­rically 
most meaningful metric and solving a variational problem for simplicity, ease-of-use and transparency. 
In addition, we ex­tend these methods from rotations and rigid body motion to general transformations. 
The results of our techniques are on an abstract level identical to those from Geometric Algebra (GA) 
[Hestenes 1991], a .eld recently introduced to the graphics community [Naeve and Rock­wood 2001]. Current 
implementations of GA [Dorst and Mann 2001] use explicit representations of all sub-elements (i.e. points, 
lines, planes, volumes), which results in R3 being represented with 8 × 8 matrices. In a sense, our approach 
could be seen as an alter­native implementation using more complex operations on the ma­trices, however, 
in smaller dimension. 3 Motivation and de.nition of scalar mul­tiples of transformations Suppose that 
we have some transformation, T , and we want to de­.ne a scalar mutliple, a . T . What conditions should 
such a scalar 1 multiple satisfy? Well, in the particular case a = 2 , i.e., half of T , we want the 
resulting transformation to have the property that when it s applied twice, the result is the original 
transformation T , i.e., that 11 . T .. T = T ; (1) 22 an illustration of our goal is given in Figure 
2. We ll require analogous behavior for one-third of a transforma­tion, one fourth, and so forth. We 
ll also want a .T to be a contin­uous function of both a and T . Let s explore what this entails by examining 
the consequences for some standard transformations: translation, rotation, and scal­ing. Translation: 
If T is a translation by some amount v, then clearly translation by av is a good candidate for a . T 
; it satis.es the requirements of equation 1 and its analogues, and has the advantage that it s also 
a translation. Rotation: If T is a rotation of angle . about the axis v, then ro­tation about the axis 
v by angle a. is a good candidate for a . T , for simialr reasons. Scaling: Finally, if T is a scaling 
transformation represented by a scale-matrix with diagonal entries d1,d2,... then diag(d1 a , d2 a ,...) 
is a candidate for a . T . In all three cases, we see that for positive integer values of a, our candidate 
for a . T corresponds to T a ; the same is true for the matrix representing the transformation. If we 
had a way to de.ne arbitrary real powers of a matrix, we d have a general solution to the problem of 
de.ning scalar multiples; we d de.ne a . T to be T a (where what we mean by this is that a .T is the 
transformation representated by the matrix Ma , where M is the matrix for T ). Fortunately, for a very 
wide class of matrices (those with no neg­ative real eigenvalues), there is a consistent de.nition of 
Ma , and computing Ma is not particularly dif.cult (see Appendices A and C). Furthermore, it has various 
familiar properties, the most critical being that Ma Mß = Ma+ß = Mß Ma (i.e. scalar multiples of the 
same transform commute), and M0 = I (the identity matrix). Some other properties of exponents do not 
carry over from real-number arithmetic, though: in general it s not true that (AB)a = Aa Ba . One more 
property is important: although a matrix may have two (or more) square roots (for example, both the identity 
and the nega­tive identity are square roots of the identity!), for matrices with non negative-real eigenvalues, 
one can de.ne a preferred choice of Ma which is continuous in M and a. While techniques for computing 
parts of the above transforma­tions are well known (see e.g. [Shoemake 1985; Shoemake 1991; Shoemake 
and Duff 1992; Park and Ravani 1997]) the idea of our approach is that taking powers of transformation 
matrices works for arbitrary transformations without .rst factoring the matrix into these components. 
Following this intuitive de.nition of scalar multiples of trans­formations we need a commutative addition 
for transformations. Together, these operations will form the basic building blocks for linear combination 
of transformation. 4 Commutative addition of transforma­tions In this section, we motivate and de.ne 
an operation we ll call addi­tion of transformations the word addition meant to remind the reader that 
the operation being de.ned is commutative. The ordi­nary matrix product combines two matrices by multiplying 
one by the other, which is not symmetric in the factors. For a commutative operation we rather expect 
the two transformations to be applied at the same time, or intertwined. We want to stress that the addition 
is not intended to replace the standard matrix product but to com­plement it. Clearly, both will have 
their uses and one has to choose depending on the effect to be achieved. Let A,B be two square real matrices 
of the same dimension. Clearly, AB and BA are different in general, however, are the same if A and B 
commute. In this case the standard matrix product is ex­actly what we want, in all other cases we need 
to modify the product operation. The main idea of this work is to break each of the trans­formations 
A and B into smaller parts and perform (i.e. multiply) these smaller parts alternately. Small parts of 
A and B are generated by scalar multiplication with a small rational number, e.g. 1/n. Loosely speaking, 
we ex­ ..n ..n A1/nB1/nB1/nA1/n pect that differs less from than AB from BA. This is because a large 
part of the product is the same and the difference is represented by n-1 . A respectively n-1 . B. Since 
0 . X = I this difference would vanish for n-1 . 0 and we conse­quently de.ne ..n 11 A . B = lim A nB 
n . (2) n.8 The idea of this de.nition is visualized in Figure 3. Several ques­tions arise: Existence 
Does the limit exist? Does it exist for all inputs? Is it real if the input is real? Commutativity Is 
the addition indeed commutative? Geometric properties What geometric properties has the new de.nition? 
For example, is the addition of two rotations a rotation? The questions regarding existence and commutativity 
of the two operations are discussed in Appendix B. It can be shown that the limit indeed exists under 
reasonable conditions. Here we analyze some geometric properties of the addition. The addition was designed 
to be commutative while preserving the properties of the standard matrix product. Thus, it is desirable 
that A . B = AB if AB = BA. If A and B commute then ..n An = BAB-1 = BAB-1BAB-1B···B-1 = BAnB-1 , i.e. 
also An and B commute. The same argument leads to AnBn = BnAn and, assuming again that primary roots 
exist and are conti­B1/nA1/n nous in their inputs, this result extends also to A1/nB1/n = . Thus ..n 
..n ..n 11 11 AB = An Bn = A nB n and assuming the limit for n . 8 exists it follows that the matrix 
product and . are indeed the same if A and B commute. Further­more, since A commutes with A-1 the inverse 
of . is the standard matrix (product) inverse. Another important geometric property is the measure (area, 
vol­ume) of a model. The change of this measure due to a transfor­mation is available as the determinant 
of the matrix. Note that the order of two transformations is irrelevant for the change in size, i.e. 
det(AB)= det(A) det(B)= det(B)det(A)= det(BA). It is easy to see that the addition of transformations 
conforms with this invari­ant: A1/nB1/nn det(A . B)= det lim n.8 . ..n.. . .. n A1/nB1/n = det lim det 
lim n.8 n.8 = det(A)det(B). In conclusion, the geometric behavior of . is very similar to the standard 
matrix product. Loosely speaking, A.B is the application of A and B at the same time.  5 Computation 
and Implementation Both the addition and scalar multiplication operators can be com­puted using matrix 
exponential and logarithm (see Appendix A for details). The de.nition of the matrix exponential is analogous 
to the scalar case, i.e. Ak eA = , (3) .8 k! k=0 which immediately de.nes the matrix logarithm as its 
inverse func­tion: eX = A .. X = logA. (4) The existence of matrix logarithms (as well as matrix roots) 
is dis­cussed in Appendix B. Here, it may suf.ce to say that logarithms exist for transformation matrices, 
given that the transformation con­tains no re.ection. Using exponential and logarithm scalar multiples 
may be ex­pressed as r logA r . A = e (5) and the limit in Equation 2 is equivalent to log A+logB A . 
B = e. (6) Using these equations a linear combination of an arbitrary number of transformations Ti with 
weights wi is computed as .i wi ·logTi (7) wi . Ti = e i Note that the use of the exponential and logarithm 
hint at po­tential problems of this approach, or more generally, show the the Figure 3: The addition 
of transformations. Given two transformations, A and B, applying one after the other (i.e. multiplying 
the matrices) generally leads to different results depeneding on the order of the operations. By performing 
n-th parts of the two transformations in turns the difference of the two orders becomes smaller. The 
limit n . 8 could be understood as performing both transformations concurrently. This is the intuitive 
geometric de.nition of a commutative addition for transformations based on the matrices. non-linearity 
and discontinuity between the group of transforma­tions and the space in which we perform our computations 
(i.e. the corresponding algebra). For example, both operators are in gen­eral not continous in their 
input, i.e. small changes in one of the transformations might introduce large changes in the result. 
Fur­ther potential problems and limitations are discussed together with applications in Section 6. In 
order to implement this approach, routines for computing ma­trix exponential and logarithm are required. 
We suggest the meth­ods described in Appendix C because they are stable and the most complex operation 
they require is matrix inversion, making them easy to integrate in any existing matrix package. Using 
an object-oriented programming language with operator overloading it is possible to design a transform 
object that directly supports the new operations. The important observation is that the logarithm of 
a matrix has to be computed only once at the instantia­tion of an object. Any subsequent operation is 
performed in the log­matrix representation of the transformation. Only when the trans­formation has to 
be sent to the graphics hardware a conversion to original representation (i.e. exponentiation) is necessary. 
Our current implementation needs 3 · 10-5sec to construct a transform object, which is essentially the 
time needed to compute the matrix logarithm. The conversion to standard matrix represen­tation (i.e. 
exponentiation) requires 3 · 10-6sec. Timings have been Note that for most applications transform objects 
are created at the initialization of processes, while the conversion to standard repre­sentation is typically 
needed in in every frame. However, we have found the 3µs necessary for this conversion to be negligible 
in prac­tice.  6 Applications &#38; Results Using the implementation discussed above, several interesting 
ap­plications are straightforward to implement. 6.1 Smooth animations A simple animation from a transformation 
state represented by A to a transformation B is achieved with C(t)=(1 -t) . A .t . B,t . [0, 1]. Using 
a cubic Bezier curve [Hoschek and Lasser 1993] al­lows one to de.ne tangents in the start and endpoint 
of the interpo­lation. Using the Bezier representation, tangents are simply de.ned by supplying two transformations. 
Tangents could be used to gen­erate e.g. fade-in/fade-out effects for the transformation. Figure 4 shows 
a linear and cubic interpolation of two given transformations. To generate a smooth interpolant through 
a number of key frame transformations Ti one can use standard techniques from linear acquired on a 1GHz 
Athlon PC under normal working conditions. spaces such as splines [Bartels et al. 1985] or subdivision 
curves Figure 4: Interpolation sequences between given transformations A and B. The top row shows a 
simple linear interpolation using the matrix operators de.ned here, i.e. (1 - t) . A . t . B. The bottom 
row shows a Bezier curve from A to B with additional control transformations. These extra transformations 
de.ne the tangents in the start and end point of the sequence. [Zorin and Schr¨oder 1999]. Note that 
the transparent implemen­tation of the operators allows solving linear systems of equations in transformations 
using standard linear algebra packages. Using these techniques one can solve for the necessary tangent 
matrices which de.ne e.g. a cubic spline. However, we .nd an interpolating subdivision scheme (e.g. the 
4pt scheme [Dyn et al. 1987]) partic­ularly appealing because it is simple to implement. It seems that 
implementations of quaternion splines or other elaborated techniques are hardly available in common graphics 
APIs. Note how simple the implementation of interpolating or ap­proximating transformation curves is 
with the approach presented here. One simply plugs the transform object into existing imple­mentations 
for splines in Euclidean spaces. The exponential map, on the other hand, has some drawbacks. Essentially, 
a straight line in parameter space doesn t necessarily map to a straight line (i.e. a geodesic) in the 
space of transforma­tions. This means the linear interpolation between two transfor­mations as de.ned 
above could have non-constant speed. Further­more, also spline curves, which could be thought of as approxi­mating 
straight lines as much as possible, are minimizers of a ge­ometrically doubtful quantity. Nevertheless, 
we found the results pleasing. We would also like to point at an interesting difference to quater­nions: 
The log-matrix representation allows angles of arbitrary de­gree. Computing the logarithm of a rotation 
by p and then mul­tiplying this log-matrix leads to a representation of rotations more than 2p. While 
this could be useful in some applications it might be disadvantageous in others. For example, the interpolation 
between two rotations of ±(p - e) results in a rotation by almost 2p rather than a rotation by 2e. However, 
using the tools presented in the following section this could be easily avoided.  6.2 Factoring transformations 
Transformations form a linear space in the log-matrix representa­tion. This allows us to write any transformation 
as a kind of lin­ear combination of transformations from an arbitrary basis . The quotation marks indicate 
that this linear combination takes place in log-space an associated space in which such combinations 
make sense. For example, three rotations Rx, Ry,Rz by an angle 0 < f < p around the canonical axes form 
a basis for the subspace of rotations. Since they are orthogonal, any transformation T can be factored 
by computing inner products of the log-representation: x = .log T,logRx., y = .logT,logRy.,z = .logT,logRz., 
(8) where the inner product is computed entry-wise, i.e. .{ai j},{bi j}. = .ai jbi j. Note that the values 
x,y,z do not represent Euler angles because the rotations around the axes are performed concurrently 
and not one after the other. Rather, x,y,z de.ne axis and angle of rotation with (x,y,z)/||(x,y,z)|| 
being the axis and (x + y + z)/f being the angle. The factors x, y, z could be useful to avoid the interpolation 
prob­lem mentioned at the end of the last Section. Assuming a represen­tation as above the inner products 
will lead to (x,y, z) . [-r, r]3, where r depends on the angle of rotation in each of Rx,Ry,Rz. Speci.cally, 
values -r and r represent the same orientation and one can imagine the angles to form a circle starting 
in -r and end­ing in r with 0 diametrical to ±r. To interpolate along the shortest path from R1 to R2 
one chooses for each of the factors x1,y1, z1 and x2,y2,z2 the shorter path on the circle. Speci.cally, 
if the differ­ence between two corresponding factors is larger than |r|, then the shorter interpolation 
path is via ±r rather than via 0. Clearly, factoring has more applications than analyzing rotations. 
It could be done with respect to any (application speci.c) orthogo­nal or non-orthogonal basis. In order 
to .nd the representation of a transformation T in an arbitrary transformation basis {Bi} we .rst compute 
the inner products of the bases. The matrix . . .logB1,log B1. ... .logB1,logBn.We have compared the 
computation times of this approach with .. .. standard techniques. A SLERP based on quaternions between 
two V .. . = formations. Quaternion splines are subtantially slower. They typi-describes a mapping from 
the orthogonal canonical base to the pos­cally do not allow interactively adjusting the key transformations. 
sible skew or de.cient one formed by {Bi}. Computing the inverse Figure 5: Animation analysis and compression 
based on the log-matrix representation. The upper rows shows 6 of 580 frames from a humanoid animation 
de.ned by key frame transformations in the joints of a skeleton. The respective log-matrices have been 
analyzed using the SVD. The bottom row shows the .rst 8 principal components. of V reveals, .rst, whether 
the basis {Bi} has full rank and, second, allows transforming a vector of inner products with the basis 
to the representation vector. We use singular value decomposition (SVD) [Golub and Van Loan 1989] for 
computing the inverse of V in order to get some information about the condition of the base. Factoring 
has great applications in constraining transformations. The idea is to de.ne an application-centered 
basis for a (sub)space of transformations and to factor and interpolate transformations in that (sub)space. 
Interpolating the factors allows one to gen­erate smooth curves that naturally respect the constraints 
as de­.ned by the subspace. In general, a suitable basis for the intended application-speci.c subspace 
might be hard to .nd. A simple solu­tion is to .rst generate a number of permissible transformations 
Ti. The logarithms of the transformation matrices are written as rows of a matrix. . . de.ning the local 
transformations are recorded over 580 key frames of the animation. The log-matrix representations of 
a key frame comprise a row of the representation matrix, which is then fac­tored. The humanoid in the 
given animation follows the H-Anim speci.cation [Web3D Consortium 1999] and has 17 joints, each of which 
provides 6 degrees of freedom (see Figure 5). The de­composition reveals that only 10 base elements are 
necessary to represent the 580 key frames faithfully. This reduces the originally 580·17·16 = 157760 
scalars to 580·10+17·16·10 = 8520, which is a compression factor of roughly 20. This approach might also 
be applied to mesh animations. One has to assign a transformation to each primitive (e.g. vertex or face). 
This might require additional constraints as af.ne transformations offer more degrees of freedom than 
speci.ed by single primitive. Applying a PCA to a deforming mesh could reveal an underlying -log T0- 
process generating the deformation and, thus, essentially decom­ .. .. . . . pose it into af.ne transformations. 
This problem has been identi­ .ed to be the key to ef.cient compression of animations [Lengyel 1999].-log 
Tn- This matrix is decomposed using the SVD, which yields an or­thonormal basis of the subspace of the 
permissible transformations.  6.3 Animation analysis Analysis and compression of motions or animations 
is still a dif.­cult subject. A reason might be that motions are typically non-linear so that powerful 
techniques such as a principal component analysis (PCA) [Jolliffe 1986] are dif.cult to apply. However, 
the techniques presented here allow the application of matrix techniques to analyze arbitrary transformations. 
The SVD has been used by Alexa &#38; M¨ uller [2000] to generate a compact basis for a sequence of deforming 
meshes. The decompo­sition is applied to the vertex positions of key-frame meshes. This approach essentially 
decomposes the translational parts of an ani­mation, while the rotational and scaling parts are not represented 
in a meaningful way. If an animation is mainly comprising local trans­formations a decomposition in the 
space of transformations would be more reasonable. Using the linear matrix operators allows applying 
the SVD to sequences of transformations. As an example, we decompose a given skeleton animation of a 
walking humanoid. The matrices  7 Conclusions In this work, scalar multiples and commutative addition 
of transfor­mations are de.ned, which are geometrically meaningful and easy to compute on the basis of 
the transformation matrices. Together, these operations enable the generation of linear combinations 
of transformations. This allows one to use common techniques for the synthesis and analysis of sets of 
transformations, e.g. to generate animations. The main feature of this approach is the simplicity and 
.exi­bility in developing graphics software. We believe that many of the possible results of this approach 
might be generated by other means, though with considerably more programming effort and use of complex 
numerical techniques. We hope that the simple iterative implementations of matrix exponential and logarithm 
.nd their way in every graphics API. Future work will concentrate on the aspect of analyzing motions 
and animations using linear techniques. Note that the approach works in any dimension and that we have 
not yet evaluated the re­sulting possibilities.  Acknowledgements I would like to thank Roy Mathias 
for introducing me to ma­trix functions and Reinhard Klein for discussions on transforma­tions and Lie 
groups. Johannes Behr has helped with coding and provided the implementation of the walking humanoid 
animation. Wolfgang M¨ullerand the anonymous referees have given invaluable advice and helped tremendously 
in improving this document. This work has been supported by the BMBF grant OpenSG PLUS . References 
ALEXA, M., AND M¨ ULLER, W. 2000. Representing animations by principal compo­nents. Computer Graphics 
Forum 19, 3 (August), 411 418. ISSN 1067-7055. BARR, A. H., CURRIN, B., GABRIEL, S., AND HUGHES, J. F. 
1992. Smooth interpolation of orientations with angular velocity constraints using quaternions. Computer 
Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 313 320. ISBN 0-201-51585-7. Held in Chicago, Illinois. 
BARTELS, R. H., BEATTY, J. C., AND BARSKY, B. A. 1985. An introduction to the use of splines in computer 
graphics. DENMAN, E. D., AND BEAVERS JR., A. N. 1976. The matrix sign function and computations in systems. 
Appl. Math. Comput. 2, 63 94. DO CARMO, M. P. 1992. Riemannian Geometry. Birkh¨auser Verlag, Boston. 
DORST, L., AND MANN, S. 2001. Geometric algebra: a computation framework for geometrical applications. 
submitted to IEEE Computer Graphics &#38; Applications. available as http://www.cgl.uwaterloo.ca/ smann/Papers/CGA01.pdf. 
DYN, N., LEVIN, D., AND GREGORY, J. 1987. A 4-point interpolatory subdivision scheme for curve design. 
Computer Aided Geometric Design 4, 4, 257 268. GABRIEL, S. A., AND KAJIYA, J. T. 1985. Spline interpolation 
in curved space. In SIGGRAPH 85 State of the Art in Image Synthesis seminar notes. GOLUB, G. H., AND 
VAN LOAN, C. F. 1989. Matrix Computations, second ed., vol. 3 of Johns Hopkins Series in the Mathematical 
Sciences. The Johns Hopkins University Press, Baltimore, MD, USA. Second edition. GRASSIA, F. S. 1998. 
Practical parameterization of rotations using the exponential map. Journal of Graphics Tools 3, 3, 29 
48. ISSN 1086-7651. HESTENES, D. 1991. The design of linear algebra and geometry. Acta Applicandae Mathematicae 
23, 65 93. HIGHAM, N. J. 1997. Stable iterations for the matrix square root. Numerical Algo­rithms 15, 
2, 227 242. HORN, R. A., AND JOHNSON, C. A. 1991. Topics in Matrix Analysis. Cambridge University press, 
Cambridge. HOSCHEK, J., AND LASSER, D. 1993. Fundamentals of computer aided geometric design. ISBN 1-56881-007-5. 
JOLLIFFE, I. T. 1986. Principal Component Analysis. Series in Statistics. Springer-Verlag. KENNEY, C., 
AND LAUB, A. J. 1989. Condition estimates for matrix functions. SIAM Journal on Matrix Analysis and Applications 
10, 2, 191 209. KIM, M.-J., SHIN, S. Y., AND KIM, M.-S. 1995. A general construction scheme for unit 
quaternion curves with simple high order derivatives. Proceedings of SIG-GRAPH 95 (August), 369 376. 
ISBN 0-201-84776-0. Held in Los Angeles, Cali­fornia. LENGYEL, J. E. 1999. Compression of time-dependent 
geometry. 1999 ACM Sympo­sium on Interactive 3D Graphics (April), 89 96. ISBN 1-58113-082-1. MARTHINSEN, 
A. 2000. Interpolation in Lie groups. SIAM Journal on Numerical Analysis 37, 1, 269 285. MOLER, C. B., 
AND LOAN, C. F. V. 1978. Nineteen dubious ways to compute the matrix exponential. SIAM Review 20, 801 
836. MURRAY, R. M., LI, Z., AND SASTRY, S. S. 1994. A Mathematical Introduction to Robotic Manipulation. 
CRC Press. NAEVE, A., AND ROCKWOOD, A. 2001. Geometric algebra. SIGGRAPH 2001 course #53. PARK, F. C., 
AND RAVANI, B. 1997. Smooth invariant interpolation of rotations. ACM Transactions on Graphics 16, 3 
(July), 277 295. ISSN 0730-0301. RAMAMOORTHI, R., AND BARR, A. H. 1997. Fast construction of accurate 
quater­nion splines. Proceedings of SIGGRAPH 97 (August), 287 292. ISBN 0-89791­896-7. Held in Los Angeles, 
California. SHOEMAKE, K., AND DUFF, T. 1992. Matrix animation and polar decomposition. Graphics Interface 
92 (May), 258 264. SHOEMAKE,K. 1985.Animatingrotationwithquaternioncurves. Computer Graphics (Proceedings 
of SIGGRAPH 85) 19, 3 (July), 245 254. Held in San Francisco, California. SHOEMAKE, K. 1991. Quaternions 
and 4x4 matrices. Graphics Gems II, 351 354. ISBN 0-12-064481-9. Held in Boston. WEB3D CONSORTIUM. 1999. 
H-Anim. http://ece.uwaterloo.ca:80/ h-anim. ZEFRAN, M., AND KUMAR, V. 1998. Rigid body motion interpolation. 
Computer Aided Design 30, 3, 179 189. ZEFRAN, M., KUMAR, V., AND CROKE, C. 1996. Choice of riemannian 
metrics for rigid body kinematics. In ASME 24th Biennial Mechanisms Conference. ZEFRAN, M. 1996. Continuous 
methods for motion planning. PhD-thesis, U. of Pennsylvania, Philadelphia, PA. ZORIN, D., AND SCHR ODER¨ 
, P. 1999. Subdivision for modeling and animation. SIGGRAPH 1999 course # 47. A Existence of matrix roots 
In the following we will analyze the conditions for the existence of matrix roots, which are intuitively 
parts of the transformation such that all parts are identical and their combined application yields the 
original transformation. We will rather use this intuitive geometric point of view a formal proof of 
the claims made here could be found in [Horn and Johnson 1991, Thm. 6.4.14]. First, it is clear that 
a re.ection cannot be split into several equiv­alent parts and, consequently, transformation matrices 
must have positive determinant. This property is obviously necessary, how­ever, not suf.cient. To understand 
this, we need to analyze the eigenvalues of the transformation matrix as they are representative for 
the nature of the transformation. Note that the product of all eigenvalues is the determinant and, therefore, 
has to be real. If all eigenvalues are (real) positive the transformation is a pure scale and taking 
roots is simple. If the eigenvalues have an imag­inary part the respective transformation has a rotational 
(or shear) component. Because the product of all eigenvalues is real they form two conjugate groups. 
These groups stay conjugate when roots of the eigenvalues are taken so that determinant and, thus, the 
trans­formation matrix is still real. A problem occurs in case of real negative eigenvalues (i.e. the 
imaginary part is zero), which is why we have excluded these transsformations so far. Taking roots of 
these values introduces imaginary parts in the determinant. Because the determinant is positive the number 
of negative eigenvalues has to be even, which allows one to analyze them pairwise. A pair of eigenvalues 
essen­tially de.nes a transformation in 2D and since both eigenvalues are real and negative this transformation 
contains a scale part and ei­ther a rotation by p or two re.ections. If both eigenvalues have the same 
magnitude the transformation is a rotation by p and a uniform scale. Taking roots intuitively means reducing 
the angle of rota­tion and adjusting the uniform scale. However, if the eigenvalues have different magnitude 
the corresponding transformation can be seen as two re.ections or as a rotation together with a non-uniform 
scale. It is impossible to split this transformation into equivalent parts, because the non-uniform scale 
is orientation dependent and the orientation changes due to the rotation. Note that compared to other 
rotational angles it is not possible to interpret this transfor­mation as a shear. Rephrasing this in 
terms of eigenvalues: if the imaginary parts have same magnitude their roots could be assigned different 
signs so that they form a conjugate pair; if they have dif­ferent magnitude this is not possible. Concluding, 
a transformation is divisible, if the real negative eigenvalues of the matrix representing the transformation 
have even multiplicity. Assuming a positive determinant, it is not divisible if it has a pair of negative 
real eigenvalues with different magni­tude. Geometrically, a pair of real negative eigenvalues with differ­ent 
magnitude indicate a rotation by p together with a non-uniform scale. This is the only type of transformation 
that cannot be han­dled (without further treatment) with the approach presented here. A rotation by p 
together with uniform scales as well as other rota­tional angles together with non-uniform scales are 
permissible. For later use we denote this class of transformation matrices T. Note, however, that for 
divisible transforms with real negative eigenval­ues there is no preferred choice for the primary roots 
and, thus, the scalar multiplication operation is not continous for such arguments. B Matrix exponential 
and logarithm &#38; Lie products The connection between the matrix operators de.ned in Sections 3 and 
4 and matrix exponential and logarithm is not quite obvious. Recall the de.nition of exponential and 
logarithms from Equa­tions 3 and 4. One has to be careful when carrying over equivalence transforms for 
exponentials and logarithms from the scalar case, i.e. A+BAB e, eeB, and eeA are generally not the same. 
However, a suf­.cient condition for the expressions to be the same is that A and B commute (see [Horn 
and Johnson 1991, Thm. 6.2.38]). This leads to the identities ..m mA A+...+AAA A e= e= e···e= e, m . 
N. Assuming that emA . Twe can take m-the roots on both sides1, .1/m 1 .1/m mA AA e= eA . em = e thus 
..r erA = eA , r . Q. (9) log AA By de.nition e= A and log e= A. Setting A = log B in Eq. 9 and assuming 
the logarithms of both sides exist yields r log B log Br log e= log e r logB = log(Br). (10) This immediately 
leads to the result for the scalar multiplication given in Equation 5. From this connection of roots 
and logarithms it is clear that real matrix logarithms exist exactly when real matrix roots exist (see 
also [Horn and Johnson 1991, Thm. 6.4.15]). A As said before, eA+B and eeB are generally not the same 
if A and B do not commute. A way of connecting these expressions in the general case is the Lie product 
formula (see [Horn and Johnson 1991, Chapter 6.5] for a derivation): ..n 11 A+B AB e= lim en en (11) 
n.8 Applying this to logA,log B instead of A and B leads to ..nlogA+logB 1 log A1 log B e= lim en en 
n.8 . n log A logB .1/n .1/n = lim ee n.8 ..n A1/nB1/n = lim , (12) n.8 which leads to the representation 
of the addition given in Equa­tion 6. The use of the standard matrix addition in the exponent proves 
that the addition operator is indeed commutative. C Implementation The computation of exponential and 
logarithm of rotations or rigid body motions could be performed using Rodrigues formula (see [Murray 
et al. 1994]). The transformations considered here are more general, however, including (non-uniform) 
scales. We are unclear whether Rodrigues formula generalizes to this group and, therefore, propose an 
implementation based on matrix series. Note that Rodrigues formula is the method of choice if scaling 
is not needed because it is both faster and more robust. The computation of matrix functions such as 
the exponential and the logarithm is non-trivial. For example, evaluating Equation 4 for computing the 
exponential is numerically unstable. The preferred way of computing matrix functions is in many cases 
to use a Schur 1This depends also on our choice of primary roots, which could be a problem where the 
primary matrix root function is discontinous, i.e. for matrices with negative real eigenvalues decomposition 
and evaluate the function on the upper triangle ma­trix [Golub and Van Loan 1989]. However, this work 
is intended for graphics where standard ma­trix packages only offer elementary matrix operations. For 
this reason, implementations are provided using only matrix inversion, multiplication, and addition. 
For the sake of completeness the pseudo-code from some of the original publications is repeated here. 
Moler and van Loan [1978] have investigated several ways of computing the exponential of a matrix A in 
an iterative way and recommend a Pad´e approximation with scaling. Scaling A leads to smaller eigenvalues, 
which in turn, speeds up the convergence of iterative solvers. This is the pseudo-code of the procedure 
(see also [Golub and Van Loan 1989]): A Compute X = ej = max(0,1 + .log2 (.A.).) A = 2- jA D = I; N = 
I; X = I; c = 1 for k = 1 to q c = c(q - k + 1)/(k(2q - k + 1)) X = AX; N = N + cX; D = D +(-1)kcX end 
for X = D-1N X = X2 j The number of iterations q depends on the desired accuracy, q = 6 has proven to 
be a good choice for the applications intended here. The logarithm of a matrix A can be computed using 
a truncated Taylor series. However, convergence is not guaranteed or poor if A is not near the identity 
matrix. By using the identity log A = 2k logA1/2k and, thus, repeatedly taking the square root A can 
be made close enough to identity. Exploiting this equation and using a Taylor approximation has been 
introduced by Kenney and Laub [1989] and leads to the following algorithm: Compute X = log A k = 0 while 
.A - I. > 0.5 A = A1/2 k = k + 1 end while A = I - A Z = A; X = A; i = 1 while .Z. > e Z = ZA; i = i 
+ 1 X = X + Z/i end while X = 2kX However, this algorithms needs to compute square roots of ma­trices. 
Higham [1997] has compared several iterative methods to compute matrix square roots and generally recommends 
the fol­lowing simple method due to Denman and Beavers [1976]: X = A1/2 X = A; Y = I while .XX - A. > 
e iX = X-1; iY = Y -1 Compute 11 X =(X + iY ); Y =(Y + iX) 22 end while Note that all while loops in 
the pseudo codes should terminate after a .xed number of iterations since numerical problems might lead 
to poor convergence.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566593</section_id>
		<sort_key>388</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Character animation]]></section_title>
		<section_page_from>388</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14159670</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566594</article_id>
		<sort_key>388</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Trainable videorealistic speech animation]]></title>
		<page_from>388</page_from>
		<page_to>398</page_to>
		<doi_number>10.1145/566570.566594</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566594</url>
		<abstract>
			<par><![CDATA[We describe how to create with machine learning techniques a generative, speech animation module. A human subject is first recorded using a videocamera as he/she utters a predetermined speech corpus. After processing the corpus automatically, a visual speech module is learned from the data that is capable of synthesizing the human subject's mouth uttering entirely novel utterances that were not recorded in the original video. The synthesized utterance is re-composited onto a background sequence which contains natural head and eye movement. The final output is videorealistic in the sense that it looks like a video camera recording of the subject. At run time, the input to the system can be either real audio sequences or synthetic audio produced by a text-to-speech system, as long as they have been phonetically aligned.The two key contributions of this paper are 1) a variant of the <i>multidimensional morphable model</i> (MMM) to synthesize new, previously unseen mouth configurations from a small set of mouth image prototypes; and 2) a <i>trajectory synthesis technique</i> based on regularization, which is automatically trained from the recorded video corpus, and which is capable of synthesizing trajectories in MMM space corresponding to any desired utterance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[facial modeling]]></kw>
			<kw><![CDATA[lip synchronization]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[optical flow]]></kw>
			<kw><![CDATA[speech synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Video analysis</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P284295</person_id>
				<author_profile_id><![CDATA[81100518553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ezzat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382435</person_id>
				<author_profile_id><![CDATA[81100325458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14215935</person_id>
				<author_profile_id><![CDATA[81100627238]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomaso]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poggio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>181452</ref_obj_id>
				<ref_obj_pid>181447</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARRON, J. L., FLEET, D. J., AND BEAUCHEMIN, S. S. 1994. Performance of optical flow techniques. International Journal of Computer Vision 12, 1, 43-77.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BEIER, T., AND NEELY, S. 1992. Feature-based image metamorphosis. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), vol. 26(2), ACM, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>648720</ref_obj_id>
				<ref_obj_pid>645305</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BERGEN, J., ANANDAN, P., HANNA, K., AND HINGORANI, R. 1992. Hierarchical model-based motion estimation. In Proceedings of the European Conference on Computer Vision, 237-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BEYMER, D., AND POGGIO, T. 1996. Image representations for visual learning. Science 272, 1905-1909.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>889073</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BEYMER, D., SHASHUA, A., AND POGGIO, T. 1993. Example based image analysis and synthesis. Tech. Rep. 1431, MIT AI Lab.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>525960</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BISHOP, C. M. 1995. Neural Networks for Pattern Recognition. Clarendon Press, Oxford.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BLACK, A., AND TAYLOR, P. 1997. The Festival Speech Synthesis System. University of Edinburgh.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344569</ref_obj_id>
				<ref_obj_pid>344564</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BLACK, M., FLEET, D., AND YACOOB, Y. 2000. Robustly estimating changes in image appearance. Computer Vision and Image Understanding, Special Issue on Robust Statistical Techniques in Image Understanding, 8-31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[BLANZ, V., AND VETTER, T. 1999. A morphable model for the synthesis of 3D faces. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Los Angeles, A. Rockwood, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 187-194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[BRAND, M., AND HERTZMANN, A. 2000. Style machines. In Proceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, K. Akeley, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 183-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311537</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[BRAND, M. 1999. Voice puppetry. In Proceedings of SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Los Angeles, A. Rockwood, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 21-28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258880</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[BREGLER, C., COVELL, M., AND SLANEY, M. 1997. Video rewrite: Driving visual speech with audio. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, Los Angeles, CA, Computer Graphics Proceedings, Annual Conference Series, ACM, 353-360.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[BROOKE, N., AND SCOTT, S. 1994. Computer graphics animations of talking faces based on stochastic models. In Intl. Symposium on Speech, Image Processing, and Neural Networks.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[BURT, P. J., AND ADELSON, E. H. 1983. The laplacian pyramid as a compact image code. IEEE Trans. on Communications COM-31, 4 (Apr.), 532-540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E., AND WILLIAMS, L. 1993. View interpolation for image synthesis. In Proceedings of SIGGRAPH 1993, ACM Press / ACM SIGGRAPH, Anaheim, CA, Computer Graphics Proceedings, Annual Conference Series, ACM, 279-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. M., AND MASSARO, D. W. 1993. Modeling coarticulation in synthetic visual speech. In Models and Techniques in Computer Animation, N. M. Thalmann and D. Thalmann, Eds. Springer-Verlag, Tokyo, 139-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>648939</ref_obj_id>
				<ref_obj_pid>645312</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[COOTES, T. F., EDWARDS, G. J., AND TAYLOR, C. J. 1998. Active appearance models. In Proceedings of the European Conference on Computer Vision.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>580470</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[CORMEN, T. H., LEISERSON, C. E., AND RIVEST, R. L. 1989. Introduction to Algorithms. The MIT Press and McGraw-Hill Book Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791528</ref_obj_id>
				<ref_obj_pid>521641</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[COSATTO, E., AND GRAF, H. 1998. Sample-based synthesis of photorealistic talking heads. In Proceedings of Computer Animation '98, 103-110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>355343</ref_obj_id>
				<ref_obj_pid>355338</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[EZZAT, T., AND POGGIO, T. 2000. Visual speech synthesis by morphing visemes. International Journal of Computer Vision 38, 45-57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>889072</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[GIROSI, F., JONES, M., AND POGGIO, T. 1993. Priors, stabilizers, and basis functions: From regularization to radial, tensor, and additive splines. Tech. Rep. 1430, MIT AI Lab, June.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[GUENTER, B., GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. 1998. Making faces. In Proceedings of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Orlando, FL, Computer Graphics Proceedings, Annual Conference Series, ACM, 55-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[HORN, B. K. P., AND SCHUNCK, B. G. 1981. Determining optical flow. Artificial Intelligence 17, 185-203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[HUANG, X., ALLEVA, F., HON, H.-W., HWANG, M.-Y., LEE, K.-F., AND ROSENFELD, R. 1993. The SPHINX-II speech recognition system: an overview (http://sourceforge.net/projects/cmusphinx/). Computer Speech and Language 7, 2, 137-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>939154</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[JONES, M., AND POGGIO, T. 1998. Multidimensional morphable models: A framework for representing and maching object classes. In Proceedings of the International Conference on Computer Vision.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218501</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[LEE, S. Y., CHWA, K. Y., SHIN, S. Y., AND WOLBERG, G. 1995. Image metemorphosis using snakes and free-form deformations. In Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, vol. 29 of Computer Graphics Proceedings, Annual Conference Series, ACM, 439-448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[LEE, Y., TERZOPOULOS, D., AND WATERS, K. 1995. Realistic modeling for facial animation. In Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Series, ACM, 55-62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618498</ref_obj_id>
				<ref_obj_pid>616050</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[LEE, S. Y., WOLBERG, G., AND SHIN, S. Y. 1998. Polymorph: An algorithm for morphing among multiple images. IEEE Computer Graphics Applications 18, 58-71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[LEGOFF, B., AND BENOIT, C. 1996. A text-to-audiovisual-speech synthesizer for french. In Proceedings of the International Conference on Spoken Language Processing (ICSLP).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[MASUKO, T., KOBAYASHI, T., TAMURA, M., MASUBUCHI, J., AND TOKUDA, K. 1998. Text-to-visual speech synthesis based on parameter generation from hmm. In ICASSP.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>116064</ref_obj_id>
				<ref_obj_pid>116058</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[MOULINES, E., AND CHARPENTIER, F. 1990. Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Communication 9, 453-467.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>907312</ref_obj_id>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. I. 1974. A parametric model of human faces. PhD thesis, University of Utah.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>16588</ref_obj_id>
				<ref_obj_pid>16564</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[PEARCE, A., WYVILL, B., WYVILL, G., AND HILL, D. 1986. Speech and expression: A computer solution to face animation. In Graphics Interface.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, D. 1998. Synthesizing realistic facial expressions from photographs. In Proceedings of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Orlando, FL, Computer Graphics Proceedings, Annual Conference Series, ACM, 75-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>889029</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[POGGIO, T., AND VETTER, T. 1992. Recognition and structure from one 2D model view: observations on prototypes, object classes and symmetries. Tech. Rep. 1347, Artificial Intelligence Laboratory, Massachusetts Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>302762</ref_obj_id>
				<ref_obj_pid>302528</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[ROWEIS, S. 1998. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing Systems, The MIT Press, M. I. Jordan, M. J. Kearns, and S. A. Solla, Eds., vol. 10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[SCOTT, K., KAGELS, D., WATSON, S., ROM, H., WRIGHT, J., LEE, M., AND HUSSEY, K. 1994. Synthesis of speaker facial movement to match selected speech sequences. In Proceedings of the Fifth Australian Conference on Speech Science and Technology, vol. 2, 620-625.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[SJLANDER, K., AND BESKOW, J. 2000. Wavesurfer - an open source speech tool. In Proc of ICSLP, vol. 4, 464-467.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[TENENBAUM, J. B., DE SILVA, V., AND LANGFORD, J. C. 2000. A global geometric framework for nonlinear dimensionality reduction. Science 290 (Dec), 2319-2323.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>309427</ref_obj_id>
				<ref_obj_pid>309394</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[TIPPING, M. E., AND BISHOP, C. M. 1999. Mixtures of probabilistic principal component analyzers. Neural Computation 11, 2, 443-482.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[WAHBA, G. 1900. Splines Models for Observational Data. Series in Applied Mathematics, Vol. 59, SIAM, Philadelphia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[WATERS, K. 1987. A muscle model for animating three-dimensional facial expressions. In Computer Graphics (Proceedings of ACM SIGGRAPH 87), vol. 21(4), ACM, 17-24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[WATSON, S., WRIGHT, J., SCOTT, K., KAGELS, D., FREDA, D., AND HUSSEY, K. 1997. An advanced morphing algorithm for interpolating phoneme images to simulate speech. Jet Propulsion Laboratory, California Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[WOLBERG, G. 1990. Digital Image Warping. IEEE Computer Society Press, Los Alamitos, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Trainable Videorealistic Speech Animation Tony Ezzat. Gadi Geiger Tomaso Poggio Center for Biological 
and Computational Learning Massachusetts Institute of Technology Abstract We describe how to create with 
machine learning techniques a gen­erative, videorealistic, speech animation module. A human sub­ject 
is .rst recorded using a videocamera as he/she utters a pre­determined speech corpus. After processing 
the corpus automati­cally, a visual speech module is learned from the data that is capable of synthesizing 
the human subject s mouth uttering entirely novel utterances that were not recorded in the original video. 
The synthe­sized utterance is re-composited onto a background sequence which contains natural head and 
eye movement. The .nal output is vide­orealistic in the sense that it looks like a video camera recording 
of the subject. At run time, the input to the system can be either real audio sequences or synthetic 
audio produced by a text-to-speech system, as long as they have been phonetically aligned. The two key 
contributions of this paper are 1) a variant of the multidimensional morphable model (MMM) to synthesize 
new, previously unseen mouth con.gurations from a small set of mouth image prototypes; and 2) a trajectory 
synthesis technique based on regularization, which is automatically trained from the recorded video corpus, 
and which is capable of synthesizing trajectories in MMM space corresponding to any desired utterance. 
CR Categories: I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism Animation; I.2.10 [Arti.cial 
Intelligence]: Vision and Scene Understanding Video Analysis I.2.10 [Arti.cial Intelligence]: Vision 
and Scene Understanding Motion Keywords: facial modeling, facial animation, morphing, optical .ow, speech 
synthesis, lip synchronization. 1 Overview Is it possible to record a human subject with a video camera, 
pro­cess the recorded data automatically, and then re-animate that sub­ject uttering entirely novel utterances 
which were not included in the original corpus? In this work, we present such a technique for achieving 
videorealistic speech animation. We choose to focus our efforts in this work on the issues related to 
the synthesis of novel video, and not on novel audio synthesis. Thus, novel audio needs to be provided 
as input to our system. This . e-mail: tonebone@ai.mit.edu e-mail:gadi@ai.mit.edu e-mail:tp@ai.mit.edu 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 Figure 1: Some of the synthetic facial con.gurations output by our system. audio can be either 
real human audio (from the same subject or a different subject), or synthetic audio produced by a text-to-speech 
system. All that is required by our system is that the audio be pho­netically transcribed and aligned. 
In the case of synthetic audio from TTS systems, this phonetic alignment is readily available from the 
TTS system itself [Black and Taylor 1997]. In the case of real audio, publicly available phonetic alignment 
systems [Huang et al. 1993] may be used. Our visual speech processing system is composed of two mod­ules: 
The .rst module is the multidimensional morphable model (MMM), which is capable of morphing between a 
small set of pro­totype mouth images to synthesize new, previously unseen mouth con.gurations. The second 
component is a trajectory synthesis module, which uses regularization [Girosi et al. 1993] [Wahba 1990] 
to synthesize smooth trajectories in MMM space for any speci.ed utterance. The parameters of the trajectory 
synthesis mod­ule are trained automatically from the recorded corpus using gradi­ent descent learning. 
Recording the video corpus takes on the order of 15 minutes. Processing of the corpus takes on the order 
of several days, but, apart from the speci.cation of head and eye masks shown in Figure 3, is fully automatic, 
requiring no intervention on the part of the user. The .nal visual speech synthesis module consists of 
a small set of prototype images (46 images in the case presented here) ex­tracted from the recorded corpus 
and used to synthesize all novel sequences. Application scenarios for videorealistic speech animation 
in­clude: user-interface agents for desktops, TVs, or cell-phones; dig­ital actors in movies; virtual 
avatars in chatrooms; very low bitrate coding schemes (such as MPEG4); and studies of visual speech production 
and perception. The recorded subjects can be regular people, celebrities, ex-presidents, or infamous 
terrorists. In the following section, we begin by .rst reviewing the relevant prior work and motivating 
our approach. 2 Background 2.1 Facial Modeling One approach to model facial geometry is to use 3D methods. 
Parke [1974] was one of the earliest to adopt such an approach by creat­ing a polygonal facial model. 
To increase the visual realism of the underlying facial model, the facial geometry is frequently scanned 
in using Cyberware laser scanners. Additionally, a texture-map of the face extracted by the Cyberware 
scanner may be mapped onto the three-dimensional geometry [Lee et al. 1995b]. Guenter [1998] demonstrated 
recent attempts at obtaining 3D face geometry from multiple photographs using photogrammetric techniques. 
Pighin et al. [1998] captured face geometry and textures by .tting a generic face model to a number of 
photographs. Blanz and Vetter [1999] demonstrated how a large database of Cyberware scans may be morphed 
to obtain face geometry from a single photograph. An alternative to the 3D modeling approach is to model 
the talk­ing face using image-based techniques, where the talking facial model is constructed using a 
collection of example images captured of the human subject. These methods have the potential of achiev­ing 
very high levels of videorealism, and are inspired by the recent success of similar sample-based methods 
for audio speech synthe­sis [Moulines and Charpentier 1990]. Image-based facial animation techniques 
need to solve the video generation problem: How does one build a generative model of novel video that 
is simultaneously photorealistic, videorealistic, and parsimonious? Photorealism means that the novel 
generated images exhibit the correct visual structure of the lips, teeth, and tongue. Videorealism means 
that the generated sequences exhibit the correct motion, dynamics, and coarticulation effects [Cohen 
and Massaro 1993]. Parsimony means that the generative model is rep­resented compactly using a few parameters. 
Bregler, Covell, and Slaney [1997] describe an image-based fa­cial animation system called Video Rewrite 
in which the video generation problem is addressed by breaking down the recorded video corpus into a 
set of smaller audiovisual basis units. Each one of these short sequences is a triphone segment, and 
a large database with all the acquired triphones is built. A new audiovi­sual sentence is constructed 
by concatenating the appropriate tri­phone sequences from the database together. Photorealism in Video 
Rewrite is addressed by only using recorded sequences to generate the novel video. Videorealism is achieved 
by using triphone con­texts to model coarticulation effects. In order to handle all the pos­sible triphone 
contexts, however, the system requires a library with tens and possibly hundreds of thousands of subsequences, 
which seems to be an overly-redundant and non-parsimonious sampling of human lip con.gurations. Parsimony 
is thus sacri.ced in favor of videorealism. Essentially, Video Rewrite adopts a decidedly agnostic approach 
to animation: since it does not have the capacity to generate novel lip imagery from a few recorded images, 
it relies on the re­sequencing of a vast amount of original video. Since it does not have the capacity 
to model how the mouth moves, it relies on sam­pling the dynamics of the mouth using triphone segments. 
The approach used in this work presents another approach to solving the video generation problem which 
has the capacity to gen­erate novel video from a small number of examples as well as the capacity to 
model how the mouth moves. This approach is based on the use of a multidimensional morphable model (MMM), 
which is capable of multdimensional morphing between various lip images to synthesize new, previously 
unseen lip con.gurations. MMM s have already been introduced in other works [Poggio and Vetter 1992] 
[Beymer and Poggio 1996] [Cootes et al. 1998] [Jones and Poggio 1998] [Lee et al. 1998] [Blanz and Vetter 
1999] [Black et al. 2000]. In this work, we develop an MMM variant and show its util­ity for facial animation. 
MMM s are powerful models of image appearance because they combine the power of vector space representations 
with the real­ism of morphing as a generative image technique. Prototype exam­ple images of the mouth 
are decomposed into pixel .ow and pixel appearance axes that represent basis vectors of image variation. 
These basis vectors are combined in a multidimensional fashion to produce novel, realistic, previously 
unseen lip con.gurations. As such, an MMM is more powerful than other vector space rep­resentations of 
images which do not model pixel .ow explicitly. Cosatto and Graf [1998], for example, describe an approach 
which is similar to ours, except that their generative model involved simple pixel blending of images, 
which fails to produce realistic transitions between mouth con.gurations. An MMM is also more powerful 
than simple 1-dimensional mor­phing between 2 image end-points [Beier and Neely 1992], as well as techniques 
such as those of Scott et al. [1994] [1997] and Ezzat and Poggio [2000], which morphed between several 
visemes in a pairwise fashion. By embedding the prototype images in a vector space, an MMM is capable 
of generating smooth curves through lip space which handle complex speech animation effects in a non-ad­hoc 
manner. 2.2 Speech Animation Speech animation techniques have traditionally included both keyframing 
methods and physics-based methods, and have been extended more recently to include machine learning methods. 
In keyframing, the animator speci.es particular key-frames, and the system generates intermediate values 
[Parke 1974] [Pearce et al. 1986] [Cohen and Massaro 1993] [LeGoff and Benoit 1996]. In physics-based 
methods, the animator relies on the laws of physics to determine the mouth movement, given some initial 
conditions and a set of forces for all time. This technique, which requires modeling the underlying facial 
muscles and skin, was demonstrated quite effectively by [Waters 1987] [Lee et al. 1995b]. Finally, ma­chine 
learning methods are a new class of animation tools which are trained from recorded data and then used 
to synthesize new motion. Examples include hidden markov models (HMMs), which were demonstrated effectively 
for speech animation by [Brand 1999] [Masuko et al. 1998] [Brooke and Scott 1994]. Speech animation needs 
to solve several problems simultane­ously: .rstly, the animation needs to have the correct motion, in 
the sense that the appropriate phonemic targets need to be realized by the moving mouth. Secondly, the 
animation needs to be smooth, not exhibiting any unnecessary jerks. Thirdly, it needs to display the 
correct dynamics: plosives such as b and p need to occur fast. Finally, speech animation needs to display 
the correct coarticula­tion effects, which determine the effects of neighboring phonemes on the current 
phoneme shape. In this work, we present a trajectory synthesis module to ad­dress the issues of synthesizing 
mouth trajectories with correct mo­tion, smoothness, dynamics, and coarticulation effects. This mod­ule 
maps from an input stream of phonemes (with their respective frame durations) to a trajectory of MMM 
shape-appearance param­eters. This trajectory is then fed into the MMM to synthesize the .nal visual 
stream that represents the talking face. Unlike Video Rewrite [Bregler et al. 1997], which relies on 
an exhaustive sampling of triphone segments to model phonetic con­texts, coarticulation effects in our 
system emerge directly from our speech model. Each phoneme in our model is represented as a lo­calized 
Gaussian target region in MMM space with a particular po­sition and covariance. The covariance of each 
phoneme acts as a Analysis Figure 2: An overview of our videorealistic speech animation sys­tem. spring 
whose tension pulls the trajectory towards each phonetic re­gion with a force proportional to observed 
coarticulation effects in the data. However, unlike Cohen and Massaro [1993] (who also modeled coarticulation 
using localized Gaussian-like regions), our model of coarticulation is not hand-tuned, but rather trained 
from the recorded corpus itself using a gradient descent learning procedure. The training process determines 
the position and shape of the pho­netic regions in MMM space in a manner which optimally recon­structs 
the recorded corpus data.  3 System Overview An overview of our system is shown in Figure 2. After recording 
the corpus (Section 4), analysis is performed to produce the .nal visual speech module. Analysis itself 
consists of three sub-steps: First, the corpus is pre-processed (Section 5) to align the audio and normalize 
the images to remove head movement. Next, the MMM is created from the images in the corpus (Section 6.2). 
Finally, the corpus sequences are analyzed to produce the phonetic models used by the trajectory synthesis 
module (Sections 6.4 and 7.2). Given a novel audio stream that is phonetically aligned, synthe­sis proceeds 
in three steps: First, the trajectory synthesis module is used to synthesize the trajectory in MMM space 
using the trained phonetic models (Section 7). Secondly, the MMM is used to syn­thesize the novel visual 
stream from the trajectory parameters (Sec­tion 6.3). Finally, the post-processing stage composites the 
novel mouth movement onto a background sequence containing natural eye and head movements (Section 8). 
 4 Corpus An audiovisual corpus of a human subject uttering various utter­ances was recorded. Recording 
was performed at a TV studio against a blue chroma-key background with a standard Sony ana­log TV camera. 
The data was subsequently digitized at a 29.97 fps NTSC frame rate with an image resolution of 640 by 
480 and an audio resolution of 44.1KHz. The .nal sequences were stored as Quicktime sequences compressed 
using a Sorenson coder. The recorded corpus lasts for 15 minutes, and is composed of approxi­mately 30000 
frames. The recorded corpus consisted of 1-syllable and 2-syllable words, such as bed and dagger . 
A total of 152 1-syllable words and 156 2-syllable words were recorded. In addition, the corpus included 
105 short sentences, such as The statue was Figure 3: The head, mouth, eye, and background masks used 
in the pre-processing and post-processing steps. Speci.cation of these masks is the only manual step 
required by this system. closed to tourists Sunday . The subject was asked to ut­ter all sentences in 
a neutral expression. In addition, the sentences themselves were designed to elicit no emotions from 
the subject. 5 Pre-Processing The recorded corpus data needs to be pre-processed in several ways before 
it may be processed effectively for re-animation. Firstly, the audio needs to be phonetically aligned 
in order to be able to associate a phoneme for each image in the corpus. We per­form audio alignment 
on all the recorded sequences using the CMU Sphinx system [Huang et al. 1993], which is publicly available. 
Given an audio sequence and an associated text transcript of the speech being uttered, alignment systems 
use forced Viterbi search to .nd the optimal start and end of phonemes for the given audio sequence. 
The alignment task is easier than the speech recognition task because the text of the audio being uttered 
is known apriori. Secondly, each image in the corpus needs to be normalized so that only movement occurring 
in the entire frame is the mouth movement associated with speech. Although the subject was in­structed 
to keep her head steady during recording, residual head movement nevertheless still exists in the .nal 
recorded sequences. Since the head motion is small, we make the simplifying assump­tion that it can be 
approximated as the perspective motion of a plane lying on the surface of the face. Planar perspective 
deformations [Wolberg 1990] have 8 degrees of freedom, and can be inferred using 4 corresponding points 
between a reference frame and the current frame. We employ optical .ow [Horn and Schunck 1981] [Barron 
et al. 1994] [Bergen et al. 1992] to extract correspondences for 640x480 pixels, and use least squares 
to solve the overdeter­mined system of equations to obtain the 8 parameters of the per­spective warp. 
Among the 640x480 correspondences, only those lying within the head mask shown in Figure 3 are used. 
Pixels from the background area are not used because they do not exhibit any motion at all, and those 
from the mouth area exhibit non-rigid mo­tion associated with speech. The images in the corpus also exhibit 
residual eye movement and eye blinks which need to be removed. An eye mask is created (see Figure 3) 
which allows just the eyes from a single frame to be pasted onto the rest of the corpus imagery. The 
eye mask is blurred at the edges to allow a seamless blend between the pasted eyes and the rest of face. 
 6 Multidimensional Morphable Models At the heart of our visual speech synthesis approach is the multidi­mensional 
morphable model representation, which is a generative model of video capable of morphing between various 
lip images to synthesize new, previously unseen lip con.gurations. The basic underlying assumption of 
the MMM is that the com­plete set of mouth images associated with human speech lies in a low-dimensional 
space whose axes represent mouth appearance variation and mouth shape variation. Mouth appearance is 
repre­sented in the MMM as a set of prototype images extracted from the recorded corpus. Mouth shape 
is represented in the MMM as a set of optical .ow vectors [Horn and Schunck 1981] computed auto­matically 
from the recorded corpus. In the work presented here, 46 images are extracted and 46 optical .ow correspondences 
are computed. The low-dimensional MMM space is parameterized by shape parameters . and appearance parameters 
. . The MMM may be viewed as a black box capable of perform­ing two tasks: Firstly, given as input a 
set of parameters (., . ), the MMM is capable of synthesizing an image of the subject s face with that 
shape-appearance con.guration. Synthesis is performed by morphing the various prototype images to produce 
novel, previ­ously unseen mouth images which correspond to the input parame­ters (., . ). Conversely, 
the MMM can also perform analysis: given an in­put lip image, the MMM computes shape and appearance parame­ters 
(., . ) that represent the position of that input image in MMM space. In this manner, it is possible 
to project the entire recorded corpus onto the constructed MMM, and produce a time series of (.t , .t 
) parameters that represent trajectories of mouth motion in MMM space. We term this operation analyzing 
the recorded cor­pus. In the following sections, we describe how a multidimensional morphable model is 
de.ned, how it may be acquired automatically from a recorded video corpus, how it may be used for synthesis, 
and, .nally, how such a morphable model may be used for analysis. 6.1 De.nition An MMM consists of a 
set of prototype images {Ii}Ni=1 that repre­sent the various lip textures that will be encapsulated by 
the MMM. One image is designated arbitrarily to be the reference image I1. Additionally, the MMM consists 
of a set of prototype .ows {Ci}Ni=1 that represent the correspondences between the reference image I1 
and the other prototype images in the MMM. The corre­spondence from the reference image to itself, C1, 
is designated to be an empty, zero, .ow. In this work, we choose to represent the correspondence maps 
using relative displacement vectors: Ci(p)= {di (p), di (p)}. (1) xy A pixel in image I1 at position 
p =(x, y) corresponds to a pixel in image Ii at position (x + di (x, y), y + di (x, y)). xy Previous 
methods for computing correspondence [Beier and Neely 1992] [Scott et al. 1994] [Lee et al. 1995a] adopted 
feature­based approaches, in which a set of high-level shape features com­mon to both images is speci.ed. 
When it is done by hand, how­ever, this feature speci.cation process can become quite tedious and complicated, 
especially in cases when a large amount of im­agery is involved. In this work, we make use of optical 
.ow [Horn and Schunck 1981] [Barron et al. 1994] [Bergen et al. 1992] al­gorithms to estimate this motion. 
This motion is captured as a two-dimensional array of displacement vectors, in the same exact format 
shown in Equation 1. In particular, we utilize the coarse-to­.ne, gradient-based optical .ow algorithms 
developed by [Bergen Figure 4: 24 of the 46 image prototypes included in the MMM. The reference image 
is the top left frame. et al. 1992]. These algorithms compute the desired .ow displace­ments using the 
spatial and temporal image derivatives. In addition, they embed the .ow estimation procedure in a multiscale 
pyrami­dal framework [Burt and Adelson 1983], where initial displacement estimates are obtained at coarse 
resolutions, and then propagated to higher resolution levels of the pyramid. 6.2 Building an MMM An 
MMM must be constructed automatically from a recorded cor­pus of {Ij}Sj=1 images. The two main tasks 
involved are to choose the image prototypes {Ii}Ni=1, and to compute the correspondence {Ci}Ni=1 between 
them. We discuss the steps to do this brie.y be­low. Note that the following operations are performed 
on the entire face region, although they need only be performed on the region around the mouth. 6.2.1 
PCA For the purpose of more ef.cient processing, principal component analysis (PCA) is .rst performed 
on all the images of the recorded video corpus. PCA allows each image in the video corpus to be represented 
using a set of low-dimensional parameters. This set of low-dimensional parameters may thus be easily 
loaded into mem­ory and processed ef.ciently in the subsequent clustering and Dijk­stra steps. Performing 
PCA using classical autocovariance methods [Bishop 1995], however, usually requires loading all the images 
and computing a very large autocovariance matrix, which requires a lot of memory. To avoid this, we adopt 
an on-line PCA method, termed EM-PCA [Roweis 1998] [Tipping and Bishop 1999], which allows us to perform 
PCA on the images in the corpus without loading them all into memory. EM-PCA is iterative, requiring 
several itera­tions, but is guaranteed to converge in the limit to the same principal components that 
would be extracted from the classical autocovari­ance method. The EM-PCA algorithm is typically run in 
this work for 10 iterations. Performing EM-PCA produces a set of D 624x472 principal components and a 
matrix . of eigenvalues. In this work, D = 15 PCA bases are retained. The images in the video corpus 
are subse­quently projected on the principal components, and each image I j is represented with a D-dimensional 
parameter vector p j. 6.2.2 K-means Clustering Selection of the prototype images is performed using 
k-means clus­tering [Bishop 1995]. The algorithm is applied directly on the { p j}Sj=1 low dimensional 
PCA parameters, producing N cluster centers. Typically the cluster centers extracted by k-means clus­tering 
do not coincide with actual image datapoints, so the nearest images in the dataset to the computed cluster 
centers are chosen to be the .nal image prototypes {Ii}Ni=1 for use in our MMM. It should be noted that 
k-means clustering requires the use of an internal distance metric with which to compare distances between 
datapoints and the chosen cluster centers. In our case, since the image parameters are themselves produced 
by PCA, the appropriate distance metric between two points pm and pn is the Mahalanobis distance metric: 
d(pm, pn)=( pm - pn)T .-1(pm - pn) (2) where . is the afore-mentioned matrix of eigenvalues extracted 
by the EM-PCA procedure. We selected N = 46 image prototypes in this work, which are partly shown in 
Figure 4. The top left image is the reference im­age I1. There is nothing magical about our choice of 
46 prototypes, which is in keeping with the typical number of visemes other re­searchers have used [Scott 
et al. 1994] [Ezzat and Poggio 2000]. It should be noted, however, that the 46 prototypes have no explicit 
re­lationship to visemes, and instead form a simple basis set of image textures. 6.2.3 Dijkstra After 
the N = 46 image prototypes are chosen, the next step in building an MMM is to compute correspondence 
between the refer­ence image I1 and all the other prototypes. Although it is in princi­ple possible to 
compute direct optical .ow between the images, we have found that direct application of optical .ow is 
not capable of estimating good correspondence when the underlying lip displace­ments between images are 
greater than 5 pixels. It is possible to use .ow concatenation to overcome this prob­lem. Since the original 
corpus is digitized at 29.97 fps, there are many intermediate frames that lie between the chosen prototypes. 
A series of consecutive optical .ow vectors between each interme­diate image and its successor may be 
computed and concatenated into one large .ow vector that de.nes the global transformation be­tween the 
chosen prototypes (see Appendix A for details on .ow concatenation). Typically, however, prototype images 
are very far apart in the recorded visual corpus, so it is not practical to compute concate­nated optical 
.ow between them. The repeated concatenation that would be involved across the hundreds or thousands 
of intermediate frames leads to a considerably degraded .nal .ow. To compute good correspondence between 
prototypes, a method is needed to .gure out how to compute the path from the reference example I1 to 
the chosen image prototypes Ii without repeated con­catenation over hundreds or thousands of intermediates 
frames. We accomplish this by constructing the corpus graph representation of the corpus: A corpus graph 
is an S-by-S sparse adjacency graph matrix in which each frame in the corpus is represented as a node 
  synthW(C 1 -C i ,C i ) synth C 1 synth -C i Figure 5: The .ow reorientation process: First, Ci is 
subtracted from the synthesized .ow Csynth . Second, this .ow vector is itself 1 forward warped along 
Ci. in a graph connected to k nearest images. The k nearest images are chosen using the k-nearest neighbors 
algorithm [Bishop 1995], and the distance metric used is the Mahalanobis distance in Equation 2 applied 
to the PCA parameters p. Thus, an image is connected in the graph to the k other images that look most 
similar to it. The edge-weight between a frame and its neighbor is the value of the Mahalanobis distance. 
We set k = 20 in this work. After the corpus graph is computed, the Dijkstra shortest path algorithm 
[Cormen et al. 1989] [Tenenbaum et al. 2000] is used to compute the shortest path between the reference 
example I1 and the other chosen image prototypes Ii. Each shortest path produced by the Dijkstra algorithm 
is a list of images from the corpus that cumulatively represent the shortest deformation path from I1 
to Ii as measured by the Mahalanobis distance. Concatenated .ow from I1 to Ii is then computed along 
the intermediate images produced by the Dijkstra algorithm. Since there are 46 images, N = 46 corre­spondences 
{Ci}Ni=1 are computed in this fashion from the reference image I1 to the other image prototypes {Ii}Ni=1. 
  6.3 Synthesis The goal of synthesis is to map from the multidimensional param­eter space (., . ) to 
an image which lies at that position in MMM space. Since there are 46 correspondences, . is a 46-dimensional 
parameter vector that controls mouth shape. Similarly, since there are 46 image prototypes, . is a 46-dimensional 
parameter vector that controls mouth texture. The total dimensionality of (., . ) is 92. Synthesis .rst 
proceeds by synthesizing a new correspondence Csynth using linear combination of the prototype .ows Ci: 
N Csynth = . .iCi. (3) 1 i=1 The subscript 1 in Equation 3 above is used to emphasize that Csynth originates 
from the reference image I1, since all the prototype .ows are taken with I1 as reference. Forward warping 
may be used to push the pixels of the reference image I1 along the synthesized correspondence vector 
Csynth . No­ 1 tationally, we denote the forward warping operation as an operator W(I,C) that operates 
on an image I and a correspondence map C (see Appendix B for details on forward warping). Figure 6: 
Top: Original images from our corpus. Bottom: Corre­sponding synthetic images generated by our system. 
However, a single forward warp will not utilize the image tex­ture from all the examples. In order to 
take into account all image texture, a correspondence re-orientation procedure .rst described in [Beymer 
et al. 1993] is adopted that re-orients the synthesized correspondence vector Csynth so that it originates 
from each of the 1 other example images Ii. Reorientation of the synthesized .ow Csynth 1 proceeds in 
two steps, shown .guratively in Figure 5. First, Ci is subtracted from the synthesized .ow Csynth to 
yield a .ow that 1 contains the correct .ow geometry, but which originates from the reference example 
I1 rather than the desired example image Ii. Sec­ondly, to move the .ow into the correct reference frame, 
this .ow vector is itself warped along Ci. The entire re-orientation process may be denoted as follows: 
Csynth = W(Csynth (4) i 1 - Ci,Ci). Re-orientation is performed for all examples in the example set. 
The third step in synthesis is to warp the prototype images Ii along the re-oriented .ows Csynth to generate 
a set of N warped i image textures Iwarped : i Iwarped = W(Ii,Csynth ii ). (5) The fourth and .nal step 
is to blend the warped images Iwarped i using the . parameters to yield the .nal morphed image: N Imorph 
= .iIwarped i . (6) i=1 Figure 7: Top: Analyzed .i .ow parameters computed for one im­age. Bottom: The 
corresponding analyzed .i texture parameters computed for the same image. The .i texture parameters are 
typi­cally zero for all but a few image prototypes. [Jones and Poggio 1998] [Blanz and Vetter 1999], 
the synthesis al­gorithm is used to synthesize an image Isynth(., . ), which is then compared to the 
novel image using an error metric (ie, the L2 norm). Gradient-descent is then usually performed to change 
the parameters in order to minimize the error, and the synthesis process is repeated. The search ends 
when a local minimum is achieved. Analysis-by-synthesis, however, is very slow in the case when a large 
number of images are involved. In this work we choose another method that is capable of ex­tracting parameters 
(., . ) in one iteration. In addition to the image Inovel to be analyzed, the method requires that the 
correspondence Cnovel from the reference image I1 in the MMM to the novel image Inovel be computed beforehand. 
In our case, most of the novel im­agery to be analyzed will be from the recorded video corpus itself, 
so we employ the Dijkstra approach discussed in Section 6.2.3 to compute good quality correspondences 
between the reference im­ age I1 and Inovel . Combining Equations 3 through 6 together, our MMM synthesis 
Given a novel image Inovel and its associated correspondence may be written as follows: Cnovel , the 
.rst step of the analysis algorithm is to estimate the pa­rameters . which minimize NN Imorph(., . )= 
.iW(Ii, W( .jCj - Ci,Ci)). (7) N i=1 j=1 .Cnovel - .iCi.. (8) Empirically we have found that the MMM 
synthesis technique is capable of surprisingly realistic re-synthesis of lips, teeth, and tongue. However, 
the blending of multiple images in the MMM for synthesis tends to blur out some of the .ner details in 
the teeth and tongue (See Appendix C for a discussion of synthesis blur). Shown in Figure 6 are some 
of the synthetic images produced by our system, along with their real counterparts for comparison.  
6.4 Analysis The goal of analysis is to project the entire recorded corpus i=1 This is solved using the 
pseudo-inverse: -1CT Cnovel . =(CT C) (9) where C above is a matrix containing all the prototype correspon­dences 
{Ci}Ni=1. After the parameters . are estimated, N image warps are syn­thesized in the same manner as 
described in Section 6.3 using .ow­reorientation and warping: {Ij}Sj=1 onto the constructed MMM, and 
produce a time series N Iwarp i = W(Ii, W( .iCi - Ci,Ci)). (10) of (. j, .j)Sj=1 parameters that represent 
trajectories of the original i=1 mouth motion in MMM space. One possible approach for analysis of images 
is to perform The .nal step in analysis is to estimate the values of . as the values analysis-by-synthesis: 
In this approach, used in various forms in which minimize WM corpus reveals that parameters representing 
the same phoneme tend to cluster in MMM space. We represent each phoneme p mathemat­ the .ow and texture 
parameters 1 . The trajectory synthesis problem is framed mathematically as a regularization problem 
[Girosi et al. 1993] [Wahba 1990]. The goal is to synthesize a trajectory y which minimizes an objective 
function E consisting of a target term and a smoothness term: AA OW E =(y - µ)T DT .-1D(y - µ)+. yT 
W T Wy . (12) .. target term smoothness The desired trajectory y is a vertical concatenation of the individ­ 
ual yt = .t terms at each time step (or yt = .t , since we treat .ow and texture parameters separately): 
. yt . . . .. (13)Figure 8: Histograms for the .1 parameter for the \w\, \m\, y = \aa\ and \ow\ phones. 
yT The target term consists of the relevant means µ and covariances . constructed from the phone stream: 
.Pt .. µPt .Inovel - .Ni=1 .iIwarp . subject to i (11) i=1 .i . . . .... . (14) .N .i > 0 .i and µ 
= = 1. . . The non-negativity constraint above on the .i parameters ensures that pixel values are not 
negated. The normalization constraint en­sures that the .i parameters are computed in a normalized manner 
for each frame, which prevents brightness .ickering during synthe­sis. The form of the imposed constraints 
cause the computed .i parameters to be sparse (see Figure 7), which enables ef.cient syn­thesis by requiring 
only a few image warps (instead of the complete µPT .PT The matrix D is a duration-weighting matrix which 
emphasizes the shorter phonemes and de-emphasizes the longer ones, so that the objective function is 
not heavily skewed by the phonemes of longer duration: . DP1 set of 46 warps). Equation 11, which involves 
the minimization I - T of a quadratic cost function subject to constraints, is solved using quadratic 
programming methods. In this work, we use the Matlab function quadprog. Each utterance in the corpus 
is analyzed with respect to the DP2 I - T D = ....... ....... (15). . . DPT 92-dimensional MMM created 
in Section 6.2, yielding a set of I - T zt =(.t , .t ) parameters for each utterance. Analysis takes 
on the order of 15 seconds per frame on a circa 1998 450 MHz Pentium II machine. Shown in Figure 9 in 
solid blue are example analyzed trajectories for .12 and .28 computed for the word tabloid. One possible 
smoothness term consists of the .rst order differ­ ence operator: -II -II . ...  7 Trajectory Synthesis 
W (16) = . . . -I I 7.1 Overview The goal of trajectory synthesis is to map from an input phone stream 
{Pt } to a trajectory yt =(.t , .t ) of parameters in MMM space. After the parameters are synthesized, 
Equation 7 from Sec­tion 6.3 is used to create the .nal visual stream that represents the talking face. 
The phone stream is a stream of phonemes {Pt } represent­ing that phonetic transcription of the utterance. 
For example, the word one may be represented by a phone stream {Pt }15 t=1 = (\w\, \w\, \w\, \w\, \uh\, 
\uh\, \uh\, \uh\, \uh\, \uh\, \n\, \n\, \n\, \n\, \n\). Each element in the phone stream represents one 
image frame. We de.ne T to be the length of the entire utterance in frames. Since the audio is aligned, 
it is possible to examine all the .ow and texture parameters for any particular phoneme. Shown in Fig­ure 
8 are histograms for the .1 parameter for the \w\, \m\, \aa\ and \ow\ phones. Evaluation of the analyzed 
parameters from the Higher orders of smoothness are formed by repeatedly multi­plying W with itself: 
second order W T W T WW , third order W T W T W T WWW , and so on. Finally, the regularizer . determines 
the trade-off between both terms. Taking the derivative of Equation 12 and minimizing yields the following 
equation for synthesis: (DT .-1D + .W T W )y = DT .-1Dµ. (17) Given known means µ, covariances ., and 
regularizer . , syn­thesis is simply a matter of plugging them into Equation 17 and 1Technically, since 
the texture parameters are non-negative, they are best modeled using Gamma distributions not Gaussians. 
In that case, Equation 12 needs to be re-written for Gamma distributions. In practice, however, we have 
found Gaussians to work well enough for texture parameters. 0.5 0 -0.5 Figure 9: Top: The analyzed trajectory 
for .12 (in solid blue), com­pared with the synthesized trajectory for .12 before training (in green 
dots) and after training (in red crosses). Bottom: Same as above, but the trajectory is for .28. Both 
trajectories are from the word tabloid. solving for y using Gaussian elimination. This is done separately 
for the .ow and the texture parameters. In our experiments a regu­larizer of degree four yielding multivariate 
additive quintic splines [Wahba 1990] gave satisfactory results (see next subsection). Coarticulation 
effects in our system are modeled via the magni­tude of the variance .P for each phoneme. Small variance 
means the trajectory must pass through that region in phoneme space, and hence neighboring phonemes have 
little coarticulatory effect. On the other hand, large variance means the trajectory has a lot of .ex­ibility 
in choosing a path through a particular phonetic region, and hence it may choose to pass through regions 
which are closer to a phoneme s neighbors. The phoneme will thus experience large coarticulatory effects. 
There is no explicit model of phonetic dynamics in our system. Instead, phonetic dynamics emerge implicitly 
through the interplay between the magnitude of the variance .P for each phoneme (which determines the 
phoneme s spatial extent), and the input phone stream (which determines the duration in time of each 
phoneme). Equation 12 then determines the speed through a phonetic region in a manner which balances 
nearness to the phoneme with smoothness of the overall trajectory. In general, we .nd the trajectories 
speed up in regions of small duration and small variance (ie plosives), while they slow down in regions 
of large duration and large variance (ie silences). 7.2 Training The means µp and covariances .p for 
each phone p are initialized directly from the data using sample means and covariances. How­ever, the 
sample estimates tend to average out the mouth movement so that it looks under-articulated. As a consequence, 
there is a need to adjust the means and variances to better re.ect the training data. Gradient descent 
learning [Bishop 1995] is employed to adjust the mean and covariances. First, the Euclidean error metric 
is cho­sen to represent the error between the original utterance z and the synthetic utterance y: E =(z 
- y)T (z - y). (18) The parameters {µp, .p} need to be changed to minimize this ob­jective function E. 
The chain rule may be used to derive the rela­tionship between E and the parameters: . . E . . E .T 
. . y = (19) .µi . y .µi .T .. . E . . E . y = . (20) ..ij . y ..ij . E may be obtained from Equation 
18: .y . E = -2(z - y). (21) . y Since y is de.ned according to Equation 17, we can take its derivative 
to compute .y and . y : .µi ..ij . y .µ (DT .-1D + .W T W )= DT .-1D (22) .µi .µi . y (DT .-1D + .W T 
W )= ..ij 2DT .-1 . ..-1D(y - µ). (23) ..ij Finally, gradient descent is performed by changing the previous 
values of the parameters according to the computed gradient: µnew = µold - a. E (24) .µ . E .new .old 
- a = (25) Cross-validation sessions were performed to evaluate the appro­priate value of . and the correct 
level of smoothness W to use. The learning rate a was set to 0.00001 for all trials, and 10 iterations 
performed. Comparison between batch and online updates indi­cated that online updates perform better, 
so this method was used throughout training. Testing was performed on a set composed of 1-syllable words, 
2-syllable words, and sentences not contained in the training set. The Euclidean norm between the synthesized 
tra­jectories and the original trajectories was used to measure error. The results showed that the optimal 
smoothness operator is fourth order and the optimal regularizer is . = 1000. Figure 9 depicts syn­thesized 
trajectories for the .12 and .28 parameters before training (in green dots) and after training (in red 
crosses) for these optimal values of W and . .  8 Post-Processing Due to the head and eye normalization 
that was performed dur­ing the pre-processing stage, the .nal animations generated by our system exhibit 
movement only in the mouth region. This leads to an unnerving zombie -like quality to the .nal animations. 
As in [Cosatto and Graf 1998] [Bregler et al. 1997], we address this issue by compositing the synthesized 
mouth onto a background sequence which contains natural head and eye movement. The .rst step in the composition 
process is to add Gaussian noise to the synthesized images to regain the camera image sensing noise that 
is lost as a result of blending multiple image prototypes in the MMM. We estimate means and variances 
for this noise by comput­ing differences between original images and images synthesized by our system, 
and averaging over 200 images.  guide the compositing process. After noise is added, the synthesized 
sequences are composited onto the chosen background sequence with the help of the masks shown in Figure 
3. The head mask is .rst forward warped using optical .ow to .t across the head of each image of the 
background sequence. Next, optical .ow is computed between each background image and its corresponding 
synthetic image. The synthetic im­age and the mouth mask from Figure 3 are then perspective-warped back 
onto the background image. The perspective warp is estimated using only the .ow vectors lying within 
the background head mask. The .nal composite is made by pasting the warped mouth onto the background 
image using the warped mouth mask. The mouth mask is smoothed at the edges to perform a seamless blend 
between the background image and the synthesized mouth. The compositing process is depicted in Figure 
10. 9 Computational Issues To use our system, an animator .rst provides phonetically anno­tated audio. 
The annotation may be done automatically [Huang et al. 1993], semi-automatically using a text transcript 
[Huang et al. 1993], or manually [Sjlander and Beskow 2000]. Trajectory synthesis is performed by Equation 
17 using the trained phonetic models. This is done separately for the .ow and the texture parameters. 
After the parameters are synthesized, Equa­tion 7 from Section 6.3 is used to create the visual stream 
with the desired mouth movement. Typically only the image prototypes Ii which are associated with top 
10 values of .i are warped, which yields a considerable savings in computation time. MMM synthe­sis takes 
on the order of about 7 seconds per frame for an image resolution of 624x472. The background compositing 
process adds on a few extra seconds of processing time. All times are computed on a 450 MHz Pentium II. 
10 Evaluation We have synthesized numerous examples using our system, span­ning the entire range of 1-syllable 
words, 2-syllable words, short sentences, and long sentences. In addition, we have synthesized songs 
and foreign speech examples. Experiment # subjects % correct t p< Single pres. 22 54.3% 1.243 0.3 Fast 
single pres. 21 52.1% 0.619 0.5 Double pres. 22 46.6% -0.75 0.5 Table 1: Levels of correct identi.cation 
of real and synthetic se­quences. t represents the value from a standard t-test with signif­icance level 
indicated in the p< column. Experimentally we have found that reducing the number of pro­totypes below 
30 degrades the quality of the .nal animations. An open question is whether increasing the number of 
prototypes sig­ni.cantly beyond 46 will lead to even higher levels of videorealism. In terms of corpus 
size, it is possible to optimize the spoken cor­pus so that several words alone elicit the 46 prototypes. 
This would reduce the duration of the corpus from 15 minutes to a few seconds. However, this would degrade 
the quality of the correspondences computed by the Dijkstra algorithm. In addition, the phonetic train­ing 
performed by our trajectory synthesis module would degrade as well. Further systematic experiments need 
to be made in order to evaluate how .nal performance changes with the size of the corpus. We evaluated 
our results by performing three different visual Turing tests to see whether human subjects can distinguish 
be­tween real sequences and synthetic ones. In the .rst experiment ( single presentation ), subjects 
were asked to view one visual se­quence at a time, and identify whether it is real or synthetic. In a 
similar second experiment ( fast single presentation ), the sub­jects were asked to make the judgments 
in a fast manner while the utterances were being presented without pauses in between. In a third experiment 
( double presentation ), the subjects were asked to view pairs of the same utterance, where one item 
in the pair is real and the other is synthetic (but randomly ordered). The subjects in this experiment 
were asked to identify which utterance in the pair is real, and which is synthetic. 16 or 18 utterances 
were presented to each subject, with half being real and half being synthetic. As seen from Table 1, 
performance in all three experiments was close to chance level (50%) and not signi.cantly different from 
it. Finally, we also evaluated our system by performing intelligibil­ity tests in which subjects were 
asked to lip read a set of natural and synthetic utterances. Details on all experiments are forthcoming 
in a separate article. 11 Further Work The main limitation of our technique is the dif.culty of re­compositing 
synthesized mouth sequences into background se­quences which involve 1) large changes in head pose, 2) 
changes in lighting conditions, and 3) changes in viewpoint. All these limi­tations can be alleviated 
by extending our approach from 2D to 3D. It is possible to envision a real-time 3D scanner that is capable 
of recording a 3D video corpus of speech. Alternatively, techniques such as those presented in [Guenter 
et al. 1998] [Pighin et al. 1998] [Blanz and Vetter 1999] can be used to map a 2D video corpus into 3D. 
The geodesic trajectory synthesis equations described by Brand et al. [1999] [2000] are analogous (and 
more sophisticated) than the trajectory synthesis techniques we use (Equations 12 and 17). Although those 
equations require considerably more training data, it is possible they could lead to higher levels of 
videorealism. Clearly the face is used as a conduit to transmit emotion, so one possible avenue to explore 
is the synthesis of speech under vari­ous emotional states. It is possible to record various corpora 
under different emotional states and create MMMs for each state. Dur­ing synthesis, the appropriate MMM 
is selected. An open question to explore is emotional dynamics: how does one transition from a happy 
MMM to a sad MMM? Additionally, there is also a need to learn generative models of head movement and 
eye movement tailored for the type of speech being synthesized. 12 Acknowledgments The authors would 
like to dedicate this work in the memory of Christian Benoit [LeGoff and Benoit 1996] who was a pioneer 
in audiovisual speech research. The authors would like to thank Meredith and Dynasty Models; Craig Milanesi, 
Dave Konstine, Jay Benoit from MIT Video Productions; Marypat Fitzgerald and Casey Johnson from CBCL; 
Volker Blanz, Thomas Vetter, Demetri Terzopoulos, Ryan Rifkin, and anonymous reviewers for countless 
helpful comments on the paper; David Beymer, Mike Jones, Vinay Kumar, Steve Lines, Roberto Brunelli and 
Steve Librande for lay­ing the groundwork. This work was partially funded by the Association Christian 
Benoit, NTT Japan, Of.ce of Naval Research, DARPA, National Science Foundation (Adaptive Man-Machine 
Interfaces). Addi­tional support was provided by: Central Research Institute of Elec­tric Power Industry, 
Eastman Kodak Company, DaimlerChrysler AG, Honda R&#38;D Co., Ltd., Komatsu Ltd., Toyota Motor Corpo­ration 
and The Whitaker Foundation. References BARRON, J. L., FLEET, D. J., AND BEAUCHEMIN, S. S. 1994. Performance 
of optical .ow techniques. International Journal of Computer Vision 12, 1, 43 77. BEIER, T., AND NEELY, 
S. 1992. Feature-based image metamorphosis. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), vol. 
26(2), ACM, 35 42. BERGEN, J., ANANDAN, P., HANNA, K., AND HINGORANI, R. 1992. Hierarchical model-based 
motion estimation. In Proceedings of the European Conference on Computer Vision, 237 252. BEYMER, D., 
AND POGGIO, T. 1996. Image representations for visual learning. Science 272, 1905 1909. BEYMER, D., SHASHUA, 
A., AND POGGIO, T. 1993. Example based image analysis and synthesis. Tech. Rep. 1431, MIT AI Lab. BISHOP, 
C. M. 1995. Neural Networks for Pattern Recognition. Clarendon Press, Oxford. BLACK, A., AND TAYLOR, 
P. 1997. The Festival Speech Synthesis System. University of Edinburgh. BLACK, M., FLEET, D., AND YACOOB, 
Y. 2000. Robustly estimating changes in image appearance. Computer Vision and Image Understanding, Special 
Issue on Robust Statistical Techniques in Image Understanding, 8 31. BLANZ, V., AND VETTER, T. 1999. 
A morphable model for the synthesis of 3D faces. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, 
Los Angeles, A. Rockwood, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 187 194. 
BRAND, M., AND HERTZMANN, A. 2000. Style machines. In Proceedings of SIG-GRAPH 2000, ACM Press / ACM 
SIGGRAPH, K. Akeley, Ed., Computer Graph­ics Proceedings, Annual Conference Series, ACM, 183 192. BRAND, 
M. 1999. Voice puppetry. In Proceedings of SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Los Angeles, A. Rockwood, 
Ed., Computer Graphics Proceed­ings, Annual Conference Series, ACM, 21 28. BREGLER, C., COVELL, M., AND 
SLANEY, M. 1997. Video rewrite: Driving visual speech with audio. In Proceedings of SIGGRAPH 1997, ACM 
Press / ACM SIG-GRAPH, Los Angeles, CA, Computer Graphics Proceedings, Annual Conference Series, ACM, 
353 360. BROOKE, N., AND SCOTT, S. 1994. Computer graphics animations of talking faces based on stochastic 
models. In Intl. Symposium on Speech, Image Processing, and Neural Networks. BURT, P. J., AND ADELSON, 
E. H. 1983. The laplacian pyramid as a compact image code. IEEE Trans. on Communications COM-31, 4 (Apr.), 
532 540. CHEN, S. E., AND WILLIAMS, L. 1993. View interpolation for image synthesis. In Proceedings of 
SIGGRAPH 1993, ACM Press / ACM SIGGRAPH, Anaheim, CA, Computer Graphics Proceedings, Annual Conference 
Series, ACM, 279 288. COHEN, M. M., AND MASSARO, D. W. 1993. Modeling coarticulation in synthetic visual 
speech. In Models and Techniques in Computer Animation, N. M. Thalmann and D. Thalmann, Eds. Springer-Verlag, 
Tokyo, 139 156. COOTES, T. F., EDWARDS, G. J., AND TAYLOR, C. J. 1998. Active appearance models. In Proceedings 
of the European Conference on Computer Vision. CORMEN, T. H., LEISERSON, C. E., AND RIVEST, R. L. 1989. 
Introduction to Algorithms. The MIT Press and McGraw-Hill Book Company. COSATTO, E., AND GRAF, H. 1998. 
Sample-based synthesis of photorealistic talking heads. In Proceedings of Computer Animation 98, 103 
110. EZZAT, T., AND POGGIO, T. 2000. Visual speech synthesis by morphing visemes. International Journal 
of Computer Vision 38, 45 57. GIROSI, F., JONES, M., AND POGGIO, T. 1993. Priors, stabilizers, and basis 
func­tions: From regularization to radial, tensor, and additive splines. Tech. Rep. 1430, MIT AI Lab, 
June. GUENTER, B., GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. 1998. Mak­ing faces. In Proceedings 
of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Orlando, FL, Computer Graphics Proceedings, Annual Conference 
Series, ACM, 55 66. HORN, B. K. P., AND SCHUNCK, B. G. 1981. Determining optical .ow. Arti.cial Intelligence 
17, 185 203. HUANG, X., ALLEVA, F., HON, H.-W., HWANG, M.-Y., LEE, K.-F., AND ROSEN-FELD, R. 1993. The 
SPHINX-II speech recognition system: an overview (http://sourceforge.net/projects/cmusphinx/). Computer 
Speech and Language 7, 2, 137 148. JONES, M., AND POGGIO, T. 1998. Multidimensional morphable models: 
A frame­work for representing and maching object classes. In Proceedings of the Interna­tional Conference 
on Computer Vision. LEE, S. Y., CHWA, K. Y., SHIN, S. Y., AND WOLBERG, G. 1995. Image metemor­phosis 
using snakes and free-form deformations. In Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, vol. 
29 of Computer Graphics Proceed­ings, Annual Conference Series, ACM, 439 448. LEE, Y., TERZOPOULOS, D., 
AND WATERS, K. 1995. Realistic modeling for facial animation. In Proceedings of SIGGRAPH 1995, ACM Press 
/ ACM SIGGRAPH, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Se­ries, ACM, 
55 62. LEE, S. Y., WOLBERG, G., AND SHIN, S. Y. 1998. Polymorph: An algorithm for morphing among multiple 
images. IEEE Computer Graphics Applications 18, 58 71. LEGOFF, B., AND BENOIT, C. 1996. A text-to-audiovisual-speech 
synthesizer for french. In Proceedings of the International Conference on Spoken Language Pro­cessing 
(ICSLP). MASUKO, T., KOBAYASHI, T., TAMURA, M., MASUBUCHI, J., AND TOKUDA, K. 1998. Text-to-visual speech 
synthesis based on parameter generation from hmm. In ICASSP. MOULINES, E., AND CHARPENTIER, F. 1990. 
Pitch-synchronous waveform process­ing techniques for text-to-speech synthesis using diphones. Speech 
Communication 9, 453 467. PARKE, F. I. 1974. A parametric model of human faces. PhD thesis, University 
of Utah. PEARCE, A., WYVILL, B., WYVILL, G., AND HILL, D. 1986. Speech and expres­sion: A computer solution 
to face animation. In Graphics Interface. PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, 
D. 1998. Synthesizing realistic facial expressions from photographs. In Proceedings of SIG-GRAPH 1998, 
ACM Press / ACM SIGGRAPH, Orlando, FL, Computer Graphics Proceedings, Annual Conference Series, ACM, 
75 84. POGGIO, T., AND VETTER, T. 1992. Recognition and structure from one 2D model view: observations 
on prototypes, object classes and symmetries. Tech. Rep. 1347, Arti.cial Intelligence Laboratory, Massachusetts 
Institute of Technology. ROWEIS, S. 1998. EM algorithms for PCA and SPCA. In Advances in Neural Infor­mation 
Processing Systems, The MIT Press, M. I. Jordan, M. J. Kearns, and S. A. Solla, Eds., vol. 10. SCOTT, 
K., KAGELS, D., WATSON, S., ROM, H., WRIGHT, J., LEE, M., AND HUSSEY, K. 1994. Synthesis of speaker facial 
movement to match selected speech sequences. In Proceedings of the Fifth Australian Conference on Speech 
Science and Technology, vol. 2, 620 625. SJLANDER, K., AND BESKOW, J. 2000. Wavesurfer -an open source 
speech tool. In Proc of ICSLP, vol. 4, 464 467. TENENBAUM, J. B., DE SILVA, V., AND LANGFORD, J. C. 2000. 
A global geometric framework for nonlinear dimensionality reduction. Science 290 (Dec), 2319 2323. TIPPING, 
M. E., AND BISHOP, C. M. 1999. Mixtures of probabilistic principal component analyzers. Neural Computation 
11, 2, 443 482. WAHBA, G. 1990. Splines Models for Observational Data. Series in Applied Mathe­matics, 
Vol. 59, SIAM, Philadelphia. WATERS, K. 1987. A muscle model for animating three-dimensional facial expres­sions. 
In Computer Graphics (Proceedings of ACM SIGGRAPH 87), vol. 21(4), ACM, 17 24. WATSON, S., WRIGHT, J., 
SCOTT, K., KAGELS, D., FREDA, D., AND HUSSEY, K. 1997. An advanced morphing algorithm for interpolating 
phoneme images to simulate speech. Jet Propulsion Laboratory, California Institute of Technology. WOLBERG, 
G. 1990. Digital Image Warping. IEEE Computer Society Press, Los Alamitos, CA.  A Appendix: Flow Concatenation 
Given a series of consecutive images I0, I1,... In, we would like to construct the correspondence map 
C0(n) relating I0 to In. We focus on the case of the 3 images Ii-1, Ii, Ii+1 since the concate­nation 
algorithm is simply an iterative application of this 3-frame base case. Optical .ow is .rst computed 
between the consecutive frames to yield C. Note that it is not correct to con­ (i-1)i,Ci(i+1) struct 
Cas the simple addition of Cbecause (i-1)(i+1) (i-1)i +Ci(i+1) the two .ow .elds are with respect to 
two different reference im­ages. Vector addition needs to be performed with respect to a com­mon origin. 
Our concatenation thus proceeds in two steps: to place all vector .elds in the same reference frame, 
the correspondence map Ci(i+1) itself is warped backwards [Wolberg 1990] along C(i-1)i to create Cwarped 
Now Cwarped . and Care both added to produce an i(i+1) i(i+1)(i-1)i approximation to the desired concatenated 
correspondence: for j = 0...height, for i = 0...width, x=i+ dx(i,j); y=j+ dy(i,j); Iwarped (i,j) = BILINEAR 
(I, x, y); Figure 11: BACKWARD WARP algorithm B Appendix: Forward Warping Forward warping may be viewed 
as pushing the pixels of an im­age I along the computed .ow vectors C. We denote the forward warping 
operation as an operator W(I,C) that operates on an image I and a correspondence map C, producing a warped 
image Iwarped as .nal output. A procedural version of our forward warp is shown in Figure 12. It is also 
possible to forward warp a correspondence map C. along another correspondence C, which we denote as W(C. 
,C). In this scenario, the x and y components of C.(p)= {d. (p), d. (p)} xy are treated as separate images, 
and warped individually along C: W(dx. ,C) and W(dy. ,C). for j=0...height, for i=0...width, x = ROUND 
(i + .dx(i,j) ); y = ROUND (j + .dy(i,j) ); if (x,y) are within the image Iwarped (x,y) = I(i,j); Figure 
12: FORWARD WARP algorithm  C Appendix: Hole-Filling Forward warping produces black holes which occur 
in cases where a destination pixel was not .lled in with any source pixel value. This occurs due to inherent 
nonzero divergence in the optical .ow, particularly around the region where the mouth is expanding. To 
remedy this, a hole-.lling algorithm [Chen and Williams 1993] was adopted which pre-.lls a destination 
image with a special reserved background color. After warping, the destination image is traversed in 
rasterized order and the holes are .lled in by interpolating lin­early between their non-hole endpoints. 
In the context of our synthesis algorithm in Section 6.3, hole-.lling can be performed before blending, 
or after blending. Throughout this paper, we assume hole-.lling is performed before blending, which allows 
us to subsume the hole-.lling procedure into our forward warp operator W and simplify our notation. Con­sequently 
(as in Equation 6), the blending operation becomes a simple linear combination of the hole-.lled warped 
intermediates Iwarped i . In practice, however, we perform hole-.lling after blending, which reduces 
the size of the holes that need to be .lled, and leads to a considerable reduction in synthesis blur. 
Post-blending hole­.lling requires a more complex blending algorithm than as noted in Equation 6 because 
the blending algorithm now needs to keep track of holes and non-holes in the warped intermediate images 
Iiwarped : .iIwarped .Iwarped (x, y) (x,y).i=hole Imorph(x, y)= i (27) .Iwarped .i (x,y). =hole i (i-1)i 
+ Cwarped Typically an accumulator array is used to keep track of the denom- C= C. (26) (i-1)(i+1) i(i+1) 
inator term in Equation 27 above. The synthesized mouth images A procedural version of our backwarp 
warp is shown in .gure 11. shown in Figure 6 were generated using post-blending hole-.lling. BILINEAR 
refers to bilinear interpolation of the 4 pixel values clos­est to the point (x,y).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566595</article_id>
		<sort_key>399</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Turning to the masters]]></title>
		<subtitle><![CDATA[motion capturing cartoons]]></subtitle>
		<page_from>399</page_from>
		<page_to>407</page_to>
		<doi_number>10.1145/566570.566595</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566595</url>
		<abstract>
			<par><![CDATA[In this paper, we present a technique we call "cartoon capture and retargeting" which we use to track the motion from traditionally animated cartoons and retarget it onto 3-D models, 2-D drawings, and photographs. By using animation as the source, we can produce new animations that are expressive, exaggerated or non-realistic.Cartoon capture transforms a digitized cartoon into a cartoon motion representation. Using a combination of affine transformation and key-shape interpolation, cartoon capture tracks non-rigid shape changes in cartoon layers. Cartoon retargeting translates this information into different output media. The result is an animation with a new look but with the movement of the original cartoon.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[computer vision]]></kw>
			<kw><![CDATA[deformations]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[object tracking]]></kw>
			<kw><![CDATA[shape blending]]></kw>
			<kw><![CDATA[video]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Morphological</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15019977</person_id>
				<author_profile_id><![CDATA[81100027036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bregler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P173241</person_id>
				<author_profile_id><![CDATA[81100576482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lorie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loeb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382432</person_id>
				<author_profile_id><![CDATA[81100637115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Erika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382436</person_id>
				<author_profile_id><![CDATA[81100017193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hrishi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deshpande]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALEXA, M., AND M&#220;LLER, W. 2000. Representing animations by principal components. 411-418. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344859</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALEXA, M., COHEN-OR, D., AND LEVIN, D. 2000. As-rigid-as-possible shape interpolation. In Proceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, 157-164. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>648720</ref_obj_id>
				<ref_obj_pid>645305</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BERGEN, J., ANANDAN, P., HANNA, K., AND HINGORANI, R. 1992. Hierarchical model-based motion estimation. In ECCV, 237-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>648985</ref_obj_id>
				<ref_obj_pid>645309</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLACK, M., AND JEPSON, A., 1996. Eigentracking: robust matching and tracking of articulated objects using a view-based representation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BLANZ, V., AND VETTER, T. August 1999. A morphable model for the synthesis of 3d faces. Proceedings of SIGGRAPH 99, 187-194. ISBN 0-20148-560-5. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BREGLER, C., AND OMOHUNDRO, S. 1994. Surface learning with applications to lipreading. In Advances in Neural Information Processing Systems, Morgan Kaufman, San Francisco, A. Cowan, Tesauro, Ed.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>11275</ref_obj_id>
				<ref_obj_pid>11274</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CANNY, J. 1986. A computational approach to edge detection. IEEE Trans. Pattern Anal. Mach. Intell. 8, 679-698.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GILL, P., MURRAY, W., AND WRIGHT, M. 1981. Practical Optimization. Academic Press, London, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargeting motion to new characters. In SIGGRAPH 98 Conference Proceedings, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, 33-42. ISBN 0-89791-999-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HORN, B., AND SCHUNK, G. 1981. Determining optical flow. Artificial Intelligence 17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[JOLLIFE, I. T. 1986. Principal Components Analysis. New York: Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LANITIS, A., TAYLOR, C., COOTES, T., AND AHMED, T. 1995. Automatic interpretation of human faces and hand gestures using flexible models. In International Workshop on Automatic Face- and Gesture-Recognition.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LUCAS, B., AND KANADE, T. 1981. An iterative image registration technique with an application to stereo vision. Proc. 7th Int. Joinnt Conf. on Art. Intell.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344964</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[NGO, T., DANA, J., CUTRELL, D., DONALD, B., LOEB, L., AND ZHU, S. 2000. Accessible animation and customizable graphics via simplicial configuration modeling. In SIGGRAPH 2000 Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569955</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. 1972. Computer generated animation of faces. In ACM National Conferences, 451-457.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, D. H. 1998. Synthesizing realistic facial expressions from photographs. In SIGGRAPH 98 Conference Proceedings, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, 75-84. ISBN 0-89791-999-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[POPOVI&#262;, Z., AND WITKIN, A. 1999. Physically based motion transformation. In SIGGRAPH 99 Conference Proceedings, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SHI, J., AND TOMASI, C. 1994. Good features to track. In CVPR.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WHITAKER, H., AND HALAS, J. 1981. Timing for Animation. FOCAL PRESS LTD.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383290</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[YONG NOH, J., AND NEUMANN, U. 2001. Expression cloning. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 277-288. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Turning to the Masters: Motion Capturing Cartoons* Christoph Bregler Lorie Loeb Erika Chuang Hrishi 
Deshpande Stanford University Abstract In this paper, we present a technique we call cartoon capture 
and retargeting which we use to track the motion from traditionally animated cartoons and retarget it 
onto 3-D models, 2-D drawings, and photographs. By using animation as the source, we can produce new 
animations that are expressive, exaggerated or non-realistic. Cartoon capture transforms a digitized 
cartoon into a cartoon motion representation. Using a combination of af.ne transforma­tion and key-shape 
interpolation, cartoon capture tracks non-rigid shape changes in cartoon layers. Cartoon retargeting 
translates this information into different output media. The result is an animation with a new look but 
with the movement of the original cartoon. Keywords: Animation, Computer Vision, Deformations, Morph­ing, 
Object Tracking, Shape Blending, Video. 1 Introduction In this paper, we present techniques to extract 
motion from tradi­tional animation, a process called cartoon capture, and reuse these motions for other 
characters, a process called cartoon retargeting. 1.1 Motivation We can think of animation as having 
two dimensions: the visual style (how the image looks, how it is rendered, the style of the draw­ing 
or model) and the motion style (how the characters move, the amount of exaggeration, use of cartoon physics 
and way in which the animation principles are used). The visual style of an anima­tion can be anything 
from photo-realistic to abstract. The motion style also varies from one animation to another. It can 
range from robotic, to realistic to highly expressive (Figure 1). Visual style and motion style usually 
go together. Most tra­ditional animated cartoons have highly exaggerated drawings and highly exaggerated 
motions. These animations are usually created by highly trained animators who use animation principles 
to create motion that is expressive and stylized. In contrast, photo-realistic computer animations are 
generally animated with realistic motion. Physical simulation and motion capture have been fairly effective 
for creating such realistic motion. Simulation techniques are mainly used to create low-level physical 
*http://graphics.stanford.edu/projects/tooncap bregler,lorie,echuang,hrshi@graphics.stanford.edu Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 Figure 1: Animation Styles: Animation has both a visual style and a motion style. Motion capture works 
within the green area of this .gure. The pink area represents the realm of cartoon capture. phenomena 
and motion capture is used to create realistic human motions (Green area in Figure 1). Many research 
efforts have been devoted recently on how to reuse motion capture for different animation domains, how 
to adapt motion capture to different characters, how to retarget to different activities, and how to 
apply machine learning techniques to anima­tion. In the game and special effects industry, motion capture 
is increasingly used to ef.ciently create realistic motion for movies and video games. Furthermore, much 
progress has been made in improving motion capture technologies. Higher accuracy, real-time and user-friendly 
systems, and the increased availability of large public motion capture databases and service bureaus, 
give even more reasons to use motion capture data for many kinds of char­acter animation. In contrast, 
stylized and expressive animation, as done by tra­ditional animators, is seen as time consuming or even 
impossible. Skilled artists are rare and the costs are often prohibitive. Currently, traditionally drawn 
animations cannot be easily reused or transfered to different domains and characters, as is possible 
with motion cap­ture. The main point of this paper is to present new techniques that bridge the gap between 
those two worlds. Computer animation techniques using motion capture input are limited to a realistic 
mo­tion style. Cartoon capture is able to transfer the cartoon motion style into a representation that 
can be used in the same fashion as standard motion capture. It can be seen as a new front-end when­ever 
a more expressive and stylized motion style is needed. We turn to the masters highly trained traditional 
animators in order to .nd the best examples of expressive motion. Example applications that call for 
expressive and stylized mo­tions include game development, consumer level applications, fea­ture .lm 
production, and general research on motion styles.  Figure 2: Overview chart of cartoon capture and 
retargeting. Often traditional animators can create the desired motion style much quicker with pencil 
test drawings, than with tweaking 3D pa­rameters in a computer animation program. This technique allows 
them to animate on paper, and then transfer the motion to a 2D or 3D character. 1.2 Scope The pink area 
in Figure 1, illustrates the range of experiments we are conducting with this paper. We demonstrate how 
we can isolate the motion style of cartoon animation (upper part of Y-axis) and apply it to different 
visual styles (the entire range on the X-axis). One future direction of this research is to qualify and 
quantify that pink area. As we better understand the differences between realistic motion and expressive 
cartoon motion, we can create .l­ters that could be used in conjunction with motion capture to inten­sify 
or change the style of the movement. Examples include .lters that could add squash and stretch, exaggeration 
or anticipation and follow-through. This could be used to create charicatures of famous people, or simply 
to allow more variety of motion from a single mo­tion capture shot. 1.3 Challenges Because we begin 
with a 2-dimensional animated video, existing motion capture techniques are not adequate. There are new 
chal­lenges that need to be addressed: 1. Cartoon characters have no markers. Conventional tracking techniques 
that rely on point features cannot be applied here. 2. The low frame rate makes tracking dif.cult. Typical 
motion capture systems sample at 60-200 frames per second, while animation is usually recorded at 24 
frames per second. Each image is often held for 2 frames, thus using only 12 images per second. This 
makes the change between images relatively large. 3. Identifying limb locations in cartoons is dif.cult. 
Also, car­toon objects tend to undergo large degrees of nonrigid de­formation throughout the sequence. 
Standard skeletal model­based motion capture techniques are not able to handle such motions.  Vision-based 
tracking techniques and new modeling techniques are beginning to tackle many of these issues. Much of 
our cartoon capture process builds on such vision based techniques. Because we want to retarget the motion 
onto a new image, model or photograph, there are other unique challenges: 1. Much of the current retargeting 
techniques are based on skele­tal models, and address challenges on how to map and scale parameters from 
one character s kinematics to a different character s kinematics. We circumvent those problems in us­ing 
a key-shape based retargeting technique. 2. Since we capture from cartoons, we only have 2D informa­tion. 
Some of the investigated retargeting domains are 3D output models. This is, in general, an under-constrained 
setup. We also show how we can tackle 3D motion with 2D to 3D key-shape mappings. 3. In order to preserve 
the style, the absolute motion might differ signi.cantly between input and output. For instance a small 
character with long arms will swing those arms differently than a bigger character with shorter arms. 
We can handle some of those mappings in de.ning different key-shapes and different interpolation functions 
for the input and output do­main. But there are many other motion changes, due to dif­ferent character 
physics, that we can not tackle. A heavy walk will remain a heavy walk , no matter how the weight changes 
from input to output character.  1.4 Cartoon Capture and Retargeting Figure 2 shows an overview chart 
of our technique. The input to cartoon capture is the digitized video, and a user-de.ned set of key­shapes 
(chosen from the source sequence). Cartoon capture trans­forms a digitized cartoon into a cartoon motion 
representation. We parameterize the motion with a combination of af.ne transforma­tion and key-weight 
vectors. In this way, we can describe a wide range of motion and non-rigid shape deformations. For the 
cartoon retarget process, the user has to de.ne for each input key-shape a corresponding output key-shape, 
or key-image, or 3D key-model. The motion parameters are mapped from the source to the target. We believe 
that by maintaining the timing and motion parameters from the original animation, we can maintain most 
of the essence of the expressive movement [Whitaker and Halas 1981]. The organization of this paper is 
as follows: after a discussion of related work in section 2, we introduce the cartoon motion repre­sentation 
in section 3.1. In section 3.2 and 3.3, we will describe two capture processes. Section 3.4 describes 
cartoon retargeting, using the examples we created. Finally, section 4 summarizes our results and discusses 
future work and improvement.  2 Related Work Some previous work has addressed parts of the challenges 
above. [Gleicher 1998] describes a method for retargeting motions onto .gures with different proportions. 
The retargeting problem was framed as a constraint optimization problem. [Popovi´c and Witkin 1999] applied 
the principles of physics based animation and con­straint optimization formulation on motion captured 
data. This method produces realistic motions and can be used on characters with different kinematic structure. 
Both of these methods require the input as joint angle data and applies the motion to articulated .gures 
only. Expression Cloning [yong Noh and Neumann 2001] introduces a method to retarget facial expression 
animation to another head model. This method maps the motion vectors of the source face mesh to the target 
mesh. This approach can be used for retargeting of non-rigid shapes, but it requires the input as a set 
of 3D motion vectors. It also assumes that the source and target object have very similar topology, otherwise 
the retargeting of motion vectors would not be meaningful. In cartoon retargeting, we want a variety 
of output formats, such as 3D models, 2D images and photographs, therefore a more .exible technique is 
necessary. There are many other ways to parameterize motion in computer animation. Our work is most related 
to representing animation as interpolation of key shapes as is often used in facial expression ani­mation 
[Parke 1972; Pighin et al. 1998]. Professional 3D animation tools also have software for creating animation 
from key-shapes. Most of this work focuses on the synthesis aspect of this method and are applied to 
the area of facial animation. Very little work addresses the inverse problem of extracting the weights 
for the key­shaped animation. There has been extensive work on using Principle Component Analysis (PCA) 
and other sub-space techniques to estimate shape or motion variations [Bregler and Omohundro 1994; Lanitis 
et al. 1995; Black and Jepson 1996; Blanz and Vetter August 1999], or sequences of animation [Alexa and 
M¨uller 2000] but this has not been used for retargeting. So far, the focus of those papers is ef­.cient 
data representation, learning, or compression applications. Furthermore, PCA decompositions result in 
a set of orthogonal ba­sis vectors. While this is a mathematically elegant representation, the basis 
vectors do not have any physical meanings. In cartoon capture, we want to use key poses that visually 
make sense to the animators. The goal is that animators can design key poses for the output media of 
their choice by looking at the key poses for the in­put cartoon and retarget the motion onto the output 
characters. We also want something that is intuitive and easy to use. The animators can make sensible 
animations using this method without having to understand the details of the technology. A related cartoon 
interpolation system has been demonstrated by [Ngo et al. 2000]. It uses a con.guration-space model based 
on simplicial complexes, instead of a key-shape based model. It has only been demonstrated for interpolation 
tasks, but could also be extended to be used for the capture process.  3 Technical Approach As mentioned 
previously, the goal of this project is to isolate the motion style of an existing cartoon animation 
and apply the same style to a new output domain. We .rst describe the parametric mo­tion representation. 
Then, we explain how we can capture the pa­rameters from cartoon input. Finally, we show how we apply 
the captured motion style to different output characters.  Figure 3: Shows the motion of the bouncing 
ball encoded in terms of the 6 af.ne parameters. 3.1 Modeling Cartoon Motion We describe cartoon motion 
as a composition of two types of defor­mations: 1) Af.ne deformations, that encode the global translation, 
rotation, scaling, and sheer factors, and 2) Key-shape deformations, that are de.ned relative to a set 
of key-shapes. 3.1.1 A.ne Deformations An important part of cartoon motion comes from the velocity of 
the entire body, and how it stretches and squashes in different direc­tions. We demonstrate this on the 
bouncing-ball motion in Figure 3. The motion style is determined by how fast the ball travels, the arcing 
(shape of the ball trajectory), how much the ball rotates from frame-to-frame, how much it squashes and 
stretches, and the tim­ing of the squash and stretch. We can approximate the motion style with a sequence 
of af.ne deformations as illustrated with the over­layed grid in Figure 3. The ball shape S is deformed 
to a shape V (t) at time frame t with af.ne parameters . (t)=[a1,a2, a3,a4,dx,dy]: dx V = warp(.,S)= 
. a1 a2 .· S (1) a3 a4 dy a1,a2,a3, and a4 describe rotation, x/y scale, and shear, and dx, dy code the 
x/y translation. S is a 3×N shape matrix [s1,...,sN ] coding N points in homogenous form si =[xi,yi, 
1]T . For instance, S could be a set of points along the contours of the ball. If we replace S with a 
new contour, for example a donut-shape, or a photograph, but apply the same af.ne motions . (1),...,. 
(t), the moving shapes V (1),...,V (t) completely change, but the motion style remains the same (See 
video). 3.1.2 Key-Shape Deformations Consider a more complicated motion such as the frog jumping on 
the left part of Figure 2. With af.ne parameters, we can approx­imate the coarse motion, but we miss 
several important deforma­tions, such as the extension and contraction of the legs. To cover those deformations, 
we use a set of characteristic key-shapes Si (or blend-shapes). These shapes are picked by the user, 
and should include all possible extreme deformations. Figure 2 shows three example key-shapes. The example 
shows how the frog transforms from a stretched-out shape S1 in the air to a squashed shape S2 in landing. 
All in-between shapes can be approximated as multi-way linear interpolations. Our motion model extends 
to: a1 a2 dx V = warp(. ,S1...Sk)= . a3 a4 dy .· (.wk · Sk) (2) k approximation error. If we constrain 
.l al = 1, we can write: Figure 4: Multi-dimensional Warping Space The cartoon shape V is now parameterized 
by 6 + K variables, . =[a1,a2, a3,a4,dx,dy, w1,...,wk], the 6 af.ne parameters (as de­.ned previously), 
and the K key-shape interpolation weights. 3.1.3 Extended linear warping space In some domains linear 
interpolation is a good approximation, but in many domains, it produces in-between shapes with undesirable 
visual artifacts. Restrictions of linear interpolations are demon­strated in Figure 4. (Shape S3 is the 
linear interpolation (average) of shape S1 and S2). It is possible to approximate those in-between shapes 
with ad­ditional linear key-shapes. For example adding shape S4 as a new key-shape allows us to better 
approximate the visually meaning­ful in-betweens of S1 and S2. In practice, a large number of such additional 
key-shapes are needed. We like to avoid this, since it puts additional burden on the user for de.ning 
the large amount of source and retarget key-shapes. Another possibility is to replace the linear warping 
function with a nonlinear function. For instance, [Alexa et al. 2000] proposed a nonlinear warping function, 
that avoids many visual artifacts. Us­ing such a function helps us in keeping the number of key-shapes 
to a compact size, but introduces two new challenges: 1) Alexa s and many other nonlinear functions are 
well de.ned for the inter­polation between two key-shapes, but multi-way warping functions are non-trivial. 
2) These functions are highly nonlinear (especially the multi-way extensions), and their inverse function 
(that is needed for cartoon capture) is numerically challenging due to many local minima and singularities. 
We propose a technique that avoids those challenges: We use the Alexa function as a preprocessing step 
to enlarge the linear key­shape set: 1) We automatically generate M in-between shapes for each pair of 
hand-picked key-shapes Si,Sj. This produces a large set of (K - 1) × (K - 2) × M shape examples, where 
K is the num­ber the original key shapes. It densely covers the entire cartoon shape space. 2) We apply 
Principal Components Analysis (PCA) on this large shape database [Jollife 1986]. PCA will generate a 
mean shape M, and eigen-vectors E1, ..., EL that span principal shape variations in orthogonal directions 
of the shape space. Every original example can be approximated with V = M +.l al ·El . The number of 
eigen-vectors is determined by the maximum allowed LL+1 V = aL+1 · M + .al (M + El )= .al · Sl (3) l=1 
l=1 Therefore the extended shape space is computed in setting Sl := M + El and SL+1:= M. Usually the 
number of automatically de­rived key-shapes is higher than the original hand-picked shapes (to cover 
the nonlinear in-betweens), but sometimes it can be lower, when the user has chosen redundant key-shapes. 
The extended linear warping space gives us the modeling power of more complex nonlinear warping functions, 
but we keep all nu­merical advantages, due to the linear key-shape representation. Fur­thermore, this 
technique allows us to transform a function (like Alexa s) that is only de.ned for shape pairs into a 
multiway warp­ing function of more than 2 key-shapes. This technique will show its full potential especially 
for our video-capture process, as we will describe later. 3.1.4 Sub-Part Decomposition Simple characters 
like the bouncing ball example or the frog defor­mations can be modeled with a global set of af.ne and 
key-shape parameterizations, but more complicated characters should be split into sub-parts. For example, 
in articulated .gures, the leg deforma­tion can be modeled separately from the arm deformations, and 
the head deformations. In the following section we describe three tasks that use the mo­tion model (2): 
1) Contour capture, that assumes a known contour V and solves for ., 2) Video capture, that applies (2) 
directly to the unlabeled video to solve for ., and 3) Cartoon-retargeting, which takes a . and different 
Si to generate a new shape motion V .  3.2 Contour Capture The input is a sequence of cartoon contours: 
V (1),...,V (t) and the hand-labeled key-shapes S1,...,SK . If the animator used a computer tool, those 
V vectors can come directly from the tool, or the vectors will be hand-rotoscoped from stock-footage 
animation. Given V , the equation (2) is used to solve for . . Since our cartoon motion model will not 
exactly match the con­tour input, we estimate a motion vector . that will approximate the contour input. 
This can be done in minimizing the following error term: Err = ||V - warp(. , S1,...,Sk)||2 (4) We minimize 
this term with a 2 step procedure. 1) First we com­pute the af.ne motion parameters, 2) We estimate the 
key-weights on af.ne aligned shapes. A.ne Capture: Estimating af.ne motion of contours can be done with 
a closed-form solution. We measure the af.ne motion in minimizing the following error term: dx = ||V 
- . a1 a2 .· S1||2 (5) Erraf f a3 a4 dy The standard least-squares solution is . a1 a2 dx . := V · ST 
(S · ST )-1 (6) a3 a4 dy  Figure 5: Left side illustrates how the shape vector S is used to index all 
pixel locations of the cartoon. Right side shows four key­shape examples. c .Disney. This equation states 
that if we warp It it should look similar to I0. The error term of (8) is in a sense the previous error 
term (7) mapped through the image function I. A.ne Capture: For af.ne motion only, we can rewrite equation 
(8) to: = erraf f . (It (A · si + D) - I0(si))2 (9) Key-Weight Capture: Now we need to .nd the set 
of key­weights wi in minimizing the full approximation error: Err = ||V - . · wk · Sk||2 (7) a1 a2 dx 
. a3 a4 dy k We experienced improved results if we put additional constraints on the key-weights wk. 
So far we have ignored the fact that the shape space of S1,..., Sk is much larger than just the visually 
correct in-between shapes. We would like to avoid the case that noise and other small (unmodeled) variations 
of V will cause very large posi­tive and negative key-weights wi. This causes severe problems for the 
retargeting task. Many illegal shapes can be generated with large weights. Usually most in-between shapes 
are generated by using only a few key-shapes (only a few wi are non-zero, and they sum to 1). Furthermore, 
the key-shapes work best for interpolation, but only have limited power for extrapolation. We enforce 
this with the following constraints: Constraint 1: Only J interpolation weights are non-zero. This enforces 
that each possible shape lies in a smaller (local­linear) J dimensional subspace. Such shapes are closer 
to the blue curve in Figure 4.  Constraint 2: All key-weights add to 1. If all key-shapes have  the 
same volume, and we also constrain the af.ne matrix A to be a rotation matrix (without scale change), 
this constraint approximates an important animation principle called preser­vation of volume . The volume 
preservation is violated, if the key-shapes are too far from each other (like S1 and S2 in .gure 2). 
Constraint 3: All weights must lie in a margin [T1-T2]. Usu­ally T1 = -0.5 and T2 = 1.5. This enforces 
limited extrapola­tion. Minimizing the quadratic term of (7) due to the linear equality and inequality 
constraints can be done with quadratic programming [Gill et al. 1981]. We use an implementation that 
is available in the optimization toolbox in Matlab. Since the af.ne estimation was done relative to S1 
and not to the weighted combination of all key-shapes, we need to iterate. Given the key-weights, we 
compute the weighted interpolation S. The new af.ne parameters are estimated based on S. We then re-compute 
the key-weights based on the new af.ne adjusted V , and iterate until si.S where A is the rotation, scale 
and shear part of the af.ne parame­ters, and D is the translation. This error function can be minimized 
using the well known af.ne version of the Lucas-Kanade technique [Bergen et al. 1992; Shi and Tomasi 
1994] (a standard least-squares based technique in the computer vision literature). It is beyond the 
scope of this paper to explain this technique in full detail, but we can brie.y summarize the estimation: 
We can not directly apply linear least-squares estimation, since It (A · si + D) is nonlinear. [Lucas 
and Kanade 1981; Horn and Schunk 1981] use following linear approximation of It : It (A · si + D) It 
(si)+[Ix(si),Iy(si)] · (A · si + D) (10) where .I =[Ix,Iy] is the image gradient of It in x and y direction. 
We use a 2D Gaussian convolution .lter as in [Canny 1986] to es­timate the image gradient in a noise-robust 
way. Equation (9) can we rewritten to: convergence. (It (si)+ .I(si) · (A · si + D) - I0(si))2 .  erraf 
f si.S  3.3 Video-Capture (11) (Hi · .af f + zi)2 . = As in contour-capture, the output of video-capture 
is a sequence of motion vectors . that .t the input data. Now our input data is a sequence of images 
I instead of contours. We extend our cartoon motion model such that it directly models image pixel variations. 
With this extension we can incorporate the cartoon motion model into a vision-based region tracking technique 
[Lucas and Kanade 1981; Shi and Tomasi 1994; Bergen et al. 1992]. This allows us to track cartoons without 
contour labels. We use the following notation: S2×N =[s1,..., sN ] contains the x/y coordinates of all 
N pixels of the cartoon image region (Figure si.S =[a1, a2,a3,a4,dx,dy]T .af f Hi =[Ix(i) · xi,Ix(i) 
· yi, Iy(i) · xi,Iy(i) · yi,Ix(i),Iy(i)] zi = It (i) - I0(i) The standard least-squares solution of this 
linearized term is · H)-1 · HT · Z (12) .af f =(HT 5). I(si) is the graylevel value of an image I at 
pixel location si. with I(S) denotes the vector of all pixel graylevels in the cartoon region. .. .... 
.. H1 ... z1 ... And I(warp(S, . )) is the warped image using the warp(S,.) func- H = and Z = (13)tion. 
As in contour-capture, we cannot exactly match the image warp with our motion model (due to noise and 
inaccuracies of our model). We therefore want to minimize the following error func- HN zN Since we use 
the linear approximation (10), the optimal motion tion: parameter . is found in using equation (11) 
iteratively in a Newton­ (8) Raphson style minimization. errimage = ||It (warp(.,S)) - I0(S)||2 A.ne 
and Key-Shape Capture: If our motion model includes the af.ne and key-shape deformation model, we need 
to further extend the estimation framework. We replace the image template I0 with a combination of L 
key-images: .l wlEl : errkeys = . (Hi · .af f + It (si) - .wl · Ek(si))2 (14) si.Sl = . ([Hi,E1(i),..., 
EL(i)] · . + It (i))2 (15) si.S The extended vector . =[a1,a2,a3,a4,dx,dy,w1,...,wL]T can be estimated 
with standard least squares estimation as above. The right side of Figure 5 shows example key-images 
for the Baloo sequence. Those are the original hand picked key-shapes. Since the linear interpolation 
of those hand picked key-images pro­duce illegal shapes (linear interpolating the arm motion merely generates 
a double image of both arm con.gurations), it is essen­tial, that we use an extended key-shape set as 
described in 3.2. Many in-between images are generated from the key-shapes us­ing the non-linear Alexa-warping 
function. Applying PCA to the enlarged dataset, resulted in L eigen-images El . This estimation framework 
is closely related to [Black and Jepson 1996; Lanitis et al. 1995; Blanz and Vetter August 1999]. We 
had good experience with generating 5 inbetween images for each hand-picked key-image. For the tracking 
we used L = 10 eigen-images, and the algorithm converged usually after 40 itera­tions. We did not quantify 
the accuracy of the tracking, but show for all sequences the tracking points in the video. Sub-Part Layers: 
The video-capture process is very sensitive to outliers. Outliers are pixels that are not part of the 
cartoon region. They could be part of the background, or occluding foreground (in­cluding self-occlusion 
from other cartoon parts). We discount those pixels automatically in computing an adaptive alpha matte. 
Usually, the cartoon region has a speci.c color range. We can generate an alpha-matte in using a probabilistic 
color segmentation technique. For instance, the blue hat in Figure 3.3 can be segmented automat­ically. 
The second row in Figure 3.3 and Figure 7 shows some examples. Our video-capture process is then only 
performed on pixels that are included in this matte. This can be done by reducing the summation to only 
those pixels: errkeys = . (It (A · si + D) - .wl · Ek(si))2 (16) si.Layer l  3.4 Retargeting Cartoon 
Motion We experiment with different output media, including 2D cartoon animation, 3D CG models, and photo-realistic 
output. For each do­main, we need to model how a speci.c input key-shape looks in the output domain. 
For simple af.ne deformations, we simply replace the input template with a template in the output domain. 
For key­shape models, we need to design the corresponding key-shape and the interpolation function in 
the output domain. The corresponding output shape can look similar or drastically different from the 
input as long as the key poses are consistent in their meanings. For ex­ample, if key pose 1 in the input 
cartoon is more extreme than key pose 2, then key pose 1 in the output image should also be more extreme, 
in the same way, than key pose 2 in the output image. 3.4.1 Designing the Output Model and Retargeting 
2D Drawing and Photographs: For each key-shape used in the key-shape deformation, a corresponding 2D 
shape is drawn in the output domain. In addition, the corresponding control points Figure 7: Shows the 
color-clustered layer and the key-shape based tracking of Baloo. .Disney cFigure 8: 2D example of key-shapes 
for the input cartoon and cor­responding output key-shapes  (or contours) between the different output 
key-shapes are labeled. Figure 8 shows the key-shapes of a frog and the corresponding key­shapes of a 
bunny. The retargeting process is as follows: First, we extract the af.ne motion of each output key-shapes 
with respect to some reference frame using equation (5). Then, we apply the cho­sen interpolation function 
to the af.ne adjusted key-shapes using the weights obtained from the cartoon capture process. Finally, 
the af.ne motions of the input cartoon are added to the resulting key­shapes to produce the .nal animation. 
3D Models: To retarget the input motion to 3D models, the ani­mator uses a 3D modeling tool to make the 
key-shapes for the out­put animation. To retarget the af.ne motion of the input cartoon to a 3D model, 
we need the equivalent of af.ne motion in 3D. For the in-plane motion (image plane), we map the af.ne 
parameters just as in the 2D case. We do not explicitly recover the out-of-plane motion from the input 
cartoon. This does not however imply that the models are .at. The 3D information is inferred when we 
design the key poses for the output 3D model. For example, as shown in the top row of Figure 9, it is 
dif.cult to measure which direction the character s right foot is pointing from the input cartoon draw­ing. 
The animator, while doing the 3D modeling, interprets the right foot as pointing toward the viewer in 
the .rst key-shape, as shown in the bottom row of .gure 9. Since the solution to the equation is such 
that the weight has to be one for that key-shape and zero for the other keys, the retargeted output character 
naturally points his right foot toward the viewer. Applying the key-shape deformation to 3D models works 
the same way as the 2D examples, except now we might use con­trol vertices for the mesh or the nurb, 
depending on the model­ing choice. Linear interpolation is used for all the 3D examples. However, we 
make a special case for retargeting to 3D articulated .gures. We choose to interpolate in the joint angle 
space since in­terpolating in the joint-angle space is analogous to the non-linear shape interpolation 
function described in section 3.1. The 3D joint angles are .rst taken from the 3D model. The joint angles 
for the in-between frames are then interpolated using the weights from the capture process. Note that 
this is different from interpolating each joint angle independently. All the joint angles are interpolated 
with the same weights from the cartoon capture process and more than 2 key-shapes can be used for each 
output pose. Additional Constraints and Post-processing: In many cases, the translation of the original 
cartoon needs to be modi.ed to sat­isfy certain constraints in the output domain. This is due to the 
fact that we factor out the af.ne parameters of the output key poses and apply the af.ne parameters of 
the input cartoon to the output se­quence. Since the output character can have different proportion Figure 
9: 3D example of key-shapes for the input cartoon and cor­responding output key-shapes Figure 10: The 
hat retargeting sequence. The hat is replaced with a witch hat. c .Disney and dimensionality from the 
input character, using the same af.ne motion results in undesirable effects, such as the output character 
s foot going through the ground. In most cases, we found that sim­ple ad-hoc global translations produced 
a reasonable result. These global translations include constraints that ensure that the foot is at a 
certain position at a speci.c time frame.   4 Examples To demonstrate and test cartoon capture, we 
create several exam­ples. First, we use the video-capture technique described in section 3.3 to follow 
the motion of an animated hat that we then retarget onto a simple two-dimensional drawing of a different 
hat. We re.t the new hat onto the original footage. Figure 10 shows few frames and the video shows the 
entire capture and retargeting experiment. We then capture the dance of Baloo from The Jungle Book and 
retarget the motion to a drawing of a .ower. Again, we use video­capture techniques to extract the motion 
parameters and apply them on the output drawing of the .ower. Figure 11 shows the results. We capture 
the broom motion from the Sorcerer s Apprentice se­quence of Fantasia, and retarget the motion onto a 
digitized photo of a broom. We use contour-capture with constraints and retarget­ing with additional 
constraints as described in section 3.4, since we change the broom dimensions. Then we composite the 
broom se­quence onto a live-action video sequence. Figure 12 shows several retargeted brooms. We also 
experiment with capturing only one contour, the line­of-action, as the source of the motion. The line-of-action 
is a very important principle in tradition animation. The line-of-action is a  the complexity or lowering 
numerical robustness for the video cap-Figure 13: Capturing line-of-action and retargeting of Porky Pig 
ture. Furthermore, the linearized PCA space allows us to perform images from Corny Concerto (1943), onto 
a new 2D character multiway warping with more than two key-shapes, even if the non­linear function we 
approximate, does not offer this feature. The new constraints on the interpolation weights are usually 
not necessary for tracking applications only, but has proven useful for retargetting domains. With our 
strategy of designing input-output key-shape pairs, we circumvented many problems that arise in standard 
skeleton based motion adaption. We believe this work is signi.cant for its technical contribution, but 
more importantly, this project attempts to bridge the gap be­tween techniques that target the traditional 
expressive animation world and the motion capture based animation world. Certain do­mains require increased 
realism, and motion capture data is a more appropriate source, but many other domains call for more expres­sive 
and stylistic motion, and traditional animation data is the ap­propriate source. We have shown, that 
we can treat traditional ani­mation footage in ways similar to what has been done with motion capture 
data. Therefore we hope to create new opportunities for researching and using a wider range of motion 
styles. The results, while compelling for many reasons, have room for improvement. We addressed several 
of the challenges listed in sec­tion 1, but only touched on many other challenges. The 2D to 3D mapping 
is possible, but puts a large burden on the key-shape de­signer. The more complex the motion of a character 
is, the more key-shapes are needed. In some very complex domains, it might even be more labor to create 
the key-shapes, than to animate in the new domain directly by hand. Furthermore, the capture techniques 
can deal with very complex and challenging motion, but also need to be improved in accuracy. We have 
not employed any temporal constraints, like smoothness or speci.c dynamic model constraints. Many of 
our animations contain jitter, although very often the jitter is not that distracting, since the exaggerated 
motion dominates the perception. Some of our future research directions will include how to in­corporate 
further dynamical constraints to increase the robustness. Also, we plan to experiment with alternative 
minimization tech­niques for the capture process. Sampling techniques have been proven to be more robust 
for video motion tracking. Currently, we are investigating methods to allow editing of mo­tion styles. 
This would allow a single source motion to be used in a variety of scenes. We also want to work on .lters 
for motion capture that add animation principles to create more expressiveness in the motion and allow 
a single motion capture sequence to have many motion styles.  Acknowledgements We would like to thank 
Pat Hanrahan, Marc Levoy, James Davis, Henrik Wann Jensen, Maneesh Argrawala, Li-Yi Wei, Charles Ying, 
Kingsley Willis, Rahul Gupta, Erica Robles, Leslie Ikemoto, Alejandra Meza, Alana Chan, Jessica Bernstein-Wax, 
John Gerth, Ada Glucksman, Heather Genter, the Stanford Graphics Lab, and the SIGGRAPH reviewers for 
their signi.cant help, support, and input on this project. We would like to thank Catherine Margerin, 
Greg LaSalle, and Jennifer Balducci from Rearden Steel for their invaluable help with the motion capture 
and retargeting tasks. We would like to thank Craig Slagel, Steve Taylor, and Steve Ander­son from Electronic 
Arts for providing and helping us with the 3D Otter model in Maya. We would like to thank Disney Feature 
An­imation, and especially Lance Williams, Stephanie Carr, and Bob Bacon for giving us the permission 
to use and reprint some of the famous Disney Animation Cells. This research has been funded by the National 
Science Founda­tion, Electronic Arts, Microsoft Corporation, Sony, Intel, and Vul­can Ventures through 
the Stanford IMTV project, and the Stanford OTL of.ce. References ALEXA, M., AND M¨ ULLER, W. 2000. 
Representing animations by principal compo­nents. 411 418. ISSN 1067-7055. ALEXA, M., COHEN-OR, D., AND 
LEVIN, D. 2000. As-rigid-as-possible shape interpolation. In Proceedings of SIGGRAPH 2000, ACM Press 
/ ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, 157 
164. ISBN 1-58113-208-5. BERGEN, J., ANANDAN, P., HANNA, K., AND HINGORANI, R. 1992. Hierarchical model-based 
motion estimation. In ECCV, 237 252. BLACK, M., AND JEPSON, A., 1996. Eigentracking: robust matching 
and tracking of articulated objects using a view-based representation. BLANZ, V., AND VETTER, T. August 
1999. A morphable model for the synthesis of 3d faces. Proceedings of SIGGRAPH 99, 187 194. ISBN 0-20148-560-5. 
Held in Los Angeles, California. BREGLER, C., AND OMOHUNDRO, S. 1994. Surface learning with applications 
to lipreading. In Advances in Neural Information Processing Systems, Morgan Kaufman, San Francisco, A. 
Cowan, Tesauro, Ed. CANNY, J. 1986. A computational approach to edge detection. IEEE Trans. Pattern Anal. 
Mach. Intell. 8, 679 698. GILL, P., MURRAY, W., AND WRIGHT, M. 1981. Practical Optimization. Academic 
Press, London, UK. GLEICHER, M. 1998. Retargeting motion to new characters. In SIGGRAPH 98 Con­ference 
Proceedings, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, 33 42. ISBN 0-89791-999-8. 
HORN, B., AND SCHUNK, G. 1981. Determining optical .ow. Arti.cial Intelligence 17. JOLLIFE, I. T. 1986. 
Principal Components Analysis. New York: Springer. LANITIS, A., TAYLOR, C., COOTES, T., AND AHMED, T. 
1995. Automatic inter­pretation of human faces and hand gestures using .exible models. In International 
Workshop on Automatic Face-and Gesture-Recognition. LUCAS, B., AND KANADE, T. 1981. An iterative image 
registration technique with an application to stereo vision. Proc. 7th Int. Joinnt Conf. on Art. Intell.. 
NGO, T., DANA, J., CUTRELL, D., DONALD, B., LOEB, L., AND ZHU, S. 2000. Accessible animation and customizable 
graphics via simplicial con.guration mod­eling. In SIGGRAPH 2000 Proceedings. PARKE, F. 1972. Computer 
generated animation of faces. In ACM National Confer­ences, 451 457. PIGHIN, F., HECKER, J., LISCHINSKI, 
D., SZELISKI, R., AND SALESIN, D. H. 1998. Synthesizing realistic facial expressions from photographs. 
In SIGGRAPH 98 Conference Proceedings, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM SIGGRAPH, 
75 84. ISBN 0-89791-999-8. POPOVI C´, Z., AND WITKIN, A. 1999. Physically based motion transformation. 
In SIGGRAPH 99 Conference Proceedings, ACM SIGGRAPH. SHI, J., AND TOMASI, C. 1994. Good features to track. 
In CVPR. WHITAKER, H., AND HALAS, J. 1981. Timing for Animation. FOCAL PRESS LTD. YONG NOH, J., AND NEUMANN, 
U. 2001. Expression cloning. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics 
Proceed­ings, Annual Conference Series, 277 288. ISBN 1-58113-292-1.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566596</article_id>
		<sort_key>408</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Synthesis of complex dynamic character motion from simple animations]]></title>
		<page_from>408</page_from>
		<page_to>416</page_to>
		<doi_number>10.1145/566570.566596</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566596</url>
		<abstract>
			<par><![CDATA[In this paper we present a general method for rapid prototyping of realistic character motion. We solve for the natural motion from a simple animation provided by the animator. Our framework can be used to produce relatively complex realistic motion with little user effort.We describe a novel constraint detection method that automatically determines different constraints on the character by analyzing the input motion. We show that realistic motion can be achieved by enforcing a small set of linear and angular momentum constraints. This simplified approach helps us avoid the complexities of computing muscle forces. Simpler dynamic constraints also allow us to generate animations of models with greater complexity, performing more intricate motions. Finally, we show that by learning a small set of key parameters that describe a character pose we can help a non-skilled animator rapidly create realistic character motion.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[animation w/constraints]]></kw>
			<kw><![CDATA[motion transformation]]></kw>
			<kw><![CDATA[physically based animation]]></kw>
			<kw><![CDATA[physically based modeling]]></kw>
			<kw><![CDATA[spacetime constraints]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35030955</person_id>
				<author_profile_id><![CDATA[81452598193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[Karen]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P310471</person_id>
				<author_profile_id><![CDATA[81100620346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALEXANDER, R. M. 1980. Optimum walking techniques for quadrupeds and bipeds. J. Zool., London 192, 97-117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALEXANDER, R. M. 1989. Optimization and gaits in the locomotion of vertebrates. Physiol. Rev. 69, 1199-1227.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ALEXANDER, R. M. 1990. Optimum take-off techniques for high and long jumps. Phil. Trans. R. Soc. Lond. 329, 3-10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ALEXANDER, R. M. 1991. Optimum timing of muscle activation for simple models of throwing. J. Theor. Biol. 150, 349-372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BLICKHAN, R., AND FULL, R. J. 1993. Similarity in multilegged locomotion: bouncing like a monopode. J Comp. Physiol. A. 173, 509-517.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BLICKHAN, A. S. A. F. V. W. R. 1999. Dynamics of the long jump. Jornal of Biomechanics 32, 1259-1267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND CALVERT, T. W. 1989. Goal-directed, dynamic animation of human walking. Computer Graphics 23, 3 (July), 233-242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND WILLIAMS, L. 1995. Motion signal processing. In Computer Graphics (SIGGRAPH 95 Proceedings), 97-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. F. 1992. Interactive spacetime control for animation. In Computer Graphics (SIGGRAPH 92 Proceedings), vol. 26, 293-302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DE LEVA, P. 1996. Adjustments to Zatsiorsky-Seluyanov's segment inertia parameters. J. of Biomechanics 29, 9, 1223-1230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DISCREET. Character studio. http://www.discreet.com/products/cs/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for physics-based character animation. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 251-260. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GILL, P., SAUNDERS, M., AND MURRAY, W. 1996. SNOPT: An SQP algorithm for large-scale constrained optimization. Tech. Rep. NA 96-2, University of California, San Diego.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M., AND LITWINOWICZ, P. 1998. Constraint-based motion adaptation. The Journal of Visualization and Computer Animation 9, 2, 65-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253321</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1997. Motion editing with spacetime constraints. In 1997 Symposium on Interactive 3D Graphics, M. Cohen and D. Zeltzer, Eds., ACM SIGGRAPH, 139-148. ISBN 0-89791-884-3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargeting motion to new characters. In Computer Graphics (SIGGRAPH 98 Proceedings), 33-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364400</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 2001. Motion path editing. In 2001 ACM Symposium on Interactive 3D Graphics, 195-202. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K., AND POLLARD, N. S. August 1997. Adapting simulated behaviors for new characters. Proceedings of SIGGRAPH 97, 153-162. ISBN 0-89791-896-7. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O'BRIEN, J. F. 1995. Animating human athletics. Proceedings of SIGGRAPH 95 (August), 71-78. ISBN 0-201-84776-0. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K. 1998. Animating human motion. Scientific American 278, 3 (Mar.), 64-69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[HULL, M. P. F. C. A. D. G. 1991. A parameter optimization approach for the optimal control of large-scale musculoskeletal systems. Journal of Biomechanical Engineering 114, 450-460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[IGARASHI, T., MATSUOKA, S., AND TANAKA, H. 1999. Teddy: A sketching interface for 3d freeform design. Proceedings of SIGGRAPH 99 (August), 409-416. ISBN 0-20148-560-5. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[KING, D. 1999. Generating vertical velocity and angular momentum during skating jumps. 23rd Annual Meeting of the American Society of Biomechanics (Oct).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344876</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LASZLO, J., VAN DE PANNE, M., AND FIUME, E. L. 2000. Interactive control for physically-based animation. Proceedings of SIGGRAPH 2000 (July), 201-208. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like figures. In Computer Graphics (SIGGRAPH 99 Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[LIU, Z., GORTLER, S. J., AND COHEN, M. F. 1994. Hierarchical spacetime control. In Computer Graphics (SIGGRAPH 94 Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PANDY, M., AND ZAJAC, F. E. 1991. Optimum timing of muscle activation for simple models of throwing. J. Biomechanics 24, 1-10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PANDY, M., ZAJAC, F. E., SIM, E., AND LEVINE, W. S. 1990. An optimal control model of maximum-height human jumping. J. Biomechanics 23, 1185-1198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PANDY, M., ANDERSON, F. C., AND HULL, D. G. 1992. A parameter optimization approach for the optimal control of large-scale musculoskeletal systems. J. of Biomech. Eng. (Nov.), 450-460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[PEARSALL, D., REID, J., AND ROSS, R. 1994. Inertial properties of the human trunk of males determined from magnetic resonance imaging. Annals of Biomed. Eng. 22, 692-706.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[POLLARD, N. S., AND BEHMARAM-MOSAVAT, F. 2000. Force-based motion editing for locomotion tasks. In Proceedings of the IEEE International Conference on Robotics and Automation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[POLLARD, N. S., AND REITSMA, P. S. A. 2001. Animation of humanlike characters: Dynamic motion filtering with a physically plausible contact model. In Yale Workshop on Adaptive and Learning Systems.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[POLLARD, N. S. 1999. Simple machines for scaling human motion. In Computer Animation and Simulation '99, Eurographics, Milano, Italy. ISBN 3-211-83392-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[POPOVI&#262;, Z., AND WITKIN, A. 1999. Physically based motion transformation. In Computer Graphics (SIGRAPH 99 Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344880</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[POPOVI&#262;, J., SEITZ, S. M., ERDMANN, M., POPOVI&#262;, Z., AND WITKIN, A. P. 2000. Interactive manipulation of rigid body simulations. Proceedings of SIGGRAPH 2000 (July), 209-218. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122755</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[RAIBERT, M. H., AND HODGINS, J. K. 1991. Animation of dynamic legged locomotion. In Computer Graphics (SIGGRAPH 91 Proceedings), vol. 25, 349-358.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237229</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., GUENTER, B., BODENHEIMER, B., AND COHEN, M. 1996. Efficient generation of motion transitions using spacetime constraints. In Computer Graphics (SIGGRAPH 96 Proceedings), 147-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., COHEN, M. F., AND BODENHEIMER, B. 1998. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics & Applications 18, 5 (Sept. - Oct.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>502123</ref_obj_id>
				<ref_obj_pid>502122</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[SHIN, H. J., LEE, J., GLEICHER, M., AND SHIN, S. Y. 2001. Computer puppetry: An impotance-based approach. ACM Transactions on Graphics 20, 2 (April), 67-94. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[TAK, S., SONG, O.-Y., AND KO, H.-S. 2000. Motion balance filtering. In Proceedings of the 21th European Conference on Computer Graphics (Eurographics-00), Blackwell Publishers, Cambridge, S. Coquillart and J. Duke, David, Eds., vol. 19, 3 of Computer Graphics Forum, 437-446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[TORKOS, N., AND VAN DE PANNE, M. 1998. Footprint-based quadruped motion synthesis. In Graphics Interface '98, 151-160. ISBN 0-9695338-6-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[VAN DE PANNE, M., AND FIUME, E. 1993. Sensor-actuator networks. In Computer Graphics (SIGGRAPH 93 Proceedings), vol. 27, 335-342.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[VAN DE PANNE, M., AND FIUME, E. 1994. Virtual wind-up toys. In Proceedings of Graphics Interface 94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[VAN DE PANNE, M., KIM, R., AND FIUME, E. 1994. Virtual wind-up toys for animation. Graphics Interface '94 (May), 208-215. Held in Banff, Alberta, Canada.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[VAN DE PANNE, M. 1997. From footprints to animation. Computer Graphics Forum 16, 4, 211-224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. 1988. Spacetime constraints. In Computer Graphics (SIGGRAPH 88 Proceedings), vol. 22, 159-168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND POPOVI&#262;, Z. 1995. Motion warping. In Computer Graphics (SIGGRAPH 95 Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>926426</ref_obj_id>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[WOOTEN, W. L. 1998. Simulation of leaping, tumbling, landing, and balancing humans. PhD thesis, Georgia Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[YEADON, M. R. 1990. The simulation of aerial momement - iii the determination of the angular momentum of the human body. Journal of Biomechanics 23, 75-83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312372</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[ZORDAN, V. B., AND HODGINS, J. K. 1999. Tracking and modifying upper-body human motion data with dynamic simulation. In Computer Animation and Simulation '99, Eurographics, Milano, Italy. ISBN 3-211-83392-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Synthesis of Complex Dynamic Character Motion from Simple Animations C. Karen Liu Zoran Popovic´ University 
of Washington Figure 1 Top: Simple input animation depicting hopscotch (a popular child game consisting 
of hops, broad jumps and a spin jump). Bottom: Synthesized realistic hopscotch animation. Abstract In 
this paper we present a general method for rapid prototyping of realistic character motion. We solve 
for the natural motion from a simple animation provided by the animator. Our framework can be used to 
produce relatively complex realistic motion with little user effort. We describe a novel constraint detection 
method that automatically determines different constraints on the character by analyzing the input motion. 
We show that realistic motion can be achieved by enforcing a small set of linear and angular momentum 
constraints. This simpli.ed approach helps us avoid the complexities of com­puting muscle forces. Simpler 
dynamic constraints also allow us to generate animations of models with greater complexity, performing 
more intricate motions. Finally, we show that by learning a small set of key parameters that describe 
a character pose we can help a non-skilled animator rapidly create realistic character motion. CR Categories: 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; Keywords: Animation, Animation 
w/Constraints, Physically Based Animation, Phys­ically Based Modeling, Motion Transformation, Spacetime 
Constraints 1 Introduction Generating realistic character animation remains one of the great challenges 
in computer graphics. To appear realistic, a character motion needs to satisfy the laws of physics, and 
stay within the email: {karenliu,zoran}@cs.washington.edu Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 space of naturally occurring 
movements. Simulated human mod­els can move their muscles in many different ways to accomplish the same 
task, but only a small subset of these motions appears re­alistic. Creating such natural models of motion 
has proven to be an extremely dif.cult task, especially for characters as complex as humans. Complex 
models of character dynamics are also dif.cult to control. Computing the correct dynamics requires an 
extensive mathemati­cal infrastructure that often hinders artistic expressiveness. On the other hand, 
granting more control to animators provides greater ex­pressive freedom often at the cost of realism 
because the burden of being physically correct falls into the animators hands. An ideal realistic character 
animation system should be able to syn­thesize natural motion of arbitrary characters, and, at the same 
time, provide the expressive power of keyframing. Furthermore, the sys­tem should allow non-skilled users 
to create realistic animations easily with minimal training. This paper strives to make a step in that 
direction. We present a novel approach for rapid prototyping of realistic character motion. We focus 
on the synthesis of highly dynamic movement such as jumping, kicking, running, and gym­nastics. Less 
energetic motions such as walking or reaching are not addressed in this paper. Our system could be used 
by both experts and non-skilled animators alike. The animator .rst creates a rough sketch of the desired 
animation. From this initial simple animation the system infers environmental constraints on the character 
motion (e.g. footsteps). We choose not to use the full character formulation of dynamics. In fact, we 
avoid solving for muscle forces altogether. Instead, we focus on deter­mining more fundamental properties 
of realistic, highly dynamic motion and try to preserve these features throughout the process of animation. 
In particular, our system extracts the general patterns of linear and angular momentum transfer during 
dynamic motion and tries to preserve these patterns during animation. We ask the animator to fully specify 
a small set of speci.c keyframes. Since our target users are animators of all skilled levels, our system 
can also make suggestions for each of those keyframes. Finally, the an­imator can .ne-tune the motion 
by specifying additional keyframes anywhere in the motion sequence. The rest of the paper describes our 
approach in more detail. In Sec­tion 2, we discuss related work. Section 3 gives a short overview of 
our motion synthesis approach. Subsequent sections describe various aspects of our algorithms in more 
detail. In Section 9, we describe a collection of example animations generated by our sys­tem. Section 
10 summarizes our contributions and outlines possible future research directions. 2 Related work Synthesis 
of natural motion has its roots in a variety of research areas ranging from robotics and spacetime optimization 
to biome­chanics and kinesiology. Robot controller simulation has been successfully applied to the do­main 
of realistic computer animation [Raibert and Hodgins 1991; van de Panne et al. 1994; van de Panne and 
Fiume 1993; Hod­gins 1998]. Controllers drive actuator forces based on the current state of the environment. 
Actuator forces, through simulation, pro­duce realistic motion. Once the controllers have been .ne-tuned 
and synchronized to each other, a wide range of realistic anima­tions can be produced, ranging from human 
running, diving [Hod­gins et al. 1995], leaping and vaulting [Wooten 1998], to motion of non-human characters 
[van de Panne et al. 1994; Laszlo et al. 2000; Torkos and van de Panne 1998]. Although there have been 
some recent promising advances towards automatic controller synthesis [Hodgins and Pollard August 1997; 
Faloutsos et al. 2001], creating controllers for a given task remains a dif.cult process. In addition, 
simulated robot controllers do not expose suf.cient control to allow for expressive animations. A number 
of researchers take the approach of modeling physical behavior on simpler machines, instead of full complex 
characters [Torkos and van de Panne 1998; Popovi´ c and Witkin 1999; van de Panne 1997; Pollard 1999; 
Discreet n. d.; Bruderlin and Calvert 1989]. In these methods, the physically modeled motion of simple 
machines is mapped onto the full character. Our simpli.ed physics constraints are in part inspired by 
the idea that simpler physical models can approximate the behavior of more complex models. In contrast 
to the approach described by Popovi´ c [1999], we do not simplify the character, nor do we compute full 
dynamics of the simpli.ed character. Instead, we compute signi.cantly simpler mo­mentum constraints directly 
on the complex character. Simpler and more general dynamics constraints allow us to synthesize realistic 
motion starting from highly unrealistic motion. The spacetime constraints framework, in contrast to simulation, 
casts the motion synthesis into a variational optimization problem [Witkin and Kass 1988; Cohen 1992; 
Liu et al. 1994; Rose et al. 1996]. The animator speci.es an objective function that is a metric of performance 
or style (e.g. total power consumption of all of the character s muscles). The algorithm minimizes the 
objective func­tion while satisfying the pose and Newtonian physics constraints across all animation 
frames. Optimal energy movement and in­tuitive control give this method great appeal. Unfortunately, 
for complex characters the Newtonian physics constraints are highly nonlinear, preventing the spacetime 
optimization from converging to a solution. Spacetime constraints are also highly sensitive to the starting 
position of the optimization if the initial state is far away from the solution, the optimization often 
does not converge. To date, these drawbacks prevent spacetime constraints from being used in generating 
complex character motion. Our framework uses spacetime constraint optimization, but we circumvent its 
drawbacks by choosing a simpler set of dynamics constraints. Realistic motion can also be obtained directly 
from the real world. Recently, a number of methods for editing motion capture data have been proposed 
[Witkin and Popovi´c 1995; Bruderlin and Williams 1995; Gleicher 1998; Gleicher 1997; Gleicher and Litwinowicz 
1998; Rose et al. 1998; Gleicher 2001; Lee and Shin 1999; Shin Figure 2 Algorithm overview. et al. 2001], 
including a few that try to preserve physical proper­ties of the motion [Popovi´c and Witkin 1999; Pollard 
and Reitsma 2001; Pollard and Behmaram-Mosavat 2000; Zordan and Hodgins 1999]. In general, these methods 
produce motion that does not de­viate signi.cantly from the input motion. Motion editing tools rely on 
the existence of captured motion that is similar to what the ani­mator intends to create. Also, it is 
inherently dif.cult to introduce new expressive content into the animations, since most editing tools 
are designed to preserve the original motion features. Research in biomechanics and kinesiology provides 
a great source of information on the kinematic and dynamic behaviors of ani­mals [Blickhan 1999; Hull 
1991; Yeadon 1990; Alexander 1990; Alexander 1989; Pandy et al. 1990]. Their analysis of ground and .ight 
stages helped us in designing realistic motion constraints. Blickhan and Full [1993] demonstrate the 
similarity in the multi­legged locomotion of kinematically different animals. They show striking similarities 
between a human run, a horse run and the monopod bounce (i.e. pogo-stick). This similarity motivates 
our approach of .nding the least common denominator for a varied set of dynamic motions. Our motion sketching 
approach to synthesizing motion was in­spired by the effectiveness of Igarashi s sketching interface 
for 3d free-form design [1999], and the work on sketching realistic rigid­body motion [Popovi´ c et al. 
2000]. 3 Overview Our system transforms simple animations into realistic character motion by applying 
laws of physics and the biomechanics domain knowledge. The input to our system consists of an articulated 
char­acter with its mass distribution, and an arbitrary character animation containing values of joint 
angles on the character at each frame. An­imators are free to provide input animations with an arbitrary 
level of detail. In all of our examples we started with rough low-quality animations. Figure 1 shows 
a synthesized realistic hopscotch mo­tion and its original simple animation. We frame the motion synthesis 
problem as a spacetime optimiza­tion. The unknowns to this problem include the values of joint an­gles 
at each frame, along with parameters that determine the behav­ior of angular and linear momentum. The 
entire synthesis process breaks down to four key stages (see .gure 2): Constraint and stage detection. 
Automatically detect environ­ment constraints that correspond to user-intended motions, and separate 
the original motion sequence into constrained and unconstrained stages. Transition pose generation. Establish 
transition poses between constrained and unconstrained animation stages. Momentum control. Generate physical 
constraints according to the Newtonian laws and the biomechanics knowledge. (a) (b) (c) Figure 3 Positional 
constraint types: (a) A single positional constraint on a toe, (b) a line positional constraint on the 
front of the foot, and (c) a plane positional constraint. The green arrows indicate the free motion range. 
 Figure 4 Left: A .xed-point positional constraint occurs at the intersection of three lines representing 
the solutions for the linear systems of three consec­utive animation frames. Right: A .xed-line positional 
constraint occurs at the intersection of three planes. Objective function generation. Construct the objective 
function that favors motion that is smooth, similar to the input motion, and balanced when stationary. 
Each stage improves speci.c aspects of the input motion sequences by introducing constraints or objective 
function components to the spacetime optimization problem. We then use a sequential quadratic programming 
method to .nd the optimal animation. In the subsequent sections we describe each of these steps in detail. 
4 Constraints and stage detection From the standpoint of a user, each input motion sequence simply comprises 
two parts: the part that needs to be improved and the part that needs to be kept intact. For example, 
a user might wish that the hands of the character stay stationary on a high bar while our sys­tem makes 
the rest of the motion look realistic (.gure 8). Moreover, users usually require a number of environmental 
restrictions on the movement of the character. For example, the feet should always remain above the ground. 
Violating these restrictions modi.es the semantics of the animation that the user conveyed in the input 
an­imation. We represent these environmental restrictions with posi­tional and sliding constraints. Since 
the rough sketch animation does not explicitly contain these requirements from users, our sys­tem automatically 
extracts the constraints from the input motion. In this section, we present an algorithm that automatically 
detects po­sitional and sliding constraints from the original motion sequence. 4.1 Positional constraints 
detection A positional constraint .xes a speci.c point on the character to a stationary location for 
a period of time. For example, when a char­acter s heel touches the ground at landing, a positional constraint 
occurs on the heel across a number of frames. Violating positional constraints frequently causes undesirable 
artifacts such as feet slid­ing or penetrating the ground. To detect positional constraints, we need 
to .nd all points on the body that stay .xed in space for some period of time. We would also like to 
determine if these points are isolated in space or if they lie on a line or a plane in order to detect 
constraints at a .ner gran­ularity (.gure 3). For example, when the character takes off in a jumping 
motion, a plane of positional constraints on the bottom of the foot is detected .rst, followed by a single 
point constraint when the character transfers from standing on the entire foot to being on its toes. 
Because each body part of the character is a rigid body, we reduce the problem of .nding the constrained 
points on the whole character to .nding constrained points on each body part. To illustrate our approach, 
we describe the movement of a body point through time. Let Wi be the matrix that transforms a point in 
a local coordinate frame p to its world position xi at time i, or xi = Wip. (1) At time i + 1, p will 
be transformed to Wi+1W-1Wip. We then i de.ne = Wi+1W-1 , (2) Ti+1 i as the transformation that brings 
xi to xi+1. A positional constraint on p from time 1 to time n implies that T1 through Tn all bring p 
to the same global position or Tixi = xi or (Ti - I)xi = 0, (3) for i = 1... n. The solution for each 
time i is the eigenvector for Ti corresponding to the unit eigenvalue. Because Ti is an af.ne matrix, 
it can be written as . T.i bi . Ti = (4) 01 so that we can reformulate equation (3) as a linear system 
(T.i - I)x i = -bi. (5) The linear system in equation (5) is not always consistent, so we solve it in 
a least-squares sense. Depending on the rank of (T.i - I), the solution for each time i can be represented 
as a point, a line or a plane. If the intersection of the geometries representing x 1 through x n, X, 
exists and falls on the body, then we de.ne a constraint that .xes p to X. In other words, we establish 
position constraints where there exists a collection of points that remains stationary over a time period 
1 ... n (.gure 4). Our algorithm can be .ne-tuned by modifying following parame­ters: Minimal frames 
required (n) The intersection has to exist across at least n frames to be considered a positional constraint. 
Tolerance of intersections (e) Two solutions are considered inter­secting if the distance between them 
is less than e. Constrainable body parts Constraint detection is performed only on a subset of body parts 
where users are interested in .nding positional constraints. This is useful when we want to detect constraints 
only on speci.c body parts in the case when the entire character stands still. 4.2 Sliding constraints 
Sliding constraints are a generalization of positional constraints. In­stead of .xing a point on the 
character to a single world coordinate, a sliding constraint limits the point s motion to a particular 
line or plane in world space. For example, a .gure skater is free to change the location of the foot 
on the ice as long as it slides along the same line. Thus, we need to solve for both the constrained 
body points and the line or plane to which they are constrained. We describe the algorithm for .nding 
a line constraint. The plane constraint is computed analogously. We want to .nd a body point p that is 
constrained to a line l. Instead of establishing a closed-form solution, we construct a least-squares 
problem, where unknowns are the parameters for p and l minp,l .Dist(TiWip, l) (6) i In other words, we 
minimize the sum of distances between the xi and line l at each frame i. Because the de.nition of a plane 
sliding constraint subsumes line sliding constraints, and a line sliding constraint subsumes a point 
constraint, we perform our constraint detection in the order of de­creasing restriction. First we solve 
for position constraints and then line sliding constraints and .nally plane sliding constraints. Although 
we tried to design our constraint detection to be as gen­eral as possible, in practice the rough sketch 
motion is rarely de­tailed enough to be able to .nd the exact location of the constraint. For example, 
the animator often leaves the entire character static during the time when character is on the ground. 
In that case, our constraint detection would .nd constraints on each body part. In those situations, 
we allow the animator to select speci.c body parts that should be tested for constraints. For example, 
we only select the feet of the character to be detected in the hopscotch example. This approach would 
also not fare well on extremely noisy data such as poor-quality motion capture.  4.3 Stage detection 
Given the list of detected constraints, the system separates the orig­inal animation into unconstrained 
(.ight) and constrained (ground) stages. We draw the distinction between unconstrained and con­strained 
stages because the physics and biomechanics rules in the air are different from those on the ground. 
During the unconstrained stage, gravity is the only external force acting upon the character. 5 Transition 
poses A transition pose separates constrained and unconstrained stages. We ask animators to specify these 
speci.c poses because all other non-transition frames are more directly controlled by the realism constraints. 
Transition poses also tend to be interesting from the an­imator s perspective. The following two sections 
describe a learned estimator that suggests a pose at each transition frame. We also de­scribe a set of 
tools that allow users to position the character at the phase transitions. 5.1 Suggesting learned poses 
Our pose estimator predicts poses at transition frames based on the input motion sequence. The estimator 
is a K-nearest neighbor (KNN) algorithm. The training consists of storing a speci.c set of parameters 
about the transition poses for each example motion. The examples can be generated by the animators or 
captured from real world. We also incrementally update the database by inserting mo­tions as they are 
speci.ed by the user during the animation process.  Cu Cl ps Figure 5 Left: The input parameters of 
a training example in the motion database include .ight distance (D), .ight height (H), previous .ight 
distance (D' ), takeoff angle (. ), landing angle (f), spin angle (.), feet speed at takeoff (dr , dl 
) and landing (.r , .l ), and horizontal average speed (v). Right: The KNN algorithm selects the k most 
similar examples based on the input pa­rameters and outputs a simpli.ed representation of a character. 
The simpli.ed representation consists of centers of mass for the lower body (Cl ), upper body (Cu), and 
arms (Ca), all relative to the center of support (ps). The training input parameters include .ight distance, 
.ight height, previous .ight distance, takeoff angle, landing angle, spin angle, foot speed at takeoff 
and landing, and average horizontal speed (.gure 5). To compute an appropriate distance between training 
examples, we scale each input parameter by its natural bounds. The estimator predicts a simpli.ed representation 
of the transition poses at the constraint release (takeoff) and constraint creation (landing). For each 
pose, the characteristics of the target motion must match the prediction. The output representation consists 
of three mass points: center of mass (COM) of the lower body, COM of the upper body, and COM of two arms 
(.gure 5). The locations of mass points are stored relative to character s center of support. This makes 
our parameterization invariant of the global transformation. To predict a candidate pose from the input, 
the KNN algorithm se­lects the k most similar examples from the motion database (k = 3, in our implementation). 
The estimator computes the candidate pose, which consists of the positions of the three mass points, 
by interpolating the poses of the selected neighbors, weighted by their similarities to the input. We 
reconstruct a full character pose from three mass points by solv­ing an inverse kinematics (IK) problem, 
constraining the three mass points to values returned by KNN, while minimizing the deviation between 
the suggested and original poses. We set the initial states of the unknown DOFs equivalent to the DOFs 
of the poses from the nearest training-set example, so that the solution pose keeps some plausible details 
from the database examples. There are a number of advantages to estimating a small set of pose parameters. 
Joint angles themselves are poor estimators of the pose since they are not uniformly scaled. Furthermore, 
with our repre­sentation we can use the same motion database to learn the poses of characters with drastically 
different skeletal structures. We use a simple formula to scale the estimated relative mass points positions 
to accommodate a different skeleton. Let CA be one of character A s COM output parameters from the learning 
algorithm and CA be the corresponding COM for the default pose. Suppose we know CB, the corresponding 
default pose of another skeletal structure, we can compute CB as follows: CB 2 CB = CB +(CA - CA)(7) 
CA 2 Intuitively, we displace the center of mass parameter by the rescaled difference between the default 
and suggested pose of the charac­ d d Figure 6 The general angular momentum pattern modeled after biomechanics 
data. During the unconstrained stage (left of p1 and right of p4), the angular momentum is constant. 
During the constrained stage (between p1 and p4), the curve is smooth, p2 is lower than p1, d2 is less 
than d1, and p2 and p3 are on opposite sides of p4. ter in the database. We also found that this parameterization 
al­lows us to learn poses from a relatively small set of examples. We used a database of about 50 motion 
captured constrained and un­constrained segments to synthesize all of animations described in Section 
9.  5.2 User pose adjustment Because the estimated poses do not always meet the needs of the animation, 
we allow animators to directly modify transition poses. Our posing tool gives the user both .ne-grain 
and high-level control of the pose edit process. Users can directly modify the learned mass points by 
dragging them to a new location. The IK solver adjusts the joint DOFs accordingly. For greater control, 
the animator can modify the transition pose directly. The user can also customize the importance of the 
similarity between the current pose and its corresponding pose in the original sketch. By .ne-tuning 
transi­tion poses animators can impart expression and style to the overall motion. Animators are also 
free to introduce new keyframes anywhere in the animation to re.ne more detailed aspects of motion. How­ever, 
when an animator creates too many of these constraints, the spacetime optimization problem becomes over-constrained. 
At that point, the animator has the option of turning momentum constraints into soft constraints. The 
optimization honors all of the animator s constraints while trying to satisfy soft realism constraints 
as much as possible. This approach provides graceful degradation of realism in the event that the animator 
s keyframe poses force the character into unrealistic movement. 6 Controlling the character momentum 
The transition poses constrain the motion at a few key points of the animation. In a sense, they provide 
scaffolding for the motion, whereas dynamic constraints ensure realistic motion during each animation 
segment. We achieve this realism by formulating con­straints on the behavior of the character s linear 
and angular mo­mentum. We derive these constraints from the laws of physics and biomechanics domain knowledge. 
Linear momentum determines the location of COM at each frame. The computation of angular momentum involves 
different body parts and their moments of inertia relative to the center of mass (for computation of 
angular momentum on an articulated character see Appendix A). The constraints on linear and angular momentum 
are different for unconstrained and constrained stages, and we discuss them separately. 6.1 Momentum 
during unconstrained stages Since gravity is the only external force acting on the unconstrained character, 
the following equation holds dP(q)/dt = mC¨(q)= mg, (8) where C is character s center of mass, and q 
are character s degrees of freedom. During .ight there are no external torques acting on the character, 
so the angular momentum is constant dL(q)/dt = 0. (9) The spacetime optimization enforces these two constraints 
during an unconstrained stage. Effectively, these constraints ensure that the center of mass falls into 
a parabolic trajectory and the joints move in such way that the angular momentum of the whole body remains 
constant (.gure 6). 6.2 Momentum during constrained stages Unlike the unconstrained stage where gravity 
is the only external force acting on the character, the momentum at a constrained stage results from 
a complex exchange of energy between the character and the environment constraints. We would like to 
avoid computing linear and angular momentum by complex physical simulation. In­stead, we build an empirical 
model for the behavior of momentum based on biomechanics studies [Pandy et al. 1992; King 1999] and the 
analysis of motion capture data. We observe that the momen­tum during a constrained stage has a characteristic 
shape shown in .gure 6. The .gure shows a graph of an angular momentum component during a constrained 
stage between two unconstrained stages. Note that the angular momentum is constant during the two unconstrained 
stages. During the constrained stage, the momentum transfers from one constant value to another. Natural 
dynamic systems achieve this transfer by .rst storing energy (momentum decreases), and then releasing 
it in a burst which causes a small overshoot at p3. The characteristic pattern of the linear momentum 
is the same with the exception that the linear momentum in the neighboring uncon­strained stages has 
a slope mg instead of 0. We try to capture all aspects of this curve by enforcing the following invariants: 
 the curve is C1 continuous at transition points p1 and p4  p2 is less than p1  d1 is larger than d2 
 p2 and p3 are on the opposite sides of p4  Since the momentum pattern during the constrained stage 
is fully determined by the control point vector qm =[p1, p2, p3, p4], we for­mulate the linear and angular 
momentum constraints as P(q)= Sl (qm) (10) L(q)= Sa(qm) (11) j During optimization we solve for both 
q vectors for each con­ m strained stage j and qi for each time frame i enforcing the con­straints in 
equations 8,9,10,11, as well as the inequality constraints j on q governing the shape of the momentum 
curves. m 7 Objective function The momentum constraints enforce realism of the motion while de­tected 
constraints take into account the user intent and environmen­tal restrictions. However, natural looking 
motion also requires nat­ural joint movements, smoothness across frames, and static balance during stationary 
points of the animation. We formulate each as an objective function component. Minimum mass displacement. 
To achieve natural joint move­ment, we use the minimum mass displacement metric [Popovi´c and Witkin 
1999]. Instead of comparing DOFs directly, this met­ric computes the integral of mass displacement over 
the character s body. This metric is loosely analogous to the measurement of power consumption. Our results 
show that minimum mass displacement presents its own merits in producing natural looking animations. 
For example, the compression on the body of the character before the unconstrained stages would not affect 
the lower body (knees, especially) without the minimum mass displacement as an objec­tive function. Without 
it, the character tends to bend at the waist in order to lower the COM. Minimal velocity of DOFs. Time 
coherence plays a major role in creating visually plausible animations. To account for the smoothness 
across frames, we de.ne an objective function that min­imizes the deviation of each DOF between two consecutive 
frames, effectively minimizing the velocity of each joint angle over the en­tire animation. Static balance. 
The static balance is important during con­strained stages when the character is standing still [Tak 
et al. 2000]. We measure balance by the distance between the COM and con­straints when projected onto 
the plane normal to gravity. The spacetime objective function is a weighted sum of the three objective 
components. 8 Putting it all together All constraints and the objective function .t naturally within 
the spacetime framework. The unknowns of our system Q are the char­acter DOFs qi for each time i and 
the control point vectors for all j constrained-stage momentum curves q . The optimization needs m to 
enforce three types of constraints: Environment constraints(Ce). Constraint detection produces a collection 
of user-intended constraints that partition the mo­ tion into constrained and unconstrained stages. 
 Transition pose constraints(Cp). These constraints were de.ned between each motion phase either by 
our pose estimation method, or explicitly by the user. Momentum constraints(Cm). During both unconstrained 
and constrained phases we dictate the behavior of the linear and angular momentum through constraints 
de.ned in equa­tions 8,9,10,11. The spacetime constraints formulation .nds the unknowns Q that minimize 
the objective function while satisfying all the constraints: Ce(Q)= 0 min .Ei(qi) subject to Cp(Q)= 0 
(12) Q Cm(Q)= 0 .. . 9 Results We used our framework to generate a wide range of animations on a male, 
female and child .gure, all of which comprise 51 DOFs, including 16 Euler rotations, 3 translations, 
and 32 quaternion ro­tations. We also created a three-legged creature with 58 DOFs. In­equality constraints 
enforce bounds on each DOF. We obtained the body dimensions and mass distributions from the biomechanics 
lit­erature [de Leva 1996; Pearsall et al. 1994]. To start the animation process, the animator creates 
a simple animation as an input to the synthesis process. In some cases the animator also selects the 
parts of the body to be used for constraint detection (e.g. feet and hands). Once motion phases have 
been determined, the animator can also change the relative timing between each phase. In some cases, 
the animator adjusted the learned transition poses to achieve a desired effect. For the karate-kick animation, 
the animator also created an additional pose. We solve our optimizations using SNOPT [Gill et al. 1996], 
a gen­eral nonlinearly-constrained optimization package. The optimiza­tion times depend on the duration 
of the motion sequence. All of the simple animations took only a few minutes to sketch. For all examples, 
the synthesis process took less than .ve minutes. Broad jump. We synthesized a broad jump motion that 
clearly improves a crude input animation where only global translations of the character (3 out of 51 
DOFs) are keyframed. The original animation is created by interpolating only 3 keyframes at takeoff, 
peak, and landing. The appropriate movement on the arms and legs results from enforcing the momentum 
constraints and learning the realistic transition poses. Twist jumps. The input motion for this sequence 
consists of two jumps, each with a 90. turn. The output animation shows the prepa­ration before each 
take-off. The character twists away from the turn to increase the potential energy so that it can generate 
enough an­gular momentum at take-off to accomplish the 90. turn. Hopscotch. Much like for the broad jump 
example, the animator created an animation of a popular child game, consisting of hops, broad jumps and 
a spin jump. Each hop only requires 3 keyframes, each of which has fewer than 7 DOFs speci.ed. This example 
shows that our system can deal with asymmetric motions by coordi­nating different parts of body to accommodate 
the momentum con­straints. This example also demonstrates smooth transitions among different types of 
jump styles (.gure 1). Running. The input for the running motion sequence required keyframing of 7 DOFs. 
Originally, the upper body is completely stiff since no upper body DOFs were keyframed. The angular mo­mentum 
constraint creates a counter-body movement by the shoul­ders and arms to counteract the angular momentum 
generated by the legs. In the synthesized animation the arms clearly twist to counter the leg movement. 
Handspring. We generated a rough sketch of an advanced hand­spring motion on an uneven terrain (.gure 
7). This hazardous movement of landing and jumping with arms would be dif.cult to capture in the real 
world. The constraint detector successfully .nds constraints on both the feet and hands. Since there 
were no handstands within the learning example database, the animator had to substantially modify the 
suggested handstand transitions poses. This example demonstrates that momentum constraints are general 
enough to capture the dynamics of movement regardless the orien­tation of the model.  Figure 7 A handspring 
motion: simple and synthesized animation. Figure 8 A high-bar gymnastic exercise: simple and synthesized 
animation. Figure 9 Jumping on ice-skates: simple and synthesized animation. High-bar. Figure 8 shows 
a character performing a high-bar gymnastic exercise. The two positional constraints on the hands during 
bar-contact time create a hanging constrained stage. The constraints of linear momentum and angular momentum 
apply equally to the bar-contact and the more common ground con­strained stage. Skating. This example 
demonstrates sliding constraint detection. The motion is similar to the 180. spin jump, except that the 
charac­ter slides a single leg along a straight line through the take-off phase and landing phase. The 
resulting motion resembles an ice-skating .gure (.gure 9). Karate kick. The karate kick animation was 
created by an in­cremental synthesis process. First the animator created a simple side-jump animation. 
The .rst synthesis pass created a realistic side jump. The animator then introduced an additional keyframe 
at the peak of the jump indicating a leg kick. The second synthesis pass created the .nal karate-kick 
animation by enforcing the origi­nal constraints augmented by the additional mid-.ight constraint. Other 
humanoid characters. Our framework can also animate characters with different skeletal structures and 
mass distributions. We created a child character whose limbs are shorter and whose torso is relatively 
larger. Our algorithm successfully scaled down the poses learned from motion database to accommodate 
a differ­ent skeletal structure. In another example, we removed the left knee DOF of the character and 
generated a motion sequence of the twist jumps. The character had to drastically twist the pelvis while 
land­ing to place the stiff leg on the ground properly. Non-humanoid characters. Our method also generalizes 
to non-humanoid characters. We created a strange three-legged crea­ture and synthesized a number of animations 
experimenting with various locomotion gaits. In .gure 10 we show the creature jump­ing to the side and 
doing a 180. turn jump. 10 Conclusion and future work In this paper we present a general method for rapid 
prototyping of dynamic character motion that could be used by both experts and non-skilled animators. 
Animators can use our system to produce relatively complex realistic motion with little effort on behalf 
of the animator. Creating the initial simple animation often takes less than two minutes, while modi.cation 
of the transition poses can be avoided completely by accepting suggestions presented by the learned-pose 
estimator. In addition, a skilled animator can .ne-tune transition poses as well as add any number of 
additional keyframes to achieve desired details. (e.g. motion capture data), or by storing each of the 
previously cre­ated poses. In both cases, a very small data set creates useful pose suggestions. Our 
methods are best suited for synthesis of highly dynamic motion, since such motions are mainly governed 
by Newtonian physics. One clear future research direction would be to extend these methods so that they 
apply to low-energy character motion such as walking, reaching, or picking up an object. Many aspects 
of our approach could be potentially applied to other animation problems, most notably realistic editing 
of motion cap­ture data. The animator could start from a motion capture sequence instead of a simple 
animation. The automatic constraint detection methods could be useful in processing motion capture data, 
since they can accurately .nd the foot-ground contact points. Trans­forming a motion capture sequence 
by adding additional keyframes would create a new realistic animation keeping as much of the cap­tured 
detail as possible. It is worth noting that our synthesis approach does not funda­mentally need to start 
with an input animation. A skilled anima­tor could, alternatively, specify the environment constraints 
and keyframes explicitly. This is probably the most likely approach for using our algorithms in the production 
environment. To make our methods more accessible to a wider audience, we need to develop a more effective 
user interface. In the future, we hope to make our tools accessible to animators of varying skill levels. 
11 Acknowledgments We thank Jiwon Kim and Jia-chi Wu for their assistance with cre­ating .gures and videos 
for this paper. Special thanks goes to Gary Yngve and David Hsu for their invaluable comments on various 
versions of this document. We also thank Charles Gordon, Eugene Hsu, Adam Kirk, and Keith Grochow for 
their help with processing motion capture data. Funding and research facilities were provided by the 
University of Washington Animation Research Labs, Mi­crosoft Research, Intel, and NSF grant CCR-0092970. 
Appendix A We compute the angular momentum of a body point x as L = µr × x., (13) where r is the vector 
between x and COM, x.is the velocity of x and µ is the mass of x. To compute the net angular momentum 
of the whole body, we sum the angular momentum contributions for each body part (node) i, computed by 
integrating each body point x j: ... µj(x j - C) × (x.j - C.)dxdydz = . We show that realism can be 
approximated by a small number of L constraints on the behavior of linear and angular momentum. This 
ij approach helps us avoid the complexities of Newtonian constraints. . cr(WiMiW.iT )+ . miC × WiCi 
= Since we don t solve for muscle forces, we also avoid computing ii the right distribution of muscle 
usage needed to produce natural­ + . miWiCi × C.+ . miC × C.(14)looking motion. Simpler dynamic constraints 
allow us to generate more complex animations in terms of the character model and mo­tion description 
complexity. Simpler constraints also allow greater variability of the resulting motion. This feature 
enables the anima­ ii where Ci is the COM of the node i in its local coordinate frame. tor to add more 
expressive detail to the motion by providing addi-We de.ne operator cr() that transforms a 3 × 3 matrix 
A to a 3 × 1 tional keyframes. vector as follows: We also show that by learning a small set of key parameters 
that .. a23 - a32 describe a pose, we can create realistic transition poses. We show cr(A)= . a31 - 
a13 . that the sample space can be populated either by realistic motion a12 - a21 We compute the mass 
matrix tensor Mi of the node i as an integral of outer products over all body points xj, scaled by the 
node mass mi. ... T Mi = mi xjxj dxdydz j References ALEXANDER, R. M. 1980. Optimum walking techniques 
for quadrupeds and bipeds. J. Zool., London 192, 97 117. ALEXANDER, R. M. 1989. Optimization and gaits 
in the locomotion of vertebrates. Physiol. Rev. 69, 1199 1227. ALEXANDER, R. M. 1990. Optimum take-off 
techniques for high and long jumps. Phil. Trans. R. Soc. Lond. 329, 3 10. ALEXANDER, R. M. 1991. Optimum 
timing of muscle activation for simple models of throwing. J. Theor. Biol. 150, 349 372. BLICKHAN, R., 
AND FULL, R. J. 1993. Similarity in multilegged locomotion: bounc­ing like a monopode. J Comp. Physiol. 
A. 173, 509 517. BLICKHAN, A. S. A. F. V. W. R. 1999. Dynamics of the long jump. Jornal of Biomechanics 
32, 1259 1267. BRUDERLIN, A., AND CALVERT, T. W. 1989. Goal-directed, dynamic animation of human walking. 
Computer Graphics 23, 3 (July), 233 242. BRUDERLIN, A., AND WILLIAMS, L. 1995. Motion signal processing. 
In Computer Graphics (SIGGRAPH 95 Proceedings), 97 104. COHEN, M. F. 1992. Interactive spacetime control 
for animation. In Computer Graphics (SIGGRAPH 92 Proceedings), vol. 26, 293 302. DE LEVA, P. 1996.AdjustmentstoZatsiorsky-Seluyanov 
ssegmentinertiaparameters. J. of Biomechanics 29, 9, 1223 1230. DISCREET. Character studio. http://www.discreet.com/products/cs/. 
FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for physics-based character 
animation. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, 
Annual Conference Series, 251 260. ISBN 1-58113-292-1. GILL, P., SAUNDERS, M., AND MURRAY, W. 1996. SNOPT: 
An SQP algorithm for large-scale constrained optimization. Tech. Rep. NA 96-2, University of California, 
San Diego. GLEICHER, M., AND LITWINOWICZ, P. 1998. Constraint-based motion adaptation. The Journal of 
Visualization and Computer Animation 9, 2, 65 94. GLEICHER, M. 1997. Motion editing with spacetime constraints. 
In 1997 Symposium on Interactive 3D Graphics, M. Cohen and D. Zeltzer, Eds., ACM SIGGRAPH, 139 148. ISBN 
0-89791-884-3. GLEICHER, M. 1998. Retargeting motion to new characters. In Computer Graphics (SIGGRAPH 
98 Proceedings), 33 42. GLEICHER, M. 2001. Motion path editing. In 2001 ACM Symposium on Interactive 
3D Graphics, 195 202. ISBN 1-58113-292-1. HODGINS, J. K., AND POLLARD, N. S. August 1997. Adapting simulated 
behaviors for new characters. Proceedings of SIGGRAPH 97, 153 162. ISBN 0-89791-896­ 7. Held in Los Angeles, 
California. HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O BRIEN, J. F. 1995. Animating human athletics. 
Proceedings of SIGGRAPH 95 (August), 71 78. ISBN 0-201-84776-0. Held in Los Angeles, California. HODGINS, 
J. K. 1998. Animating human motion. Scienti.c American 278, 3 (Mar.), 64 69. HULL, M. P. F. C. A. D. 
G. 1991. A parameter optimization approach for the optimal control of large-scale musculoskeletal systems. 
Journal of Biomechanical Engineering 114, 450 460. IGARASHI, T., MATSUOKA, S., AND TANAKA, H. 1999. Teddy: 
A sketching in­terface for 3d freeform design. Proceedings of SIGGRAPH 99 (August), 409 416. ISBN 0-20148-560-5. 
Held in Los Angeles, California. KING, D. 1999. Generating vertical velocity and angular momentum during 
skating jumps. 23rd Annual Meeting of the American Society of Biomechanics (Oct). LASZLO, J., VAN DE 
PANNE, M., AND FIUME, E. L. 2000. Interactive control for physically-based animation. Proceedings of 
SIGGRAPH 2000 (July), 201 208. ISBN 1-58113-208-5. LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach 
to interactive motion editing for human-like .gures. In Computer Graphics (SIGGRAPH 99 Proceedings). 
LIU, Z., GORTLER, S. J., AND COHEN, M. F. 1994. Hierarchical spacetime control. In Computer Graphics 
(SIGGRAPH 94 Proceedings). PANDY, M., AND ZAJAC, F. E. 1991. Optimum timing of muscle activation for 
simple models of throwing. J. Biomechanics 24, 1 10. PANDY, M., ZAJAC, F. E., SIM, E., AND LEVINE, W. 
S. 1990. An optimal control model of maximum-height human jumping. J. Biomechanics 23, 1185 1198. PANDY, 
M., ANDERSON, F. C., AND HULL, D. G. 1992. A parameter optimiza­tion approach for the optimal control 
of large-scale musculoskeletal systems. J. of Biomech. Eng. (Nov.), 450 460. PEARSALL, D., REID, J., 
AND ROSS, R. 1994. Inertial properties of the human trunk of males determined from magnetic resonance 
imaging. Annals of Biomed. Eng. 22, 692 706. POLLARD, N. S., AND BEHMARAM-MOSAVAT, F. 2000. Force-based 
motion editing for locomotion tasks. In Proceedings of the IEEE International Conference on Robotics 
and Automation. POLLARD, N. S., AND REITSMA, P. S. A. 2001. Animation of humanlike charac­ters: Dynamic 
motion .ltering with a physically plausible contact model. In Yale Workshop on Adaptive and Learning 
Systems. POLLARD, N. S. 1999. Simple machines for scaling human motion. In Computer Animation and Simulation 
99, Eurographics, Milano, Italy. ISBN 3-211-83392-7. POPOVI C´ , Z., AND WITKIN, A. 1999. Physically 
based motion transformation. In Computer Graphics (SIGGRAPH 99 Proceedings). POPOVI C´, J., SEITZ, S. 
M., ERDMANN, M., POPOVI C´, Z., AND WITKIN, A. P. 2000. Interactive manipulation of rigid body simulations. 
Proceedings of SIG-GRAPH 2000 (July), 209 218. ISBN 1-58113-208-5. RAIBERT, M. H., AND HODGINS, J. K. 
1991. Animation of dynamic legged loco­motion. In Computer Graphics (SIGGRAPH 91 Proceedings), vol. 25, 
349 358. ROSE, C., GUENTER, B., BODENHEIMER, B., AND COHEN, M. 1996. Ef.cient gen­eration of motion transitions 
using spacetime constraints. In Computer Graphics (SIGGRAPH 96 Proceedings), 147 154. ROSE, C., COHEN, 
M. F., AND BODENHEIMER, B. 1998. Verbs and adverbs: Mul­tidimensional motion interpolation. IEEE Computer 
Graphics &#38; Applications 18, 5 (Sept. Oct.). SHIN, H. J., LEE, J., GLEICHER, M., AND SHIN, S. Y. 
2001. Computer puppetry: An importance-based approach. ACM Transactions on Graphics 20, 2 (April), 67 
94. ISSN 0730-0301. TAK, S., SONG, O.-Y., AND KO, H.-S. 2000. Motion balance .ltering. In Proceed­ings 
of the 21th European Conference on Computer Graphics (Eurographics-00), Blackwell Publishers, Cambridge, 
S. Coquillart and J. Duke, David, Eds., vol. 19, 3 of Computer Graphics Forum, 437 446. TORKOS, N., AND 
VAN DE PANNE, M. 1998. Footprint-based quadruped motion synthesis. In Graphics Interface 98, 151 160. 
ISBN 0-9695338-6-1. VAN DE PANNE, M., AND FIUME, E. 1993. Sensor-actuator networks. In Computer Graphics 
(SIGGRAPH 93 Proceedings), vol. 27, 335 342. VAN DE PANNE, M., AND FIUME, E. 1994. Virtual wind-up toys. 
In Proceedings of Graphics Interface 94. VAN DE PANNE, M., KIM, R., AND FIUME, E. 1994. Virtual wind-up 
toys for ani­mation. Graphics Interface 94 (May), 208 215. Held in Banff, Alberta, Canada. VAN DE PANNE, 
M. 1997. From footprints to animation. Computer Graphics Forum 16, 4, 211 224. WITKIN, A., AND KASS, 
M. 1988. Spacetime constraints. In Computer Graphics (SIGGRAPH 88 Proceedings), vol. 22, 159 168. WITKIN, 
A., AND POPOVI C´, Z. 1995. Motion warping. In Computer Graphics (SIGGRAPH 95 Proceedings). WOOTEN, W. 
L. 1998. Simulation of leaping, tumbling, landing, and balancing humans. PhD thesis, Georgia Institute 
of Technology. YEADON, M. R. 1990. The simulation of aerial momement -iii the determination of the angular 
momentum of the human body. Journal of Biomechanics 23, 75 83. ZORDAN, V. B., AND HODGINS, J. K. 1999. 
Tracking and modifying upper-body hu­man motion data with dynamic simulation. In Computer Animation and 
Simulation 99, Eurographics, Milano, Italy. ISBN 3-211-83392-7.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566597</article_id>
		<sort_key>417</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Integrated learning for interactive synthetic characters]]></title>
		<page_from>417</page_from>
		<page_to>426</page_to>
		<doi_number>10.1145/566570.566597</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566597</url>
		<abstract>
			<par><![CDATA[The ability to learn is a potentially compelling and important quality for interactive synthetic characters. To that end, we describe a practical approach to real-time learning for synthetic characters. Our implementation is grounded in the techniques of reinforcement learning and informed by insights from animal training. It simplifies the learning task for characters by (a) enabling them to take advantage of predictable regularities in their world, (b) allowing them to make maximal use of any supervisory signals, and (c) making them easy to train by humans.We built an autonomous animated dog that can be trained with a technique used to train real dogs called "clicker training". Capabilities demonstrated include being trained to recognize and use acoustic patterns as cues for actions, as well as to synthesize new actions from novel paths through its motion space.A key contribution of this paper is to demonstrate that by addressing the three problems of state, action, and state-action space discovery at the same time, the solution for each becomes easier. Finally, we articulate heuristics and design principles that make learning practical for synthetic characters.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[behavioral animation]]></kw>
			<kw><![CDATA[computer games]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.6</cat_node>
				<descriptor>Concept learning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Heuristic methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205.10010206</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies->Heuristic function construction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39024760</person_id>
				<author_profile_id><![CDATA[81100047544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blumberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Media Lab, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30028746</person_id>
				<author_profile_id><![CDATA[81100369938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Downie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Media Lab, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P308289</person_id>
				<author_profile_id><![CDATA[81100375009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ivanov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Media Lab, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30028895</person_id>
				<author_profile_id><![CDATA[81100380885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Media Lab, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31042882</person_id>
				<author_profile_id><![CDATA[81332507042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[Patrick]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Media Lab, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032752</person_id>
				<author_profile_id><![CDATA[81100457381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomlinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Media Lab, MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>272363</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BALLARD, D. 1997. An Introduction to Natural Computation. MIT Press, Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218405</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLUMBERG, B., AND GAYLEAN, T. 1995. Multi-level direction of autonomous creatures for real-time virtual environments. In Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BURKE, R., ISLA, D., DOWNIE, M., IVANOV, Y., AND BLUMBERG, B. 2001. Creature smarts: The art and architecture of a virtual brain. In Proceedings of the Computer Game Developers Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BURKE, R. 2001. Its about Time:Temporal Representation for Synthetic Characters. Master's thesis, The Media Lab, MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[COPPINGER, R., AND COPPINGER, L. 2001. Dogs: A Startling New Understanding of Canine Origin, Behavior, and Evolution. Scribner, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DOWNIE, M. 2000. behavior, animation, music: the music and movement of synthetic characters. Master's thesis, The Media Lab, MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>113149</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DRESCHER, G. 1991. Made-Up Minds:A Constructivist Approach to Artificial Intelligence. MIT Press, Cambridge MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>515547</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[EVANS, R. 2002. Varieties of learning. In AI Game Programming Wisdom, E. Rabin, Ed. Charles River Media, Hingham MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOLOUS, D. 2001. Composible controllers for physics-based character animation. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311538</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FUNGE, J., TU, X., AND TERZOPOLOUS, D. 1999. Cognitive modeling: Knowledge, reasoning and planning for intelligent characters. In Proceedings of SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GALLISTEL, C. R., AND GIBBON, J. 2000. Time, rate and conditioning. Psychological Review 107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargetting motion to new characters. In Proceedings of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GOULD, J., AND GOULD, C. 1999. The Animal Mind. W.H. Freeman, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267663</ref_obj_id>
				<ref_obj_pid>267658</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GRAND, S., CLIFF, D., AND MALHOTRA, A. 1996. Creatures: Artificial life autonomous agents for home entertainment. In Proceedings of the Autonomous Agents '97 Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218411</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GRZESZCZUK, R., AND TERZOPOULOS, D. 1995. Automated learning of muscle-actuated locomotion through control abstraction. In Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280816</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[GRZESZCZUK, R., TERZOPOULOS, D., AND HINTON, G. 1998. Neuroanimator: Fast neural network emulation and control of physics-based models. In Proceedings of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J., AND POLLARD, N. 1997. Adapting simulated behaviors for new characters. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1642235</ref_obj_id>
				<ref_obj_pid>1642194</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[ISLA, D., BURKE, R., DOWNIE, M., AND BLUMBERG, B. 2001. A layered brain architecture for synthetic creatures. In Proceedings of The International Joint Conference on Artificial Intelligence.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[ISLA, D. 2001. The Virtual Hippocampus: Spatial Common Sense for Synthetic Creatures. Master's thesis, The Media Lab, MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>655656</ref_obj_id>
				<ref_obj_pid>645530</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[IVANOV, Y., BLUMBERG, B., AND PENTLAND, A. 2001. Expectation maximization for weakly labeled data. In Proceedings of the 18th International Conference on Machine Learning.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[IVANOV, Y. 2001. State Discovery for Autonomous Creatures. PhD thesis, The Media Lab, MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>917277</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[KAELBLING, L. 1990. Learning in embedded systems. PhD thesis, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[KAPLAN, F., OUDEYER, P.-Y., KUBINYI, E., AND MIKLOSI, A. 2001. Taming robots with clicker training : a solution for teaching complex behaviors. In Proceedings of the 9th European workshop on learning robots, LNAI, Springer, M. Quoy, P. Gaussier, and J. L. Wyatt, Eds.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LINDSAY, S. 2000. Applied Dog Behavior and Training. Iowa State University Press, Ames, IA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[LORENZ, K., AND LEYAHUSEN, P. 1973. Motivation of Human and Animal Behavior: An Ethological View. Van Nostrand Reinhold Co., New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[LORENZ, K. 1981. The Foundations of Ethology. Springer-Verlag, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>541177</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[MITCHELL, K. 1997. Machine Learning. McGraw Hill, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND GOLDBERG, A. 1996. Improv: A system for scripting interactive actors in virtual worlds. In Proceedings of SIGGRAPH 1996, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PRYOR, K. 1999. Clicker Training for Dogs. Sunshine Books, Inc., Waltham, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>153687</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[RABINER, L., AND JUANG, B.-H. 1993. Fundamentals of Speech Recognition. Prentice Hall, Englewood Cliffs, NJ.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[RAMIREZ, K. 1999. Animal Training:Successful Animal Management Through Positive Reinforcement. Shedd Aquarium, Chicago, IL.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[RESNER, B., STERN, A., AND FRANK, A. 1997. The truth about catz and dogz. In The Computer Games Developer Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[REYNOLDS, C. 1987. Flocks, herds and schools: A distributed behavioral model. In Proceedings of SIGGRAPH 1987, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., COHEN, M., AND BODENHEIMER, B. 1999. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics And Applications 18, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[SHETTLEWORTH, S. J. 1998. Cognition, Evolution and Behavior. Oxford University Press, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551283</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[SUTTON, R., AND BARTO, A. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>116550</ref_obj_id>
				<ref_obj_pid>116517</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[SUTTON, R. 1991. Reinforcement learning architectures for animats. In The First International Conference on Simulation of Adaptive Behavior, MIT Press, Paris, Fr.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>61950</ref_obj_id>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[THERRIEN, C. 1989. Decision Estimation and Classification: An Introduction to Pattern Recognition and Related Topics. John Wiley and Sons, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[TOMLINSON, B., AND BLUMBERG, B. 2002. Alphawolf: Social learning, emotion and development in autonomous virtual agents. In First GSFC/JPL Workshop on Radical Agent Concepts.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[TU, X., AND TERZOPOULOS, D. 1994. Artificial fishes: Physics, locomotion, perception, behavior. In Proceedings of SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[VAN DE PANNE, M., AND FIUME., E. 1993. Sensor-actuator networks. In Proceedings of SIGGRAPH 1993, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[VAN DE PANNE, M., KIM, R., AND FIUME., E. 1994. Synthesizing parameterized motions. In 5th Eurographics Workshop on Simulation and Animation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>139618</ref_obj_id>
				<ref_obj_pid>139611</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[WATKINS, C. J., AND DAYAN, P. 1992. Q-learning. Machine Learning 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[WILKES, G. 1995. Click and Treat Training Kit. Click and Treat Inc., Mesa, AZ.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>337537</ref_obj_id>
				<ref_obj_pid>336595</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[YOON, S., BLUMBERG, B., AND SCHNEIDER, G. 2000. Motivation-driven learning for interactive synthetic characters. In Proceedings of the Fourth International Conference on Autonomous Agents.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>721583</ref_obj_id>
				<ref_obj_pid>647288</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[YOON, S., BURKE, R., AND BLUMBERG, B. 2000. Interactive training for synthetic characters. In Proceedings of AAAI 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Integrated Learning for Interactive Synthetic Characters Bruce Blumberg, Marc Downie, Yuri Ivanov, Matt 
Berlin, Michael Patrick Johnson, Bill Tomlinson Synthetic Characters Group, The Media Lab, MIT . Abstract 
The ability to learn is a potentially compelling and important qual­ity for interactive synthetic characters. 
To that end, we describe a practical approach to real-time learning for synthetic characters. Our implementation 
is grounded in the techniques of reinforcement learning and informed by insights from animal training. 
It simpli­.es the learning task for characters by (a) enabling them to take ad­vantage of predictable 
regularities in their world, (b) allowing them to make maximal use of any supervisory signals, and (c) 
making them easy to train by humans. We built an autonomous animated dog that can be trained with a technique 
used to train real dogs called clicker training . Ca­pabilities demonstrated include being trained to 
recognize and use acoustic patterns as cues for actions, as well as to synthesize new actions from novel 
paths through its motion space. Akey contribution of this paper is to demonstrate that by ad­dressing 
the three problems of state, action, and state-action space discovery at the same time, the solution 
for each becomes easier. Fi­nally, we articulate heuristics and design principles that make learn­ing 
practical for synthetic characters. CR Categories: I.2.6 [Arti.cial Intelligence]: Learning Concept Learning; 
I.2.8 [Arti.cial Intelligence]: Problem Solving, Control Methods and Search Heuristic Methods; I.3.6 
[Computer Graphics]: Methodology and Techniques Interaction Techniques Keywords: behavioral animation, 
animation, computer games 1 Introduction We believe that interactive synthetic characters must learn 
from ex­perience if they are to be compelling over extended periods of time. Furthermore, they must adapt 
in ways that are immediately under­standable, important and ultimately meaningful to the people inter­acting 
with them. Nature provides an excellent example of systems that do just this: pets such as dogs. Remarkably, 
dogs do this with minimal insight into our behav­ior, and little understanding of words and gestures 
beyond their use as cues. In addition, dogs are only able to learn causality if the events, actions and 
consequences are proximate in space and time, and as long as the consequences are motivationally signi.­cant. 
Nonetheless, the learning dogs do allow them to behave com­monsensically and ultimately exploit the highly 
adaptive niche of .email:bruce,marcd,yivanov,mattb,aries,badger@media.mit.edu Copyright &#38;#169; 2002 
by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use is granted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
man s best friend. Our belief is that by embedding the kind of learning of which dogs are capable into 
synthetic characters, we can provide them with an equally robust mechanism for adapting in meaningful 
ways to the people with whom they are interacting. In this paper, we describe a practical approach to 
real-time learn­ing for synthetic characters that allows them to learn the kinds of things that dogs 
seem to learn so easily. We ground our work in the traditional techniques of reinforcement learning, 
in which a crea­ture learns to maximize reward in the absence of a teacher. Addi­tionally, our approach 
is informed by insights from animal training, where a teacher is available. Animals and their trainers 
act as a coupled system to guide the animal s exploration of its state, ac­tion, and state-action spaces 
(see Section 3.2). Therefore, we can simplify the learning task for autonomous animated characters by 
(a) enabling them to take advantage of predictable regularities in their world, (b) allowing them to 
make maximal use of any super­visory signals, either explicit or implicit, that the world offers, and 
 (c) making them easy to train by humans.  Figure 1: Terence is an autonomous animated pup that can 
be trained using clicker training. The trainer s interface is a micro­phone and pair of virtual hands 
controlled by a gamepad. The left hand holds a clicker that makes a sound when pressed. The right hand 
serves as a target for luring, and can also give extra reward by scratching the dog s head. Using this 
system, we implemented the autonomous animated dog shown in Figure 1 that can be trained with a technique 
used to train real dogs. The synthetic dog thus mimics some of a real dog s ability to learn including: 
. The best action to perform in a given context. . What form of a given action is most reliable in producing 
re­ward. . The relative reliability of its actions in producing a reward and altering its choice of action 
accordingly. . To recognize new and valuable contexts such as acoustic pat­ terns. . To synthesize new 
actions by being lured into novel con.g­urations or trajectories by the trainer. In order to accomplish 
these learning tasks, the system must address the three important problems of state, action and state-action 
space discovery. A key contribution of this paper is to show how these processes may be addressed in 
an integrated approach that guides and simpli.es the individual processes. We emphasize that our behavioral 
architecture is one in which learning can occur, rather than an architecture that solely performs learning. 
As we will see, learning has important implications for many aspects of a general behavior architecture, 
from the design of the perceptual mechanism to the design of the motor system. Conversely, careful attention 
to the design of these components can dramatically facilitate the learning process. Hence, an important 
goal of this paper is to highlight some of these key design consider­ations and to provide useful insights 
apart from the speci.cs of the approach that we have taken. We begin by surveying related work and putting 
our work in per­spective. We then turn to a discussion of reinforcement learning. We introduce the core 
concepts and terminology, discuss why a na¨Ùve application of reinforcement learning to synthetic characters 
is problematic, and .nally draw on insights from animal training on how animals conceptually address 
the same issues. We then describe our approach, reviewing our key representations and pro­cesses for 
state, action and state-action space discovery. We present our experience with Terence, our virtual pup, 
and discuss limita­tions of our approach. We conclude with a summary of what we see as the key lessons 
from our work. 2 Related Work The approach described below is in the general category of rein­forcement 
learning. (See [Kaelbling 1990; Ballard 1997; Mitchell 1997; Sutton and Barto 1998] for good introductions 
to the .eld.) The incremental exploration of state-action space proposed below is similar to an approach 
originally suggested by Drescher [1991]. What is new in our work is an integrated approach to state, 
action and state-action space discovery within the context of reinforce­ment learning and an articulation 
of heuristics and design principles that make learning practical for synthetic characters. Our approach 
is also informed by a close study of animal train­ing and what it seems to imply about how animals learn. 
For good introductions to animal learning, see [Lorenz and Leyahusen 1973; Lorenz 1981; Shettleworth 
1998; Gould and Gould 1999; Gallistel and Gibbon 2000; Lindsay 2000; Coppinger and Coppinger 2001]; for 
an introduction to the .eld of animal training see [Ramirez 1999]; and for an introduction to the speci.c 
approach to training that we take as our inspiration, i.e., clicker training , see [Wilkes 1995; Pryor 
1999; Ramirez 1999]. Clicker training has been suc­cessfully adapted by researchers at SONY CSL to train 
their robotic dog AIBO [Kaplan et al. 2001]. See [Yoon et al. 2000a] for an early application of clicker 
training to training animated characters. What is novel in our research is a computational model that 
not only uses animal training as a starting point, but places learning within the larger behavioral context. 
In an effort to reduce the work required by animators, learning has been applied to the problem of generating 
motion primitives. (See [van de Panne and Fiume. 1993; van de Panne et al. 1994; Grzeszczuk and Terzopoulos 
1995; Grzeszczuk et al. 1998; Hod­gins and Pollard 1997; Gleicher 1998].) Most recently, [Falout­sos 
et al. 2001] have done exciting work showing how a statistical learning technique (SVM) can be used to 
learn the pre-conditions from which a given specialist controller can succeed at its task, thus allowing 
them to be combined into a general purpose motor system for physically based animated characters. The 
approaches to motor learning described above focus on learning how to move subject to some criteria such 
as energy min­imization, whereas the motor learning that is described in this paper focuses on learning 
the value with respect to a motivational goal of moving in a certain way . As such, our approach represents 
a layer above many of these prior approaches. Finally we note our empha­sis is on learning as an online 
capability to enhance interaction with a human participant rather than as a design tool. A number of 
noteworthy architectures for control of animated autonomous characters have been proposed [Reynolds 1987; 
Tu and Terzopoulos 1994; Blumberg and Gaylean 1995; Perlin and Goldberg 1996; Funge et al. 1999; Burke 
et al. 2001]. While pro­ducing impressive results, most of these systems have not incorpo­rated behavioral 
learning and thus cannot modify the pre-speci.ed behavior on the basis of experience. Our contribution 
is to integrate learning into a general-purpose behavior architecture. Higher-level behavioral learning 
has only begun to be explored in computer graphics. (For examples, see [Yoon et al. 2000b; Burke et al. 
2001; Tomlinson and Blumberg 2002].) Several of the current generations of digital pets such as Dogz 
[Resner et al. 1997], Crea­tures [Grand et al. 1996] and AIBO also incorporate simple learn­ing. This 
is done particularly well in Dogz,to the point that many people are convinced that more learning is going 
on than is actually the case. Factors contributing to this assumption include: imme­diate emotional responses 
by the creature to good or bad conse­quences, intuitive means for delivering reward or punishment, and 
an immediate and noticeable change in behavior in response. The popular video game Black and White [Evans 
2002] centrally fea­tures a character that learns from a person s actions. Our contribu­tion is to provide 
insights into how state and action space discovery can be integrated into the learning process. 3 Background 
on Learning and Training The approach taken in our work is best understood as a variant of a popular 
machine learning technique known as reinforcement learn­ing. In this section we begin by introducing 
the key ideas and termi­nology. We then look at the problem from the perspective of animal training and 
highlight the key ideas from animal training that can help make reinforcement learning practical for 
interactive synthetic characters. 3.1 Introduction to Reinforcement Learning Reinforcement learning (RL) 
is often used by autonomous systems that must learn from experience. In reinforcement learning, the world 
in which the creature lives is assumed to be in one of a set of perceivable states. The goal of reinforcement 
learning is to learn an optimal sequence of actions that will take the creature from an ar­bitrary state 
to a goal state in which it receives a reward. The main approach taken by reinforcement learning is to 
probabilistically ex­plore states, actions and their outcomes to learn how to act in any given situation. 
Before we describe how this is done, we need to de.ne state, action and reward a bit more formally. State 
refers to a speci.c, hopefully useful, con.guration of the world as sensed by the creature s entire sensory 
system. As such, state can be thought of as a label that is assigned to a sensed con­.guration. The space 
of all represented con.gurations of the world is known as the state space. Performing an action is how 
a creature can affect the state of its world. Typically, the creature is assumed to have a .nite set 
of actions, from which it can perform exactly one at any given instant, e.g., walk or eat. The set of 
all possible actions is referred to as the action space. A state-action pair, denoted as <S/A>,isa relationship 
between a state S and an action A.Itis typically accompanied by some nu­meric value, e.g., future expected 
reward, that indicates how much bene.t there is in taking the action A when the creature senses state 
S. Based on this relationship a policy is built, which represents a probability with which the creature 
selects an action given a spe­ci.c state. The creature receives reinforcement (or reward) when it reaches 
a state in which it can satisfy a goal. For example, if a dog sits and gets a treat for doing so, the 
reward or reinforcement is the resulting decrease in hunger or pleasure in eating the treat. Credit assignment 
is the process of updating the associated value of a state-action pair to re.ect its apparent utility 
for ultimately re­ceiving reward. While there are a number of variants of reinforcement learn­ing, Q-Learning 
is a simple and popular representative that can be used to illustrate some key concepts. In Q-Learning, 
introduced by Watkins [Watkins and Dayan 1992], the state-action space is dis­cretized if necessary and 
stored in a lookup table. In the table, each row represents a state, and each column represents an action. 
An entry in the table represents the utility , or Q-Value, ofagiven state-action pair with respect to 
getting a reward. Watkins showed that the optimal value for each state-action pair could be learned by 
incrementally (and exhaustively) exploring the space of state-action pairs and by using a local update 
rule to re.ect the consequences of taking a given action in a given state with respect to achieving the 
goal state [Sutton 1991]. It is important to note that techniques such as Q-Learning that focus on learning 
an optimal sequence of actions to get to a goal state solve a much harder problem than either animals 
solve or that we need to solve for synthetic characters. As we will see, animals are biased to learn 
proximate causality. Even in the case of se­quences, the noted ethologist Leyahusen suggests that the 
individ­ual actions may be largely self-reinforcing, rather than being re­inforced via back propagation 
[Lorenz and Leyahusen 1973]. In addition, Nature places a premium on learning adequate solutions quickly. 
Reinforcement learning is an example of an unsupervised learn­ing technique in that the only supervisory 
signal is the reward re­ceived when it achieves a goal. On the other hand, it is clear that a trainer 
could signi.cantly expedite exploration of the respective spaces by guiding the search. In the following 
section we discuss how trainers and their animals cooperate to simplify the learning task. 3.2 The Perspective 
of Animal Training Here we describe a popular and easy technique for animal train­ing called clicker 
training and what it seems to imply about how animals learn. Clicker training unfolds in three basic 
steps. The .rst step is to create an association between the sound of a toy clicker and a food reward. 
A dog conditioned to the clicker will expectantly look for a treat upon hearing the click sound. Once 
the association between clicks and treats is made, trainers use the click sound to mark behaviors that 
they wish to encourage. By clicking when the dog performs a desired behavior, and subsequently treating, 
the dog be­gins to perform the behavior more frequently. Animals appear to make an important simplifying 
assumption: an action or stimulus that immediately precedes a motivationally signi.cant consequence is 
as good as causal. Hence, clicker train­ing is a particularly effective training technique because it 
makes it easy to provide immediate feedback. Indeed, the sound of the clicker marks the exact behavior 
that leads to the subsequent treat, as well as signaling that the action is complete. In addition, it 
acts as a bridge between when the dog earned the reward and when it actually receives it. Since clicker 
training relies on the dog to produce some approx­imation of a desired behavior before it can be rewarded 
(and pro­ducing a high level of reinforcement keeps the dog interested in the process), trainers utilize 
a variety of techniques to encourage the dog to perform behaviors they might otherwise perform infre­quently, 
or not at all. A useful and popular technique is to train the dog to touch an object such as the trainer 
s hand or a target stick . By subsequently manipulating the position of the target, the trainer can, 
in effect, lure the dog through a trajectory or into a pose as it follows its nose. For example, by moving 
the target over the dog s head, a dog may be lured into sitting down. If lured and rewarded repeatedly, 
the dog will begin to produce the action (e.g., sit) with­out being lured. This suggests that the animal 
is associating reward with its resulting body con.guration or trajectory, and not for the action of simply 
following its nose. The dog is unlikely to perform the desired .nal form of the be­havior immediately, 
especially if it is an unusual behavior, e.g., dancing on the two rear feet . As a result, the trainer 
will often guide the dog toward the desired behavior by rewarding ever-closer approximations in a process 
known as shaping. The third and .nal step in clicker training is to add a discrimi­native stimulus such 
as a gesture or vocal cue. Trainers typically introduce the cue by presenting it as the animal is just 
beginning to perform the action, and then subsequently rewarding the action. Signi.cantly, the animal 
has already decided what to do before the trainer issues a cue butis still able to learn to associate 
the ac­tion (and its subsequent reward) with a cue occurring in a temporal window proximate to the action 
onset. Note, unlike other training techniques, clicker trainers teach the action .rst, and then the cue. 
The superiority of this decomposition suggests that animals make associations more easily if they already 
know a particular action is valuable. 3.3 Making Learning Practical for Synthetic Char­acters While 
reinforcement learning provides a theoretically sound basis for building systems that learn, there are 
a number of issues that make it problematic in the context of autonomous animated crea­tures. Borrowing 
ideas from animal training, however, we can ad­dress these problems in a way that makes real-time learning 
practi­cal for synthetic characters. Enable them to take advantage of predictable regularities in their 
world.We saw that dogs use predictable regularities of how the world works to simplify the learning task. 
For example, they bias their choice of action toward those actions that have been suc­cessful at receiving 
reward in the past. Similarly, they limit their at­tention to stimuli or cues that occur in a temporal 
window around an action s onset in order to identify reliable contexts in which to per­form the action. 
Through variations in how the action is performed and by attending to correlations between the action 
s reliability in producing reward and the state of contemporaneous stimuli, they are performing a local 
search in a potentially valuable neighbor­hood. This model of causality, while very simple, is nonetheless 
suf.­cient to capture many aspects of how the world works. Perhaps as important for synthetic characters, 
learning proximate causality is exactly the kind of learning that is most apparent and easiest to un­derstand 
for an observer. A .nal insight is that the state and action spaces often contain a natural hierarchical 
organization that facili­tates the search process. Allow them to make maximal use of any supervisory 
signals, either explicit or implicit, that the world offers. Biasing the choice of behavior based on 
consequences is an example of making use of explicit supervisory signals (such as getting a treat). The 
con­sequences of the action can also be used as an implicit (secondary) supervisory signal for guiding 
the exploration of the character s state and action spaces. This guidance is signi.cant because syn­thetic 
characters, by their very nature, have state and action spaces that are both continuous and far too big 
to permit an exhaustive search, even if discretized. For example, the a priori state space for a character 
that must learn to respond to arbitrary verbal or gestural cues, will be intractably huge since it will 
include the entire set of possible acoustic and gestural patterns. Similarly, in the case of an expressive 
character for whom the style of the action is as important as the action itself, the action space will 
be the space of all possible motions. Ironically though, most of the volume of these respec­tive spaces 
is irrelevant from the character s standpoint of getting reward. Our observation from animal training 
is that animals seem to solve this problem by building models of important sensory cues on demand , using 
rewarded actions as the context for identifying important sensory cues and for guiding the perceptual 
model of the cue. For example, a good example of the acoustic pattern sit is the one that occurs just 
before or during a sit action that results in reward. This point suggests a computational strategy discover, 
based on experience, those patterns (in the case of state space) or motions (in the case of action space) 
that do seem to matter and add them dynamically to their respective spaces. These processes are known 
as state space discovery and action space discovery respectively. While there are established techniques 
for performing state-space discovery (see, for example, [Ivanov 2001]) they often require a lot of data. 
A key insight is that these processes can be guided by using the context of a rewarded action to facilitate 
the classi.cation pro­cess. Indeed, by choosing the right representation, state and action space discovery 
can be done using exactly the same mechanism. Make them easy to train.For training to be a compelling 
ex­perience for the human participant, the character needs to be easy to train using observable behavior, 
without the trainer having any visibility into the character s internal state. On the simplest level, 
the character must be sensitive to the im­mediate consequences of its actions, attend to changes in stimuli 
that occur right before and during its performance of an action, and its observable behavior must change 
quickly in response. The abil­ity to be trained via luring is especially important since otherwise the 
trainer has to wait for the animal to randomly choose the action, which could take forever. Our discussion 
of animal training suggests that animals perform the equivalent of credit assignment in a way that makes 
it easier to train them than it might be otherwise. In the case of luring, they generalize from being 
rewarded for following their nose to being rewarded for their resulting con.guration or trajectory. In 
the lan­guage of reinforcement learning, it is as if during credit assignment the follow your nose state-action 
pair lets another state-action pair get the credit, namely the one associated with the con.gura­tion 
or trajectory. Similarly, when associating a cue with an action, animals act as if they form and assign 
credit to new state-action pairs based on evidence acquired while performing an existing but related 
state-action pair (i.e., one that shares the same action). The computational implication of luring and 
cue association is that by allowing the state-action pair that would normally get credit to dele­gate 
its credit to another pair, the training process can be facilitated.  4 System Description We now turn 
to a moderately detailed discussion of the representa­tions and processes used in our system. While due 
to space we can­not provide all the implementation details, we try to offer insights in design choices. 
The system described below has been implemented as part of a system that is similar to that described 
in [Burke et al. 2001; Isla et al. 2001]. 4.1 Key Representations 4.1.1 State Many state spaces have 
a natural hierarchical organization, e.g., the space of acoustic patterns, the space of utterances, and 
individual utterances such as sit , down and roll over . By incorporating a similar hierarchical representation 
of state space into our system, we can notice that a given action is more reliable when a whole class 
of states is active. This information provides evidence that further exploration and re.nement within 
a class of states might be fruitful for increasing reliability of reward. Figure 2: In our work the 
state space is represented by a percept tree. The percept tree maintains a hierarchical representation 
of the sensory input where leaf nodes represent the highest degree of specialization and the root node 
matches any sensory input. The structure of the tree is sequentially discovered and re.ned with time 
as indicated by its utility with respect to getting reward. As illustrated in Figure 2, we use a hierarchical 
mechanism called a percept tree to extract state information from the world. Each node in the tree is 
called a percept, with more speci.c per­cepts nearer to the leaves. Percepts are atomic perception units, 
with arbitrarily complex logic, whose job it is to recognize and ex­tract features from raw sensory data. 
For example, one percept may recognize the presence of the utterance sit in an auditory stream, and another 
might recognize the performance of a particular mo­tor trajectory. Similarly, an utterance percept might 
recognize the presence of utterances in an auditory .eld, and its children might recognize the presence 
of speci.c utterances such as sit , down , roll-over , etc. The root of the tree is the most general 
percept, which we call True since it is always active. Percepts are model-based recognizers, meaning 
that on each simulation cycle they compare raw sensory data to an internal model and become active if 
they match within some threshold. If a percept is active, the sensory data is passed recursively to the 
per­cept s children for more speci.c classi.cation. If not, all its chil­dren can be pruned from the 
update cycle. This culling is impor­tant since percept models can vary in complexity. For symbolic data, 
the model is trivial: it is a string and the matching criterion is simple string equality. In the case 
of an utterance percept, how­ever, the model may be a collection of vectors of cepstral coef.­cients[Rabiner 
and Juang 1993] that represent the mean of a set of previously learned examples [Ivanov 2001], and the 
compari­son between sensory data and the model is more complex (section 4.2.3). Motion percepts use a 
model that represents a path through the space of possible motions. Also associated with each percept 
is a short-term memory mechanism that keeps track of its activation history over some period of time. 
In the language of RL, a percept represents a subset of the entire state space. That is, it looks for 
a speci.c feature in the state space. In RL, state refers to the entire sensed con.guration of the world; 
a percept is focused on only one aspect of that con.guration. As we will see, percept decomposition of 
state allows for a heuristic search through potentially intractable state and state-action spaces. The 
downside is that it makes learning conjunctions of features harder. It is important to note that the 
percept tree is a dynamic structure that is modi.ed as a result of state space discovery as described 
in Section 4.2.3. 4.1.2 Action Actions refer to identi.able patterns of motion through time. They are 
often conceptualized and implemented as discrete verbs, per­haps parameterized with associated adverbs 
(see [Rose et al. 1999]). While this approach has the desirable property that other parts of the system 
can treat the action as a label, the representation is not amenable to the type of action space discovery 
needed to support luring. In contrast, if we consider a creature as having a pose space that contains 
all of its possible body con.gurations, then an action can be thought of as a speci.c path through pose 
space. Just as a percept is a label for a class of observations, an action can be thought of as a label 
associated with a path or class of paths in pose space. For the purposes of learning, the analogy to 
state learning is complete if one assumes the existence of a distance metric that evaluates the similarity 
of two paths. This is the fundamental rep­resentation of action used by our system. Each creature in 
our system has a motor system with a represen­tation of the creature s pose space encoded in a structure 
called a pose-graph. The nodes in the pose-graph represent annotated con­.gurations that are generated 
originally from source animation ma­terial. A node includes a complete set of joint angles and veloci­ties 
as well as a number of annotations including time and source­labeling (i.e., what animation it came from 
and at what point within the animation), connectivity information (e.g., the preceding and following 
poses in the source animation), and over time, a distri­bution of the likelihood of being in the current 
pose as a result of all known actions. For example, a pose associated with a sitting con.guration might 
be the result of sitting or shaking a paw but is unlikely to be associated with being told to jump. The 
nodes of this graph are connected together in tangled di­rected, weighted graphs.By associating a distance 
metric between poses, paths taking the body from pose to pose can be ef.ciently found and animations 
can be reformed in real-time by interpolat­ing through nodes together again as needed. Details regarding 
the actual metric may be found in [Downie 2000] but essentially it cap­tures the intuition that transitioning 
between similar joint con.gura­tions should be preferred over widely differing joint con.gurations, and 
that transitions that require less acceleration should be favored over those that require more. Because 
the pose-graph is derived from correct examples, it implicitly captures, to some approx­imation, many 
of the biological and physical constraints of how the creature moves at the very least we are always 
interpolating within the convex hull of these correct examples. In addition to the pose-graph, the motor 
system contains motor programs that are capable of generating paths through pose-graphs in response to 
requests from actions. These programs may be quite simple (essentially no more than playing out a particular 
animation) or more complex (for example, luring towards an object). One branch of the percept tree is 
devoted to motor percepts that recognize paths taken by the motor system through pose space. That is, 
a given motor percept has a model of a path and the ca­pability to compare a novel path to this model. 
As we will see in section 4.2.4 this allows us to treat action space discovery using almost the identical 
mechanism as used in state space discovery. The key points about action are that (a) our underlying represen­tation 
of action is that of a path through a space of body con.gu­rations, (b) we can calculate a distance metric 
between paths that re.ects the similarity between two paths, (c) associated with each path is a label 
and (d) the label is used to specify which path through pose space the motor system should follow at 
any given point in time. 4.1.3 State-Action The representation of a particular state-action pair in 
our system is called an action tuple.An action tuple is composed of .ve ele­ments that specify: what 
to do, when, to what, for how long, and why. However, one can think of an action tuple as an augmented 
state-action pair in which the state information is provided by an associated percept (when), and the 
action (what) is the label for a given path through pose space. Action tuples are organized into groups 
and compete probabilistically for activation based on their value and applicability (i.e., if their associated 
percept is active). In the discussion below, we will use action tuple and percept-action pair interchangeably.1 
Each action tuple keeps reliability and novelty statistics for its associated percept and the percept 
s children. Reliability models the correlation between an action tuple being rewarded and a per­cept 
being active (in an overlapping temporal window). The novelty statistic re.ects the relative frequency 
of the event of the percept being active; a novel percept is one has been rarely active. These statistics 
are used by the system to guide the exploration of poten­tially useful states by identifying more speci.c 
percepts that seem correlated with an increased reliability of the action in producing reward. Mirroring 
our hierarchical representation of state, action tuples that invoke the same action but that depend on 
different percepts are organized hierarchically according to the speci.city of the percept. When a transition 
between active actions occurs, we perform credit assignment and the outgoing action chooses its best 
action tuple to receive credit. For this approach to work, we need a metric to determine the best candidate 
for credit assignment. This need not be the percept-action pair that actually performed the action. In­stead, 
we .nd the percept-action pair with the same action, but with a percept that was not only active, but 
also the most reliable, novel and speci.c. We search for this pair within a temporal window overlapping 
with the action performed by some speci.ed amount. This is illustrated in Figure 3. Similarly, during 
action selection, each action gets to choose its best action tuple to compete with the best action tuples 
associated with other actions. sit utterance click perceived perceived <true/sit> <true/sit>begins 
ends < sit-utterance /sit> gets the credit Figure 3: In this example, the < true /sit>action tuple delegates 
credit to < sit-utterance /sit>since the sit-utterance percept be­came active during the attention window 
(gray bar) associated with < true /sit>and is a more novel and reliable predictor of reward than true 
. By allowing the credit assignment phase to choose who gets credit we can dramatically simplify the 
learning and training process, as we will see in the section on action space discovery. 1We use percept-action 
pair rather than state-action pair to remind the reader that an action tuple makes its when decision 
based on a subset of the entire state of the world as indicated by its when percept. 4.1.4 Reward An 
action tuple may have good, indifferent or bad consequences. Consequences are expressed on an absolute 
scale, and certain events are labeled a priori as being good or bad .  4.2 Key Processes 4.2.1 Credit 
Assignment Our approach to credit assignment varies from the traditional RL approach in a number of ways: 
. Delegate credit assignment. The action tuple that is deacti­vating and normally the candidate for credit 
assignment has the option to delegate credit assignment to another action tu­ple. This is perhaps the 
most signi.cant difference and plays an important role in our algorithm. . Selective propagation of value. 
The key implication of the bias to learn immediate consequences is that we do not prop­agate value unless 
a good or bad consequence is observed, or unless the novelty of the percept associated with the succeed­ing 
action tuple is above a threshold. The intuition is that the percept-action pair should only get credit 
if it produced re­ward or if it seems causal in making a novel percept active, thereby allowing another 
potentially more valuable percept­action pair to become active. . A rate-based model.In traditional RL, 
the scalar value of a state-action pair tends towards the average value of perform­ing that action in 
that state. An action tuple, on the other hand, explicitly learns a model of its rate of producing reward; 
ulti­mately, its value is a function of this learned rate and the value assigned to the consequences. 
During credit assignment, an action tuple updates a model of its rate of producing reward based on consequence. 
. Non-stationary estimate. The rate of producing a signi.cant consequence is estimated over the most 
recent N trials, where N is typically a small number. Should the world change, a creature can rapidly 
update its rate estimates and adapt to the changes. Trials are measured in the number of activations 
of the action tuple that led to a reward. Hence, they are variable in length, re.ecting the pattern of 
rewards. The most important reason for using a rate-based model is that by maintaining an explicit model 
of rate, the action tuple is able to inform the rest of the system whether a consequence is consistent 
with its model or not, and hence expected or unexpected. For ex­ample, this information can be used by 
a proto-emotion system to decide whether the creature should show surprise or not, and if so, whether 
the surprise should be positive or negative. 4.2.2 State-Action Space Discovery State-action space discovery 
is the process of discovering the best percept-action pair to perform in any given state. In our earlier 
discussion of RL, we saw that the set of state-action pairs is typi­cally speci.ed a priori and the task 
for the learning algorithm is to exhaustively explore the space and learn the appropriate value for each 
pair. Our hierarchical representation of state allows us to adopt a different approach the system is 
initially populated with only a few percept-action pairs (i.e., action tuples) that represent general 
world states (i.e., reference percepts at the top of the percept tree). Over time, new percept-action 
pairs are added as the system gath­ers evidence that a promising action associated with a given state 
might be made even more reliable if associated with a more spe­ci.c child of the state. This process 
of creating new children action tuples is referred to as specialization.At the same time, of course, 
the system must learn the appropriate value for the percept-action pairs. The advantage of this approach 
is twofold. First, the sys­tem only explores areas of the space for which there is evidence of possible 
improvement. Second, fewer resources are required when action tuples are not created a priori.In this 
section, we discuss how specialization occurs. a) < true / sit / 60 > b) < true / sit / 50 > true 
any-utterance sit < sit / sit / 120 > down Figure 4: This .gure illustrates the process of state-action 
space discovery. In (a) the trainer begins by rewarding the performance of < true /sit>, with the effect 
being that the reliability and value of < true /sit>increases. This in turn increases the frequency of 
sitting. Once the dog is sitting frequently, the trainer starts saying sit as the sit action is performed, 
while continuing to reward the sit. As the trainer continues this process, the system begins to build 
a classi.er for the speci.c utterance that occurs during the atten­tion window associated with rewarded 
sits, and eventually spawns a < sit-utterance /sit>percept-action pair (b). Over time the trainer will 
stop rewarding spontaneous sits or sits in response to other ut­terances (i.e., < true /sit>or < any-utterance 
/sit>). The effect is that the reliability (and value) of these action tuples will drop in comparison 
with that of < sit-utterance /sit>and these less spe­ci.c and reliable action tuples are expressed less 
frequently. During the credit assignment phase, the percept-action pair se­lected for credit assignment 
has the option of specializing. Two conditions must be met to be eligible for specialization. First, 
the value of the percept-action pair must be over some threshold. That is, there needs to be some evidence 
that the percept-action pair or avariant is potentially valuable. Second, the percept must have a child 
whose reliability and novelty is above a certain threshold. These statistics essentially provide evidence 
that a new percept­action pair utilizing that child percept could be more reliable than a percept-action 
pair relying on the parent percept. If these con­ditions are met then a new child of the parent percept-action 
pair is created with the same action as the parent, but with the percept s child. Once added to the parent, 
it becomes eligible to be selected as the most appropriate representative of all of the percept-action 
pairs that share its action. The process of specialization is illustrated in Figure 4. The mechanism 
described above provides a simple hierarchical search of the state-action space, focusing on those areas 
that seem most promising and exploring variants of percept-action pairs for which there is evidence that 
a variant may prove more valuable than its parent. 4.2.3 State Space Discovery As suggested in Section 
3.3, there are important advantages to in­tegrating state space discovery into the learning process. 
For exam­ple, assume a creature is to be taught to perform tricks in response to arbitrary acoustic patterns 
(utterances, whistles, etc.) If state­space discovery is being performed the only acoustic patterns that 
need be considered are (a) those that are actually experienced and (b) those for which there is some 
evidence that they matter with respect to the creature s goals. An unsupervised technique such as k-means 
clustering can be employed to partition the observed patterns into distinct clusters or classes [Therrien 
1989]. In this case, each cluster or class represents aregion of the state space. K-means clustering 
partitions observed patterns into k clusters such that the distance between the center of a cluster and 
all of the observations that comprise that cluster is minimized across all clusters and patterns. This 
algorithm is an example of unsupervised learning since the clusters emerge from the data without any 
supervisory signal providing feedback. Our experience with dog learning suggests a different approach: 
treat all patterns that occur contemporaneously with an action that directly leads to a signi.cant outcome 
(i.e., a reward) as belong­ing to the same cluster. The action itself becomes the label for the cluster 
and the reward acts as a natural supervisory signal that indi­cates if the pattern is a good example 
either of the cluster in which it was classi.ed (and so should be included in the cluster) or as a seed 
for a new cluster. This idea is incorporated into the algorithm used in our system, a variation on an 
incremental k-nearest neigh­bors technique [Ivanov 2001]. For example, in the case of acoustic processing, 
there is a percept that recognizes the presence of acous­tic patterns, and each of its children percepts 
represent a cluster of similar patterns. The child percepts are created dynamically as fol­lows: 1. When 
an acoustic pattern is observed, the acoustic pattern per­cept and its children responsible for classifying 
acoustic pat­terns will attempt to .nd a match. If a match is found, the associated percept becomes active. 
 2. If the percept becomes active, the active percept-action pair may change if the percept is referenced 
by another existing percept-action pair, and if that pair is more reliable in produc­ing good consequences. 
 3. The pattern is stored in short-term memory. 4. The matching percept s model of the pattern is subsequently 
updated during credit assignment if: (a) The deactivating percept-action pair is directly followed by 
good consequences. (b) The percept is a child of the deactivating percept-action pair s percept and 
it became active during the percept­action pair s attention window. (c) The observation was not classi.ed 
by one of the per­cept s children, but 4a) is true. In this case the percept may create a new child and 
initialize the child s model with the observation as its .rst sample.  5. Update reliability statistics 
 Forexample, assume that initially the acoustic pattern percept has no children, and there is a < true 
/sit>percept-action pair (i.e., sit ) that periodically becomes active. Now suppose that the acoustic 
pattern percept repeatedly becomes active in the context of a sit that consistently leads to a reward. 
The .rst time this occurs, it will create a new child percept and initialize it with the pattern that 
activated it. Every subsequent time that a pattern is detected in the context of a rewarded sit , that 
child percept will update its model using the observed pattern. As the child starts classify­ing incoming 
patterns correctly (according to its model) within the context of a rewarded sit , its reliability will 
increase. Finally, as a result of specialization (Section 4.2.2), when its reliability rises above a 
threshold, a new percept-action pair will be created, i.e., < sit /sit>. While simple, this algorithm 
captures what is necessary to learn the kinds of acoustic cues that dogs seem capable of learning. In 
ad­dition, Ivanov [Ivanov 2001; Ivanov et al. 2001] has explored these ideas more formally and has shown 
how this simple idea can be in­corporated into the well-known Expectation-Maximization learning algorithm 
as well as SVM. (See [Ivanov et al. 2001] for a detailed discussion of the algorithm used to perform 
clustering and classi.­cation, as well as clustering with a reduced set of examples.) 4.2.4 Action Space 
Discovery As suggested in Section 3.3, we can perform action space discovery using almost the same approach 
as taken for state space discovery. This simpli.cation is made possible by our representation of action 
(labeled paths through pose space) and by the existence of motor­percepts that can classify a path just 
taken as being either an exam­ple of an existing path or a novel path (Section 4.1.1). Since ac­tion 
space discovery occurs as a result of luring and shaping, how­ever, we need additional machinery. Speci.cally, 
luring requires (a) a follow-your-nose motor program, (b) a motor memory that continuously records recent 
poses that have been visited and (c) the modi.cation to the credit assignment rule as suggested in Section 
3.3. Even though follow-your-nose may directly precede areward, the algorithm can give the credit to 
another action whose associated path is close to that just taken. Using this idea, the algo­rithm for 
performing action space discovery that supports luring is straightforward. When assigning credit (at 
an action s end):  1. If the creature received a direct reward, compare the path taken to known paths: 
(a) If the path is similar to an existing path, then reward the action associated with that path (i.e., 
give it the credit) and update the model of the rewarded path using the path just taken as a new example. 
 (b) If the path is novel (not well captured by some other action), then create a new motor percept and 
initialize its model using the path just taken.  2. If no reward is received, ignore the path. Once 
a motor-percept is added to the percept tree, reliability statis­tics are kept just as in the case of 
other percepts. When a motor­percept s reliability gets above a threshold, a new action tuple is created 
that uses the motor-percept s path model as its action. Once this is done, the action tuple is a candidate 
for specialization and can explore to .nd the context in which it is maximally reliable. Another kind 
of motor learning in animals that we have noted is shaping. In our system we adopt a parameterized approach. 
That is, if the action can be parameterized (e.g., the amplitude of shake­paw ) the parameters can be 
drawn from a local probability distri­bution that re.ects the pattern of rewards. When an action is about 
to be performed, a value for the parameter is chosen probabilisti­cally. If the action is subsequently 
rewarded, the probability distri­bution is adjusted to make it more likely in the future that a value 
near the chosen value will be selected. If the action is not rewarded, the probability distribution is 
either left unchanged or adjusted to make it less likely that a similar value will be chosen in the future. 
  5 Results and Discussion The system described in Section 4 has been incorporated into a general-purpose 
behavior architecture described elsewhere [Burke et al. 2001; Isla et al. 2001]. In the accompanying 
video .gure, we demonstrate some aspects of clicker training and luring on our syn­thetic pup, as discussed 
in Section 3 and illustrated in Figures 1 and 5. Figure 5: On the left we see Terence performing a Beg. 
On the right, an example of our action tuple visualizer. Initially, the pup experiments among its known 
actions. As the trainer preferentially rewards sitting, the frequency of sitting in­creases. When sitting 
is performed reliably, the trainer starts giving the verbal cue sit as the pup begins to sit, while also 
reducing the rate of reinforcement if the pup sits in the absence of the cue. The system, through state 
space discovery, creates a new percept that contains a model of the (arbitrary) acoustic pattern associated 
with the rewarded sit and adds it to the pup s percept tree. Eventually, anew percept-action pair is 
created that represents < sit /sit>.At the same time, we see that the frequency of spontaneous sitting 
de­creases. Next, we demonstrate simple luring of the dog by moving the target hand over the dog s head 
and clicking as he gets into the sit pose. We also illustrate the more complex example of luring the 
dog through a novel trajectory in this case, walking in an S pattern on the ground. When rewarded, this 
lured trajectory is added to the action space as a new action (through action space discovery), and can 
thus be associated with a cue and can be selected randomly by the pup in the future just like any of 
the previously known actions. Finally, we demonstrate shaping. The pup experiments with dif­ferent forms 
of his parameterized shake-paw action. The trainer rewards ever higher versions of the shake action until 
the pup shakes his paw high reliably. 5.1 Limitations and Future Work Our system has a number of important 
limitations and areas for future work: . The system is biased to learn immediate consequences rather 
than extended sequences. Nonetheless, learning sequences is important, and we will be addressing this 
area in our future work. . The system does not address spatial and social learning. Our sense is that 
while much can be shared across learning tasks, it is very likely that the right solution will have specialized 
mechanisms and representations for speci.c learning tasks. (See [Isla 2001] for an example of spatial 
learning.) . There are things the system should be able to learn which it cannot for example, states 
that are conjunctions or disjunc­tions of percepts. In addition, it cannot generalize from spe­ci.c percepts 
to more general ones. These, however, are hard problems. An easier problem, and one that has been addressed 
by a variant of the system discussed here, is to learn impor­tant correlations among events that enable 
the creature to act proactively[Burke 2001]. . The existence, speed and quality of classi.ers, such as 
our utterance or path classi.ers, are critically important to the functioning of the system, but we have 
only touched on them brie.y here. While our integrated approach helps the classi­.ers build better models, 
more could be done. For example, the classi.ers do not currently make use of negative examples. (See 
[Ivanov 2001] for an in-depth discussion of this topic.) . How will the system scale? We feel that our 
integrated ap­proach as well as our hierarchical representations of the learn­ing spaces will allow our 
system to scale better than a tradi­tional RL system, but more work needs to be done to support this 
claim. 5.2 Useful Insights While our results are from a speci.c learning system, there are a number 
of ideas that we believe are generally useful in the context of learning for synthetic characters, regardless 
of the speci.cs of the implementation. . Use temporal proximity to limit search. We utilize a tempo­ral 
attention window that overlaps the beginning of an action to identify potentially relevant states. Similarly, 
we generally assign credit to the action that immediately precedes a moti­vationally signi.cant event. 
. Use hierarchical representations of state, action and state­action space. We utilize loosely hierarchical 
representations of state, action and state-action space and use simple statistics to identify potentially 
promising areas of the respective spaces for exploration. We grow these hierarchies downward toward more 
.ne-grained representations of state and more speci.c (and hopefully more reliable) state-action pairs. 
. Use natural feedback signals to guide exploration of the three spaces. The practical effect in both 
cases is that fewer models are built, and those that are built tend to be more rele­vant and robust. 
. Bias frequency and variability of action so as to facilitate learning. This not only allows the creature 
to exploit what it knows, but also gives it more opportunities to discover more reliable variations. 
.Give credit where credit is due. The state-action pair that would normally receive credit should be 
given the option to delegate its credit to another, potentially more appropriate, state-action pair. 
We saw that this was particularly useful in the context of luring .  6 Conclusion We described a practical 
approach to real-time learning for syn­ thetic characters that allows them to learn the same kinds of 
things that dogs seem to learn so easily. We believe that by embedding dog-level learning into synthetic 
characters, we can provide them with a way to meaningfully adapt to human interaction. By addressing 
the three problems of state, action, and state­ action space discovery at the same time, the solution 
for each be­ comes easier. Similarly, by viewing learning and training as a cou­ pled system we were 
able to gain valuable insights into each.  7Acknowledgments The authors wish to thank the other members 
and friends of the Synthetic Characters Group, past and present who contributed to this work, including: 
Robert Burke, Damian Isla, Geoff Beatty, Jennie Cochran, Scott Eaton, Ashley Eden, Jesse Gray, Matthew 
Grimes, Michal Hlavac, Chris Kline, Derek Lyons, Ben Resner, Ken Russell, Adolph Wong, and Soon-Yee Yoon. 
Thanks also to Prof. Gerald Schneider. We are indebted to Gary Wilkes for his insights into dog training 
and behavior. We would especially like to thank Sydney, the .rst author s silky terrier, who continues 
to be the best teacher of all. This work was funded by the Digital Life Consortium of the MIT Media Lab. 
 References BALLARD,D. 1997. An Introduction to Natural Computation. MIT Press, Cambridge, MA. BLUMBERG, 
B., AND GAYLEAN,T. 1995. Multi-level direc­tion of autonomous creatures for real-time virtual environments. 
In Proceedings of SIGGRAPH 1995,ACM Press / ACM SIG-GRAPH, Computer Graphics Proceedings, Annual Conference 
Series, ACM. BURKE, R., ISLA, D., DOWNIE, M., IVANOV,Y., AND BLUM-BERG,B. 2001. Creature smarts: The 
art and architecture of a virtual brain. In Proceedings of the Computer Game Developers Conference. BURKE,R. 
2001. Its about Time:Temporal Representation for Synthetic Characters. Master s thesis, The Media Lab, 
MIT. COPPINGER, R., AND COPPINGER,L. 2001. Dogs: A Startling New Understanding of Canine Origin, Behavior, 
and Evolution. Scribner, New York, NY. DOWNIE,M. 2000. behavior, animation, music: the music and movement 
of synthetic characters. Master s thesis, The Media Lab, MIT. DRESCHER,G. 1991. Made-Up Minds:A Constructivist 
Approach to Arti.cial Intelligence. MIT Press, Cambridge MA. EVANS,R. 2002. Varieties of learning. In 
AI Game Programming Wisdom,E. Rabin, Ed. Charles River Media, Hingham MA. FALOUTSOS,P., VAN DE PANNE, 
M., AND TERZOPOLOUS,D. 2001. Composible controllers for physics-based character ani­mation. In Proceedings 
of SIGGRAPH 2001,ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Confer­ence Series, 
ACM. FUNGE, J., TU, X., AND TERZOPOLOUS,D. 1999. Cognitive modeling: Knowledge, reasoning and planning 
for intelligent characters. In Proceedings of SIGGRAPH 1999,ACM Press /ACM SIGGRAPH, Computer Graphics 
Proceedings, Annual Conference Series, ACM. GALLISTEL,C.R., AND GIBBON,J. 2000. Time, rate and condi­tioning. 
Psychological Review 107. GLEICHER,M. 1998. Retargetting motion to new characters. In Proceedings of 
SIGGRAPH 1998,ACM Press / ACM SIG-GRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM. 
GOULD, J., AND GOULD,C. 1999. The Animal Mind.W.H. Freeman, New York, NY. GRAND, S., CLIFF, D., AND MALHOTRA,A. 
1996. Creatures: Arti.cial life autonomous agents for home entertainment. In Proceedings of the Autonomous 
Agents 97 Conference. GRZESZCZUK, R., AND TERZOPOULOS,D. 1995. Automated learning of muscle-actuated 
locomotion through control abstrac­tion. In Proceedings of SIGGRAPH 1995,ACM Press / ACM SIGGRAPH, Computer 
Graphics Proceedings, Annual Confer­ence Series, ACM. GRZESZCZUK, R., TERZOPOULOS, D., AND HINTON,G. 
1998. Neuroanimator: Fast neural network emulation and control of physics-based models. In Proceedings 
of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Computer Graphics Proceed­ings, Annual Conference Series, 
ACM. HODGINS, J., AND POLLARD,N. 1997. Adapting simulated be­haviors for new characters. In Proceedings 
of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, Computer Graphics Proceed­ings, Annual Conference Series, 
ACM. ISLA, D., BURKE, R., DOWNIE, M., AND BLUMBERG,B. 2001. A layered brain architecture for synthetic 
creatures. In Proceed­ings of The International Joint Conference on Arti.cial Intelli­gence. ISLA,D. 
2001. The Virtual Hippocampus: Spatial Common Sense for Synthetic Creatures. Master s thesis, The Media 
Lab, MIT. IVANOV,Y., BLUMBERG, B., AND PENTLAND,A. 2001. Expec­tation maximization for weakly labeled 
data. In Proceedings of the 18th International Conference on Machine Learning. IVANOV,Y. 2001. State 
Discovery for Autonomous Creatures. PhD thesis, The Media Lab, MIT. KAELBLING,L. 1990. Learning in embedded 
systems. PhD thesis, Stanford University. KAPLAN,F., OUDEYER,P.-Y., KUBINYI, E., AND MIKLOSI, A. 2001. 
Taming robots with clicker training : a solution for teaching complex behaviors. In Proceedings of the 
9th Euro­pean workshop on learning robots, LNAI, Springer, M. Quoy, P. Gaussier, and J. L. Wyatt, Eds. 
LINDSAY,S. 2000. Applied Dog Behavior and Training.Iowa State University Press, Ames, IA. LORENZ, K., 
AND LEYAHUSEN,P. 1973. Motivation of Human and Animal Behavior: An Ethological View.Van Nostrand Rein­hold 
Co., New York, NY. LORENZ,K. 1981. The Foundations of Ethology. Springer-Verlag, New York, NY. MITCHELL,K. 
1997. Machine Learning. McGraw Hill, New York, NY. PERLIN, K., AND GOLDBERG,A. 1996. Improv: A system 
for scripting interactive actors in virtual worlds. In Proceedings of SIGGRAPH 1996,ACM Press / ACM SIGGRAPH, 
Computer Graphics Proceedings, Annual Conference Series, ACM. PRYOR,K. 1999. Clicker Training for Dogs. 
Sunshine Books, Inc., Waltham, MA. RABINER, L., AND JUANG, B.-H. 1993. Fundamentals of Speech Recognition. 
Prentice Hall, Englewood Cliffs, NJ. RAMIREZ,K. 1999. Animal Training:Successful Animal Man­agement Through 
Positive Reinforcement. Shedd Aquarium, Chicago, IL. RESNER, B., STERN, A., AND FRANK,A. 1997. The truth 
about catz and dogz. In The Computer Games Developer Conference. REYNOLDS,C. 1987.Flocks,herdsandschools:Adistributedbe­havioral 
model. In Proceedings of SIGGRAPH 1987,ACM Press /ACM SIGGRAPH, Computer Graphics Proceedings, Annual 
Conference Series, ACM. ROSE,C., COHEN,M., ANDBODENHEIMER,B. 1999.Verbsand adverbs: Multidimensional 
motion interpolation. IEEE Com­puter Graphics And Applications 18,5. SHETTLEWORTH,S.J. 1998. Cognition, 
Evolution and Behavior. Oxford University Press, New York, NY. SUTTON, R., AND BARTO,A. 1998. Reinforcement 
Learning: An Introduction. MIT Press, Cambridge MA. SUTTON,R. 1991. Reinforcement learning architectures 
for an­imats. In The First International Conference on Simulation of Adaptive Behavior, MIT Press, Paris, 
Fr. THERRIEN,C. 1989. Decision Estimation and Classi.cation: An Introduction to Pattern Recognition and 
Related Topics. John Wiley and Sons, New York, NY. TOMLINSON, B., AND BLUMBERG,B. 2002. Alphawolf: So­cial 
learning, emotion and development in autonomous virtual agents. In First GSFC/JPL Workshop on Radical 
Agent Con­cepts. TU, X., AND TERZOPOULOS,D. 1994. Arti.cial .shes: Physics, locomotion, perception, behavior. 
In Proceedings of SIGGRAPH 1994,ACM Press / ACM SIGGRAPH, Computer Graphics Pro­ceedings, Annual Conference 
Series, ACM. VAN DE PANNE, M., AND FIUME., E. 1993. Sensor-actuator net­works. In Proceedings of SIGGRAPH 
1993,ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Confer­ence Series, ACM. VAN DE 
PANNE, M., KIM, R., AND FIUME., E. 1994. Synthe­sizing parameterized motions. In 5th Eurographics Workshop 
on Simulation and Animation. WATKINS,C. J., AND DAYAN,P. 1992. Q-learning. Machine Learning 8. WILKES,G. 
1995. Click and Treat Training Kit. Click and Treat Inc., Mesa, AZ. YOON, S., BLUMBERG, B., AND SCHNEIDER,G. 
2000. Motivation-driven learning for interactive synthetic characters. In Proceedings of the Fourth International 
Conference on Au­tonomous Agents. YOON, S., BURKE, R., AND BLUMBERG,B. 2000. Interactive training for 
synthetic characters. In Proceedings of AAAI 2000.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566598</section_id>
		<sort_key>427</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[3D acquisition and image based rendering]]></section_title>
		<section_page_from>427</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP40025406</person_id>
				<author_profile_id><![CDATA[81100260276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Zurich]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566599</article_id>
		<sort_key>427</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Image-based 3D photography using opacity hulls]]></title>
		<page_from>427</page_from>
		<page_to>437</page_to>
		<doi_number>10.1145/566570.566599</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566599</url>
		<abstract>
			<par><![CDATA[We have built a system for acquiring and displaying high quality graphical models of objects that are impossible to scan with traditional scanners. Our system can acquire highly specular and fuzzy materials, such as fur and feathers. The hardware set-up consists of a turntable, two plasma displays, an array of cameras, and a rotating array of directional lights. We use multi-background matting techniques to acquire alpha mattes of the object from multiple viewpoints. The alpha mattes are used to construct an <i>opacity hull.</i> The opacity hull is a new shape representation, defined as the visual hull of the object with view-dependent opacity. It enables visualization of complex object silhouettes and seamless blending of objects into new environments. Our system also supports relighting of objects with arbitrary appearance using surface reflectance fields, a purely image-based appearance representation. Our system is the first to acquire and render surface reflectance fields under varying illumination from arbitrary viewpoints. We have built three generations of digitizers with increasing sophistication. In this paper, we present our results from digitizing hundreds of models.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D photography]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027493</person_id>
				<author_profile_id><![CDATA[81100458116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matusik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14079289</person_id>
				<author_profile_id><![CDATA[81100199891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hanspeter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfister]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P338033</person_id>
				<author_profile_id><![CDATA[81100360585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Addy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ngan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P220909</person_id>
				<author_profile_id><![CDATA[81100489358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beardsley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18001699</person_id>
				<author_profile_id><![CDATA[81100444984]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Remo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ziegler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MERL, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14058906</person_id>
				<author_profile_id><![CDATA[81100137780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Leonard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BEARDSLEY, P. 2002. Calibration of stereo cameras for a turntable 3d scanner. Tech. rep., MERL. TR 2002/20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614503</ref_obj_id>
				<ref_obj_pid>614284</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BERNARDINI, F., MARTIN, I., AND RUSHMEIER, H. 2001. High-quality texture reconstruction from multiple scans. IEEE Trans. on Vis. and Comp. Graph. 7, 4 (Oct.-Dec.), 318-332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383309</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BUEHLER, C., BOSSE, M., MCMILLAN, L., GORTLER, S., AND COHEN, M. 2001. Unstructured lumigraph rendering. In Computer Graphics, SIGGRAPH 2001 Proceedings, 425-432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E., AND WILLIAMS, L. 1993. View interpolation for image synthesis. In Computer Graphics, SIGGRAPH 93 Proceedings, 279-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566601</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHEN, W.-C., GRZESZCZUK, R., AND BOUGUET, J.-Y. 2002. Light field mapping: Efficient representation and hardware rendering of surface light fields. In Computer Graphics, To appear in the SIGGRAPH 2002 Proceedings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., AND LEVOY, M. 1996. A volumetric method for building complex models from range images. In Computer Graphics, SIGGRAPH 96 Proceedings, 303-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., AND MALIK, J. 1997. Recovering high dynamic range radiance maps from photographs. In Computer Graphics, SIGGRAPH 97 Proceedings, 369-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., TAYLOR, C., AND MALIK, J. 1996. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. In Computer Graphics, SIGGRAPH 96 Proceedings, 11-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>893689</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., YU, Y., AND BORSHUKOV, G. 1998. Efficient view-dependent image-based rendering with projective texture-mapping. In Proceedings of the 9th Eurographics Workshop on Rendering, 105-116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND SAGAR, M. 2000. Acquiring the reflectance field of a human face. In Computer Graphics, SIGGRAPH 2000 Proceedings, 145-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>825150</ref_obj_id>
				<ref_obj_pid>519933</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GEORGHIADES, A., BELHUMEUR, P., AND KRIEGMAN, D. 1999. Illumination-based image synthesis: Creating novel images of human faces under differing pose and lighting. In IEEE Workshop on Multi-View Modeling and Analysis of Visual Scenes, 47-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S., GRZESZCZUK, R., SZELISKI, R., AND COHEN, M. 1996. The lumigraph. In Computer Graphics, SIGGRAPH 96 Proceedings, 43-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>585053</ref_obj_id>
				<ref_obj_pid>584993</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HAWKINS, T., COHEN, J., AND DEBEVEC, P. 2001. A photometric approach to digitizing cultural artifacts. In 2nd International Symposium on Virtual Reality, Archaeology, and Cultural Heritage.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KOUDELKA, M., BELHUMEUR, P., MAGDA, S., AND KRIEGMAN, D. 2001. Image-based modeling and rendering of surfaces with arbitrary brdfs. In Proc. of Computer Vision and Pattern Recognition, in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628563</ref_obj_id>
				<ref_obj_pid>628309</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LAURENTINI, A. 1994. The visual hull concept for silhouette-based image understanding. PAMI 16, 2 (February), 150-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364407</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LENGYEL, J., PRAUN, E., FINKELSTEIN, A., AND HOPPE, H. 2001. Real-time fur over arbitrary surfaces. In Symposium on Interactive 3D Graphics, 227-232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732303</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LENSCH, H., KAUTZ, J., GOESELE, M., HEIDRICH, W., AND SEIDEL, H.-P. 2001. Image-based reconstruction of spatially varying materials. In Proceedings of the 12th Eurographics Workshop on Rendering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND HANRAHAN, P. 1996. Light field rendering. In Computer Graphics, SIGGRAPH 96 Proceedings, 31-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND WHITTED, T. 1985. The use of points as display primitives. Tech. Rep. TR 85-022, The University of North Carolina at Chapel Hill, Department of Computer Science.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., PULLI, K., CURLESS, B., RUSINKIEWICZ, S., KOLLER, D., PEREIRA, L., GINZTON, M., ANDERSON, S., DAVIS, J., GINSBERG, J., SHADE, J., AND FULK, D. 2000. The digital michelangelo project: 3d scanning of large statues. In Computer Graphics, SIGGRAPH 2000 Proceedings, 131-144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T., GELB, D., AND WOLTERS, H. 2001. Polynomial texture maps. In Computer Graphics, SIGGRAPH 2001 Proceedings, 519-528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383829</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MARSCHNER, S., WESTIN, S., LAFORTUNE, E., TORRANCE, K., AND GREENBERG, D. 1999. Image-based brdf measurement including human skin. In Proceedings of the 10th Eurographics Workshop on Rendering, 139-152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MATUSIK, W., BUEHLER, C., RASKAR, R., GORTLER, S., AND MCMILLAN, L. 2000. Image-based visual hulls. In Computer Graphics, SIGGRAPH 2000 Proceedings, 369-374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MCMILLAN, L., AND BISHOP, G. 1995. Plenoptic modeling: An image-based rendering system. In Computer Graphics, SIGGRAPH 95 Proceedings, 39-46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[MILLER, G., RUBIN, S., AND PONCELEON, D. 1998. Lazy decompression of surface light fields for precomputed global illumination. In Proceedings of the 9th Eurographics Workshop on Rendering, 281-292.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[NISHINO, K., SATO, Y., AND IKEUCHI, K. 1999. Appearance compression and synthesis based on 3d model for mixed reality. In Proceedings of IEEE ICCV '99, 38-45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[NISHINO, K., SATO, Y., AND IKEUCHI, K. 1999. Eigen-texture method: Appearance compression based on 3d model. In Proc. of Computer Vision and Pattern Recognition, 618-624.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344936</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PFISTER, H., ZWICKER, M., VAN BAAR, J., AND GROSS, M. 2000. Surfels: Surface elements as rendering primitives. In Computer Graphics, SIGGRAPH 2000 Proceedings, 335-342.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732115</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PULLI, K., COHEN, M., DUCHAMP, T., HOPPE, H., SHAPIRO, L., AND STUETZLE, W. 1997. View-based rendering: Visualizing real objects from scanned range and color data. In Eurographics Rendering Workshop 1997, 23-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND HANRAHAN, P. 2001. A signal-processing framework for inverse rendering. In Computer Graphics, SIGGRAPH 2001 Proceedings, 117-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[RUSHMEIER, H., BERNARDINI, F., MITTLEMAN, J., AND TAUBIN, G. 1998. Acquiring input for rendering at appropriate levels of detail: Digitizing a piet&#224;. In Proceedings of the 9th Eurographics Workshop on Rendering, 81-92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[RUSINKIEWICZ, S., AND LEVOY, M. 2000. Qsplat: A multiresolution point rendering system for large meshes. In Computer Graphics, SIGGRAPH 2000 Proceedings, 343-352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344935</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[SANDER, P., GU, X., GORTLER, S., HOPPE, H., AND SNYDER, J. 2000. Silhouette clipping. In Computer Graphics, SIGGRAPH 2000 Proceedings, 327-334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[SATO, Y., WHEELER, M. D., AND IKEUCHI, K. 1997. Object shape and reflectance modeling from observation. In Computer Graphics, SIGGRAPH 97 Proceedings, 379-387.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237263</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R., AND BLINN, J. F. 1996. Blue screen matting. In Computer Graphics, vol. 30 of SIGGRAPH 96 Proceedings, 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[TURK, G., AND LEVOY, M. 1994. Zippered polygon meshes from range images. In Computer Graphics, SIGGRAPH 94 Proceedings, 311-318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617876</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[UDUPA, J., AND ODHNER, D. 1993. Shell rendering. IEEE Computer Graphics & Applications 13, 6 (Nov.), 58-67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344925</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[WOOD, D., AZUMA, D., ALDINGER, K., CURLESS, B., DUCHAMP, T., SALESIN, D., AND STUETZLE, W. 2000. Surface light fields for 3d photography. In Computer Graphics, SIGGRAPH 2000 Proceedings, 287-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311559</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[YU, Y., DEBEVEC, P., MALIK, J., AND HAWKINS, T. 1999. Inverse global illumination: Recovering reflectance models of real scenes from photographs. In Computer Graphics, SIGGRAPH 99 Proceedings, 215-224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[ZONGKER, D., WERNER, D., CURLESS, B., AND SALESIN, D. 1999. Environment matting and compositing. In Computer Graphics, SIGGRAPH 99 Proceedings, 205-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383300</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[ZWICKER, M., PFISTER., H., BAAR, J. V., AND GROSS, M. 2001. Surface splatting. In Computer Graphics, SIGGRAPH 2001 Proceedings, 371-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image-Based 3D Photography using Opacity Hulls Wojciech Matusik * Hanspeter P.ster Addy Ngan* Paul 
Beardsley Remo Ziegler Leonard McMillan* Figure 1: Renditions of acquired objects with a mixture of 
highly specular and fuzzy materials. Abstract We have built a system for acquiring and displaying high 
quality graphical models of objects that are impossible to scan with tradi­tional scanners. Our system 
can acquire highly specular and fuzzy materials, such as fur and feathers. The hardware set-up consists 
of a turntable, two plasma displays, an array of cameras, and a ro­tating array of directional lights. 
We use multi-background matting techniques to acquire alpha mattes of the object from multiple view­points. 
The alpha mattes are used to construct an opacity hull.The opacity hull is a new shape representation, 
de.ned as the visual hull of the object with view-dependent opacity. It enables visualization of complex 
object silhouettes and seamless blending of objects into new environments. Our system also supports relighting 
of objects with arbitrary appearance using surface re.ectance .elds, a purely image-based appearance 
representation. Our system is the .rst to acquire and render surface re.ectance .elds under varying illumi­nation 
from arbitrary viewpoints. We have built three generations of digitizers with increasing sophistication. 
In this paper, we present our results from digitizing hundreds of models. CR Categories: I.3.2 [Computer 
Graphics]: Picture/Image Generation Digitizing and Scanning, Viewing Algorithms; Keywords: Image-based 
rendering, 3D photography. *MIT, Cambridge, MA. Email: [wojciech,addy,mcmillan]@graphics.lcs.mit.edu 
MERL, Cambridge, MA. Email: [p.ster,beardsley,ziegler]@merl.com Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 1 Introduction Creating 3D 
models manually is time consuming and creates a bot­tleneck for many practical applications. It is both 
dif.cult to model complex shapes and to recreate complex object appearance using standard parametric 
re.ectance models. Not surprisingly, tech­niques to create 3D models automatically by scanning real objects 
have greatly increased in signi.cance. An ideal system would ac­quire an object automatically and construct 
a detailed shape and appearance model suf.cient to place the synthetic object in an arbi­trary environment 
with new illumination. Although there has been much recent work towards this goal, no system to date 
ful.lls all these requirements. Most current acquisi­tion systems require substantial manual involvement. 
Many meth­ods, including most commercial systems, focus on capturing accu­rate shape, but neglect accurate 
appearance capture. Even when the re.ectance properties of 3D objects are captured they are .tted to 
parametric BRDF models. This approach fails to represent com­plex anisotropic BRDFs and does not model 
important effects such as inter-re.ections, self-shadowing, translucency, and subsurface scattering. 
There have also been a number of image-based tech­niques to acquire and represent complex objects. But 
all of them have some limitations, such as lack of a 3D model, static illumina­tion, or rendering from 
few viewpoints. We have developed an image-based 3D photography system that comes substantially closer 
to the ideal system outlined above. It is very robust and capable of fully capturing 3D objects that 
are dif.­cult if not impossible to scan with existing scanners (see Figure 1). It automatically creates 
object representations that produce high quality renderings from arbitrary viewpoints, either under .xed 
or novel illumination. The system is built from off-the-shelf compo­nents. It uses digital cameras, leveraging 
their rapid increase in quality and decrease in cost. It is easy to use, has simple set-up and calibration, 
and scans objects that .t within a one cubic foot volume. The acquired objects can be accurately composited 
into synthetic scenes. After a review of previous work, we give an overview of our system in Section 
3. In Section 4 we present the opacity hull,a new shape representation especially suited for objects 
with com­plex small-scale geometry. Section 5 describes surface re.ectance .elds, an appearance representation 
that allows us to render objects with arbitrary re.ectance properties under new illumination. Sec­tion 
6 describes our novel data structure that parameterizes surface light .elds and surface re.ectance .elds 
onto point-sampled opac­ity hulls. In Section 7 we show how to interpolate surface light .elds and surface 
re.ectance .elds to generate views from arbi­trary positions. Section 8 presents results, including quantitative 
evaluations. 2Previous Work There are many approaches for acquiring high quality 3D shape from real-world 
objects, including contact digitizers, passive stereo depth-extraction, and active light imaging systems. 
Passive digi­tizers are not robust in cases where the object being digitized does not have suf.cient 
texture. Nearly all passive methods assume that the BRDF is Lambertian or does not vary across the surface. 
They often fail in the presence of subsurface scattering, inter-re.ections, or surface self-shadowing. 
Active light systems, such as laser range scanners, are very pop­ular and have been employed to acquire 
large models [Levoy et al. 2000; Rushmeier et al. 1998]. All active light systems place restric­tions 
on the types of materials that can be scanned, as discussed in detail in [Hawkins et al. 2001]. They 
also require a registration step to align separately acquired scanned meshes [Turk and Levoy 1994; Curless 
and Levoy 1996] or to align the scanned geometry with separately acquired texture images [Bernardini 
et al. 2001]. Filling gaps due to missing data is often necessary as well. Systems have been constructed 
where multiple lasers are used to acquire a surface color estimate along the line of sight of the imaging 
sys­tem. However, this is not useful for capturing objects in realistic illumination environments. To 
acquire objects with arbitrary materials we use an image­based modeling and rendering approach. Image-based 
represen­tations have the advantage of capturing and representing an object regardless of the complexity 
of its geometry and appearance. Early image-based methods [McMillan and Bishop 1995; Chen and Williams 
1993] allowed for navigation within a scene using correspondence information. Light .eld methods [Levoy 
and Han­rahan 1996; Gortler et al. 1996] achieve similar results without geometric information, but with 
an increased number of images. Gortler et al. [1996] combine the best of these methods by includ­ing 
a visual hull of the object for improved ray interpolation. These methods assume static illumination 
and therefore cannot accurately render objects into new environments. An intermediate between purely 
model-based and purely image­based methods is the view-dependent texture mapping systems de­scribed by 
Pulli et al. [1997] and Debevec et al. [1998; 1996]. These systems combine simple geometry and sparse 
texture data to accurately interpolate between the images. These methods are extremely effective despite 
their approximate 3D shapes, but they have some limitations for highly specular surfaces due to the rela­tively 
small number of textures. As noted in [Debevec et al. 1998], surface light .elds [Miller et al. 1998; 
Wood et al. 2000; Nishino et al. 1999a; Nishino et al. 1999b; Chen et al. 2002] can be viewed as a more 
general and more ef.cient representation of view-dependent texture maps. Wood et al. [2000] store light 
.eld data on accurate high-density geometry, whereas Nishino et al. [1999a] use a coarser triangular 
mesh for ob­jects with low geometric complexity. Chen at al. [2002] are using a decomposition of surface 
light .elds that can be ef.ciently ren­dered on modern graphics hardware. Surface light .elds are capa­ble 
of reproducing important global effects such as inter-re.ections and self-shadowing. Our system is capable 
of surface light .eld acquisition and rendering. Images generated from a surface light .eld always show 
the ob­ject under a .xed lighting condition. To overcome this limitation, inverse rendering methods estimate 
the surface BRDF from images and geometry of the object. To achieve a compact BRDF represen­tation, most 
methods .t a parametric re.ection model to the image data [Sato et al. 1997; Yu et al. 1999; Lensch et 
al. 2001]. Sato et al.[1997] and Yu et al. [1999] assume that the specular part of the BRDF is constant 
over large regions of the object, while the diffuse component varies more rapidly. Lensch et al. [2001] 
partition the objects into patches and estimate a set of basis BRDFs per patch. Simple parametric BRDFs, 
however, are incapable of represent­ing the wide range of effects seen in real scenes. As observed in 
[Hawkins et al. 2001], objects featuring glass, fur, hair, cloth, leaves, or feathers are very challenging 
or impossible to represent this way. As we will show in Section 8, re.ectance functions for points in 
highly specular or self-shadowed areas are very complex and cannot easily be approximated using smooth 
basis functions. In our work we make no assumptions about the re.ection property of the material we are 
scanning. An alternative is to use image-based, non-parametric represen­tations for object re.ectance. 
Marschner et al. [1999] use a tabu­lar BRDF representation and measure the re.ectance properties of convex 
objects using a digital camera. Their method is restricted to objects with a uniform BRDF, and they incur 
problems with ge­ometric errors introduced by 3D range scanners. Georghiades et al. [1999] apply image-based 
relighting to human faces by assum­ing that the surface re.ectance is Lambertian. More recent approaches 
[Malzbender et al. 2001; Debevec et al. 2000; Hawkins et al. 2001; Koudelka et al. 2001] use image databases 
to relight objects from a .xed viewpoint without acquir­ing a full BRDF. Debevec et al. [2000] de.ne 
the re.ectance .eld of an object as the radiant light from a surface under every pos­sible incident .eld 
of illumination. They use a light stage with few .xed camera positions and a rotating light to acquire 
the re­.ectance .eld of a human face [Debevec et al. 2000] or of cultural artifacts [Hawkins et al. 2001]. 
The polynomial texture map system described in [Malzbender et al. 2001] uses a similar technique for 
objects with approximately planar geometry and diffuse re.ectance properties. Koudelka et al. [2001] 
use essentially the same method as [Debevec et al. 2000] to render objects with arbitrary appearance. 
These re.ectance .eld approaches are limited to renderings from a single viewpoint. 3 System Overview 
 3.1 Modeling Approach Our system uses a variant of the image-based visual hull (IBVH) [Matusik et al. 
2000] as the underlying geometric model. The IBVH can be computed robustly using active backlighting. 
We augment the IBVH with view-dependent opacity to accurately rep­resent complex silhouette geometry, 
such as hair. We call this new shape representation the opacity hull. To construct the opacity hull we 
use the multi-background matting techniques similar to Smith et al. [1996]. Our system can acquire a 
surface light .eld of the object. It can also acquire re.ectance .elds of the object from multiple view­points. 
We call this representation a surface re.ectance .eld, be­cause the data is parameterized on the surface 
of the visual hull of the object. Surface re.ectance .elds can be rendered from any viewpoint under new 
illumination. We use images from the same viewpoints to compute the opacity hull and the surface re.ectance 
.eld. This avoids any registration inaccuracies and has proven to be extremely robust. Laurentini [1994] 
introduced the visual hull as the maximal vol­ume that is consistent with a given set of silhouettes. 
The visual hull cannot represent surface concavities. Yet, due to its hull property, it provides a conservative 
estimate of an object s structure. The opacity hull and surface re.ectance .eld extend the utility of 
vi­sual hull considerably by faithfully representing complex silhou­ettes and materials. Instead of 
relying on accurate geometry, our representation re­lies heavily upon acquired radiance information to 
produce accu­rate renderings of the object. We can adaptively acquire more im­ages for objects with concavities 
or high specularity, and fewer im­ages for objects with simple geometry and mostly diffuse surfaces. 
Naturally, this approach is not useful for applications where geo­metric .delity is required. In this 
paper we demonstrate that the combination of opacity hull geometry and the image-based surface re.ectance 
.eld leads to an effective representation for rendering applications. Our system is capable of acquiring 
and rendering ob­jects that are fuzzy, highly specular, or that contain any mixture of materials. 3.2 
Hardware Set-Up Figure 2 shows an overview of our hardware set-up. Objects are placed on a plasma monitor 
that is mounted onto a rotating turntable. An array of light sources is mounted on an overhead turntable. 
The lights are spaced roughly equally along the elevation angle of the hemisphere. During object scanning, 
the lights can be    Rotating Platform Figure 2: Our 3D digitizing system combines both active and 
pas­sive imaging methods. Objects are rotated on a turntable while im­ages are acquired. Plasma monitors 
are used to extract high quality alpha mattes. An overhead array of light sources can be rotated to acquire 
surface re.ectance .elds. .xed, rotate around the object for a .xed point of view, or made to rotate 
with the object. Six video cameras are pointed at the ob­ject from various angles. To facilitate consistent 
back lighting we mount the cameras roughly in the same vertical plane. A second plasma monitor is placed 
directly opposite of the cameras. Figure 3 shows a picture of our third-generation scanner. The two plasma 
monitors have a resolution of 1024 × 768 pixels. We currently use six QImaging QICAM cameras with 1360 
× 1036 pixel color CCD imaging sensors. The cameras are photometrically calibrated. They are connected 
via FireWire to a 2 GHz Pentium­4PCwith1GBofRAM.Wealternativelyuse 15 mm or 8mm Figure 3: Photograph 
of our digitizing system. C-mount lenses, depending on the size of the acquired object. The cameras are 
able to acquire full resolution RGB images at 11 frames per second. The light array holds four to six 
directional light sources. Each light uses a 32 Watt HMI Halogen lamp and a parabolic re.ector to approximate 
a directional light source at in.nity. The lights are controlled by an electronic switch and individual 
dimmers. The dimmers are set once such that the image sensor is not oversaturated for viewpoints where 
the lights are directly visible. In many ways, our set-up is similar to the enhanced light stage that 
has been proposed as future work in [Hawkins et al. 2001]. A key difference is that our system uses multicolor 
backlights for alpha matte extraction and construction of the opacity hull. As we will show, the availability 
of approximate geometry and view­dependent alpha greatly extends the class of models that can be captured. 
 3.3 Data Acquisition Process Calibration: The scanning sequence starts by placing the object onto the 
turntable and, if necessary, adjusting the position and aper­ture of the cameras. If any camera adjustments 
are required, we must .rst acquire images of a known calibration object, a patterned cube in our case. 
An image of the calibration target is taken from each of the viewpoints. Intrinsic and extrinsic camera 
parameters are computed using a special calibration procedure for turntable systems with multiple cameras 
[Beardsley 2002]. Calibration can be computed reliably given the .xed rotation axis and the large numbers 
of images. Reference images: Next, the plasma monitors are turned on and we acquire images of patterned 
backdrops used for multi­background matting. For each viewpoint, each patterned back­drop is photographed 
alone without the foreground object. As in [Zongker et al. 1999], we call these images the reference 
images. Reference images only have to be acquired once after calibration. They are stored and used for 
subsequent object scans. Object images: The object is then put on the turntable and a sequence of images 
is automatically acquired. The number of turntable positions is user speci.ed and depends on the object 
(see Section 8). During this .rst rotation, both plasma monitors illu­minate the object from below and 
behind with the patterned back­drops. As in [Zongker et al. 1999], we call the images of the fore­ground 
object in front of the backdrops object images. The object images and reference images are used to compute 
alpha mattes and the opacity hull as described in Section 4. We depend on good re­peatability of the 
turntables to ensure that the reference images and the object images are well registered. Radiance images: 
We then switch off the plasma monitors and turn on one or more directional lights of the array. We found 
that we get best results when using additional .ll light to avoid dark shadows and high contrast in the 
images. We avoid specular re.ec­tions from the monitors by covering the vast majority of the display 
surface with black felt without upsetting the object position. We acquire a set of radiance images of 
the illuminated object during the second rotation of the turntable. The radiance images are used for 
surface light .eld rendering. The directional lights can be .xed or made to rotate with the object. The 
coupled rotation case leads to greater coherence of radiance samples in each surface point. Re.ectance 
images: If we want to relight the acquired object, we acquire an additional set of images used to construct 
the surface re.ectance .eld. The array of lights is rotated around the object. For each rotation position, 
each light in the light array is sequen­tially turned on and an image is captured with each camera. We 
use four lights and typically increment the rotation angle by 24 . for a total of 4 × 15 images for each 
camera position. This procedure is repeated for all viewpoints. We call the set of all images the re­.ectance 
images. They are used to construct the surface re.ectance .eld as described in Section 5. HDR images: 
All radiance and re.ectance images are captured using a high dynamic range technique similar to that 
of Debevec et al. [1997]. Since raw output from the CCD array of the cameras is available, the relationship 
between exposure time and radiance val­ues is linear over most of the operating range. For each viewpoint, 
we take four pictures with exponentially increasing exposure times and use a least squares linear .t 
to determine the response line. Our imager has 10 bits of precision. Due to non-linear saturation effects 
at the extreme ends of the scale we only use values in the range of 5 to 1000 in our least squares computation. 
We can ignore the DC offset of this calculation, which was small for our cameras1,and store only the 
slope of the response line as one .oating point num­ber per pixel. This image representation allows for 
the speci.cation of a desired exposure interval at viewing time. The next section describes our procedure 
to compute alpha mat­tes and how we use them to compute the opacity hull of the object. 4 The Opacity 
Hull 4.1 Acquiring Alpha Mattes To construct the image-based visual hull on which we parameterize the 
opacity hull, we extract silhouette images from various view­points. Earlier versions of our system use 
.uorescent lights to ac­quire silhouette views. Backlighting is a common segmentation ap­proach that 
is often used in commercial two-dimensional machine vision systems. The backlights saturate the image 
sensor in areas where they are visible. We then threshold the silhouette images to establish a binary 
segmentation for the object. However, binary thresholding is not accurate enough for objects with small 
silhouette features, such as hair. It also does not per­mit sub-pixel accurate compositing of the objects 
into new environ­ 1DC offsets are due to thermal and .xed pattern noise of the imager. ments. An additional 
problem is color spill [Smith and Blinn 1996], the re.ection of backlight on the foreground object. Spill 
typically happens near object silhouettes because the Fresnel effect increases the specularity of materials 
near grazing angles. With a single color active backlight, spill is particularly prominent for highly 
specular surfaces, such as metal or ceramics. We use a variant of the multi-background matting technique 
of Smith et al. [1996] to solve these problems. We acquire alpha mat­tes of the object from each viewpoint. 
An alpha matte of a fore­ground object can be extracted by imaging the object against two background 
images with different colors. We display the following sinusoidal background patterns on the plasma monitors: 
2p(x + y) p Ci(x, y, n)=(1 + nsin(+ i )) × 127. (1) . 3 Ci(x, y, n) is the intensity of color channel 
i = 0,1, 2 at pixel location (x,y). To maximize the per-pixel difference between the two back­drops, 
the patterns are phase shifted by 180. (n = -1or1). The user de.nes the period of the sinusoidal stripes 
with the parameter . . Using the multi-background matting equation from [Smith and Blinn 1996], the per-pixel 
object alpha ao is computed using sum­mation over all color channels as: .i=r,g,b(On - On¯)(Rn - Rn¯) 
ao = 1 - , (2) .i=r,g,b(Rn - Rn¯)2 where Rn and Rn¯are per-pixel background colors of the reference images, 
and On and On¯are per-pixel foreground colors of the object images for n = ±1, respectively. If we measure 
the same color at a pixel both with and without the object for each background, Equation (2) equals zero. 
This cor­responds to a pixel that maps straight through from the background to the sensor. The phase 
shifts in the color channels of Equation (1) assures that the denominator of Equation (2) is never zero. 
The si­nusoidal pattern reduces the chance that a pixel color observed due to spill matches the pixel 
color of the reference image. Neverthe­less, we still observed spill errors for highly specular objects, 
such as the teapot or the bonsai pot. To reduce these errors we apply the same procedure multiple times, 
each time varying the wavelength . of the backdrop pat­terns. For the .nal alpha matte we store the maximum 
alpha from all intermediate mattes. We found that acquiring three intermedi­ate alpha mattes with relatively 
prime periods . = 27,40 and 53 is suf.cient. The overhead of taking the additional images is small, and 
we need to store only the .nal alpha matte. Figure 4 shows two alpha mattes acquired with our method. 
We found that in practice Figure 4: Alpha mattes acquired using our backdrops. this method works very 
well for a wide variety of objects, including specular and fuzzy materials.  4.2 Opacity Hull Construction 
Using the alpha mattes of the object from various viewpoints, we construct the opacity hull. First, we 
use binary thresholding on the alpha mattes to get binary silhouette images. Theoretically, each pixel 
with a > 0 (i.e., not transparent) belongs to the foreground object. We use a slightly higher threshold 
because of noise in the system and calibration inaccuracies. We found that a threshold of a > 0.05 yields 
a segmentation that covers all of the object and parts of the background. The binary silhouettes are 
then used to construct the image-based visual hull (IBVH) [Matusik et al. 2000]. The IBVH algorithm can 
be counted on to remove improperly classi.ed foreground regions as long as they are not consistent with 
all other images. We re­sample the IBVH into a dense set of surface points as described in Section 6. 
Each point on the visual hull surface is projected onto the alpha mattes to determine its opacity from 
a particular observed viewpoint. The opacity hull is similar to a surface light .eld, but instead of 
storing radiance it stores opacity values in each surface point. It is useful to introduce the notion 
of an alphasphere A .If . is an outgoing direction at the surface point p,then A (p, .) is the opacity 
value seen along direction .. Figure 5 shows the observed alpha values for three surface points on an 
object for all 6 × 36 viewpoints. Each pixel has been colored BC  Figure 5: Observed alpha values for 
points on the opacity hull. Red color indicates invisible camera views. according to its opacity. Black 
color corresponds to a =0, white color corresponds to a =1, and grey color corresponds to values in between. 
Red color indicates camera views that are invisible from the surface point. The function A is de.ned 
over the entire direction sphere. Any physical scanning system acquires only a sparse set of samples 
of this function. As is done for radiance samples of lumispheres in [Wood et al. 2000], one could estimate 
a parametric function for A and store it in each alphasphere. However, as shown in Figure 5, the view-dependent 
alpha is not smooth and not easily amenable to parametric function .tting. Consequently, we store the 
acquired al­pha mattes and interpolate between them to render the opacity hull from arbitrary viewpoints 
(see Section 7). It is important to keep in mind that the opacity hull is a view­dependent representation. 
It captures view-dependent partial occu­pancy of a foreground object with respect to the background. 
The view-dependent aspect sets the opacity hull apart from voxel shells, which are frequently used in 
volume graphics [Udupa and Odhner 1993]. Voxel shells are not able to accurately represent .ne silhou­ette 
features, which is the main bene.t of the opacity hull. Recognizing the importance of silhouettes, Sander 
et al. [2000] use silhouette clipping to improve the visual appearance of coarse polygonal models. However, 
their method depends on accurate ge­ometric silhouettes, which is impractical for complex silhouette 
ge­ometry like fur, trees, or feathers. Opacity hulls are somewhat sim­ilar to the concentric, semi-transparent 
textured shells that Lengyel et al. [2001] used to render hair and furry objects. They use ge­ometry 
 called textured .ns to improve the appearance of object silhouettes. A single instance of the .n texture 
is used on all edges of the object. In contrast, opacity hulls can be looked at as textures with view-dependent 
alphas for every surface point of the object. They accurately render silhouettes of high complexity using 
only visual hull geometry.  5 Surface Re.ectance Fields Similar to constructing the opacity hull, we 
re-parameterize the ac­quired radiance images into rays emitted from surface points on the visual hull. 
This representation is a surface light .eld as described by Miller [Miller et al. 1998] and Wood [Wood 
et al. 2000]. How­ever, our surface light .elds are created on the surface of the visual hull rather 
than on the surface of the object. Surface light .elds can only represent models under the origi­nal 
illumination. To address this limitation we acquire surface re­.ectance .elds from multiple viewing positions 
around the object. Debevec et al. [2000] de.ne the re.ectance .eld under directional illumination as 
a six-dimensional function R(P,.i,.r). For each surface point P, it maps incoming light directions .i 
to re.ected color values along direction .r. Thus, for each point P we have a four-dimensional function 
RP(.i,.r). During acquisition, we sample the four dimensional function RP(.i,.r) from a set of viewpoints 
.r and a set of light direc­tions .i. In previous re.ectance .eld approaches [Debevec et al. 2000; Hawkins 
et al. 2001; Koudelka et al. 2001], the sampling of light directions is dense (e.g., |.i| =64 × 32 in 
[Debevec et al. 2000]), but only a single viewpoint is used. In our system, we sam­ple the re.ectance 
.eld from many directions (|.r| =6 × 36). To limit the amount of data we acquire and store, our system 
uses a sparse sampling of light directions (|.i| =4 × 15). Thus, our illu­mination environment has to 
be .ltered down substantially, and our re-illumination is accurate only for relatively diffuse surfaces 
[Ra­mamoorthi and Hanrahan 2001]. Reconstruction of an image from a new viewing direction un­der a new 
lighting con.guration is a two-pass process. First, we reconstruct the images from the original viewpoints 
under novel illumination. Once we have computed these images, we interpo­late the image data to new viewpoints 
as described in Section 7. For a particular image from the original viewpoint, it is useful to de.ne 
a slice of the re.ectance .eld called a re.ectance function Rxy(.i)[Debevec et al. 2000]. It represents 
how much light is re­.ected toward the camera by pixel (x,y)as a result of illumination from direction 
.i. We can reconstruct the image L(x, y)from the original viewpoint under novel illumination as a weighted 
linear combination of the light sources L(.i)as follows: L(x, y)=.Rxy(.i)L(.i)dA(.i), (3) .i where dA(.i) 
is the solid angle covered by each of the original illumination directions.  6 Point-Sampled Data Structure 
We use an extended point representation based on the layered depth cube (LDC) tree [P.ster et al. 2000] 
as our shape model on which we parameterize the view-dependent appearance data. In a pre­process, we 
compute the octree-based LDC tree from the IBVH. The creation of the LDC tree starts with the sampling 
of the visual hull from three orthogonal directions. The sampling density de­pends on the model complexity 
and is user speci.ed. The layered depth images are then merged into a single octree model. Since our 
visual hulls are generated from virtual orthographic viewpoints, their registration is exact. This merging 
also insures that the model is uniformly sampled. Point samples have several bene.ts for 3D scanning 
applications. From a modeling point of view, the point-cloud representation elim­inates the need to establish 
topology or connectivity. This facilitates the fusion of data from multiple sources, as pointed out by 
[Levoy and Whitted 1985]. They also avoid the dif.cult task of computing a consistent parameterization 
of the surface for texture mapping. We found that point models are able to represent complex organic 
shapes, such as a bonsai tree or a feather, more easily than polyg­onal meshes. In particular, it would 
be hard to represent the view­dependent opacity values at each point of the opacity hull using polygonal 
models and texture mapping. Each surfel (surface element) in the LDC tree stores depth, nor­mal, and 
a camera-visibility bit vector. The visibility vector stores a value of one for each camera position 
from which the surfel was visible. It can be quickly computed during IBVH construction us­ing the visibility 
algorithm described in [Matusik et al. 2000]. Our representation stores all of the acquired radiance 
and re.ectance im­ages with irrelevant information removed. This is accomplished by dividing each source 
image into 8 by 8 blocks and removing those blocks that lie outside the object s silhouette. For each 
image, we compute a simple mask by back-projecting all surfels from which this view is visible. Only 
the 8 by 8 pixel blocks that contain at least one back-projected surfel are stored. This simple scheme 
typically reduces the total amount of image data by a factor of .ve to ten, depending on the geometry 
of the model. A relightable model requires more than 20 GB of raw image data. In order to make this data 
more manageable, we have im­plemented a simple compression scheme for re.ectance images. For each original 
viewpoint, we apply principal component analy­sis (PCA) to corresponding 8 by 8 image blocks across the 
varying 4 ×15 illumination directions taken from a common viewpoint. We set a global threshold for the 
RMS reconstruction error and store a variable number of principal components per block. As shown in Section 
8, the average number of components per block is typi­cally four to .ve. PCA compression typically reduces 
the amount of re.ectance data by a factor of 10. Figure 6 shows a depiction of our data structure for 
surface re­.ectance .elds, simpli.ed for clarity. The .gure shows the .rst six PCA images for two original 
views. These images are com­bined into new radiance images from the same viewpoints under new illumination 
using the method described in Section 5. During rendering, points on the opacity hull of the object are 
projected into the radiance images based on their visibility. Each surfel s color is determined using 
interpolation among the four closest views. Note that the .gure shows the two closest views. 7 Rendering 
To render our point-sampled models we use the elliptical weighted average (EWA) surface splatting approach 
of [Zwicker et al. 2001]. First, the opacity and color of each surfel is interpolated from the ra­diance 
images as discussed below. A hierarchical forward-warping algorithm projects the surfels onto the screen. 
A screen space EWA PCA Images PCA Images 0 3 0 3 for View 1 for View 2 1 4 1 4  Relighted Model 
 2  52 5 New View ... ...  Relighted View 2 Relighted View 1 Figure 6: Data structure for surface 
re.ectance .elds. .lter reconstructs the image using the opacity, color, and normal stored per surfel. 
A modi.ed A-buffer provides order-independent alpha blending and edge anti-aliasing. To compute the radiance 
data for novel illumination, we .rst compute new images from the original re.ectance .eld data us­ing 
linear combinations as explained in Section 5. For each 8 by 8 pixel block, we compute the linear combination 
directly on the co­ef.cients of the PCA basis. Once we have a new set of coef.cients, we can easily reconstruct 
the new radiance images from the princi­pal components. This computation is performed for each change 
of the light con.guration. To interpolate the radiance images of the original viewpoints to arbitrary 
viewpoints, we use the unstructured lumigraph interpo­lation of Buehler et al. [2001]. For each surfel, 
we use k-nearest neighbor interpolation to reconstruct view-dependent alpha and ra­diance values. This 
assures continuous transitions between camera views. For each frame, we compute the normalized direction 
rc(i)from each surfel position to each visible camera i using the visibility bit vector and a global 
array of camera positions. We also compute the normalized viewing direction rv from the surfel position 
to the center of projection of the current view. We then assign a penalty p(i)=1 - cos.i to each visible 
camera, where cos .i =rc · rv.We consider only the k =4 cameras with smallest penalty p(i)when in­terpolating 
a value. All other cameras are assigned an interpolation weight w(i)of zero. We take care that a particular 
camera s weight falls to zero as it leaves the set of the closest four cameras. We accomplish this by 
de.ning an adaptive threshold cos .t =r4 · rv, where r4 is the direction of the surfel to the fourth 
closest camera. The blending weight w(i)for each camera is: cos .i - cos .t w(i)= (4) 1 - cos.t This 
weight function has its maximum value of one for cos.i =1, and it falls off to zero at cos.i =cos.t . 
To ensure epipole con­sistency, we multiply w(i)by 1/p(i). This ensures that rendering the object from 
original camera viewpoints reproduces exactly the original images. We also normalize all w(i)so that 
they sum up to one. 8Results We have collected a wide range of objects and surface types with our system. 
We have acquired many dif.cult surfaces including those of various genuses, with concavities, and with 
.ne scale fea­tures. We have also captured a wide range of materials, including fuzzy and highly specular 
materials. A variety of different models are shown in Figure 1 and Figure 13. Figure 14 shows a model 
un­der new illumination. Figure 7 shows several scanned objects com­posited into real environments. We 
acquired spherical light probe images [Debevec and Malik 1997] at the respective locations to cap­ture 
the illumination. All objects shown in this paper are rendered from novel viewpoints that are not part 
of the acquired image se­quence.  For all objects, we use six cameras and 36 turntable positions. We 
acquire six object images for alpha matting from each view­point (over three . values with n =±1). All 
radiance and re­.ectance images are acquired in high dynamic range by capturing four frames. For surface 
light .elds, we capture one radiance im­age from each viewpoint for a total of 6 × 36 × (4 × 1 +6)=2160 
images. For surface re.ectance .elds, we acquire re.ectance im­ages using 4 × 15 light directions from 
each viewpoint for a total of 6 × 36 × (4 × (4 × 15))+6)=53136 images. The entire dig­itizing process 
takes about one hour for a surface light .eld and about 14 hours for a surface re.ectance .eld. The whole 
process is fully automated without any user intervention. All of our models are created from a single 
scan. We resampled all of our visual hull models to 512 × 512 resolu­tion of the LDC tree. The processing 
time to segment the images, compute the opacity hull, and build the point-based data structure is less 
than 10 minutes. The PCA analysis of the surface re.ectance .eld takes about 36 hours on a single PC 
using non-optimized Mat­lab code. To speed up the PCA computation, we are using multiple PCs to process 
different image sets in parallel. In the process of acquiring models, we have made many inter­esting 
measurements and observations. Figure 8 shows plots of the measured re.ectance .eld data for three surface 
points on an object. We chose the surfels to be in specular and self-shadowed areas of the object. The 
dark parts of the plots are attributable to self shad-owing. The data lacks any characteristics that 
would make it a good .t to standard parametric BRDF models or function approximation techniques. This 
is typical for the data we observed.  Figure 9 shows a visualization of the number of PCA compo­nents 
per 8 by 8 pixel block of the re.ectance images from an orig­inal viewpoint. We set the global RMS reconstruction 
error to be  Figure 9: a) Original view. b) Visualization of number of PCA components per block (Max. 
= 15, Mean = 5). within 1% of the average radiance values of all HDR re.ectance im­ages. Note that areas 
with high texture frequency require more com­ponents than areas of similar average color. The maximum 
number of components for this view is 10, the average is .ve. This is typical for all of our data. Figure 
10: a) Photo of the object. b) Rendering using the opacity hull. c) Visual hull. d) Opacity hull. and 
the much improved rendition using the opacity hull, despite the fact that their geometry is identical. 
The opacity hull also allows high quality compositing over complex backgrounds without edge aliasing. 
Unstructured lumigraph interpolation for viewpoints other than those seen by reference cameras introduces 
small artifacts, most notably for specular or concave areas. Figure 11 shows acquired images of an object 
(Figures 11a and c). Figure 11b shows the object from an intermediate viewpoint. Note that the .gure 
shows only the two closest views, although we use the four closest views for interpolation. As can be 
seen in the .gure, the artifacts are generally small. The animations on the companion videotape show 
that the k-nearest neighbor interpolation leads to nice and smooth transitions. To evaluate the number 
of images required to compute the visual hull, we instrumented our code to compute the change in volume 
of orthographic visual hulls as each silhouette is processed. We then randomized the processing order 
of the images and repeated the IBVH calculation multiple times. The plots shown in Figure 12 illustrate 
the rather typical behavior. Generally, the visual hull con- Figure 12: The volume of the visual hull 
as a function of the number of images used to construct the visual hull. verges to within 5% of its .nal 
volume after processing around 20 images, and seldom is this plateau not reached by 30 images. Collecting 
data over the entire hemisphere ensures that this volume closely approximates the actual visual hull. 
This implies that the vi­sual hull processing time can be dramatically reduced by consider­ing fewer 
images to compute the hull model. However, dense alpha mattes are still important for representing view-dependent 
opacity. These view-dependent opacities and radiance measurements dra­matically improve the .nal renderings. 
9 Future Work We are currently working on incorporating environment matting techniques [Zongker et al. 
1999] to correctly represent re.ection and refraction for transparent objects. We plan to store this 
view­dependent data using a representation similar to the opacity hull. In order to preserve the quality 
of our models, we have applied only minimal lossy compression to the data. Improving on this compression 
is clearly one of our next goals. Storing the data in 8 by 8 image blocks allows the application of traditional 
image com­pression tools. The availability of alpha mattes for each image al­lows the application of 
the shape adaptive compression available in JPEG 2000 and MPEG-4. The temporal coherence of the acquired 
images should help in achieving high compression ratios. We also plan to use adaptive reconstruction 
errors for lossy compression of the re.ectance .eld data. Due to the approximate visual hull shape, our 
technique has problems in areas of concavities. The lack of accurate geometry can lead to jumping of 
features over a concavity with a pronounced texture. This could be addressed by improving the geometry 
us­ing computer vision techniques. Another solution is to use adap­tive sampling by taking more images 
in areas where the change of view-dependent radiance data per surface point is suf.ciently non­smooth. 
 We are investigating real-time rendering methods for our mod­els. Our non-optimized implementation 
takes about 30 seconds per frame on current PCs. As shown in [Rusinkiewicz and Levoy 2000; P.ster et 
al. 2000], a point-based representation allows interactive rendering even if the point set is very large. 
We have already im­plemented an interactive renderer for surface light .elds, and we believe we can substantially 
accelerate the rendering of surface re­.ectance .elds. Another avenue for future research is the use 
of graphics accelerators to render our models. Our scanning hardware currently limits the size of the 
acquired objects. It also does not allow scanning of faces, people, or dynamic objects. One could imagine 
extending our approach to hand-held or room size scanners. Major technical dif.culties include accurate 
camera calibration, alpha matte extraction, and controlled illumina­tion. However, we believe there is 
a spectrum of possible digitizer implementations with varying quality and features based on our ap­proach. 
10 Conclusions We have developed a fully automated and robust 3D photography system optimized for the 
generation of high quality renderings of objects. The basic premise of our scanning approach is to use 
large amounts of radiance and opacity information to produce accurate renderings of the object instead 
of relying on accurate geometry. We have introduced the opacity hull, a new shape representation that 
stores view-dependent alpha parameterized on the visual hull of the object. Opacity hulls combined with 
surface re.ectance .elds allow us to render objects with arbitrarily complex shape and ma­terials under 
varying illumination from new viewpoints. Avenues for future research include compression, real-time 
rendering, and improved scanning hardware. 11 Acknowledgments We would like to thank Chris Buehler, Tom 
Buehler, and David Tames for helpful discussions and their efforts in producing the video; Bill Yerazunis, 
Darren Leigh, and Michael Stern for help with construction of the 3D scanner; Joe Marks for his continu­ing 
support; the anonymous reviewers for many constructive com­ments; and Jennifer Roderick P.ster for proofreading 
the paper. Parts of this work were supported by NSF grants CCR-9975859 and EIA-9802220.  References 
BEARDSLEY, P. 2002. Calibration of stereo cameras for a turntable 3d scanner. Tech. rep., MERL. TR 2002/20. 
BERNARDINI,F., MARTIN,I., AND RUSHMEIER, H. 2001. High­quality texture reconstruction from multiple scans. 
IEEE Trans. on Vis. and Comp. Graph. 7, 4 (Oct.-Dec.), 318 332. BUEHLER,C., BOSSE,M., MCMILLAN,L., GORTLER,S., 
AND COHEN, M. 2001. Unstructured lumigraph rendering. In Com­puter Graphics, SIGGRAPH 2001 Proceedings, 
425 432. CHEN,S. E., AND WILLIAMS, L. 1993. View interpolation for image synthesis. In Computer Graphics, 
SIGGRAPH 93 Pro­ceedings, 279 288. CHEN,W.-C.,GRZESZCZUK,R., AND BOUGUET, J.-Y. 2002. Light .eld mapping: 
Ef.cient representation and hardware ren­dering of surface light .elds. In Computer Graphics, To appear 
in the SIGGRAPH 2002 Proceedings. CURLESS,B., AND LEVOY, M. 1996. A volumetric method for building complex 
models from range images. In Computer Graphics, SIGGRAPH 96 Proceedings, 303 312. DEBEVEC,P., AND MALIK, 
J. 1997. Recovering high dynamic range radiance maps from photographs. In Computer Graphics, SIGGRAPH 
97 Proceedings, 369 378. DEBEVEC,P., TAYLOR,C., AND MALIK, J. 1996. Modeling and rendering architecture 
from photographs: A hybrid geometry­and image-based approach. In Computer Graphics, SIGGRAPH 96 Proceedings, 
11 20. DEBEVEC,P., YU,Y., AND BORSHUKOV, G. 1998. Ef.cient view-dependent image-based rendering with 
projective texture­mapping. In Proceedings of the 9th Eurographics Workshop on Rendering, 105 116. DEBEVEC,P., 
HAWKINS,T., TCHOU,C., DUIKER,H.-P., SAROKIN,W., AND SAGAR, M. 2000. Acquiring the re­.ectance .eld of 
a human face. In Computer Graphics,SIG-GRAPH 2000 Proceedings, 145 156. GEORGHIADES,A., BELHUMEUR,P., 
AND KRIEGMAN,D. 1999. Illumination-based image synthesis: Creating novel im­ages of human faces under 
differing pose and lighting. In IEEE Workshop on Multi-View Modeling and Analysis of Visual Scenes, 47 
54. GORTLER,S., GRZESZCZUK,R., SZELISKI,R., AND COHEN, M. 1996. The lumigraph. In Computer Graphics, 
SIGGRAPH 96 Proceedings, 43 54. HAWKINS,T., COHEN,J., AND DEBEVEC, P. 2001. A photomet­ric approach 
to digitizing cultural artifacts. In 2nd International Symposium on Virtual Reality, Archaeology, and 
Cultural Her­itage. KOUDELKA,M., BELHUMEUR,P., MAGDA,S., AND KRIEG-MAN, D. 2001. Image-based modeling 
and rendering of surfaces with arbitrary brdfs. In Proc. of Computer Vision and Pattern Recognition, 
in press. LAURENTINI, A. 1994. The visual hull concept for silhouette­based image understanding. PAMI 
16, 2 (February), 150 162. LENGYEL,J., PRAUN,E., FINKELSTEIN,A., AND HOPPE,H. 2001. Real-time fur over 
arbitrary surfaces. In Symposium on Interactive 3D Graphics, 227 232. LENSCH,H., KAUTZ,J., GOESELE,M., 
HEIDRICH,W., AND SEIDEL, H.-P. 2001. Image-based reconstruction of spatially varying materials. In Proceedings 
of the 12th Eurographics Workshop on Rendering. LEVOY,M., AND HANRAHAN, P. 1996. Light .eld rendering. 
In Computer Graphics, SIGGRAPH 96 Proceedings, 31 42. LEVOY,M., AND WHITTED, T. 1985. The use of points 
as dis­play primitives. Tech. Rep. TR 85-022, The University of North Carolina at Chapel Hill, Department 
of Computer Science. LEVOY,M., PULLI,K., CURLESS,B., RUSINKIEWICZ,S., KOLLER,D., PEREIRA,L., GINZTON,M., 
ANDERSON,S., DAVIS,J., GINSBERG,J., SHADE,J., AND FULK, D. 2000. The digital michelangelo project: 3d 
scanning of large statues. In Computer Graphics, SIGGRAPH 2000 Proceedings, 131 144. MALZBENDER,T., GELB,D., 
AND WOLTERS, H. 2001. Poly­nomial texture maps. In Computer Graphics, SIGGRAPH 2001 Proceedings, 519 
528. MARSCHNER,S., WESTIN,S., LAFORTUNE,E., TORRANCE, K., AND GREENBERG, D. 1999. Image-based brdf measure­ment 
including human skin. In Proceedings of the 10th Euro­graphics Workshop on Rendering, 139 152. MATUSIK,W., 
BUEHLER,C., RASKAR,R., GORTLER,S., AND MCMILLAN, L. 2000. Image-based visual hulls. In Computer Graphics, 
SIGGRAPH 2000 Proceedings, 369 374. MCMILLAN,L., AND BISHOP, G. 1995. Plenoptic modeling: An image-based 
rendering system. In Computer Graphics,SIG-GRAPH 95 Proceedings, 39 46. MILLER,G., RUBIN,S., AND PONCELEON, 
D. 1998. Lazy de­compression of surface light .elds for precomputed global illu­mination. In Proceedings 
of the 9th Eurographics Workshop on Rendering, 281 292. NISHINO,K., SATO,Y., AND IKEUCHI, K. 1999. Appearance 
compression and synthesis based on 3d model for mixed reality. In Proceedings of IEEE ICCV 99, 38 45. 
NISHINO,K., SATO,Y., AND IKEUCHI, K. 1999. Eigen-texture method: Appearance compression based on 3d model. 
In Proc. of Computer Vision and Pattern Recognition, 618 624. PFISTER,H., ZWICKER,M., VAN BAAR,J., AND 
GROSS,M. 2000. Surfels: Surface elements as rendering primitives. In Computer Graphics, SIGGRAPH 2000 
Proceedings, 335 342. PULLI,K., COHEN,M., DUCHAMP,T., HOPPE,H., SHAPIRO, L., AND STUETZLE, W. 1997. View-based 
rendering: Visual­izing real objects from scanned range and color data. In Euro­graphics Rendering Workshop 
1997, 23 34. RAMAMOORTHI,R., AND HANRAHAN, P. 2001. A signal­processing framework for inverse rendering. 
In Computer Graphics, SIGGRAPH 2001 Proceedings, 117 128. RUSHMEIER,H., BERNARDINI,F., MITTLEMAN,J., 
AND TAUBIN, G. 1998. Acquiring input for rendering at appropri­ate levels of detail: Digitizing a piet`a. 
In Proceedings of the 9th Eurographics Workshop on Rendering, 81 92. RUSINKIEWICZ,S., AND LEVOY, M. 2000. 
Qsplat: A multires­olution point rendering system for large meshes. In Computer Graphics, SIGGRAPH 2000 
Proceedings, 343 352. SANDER,P., GU,X., GORTLER,S., HOPPE,H., AND SNYDER, J. 2000. Silhouette clipping. 
In Computer Graphics,SIG-GRAPH 2000 Proceedings, 327 334. SATO,Y., WHEELER,M. D., AND IKEUCHI, K. 1997. 
Object shape and re.ectance modeling from observation. In Computer Graphics, SIGGRAPH 97 Proceedings, 
379 387. SMITH,A. R., AND BLINN, J. F. 1996. Blue screen matting. In Computer Graphics,vol. 30 of SIGGRAPH 
96 Proceedings, 259 268. TURK,G., AND LEVOY, M. 1994. Zippered polygon meshes from range images. In Computer 
Graphics, SIGGRAPH 94 Proceed­ings, 311 318. UDUPA,J., AND ODHNER, D. 1993. Shell rendering. IEEE Com­puter 
Graphics &#38; Applications 13, 6 (Nov.), 58 67. WOOD,D., AZUMA,D., ALDINGER,K., CURLESS,B., DUCHAMP,T., 
SALESIN,D., AND STUETZLE, W. 2000. Sur­face light .elds for 3d photography. In Computer Graphics,SIG-GRAPH 
2000 Proceedings, 287 296. YU,Y., DEBEVEC,P., MALIK,J., AND HAWKINS, T. 1999. In­verse global illumination: 
Recovering re.ectance models of real scenes from photographs. In Computer Graphics, SIGGRAPH 99 Proceedings, 
215 224. ZONGKER,D., WERNER,D., CURLESS,B., AND SALESIN,D. 1999. Environment matting and compositing. 
In Computer Graphics, SIGGRAPH 99 Proceedings, 205 214. ZWICKER,M., PFISTER., H., BAAR,J. V., AND GROSS,M. 
2001. Surface splatting. In Computer Graphics, SIGGRAPH 2001 Proceedings, 371 378.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566600</article_id>
		<sort_key>438</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Real-time 3D model acquisition]]></title>
		<page_from>438</page_from>
		<page_to>446</page_to>
		<doi_number>10.1145/566570.566600</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566600</url>
		<abstract>
			<par><![CDATA[The digitization of the 3D shape of real objects is a rapidly expanding field, with applications in entertainment, design, and archaeology. We propose a new 3D model acquisition system that permits the user to rotate an object by hand and see a continuously-updated model as the object is scanned. This tight feedback loop allows the user to find and fill holes in the model in real time, and determine when the object has been completely covered. Our system is based on a 60 Hz. structured-light rangefinder, a real-time variant of ICP (iterative closest points) for alignment, and point-based merging and rendering algorithms. We demonstrate the ability of our prototype to scan objects faster and with greater ease than conventional model acquisition pipelines.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D model acquisition]]></kw>
			<kw><![CDATA[3D scanning]]></kw>
			<kw><![CDATA[range]]></kw>
			<kw><![CDATA[real-time modeling]]></kw>
			<kw><![CDATA[scanning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Imaging geometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010235</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Epipolar geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14080481</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University, Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14149817</person_id>
				<author_profile_id><![CDATA[81339503254]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Olaf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hall-Holt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566626</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALLEN, B., CURLESS, B., AND POPOVIC, Z. 2002. "Human Body Deformation From Range Scans," Proc. ACM SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>132022</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BESL, P. AND MCKAY, N. 1992. "A Method for Registration of 3-D Shapes," Trans. PAMI, Vol. 14, No. 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628716</ref_obj_id>
				<ref_obj_pid>628321</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLAIS, G. AND LEVINE, M. 1995. "Registering Multiview Range Data to Create 3D Computer Objects," Trans. PAMI, Vol. 17, No. 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>28750</ref_obj_id>
				<ref_obj_pid>28748</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOYER, K. L. AND KAK, A. C. 1987. "Color-Encoded Structured Light for Rapid Active Ranging," Trans. PAMI, Vol. 9, No. 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>890214</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BREGLER, C., HERTZMANN, A., AND BIERMANN, H. 2000. "Recovering Non-Rigid 3D Shape from Image Streams," Proc. CVPR 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>323014</ref_obj_id>
				<ref_obj_pid>323000</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHEN, C., HUNG, Y., AND CHENG, J. 1999. "RANSAC-Based DARCES: A New Approach to Fast Automatic Registration of Partially Overlapping Range Images," Trans. PAMI, Vol. 21, No. 11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CHEN, Y. AND MEDIONI, G. 1991. "Object Modeling by Registration of Multiple Range Images," Proc. IEEE Conf. on Robotics and Automation 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CLINE, H. E., LORENSEN, W. E., LUDKE, S., CRAWFORD, C. R., AND TEETER, B. C. 1998. "Two Algorithms for the Three-Dimensional Reconstruction of Tomograms," Medical Physics, Vol. 15, No. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>299666</ref_obj_id>
				<ref_obj_pid>299660</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[COSTEIRA, J. AND KANADE, T. 1998. "A Multi-Body Factorization Method for Motion Analysis," IJCV, Vol. 29, No. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B. AND LEVOY, M. 1996. "A Volumetric Method for Building Complex Models from Range Images," Proc. ACM SIGGRAPH 96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DAVIS, J. AND CHEN, X. 2001. "A Laser Range Scanner Designed for Minimum Calibration Complexity," Proc. 3DIM 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GRUSS, A., TADA, S., AND KANADE, T. 1992. "A VLSI Smart Sensor for Fast Range Imaging," Proc. IEEE Int. Conf. on Intelligent Robots and Systems 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HALL-HOLT, O. AND RUSINKIEWICZ, S. 2001. "Stripe Boundary Codes for Real-Time Structured-Light Range Scanning of Moving Objects," Proc. ICCV 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>825373</ref_obj_id>
				<ref_obj_pid>523428</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[JOHNSON, A. AND HEBERT, M. 1997. "Surface Registration by Matching Oriented Points," Proc. 3DIM 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794602</ref_obj_id>
				<ref_obj_pid>794190</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KANADE, T., YOSHIDA, A., ODA, K., KANO, H., AND TANAKA, M. 1996. "A Stereo Machine for Video-rate Dense Depth Mapping and Its New Applications," Proc. CVPR 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., PULLI, K., CURLESS, B., RUSINKIEWICZ, S., KOLLER, D., PEREIRA, L., GINZTON, M., ANDERSON, S., DAVIS, J., GINSBERG, J., SHADE, J., AND FULK, D. 2000. "The Digital Michelangelo Project: 3D Scanning of Large Statues," Proc. ACM SIGGRAPH 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>825409</ref_obj_id>
				<ref_obj_pid>523428</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MATSUMOTO, Y., TERASAKI, H., SUGIMOTO, K., AND ARAKAWA, T. 1997. "A Portable Three-Dimensional Digitizer," Proc. 3DIM 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MATUSIK, W., BUEHLER, C., RASKAR, R., GORTLER, S., AND MCMILLAN, L. 2000. "Image-Based Visual Hulls," Proc. ACM SIGGRAPH 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628466</ref_obj_id>
				<ref_obj_pid>628301</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MAVER, J. AND BAJCSY, R. 1993. "Occlusions as a Guide for Planning the Next View," Trans. PAMI, Vol. 15, No. 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MIYAZAKI, D., OOISHI, T., NISHIKAWA, T., SAGAWA, R., NISHINO, K. TOMOMATSU, T., TAKASE, Y., AND IKEUCHI, K. 2000. Proc. VSMM 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245464</ref_obj_id>
				<ref_obj_pid>245456</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[NAYAR, S. K., WATANABE, M., AND NOGUCHI, M. 1996. "Real-Time Focus Range Sensor," Trans. PAMI, Vol. 18, No. 12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PENTLAND, A., DARRELL, T., TURK, M. AND HUANG, W. 1989. "A Simple, Real-Tie Range Camera," Proc. CVPR 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344936</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PFISTER, H., ZWICKER, M., VAN BAAR, J., AND GROSS, M. 2000. "Surfels: Surface Elements as Rendering Primitives," Proc. ACM SIGGRAPH 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>939038</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[PROESMANS, M. VAN GOOL, L., AND DEFOORT, F. 1998. "Reading Between the Lines --- A Method for Extracting Dynamic 3D with Texture," Proc. ICCV 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1889738</ref_obj_id>
				<ref_obj_pid>1889712</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[PULLI, K. 1999. "Multiview Registration for Large Data Sets," Proc. 3DIM 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J. AND BORREL, P. 1993. "Multi-Resolution 3D Approximations for Rendering Complex Scenes," Geometric Modeling in Computer Graphics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[RUSHMEIER, H., BERNARDINI, F., MITTLEMAN, J. AND TAUBIN, G. 1998. "Acquiring Input for Rendering at Appropriate Levels of Detail: Digitizing a Piet&#224;," Proc. Eurographics Rendering Workshop 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[RUSINKIEWICZ, S. AND LEVOY, M. 2000. "QSplat: A Multiresolution Point Rendering System for Large Meshes," Proc. ACM SIGGRAPH 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>934641</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[RUSINKIEWICZ, S. 2001. "Real-Time Acquisition and Rendering of Large 3D Models," Ph.D. Dissertation, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[RUSINKIEWICZ, S. AND LEVOY, M. 2001. "Efficient Variants of the ICP Algorithm," Proc. 3DIM 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[SOUCY, M. AND LAURENDEAU, D. 1992. "Multi-Resolution Surface Modeling from Multiple Range Views," Proc. CVPR 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794714</ref_obj_id>
				<ref_obj_pid>794191</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[STAMOS, I. AND ALLEN, P. 1998. "Interactive Sensor Planning," Proc. CVPR 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[TURK, G. AND LEVOY, M. 1994. "Zippered Polygon Meshes from Range Images," Proc. ACM SIGGRAPH 94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-Time 3D Model Acquisition Szymon Rusinkiewicz Olaf Hall-Holt Marc Levoy c Princeton University 
Stanford University Abstract The digitization of the 3D shape of real objects is a rapidly expand­ing 
.eld, with applications in entertainment, design, and archaeol­ogy. We propose a new 3D model acquisition 
system that permits the user to rotate an object by hand and see a continuously-updated model as the 
object is scanned. This tight feedback loop allows the user to .nd and .ll holes in the model in real 
time, and determine when the object has been completely covered. Our system is based on a 60 Hz. structured-light 
range.nder, a real-time variant of ICP (iterative closest points) for alignment, and point-based merging 
and rendering algorithms. We demonstrate the ability of our proto­type to scan objects faster and with 
greater ease than conventional model acquisition pipelines. Categories and Subject Descriptors: I.3.1 
[Computer Graphics]: Hardware Architecture Input devices; I.4.8 [Image Processing and Computer Vision]: 
Digitization and Image Capture Imag­ing geometry, scanning; B.4.2 [Input/Output and Data Communi­cations]: 
Input/Output Devices. Additional Keywords: 3D model acquisition, 3D scanning, range scanning, real-time 
modeling. 1 Introduction The desire to reduce the dependence on human input in making realistic images 
of complex scenes has, over the past ten years, re­sulted in an increased role for measurements of the 
real world in the computer graphics pipeline. In this paper, we focus on using 3D scanning to build detailed 
models of complex objects for use in rendering. Although for some uses of range scanning it is suf.cient 
to ob­tain a single range image of the object, many applications require a complete 3D model. Since most 
3D scanners only obtain data from one side of an object (or even a small region of one side) at a time, 
it is necessary to move the scanner (or, equivalently, to move the model relative to a stationary scanner) 
to cover the ob­ject from multiple views. These views must then be registered to each other and merged 
into a single, consistent model. Thus, for these 3D model acquisition applications, the range scanner 
itself is _ Department of Computer Science Department of Computer Science 35 Olden St. Gates Building 
3B Princeton University Stanford University Princeton, NJ 08544 Stanford, CA 94305 S smr@cs.princeton.edu 
olaf,levoy}@graphics.stanford.edu Copyright &#38;#169; 2002 by the Association for Computing Machinery, 
Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for commercial advantage 
and that copies bear this notice and the full citation on the first page. Copyrights for components of 
this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 (a) Layout of our system. It consists of a DLP projector 
that displays structured light patterns and an NTSC video camera. The green and blue lines have been 
added in this visualization. (b) Photograph of a turtle .gurine, approximately 18 cm. long.  (c) Shortly 
after the start of scanning, data has been accumulated relatively sparsely. The individual point primitives 
used by our merging data structure are visible.  (d) After a few seconds of scanning, the front part 
of the turtle has been covered relatively well. However, the user sees a few remaining holes.  (e) The 
user turns the object to .ll the holes. The user may try a number of different positions until the holes 
are .lled immediate feedback is available about the effectiveness of each orientation.  (f) Once all 
the data has been gathered, high-quality of.ine global registration and surface reconstruction algorithms 
are used to produce a .nal model.  Figure 1: Our real-time 3D model acquisition system was used to 
scan a small turtle .gurine. The total scanning time was 4 minutes and the .nal model, at 1e2 mm. resolution, 
contains approximately 200,000 polygons. (c) through (e) are rendered using splats (see Section 2.3), 
and (f) is rendered as a polygon mesh (see Section 2.4). only one part of a pipeline that must also include 
deciding where to take scans (view planning), aligning the scans (registration), and re­constructing 
a surface (merging). Unless the pipeline is completely automatic it is also necessary to present intermediate 
results to the user, so a .nal piece of the pipeline is rendering. Previous implementations of this pipeline 
have been proposed, with a variety of technologies at each of the stages [Soucy and Laurendeau 1992; 
Turk and Levoy 1994; Curless and Levoy 1996]. Although such systems have been used to create complex, 
high-resolution models, as in several recent sculpture digitization projects [Rushmeier et al. 1998; 
Levoy et al. 2000; Miyazaki et al. 2000], doing so is neither fast nor easy. One of the reasons for this 
dif.culty is that, in most existing pipelines, human intervention is needed at several stages. First, 
most range image registration algorithms require the user to provide an initial guess by roughly aligning 
new scans to the existing 3D model. Next, all scans are rendered together and the operator uses this 
information to perform view planning (e.g., to .nd holes). Each iteration of this alignment / view planning 
cycle is time consuming, and feedback about the effectiveness of the scanner s position is not available 
until the next scan is performed and aligned. Automated next-best-view systems have been proposed to 
eliminate the user from this loop [Maver and Bajcsy 1993; Stamos and Allen 1998], but they are often 
computationally intensive and require computer control over the position of the scanner. We propose a 
new system that returns range images in real time (60 Hz.) and lets the user perform hole detection and 
view plan­ning interactively. Our system is based on a consumer video pro­jector, used to generate sequences 
of structured light patterns, and a consumer video camera, used to observe those patterns. The user holds 
the object in his or her hands and moves it slowly. As range images are acquired, they are automatically 
aligned to the exist­ing scans and merged together to create a 3D model. Our align­ment and merging algorithms 
are optimized to work well given the small translations and rotations that occur when the object is moved 
slowly by hand. By displaying a rough merged model in real-time on a computer screen, the user can .nd 
holes (unscanned areas) vi­sually, and move the object to .ll them. Although our approach requires a 
human in the loop, it eliminates the need for expensive calibrated motion control stages, and it eliminates 
the time delay in­curred by of.ine view planning algorithms. This tradeoff makes our system fast and 
inexpensive, although not automatic. An overview of the system is presented in Figure 1. In Section 2, 
we describe the design of our system. We review the structured-light range.nder [Hall-Holt and Rusinkiewicz 
2001] and real-time alignment algorithm [Rusinkiewicz and Levoy 2001] used by our system, then describe 
our real-time merging and dis­play algorithms. In Section 3 we investigate how to choose which scans 
to align to each other, and describe what happens when the system loses track of the correct alignment 
(e.g., because the object is moved out of the .eld of view of the scanner). In Section 4 we present a 
few notes on implementation, and in Section 5 we present sample results from our prototype. In Section 
6 we analyze the per­formance of our system, and in Section 7 we present some ideas for future research. 
  2 Real-Time Range Scanning Pipeline As stated above, our model acquisition system consists of a real­time 
pipeline integrating a range.nder and algorithms for align­ment, merging, and rendering. In order to 
implement a prototype of this pipeline that operates at interactive rates on current hardware, it was 
necessary to make some tradeoffs in the technologies and al­gorithms that were used. In the following 
sections we present the structured-light range.nder, real-time alignment algorithm, voxel­based merging 
algorithm, and splat-based renderer used by our sys­tem. 2.1 Structured-Light Range Scanning Our 3D 
model acquisition pipeline begins with a real-time (60 Hz.) range.nder. Previous real-time 3D scanning 
systems have been proposed based on defocus [Pentland et al. 1989; Nayar et al. 1996], stereo [Kanade 
et al. 1996], silhouettes [Matsumoto et al. 1997; Matusik et al. 2000], structured light [Gruss et al. 
1992; Proesmans et al. 1998], and time of .ight (commercial systems by 3DV and Perceptron). We chose 
to use a system based on projected structured-light triangulation, since it uses off-the-shelf hardware, 
permits (slowly) moving objects, and returns a full range image, as opposed to a single point or line 
of 3D data, every 60th of a second. Here we present a brief summary of the design of this system; a full 
description appears in [Hall-Holt and Rusinkiewicz 2001]. Simple Structured-Light Systems: Our range.nder 
is based on the principle of structured-light triangulation. As shown in Figure 2, the simplest such 
system consists of a projector that emits a stripe (plane) of light and a camera placed at an angle with 
respect to the projector. At each point in time, the camera obtains 3D positions for points along a 2D 
contour traced out on the object by the plane of light. In order to obtain a full range image, it is 
necessary either to sweep the stripe along the surface (as is done by many commer­cial single-stripe 
laser range scanners) or to project multiple stripes. Although projecting multiple stripes leads to faster 
data acquisi­tion, such a system must have some method of determining which stripe is which. There are 
three major ways of doing this: assum­ing surface continuity so that adjacent projected stripes are adja­cent 
in the camera image [Proesmans et al. 1998], differentiating the stripes based on color [Boyer and Kak 
1987], and coding the stripes by varying their illumination over time. The .rst approach (assuming continuity) 
allows depth to be determined from a sin­gle frame but fails if the surface contains discontinuities. 
Using color allows more complicated surfaces but fails if the surface is textured. Temporal stripe coding 
is robust to moderate surface tex­ture but takes several frames to compute depth and, depending on the 
design, may fail if the object moves. In the simplest systems using temporal coherence, the entire ac­quisition 
process takes some number of frames n, and in each frame any given stripe is either turned on or off. 
The on-off pattern of each stripe is chosen to uniquely identify that stripe. For example, sup­pose that 
we wish to use 2n different stripes. We assign each stripe . a code from 0 to 2n 1 and look at each 
of these codes as an n-bit binary number. We project n different patterns, where in pattern k each stripe 
is on if and only if the k-th bit of its code is a 1. Thus, after any given pixel has been observed for 
n frames, the identity of the stripe that illuminated that pixel may be determined. Searching for Stripe 
Boundaries: The type of time-coded structured-light system described above has one signi.cant disad- 
Object  Figure 2: Schematic layout of a single-camera, single-stripe-source trian­gulation system. The 
3D positions of points on the object are determined from the intersection between the camera ray and 
the plane of light produced by the illumination source. Space Figure 3: A four-frame sequence of projected 
patterns. Each pattern has 110 stripe boundaries (111 stripes), with the property that each stripe bound­ary 
has a unique code (consisting of the black/white illumination history on either side of the boundary 
over the sequence of four frames). Figure 4: A sequence of video frames of an elephant .gurine illuminated 
with a stripe boundary code. The projector cycles among these patterns at 60 Hz., and the illuminated 
object is photographed by an NTSC camera synchronized to the projector. vantage: the object being scanned 
must not move while the n coded frames are being obtained. If the object does move, the system is likely 
to obtain an incorrect stripe code, and hence incorrect 3D data. In order to allow object motion during 
scanning, it is necessary to add frame-to-frame tracking into the general framework of time­coded structured 
light. In particular, if the object moves slowly enough that stripes may be tracked over time, it is 
possible to ac­cumulate the bits of the code over time, despite the fact that they do not come from the 
same camera pixel. In practice, one can only track a region by tracking its boundaries. Therefore, we 
design a system around the idea of tracking projected stripe boundaries, and using these boundaries to 
convey codes over time. Given the framework of tracking stripe boundaries as they move from frame to 
frame, we search for a way of coding stripes that is optimized for this application. First, we observe 
that it is desirable for stripes to be as narrow as possible, so that the set of boundaries we are tracking 
is as dense as possible. Since the bit-plane tech­nique of simple binary coding results in very wide 
stripes in some frames, this is not a desirable code for our application. Second, our shift of focus 
from stripes to boundaries permits us to convey infor­mation more ef.ciently: instead of obtaining one 
bit of information per frame, we may obtain two bits of information by looking at the on/off status on 
both sides of the boundary. Thus, we wish to de­sign a code such that every pair of adjacent stripes 
has a unique code over time. The .nal criterion in designing our codes is the observation that, since 
we are looking at both sides of a stripe boundary, that bound­ary will not be visible at every frame, 
since the stripes on either side of that boundary might both be either on or off. Thus, we de­sign a 
code that minimizes the number of these invisible ghosts, to maximize the chances of tracking these boundaries 
even if they disappear. Note that this is just a special case of the general strat­egy of making stripes 
as narrow as possible, as described above. As described in [Hall-Holt and Rusinkiewicz 2001], it is possible 
to design a code that never contains two ghosts adjacent in space or in time. Figure 3 shows the sequence 
of illumination patterns used by our system. There are four different patterns, which can uniquely code 
110 stripe boundaries. The system cycles among these frames at Video Edge Boundary Decoding frames detection 
tracking  0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 Code = 01 00 01 00 Code = 10 01 11 01 Code = 01 10 10 11 
 Figure 5: Tracking stripe boundaries. Our system identi.es edges in the video frames, tracks the stripe 
boundaries over time, and propagates the illumination history of each boundary over four frames to determine 
the boundary s code. Note that the tracking stage has to infer the presence of ghost boundaries (shown 
as dashed lines) in some of the double-wide black and white regions. Our projected patterns are designed 
to minimize the presence of such invisible boundaries, and have the property that there are never two 
ghosts adjacent in space or in time. 60 Hz., captures video with a synchronized NTSC camera (Figure 4), 
performs the boundary tracking and decoding (Figure 5), and returns a range image at each point in time. 
Since we may use any sequence of four consecutive frames to obtain depth, there is a latency of 4 frames 
in identifying a new stripe boundary but depths are obtained at every frame thereafter. 2.2 Fast 3D 
Registration The previous section described the design of a range scanner ca­pable of returning the 3D 
shape of a moving object as seen from a single viewpoint in real time. As mentioned in the introduction, 
however, the goal of our system is to produce complete models of rigid objects. Therefore, we must align 
the range images from dif­ferent viewpoints that are produced as a rigid object is moved rela­tive to 
the range scanner. There are three classes of methods that have been considered for this application. 
The .rst relies on known motion: the object and scanner are moved relative to each other by a calibrated 
rotational or translational stage. As mentioned earlier, however, a key usabil­ity improvement in our 
design comes from lifting the restriction to calibrated motion and allowing the object and scanner to 
be moved freely with respect to each other. In addition, avoiding calibrated motion helps reduce the 
cost of building and calibrating the scan­ner [Davis and Chen 2001]. A second way of obtaining the alignment 
between range images is to place a tracker on either the object or the scanner (whichever is moved relative 
to the other). Although we have chosen not to use this option because of accuracy and cost considerations, 
we believe that in many circumstances it would provide substantial bene.ts in the context of our proposed 
pipeline. As discussed in Section 6, a separate tracker would be especially useful for preventing the 
global drift that results from our use of scan-to-scan alignment. The option we use for alignment in 
our pipeline is based on reg­istering individual scans to each other based on the geometry in overlapping 
areas. The algorithm we have chosen is a variant of Figure 6: Schematic view of projection-based ICP. 
(a) A random subset of points on one scan (shown at top in blue) is selected. For each point, a ray is 
found through the point of projection of the other scan (shown below in red). The point along this ray 
that intersects the red range image is taken as the matching point. (b) For each such pair, the distance 
is minimized between one of the points and the plane through the other point and perpen­dicular to its 
normal. (c) One scan is moved to minimize the sum of squared point-to-plane distances, bringing the two 
scans into closer alignment. The process may now be repeated until the scans are aligned, according to 
some convergence criterion. ICP (Iterative Closest Points), which is widely used for geometric alignment 
of three-dimensional models when an initial estimate of the relative pose is known. In the real-time 
range scanner appli­cation, the relative motion between two consecutive range images is small, so in 
the simplest case we may simply align each range image to the previous one. The ICP algorithm [Besl and 
McKay 1992] has become the dominant method for aligning three-dimensional models based purely on the 
geometry, and sometimes color, of the meshes. ICP starts with two meshes and an initial guess for their 
rela­tive rigid-body transform, and iteratively re.nes the transform by repeatedly generating pairs of 
corresponding points on the meshes and minimizing an error metric. As described in [Rusinkiewicz and 
Levoy 2001], it is possible to decompose the ICP algorithm into six stages: 1. Selection of some set 
of points in one or both meshes. This is typically uniform or random subsampling. 2. Matching these 
points to samples in the other mesh. In the original ICP algorithm, this was done by matching to the 
clos­est point in the other mesh, though other approaches are pos­sible. 3. Weighting the corresponding 
pairs appropriately, for example based on the distance between the points or compatibility of normals. 
 4. Rejecting certain pairs based on looking at each pair individ­ually or considering the entire set 
of pairs. This is done to eliminate outliers. 5. Assigning an error metric to the current alignment, 
based on the point pairs. This is usually point-to-point distance, but Chen and Medioni have observed 
that it is more effec­tive to minimize the distance from one point to the plane containing the other 
point and perpendicular to its normal [Chen and Medioni 1991].  6. Minimizing the error metric (e.g. 
via least squares). By choosing a different variant at each stage, we obtain a variety of algorithms 
with different stability, robustness, and ef.ciency char­acteristics. For our current needs, we are interested 
in an algorithm that of­fers the highest possible performance. After some experimentation, we have chosen 
the following variant, described in more detail in [Rusinkiewicz and Levoy 2001]: 1. Random sampling 
of points on one mesh. 2. Projection-based matching [Blais and Levine 1995] see Fig­ure 6. 3. Uniform 
weighting of all point pairs.  4. Rejection of pairs with point-to-point distance greater than a threshold. 
 5. Point-to-plane error metric [Chen and Medioni 1991].  6. The standard iterative minimization. The 
result is an algorithm that, in our application, aligns scans in a few milliseconds, fast enough to be 
incorporated into our real­time pipeline. The main gain in speed over most other ICP-like algorithms 
comes from the use of projection-based matching. This relies on the fact that a range image is naturally 
organized as a 2-D array of samples. Finding a matching point by projecting into the range image is therefore 
just indexing into the array, and does not require a comparatively slow 3D closest-points search. An 
overview of the algorithm is presented in Figure 6.  2.3 Merging and Rendering The goal of a live preview 
of the scanned model is to give the user feedback about which areas of the model have been scanned, and 
whether any holes are left. Since the model accumulates incremen­tally in our system, some way must be 
found to display this con­stantly increasing set of range samples. The traditional way to dis­play multiple 
scans is to merge them using a surface reconstruction algorithm, thereby forming a 3D model represented 
by a polygon mesh. Although there are many such algorithms that produce high­quality results, running 
times are typically on the order of several seconds or minutes for models of moderate complexity. To 
avoid these long running times, we have chosen to avoid reconstructing a surface in our merging step, 
instead directly displaying the raw range samples. The dif.culty with this approach is that in a real-time 
scanning system, data is accumulating rapidly hundreds of thousands of points per second in our case. 
Thus, to maintain an acceptable in­teractive frame rate, it is necessary to perform some sort of merging 
or discarding of redundant 3D data in order to reduce the number of primitives that must be rendered. 
Our approach relies on collapsing points in cells of a voxel grid. Our merging algorithm .rst triangulates 
each new range image and computes per-vertex surface normals. Next, after aligning the range image, we 
discard the connectivity information and perform merging based only on the points themselves. This consists 
of quantizing all points to a 3D grid and combining all points that map to the same location in this 
grid (the idea of collapsing all points in a voxel grid cell is similar to the one used in the mesh simpli.ca­tion 
method of Rossignac and Borrel [1993]). A running-average normal is maintained at each grid cell for 
use in rendering. The rendering is done using a method called splatting, which has recently seen increased 
interest for rendering large, detailed 3D models [Rusinkiewicz and Levoy 2000; P.ster et al. 2000]. In 
this technique, a screen-aligned splat (e.g. an alpha-blended Gaussian or an opaque circle or square) 
is drawn for each point, scaled such that the splats for neighboring points overlap without leaving a 
gap. When the points are regularly arranged on a grid, such as in this ap­plication, their spacing is 
known a priori and it is simple to select a splat size for a given viewpoint that guarantees that the 
splats for adjacent samples overlap without leaving holes. Thus, a rendering is produced that gives the 
appearance of a merged surface without the need to triangulate the points or to reconstruct a consistent 
poly­gon mesh. The merging and rendering algorithms are illustrated in Figure 7. Although this merging 
algorithm runs quickly, it does not pro­duce the highest-quality results. If scans are misaligned by 
more than the grid spacing, samples from those scans will not be merged, creating two or more layers 
of occupied voxels. After many range images have been incorporated, the result is a region of occupied 
voxels in the vicinity of the true surface (see Figure 8). Since these voxels are rendered with Z-buffering, 
the visible voxels will be those that are outermost, causing our surface to appear thickened Range Images 
Voxel Grid Cells Quantized Positions Merged Grid with Averaged Normals Lit Splat Rendering Figure 7: 
Grid-based merging and rendering. Samples from each range image are quantized in a voxel grid and are 
merged with samples from other scans. An average normal is accumulated at each grid cell, and splatting 
is used to render the merged samples. All of these illustrations represent plan views, but the splats 
in the bottom row have been turned to face the reader so that their shape and color may be seen. Ideal 
Thickened Figure 8: If there were no noise in the scans and alignment were perfect, only a thin later 
of voxels would be accumulated in our grid-based merg­ing data structure (left). In practice, we see 
a wider layer of voxels (right). This results in thickened silhouettes and a noisier rendering. The overall 
vi­sual effect is still acceptable, however, since splats are shaded using average normals. and possibly 
noisy. However, the quality of the real-time recon­struction only needs to be good enough to guide the 
user in position­ing the object and determining the presence of holes in the model; a high-quality reconstruction 
may be performed of.ine at the con­clusion of the scanning process. Moreover, the visual quality of the 
thickened surface is better than one might expect: although the sur­face geometry is noisy, voxels are 
rendered with averaged normals so the shading appears relatively smooth. Outlier Elimination: As mentioned 
above, moderate misalign­ment and noise does not degrade the appearance of the merged grid unacceptably. 
Large outliers, however, are visible, and it is necessary to eliminate them before merging. Fortunately, 
since we acquire data at such a high rate, we can be aggressive in eliminat­ing outliers; any correct 
data that is mistakenly discarded is highly likely to be acquired again, because the user will see a 
hole and ac­quire replacement data on the spot. Thus, the rapid feedback given to the user, in addition 
to helping with view planning, actually sim­pli.es the problem of dealing with noisy data. Our outlier 
rejection algorithm operates after a range image has been triangulated. We eliminate all long and thin 
triangles, as well as triangles that are backfacing with respect to the camera or pro­jector. If this 
results in points that are not part of any triangle, those points are eliminated. Depending on the parameter 
in the above algorithm (that is, the skinniest permitted triangles), this method may eliminate a cer­tain 
amount of data that was seen at a large tilt with respect to the camera. This is not necessarily a drawback, 
since that data is likely to be of lower quality anyway. Discarding this data encourages the user to 
turn those regions of the object towards the range scanner (so that higher-quality data may be obtained). 
For creases in some objects, however, it may be impossible to orient the object such that a given portion 
of the surface is both visible from and normal to the camera. Therefore, the outlier elimination should 
not be set to be too aggressive. In practice, we currently discard triangles in which the smallest angle 
is less than 10 degrees. Grid Size: In order to perform this voxel-based merging and ren­dering, it is 
necessary to choose an appropriate grid size. Smaller grid cells result in greater detail, but at the 
expense of greater mem­ory usage, lower frame rates, and greater sensitivity to noise. Larger grids give 
higher frame rates, but the individual splats become more visible. In addition, when using larger grids 
the user may not be able to see small holes in the data. In practice, we use a grid size on the order 
of the spacing of sam­ples given by the range scanner; higher grid resolution results in substantially 
lower frame rates without corresponding increases in perceived quality. For our prototype, we usually 
use a grid size of . 12 mm., which is roughly equal to the range sample spacing near the front of our 
working volume. 2.4 Of.ine Registration and Surface Reconstruction As mentioned above, the quality achievable 
with the real-time pipeline is not as high as that of state-of-the-art of.ine registration and surface 
reconstruction algorithms. Thus, once the user has acquired data for the entire object, an of.ine post-process 
may be run to reconstruct a .nal, high-quality surface. Because the user is con.dent that all necessary 
data has been acquired and because the results of the real-time registration are available as an initial 
guess, this process may run completely automatically, without further user intervention. We use the following 
post-processing pipeline: The grid data structure used for real-time merging is dis­carded, and all 
further processing is performed on the original range images. Starting from scan positions and transforms 
computed by the real-time algorithm, ICPs are performed on consecutive scans, as well as additional pairs 
of overlapping scans. The algorithm used at this stage is not the high-speed ICP variant used in the 
real-time pipeline but a slower, higher-quality algorithm [Pulli 1999]. A globally-optimal alignment 
is computed by simultaneously considering the results of all the scan-to-scan alignments and performing 
a global relaxation [Pulli 1999]. The range images are triangulated, and merged using the VRIP algorithm 
[Curless and Levoy 1996]. A .nal, merged triangular mesh is extracted using marching cubes [Cline et 
al. 1988]. Additional processing (such as decimation, smoothing, or .lling of inaccessible holes) may 
now be performed on the model.  3 Anchor Scans and Restarting Alignment So far, we have suggested that 
the alignment stage of the pipeline always involves performing an ICP between a range image and the previous 
one. Under ideal conditions, this usually works, but in practice an ICP will occasionally fail. This 
might happen if the object is moved too fast, such that the stripe boundary tracking fails, and it will 
certainly happen if the object is moved out of the .eld of view of the scanner. Once such a situation 
has been corrected (i.e., the object slows down to a reasonable velocity and/or comes back within the 
.eld of view), it is necessary to regain the alignment of the new scans to the previously-acquired model. 
A reasonable approach to restarting after a failed alignment might be to treat it as a general problem 
of aligning two 3D models given an unknown initial pose. There exist algorithms to solve this problem 
[Johnson and Hebert 1997; Chen et al. 1999], but they are typically slow and not robust. Instead, we 
may take advantage of the human operator s strengths in performing pattern matching by simply displaying 
one or more range images, asking the user to position the object so it roughly lines up with one of them, 
and attempting to perform ICP to those range images until one of the ICPs succeeds. If we adopt this 
strategy, we must choose which range images to present to the user. We have observed that just using 
the last range image before the ICP failure is not a good choice. If the ob­ject is moving out of the 
.eld of view, the last few range images see smaller and smaller pieces of the object. A user is likely 
to have dif.culty in aligning the object to one of these. More seri­ously, having smaller and smaller 
amounts of range data decreases the constraints on ICP, thus increasing the likelihood of obtaining an 
incorrect alignment. Anchor Scans: In order to make it possible to display good range images for restarting 
ICP, we maintain some number of anchor scans to which we attempt to align. We would like these to have 
the following characteristics: The anchor scans should be large enough to be recognizable to the user 
and for ICP to operate reliably. We would like the anchors to have relatively low overlap among themselves, 
so that they cover as much of the object as possible. In order to maintain the above properties, we de.ne 
a set of an­chors A1 . . .An, initially consisting of only the .rst scan taken. As each additional range 
image R is acquired, the alignment and re­covery algorithms are as follows: 1. If R is empty or has fewer 
than align min points we do not attempt alignment, since such an alignment would be more likely to be 
incorrect. We indicate to the user that the object is out of the working volume. 2. Otherwise, we attempt 
to align R to A1. Our initial guess for the transform is the result of the last successful ICP. 3. If 
the ICP succeeds, we evaluate whether R should become a new anchor. This is done if:  R is large (more 
than anchor min points), and The overlap between R and the anchor for which ICP succeeded is less than 
overlap thresh. If both of these conditions hold, R becomes A1, the old an­ chors A1 . . .An1 become 
A2 . . .An, and the old An is deleted. 4. If the initial ICP fails, we retry it a number of times using 
more iterations, larger thresholds, and several perturbations to the starting position of R. If all of 
these attempts fail, we try to align R to each of A2 . . .An. If any of these succeeds, we move that 
anchor to the front of the list (so it becomes A1). 5. If ICP to all of the anchors has failed, we draw 
R and A1 . . .An in different colors and ask the user to move the object to line up R with one of the 
anchors.  The effects of this algorithm are shown in Figure 9. The above rules for deciding when a scan 
becomes an anchor were chosen so that anchors tend to be large and reasonably spaced out (in practice, 
we observe that 5 10% of scans become anchors). This makes it easy for the user to restart the normal 
ICP after a failed alignment. In addition, it has the bene.t of preventing the accumulation of alignment 
errors when the object is not moving. This is because, when the object is not moving, the overlap be- 
  Edge of working volume R A1 1. R too small (e.g., out of working volume): No ICP performed R A1 2. 
Regular ICP succeeds: A1 remains the anchor A1 R overlap < overlap_thresh 3. ICP succeeds, small overlap: 
R becomes new anchor A1 A2 R 4. Alignment to A1 failed: R aligned to A2 A1 R A2 5. All ICPs failed: User 
is asked to move the object to align with anchors Figure 9: As described at left, our system maintains 
a number of anchor scans A1An to which each new scan R is aligned. The different cases  of the algorithm 
ensure that the anchors are relatively large and spread out over the surface, leading to easier (manual) 
error recovery and less accumu­lated drift in alignment. Except in case 5, the different colors used 
in this visualization are not visible to the user. tween each new scan and the anchor will be large, 
so the anchor will not change. Thus, all of the scans will be aligned to the same anchor. If instead 
we performed simple scan-to-scan alignment, the small errors introduced by each ICP would have the potential 
of accumulating, leading to more global drift in the alignment. 4 Implementation Hardware: The system 
uses a Compaq MP1800 DLP projector, with a maximum resolution of 1024x768. Because of the need to synchronize 
it with the video camera, we currently send an S-Video signal to the projector, limiting us to a resolution 
of 640x240 inter­laced. The camera we use is a Sony DXC-LS1 NTSC camera, with a 1/500 sec. shutter speed. 
The video is digitized by a Pinnacle Stu­dio DC10+ capture card, yielding interlaced 640x240 video .elds 
at 60 Hz. CPU Usage: Our prototype uses a dual-CPU system, with Intel Pentium III Xeon processors running 
at 1 GHz. One CPU is used for the .rst few stages of the range scanning pipeline, namely grab­bing video 
frames, .nding stripe boundaries, matching the bound­aries across time, and identifying the boundaries 
from the accumu­lated illumination history. The second CPU performs triangulation to .nd 3D points, aligns 
the scans using the fast ICP algorithm, integrates range images into the 3D grid, and renders the updated 
grid. The .rst piece of this pipeline operates at full speed (60 Hz.), while the second operates slower, 
approximately 10 Hz. The reason for choosing this unequal division of stages among CPUs is to ensure 
that the matching stage does not drop frames; this permits the highest-possible speeds for object motion. 
It is not as critical for the rest of the pipeline to run at the full 60 Hz. camera rate, since this 
only results in a lower frame rate for the display. Layout: The layout of the system determines its working 
volume and resolution. For the scans presented here, we have positioned the camera and projector 20 cm. 
apart, with a triangulation angle of 21 degrees. This con.guration produces a working volume approx­imately 
10 cm. across. Near the front of the working volume, sam­ples are spaced roughly every 0.5 mm. in Y (parallel 
to the stripe direction) and every 0.75 mm. in X (perpendicular to the stripes). (a) (b)  Cyberware 
Model 15 Cyberware Model 15, subsampled 2x  Our scanner Cyberware Model 15 Cyberware, subsampled Range 
images 1,830 22 22 Avg. samples / scan 3,140 75,380 18,845 Sample spacing (x) 0.75 mm. 0.25 mm. 0.5 mm. 
Sample spacing (y) 0.5 mm. 0.33 mm. 0.66 mm. Alignment automatic manual manual Scanning time 4 min. 30 
min. 30 min. Figure 11: Scans of the turtle .gurine performed with a Cyberware Model 15 single-stripe 
laser triangulation scanner (compare Figure 10d). The scans have been aligned using the algorithm of 
[Pulli 1999] starting from a manual initial alignment, and have been merged using VRIP [Curless and Levoy 
1996]. Since our scanner has a lower resolution than the Cyberware, we also show a model generated by 
subsampling the original Cyberware scans by a factor of 2 in each dimension.  5 Results In Figure 10, 
we focus on the turtle .gurine of Figure 1, and com­pare the results of the real-time pipeline to the 
post-processed ver­sion (after global registration and VRIP surface reconstruction). Figure 11 compares 
our scanner to a commercially-available single­stripe laser triangulation scanner. The results are similar, 
though the model we generate is not as high-quality as that produced by the Cyberware scanner, chie.y 
because of digitizer noise and mis­calibration (see the following section). Finally, some other objects 
scanned using our prototype system are shown in Figure 12. (a) (b)  6 Analysis Accuracy and Precision: 
There are several ways in which the accuracy of our range scanner may be characterized. We may look at 
the spacing between samples on the surface, the noise in the lo­cation of each sample, and the distortion 
in each range image due to miscalibration. All of these depend on the physical arrangement of camera 
and projector, and so must be compared to the size of the working volume. The .gures below all apply 
to the front 10 cm. section of the working volume discussed above. The sample spacing in our prototype, 
as we have stated, is 0.5­ 0.75 mm. The noise in each of these samples is primarily due to the error 
in locating a stripe boundary, which may be due to noise in the camera and digitizer or due to object 
texture. For surfaces with­out high-frequency texture, we may .nd the locations of the stripe boundaries 
with subpixel precision, and we estimate the noise in each sample to be under 0.1 mm. (ignoring, for 
the moment, out­liers and distortion). This is due almost entirely to noise in the camera and capture 
card: using a higher-quality camera and dig­itizer (Toshiba IK-TU40A 3-CCD camera and DPS-465 digitizer) 
we obtain lower per-sample noise under 0.03 mm. Ultimately, using a high-quality camera and digitizer, 
the limit on the minimum achievable local noise depends on two factors. The .rst is the focus of the 
camera and projector. Using a smaller aperture (especially on the projector most commercially-available 
models use large apertures) would permit better localization of stripe boundaries. The second major limit 
on the accuracy of this system is scene texture. When the re.ectance of the surface varies rapidly (on 
the order of the camera pixel spacing), we are not able to perform accurate subpixel estimation, and 
so the per-sample error is substantially larger, on the order of 0.2 mm. Calibration and Warping: In 
addition to noise in the samples, the accuracy of our scans is degraded by the distortion in our scan­ner 
due to miscalibration. We have adopted a calibration procedure in which known 3D points in the scene 
are measured using a Faro . arm touch probe, and their u. v. camera locations as well as pro­jector p 
coordinate are found. The optimal intrinsic and extrinsic calibration parameters are found by minimizing 
the error in all the .. u. v. p... x. y. z. mappings simultaneously. Although we may estimate the error 
in the calibration directly from the convergence of the minimization algorithm, a more mean­ingful estimate 
arises from considering the maximal misalignment between range images of the same object taken at a variety 
of differ­ent positions and orientations. For the turtle data set of Figure 10, we observe a misalignment 
of approximately 0.5 mm. (after high­quality ICP and global registration), leading us to conclude that 
the distortion is of this order of magnitude. Registration Drift and Effects on Scanning Strategy: Even 
though the distortion in each individual scan due to miscalibration is fairly small, the effects of this 
distortion can easily add up from frame to frame, contributing to the registration drift problem mentioned 
in Section 3. Despite the use of anchor scans (which reduce the occurrence of the problem by a large 
factor) in some cases this drift is suf.ciently severe that it becomes impractical to use the system 
to scan completely around an object. In this case, the system may be used to scan the object in a small 
number of pieces, and the pieces joined together at the end of scanning (an initial guess for the relative 
alignment must be obtained from the user). Alternatively, a small number of scans may be taken around 
the object, a global registration algorithm [Pulli 1999] may be run to align them, then the real-time 
pipeline may be used to .ll holes in this skeleton, by always using the pre-aligned scans as the anchors. 
Regardless of the strategy chosen, the pipeline retains its main advantage of simplifying the process 
of hole-.lling. An alternative approach to solving the registration drift problem would be to integrate 
our system with conventional 6-DOF track­ers, placed either on the object or the scanner depending on 
which is moved relative to the other. Using such trackers (either jointed digitizing arms or untethered 
systems) would permit the scanner to be applied to larger objects or environments by eliminating the 
reg­istration error due to noise or miscalibration. In addition, it would make the system more stable 
in the presence of degenerate geome­try (such as planes or cylinders) for which ICP is not able to deter­mine 
all six degrees of freedom of the alignment. 7 Conclusions and Future Work This paper has described 
a new 3D model acquisition system de­signed to be inexpensive, fast, and easy to use, and demonstrated 
results from a prototype implementation. In contrast with previous systems, our design permits the user 
to rotate an object by hand and see a continuously-updated model as the object is scanned, thus providing 
instant feedback about the presence of holes and the amount of surface that has been covered. The system 
uses off-the­shelf components and runs on today s CPUs. The ability to provide real-time feedback to 
the user has yielded bene.ts throughout the model acquisition pipeline. In addition to simplifying the 
view planning problem, it has proven useful in restarting after a failed alignment and prompted an aggressive 
strat­egy for outlier rejection. We anticipate future work in exploring the ways that a high data rate 
and instant user feedback affect the 3D scanning pipeline. Here we suggest only a few ideas for future 
work; a more complete list may be found in [Rusinkiewicz 2001]. There are several ways in which the system 
described here could be improved. Some, such as obtaining texture, would be relatively easy to incorporate 
into our pipeline. Others might involve changes in hardware, such as higher-resolution cameras and projectors 
(for higher-quality data or larger working volumes), high-speed cameras and projectors (for faster allowable 
motion), or multiple cameras or projectors (for faster scanning and better coverage). In addition to 
improvements in hardware, one possible algorith­mic improvement, possible with increasing CPU speeds 
or custom hardware, would be to improve the quality of the real-time merg­ing and rendering algorithms. 
For example, with an order of mag­nitude faster CPU it would be possible to implement an implicit­surface 
reconstruction algorithm, such as VRIP, and either extract a polygon mesh or use volume rendering hardware 
to display it interactively. Another potential improvement would be to allow the pipeline to use available 
data from later stages to help the ear­lier stages. For example, ambiguities in edge detection or tracking 
might be resolved by looking at other range images, or even at the accumulated model. By casting the 
entire pipeline in a probabilis­tic framework, one could maintain multiple hypotheses, with con.­dence 
estimates, and delay making irrevocable decisions as long as possible. For certain classes of objects, 
one might consider solving the model acquisition problem in the presence of nonrigid deformation. Although 
the .rst stage of our current pipeline (the 3D scanner) can handle deformation, the alignment and merging 
stages would re­quire considerable changes. There has been recent work on tracking non-rigid objects 
[Costeira and Kanade 1998; Bregler et al. 2000], though much of it assumes either that an initial model 
is available or that the deformation is heavily constrained. Acquisition of de­formable models would 
be especially attractive for capturing hu­man animation [Allen et al. 2002]. Finally, we note that one 
major bene.t of using a triangulation­based system is that it potentially scales to many different work­ing 
volumes. One could imagine scaling the system up for scan­ning building interiors or movie sets, or scaling 
it down for appli­cations in industrial inspection or medicine. The major problems when scaling up would 
be the physical size of the baseline, emit­ting enough light by the projector (relative to ambient light), 
and depth of .eld of the camera and projector. When scaling down, the major challenges would involve 
focus and the diffraction limit of light.  Acknowledgments This research grew out of a system built 
together with Li-Wei He, and bene.tted from conversations with Lucas Pereira, Sean Ander­son, James Davis, 
and many others at the Stanford Graphics Lab. We would also like to thank Intel, Sony, and Interval for 
their .nan­cial support.  References ALLEN, B., CURLESS, B., AND POPOVIC, Z. 2002. Human Body Deformation 
From Range Scans, Proc. ACM SIG- GRAPH 2002. BESL, P. AND MCKAY, N. 1992. A Method for Registration of 
3-D Shapes, Trans. PAMI, Vol. 14, No. 2. BLAIS, G. AND LEVINE, M. 1995. Registering Multiview Range Data 
to Create 3D Computer Objects, Trans. PAMI, Vol. 17, No. 8. BOYER, K. L. AND KAK, A. C. 1987. Color-Encoded 
Structured Light for Rapid Active Ranging, Trans. PAMI, Vol. 9, No. 1. BREGLER, C., HERTZMANN, A., AND 
BIERMANN, H. 2000. Recovering Non-Rigid 3D Shape from Image Streams, Proc. CVPR 2000. CHEN, C., HUNG, 
Y., AND CHENG, J. 1999. RANSAC-Based DARCES: A New Approach to Fast Automatic Registration of Partially 
Overlapping Range Images, Trans. PAMI, Vol. 21, No. 11. CHEN, Y. AND MEDIONI, G. 1991. Object Modeling 
by Registra­ tion of Multiple Range Images, Proc. IEEE Conf. on Robotics and Automation 1991. CLINE, 
H. E., LORENSEN, W. E., LUDKE, S., CRAWFORD, C. R., AND TEETER, B. C. 1998. Two Algorithms for the Three-Dimensional 
Reconstruction of Tomograms, Medical Physics, Vol. 15, No. 3. COSTEIRA, J. AND KANADE, T. 1998. A Multi-Body 
Factoriza­tion Method for Motion Analysis, IJCV, Vol. 29, No. 3. CURLESS, B. AND LEVOY, M. 1996. A Volumetric 
Method for Building Complex Models from Range Images, Proc. ACM SIGGRAPH 96. DAVIS, J. AND CHEN, X. 2001. 
A Laser Range Scanner Designed for Minimum Calibration Complexity, Proc. 3DIM 2001. GRUSS, A., TADA, 
S., AND KANADE, T. 1992. A VLSI Smart Sensor for Fast Range Imaging, Proc. IEEE Int. Conf. on In­telligent 
Robots and Systems 1992. HALL-HOLT, O. AND RUSINKIEWICZ, S. 2001. Stripe Bound­ary Codes for Real-Time 
Structured-Light Range Scanning of Moving Objects, Proc. ICCV 2001. JOHNSON, A. AND HEBERT, M. 1997. 
Surface Registration by Matching Oriented Points, Proc. 3DIM 1997. KANADE, T., YOSHIDA, A., ODA, K., 
KANO, H., AND TANAKA, M. 1996. A Stereo Machine for Video-rate Dense Depth Mapping and Its New Applications, 
Proc. CVPR 1996. LEVOY, M., PULLI, K., CURLESS, B., RUSINKIEWICZ, S., KOLLER, D., PEREIRA, L., GINZTON, 
M., ANDERSON, S., DAVIS, J., GINSBERG, J., SHADE, J., AND FULK, D. 2000. The Digital Michelangelo Project: 
3D Scanning of Large Statues, Proc. ACM SIGGRAPH 2000. MATSUMOTO, Y., TERASAKI, H., SUGIMOTO, K., AND 
ARAKAWA, T. 1997. A Portable Three-Dimensional Digi­tizer, Proc. 3DIM 1997. MATUSIK, W., BUEHLER, C., 
RASKAR, R., GORTLER, S., AND MCMILLAN, L. 2000. Image-Based Visual Hulls, Proc. ACM SIGGRAPH 2000. MAVER, 
J. AND BAJCSY, R. 1993. Occlusions as a Guide for Planning the Next View, Trans. PAMI, Vol. 15, No. 5. 
MIYAZAKI, D., OOISHI, T., NISHIKAWA, T., SAGAWA, R., NISHINO, K. TOMOMATSU, T., TAKASE, Y., AND IKEUCHI, 
K. 2000. Proc. VSMM 2000. NAYAR, S. K., WATANABE, M., AND NOGUCHI, M. 1996. Real-Time Focus Range Sensor, 
Trans. PAMI, Vol. 18, No. 12. PENTLAND, A., DARRELL, T., TURK, M. AND HUANG, W. 1989. A Simple, Real-Tie 
Range Camera, Proc. CVPR 1989. PFISTER, H., ZWICKER, M., VAN BAAR, J., AND GROSS, M. 2000. Surfels: Surface 
Elements as Rendering Primitives, Proc. ACM SIGGRAPH 2000. PROESMANS, M. VAN GOOL, L., AND DEFOORT, F. 
1998. Reading Between the Lines A Method for Extracting Dy­namic 3D with Texture, Proc. ICCV 1998. PULLI, 
K. 1999. Multiview Registration for Large Data Sets, Proc. 3DIM 1999. ROSSIGNAC, J. AND BORREL, P. 1993. 
Multi-Resolution 3D Approximations for Rendering Complex Scenes, Geometric Modeling in Computer Graphics. 
RUSHMEIER, H., BERNARDINI, F., MITTLEMAN, J. AND TAUBIN, G. 1998. Acquiring Input for Rendering at Appro­priate 
Levels of Detail: Digitizing a Piet`a, Proc. Eurograph­ics Rendering Workshop 1998. RUSINKIEWICZ, S. 
AND LEVOY, M. 2000. QSplat: A Multireso­lution Point Rendering System for Large Meshes, Proc. ACM SIGGRAPH 
2000. RUSINKIEWICZ, S. 2001. Real-Time Acquisition and Rendering of Large 3D Models, Ph. D. Dissertation, 
Stanford University. RUSINKIEWICZ, S. AND LEVOY, M. 2001. Ef.cient Variants of the ICP Algorithm, Proc. 
3DIM 2001. SOUCY, M. AND LAURENDEAU, D. 1992. Multi-Resolution Sur­face Modeling from Multiple Range 
Views, Proc. CVPR 1992. STAMOS, I. AND ALLEN, P. 1998. Interactive Sensor Planning, Proc. CVPR 1998. 
TURK, G. AND LEVOY, M. 1994. Zippered Polygon Meshes from Range Images, Proc. ACM SIGGRAPH 94. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566601</article_id>
		<sort_key>447</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Light field mapping]]></title>
		<subtitle><![CDATA[efficient representation and hardware rendering of surface light fields]]></subtitle>
		<page_from>447</page_from>
		<page_to>456</page_to>
		<doi_number>10.1145/566570.566601</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566601</url>
		<abstract>
			<par><![CDATA[A light field parameterized on the surface offers a natural and intuitive description of the view-dependent appearance of scenes with complex reflectance properties. To enable the use of surface light fields in real-time rendering we develop a compact representation suitable for an accelerated graphics pipeline. We propose to approximate the light field data by partitioning it over elementary surface primitives and factorizing each part into a small set of lower-dimensional functions. We show that our representation can be further compressed using standard image compression techniques leading to extremely compact data sets that are up to four orders of magnitude smaller than the input data. Finally, we develop an image-based rendering method, light field mapping, that can visualize surface light fields directly from this compact representation at interactive frame rates on a personal computer. We also implement a new method of approximating the light field data that produces positive only factors allowing for faster rendering using simpler graphics hardware than earlier methods. We demonstrate the results for a variety of non-trivial synthetic scenes and physical objects scanned through 3D photography.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[compression algorithms]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[rendering hardware]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shading</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Color</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP18000377</person_id>
				<author_profile_id><![CDATA[81423593965]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei-Chao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P136983</person_id>
				<author_profile_id><![CDATA[81100366629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Yves]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouguet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382448</person_id>
				<author_profile_id><![CDATA[81547831656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Chu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P237369</person_id>
				<author_profile_id><![CDATA[81100560226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Radek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grzeszczuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>525960</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383270</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. Boivin and A. Gagalowicz. Image-Based Rendering of Diffuse, Specular and Glossy Surfaces From a Single Image. Proceedings of SIGGRAPH 2001, pages 107-116, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>339361</ref_obj_id>
				<ref_obj_pid>339355</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J-Y. Bouguet and P. Perona. 3D Photography Using Shadows in Dual-Space Geometry. International Journal of Computer Vision, 35(2):129-149, December 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383309</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Buehler, M. Bosse, L. McMillan, S. J. Gortler, and M. F. Cohen. Unstructured Lumigraph Rendering. Proceedings of SIGGRAPH 2001, pages 425-432, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344932</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J-X. Chai, X. Tong, S-C. Chan, and H-Y. Shum. Plenoptic Sampling. Proceedings of SIGGRAPH 2000, pages 307-318, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[W-C. Chen, R. Grzeszczuk, and J-Y. Bouguet. Light Field Mapping: Hardware-accelerated Visualization of Surface Light Fields. Published as part of "Acquisition and Visualization of Surface Light Fields," SIGGRAPH 2001 Course Notes for Course 46, pages 410-416, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[R. L. Cook and K. E. Torrance. A Reflectance Model for Computer Graphics. ACM Transactions on Graphics, 1(1):7-24, January 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>840039</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Curless and M. Levoy. Better Optical Triangulation through Spacetime Analysis. Proc. 5th Int. Conf. Computer Vision, Boston, USA, pages 987-993, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[P. Debevec, T. Hawkins, C. Tchou, H-P. Duiker, W. Sarokin, and M. Sagar. Acquiring the Reflectance Field of a Human Face. Proceedings of SIGGRAPH 2000, pages 145-156, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometry- and Image-Based Approach. Proceedings of SIGGRAPH 96, pages 11-20, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>893689</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[P. E. Debevec, Y. Yu, and G. D. Borshukov. Efficient View-Dependent Image-Based Rendering with Projective Texture-Mapping. Eurographics Rendering Workshop 1998, pages 105-116, June 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>902023</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Fournier. Separating Reflection Functions for Linear Radiosity. Eurographics Rendering Workshop 1995, pages 296-305, June 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Publishers, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen. The Lumigraph. Proceedings of SIGGRAPH 96, pages 43-54, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[W. Heidrich and H-P. Seidel. Realistic, Hardware-Accelerated Shading and Lighting. Proceedings of SIGGRAPH 99, pages 171-178, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312153</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Kautz and M. D. McCool. Interactive Rendering with Arbitrary BRDFs using Separable Approximations. Eurographics Rendering Workshop 1999, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>348214</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Kautz and H-P. Seidel. Towards Interactive Bump Mapping with Anisotropic Shift-Variant BRDFs. 2000 SIGGRAPH / Eurographics Workshop on Graphics Hardware, pages 51-58, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649002</ref_obj_id>
				<ref_obj_pid>645310</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. J. Koenderink and A. J. van Doorn. Bidirectional Reflection Distribution Function Expressed in Terms of Surface Scattering Modes. In European Conference on Computer Vision, pages II:28-39, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[E. P. F. Lafortune, S-C. Foo, K. E. Torrance, and D. P. Greenberg. Non-Linear Approximation of Reflectance Functions. Proceedings of SIGGRAPH 97, pages 117-126, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. D. Lee and H. S. Seung. Learning the Parts of Objects by Non-Negative Matrix Factorization. Nature, 401:788-791, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732303</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter Seidel. Image-Based Reconstruction of Spatially Varying Materials. In Twelveth Eurographics Rendering Workshop 2001, pages 104-115. Eurographics, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Levoy and P. Hanrahan. Light Field Rendering. Proceedings of SIGGRAPH 96, pages 31-42, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2323083</ref_obj_id>
				<ref_obj_pid>2322507</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Magnor and B. Girod. Data Compression for Light Field Rendering. IEEE Trans. Circuits and Systems for Video Technology, 10(3):338-343, April 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T. Malzbender, D. Gelb, and H. Wolters. Polynomial Texture Maps. Proceedings of SIGGRAPH 2001, pages 519-528, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383276</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Michael D. McCool, Jason Ang, and Anis Ahmad. Homomorphic Factorization of BRDFs for High-Performance Rendering. Proceedings of SIGGRAPH 2001, pages 171-178, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[L. McMillan and G. Bishop. Plenoptic Modeling: An Image-Based Rendering System. Proceedings of SIGGRAPH 95, pages 39-46, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[G. S. P. Miller, S. Rubin, and D. Ponceleon. Lazy Decompression of Surface Light Fields for Precomputed Global Illumination. Eurographics Rendering Workshop 1998, pages 281-292, June 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[K. Nishino, Y. Sato, and K. Ikeuchi. Eigen-Texture Method: Appearance Compression Based on 3D Model. In Proceedings of the IEEE Computer Science Conference on Computer Vision and Pattern Recognition (CVPR-99), pages 618-624, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97909</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[P. Poulin and A. Fournier. A Model for Anisotropic Reflection. Proceedings of SIGGRAPH 90, 24(4):273-282, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732115</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[K. Pulli, M. Cohen, T. Duchamp, H. Hoppe, L. Shapiro, and W. Stuetzle. View-based Rendering: Visualizing Real Objects from Scanned Range and Color Data. Eurographics Rendering Workshop 1997, pages 23-34, June 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[R. Ramamoorthi and P. Hanrahan. A Signal-Processing Framework for Inverse Rendering. Proceedings of SIGGRAPH 2001, pages 117-128, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>302762</ref_obj_id>
				<ref_obj_pid>302528</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Sam Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing Systems, volume 10. The MIT Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Y. Sato, M. D. Wheeler, and K. Ikeuchi. Object Shape and Reflectance Modeling from Observation. Proceedings of SIGGRAPH 97, pages 379-388, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[P. Schr&#246;der and W. Sweldens. Spherical Wavelets: Efficiently Representing Functions on the Sphere. Proceedings of SIGGRAPH 95, pages 161-172, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[J. Spitzer. Texture Compositing With Register Combiners. Game Developers Conference, April 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274365</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[G. Taubin and J. Rossignac. Geometric Compression Through Topological Surgery. ACM Transactions on Graphics, 17(2):84-115, April 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[K. E. Torrance and E. M. Sparrow. Polarization, Directional Distribution, and Off-Specular Peak Phenomena in Light Reflected from Roughened Surfaces. Journal of Optical Society of America, 56(7), 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[G. J. Ward. Measuring and Modeling Anisotropic Reflection. Proceedings of SIGGRAPH 92, 26(2):265-272, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344925</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[D. N. Wood, D. I. Azuma, K. Aldinger, B. Curless, T. Duchamp, D. H. Salesin, and W. Stuetzle. Surface Light Fields for 3D Photography. Proceedings of SIGGRAPH 2000, pages 287-296, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311559</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Y. Yu, P. E. Debevec, J. Malik, and T. Hawkins. Inverse Global Illumination: Recovering Reflectance Models of Real Scenes From Photographs. Proceedings of SIGGRAPH 99, pages 215-224, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light Field Mapping: Ef.cient Representation and Hardware Rendering of Surface Light Fields Wei-Chao 
Chen . Jean-Yves Bouguet Michael H. Chu Radek Grzeszczuk § University of North Carolina at Chapel Hill 
Intel Corporation Intel Corporation Intel Corporation Abstract: A light .eld parameterized on the surface 
offers a nat­ural and intuitive description of the view-dependent appearance of scenes with complex re.ectance 
properties. To enable the use of surface light .elds in real-time rendering we develop a compact representation 
suitable for an accelerated graphics pipeline. We propose to approximate the light .eld data by partitioning 
it over elementary surface primitives and factorizing each part into a small set of lower-dimensional 
functions. We show that our representa­tion can be further compressed using standard image compression 
techniques leading to extremely compact data sets that are up to four orders of magnitude smaller than 
the input data. Finally, we develop an image-based rendering method, light .eld mapping, that can visualize 
surface light .elds directly from this compact repre­sentation at interactive frame rates on a personal 
computer. We also implement a new method of approximating the light .eld data that produces positive 
only factors allowing for faster rendering using simpler graphics hardware than earlier methods. We demonstrate 
the results for a variety of non-trivial synthetic scenes and physical objects scanned through 3D photography. 
CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism color, shading, shadowing, 
texture I.4.8 [Image Processing And Computer Vision]: Scene Analysis color, shading I.2.10 [Arti.cial 
Intelligence]: Scene Analysis 3D/stereo scene analysis, texture Keywords: Image-based Rendering, Texture 
Mapping, Compres­sion Algorithms, Rendering Hardware. ............ The recent proliferation of inexpensive 
but powerful graphics hard­ware and new advances in digital imaging technology are enabling novel methods 
for realistic modeling of the appearance of physical objects. On the one hand, we see a tendency to represent 
com­plex analytic re.ectance models with their sample-based approxi­mations that can be evaluated ef.ciently 
using new graphics hard­ware features [15; 16; 17]. On the other hand, we are witnessing a proliferation 
of image-based rendering and modeling techniques .e-mail: ... ........ e-mail: ................. ........ 
e-mail: ........... ........ §e-mail: ............... ........ Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Figure 1: A combination 
of synthetic and scanned objects rendered using light .eld mapping. Complex, physically realistic re.ectance 
properties of this scene are preserved. [26; 22; 14; 4] that attempt to represent the discrete radiance 
data directly in the sample-based format without resorting to the analytic models at all. These techniques 
are popular because they promise a simple acquisition and an accurate portrayal of the physical world. 
The approach presented here combines these two trends. Similar to other image-based methods, our approach 
produces a sample-based representation of the surface light .eld data. Additionally, the pro­posed representation 
can be evaluated ef.ciently with the support of existing graphics hardware. ..................................... 
A surface light .eld is a 4-dimensional function f .r.s.. .f.that completely de.nes the outgoing radiance 
of every point on the sur­face of an object in every viewing direction. The .rst pair of pa­rameters 
of this function .r.s.describes the surface location and the second pair of parameters .. .f.describes 
the viewing direc­tion. A direct representation and manipulation of surface light .eld data is impractical 
because of the large size. Instead, we propose to approximate the light .eld function as a sum of a small 
number of products of lower-dimensional functions K f .r.s.. .f... gk.r.s.hk.. .f.. (1) k.1 We show that 
it is possible to construct the approximations of this form that are both compact and accurate by taking 
advantage of the spatial coherence of the surface light .elds. We accomplish this by partitioning the 
light .eld data across small surface primi­tives and building the approximations for each part independently. 
The partitioning is done in such a way as to ensure continuous ap­proximations across the neighboring 
surface elements. Because the functions gk.r.s.and hk.. .f.encode the light .eld data and are stored 
in a sampled form as texture maps, we call them the light .eld maps. Similarly, we refer to the process 
of rendering from this approximation as light .eld mapping. In the remainder of the paper we will refer 
to functions gk.r.s.as the surface maps and functions hk.. .f.as the view maps. As shown in earlier work, 
similar types of factorizations can be used to produce high quality renderings of objects with complex 
surface re.ectance properties at interactive frame rates [15; 17; 16]. Most previous work, however, assumes 
that the surface of the ob­ject has uniform re.ectance properties, which greatly simpli.es the problem. 
Our method shows for the .rst time how to compute such factorization for surfaces where each point has 
a unique re­.ectance property. This generalization makes light .eld mapping ideally suited for modeling 
and rendering of physical objects and scenes scanned through 3D photography. ................ We make 
the following contributions to analysis and representa­tion of image-based data and to hardware-accelerated 
rendering of image-based models. Data Partitioning: We introduce a novel type of partitioning of the 
light .eld data that ensures a continuous approximation across in­dividual triangles. It represents the 
light .eld data for each triangle as a linear blend of 3 light .elds corresponding to the vertices of 
the triangle. See Section 2.1. Approximation Through Factorization: We introduce a new type of factorization 
of light .eld data. Dense and uniform sampling of the view-dependent information, enabled through the 
use of view maps hk.. .f., ensures correct image synthesis from any camera location. See Section 2.2. 
Positive Factorization: We introduce a new method of approxi­mating the light .eld data that uses non-negative 
matrix factoriza­tion [20] to produce positive only factors. This method allows for faster rendering 
using generic graphics hardware at high approxi­mation quality. It is signi.cantly easier to implement 
than homo­morphic factorization [25]. See Section 2.2. Hardware-Accelerated Rendering: We develop a fully 
hardware­accelerated rendering routine that can visualize the light .eld data from the proposed, compact 
representation at highly interactive frame rates. The rendering algorithm is extremely simple to imple­ment, 
since it reduces to a multi-pass texture mapping operation. Rendering quality can be improved progressively 
by increasing the number of rendering passes. See Section 3. Compression: We report the highest compression 
of surface light .elds to date. Since the light .eld maps are in essence collections of images that themselves 
exhibit redundancy, they can be further compressed using standard image compression techniques. This 
compression combined with the redundancy reduction achieved through the approximation results in extremely 
high compression ratios. Our method routinely approaches 4 orders of magnitude compression ratio and 
allows for hardware-accelerated rendering directly from the compressed representation. Additional compres­sion 
is possible for transmission and storage purposes. See Sec­tion 4. .............. Traditionally, computer 
graphics researchers have favored analytic re.ectance models for their intuitive simplicity, compactness 
and .exibility [37; 7; 38; 29]. Although simple to represent, these mod­els are often costly to compute 
and dif.cult to develop. Several approaches have been developed to approximate re.ectance func­tions 
with a set of prede.ned basis function [34; 18; 19] but with­out much consideration for rendering ef.ciency. 
Inverse rendering methods [33; 40; 9; 2] approximate scene attributes, such as light­ing and surface 
re.ectance, by .tting analytic models to sample­based radiance data. Ramamoorthi and Hanrahan [31] propose 
a signal processing framework for inverse rendering. Light .eld map­ping is a related method that uses 
a factor analysis approach rather than a signal processing approach. Image-based methods [26; 22; 14; 
4; 24] represent the radiance data directly in the sample-based format. Compression of light .eld data 
is an important area of research. Levoy and Hanrahan [22] use VQ [13], Magnor and Girod [23] have developed 
a series of disparity-compensated hybrid light .eld codecs. Chai et al. [5] use spectral analysis to 
compute the minimum sampling rate for light .eld rendering and show that scene depth information signif­icantly 
decreases this rate. Miller et al. [27] propose a method of rendering surface light .elds from input 
images compressed using JPEG-like compression. Malzbender et al. [24] approximate a se­quence of images 
captured under varying lighting conditions using biquadratic polynomials. The representation is very 
compact and can be implemented in hardware. The work on sample-based approximations for some of the ana­lytic 
models listed above includes [15; 17; 21]. Kautz and McCool [16] propose a method for hardware assisted 
rendering of arbitrary BRDFs through their decomposition into a sum of 2D separable functions that is 
based on an idea proposed by Fournier [12]. Our application is fundamentally different from this work, 
since it al­lows each surface point to have different re.ectance properties and is therefore ideally 
suited for modeling and rendering of objects and scenes scanned through 3D photography. Homomorphic fac­torization 
of McCool et al. [25] generates a BRDF factorization with positive factors only, which are easier and 
faster to render on the current graphics hardware, and deals with scattered data with­out a separate 
resampling and interpolation algorithm. We present a novel method for factorization of light .eld data 
that also pro­duces positive only factors using non-negative matrix factorization [20]. Lensch et al. 
[21] reconstruct a spatially varying BRDF from a sparse set of images. First, the object is split into 
cluster with different BRDF properties, then a set of basis BRDFs is generated for each cluster, .nally 
the original samples are reprojected into the space spanned by the basis BRDFs. Data factorization is 
this case is extremely time consuming and rendering is not real-time. The work on surface light .elds 
includes view-dependent tex­ture mapping [10; 11; 30]. Additionally, Wood et al. [39] use a generalization 
of VQ and PCA (principal component analysis) [1] to compress surface light .elds and propose a new rendering 
algo­rithm that displays compressed light .elds at interactive frame rates. Their 2-pass rendering algorithm 
interpolates the surface normals using Gouraud shading producing incorrect results for large trian­gles. 
To alleviate the problem they introduce a view-dependent ge­ometry re.nement at the cost of increasing 
the complexity of the renderer. Nishino et al. [28] propose the eigen-texture method that also performs 
factorization on image data. Their basic theoretical ap­proach is similar to ours however it applies 
only to a 3D subset of the 4D light .eld and cannot be directly extended to the full 4D data. Additionally, 
our approach uses a vertex-based partitioning which ensures a continuous reconstruction, we explore a 
positive factor­ization technique, and we develop a fully hardware-accelerated ren­dering of surface 
light .elds directly from our representation.  ................................. We start the description 
of the approximation method with a novel partitioning scheme that ensures continuous approximations throughout 
the surface of the model.  Figure 2: The .nite support of the hat functions .vj around vertex vj, j 
.1.2.3. .vj denotes the portion of .vj that corresponds to .i triangle .i. Functions .v1, .v2 and .v3 
add up to one inside .i. ............................... We assume that the geometry of the models is 
represented as a tri­angular mesh. An obvious partitioning of the light .eld function is to split it 
between individual triangles. Unfortunately, an approxi­mation of surface light .eld partitioned this 
way results in visible discontinuities at the edges of the triangles. We demonstrate this ef­fect in 
the video accompanying the paper. To eliminate the discon­tinuities across triangle boundaries we propose 
to partition surface light .eld data around every vertex. We refer to the surface light .eld unit corresponding 
to each vertex as the vertex light .eld and for vertex vj denote it as fvj .r.s.. .f.. Partitioning is 
computed by weighting the surface light .eld function fvj .r.s.. .f...vj .r.s.f .r.s.. .f. (2) where 
.vj is the barycentric weight of each point in the ring of triangles relative to vertex vj. Because of 
their shape, the weighting functions are often referred to as the hat functions. The top row of Figure 
2 shows hat functions .v1, .v2, .v3 for three vertices v1, v2, v3 of triangle .i. As shown at the bottom 
of Figure 2, these 3 hat functions add up to unity inside triangle .i. Therefore, Equation (2) de.nes 
a valid partitioning of the surface light .eld because the sum of all vertex light .elds equals to the 
original light .eld data. In the .nal step of vertex-centered partitioning we reparameter­ize each vertex 
light .eld to the local coordinate system of the ver­tex. To this end, we de.ne the viewing direction 
angles . and f as the azimuth and elevation angles of the viewing direction vector in the local reference 
frame of the vertex. We de.ne the vertex ref­erence frame in such a way that its z-axis is parallel to 
the surface normal at the vertex. We use the same letters to denote the local parameters of the vertex 
light .eld in order to simplify the notation. ................................ Vertex-centered partitioning 
of light .eld data allows us to approx­imate each vertex light .eld independently while maintaining con­tinuity 
across the whole model. Analogous to Equation (1), we approximate each vertex light .eld as K fvj .r.s.. 
.f... gvk j .r.s.hvk j .. .f.. (3) k.1 We present two methods of approximating vertex light .elds: PCA 
(principal component analysis) [1] and NMF (non-negative matrix factorization) [20]. Both of these methods 
approximate the light .eld data as a linear combination of a small number of basis im­ages but each one 
produces an approximation with a different set of features. For example, each PCA-based basis image gives 
a global approximation of the data. This means that we can use a subset of them to produce a consistent 
approximation, since adding suc­cessive basis images simply improves the accuracy preceding basis images. 
NMF, on the other hand, produces basis images that form a parts-based representation and therefore all 
of them need to be used to produce a consistent approximation of the input data. PCA-based approximation 
is therefore more progressive than NMF-based ap­proximation. However, PCA allows the approximation factors 
to be of arbitrary sign. This makes rendering more dif.cult and requires more specialized graphics hardware. 
See Section 3.1 for details. NMF does not allow negative values in the factors resulting in an approximation 
that is much easier and faster to render on generic graphics hardware. We will use the matrix factorization 
framework to describe how PCA and NMF construct the light .eld data approximations. We .rst discretize 
the vertex light .eld function fvj .r.s.. .f.into a 4­dimensional grid fvj .rp.sp..q.fq., where index 
p .1.....M refers to the discrete values .rp.sp.describing the surface location within the triangle ring 
of vertex vj, and index q .1.....N refers to the discrete values ..q.fq.of the two viewing angles. We 
then rear­range the discretized vertex light .eld into the matrix . fvj .r1.s1..1.f1....fvj .r1.s1..N 
.fN . . ... . . Fvj ... ... ..(4) .. fvj .rM .sM ..1.f1....fvj .rM.sM ..N .fN . where M is the total 
number of surface samples inside the triangle ring and N is the total number of views for each surface 
sample. We refer to matrix Fvj as the vertex light .eld matrix. Each column of this matrix represents 
the appearance of the vertex ring under a different viewing direction. A detailed discussion of resampling 
the irregular input light .eld data can be found in Section 5. Both PCA and NMF construct approximate 
factorizations of the form K F.vj .T . ukvk (5) k.1 where uk is a vectorized representation of discrete 
surface map gkvj .rp.sp.and vk is a vectorized representation of discrete view map hvk j ..q.fq.. The 
differences between the two methods arise from the constraints imposed on the matrix factors uk and vk. 
PCA constrains uk s to be orthonormal and vk s to be orthogonal to each other. NMF, on the other hand, 
does not allow negative entries in the matrix factors uk and vk. Singular value decomposition can be 
used to compute PCA factorization. Since only the .rst few summation terms of the factorization are needed, 
one can use an ef.cient algo­rithm that computes partial factorization. See Appendix for details on how 
to compute the factorizations. Partitioning of light .eld data into vertex light .elds ensures that in 
the resulting approximation each triangle shares its view maps with the neighboring triangles. Therefore, 
even though we decom­pose each vertex light .eld independently, we obtain an approxi­mation that is continuous 
across triangles regardless of the number of approximation terms K. Let gvj .rp.sp.be the surface map 
and hvj ..q.fq.be the view map corresponding to one approximation term of vertex light .eld fvj .rp.sp..q.fq.. 
Let f..i .rp.sp..q.fq. denote the corresponding approximation term of the light .eld data for triangle 
.i. The following equality holds 3 vj f..i .rp.sp..q.fq... g .rp.sp.hvj ..q.fq.(6) .i j.1 vj where index 
j runs over the three vertices of triangle .i and g .i .r.s. denotes the portion the surface map corresponding 
to the triangle surface map g.v1 i[r , sp]view map h v1[. , fq] pq vertex vi expressed in the reference 
frame of vertex vj. This results in 3 texture fragments shown in the right column of Figure 3. Note that 
the texture coordinates are different for each fragment because we are using a different reference frame 
to compute them. Since vj vj the surface map texture coordinates .si .ti .do not depend on the viewing 
angle, they do not need to be recomputed when the camera v (s ,11 t )1 v1 v1 v1 (s 3 ,t3 )   (s v1 
,t )v1 2 2 v1 v (x 3 ,y3 )1  v1 v1 (x 1 ,y )1 (x 2v1 ,y2 )v1    v3 z v1  surface map g.v2 i[r 
, sp]view map h v2[. , fq] moves. pq  v (s ,12 t )1 v2 (s v2 ,tv2 ) 3 3   (s v2 ,t )v2 2 2  Evaluating 
one complete approximation term is equivalent to multiplying pixel-by-pixel the image projections of 
the surface map and view map texture fragment pairs for the 3 vertex light .elds of the triangle, each 
shown in a separate row of Figure 3, and adding the results together. The multiple term approximation 
of each tri­ v 1 angle light .eld is computed by simply adding the results of each g.i[r , sp]surface 
map v3p view map h v3[. , fq] q  approximation term. mapping using speci.c hardware features such as 
multitexturing v (s ,13 t )1 v3 v3 v3 (s 3 ,t3 )   (s v3 ,t )v3 2 2 .................................. 
In this section we discuss ef.cient implementations of light .eld v1 and extended color range. Recall 
that one of the fundamental oper­ations of the proposed rendering algorithm was the pixel-by-pixel multiplication, 
or modulation, of the surface map fragment by the corresponding view map fragment. Multitexturing hardware 
sup­port enables us to compute the modulation of multiple texture frag­ ments very ef.ciently in one 
rendering pass. Consequently, for Figure 3: Light .eld maps for one approximation term of one trian­ 
gle. Vertex reference frames are shown in the left column. .i. Note that this equality holds for all 
approximation terms. Next, we show how this property can be used to develop a very simple and ef.cient 
rendering algorithm.  ................... Our rendering algorithm takes advantage of the property of 
vertex­centered partitioning, described by Equation (6), which says that the light .eld for each triangle 
can be expressed independently as a sum of its 3 vertex light .elds. This allows us to write a very ef.­cient 
rendering routine that repeats the same sequence of operations for each mesh triangle. Additionally, 
since each light .eld approxi­mation term is also evaluated in exactly the same way, we only need to 
explain how to evaluate one approximation term of one triangle. Figure 3 shows six light .eld maps used 
in Equation (6) to com­pute one approximation term of light .eld for triangle .i. The vj middle column 
shows surface maps g .i .rp.sp.. The pixels covered by the shaded triangle correspond to the points inside 
triangle .i where we sampled the light .eld function. We will describe the texture coordinates of these 
points as .s.t.. Note that due to parti­tioning the pixels of the surface maps are weighted, as indicated 
in the .gure by gradient shading, but this does not alter the rendering algorithm in any way. The right 
column shows view maps hvj ..q.fq.. In each image, the pixels inside the circle correspond to the orthographic 
projec­tion of the hemisphere of viewing directions, expressed in the local coordinate system xyz of 
vertex vj, onto the plane xy shifted and scaled to the range .0.1.. We will describe the texture coordinates 
of these points as .x.y.. This projection allows a simple texture coordinate computation x ..d .x .1..2.y 
..d .y .1..2 (7) where d represents the normalized local viewing direction and vec­tors x and y correspond 
to the axes of the local reference frame. Other transformations from 3D directions to 2D maps are possible 
[15] but we found the one described here ef.cient and accurate. Rendering of light .eld approximations 
is done using texture mapping. Based on where the camera is located, the rendering al­gorithm needs to 
access a different 2D subset of the 4D light .eld function. This is done by recomputing the view map 
coordinates vj vj .xi .yi .every time the camera moves. To this end, we apply Equa­tions (7) to vectors 
dvj , which represent the viewing directions to i the NMF-based approximation of light .eld data, which 
produces strictly positive light .eld maps, we need at most 3 rendering passes to render each light .eld 
approximation term using multitexturing graphics hardware that supports 2 texture sources. Without multi­texturing 
hardware support we can implement the rendering algo­rithms described above using an accumulation buffer, 
though sig­ni.cantly less ef.ciently. For the PCA-based approximation, which in general will pro­duce 
light .eld maps that contain negative values, rendering can bene.t from graphics hardware that permits 
a change to the lim­its of the color range from the traditional .0.1.range to .min.max., e.g., ..1.1.. 
NVIDIA cards support extended color range through the register combiners extension [35] but they still 
clamp the nega­tive output to zero. This means that we can render PCA-based ap­proximations using register 
combiners at the expense of doubling the rendering passes as follows. Let M be the result of modulation 
of two texture fragments A and B using register combiners. Let M.be the result of clamped modulation 
of fragments A and B and let M.be the result of modulating .A and B the same way. We can compute M by 
subtracting the outputs of the two modulations M .M..M.. Even when the graphics hardware does not support 
any extended pixel range we can still render PCA-based approxima­tions, though the number of rendering 
passes required to evaluate one approximation term increases to 4. PCA-based matrix factorization requires 
that we subtract the mean view from the light .eld matrix before performing the de­composition. It changes 
the rendering routine only slightly we .rst texture map the triangle using its mean view and then add 
the approximation terms of the modi.ed light .eld matrix exactly as it was done before. One of the advantages 
of extracting the mean view and rendering it separately is that in many cases this view rep­resents an 
approximation to the diffuse component of the surface material and, as such, is interesting to visualize 
independently from the view-dependent component of the light .eld data.  ............................ 
Approximation through matrix factorization described in Sec­tion 2.2 can be thought of as a compression 
method that removes lo­cal redundancy present in the light .eld data. The compression ratio Figure 4: 
Surface maps (left) and view maps (right) computed using PCA-based approximation for the bust model. 
The lower portion of the left image represents the extracted mean textures. All other light .eld maps 
are stored in ..1.1.range, where .1 is mapped to black.  of this method is closely related to the size 
of the surface primitives used for partitioning. On the one hand, a dense mesh with many vertices will 
have unnecessarily large number of view maps. On the other hand, a coarse mesh requires more approximation 
terms for the same approximation quality. See Section 6.1 for the analysis of the relationship of the 
compression ratio and the approximation quality to the triangle size. Currently, we choose the size of 
trian­gles empirically so that we obtain about two orders of magnitude compression ratio through approximation 
while maintaining high quality without using many approximation terms. Section 6.2 gives a detailed analysis 
of compression ratios obtained through the ap­proximation. Figure 4 shows, for the bust model, surface 
maps (left) and view maps (right) computed using PCA-based approximation. It is easy to see that the 
light .eld maps are still redundant. First, the individ­ual maps are similar to each other, suggesting 
global redundancy of the data. Second, some of the light .eld maps have very little information content 
and can be compressed further using a variety of existing image compression techniques. Figure 5 gives 
an overview of the different levels of compression we apply to the light .eld data. For optimal run-time 
performance, compressed light .eld maps need to .t entirely in the texture mem­ory cache and be decompressed 
on-the-.y during rendering. This process should only introduce minimal run-time memory overhead. In the 
following paragraphs we discuss several techniques that sat­isfy these criteria. Other image compression 
techniques can be used to further reduce the of.ine storage size, as shown in Figure 5, but are not discussed 
in the paper. Data redundancy across individual light .eld maps can be re­duced effectively using VQ 
[13]. Our implementation represents vj each triangle surface map g .i .rp.sp.and each view map hvj ..q.fq. 
as a vector. The algorithm groups these vectors based on their size and generates a separate codebook 
for every group. We initialize the codebooks using either pairwise nearest neighbor or split algo­rithm. 
The codebooks are improved by the generalized Lloyd algo­rithm utilizing square Euclidean distance as 
the cost function. We then store the resulting codebooks as images. The rendering algo­rithm from VQ-compressed 
images does not change in any way it simply indexes a different set of images. We use either a user-speci.ed 
compression ratio or the average distortion to drive the VQ compression. With the distortion-driven algorithm, 
the light .eld maps corresponding to the higher approx­imation terms exhibit more redundancy and thus 
are often com­pressed into a smaller codebook. In practice, light .eld maps can be compressed using VQ 
by an order of magnitude without signi.­cant loss of quality. Data redundancy within individual light 
.eld maps can be re­duced ef.ciently using block-based algorithms. One such method, called S3TCTM, is 
often supported on commodity graphics cards Figure 5: Compression Overview. The number under each tech­nique 
describes its approximate compression ratio.  today. It offers compression ratios between 6:1 and 8:1 
and can be cascaded with VQ for further size reduction. Limited by hardware implementation cost, these 
algorithms are not very sophisticated in nature. For example, the S3TC algorithm divides the textures 
into 4-by-4 texel blocks and performs color interpolation within each block. Since the algorithm uses 
blocks that are smaller than most light .eld maps, when compared to VQ, it generates noisier images but 
it preserves the specularities better. For PCA-based approximation, we observe that the light .eld maps 
associated with higher approximation terms contain lower spatial frequency. To test if we can reduce 
the resolution of these light .eld maps without signi.cant impact on the rendering qual­ity, we implemented 
a simple method that subsamples the image resolution uniformly within each term. The results, although 
not reported in the paper, proved effective. A more sophisticated ap­proach would apply selective resolution 
reduction to each light .eld map, or simply reduce light .eld matrix resolution during resam­pling. 
 ...................... This section describes the following three implementation related issues: the 
acquisition of the geometry and the radiance data of physical objects, resampling of the radiance data, 
and tiling of the light .eld maps. ................. Figure 6 gives an illustration of the overall data 
acquisition process. A total of NI (200 .NI .400) images are captured with a hand­held digital camera 
(Fig. 6a). Figure 6b shows one sample image. Observe that the object is placed on a platform designed 
for the purpose of automatic registration. The color circles are .rst auto­matically detected on the 
images using a simple color segmentation scheme. This provides an initial guess for the position of the 
grid corners that are then accurately localized using a corner .nder. The precise corner locations are 
then used to compute the 3D position of the camera relative to the object. This may be done, given that 
the camera has been pre-calibrated. The outcome of this process is a set of NI images captured from known 
vantage points in 3D space. The object geometry is computed using a structured lighting sys­tem consisting 
of a projector and a camera. The two devices are visible in Figure 6a. Figure 6c shows an example camera 
image acquired during scanning. The projector is used to project a trans­lating stripped pattern onto 
the object. A similar temporal analysis employed by Curless et al. [8] and Bouguet et al. [3] is used 
for accurate range sensing. In order to facilitate scanning, the object is painted with white removable 
paint, a technique especially useful when dealing with dark, highly specular or semi-transparent ob­jects. 
We take between 10 and 20 scans to completely cover the  (a) (b) views computed by blending the original 
set of views (right). (c) (d) (e) Figure 6: Data acquisition. (a) The user is capturing approximately 
300 images of the object under a .xed lighting condition using a hand-held digital camera. (b) One sample 
image. The color circles guide the automatic detection of the grid corners that are used for computing 
the 3D location of the camera. (c) The painted object be­ing scanned using the structured lighting system. 
(d) The complete 3D triangular mesh consisting of 7228 triangles constructed from 20 scans. (e) The projection 
of the triangular mesh onto image (b). surface geometry of the object. Between two consecutive scans, 
the object is rotated in front of the camera and projector by about 20 degrees. For that purpose, the 
calibration platform has been designed to rotate about its central axis. The individual scans are automatically 
registered together in the object reference frame us­ing the same grid corners previously used for image 
registration. The resulting cloud of points (approx. 500,000 points) is then fed into mesh editing software 
to build the .nal triangular surface mesh shown in Figure 6d. Since we use the same calibration platform 
for both image and geometry acquisition, the resulting triangular mesh is naturally registered to the 
camera images. Figure 6e shows the projection of the mesh onto the camera image displayed in Fig­ure 
6b illustrating the precise alignment of the geometry and the images. The image reprojection error is 
less than one pixel. ................. Before we start resampling, we need to determine the visible cam­eras 
for each mesh triangle. A triangle is considered visible only if it is completely unoccluded. Repeating 
this process for all NI images results in a list of visible views for each triangle. The visi­ble triangle 
views correspond to a set of texture patches of irregular size captured from various viewing directions. 
We now explain the two-step process of resampling the irregularly sized views into a 4-dimensional grid. 
Recall that the approximation algorithms described in Sec­tion 2.2 assume light .eld data in the form 
of matrices. This re­quires that we normalize each texture patch to have the same shape and size as the 
others. The size of the normalized patch is chosen to be equal to the size of the largest view. Resampling 
is done using bilinear interpolation of the pixels in the original views. At this stage, we have a uniform 
number of samples for each triangle view but the sampling of views is still irregular. Having a regular 
grid of viewing directions in the light .eld matrix is very important for two reasons. As was shown in 
[6], factorization of the fully resampled matrix results in a more precise approximation and encodes 
the view-dependent information allowing us to syn­thesize a correct image for any camera location using 
the rendering algorithm that was described in Section 3. We will use Figure 7 to explain the view resampling 
step. Let vectors dvj be the viewing .i directions for the visible views of a given triangle expressed 
in the reference frame of vertex vj. We start by projecting these directions onto the xy plane of the 
reference frame of vertex vj using Equa­tions (7). The result of this operation is a set of texture coordinates 
(Figure 7, left). Next we perform the Delaunay triangulation of these coordinates (Figure 7, middle) 
and compute the regular grid of views by blending the original triangle views using the weighting factors 
obtained from the triangulation (Figure 7, right). We assume that the viewing direction is constant across 
a trian­gle, since the distance of the camera from the object during light .eld capture is generally 
quite large compared to the size of the tri­angles. The typical resolution of the view grid is 32 .32 
.1024, but it can vary from triangle to triangle and between the approxi­mation terms as explained in 
Section 4. ......................... To avoid excessive texture swapping and improve rendering ef.­ciency, 
we tile individual light .eld maps together to create col­lection of light .eld maps. To simplify the 
problem, we allow a prede.ned set of sizes for the light .eld maps during the resam­pling process, and 
then tile same-size light .eld maps together, as shown in Figure 4. Since one triangle requires three 
surface maps per approximation term, we tile all these maps in the same collec­tion. We split the geometry 
of the model into p segments so that the tiled view maps for each segment do not exceed maximum texture 
size allowed. We then produce one view map collection per seg­ment: .V1.V2 ....Vp.. Let .S1i .Si .....Si 
.be the list of surface map 2qi collections for vertex map collection Vi. For each approximation term, 
the rendering algorithm is as follows for i .1.....p do load view map collection Vi into texture unit 
1 for j .1.....qi do load surface map collection Sij into texture unit 2 render all triangles with surface 
maps in collection Sij end for end for   ........ We have acquired the surface light .elds and computed 
the light .eld map representations for four physical objects with diverse and Table 1: The sizes of the 
experimental data sets. The second to last column shows the total size of renormalized triangles. The 
last column shows the size of the resampled surface light .eld data used for the decomposition. Models 
Triangles Number of Images Raw Image Size Resampled Data Size Bust 6531 339 289 MB 5.1 GB Dancer 6093 
370 213 MB 2.9 GB Star 4960 282 268 MB 5.2 GB Turtle 3830 277 233 MB 3.8 GB complex re.ection properties, 
including the metallically-painted clay bust in Figure 13, dancer in Figure 12, semi-transparent and 
anisotropically re.ective glass star in Figure 14, and the furry toy turtle in Figure 11. We also constructed 
a synthetic museum room as shown in Figure 1. The scene has approximately 60K polygons. Using 1-term 
approximation, we can render it at 18 frames per sec­ond using the GeForceTM 3 graphics card on a 2GHz 
PentiumTM 4 PC. .......................... Table 1 provides information about the data sizes for the 
scanned models used in our experiments. We use 24-bit RGB images in all the experiments. The raw image 
size in this table represents the total amount of radiance data after the visibility computation and 
triangle normalization process as described in Section 5. When re­sampling the viewing directions we 
assume a resolution of 32 .32. Note that, traditionally, research on light .eld compression reports results 
based on the size of the resampled light .eld data [22; 23]. The compression ratios reported in the next 
section are computed based on the size of resampled data. Figure 8 illustrates the relationship of the 
compression ratio and the approximation quality to the triangle size for the bust object. As seen from 
the .gure, smaller triangles provide better approximation quality but lower compression ratio. It is 
also interesting that for the same target error larger triangles always give a better compression ratio. 
Note also that smaller triangles are slower to render for each approximation term. Depending on the target 
hardware platform, user may choose to reduce the number of triangles while increas­ing the number of 
required passes, or vice versa, to achieve desired frame rate and compression ratio. For the following 
experiments, based on the goal of 100:1 compression obtained for the 3 term ap­proximation with no compression 
of light .eld maps, we choose mesh resolution that gives us approximately 314 samples per trian­gle for 
all our models. .............................. We measure the quality of the surface light .eld approximation 
us­ing RMS error computed between the resampled light .eld ma­trix and the matrix reconstructed from 
the approximation. Fig­ure 9 shows PSNR (Peak Signal-to-Noise Ratio) for PCA-based and NMF-based approximations. 
The results show that both tech­niques produce high quality approximations using few approxima­tion terms. 
Note that the scale in Figure 9 is logarithmic and that all approximations that use more than 3 terms 
have the RMS error of less than 2.0. This result provides a quantitative proof of the effec­tiveness 
of light .eld approximation through matrix factorization. Between the two algorithms, PCA produces better 
quality than NMF, however, the difference is visually almost indistinguishable. Note that the frame buffer 
quantization that occurs during each tex­ture blending operation is not considered in Figure 9. This 
quanti­zation error implies that an approximation beyond 48dB would not improve the quality of hardware-accelerated 
renderer. Compression Ratio (PCA Only) Figure 8: Relationship of the compression ratio and the approxi­mation 
quality to the triangle size for the bust object. The points on each curve correspond to the different 
number of approximation terms for a given mesh resolution. Coarser mesh has more samples per triangle. 
Table 2 lists the size and compression ratio of the light .eld data obtained through the light .eld data 
approximation and the addi­tional compression of the light .eld maps. The size of geometric data falls 
below 10KB for all objects listed in the table when com­pressed using topological surgery [36] and therefore 
is negligible compared to the size of light .eld maps. By combining VQ with S3TC hardware texture compression, 
we achieve a run-time com­pression ratio of over 5000:1 for a 3-term approximation. For inter­active 
purposes, 1-term approximation is often suf.cient and thus the resulting compression ratio approaches 
4 orders of magnitude. ............ Figure 10 compares the rendering performance of PCA-based and NMF-based 
approximations. NMF-based rendering is 50% faster than PCA-based for the same number of approximation 
terms. The performance disparity will be larger if the target platform only supports positive texture 
values. The rendering performance of our algorithm is not very sensitive to the size of light .eld map 
data doubling the image size reduces the frame rate by less than 20%. Rendering from compressed and uncompressed 
light .elds is equally fast if image sets in both cases .t into the texture memory. Figures 11-14 compare 
the rendering quality of our routines against the input images and report the corresponding errors. The 
errors reported in the .gures are computed based on the difference between the input image and the rendered 
image using both APE (average pixel error) and PSNR for the foreground pixels only. Cur­rently we discard 
partially visible triangles in the resampling pro­cess, which also contributes to the error. In the future, 
we plan to address the problem of partially occluded triangles by looking at factor analysis algorithms 
that use data statistics to .ll in missing information [32].  ....................... We have developed 
a new representation of surface light .elds and demonstrated its effectiveness on both synthetic and 
real data. Us­ing our approach, surface light .elds can be compressed several thousand times and ef.ciently 
rendered at interactive speed on mod­ern graphics hardware directly from their compressed representa­tion. 
Simplicity and compactness of the resulting representation leads to a straightforward and fully hardware-accelerated 
render­ing algorithm. Additionally, we present a new type of light .eld (a) PCA Approximation Quality 
(b) NMF Approximation Quality 0.8 0.8 50 50 1.0 1.0 48 48 1.3 1.3 46 1.6 1.6 44  1-term 2-term 
2.0 2.0 42 3-term 2.6 2.6 40 4-term 3.2 3.2 38  RMS (8 bit/channel) PSNR (dB) PSNR (dB)  5-term 
4.0 4.0 36 5.1 5.1 6.4 6.4  34 34 32 32 8.1 8.1 30 30 Bust Dancer Star Turtle Bust Dancer Star Turtle 
Figure 9: Approximation quality for different models and differ­ent number of decomposition terms. PSNR 
and RMS are based on the weighted average of the approximation errors for all light .eld matrices. (a) 
PCA Rendering Speed (b) NMF Rendering Speed 180 180 tween the input images shown at the top row and the 
images synthe­sized from the 1-term PCA approximation compressed using both 160160 140140 VQ and S3TC 
shown at the bottom row. APE = 9.5, PSNR = 25.5 120 120  dB, the compression ration is 8202:1, and 
the size of compressed light .eld maps is 468 KB.  ......... [1] C. M. Bishop. Neural Networks for Pattern 
Recognition. Clarendon Press, 1995. [2] S. Boivin and A. Gagalowicz. Image-Based Rendering of Diffuse, 
Specular and Glossy Surfaces From a Single Image. Proceedings of SIGGRAPH 2001, pages 107 116, August 
2001. [3] J-Y. Bouguet and P. Perona. 3D Photography Using Shadows in Dual-Space Geometry. International 
Journal of Computer Vision, 35(2):129 149, December 1999. [4] C. Buehler, M. Bosse, L. McMillan, S. J. 
Gortler, and M. F. Cohen. Unstruc­tured Lumigraph Rendering. Proceedings of SIGGRAPH 2001, pages 425 
432, August 2001. [5] J-X. Chai, X. Tong, S-C. Chan, and H-Y. Shum. Plenoptic Sampling. Proceed­ings 
of SIGGRAPH 2000, pages 307 318, July 2000. [6] W-C. Chen, R. Grzeszczuk, and J-Y. Bouguet. Light Field 
Mapping: Hardware­accelerated Visualization of Surface Light Fields. Published as part of Acquisi­tion 
and Visualization of Surface Light Fields, SIGGRAPH 2001 Course Notes for Course 46, pages 410-416, August 
2001. [7] R. L. Cook and K. E. Torrance. A Re.ectance Model for Computer Graphics. ACM Transactions on 
Graphics, 1(1):7 24, January 1982. [8] B. Curless and M. Levoy. Better Optical Triangulation through 
Spacetime Anal­ysis. Proc. 5th Int. Conf. Computer Vision, Boston, USA, pages 987 993, 1995. [9] P. Debevec, 
T. Hawkins, C. Tchou, H-P. Duiker, W. Sarokin, and M. Sagar. Ac­quiring the Re.ectance Field of a Human 
Face. Proceedings of SIGGRAPH 2000, pages 145 156, July 2000. [10] P. E. Debevec, C. J. Taylor, and J. 
Malik. Modeling and Rendering Architecture from Photographs: A Hybrid Geometry-and Image-Based Approach. 
Proceed­ings of SIGGRAPH 96, pages 11 20, August 1996. [11] P. E. Debevec, Y. Yu, and G. D. Borshukov. 
Ef.cient View-Dependent Image-Based Rendering with Projective Texture-Mapping. Eurographics Rendering 
Workshop 1998, pages 105 116, June 1998. [12] A. Fournier. Separating Re.ection Functions for Linear 
Radiosity. Eurographics Rendering Workshop 1995, pages 296 305, June 1995. [13] A. Gersho and R. M. Gray. 
Vector Quantization and Signal Compression. Kluwer Academic Publishers, 1992. [14] S. J. Gortler, R. 
Grzeszczuk, R. Szeliski, and M. F. Cohen. The Lumigraph. Proceedings of SIGGRAPH 96, pages 43 54, August 
1996. [15] W. Heidrich and H-P. Seidel. Realistic, Hardware-Accelerated Shading and Lighting. Proceedings 
of SIGGRAPH 99, pages 171 178, August 1999. [16] J. Kautz and M. D. McCool. Interactive Rendering with 
Arbitrary BRDFs using Separable Approximations. Eurographics Rendering Workshop 1999, June 1999. [17] 
J. Kautz and H-P. Seidel. Towards Interactive Bump Mapping with Anisotropic Shift-Variant BRDFs. 2000 
SIGGRAPH / Eurographics Workshop on Graphics Hardware, pages 51 58, August 2000. [18] J.J. Koenderink 
and A.J. van Doorn. Bidirectional Re.ection Distribution Func­tion Expressed in Terms of Surface Scattering 
Modes. In European Conference on Computer Vision, pages II:28 39, 1996. [19] E. P. F. Lafortune, S-C. 
Foo, K. E. Torrance, and D. P. Greenberg. Non-Linear 100 100 FPS 80 80 60 60 40 40 20 20 0 0 Figure 
10: Rendering performance using NVIDIA GeForce 3 graphics card on a 2GHz Pentium 4 PC displayed at 1024 
.768 window with objects occupying approximately 1/3 of the window. data factorization that produces 
positive only factors. This method allows faster rendering using commodity graphics hardware. Fur­thermore, 
the paper contains a detailed explanation of the data ac­quisition and preprocessing steps, providing 
a description of the complete modeling and rendering pipeline. Finally, our PCA-based approximation technique 
is particularly useful for network trans­port and interactive visualization of 3D photography data because 
it naturally implies progressive transmission of radiance data. One of the limitations of a surface light 
.eld is that it models only the outgoing radiance of a scene. Consequently it is not possi­ble to use 
surface light .elds to render dynamic scenes where light­ing changes and objects move. In the future, 
we are planning to work on extending the approach presented in this paper to relighting and animation 
of image-based models. If successful, these results would prove that image-based modeling and rendering 
is a practi­cal and an appealing paradigm for enhancing the photorealism of interactive 3D graphics. 
 .............. We thank Srihari Sukumaran for helping with the early version of the resampling and 
tiling algorithm. We are indebted to Omid Moghadam for his support in making light .eld mapping part 
of the MPEG4 standard. We thank Gary Bishop, Bernd Girod and Pat Hanrahan for critical reads of this 
paper. Timely proofreading by Gordon Stoll is greatly appreciated. We thank Henry Fuchs, Anselmo Lastra, 
Ara Ne.an, Lars Nyland and Herman Towles for helpful suggestions and discussions and Gary Bradski, Bob 
Liang and Justin Rattner for encouraging this work. This research is in part supported by NSF Cooperative 
Agreement Science and Tech­nology Center for Computer Graphics and Scienti.c Visualization , National 
Teleimmersion Initiative funded by Advanced Networks and Services, and Link Foundation Fellowships. Approximation 
of Re.ectance Functions. Proceedings of SIGGRAPH 97, pages 117 126, August 1997. [20] D. D. Lee and H. 
S. Seung. Learning the Parts of Objects by Non-Negative Matrix Factorization. Nature, 401:788 791, 1999. 
[21] Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter Seidel. Image-Based 
Reconstruction of Spatially Varying Materi­als. In Twelveth Eurographics Rendering Workshop 2001, pages 
104 115. Euro­graphics, June 2001. [22] M. Levoy and P. Hanrahan. Light Field Rendering. Proceedings 
of SIGGRAPH 96, pages 31 42, August 1996. [23] M. Magnor and B. Girod. Data Compression for Light Field 
Rendering. IEEE Trans. Circuits and Systems for Video Technology, 10(3):338 343, April 2000. [24] T. 
Malzbender, D. Gelb, and H. Wolters. Polynomial Texture Maps. Proceedings of SIGGRAPH 2001, pages 519 
528, August 2001. [25] Michael D. McCool, Jason Ang, and Anis Ahmad. Homomorphic Factorization of BRDFs 
for High-Performance Rendering. Proceedings of SIGGRAPH 2001, pages 171 178, August 2001. [26] L. McMillan 
and G. Bishop. Plenoptic Modeling: An Image-Based Rendering System. Proceedings of SIGGRAPH 95, pages 
39 46, August 1995. [27] G. S. P. Miller, S. Rubin, and D. Ponceleon. Lazy Decompression of Surface Light 
Fields for Precomputed Global Illumination. Eurographics Rendering Workshop 1998, pages 281 292, June 
1998. [28] K. Nishino, Y. Sato, and K. Ikeuchi. Eigen-Texture Method: Appearance Com­pression Based on 
3D Model. In Proceedings of the IEEE Computer Science Conference on Computer Vision and Pattern Recognition 
(CVPR-99), pages 618 624, 1999. [29] P. Poulin and A. Fournier. A Model for Anisotropic Re.ection. Proceedings 
of SIGGRAPH 90, 24(4):273 282, August 1990. [30] K. Pulli, M. Cohen, T. Duchamp, H. Hoppe, L. Shapiro, 
and W. Stuetzle. View­based Rendering: Visualizing Real Objects from Scanned Range and Color Data. Eurographics 
Rendering Workshop 1997, pages 23 34, June 1997. [31] R. Ramamoorthi and P. Hanrahan. A Signal-Processing 
Framework for Inverse Rendering. Proceedings of SIGGRAPH 2001, pages 117 128, August 2001. [32] Sam Roweis. 
EM algorithms for PCA and SPCA. In Advances in Neural Infor­mation Processing Systems, volume 10. The 
MIT Press, 1998. [33] Y. Sato, M. D. Wheeler, and K. Ikeuchi. Object Shape and Re.ectance Modeling from 
Observation. Proceedings of SIGGRAPH 97, pages 379 388, August 1997. [34] P. Schr¨oder and W. Sweldens. 
Spherical Wavelets: Ef.ciently Representing Functions on the Sphere. Proceedings of SIGGRAPH 95, pages 
161 172, August 1995. [35] J. Spitzer. Texture Compositing With Register Combiners. Game Developers Conference, 
April 2000. [36] G. Taubin and J. Rossignac. Geometric Compression Through Topological Surgery. ACM Transactions 
on Graphics, 17(2):84 115, April 1998. [37] K. E. Torrance and E. M. Sparrow. Polarization, Directional 
Distribution, and Off-Specular Peak Phenomena in Light Re.ected from Roughened Surfaces. Journal of Optical 
Society of America, 56(7), 1966. [38] G. J. Ward. Measuring and Modeling Anisotropic Re.ection. Proceedings 
of SIGGRAPH 92, 26(2):265 272, July 1992. [39] D. N. Wood, D. I. Azuma, K. Aldinger, B. Curless, T. Duchamp, 
D. H. Salesin, and W. Stuetzle. Surface Light Fields for 3D Photography. Proceedings of SIG-GRAPH 2000, 
pages 287 296, July 2000. [40] Y. Yu, P. E. Debevec, J. Malik, and T. Hawkins. Inverse Global Illumination: 
Recovering Re.ectance Models of Real Scenes From Photographs. Proceedings of SIGGRAPH 99, pages 215 224, 
August 1999.  ........................... Let F be a matrix of size M .N. The goal of matrix factor­ization 
leading to Equation (5) is to compute an M .K matrix U ..u1 ...uK .and an N .K matrix V ..v1 ...vK .such 
that the matrix F..UVT (8) best approximates F in the least squares sense. The PCA and NMF factorizations 
are two algorithms achieving this goal while enforc­ing different constraints on the vectors uk and vk. 
PCA enforces the vectors uk and vk to be orthogonal and keeps the factorization progressive that is, 
once the order K factorization is computed, the K-1 .rst pairs of vectors provide the best approximation 
at the order K-1. NMF, on the other hand, enforces vectors uk and vk to have all positive entries. Unlike 
PCA, NMF produces a non­progressive factorization. In other words, if a different approxima­tion order 
K is chosen, the overall matrices U and V have to be recomputed. The next two sections describe the implementation. 
............ The PCA factorization is based on computing the partial singular value decomposition of 
matrix F. Since only the .rst K .N eigen­vectors v1.....vK of matrix A .FT F need to be computed, the 
power iteration algorithm is well suited to achieve this goal. The pseudo-code for this algorithm looks 
as follows: for p .1.....K do Fp .F ..p.1 k.1 ukvT k Ap .FTp Fp Initialize vp .random N .1 non-zero 
vector repeat vp .Apvp vp .vp..vp. until vp converges . .p .Apvp up .Fpvp..p maxu .max of the absolute 
values of all the entries of up maxv .max of the absolute values of all the entries of vp . a .maxu.p.maxv 
up ..pup.a vp .avp Quantize the entries of up and vp for texture storage end for The coef.cient a introduced 
in the code helps splitting the multi­plicative factor .p among the vectors up and vp so as to minimize 
the maximum quantization error. ............ The NMF factorization achieves the same matrix decomposition 
while enforcing all entries of the two matrices U and V to be posi­tive. We propose to apply the algorithm 
presented by Lee et al. [20] to compute U and V iteratively: Initialize U .random M .K matrix of all 
strictly positive entries Initialize V .random N .K matrix of all strictly positive entries repeat Un1 
..un1 .i.j...FV Un2 ..un2 .i.j...UVT V Un ..un.i.j....un1 .i.j..un2 .i.j.. Vn1 ..vn1 .i.j...FT U Vn2 
..vn2 .i.j...VUT U Vn ..vn.i.j....vn1 .i.j..vn2 .i.j.. U . .u.i.j....u.i.j..un.i.j.. V . .v.i.j.....v.i.j..vn.i.j... 
u.i.j...M U . .u.i.j...r.1 u.r.j. until U and V converge Table 2: The size and compression ratio of the 
radiance data obtained through the light .eld map approximation and additional compression of the surface 
light .eld maps. Models Light Field Maps (3-term) Compression of Light Field Maps VQ S3TC VQ+S3TC Bust 
47.7 MB (106:1) 5.7 MB (885:1) 8.0 MB (636:1) 951 KB (5475:1) Dancer 35.7 MB (82:1) 5.4 MB (542:1) 5.9 
MB (490:0) 904 KB (3303:1) Star 42.3 MB (122:1) 7.2 MB (716:1) 7.0 MB (737:1) 1.2 MB (4301:1) Turtle 
31.7 MB (121:1) 4.5 MB (847:1) 5.3 MB (726:1) 755 KB (5084:1)  Photograph 4-term PCA 4-term NMF 2-term 
PCA 2-term NMF APE=5.42, PSNR=27.63 dB APE=5.82, PSNR=27.35 dB APE=6.43, PSNR=26.77 dB APE=6.76, PSNR=26.54dB 
46.4 MB(63:1) 46.4 MB(63:1) 24.9 MB(117:1) 24.9 MB(117:1) Photograph 3-term PCA 3-term PCA+VQ 3-term 
PCA+VQ+S3TC Triangle Mesh APE=4.57, PSNR=31.04 dB APE=6.98, PSNR=27.90 dB APE=7.49, PSNR=27.51 dB 47.7 
MB (106:1) 5.7MB (885:1) 951KB (5475:1) Photograph 4-term PCA 3-term PCA 2-term PCA 1-term PCA APE=4.79, 
PSNR=30.48 dB APE=5.28, PSNR=29.85 dB APE=5.99, PSNR=28.95 dB APE=7.34, PSNR=27.51 dB 54.4MB (95:1) 42.3MB 
(122:1) 30.2MB (171:1) 18.1MB (285:1) Figure 14: The .gure demonstrates the progressive nature of PCA 
approximation. The same star model is rendered using different number of approximation terms.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566602</article_id>
		<sort_key>457</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Feature-based light field morphing]]></title>
		<page_from>457</page_from>
		<page_to>464</page_to>
		<doi_number>10.1145/566570.566602</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566602</url>
		<abstract>
			<par><![CDATA[We present a feature-based technique for morphing 3D objects represented by light fields. Our technique enables morphing of image-based objects whose geometry and surface properties are too difficult to model with traditional vision and graphics techniques. Light field morphing is not based on 3D reconstruction; instead it relies on <i>ray correspondence,</i> i.e., the correspondence between rays of the source and target light fields. We address two main issues in light field morphing: feature specification and visibility changes. For feature specification, we develop an intuitive and easy-to-use user interface (UI). The key to this UI is <i>feature polygons,</i> which are intuitively specified as 3D polygons and are used as a control mechanism for ray correspondence in the abstract 4D ray space. For handling visibility changes due to object shape changes, we introduce <i>ray-space warping.</i> Ray-space warping can fill arbitrarily large holes caused by object shape changes; these holes are usually too large to be properly handled by traditional image warping. Our method can deal with non-Lambertian surfaces, including specular surfaces (with dense light fields). We demonstrate that light field morphing is an effective and easy-to-use technqiue that can generate convincing 3D morphing effects.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D morphing]]></kw>
			<kw><![CDATA[feature polygons]]></kw>
			<kw><![CDATA[global visibility map]]></kw>
			<kw><![CDATA[light field]]></kw>
			<kw><![CDATA[ray correspondence]]></kw>
			<kw><![CDATA[ray-space warping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Morphological</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP95044884</person_id>
				<author_profile_id><![CDATA[81451593799]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhunping]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F, Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14085446</person_id>
				<author_profile_id><![CDATA[81408602731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lifeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F, Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14040907</person_id>
				<author_profile_id><![CDATA[81100085615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Baining]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F, Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14159670</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F, Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Thaddeus Beier and Shawn Neely. Feature-based image metamorphosis. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):35-42, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bryan P. Bergeron. Morphing as a means of generating variability in visual medical teaching materials. Computers in Biology and Medicine, 24:11-18, January 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383309</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chris Buehler, Michael Bosse, Leonard McMillan, Steven J. Gortler, and Michael F. Cohen. Unstructured lumigraph rendering. Proceedings of SIGGRAPH 2001, pages 425-432, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. Proceedings of SIGGRAPH 93, pages 279-288, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274366</ref_obj_id>
				<ref_obj_pid>274363</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Daniel Cohen-Or, Amira Solomovici, and David Levin. Three-dimensional distance field metamorphosis. ACM Transactions on Graphics, 17(2):116-141, April 1998. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>241075</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Douglas DeCarlo and Jean Gallier. Topological evolution of surfaces. Graphics Interface '96, pages 194-203, May 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237278</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Julie Dorsey and Pat Hanrahan. Modeling and rendering of metallic patinas. Proceedings of SIGGRAPH 96, pages 387-396, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Alexei A. Efros and William T. Freeman. Image quilting for texture synthesis and transfer. Proceedings of SIGGRAPH 2001, pages 341-346, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Oliver Faugeras. 3D Computer Vision. The MIT Press, Cambridge, MA, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>289353</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jonas Gomes, Bruno Costa, Lucia Darsa, and Luiz Velho. Warping and Morphing of Graphics Objects. Morgan Kaufmann, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. Proceedings of SIGGRAPH 96, pages 43-54, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897925</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Arthur Gregory, Andrei State, Ming C. Lin, Dinesh Manocha, and Mark A. Livingston. Interactive surface decomposition for polyhedral morphing. The Visual Computer, 15(9):453-470, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134004</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[John F. Hughes. Scheduled fourier volume morphing. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):43-46, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[James R. Kent, Wayne E. Carlson, and Richard E. Parent. Shape transformation for polyhedral objects. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):47-54, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Francis Lazarus and Anne Verroust. Three-dimensional metamorphosis: a survey. The Visual Computer, 14(8-9):373-389, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311586</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Aaron Lee, David Dobkin, Wim Sweldens, and Peter Schr&#246;der. Multiresolution mesh morphing. Proceedings of SIGGRAPH 99, pages 343-350, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218502</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Apostolos Lerios, Chase D. Garfinkle, and Marc Levoy. Feature-based volume metamorphosis. Proceedings of SIGGRAPH 95, pages 449-456, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258911</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy. Expanding the horizons of image-based modeling and rendering. In SIGGRAPH 97 Panel:Image-Based Rendering:Really New or Deja Vu, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy and Pat Hanrahan. Light field rendering. Proceedings of SIGGRAPH 96, pages 31-42, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Wojciech Matusik, Chris Buehler, Ramesh Raskar, Steven J. Gortler, and Leonard McMillan. Image-based visual hulls. In Proceedings of ACM SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, pages 369-374, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Steven M. Seitz and Charles R. Dyer. View morphing: Synthesizing 3d metamorphoses using image transforms. Proceedings of SIGGRAPH 96, pages 21-30, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>939187</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Steven M. Seitz and Kiriakos N. Kutulakos. Plenoptic image editing. In ICCV98, pages 17-24, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Jonathan Shade, Steven J. Gortler, Li wei He, and Richard Szeliski. Layered depth images. In Proceedings of SIGGRAPH 98, Computer Graphics Proceedings, Annual Conference Series, pages 231-242, Orlando, Florida, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311573</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Heung-Yeung Shum and Li-Wei He. Rendering with concentric mosaics. Proceedings of SIGGRAPH 99, pages 299-306, August 1999. ISBN 0-20148-560-5. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L&#225;szl&#243; Szirmay-Kalos and Werner Purgathofer. Global ray-bundle tracing with hardware acceleration. Eurographics Rendering Workshop 1998, pages 247-258, June 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[George Wolberg. Image morphing: a survey. The Visual Computer, 14(8-9):360-372, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Feature-Based Light Field Morphing Zhunping Zhang Lifeng Wang Baining Guo Heung-Yeung Shum Microsoft 
Research Asia.  Figure 1: Light .eld morphing: A 3D morphing sequence from a furry toy cat (real object) 
to the Stanford bunny (synthetic object). Abstract We present a feature-based technique for morphing 
3D objects rep­resented by light .elds. Our technique enables morphing of image­based objects whose geometry 
and surface properties are too dif.­cult to model with traditional vision and graphics techniques. Light 
.eld morphing is not based on 3D reconstruction; instead it relies on ray correspondence, i.e., the correspondence 
between rays of the source and target light .elds. We address two main issues in light .eld morphing: 
feature speci.cation and visibility changes. For feature speci.cation, we develop an intuitive and easy-to-use 
user interface (UI). The key to this UI is feature polygons, which are intuitively speci.ed as 3D polygons 
and are used as a control mechanism for ray correspondence in the abstract 4D ray space. For handling 
visibility changes due to object shape changes, we in­troduce ray-space warping. Ray-space warping can 
.ll arbitrarily large holes caused by object shape changes; these holes are usually too large to be properly 
handled by traditional image warping. Our method can deal with non-Lambertian surfaces, including specular 
surfaces (with dense light .elds). We demonstrate that light .eld morphing is an effective and easy-to-use 
technqiue that can gener­ate convincing 3D morphing effects. Keywords: 3D morphing, light .eld, ray correspondence, 
feature polygons, global visibility map, ray-space warping 1 Introduction .3F, Beijing Sigma Center, 
No. 49, Zhichun Road, Haidian District, Beijing 100080, P R China. email:bainguo@microsoft.com Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
Metamorphosis, or morphing, is a popular technique for visual ef­fects. When used effectively, morphing 
can give a compelling illu­sion that an object is smoothly transforming into another. Following the success 
of image morphing [1, 26], graphics researchers have developed a variety of techniques for morphing 3D 
objects [15, 10]. These techniques are designed for geometry-based objects, i.e., ob­jects whose geometry 
and surface properties are known, either ex­plicitly as for boundary-based techniques (e.g., [14, 6, 
16]) or im­plicitly as for volume-based techniques (e.g., [13, 17, 5]). In this paper, we describe a 
feature-based morphing technique for 3D objects represented by light .elds/lumigraphs [19, 11]. Un­like 
traditional graphics rendering, light .eld rendering generates novel views directly from images; no knowledge 
about object ge­ometry or surface properties is assumed [19]. Light .eld morphing thus enables morphing 
between image-based objects, whose geom­etry and surface properties, including surface re.ectance, hypertex­ture, 
and subsurface scattering [7], may be unknown or dif.cult to model with traditional graphics techniques. 
The light .eld morphing problem can be stated as follows: Given the source and target light .elds ..and 
..representing objects .. and .., construct a set of intermediate light .elds ........ ..that smoothly 
transforms ..into .., with each ..representing a plausible object ..having the essential features of 
..and ... We call the intermediate light .eld ..a light .eld morph, or simply a morph. A naive approach 
to light .eld morphing is to apply image mor­phing to individual images in the source and target light 
.elds and assemble the light .eld morphs from the intermediate images of image morphing. Unfortunately, 
this approach will fail for a fun­damental reason: light .eld morphing is a 3D morphing and image morphing 
is not. This difference manifests itself when a hidden part of the morphing object becomes visible because 
of object shape change, as image morphing will produce ghosting that betrays a compelling 3D morphing. 
The plenoptic editing proposed by Seitz and Kutulakos [22] rep­resents another approach to image-based 
3D morphing. They .rst recover a 3D voxel model from the image data and then apply traditional 3D warping 
to the recovered model. The visibility is­sues can be resolved with the recovered geometry, but there 
are problems, including the Lambertian surface assumption needed for voxel carving [22] and the dif.culties 
with recovering detailed ge­ometry. Most of the problems are related to the fundamental dif.­culties 
of recovering surface geometry from images [9]. UI Component Morphing Component Figure 2: Overview 
of light .eld morphing. The overall pipeline is illustrated in the upper part, whereas the warping of 
a light .eld is detailed in the lower part. Light .eld morphing is an image-based 3D morphing technique 
that is not based on 3D surface reconstruction. The basis of light .eld morphing is ray correspondence, 
i.e., the correspondence be­tween rays of the source and target light .elds [19]. The role of ray correspondence 
in light .eld morphing is the similar to that of vertex correspondence in geometry-based 3D morphing 
[15, 16]. Like vertex correspondence (e.g., see [6, 12]), ray correspondence is controlled by user-speci.ed 
feature elements. A key issue in light .eld morphing is thus the construction of a user interface (UI) 
for specifying feature elements. Since there is no intrinsic solution to a morphing problem, user interaction 
is essential to the success of any morphing system [17, 26, 15]. For light .eld morphing, the main challenge 
is the design of intuitive feature elements for an abstract 4D ray space [19]. To address this challenge, 
we introduce feature polygons as the central feature ele­ments for light .eld morphing. As 3D polygons, 
feature polygons are intuitive to specify. More importantly, feature polygons parti­tion a light .eld 
into groups of rays. The rays associated with a feature polygon .constitute a ray bundle, and the ray 
correspon­dence of this ray bundle is controlled by the control primitives of the feature polygon .. 
Note that feature polygons do not make a rough geometry of the underlying object; they are needed only 
at places where visibility changes (due to object shape change). Another key issue in light .eld morphing, 
and more generally in image-based 3D morphing, is visibility change. Two types of visibility change exist. 
The .rst is due to viewpoint changes. In light .eld morphing, this type of visibility change is automatically 
taken care of by the input light .elds. The second type of visibil­ity change is that caused by object 
shape changes, and this is an issue we must handle. For a given view, a hole is created when a hidden 
surface patch in the source light .eld ..becomes visible in the target light .eld ..due to object shape 
change. This type of hole may be arbitrarily large and thus cannot be dealt with properly by traditional 
image warping methods (e.g., [4, 21]). We solve this problem using a novel technique called ray-space 
warping, which is inspired by Beier and Neely s image warping [1]. With ray-space warping we can .ll 
a hole by approximating an occluded ray with the nearest visible ray . Not surprisingly, ray-space warping 
re­quires visibility processing and the key to visibility processing is the global visibility map, which 
associates each light .eld ray with a feature polygon. Ray-space warping produces accurate results under 
the popular Lambertian surface assumption [4, 21]. For non-Lambertian sur­faces, ray-space warping tries 
to minimize the errors by using the nearest visible rays . We demonstrate that, unlike plenoptic edit­ing 
[22], our method can effectively handle non-Lambertian sur­faces, including specular surfaces. Light 
.eld morphing is easy to use and .exible. Feature speci.­cation usually takes about .....minutes and 
sparse light .elds can be used as input to save storage and computation. When the input light .elds are 
very sparse (e.g. ...images per light .eld), we call light .eld morphing key-frame morphing to empha­size 
its similarity with image morphing. Key-frame morphing may be regarded as a generalization of view morphing 
[21] because key­frame morphing allows the user to add more input images as needed to eliminate the holes 
caused by visibility changes. Note that al­though view morphing can generate morphing sequences that 
ap­pear strikingly 3D, it is not a general scheme for image-based 3D morphing because the viewpoint is 
restricted to move along a pre­scribed line. We will demonstrate results for a few applications of light 
.eld morphing, including generating 3D morphs for interactive viewing, creating animation sequences of 
a 3D morphing observed by a cam­era moving along an arbitrary path in 3D, key-frame morphing, and transferring 
textures from one 3D object to another. For computing an animation sequence of a 3D morphing, we present 
an ef.cient method that generates the sequence without fully evaluating all the morphs. The techniques 
we present can be used as visualization tools for illustration/education purposes [2], in the entertainment 
industry, and for warping/sculpting image-based objects [18, 22]. The rest of the paper is organized 
as follows. In Section 2, we give an overview of our system. Section 3 describes the speci.­cation of 
feature elements, in particular feature polygons, and vis­ibility processing. Section 4 presents the 
warping algorithms for warping a light .eld and for generating 3D animation sequences. Experimental results 
are reported in Section 5, followed by conclu­sions in Section 6.  2 Overview As shown in Fig. 2, our 
system has two main components. The .rst is a UI for specifying feature element pairs through side-by-side 
interactive displays of the source and target light .elds. We use three types of feature elements: feature 
lines, feature polygons, and background edges. The second component is a morphing unit that Figure 3: 
The user interface for feature speci.cation. On the top, windows (1) and (2) are interactive renderings 
of the source and target light .elds. Three pairs of feature polygons are drawn using wireframe rendering 
(white lines) on top of the source and target objects. The background edges are drawn as yellow polylines. 
On the bottom, windows (3) and (6) are interactive renderings of the global visibility maps, showing 
the visibility of the feature polygons using color-coded polygons. Windows (4) and (5) display the .....-planes 
of the two light .elds, with each yellow dot representing a key view used for specifying background edges. 
 automatically computes the morph ..for a given .through the following steps. First, the feature elements 
of ..are obtained by linearly interpolating those of ..and ... Second, ..and ..are warped to .and .respectively 
for feature alignment. Finally, ......is obtained by linearly interpolating the warped light .elds . 
. . and . ... Of these steps, both the .rst and last steps are simple; the warping step is the main part 
of the second component. The two most important operations in light .eld morphing are feature speci.cation 
and visibility processing. The key to feature speci.cation is the feature polygons, which are 3D (non-planar) 
polygons approximating surface patches of 3D objects. The key to visibility processing is the global 
visibility map, which associates each ray of a light .eld .with a feature polygon of .. The critical 
roles of feature polygons and global visibility maps in the warping of a light .eld .are illustrated 
in the lower part of Fig. 2. From the user-speci.ed feature polygons of ., we can compute the global 
visibility map of .. The global visibility map partitions .into ray bundles such that each feature polygon 
.is associated with a ray bundle ..... Light .eld warping is then per­formed by warping one ray bundle 
at a time using ray-space warp­ing, with the ray correspondence of a ray bundle ....determined by . s 
control primitives. Feature polygons are only needed where visibility changes. Rays not in any ray bundle 
are called background rays, which can be eas­ily treated by image warping because there is no visibility 
change involved. Notation: Following the convention of [11], we call the image plane the .....-plane 
and the camera plane the .....-plane. For a given light .eld ., we can think of .either as a collection 
of im­ages ........or as a set of rays ............. An image ........ is also called a view of the light 
.eld ..In ........, the pixel at po­sition .......is denoted as ..............., which is equivalent 
to ray .............. .  3 Features and Visibility In feature-based morphing [10], the corresponding 
features of the source and target objects are identi.ed by a pair of feature elements. In this section, 
we show how to specify such feature element pairs when the source and target objects are described by 
light .elds. We also describe visibility processing using feature polygons. 3.1 Feature Speci.cation 
The user speci.es feature element pairs using the UI shown in Fig. 3. We use three types of feature elements: 
feature lines, feature polygons, and background edges. Feature Lines: A feature line is a 3D line segment 
connecting two points called its vertices, which are also called feature points. The purpose of a feature 
line is to approximate a curve on the surface of a 3D object. The user speci.es a feature line .by identifying 
the pixel locations of its vertices. Once .is speci.ed, our system displays .on top of the interactive 
rendering of the light .eld. To determine the 3D position of a vertex ., we use geometry­guided manual 
correspondence: the user manually identi.es pro­jections .....and .....of .in two different views ........and 
........under the guidance of epipolar geometry [9]. After the user speci.es .....in view ........, the 
epipolar line of .....is drawn in view ........as a guide for specifying .....since .....must be on the 
epipolar line of ...... Because the camera parameters of both views are known, calculating .from .....and 
.....is straightforward. Feature Polygons: A feature polygon .is a 3D polygon de.ned by .feature lines 
..........., which are called the edges of .. .has control primitives ... , ..., .....which includes 
both the edges of .and supplementary feature lines ..... , ..., .....for additional control inside the 
feature polygon. The purpose of a fea­ture polygon is to approximate a surface patch of a 3D object. 
In general, .is allowed to be non-planar so that it can approximate a large surface patch as long as 
the surface patch is relatively .at. Feature Polygon P1 B Ray Bundle R(P1) Occluder Ray Bundle R(P2) 
 C1 C2 C3 Figure 4: Left: Example of ray bundles in a light .eld with three cameras. The rays associated 
with a feature polygon ..across all views of the light .eld constitute ray bundle ...... In this exam­ple, 
.....includes all pink rays but not the green rays. Right: Example of nearest visible rays in a light 
.eld with three cameras. Occluded ray ...(pink) is replaced by ...(pink), while ...is replaced by .... 
To specify a feature polygon, the user draws a series of connected feature lines (two consecutive lines 
sharing a vertex) in counter­clockwise order in the interactive display of a light .eld. A techni­cal 
dif.culty in the speci.cation process is that, because light .eld rendering does not perform visible 
surface computation, all ver­tices are visible in every view. Fortunately, our experience indicates that 
the user can easily distinguish vertices on visible surfaces from those on hidden surfaces, for two reasons. 
First, there are relatively few vertices and a vertex on the visible surface can be identi.ed by the 
landmark it labels. More importantly, the interactive display gives different motion parallax to visible 
vertices in the front and invisible ones in the back. To ensure that the patches are well approximated 
by feature poly­gons, we restrict the geometry of the patches. Speci.cally, for a sur­face patch .approximated 
by a feature polygon ., we require that .has no self-occlusion and is relatively .at. We split .if either 
requirement is not met. By requiring .to have no self-occlusion, we can avoid self-occlusion in .if it 
is a suf.ciently close approxi­mation of .. For such a ., we only have to check occlusion caused by other 
feature polygons during visibility processing. Note that whether .satis.es the two conditions is solely 
judged within the viewing range of .. For example, consider any one of the faces in Fig. 3. The surface 
patch approximated by a feature polygon has no self-occlusion for the viewing range of the light .eld 
shown. However, when the viewpoint moves beyond the viewing range of this light .eld, e.g., to the side 
of the face, the nose will cause self­occlusion within the surface patch. Background Edges: We introduce 
background edges to control rays that do not belong to any feature polygons. These rays exist for two 
reasons. First, feature polygons only roughly approximate surface patches of a 3D object. In each light 
.eld view, rays near the object silhouette may not be covered by the projection of any feature polygons. 
Second, parts of the object surface may not be affected by the visibility change caused by object shape 
change. There is no need to specify feature polygons for the corresponding rays. For rays that do not 
belong to any feature polygons, we control them with background edges, which are 2D image edges speci.ed 
by the user. Background edges play the same role as the feature edges in image morphing [1]. A series 
of connected background edges form a background polyline. As shown in Fig. 3, a back­ground polyline 
is manually speci.ed in a few key views and inter­polated into other views by linear interpolation. 
3.2 Global Visibility Map After specifying all feature elements of a light .eld ., we can de­.ne the 
global visibility map (or visibility map for short) of .as follows. De.nition 1 The global visibility 
map of a light .eld .with feature polygons ..........is a mapping .....from the ray .. space .to the 
set of integers .such that . .if ray ..........belongs to .. ........... ..otherwise Intuitively, .may 
be regarded as a light .eld of false colors, with ..........indicating the id of the feature polygon 
visible at ray ........... Fig. 3 shows examples of visibility maps. Visibility Computation: The visibility 
map .is computed based on the vertex geometry of feature polygons1 as well as the fact that feature polygons 
have no self-occlusion by construction. The main calculation is that of the visibility of a set of relatively 
.at but non­planar polygons. This is a calculation that can be done ef.ciently using OpenGL. Consider 
rendering a non-planar polygon ..into a view ....... A problem with this rendering is that the projection 
of ..into the view ......may be a concave polygon, which OpenGL cannot dis­play correctly. One solution 
to this problem is a two-pass render­ing method using the stencil buffer. This method works for feature 
polygons since they have no self-occlusion as we mentioned ear­lier. Alternatively, we can simplify visibility 
map computation by restricting feature polygons to be triangles without supplementary feature lines, 
but then the user has to draw many feature polygons, which makes feature speci.cation unnecessarily tedious. 
Ray Bundles: Based on the visibility map ., we can group the rays of .according to their associated feature 
polygons. A group so obtained is called a ray bundle, denoted as .....where ..is the associated feature 
polygon. As we shall see in Section 4, ..... can be warped using ray-space warping with the control primitives 
of ..(see the ray-space warping equation (1) in Section 4). The ray correspondence of .....is thus completely 
determined by the control primitives of ... Rays that do not belong to any ray bundle are called background 
rays. Background rays are controlled by the background edges. Ray bundles have been used by Szirmay-Kalos 
in the context of global illumination [25].  4 Warping As mentioned, for each ....., the light .eld 
morph ..is obtained by blending two light .elds ...and . .., which are warped from ..and ..for feature 
alignment. In this section, we discuss the warping from ..to .since the warping from ....is es­ ..to 
.sentially the same. We also describe an ef.cient warping algorithm for animation sequences of 3D morphing. 
The warping from ..to .takes the following steps: (a) cal­ .. culate feature polygons and background 
edges of . .., (b) build the visibility map of . .., (c) compute ray bundles of the warped light .eld 
.  .., and (d) treat background rays. 4.1 Basic Ray-Space Warping Because the rays of a light .eld .are 
grouped ray bundles, the basic operation of light .eld warping is to warp a ray bundle ......For 1Using 
feature polygons to handle occlusion is related to layered repre­sentations in image-based rendering 
(e.g. [23]). (a)(b)(c) (d)(e)(f) Figure 5: Issues in light .eld warping. Top row shows interpolation 
of feature polygons, viewed from ............ (a) Light .eld .. with feature lines. (b) Light .eld ....with 
feature lines. (c) Light .eld ..with feature lines. Bottom row shows a hole caused by object shape change. 
(d) Light .eld ..viewed from ............ (e) Warped light .eld ...viewed from ............ The area 
highlighted in green is a hole corresponding to the occluded part of a feature polygon in (d). (f) Light 
.eld ..viewed from ...... ........ The feature polygon occluded at view ...........is now fully visible. 
simplicity, let us assume that .has only an .-sided feature poly­gon .., whose feature lines are .............before 
warping and ... ............afterwards. The basic ray-space warping regards the warped light .eld . .as 
a 4D ray space and directly computes color values of individual rays: ...........................where 
...............................................(1) .. ................ and .......are free variables 
in the .....-plane. The vector function ...is the Beier-Neely .eld warping function [1]. For a given 
point . .....in view ......., ....nds the preimage .......in view ....... based on the correspondence 
between the feature lines .. .., ..., . .... in ......and ......, ..., in . ..... ...... ............ 
......... For each ray . .........., the basic ray-space warping provides a set of rays ................whose 
colors can be assigned to .. ........... Possible values of ......include ....., in which case ray-space 
warping yields the same result as image warping [1].  4.2 Light Field Warping To warp the light .eld 
..to . .., we apply the basic warping meth­ods described above to feature polygons of ... The warping 
takes four steps. First, we calculate feature polygons and background edges of ...are linearly interpo­ 
... The vertices of feature lines in .lated from their counterparts of ..and ... Fig. 5 (top row) shows 
.. an example of feature interpolation. For ....., let ........... be the vertices of feature lines in 
... The vertices of feature lines in ...are .... .., ......, where .. ...................... .. .. The 
connections between the vertices are the same in .and ... .. Thus we can easily obtain the feature polygons 
of .as well as .. their control primitives. Figure 6: 3D facial morphing. Second, we build the visibility 
map of .and that gives us infor­ .. mation about the visibility changes caused by object shape change. 
Using the edge geometry of feature polygons of . .., we can per­form the visibility calculation of these 
polygons, with non-planar polygons rendered by the view-dependent triangulation as before. The result 
of this visibility calculation is the visibility map of . ... Third, we compute the warped ray bundles 
of light .eld .= . . ..view-by-view. Consider processing ray bundle ... . .......... in view .for feature 
polygon .that corresponds to feature . ........ polygon ..in ... We evaluate ............in three steps: 
visibility testing We check the visibility map of ..to see whether ..is visible at ray .............determined 
by the ray­space warping equation (1) with .......= ...... pixel mapping If ..is visible at ray ............., 
we let ........................... ray-space warping Otherwise, ............is in a hole and we . invoke 
ray-space warping to .ll the hole. Fig. 5 (top row) shows an example of a hole. The basic ray-space warping 
de­scribed earlier provides a set of values .................pa­rameterized by free variable ........ 
Using the visibility map of .., we search for the nearest visible ray ............... such that ..is 
visible at ray ...............determined by the ray-space warping equation (1) and .......is as close 
to .....as possible. This search starts from the immedi­ate neighbors of .....in the .....-plane and 
propagates out­wards, accepting the .rst valid ........ Note that the search will never fail because 
..by construction is fully visible in at least one view of ... Once .......is found, we set ........................... 
according to the ray-space warping equation (1). Fig. 4 illustrates the nearest visible ray . In the 
last step, we treat background rays, which correspond to pixels not covered by the projection of any 
feature polygon. We apply image warping to these pixels, using the background edges and (projected) feature 
polygon edges as control primitives. The idea behind choosing the nearest visible ray is the follow­ing. 
For . ..........., the basic ray-space warping provides a set of values .................. Under the 
Lambertian surface assump­tion, all rays are equally valid. However, the Lambertian surface  Figure 
7: A morphing example with large occlusions and specular surfaces. Figure 8: Morphing between two real 
objects. assumption only approximately holds despite its widespread use in image-based rendering [4, 
21]. By choosing the visible ray near­est to ray .............when ..is occluded at the latter, we are 
trying to minimize the error caused by the Lambertian surface as­sumption. Note that for the nearest 
visible ray , we choose a visible ray ...............with .......as close to .....as possible. This is 
the measure of closeness used in [19]. A more natural measure is the angle deviation in [3]. Unfortunately, 
calculation of angle devia­tion requires a good estimation of the depth at pixel ............... The 
depth estimation we can get from the associated feature poly­gon does not necessarily have enough accuracy. 
 4.3 Warping for Animation Sequences Our system can produce animation sequences that allow the user 
to observe a morphing process from a camera moving along an ar­bitrary path in 3D. In particular, the 
camera does not have to be inside the .....-plane. One way to compute such a 3D morph­ing sequence is 
to .rst compute a sequence of light .eld morphs .................and then create the 3D mor­phing sequence 
by rendering the light .eld morphs in .. Unfortu­nately, the CPU/storage costs for computing .can be 
very high. We describe a method for generating a 3D morphing sequence with­out fully evaluating the sequence 
.. Suppose we are given .and we want to compute the image .. in the morphing sequence. From the known 
camera path and ., we can .nd the camera position ... The image ..is a blend of two images ...and ..., 
where ...is warped from ..and ...is warped from ... The image ...is warped from ..by .rst calculat­ing, 
for each pixel ......in the image ..., its corresponding ray .......... . .............and then applying 
ray-space warping. The image ...is warped from ..the same way.  5 Results and Discussion We have implemented 
the light .eld morphing algorithm on a Pen­tium III 667 MHz PC. In this section we report some results 
and a few applications. 3D Morphs and Animations: A 3D morph ..represents a plausi­ble object having 
the essential features of both the source and target objects. We can interactively display ..by light 
.eld rendering [19]. We can also generate an animation sequence of the 3D mor­phing process from a camera 
moving along an arbitrary path in 3D. Fig. 6 shows a 3D facial morphing between an Asian male and a Caucasian 
male. Both light .elds are .....in the .....-plane and .......in the .....-plane. These light .elds are 
rendered in 3D Studio Max from two Cyberscan models, each having about 90k triangles; the models are 
not used for morphing. Feature spec­i.cation took about 20 minutes. We speci.ed 12 pairs of feature polygons 
and 9 pairs of supplementary feature lines with 41 pairs of feature points. We also speci.ed 8 background 
polylines made of 53 background edges. On the average, a background polyline was speci.ed in 8 out of 
the 1089 views of each light .eld and interpo­lated into other views. With our unoptimized implementation, 
the two global visibility maps took 37 seconds each, whereas light .eld warping and blending took 15 
seconds and 0.5 seconds respectively per image. Fig. 7 provides an example with large occlusion and specular 
surfaces. The light .elds are acquired the same way at the same res­olution as in the 3D facial morphing 
example. Feature speci.cation took about 30 minutes. We speci.ed 50 pairs of feature polygons and 1 pair 
of supplementary feature lines with 126 pairs of feature Figure 9: A frame (second from the left) enlarged 
from the 3D morphing sequence shown in Fig. 1. points. We also speci.ed 4 background polylines made 
of 50 back­ground edges. On the average, a background polyline was speci.ed in 8 out of the 1089 views 
of each light .eld and interpolated into other views. Fig. 11 shows the morphing between a real bronze 
statue and the famous statue of Egyptian queen Nefertiti. The surface of the antique bronze statue shows 
complicated material property. This is very dif.cult to model with a textured geometry model, although 
some nice progress has been made in this area [7]. To acquire the light .elds of the bronze statue, we 
used an outside-looking-in concentric mosaic (CM) [24], which is a slightly different parameterization 
of the light .eld. We acquired 300 im­ages of the bronze statue at a resolution of ........ The CM of 
the Nefertiti statue was rendered at the same resolution in 3D Studio Max from a textured model (model 
not used for morphing). Feature speci.cation took about 20 minutes. As for morphing time, the global 
visibility map took 15 seconds per map. Warping and blending took 7 seconds per image. Fig. 8 shows the 
morphing between two real bronze statues: a deer and a horse. The light .elds were acquired at the same 
reso­lution as in the Nefertiti example. Feature speci.cation took about 25 minutes. The global visibility 
map took 25 seconds per map. Warping and blending took 5 seconds per image. Key-Frame Morphing: As mentioned, 
key-frame morphing is light .eld morphing with very sparse light .elds. Fig. 1 and Fig. 9 show the result 
of key-frame morphing between a real furry toy cat and the Stanford bunny. Note that the fur on the cat 
surface is very dif.cult to model with textured geometry models. We took three photographs of the cat 
using a calibrated cam­era. Three images of the bunny were rendered using the same cam­era parameters 
as the real photographs. Feature speci.cation took about 7 minutes. The global visibility map took 4 
seconds to com­pute for each object. Warping and blending each image of the 3D morphing sequence took 
24 seconds. The number of key frames needed depends on both the visibil­ity complexity of the source 
and target objects and the presence of non-lambertian surfaces. As expected, the quality of key-frame 
morphing improves as more input images are used. In this regard, key-frame morphing is more .exible than 
view morphing [21]. This .exibility is particularly important when, e.g., there are a lot of vis­ibility 
changes due to object shape change, in which case the near­est visible ray will be frequently needed 
to .ll the holes, and we want the nearest visible rays to be actually nearer for highly non­ (a)(b) 
Lambertian surfaces. Plenoptic Texture Transfer: Given the source and target objects ..and ..represented 
by light .elds ..and .., we transfer the texture of ..onto ..by constructing a light .eld ...as follows. 
First, the feature elements of ...are obtained as those of ... Sec­ond, ..is warped to .for feature alignment. 
Finally, ... ..is ob­tained as the warped light .elds . ... Intuitively, we create a morph using the 
feature elements of ..and the radiance of ... Unlike 2D texture transfer (e.g., [8]), plenoptic texture 
transfer is a 3D effect. Fig. 10 shows the result of plenoptic texture transfer from the furry cat toy 
in Fig. 1 onto the Stanford bunny. Note that for plenoptic texture transfer to work well, the two objects 
should be similar to avoid texture distortions. Discussion: In light .eld morphing, it is easy to handle 
complex surface properties. Geometry-based 3D morphing, for example, will have dif.culties with the furry 
cat example in Fig. 1. On the other hand, the lack of geometry causes problems in light .eld mor­phing. 
An example is the view point restriction imposed by the input light .elds. We can incorporate geometry-based 
3D morph­ing into light .eld morphing by using image-based visual hulls [20] as rough geometry to morph 
two light .elds. However, the visual hull geometry cannot replace feature polygons because visual hull 
geometry is obtained from the silhouette and thus cannot handle visibility changes not on object silhouette. 
Light .eld morphing can be regarded as a generalization of im­age morphing (an image is a ...light .eld) 
and as such can suffer the ghost problem in image morphing for poorly-speci.ed feature lines [1]. Fortunately 
the usual .xes in image morphing also work for light .eld morphing [1].  6 Conclusions We have presented 
an algorithm for morphing 3D objects repre­sented by light .elds. The principal advantage of light .eld 
mor­phing is the ability to morph between image-based objects whose geometry and surface properties may 
be too dif.cult to model with traditional vision and graphics techniques. Light .eld morphing is based 
on ray correspondence, not surface reconstruction. The mor­phing algorithm we present is feature-based. 
We built an intuitive and easy-to-use UI for specifying feature polygons for controlling the ray correspondence 
between two light .elds. We also show that the visibility changes due to object shape changes can be 
effectively handled by ray-space warping. Finally, it is worth to note that light .eld morphing is a 
.exible morphing scheme. The user can perform 3D morphing by starting with a few input images and adding 
more input images as necessary to improve the quality of 3D morphing sequences.  A number of topics 
remains to be explored. It is desirable to ex­tend our system so that it supports a change of topology 
[6] and multiple light slabs representing the same object [19]. Another promising area is to use computer 
vision techniques to automate the feature/visibility speci.cation tasks as much as possible. Fi­nally, 
we are interested in other operations that can be performed on light .elds [18]. Acknowledgments: First 
of all, we want to thank Hua Zhong, who developed an earlier version of the light .eld morphing system 
at Microsoft Research Asia. Since then he moved onto more excit­ing things and did not have time to work 
on the current system. Many thanks to Sing Bing Kang for useful discussions, to Yin Li and Steve Lin 
for their help in video production, to Steve Lin for proofreading this paper, and to anonymous reviewers 
for their con­structive critique.  References [1] Thaddeus Beier and Shawn Neely. Feature-based image 
metamorphosis. Com­puter Graphics (Proceedings of SIGGRAPH 92), 26(2):35 42, July 1992. [2] Bryan P. 
Bergeron. Morphing as a means of generating variability in visual med­ical teaching materials. Computers 
in Biology and Medicine, 24:11 18, January 1994. [3] Chris Buehler, Michael Bosse, Leonard McMillan, 
Steven J. Gortler, and Michael F. Cohen. Unstructured lumigraph rendering. Proceedings of SIG-GRAPH 2001, 
pages 425 432, August 2001. [4] Shenchang Eric Chen and Lance Williams. View interpolation for image 
synthe­sis. Proceedings of SIGGRAPH 93, pages 279 288, August 1993. [5] Daniel Cohen-Or, Amira Solomovici, 
and David Levin. Three-dimensional dis­tance .eld metamorphosis. ACM Transactions on Graphics, 17(2):116 
141, April 1998. ISSN 0730-0301. [6] Douglas DeCarlo and Jean Gallier. Topological evolution of surfaces. 
Graphics Interface 96, pages 194 203, May 1996. [7] Julie Dorsey and Pat Hanrahan. Modeling and rendering 
of metallic patinas. Proceedings of SIGGRAPH 96, pages 387 396, August 1996. [8] Alexei A. Efros and 
William T. Freeman. Image quilting for texture synthesis and transfer. Proceedings of SIGGRAPH 2001, 
pages 341 346, August 2001. [9] Oliver Faugeras. 3D Computer Vision. The MIT Press, Cambridge, MA, 1993. 
[10] Jonas Gomes, Bruno Costa, Lucia Darsa, and Luiz Velho. Warping and Morph­ing of Graphics Objects. 
Morgan Kaufmann, 1998. [11] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. 
The lumigraph. Proceedings of SIGGRAPH 96, pages 43 54, August 1996. [12] Arthur Gregory, Andrei State, 
Ming C. Lin, Dinesh Manocha, and Mark A. Liv­ingston. Interactive surface decomposition for polyhedral 
morphing. The Visual Computer, 15(9):453 470, 1999. [13] John F. Hughes. Scheduled fourier volume morphing. 
Computer Graphics (Pro­ceedings of SIGGRAPH 92), 26(2):43 46, July 1992. [14] James R. Kent, Wayne E. 
Carlson, and Richard E. Parent. Shape transformation for polyhedral objects. Computer Graphics (Proceedings 
of SIGGRAPH 92), 26(2):47 54, July 1992. [15] Francis Lazarus and Anne Verroust. Three-dimensional metamorphosis: 
a sur­vey. The Visual Computer, 14(8-9):373 389, 1998. [16] Aaron Lee, David Dobkin, Wim Sweldens, and 
Peter Schr¨ oder. Multiresolution mesh morphing. Proceedings of SIGGRAPH 99, pages 343 350, August 1999. 
[17] Apostolos Lerios, Chase D. Gar.nkle, and Marc Levoy. Feature-based volume metamorphosis. Proceedings 
of SIGGRAPH 95, pages 449 456, August 1995. [18] Marc Levoy. Expanding the horizons of image-based modeling 
and rendering. In SIGGRAPH 97 Panel:Image-Based Rendering:Really New or Deja Vu, 1997. [19] Marc Levoy 
and Pat Hanrahan. Light .eld rendering. Proceedings of SIGGRAPH 96, pages 31 42, August 1996. [20] Wojciech 
Matusik, Chris Buehler, Ramesh Raskar, Steven J. Gortler, and Leonard McMillan. Image-based visual hulls. 
In Proceedings of ACM SIG-GRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, pages 
369 374, July 2000. [21] Steven M. Seitz and Charles R. Dyer. View morphing: Synthesizing 3d metamor­phoses 
using image transforms. Proceedings of SIGGRAPH 96, pages 21 30, August 1996. [22] Steven M. Seitz and 
Kiriakos N. Kutulakos. Plenoptic image editing. In ICCV98, pages 17 24, 1998. [23] Jonathan Shade, Steven 
J. Gortler, Li wei He, and Richard Szeliski. Layered depth images. In Proceedings of SIGGRAPH 98, Computer 
Graphics Proceed­ings, Annual Conference Series, pages 231 242, Orlando, Florida, July 1998. [24] Heung-Yeung 
Shum and Li-Wei He. Rendering with concentric mosaics. Pro­ceedings of SIGGRAPH 99, pages 299 306, August 
1999. ISBN 0-20148-560-5. Held in Los Angeles, California. [25] L´o Szirmay-Kalos and Werner Purgathofer. 
Global ray-bundle tracing with aszl´hardware acceleration. Eurographics Rendering Workshop 1998, pages 
247 258, June 1998. [26] George Wolberg. Image morphing: a survey. The Visual Computer, 14(8-9):360 372, 
1998. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566603</section_id>
		<sort_key>465</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation from motion capture]]></section_title>
		<section_page_from>465</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42053456</person_id>
				<author_profile_id><![CDATA[81319502903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michiel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van de Panne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566604</article_id>
		<sort_key>465</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Motion texture]]></title>
		<subtitle><![CDATA[a two-level statistical model for character motion synthesis]]></subtitle>
		<page_from>465</page_from>
		<page_to>472</page_to>
		<doi_number>10.1145/566570.566604</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566604</url>
		<abstract>
			<par><![CDATA[In this paper, we describe a novel technique, called motion texture, for synthesizing complex human-figure motion (e.g., dancing) that is statistically similar to the original motion captured data. We define motion texture as a set of motion textons and their distribution, which characterize the stochastic and dynamic nature of the captured motion. Specifically, a motion texton is modeled by a linear dynamic system (LDS) while the texton distribution is represented by a transition matrix indicating how likely each texton is switched to another. We have designed a maximum likelihood algorithm to learn the motion textons and their relationship from the captured dance motion. The learnt motion texture can then be used to generate new animations automatically and/or edit animation sequences interactively. Most interestingly, motion texture can be manipulated at different levels, either by changing the fine details of a specific motion at the texton level or by designing a new choreography at the distribution level. Our approach is demonstrated by many synthesized sequences of visually compelling dance motion.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[linear dynamic systems]]></kw>
			<kw><![CDATA[motion editing]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
			<kw><![CDATA[motion texture]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Time series analysis</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003688.10003693</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Statistical paradigms->Time series analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15033854</person_id>
				<author_profile_id><![CDATA[81452600289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P.R. China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382470</person_id>
				<author_profile_id><![CDATA[81100212791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tianshu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xi'an Jiaotong University, P.R.China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14159670</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P.R. China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[O. Arikan and D. Forsyth. Interactive motion generation from examples. In Proceedings of ACM SIGGRAPH 02, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614486</ref_obj_id>
				<ref_obj_pid>614282</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Z. Bar-Joseph, R. El-Yaniv, D. Lischiniski, and M. Werman. Texture movies: Statistical learning of time-varying textures. IEEE Transactions on Visualization and Computer Graphics, 7(1):120-135, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>525960</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, Oxford, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[B. Bodenheimer, A. Shleyfman, and J. Hodgins. The effects of noise on the perception of animated human running. In Computer Animation and Simulation '99, Eurographics Animation Workshop, pages 53-63, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. Bowden. Learning statistical models of human motion. In IEEE Workshop on Human Modeling, Analysis and Synthesis, CVPR, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Brand and A. Hertzmann. Style machines. In Proceedings of ACM SIGGRAPH 00, pages 183-192, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794493</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[C. Bregler. Learning and recognizing human dynamics in video sequences. In Int. Conf. on Computer Vision and Pattern Rocognition, pages 568-574, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. Bruderlin and L. Williams. Motion signal processing. In Proceedings of ACM SIGGRAPH 95, pages 97-104, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>80156</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms. The MIT Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. de Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. In Proceedings of ACM SIGGRAPH 97, pages 361-368, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[N. M. Dempster, A. P. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. J. R. Statist. Soc. B, 39:185-197, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Efros and W. Freeman. Image quilting for texture synthesis and transfer. In Proceedings of ACM SIGGRAPH 01, pages 341-346, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. W. Fitzgibbon. Stochastic rigidity: Image registration for nowhere-static scenes. In IEEE Int. Conf. on Computer Vision, pages 662-669, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>377412</ref_obj_id>
				<ref_obj_pid>376890</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Galata, N. Johnson, and D. Hogg. Learning variable length Markov models of behaviour. Computer Vision and Image Understanding, 81(3):398-413, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[M. Gleicher. Retargetting motion to new characters. In Proceedings of ACM SIGGRAPH 98, pages 33-42, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781241</ref_obj_id>
				<ref_obj_pid>781238</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[F. S. Grassia. Practical parameterization of rotations using the exponential map. Journal of Graphics Tools, 3(3):29-48, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[C. E. Guo, S. C. Zhu, and Y. N. Wu. Visual learning by integrating descriptive and generative methods. In Int. Conf. Computer Vision, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. K. Hodgins, W. L. Wooten, D. C. Brogan, and J. F. O'Brien. Animating human athletics. In Proceedings of ACM SIGGRAPH 95, pages 71-78, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>296194</ref_obj_id>
				<ref_obj_pid>296193</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[M. Isard and A. Blake. Condensation - conditional density propagation for visual tracking. Int'l J. Computer Vision, 28(1):5-28, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[B. Julesz. Textons, the elements of texture perception and their interactions. Nature, 290:91-97, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[L. Kovar, M. Gleicher, and F. Pighin. Motion graphs. In Proceedings of ACM SIGGRAPH 02, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Lee, J. Chai, P. Reisma, and J. Hodgins. Interactive control of avartas animated with human motion data. In Proceedings of ACM SIGGRAPH 02, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Lee and S. Y. Shin. A hierarchical approach to interactive motion editing for human-like figures. In Proceedings of ACM SIGGRAPH 99, pages 39-48, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851564</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T. Leung and J. Malik. Recognizing surfaces using three-dimensional textons. In Int. Conf. on Computer Vision, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L. Liang, C. Liu, Y. Xu, B. Guo, and H.-Y. Shum. Real-time texture synthesis by patch-based sampling. Technical Report MSR-TR-2001-40, Microsoft Research, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>293154</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[L. Ljung. System Identification - Theory for the User. Prentice Hall, Upper Saddle River, N.J., 2nd edition, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851546</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Malik, S. Belongie, J. Shi, and T. Leung. Textons, contours and regions: Cue integration in image segmentation. In Int. Conf. Computer Vision, pages 918-925, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>353278</ref_obj_id>
				<ref_obj_pid>353258</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[B. North, A. Blake, M. Isard, and J. Rittscher. Learning and classification of complex dynamics. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(9):1016-1034, Sep. 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>710460</ref_obj_id>
				<ref_obj_pid>646880</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[V. Pavlovi&#263;, J. M. Rehg, T. J. Cham, and K. P. Murphy. A dynamic Bayesian network approach to figure tracking using learned dynamic models. In IEEE International Conference on Computer Vision, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[V. Pavlovi&#263;, J. M. Rehg, and J. MacCormick. Impact of dynamic model learning on classification of human motion. In IEEE International Conference on Computer Vision and Pattern Recognition, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[K. Perlin. An image synthesizer. In Computer Graphics (Proceedings of ACM SIGGRAPH 85), pages 287-296, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[K. Perlin and A. Goldberg. Improv: A system for scripting interactive actors in virtual worlds. In Proceedings of ACM SIGGRAPH 96, pages 205-216, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Z. Popovi&#263; and A. Witkin. Physically based motion transformation. In Proceedings of ACM SIGGRAPH 99, pages 11-20, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[K. Pullen and C. Bregler. http://graphics.stanford.edu/~pullen/motion_texture.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566608</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[K. Pullen and C. Bregler. Motion capture assisted animation: Texturing and synthesis. In Proceedings of ACM SIGGRAPH 02, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[L. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257-285, February 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[A. Sch&#246;dl, R. Szeliski, D. H. Salesin, and I. Essa. Video textures. In Proceedings of ACM SIGGRAPH 00, pages 489-498, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>502123</ref_obj_id>
				<ref_obj_pid>502122</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[H. J. Shin, J. Lee, M. Gleicher, and S. Y. Shin. Computer puppetry: An importance-based approach. ACM Transactions on Graphics, 20(2):67-94, April 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325242</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[K. Shoemake. Animating rotation with quaternion curves. In Computer Graphics (Proceedings of ACM SIGGRAPH 85), pages 245-254, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[S. Soatto, G. Doretto, and Y. N. Wu. Dynamic textures. In IEEE International Conference on Computer Vision, pages 439-446, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>823423</ref_obj_id>
				<ref_obj_pid>822088</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[L. M. Tanco and A. Hilton. Realistic synthesis of novel human movements from a database of motion capture examples. In IEEE Workshop on Human Motion, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[M. Unuma, K. Anjyo, and R. Takeuchi. Fourier principles for emotion-based human figure animation. In Proceedings of ACM SIGGRAPH 95, pages 91-96, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and Z. Popovi&#263;. Motion warping. In Proceedings of ACM SIGGRAPH 95, pages 105-108, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649347</ref_obj_id>
				<ref_obj_pid>645318</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[S. C. Zhu, C. E. Guo, Y. Wu, and Y. Wang. What are textons. In Proc. of European Conf. on Computer Vision (ECCV), 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Texture: A Two-Level Statistical Model for Character Motion Synthesis Yan Li* Tianshu Wang Heung-Yeung 
Shum* *Microsoft Research, Asia Xi an Jiaotong University, P.R.China  Figure 1: This 320-frame sequence 
of dance motion is choreographed from (1) the starting frame, (2) the ending frame and (3) the learnt 
motion texture from motion captured dance data. Four motion textons are generated from the motion texture 
and then used to synthesize all the frames in this sequence. A number of key frames are also shown in 
the .gure to demonstrate that the synthesized motion is natural, smooth and realistic (Two red lines 
indicate the trajectories of the right hand and right foot). Abstract In this paper, we describe a novel 
technique, called motion texture, for synthesizing complex human-.gure motion (e.g., dancing) that is 
statistically similar to the original motion captured data. We de­.ne motion texture as a set of motion 
textons and their distribution, which characterize the stochastic and dynamic nature of the cap­tured 
motion. Speci.cally, a motion texton is modeled by a linear dynamic system (LDS) while the texton distribution 
is represented by a transition matrix indicating how likely each texton is switched to another. We have 
designed a maximum likelihood algorithm to learn the motion textons and their relationship from the captured 
dance motion. The learnt motion texture can then be used to gener­ate new animations automatically and/or 
edit animation sequences interactively. Most interestingly, motion texture can be manipulated at different 
levels, either by changing the .ne details of a speci.c motion at the texton level or by designing a 
new choreography at the distribution level. Our approach is demonstrated by many syn­thesized sequences 
of visually compelling dance motion. CR Categories: I.3.7 [Computer Graphics]: Three Dimensional Graphics 
and Realism Animation; G.3 [Mathematics of Comput­ing]: Probability and Statistics Time series analysis 
Keywords: Motion Texture, Motion Synthesis, Texture Synthesis, Motion Editing, Linear Dynamic Systems. 
*3F Beijing Sigma Center, No. 49 Zhichun Road, Haidian District, Bei­jing 100080, P.R. China. Email: 
{yli,hshum}@microsoft.com This work was done when Tianshu Wang was visiting at Microsoft Re­search, Asia. 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 1 Introduction Synthesis of realistic character animation is an active research area and has many 
applications in entertainment and biomechanics. Re­cent advances in motion capture techniques and other 
motion edit­ing software facilitate our generating of human animation with un­precedented ease and realism. 
By recording motion data directly from real actors and mapping them to computer characters, high quality 
motion can be generated very quickly. The captured mo­tion can also be used to generate new animation 
sequences ac­cording to different constraints. Many techniques have been de­veloped to tackle the dif.cult 
problem of motion editing. These techniques include motion signal processing [8], human locomo­tion in 
Fourier domain [42], motion warping [43], motion retar­geting [15, 38], physically based motion transformation 
[33] and motion editing with a hierarchy of displacement maps [23]. More recently, several approaches 
have been proposed to interactively synthesize human motion by reordering the preprocessed motion capture 
data [21, 1, 22]. To make the edited motion realistic , it is important to under­stand and incorporate 
the dynamics of the character motion. In physically based motion transformation, for instance, Popovi´ 
c and Witkin [33] obtain a physical spacetime optimization solution from the .tted motion of a simpli.ed 
character model. In this paper, we present a different approach to the problem of editing captured mo­tion 
by learning motion dynamics from motion captured data. We model local dynamics (of a segment of frames) 
by a linear dynamic system, and global dynamics (of the entire sequence) by switching between these linear 
systems. The motion dynamics are modeled in an analytical form which constrains the consecutive body 
postures. The meaning of dynamics as used in this paper is different from that in traditional animation 
literature, where dynamics denotes an interactive system involving force-based motion. We call our model 
motion texture because motion sequences are analogous to 2D texture images. Similar to texture images, 
motion sequences can be regarded as stochastic processes. However, while texture images assume a two-dimensional 
spatial distribution, mo­tion textures display a one-dimensional temporal distribution. We de.ne motion 
texture by a two-level statistical model: a set of mo­tion textons at the lower level, and the distributions 
of textons at the higher level. Intuitively, motion textons are those repetitive pat­terns in complex 
human motion. For instance, dance motion may ? ? t ? ?  Figure 2: With the learnt motion texture, 
a motion sequence can be divided into multiple segments, labeled as lk where k =1, ..., Ns. Each segment 
may have a different length, and can be represented by one of the Nt (Nt = Ns) textons. In a texton, 
a local dynamic system with parameters (A, C, V, W )is used to describe the dynamics of state variables 
Xt and observations Yt in a segment. consist of repeated primitives such as spinning, hopping, kicking, 
and tiptoeing. In our model, the basic element in motion texture is called a mo­tion texton. A motion 
texton is represented by a linear dynamic system (LDS) that captures the dynamics shared by all instances 
of this texton in the motion sequence. The texton distribution, or re­lationship between motion textons, 
can be modeled by a transition matrix. Once the motion texture is learnt, it can be used for syn­thesizing 
novel motion sequences. The synthesized motion is sta­tistically similar to, yet visually different from, 
the motion captured data. Our model enables users to synthesize and edit the motion at both the texton 
level and the distribution level. The remainder of this paper is organized as follows. After re­viewing 
related work in Section 2, we introduce the concept of mo­tion texture in Section 3, and show how to 
learn motion textons and texton distributions in Section 4. The synthesis algorithms using motion texture 
are explained in Section 5. Applications of motion texture including motion synthesis and motion editing 
are shown in Section 6. We conclude our paper in Section 7.  2 Related Work Motion texture. Based on 
the observation that these repetitive pat­terns of life-like motion exhibit inherent randomness, Pullen 
and Bregler [35] proposed a multi-level sampling approach to synthe­size new motions that are statistically 
similar to the original. Sim­ilar to multi-resolution representations of texture [10] and movie texture 
[2], Pullen and Bregler modeled cyclic motions by multi­resolution signals. The term motion texture was 
originally used by Pullen and Bregler (and suggested by Perlin) as their project name [34]. Our motion 
texture model is completely different from theirs. We explicitly model not only local dynamics of those 
repet­itive patterns, but also global dynamics on how these patterns are linked together. Textons. The 
concept of texton was .rst proposed by Julesz [20] some twenty years ago, although a clear de.nition 
is still in debate. Malik et al. [27] used oriented .lters, Guo et al. [17] used image templates that 
can be transformed geometrically and photometri­cally, and we use LDS as the motion texton. The concept 
of 2D texton has been extended to 3D texton by Leung and Malik [24] to represent images with varying 
lighting and viewing directions. Zhu [44] proposed to represent a 2D texture image with textons and layers 
of texton maps. However, extracting textons from a tex­ture has proven to be challenging, as shown by 
rudimentary textons in [44, 17]. Although the patches used in the patch-based texture synthesis may be 
regarded as textons as well, the concept of texton map was not explicitly discussed in [12, 25]. Linear 
dynamic system. Modeling the motion texton with LDS is related to recent work on dynamic texture analysis 
and synthesis for video clips (e.g., video textures [37]). Soatto et al. [40] pro­posed that a dynamic 
texture can be modeled by an auto-regressive, moving average (ARMA) process with unknown input distribution. 
A similar approach was also proposed by Fitzgibbon [13] with an autoregressive (AR) model. These approaches 
model the temporal behavior as samples of an underlying continuous process, and are effective for spatially 
coherent textures. But they break down when the underlying dynamics are beyond the scope of a simple 
linear dynamics system. Furthermore, the system tends to converge into the stable state at which synthesis 
degrades to noise-driven textures. Bregler [7] also used second order dynamical systems to represent 
the dynamical categories (called movemes) of human motion. How­ever, the movemes are only used to recognize 
simple human gait with two or three joint angles. Synthesizing realistic human motion is very dif.cult 
due to the high dimensionality of human body and the variability in human motion over time. Modeling 
nonlinear dynamics. Many approaches have been proposed to model complex motion with multiple linear systems. 
It is, however, dif.cult to learn these linear systems along with the transitions. For instance, North 
et al. [28] learnt multiple classes of motions by combining EM (expectation-maximization) [11] and CONDENSATION 
[19]. Approximate inference methods had to be devised to learn a switched linear dynamic system (SLDS) 
[30, 29] because exact inference cannot be found. By discretizing state vari­ables, a hidden Markov model 
(HMM) can be used to describe mo­tion dynamics as well [6, 41, 14]. With an HMM, however, the motion 
primitives cannot be edited explicitly because they are rep­resented by a number of hidden states. In 
our work, a two-level statistical model is necessary for modeling rich dynamics of human motion. The 
transition matrix implicitly models the nonlinear as­pect of complex human motion by piecewise linear 
systems.  3 Motion Texture 3.1 A Two-level Statistical Model We propose a two-level statistical model 
to represent character mo­tion. In our model, there are Nt motion textons (or textons for short from 
now on) T ={T1,T2, ..., TNt }, represented by respec­tive texton parameters T={.1,.2, ..., .Nt }. Our 
objective is to divide the motion sequence into Ns segments, such that each seg­ment can be represented 
by one of the Nt textons, as shown in Fig­ure 2. Multiple segments could be represented by the same texton. 
Texton distribution, or the relationship between any pair of textons can be described by counting how 
many times a texton is switched to another. In Figure 2, each segment is labeled as lk, where k =1,...,Ns. 
The length of each segment may be different. Because all Nt tex­tons are learnt from the entire sequence 
of Ns segments, Nt = Ns must hold. Segment k starts from frame hk and has a mini­mum length constraint 
hk+1 - hk = Tmin. We also de.ne seg­ment labels as L = {l1,l2, ..., lNs }, and segmentation points as 
H ={h1,h2, ..., hNs }. Our two-level statistical model characterizes the dynamic and stochastic nature 
of the .gure motion. First, we use an LDS to capture the local linear dynamics and a transition matrix 
to model the global non-linear dynamics. Second, we use textons to describe the repeated patterns in 
the stochastic process.  (a) (b) Figure 3: Motion texture is a two-level statistical model with textons 
and their distribution. (a) After learning, several segments may be labeled as the same texton. All these 
segments share the same dynamics. Each texton is represented by an LDS and the initial state distribution 
P(X1). (b) Texton distribution can be represented by a transition matrix.  3.2 Motion Texton Each motion 
texton is represented by an LDS with the following state-space model: Xt+1 =AtXt +Vt (1) Yt =CtXt +Wt 
where Xt is the hidden state variable, Yt is the observation, and Vt and Wt are independent Gaussian 
noises at time t. Then the pa­rameters of an LDS can be represented by . ={A,C,V,W}. Each texton should 
have at least Tmin frames so that local dynamics can be captured. In our system, we model a complex human 
.gure with its global position and 19 joints. Therefore, Yt is a 60- dimen­sional vector because each 
joint is represented by 3 joint angles. In Section 6.1, we will show how we reparameterize the joint 
rotation angles with exponential maps [16, 23]. Eventually we can represent state variables Xt by a 12~ 
15-dimensional vector, depending on the data variance of each segment. 3.3 Distribution of Textons We 
assume that the distribution of textons satis.es the .rst-order Markovian dynamics, which could be represented 
by a transition matrix Mij =P(lk =j|lk-1 =i). (2) Such a transition matrix has been commonly used in 
HMMs [36] to indicate the likelihood of switching from one discrete state to another. Transition matrix 
has also been used in video texture [37], where transition points are found such that the video can be 
looped back to itself in a minimally obtrusive way. Unlike conventional HMMs, however, we use hybrid 
discrete (Land H) and continuous (X) state variables in our model. Switched linear dynamic systems (SLDS) 
[30, 29] also use a hybrid state variable, but model each frame with a mixture of LDS . For synthesis, 
segment-based LDS models are desirable because they better capture the stochastic and dynamic nature 
of motion.  4 Learning Motion Texture Given Y1:T ={Y1,Y2,...,YT }, or the observation Yt from frame 
1to frame T, our system learns the model parameters {T,M} by .nding a maximum likelihood (ML) solution 
{T ,M } =argmaxP(Y1:T |T,M). (3) {T,M} By using Land H, and applying the .rst-order Markovian prop­erty, 
the above equation can be rewritten as: P(Y1:T |T,M)=P(Y1:T |T,M,L,H) L,H Ns =P(Yhj :hj+1-1|.lj )Mljlj+1(4) 
L,Hj=1 where MlNs lNs+1 =1. In Eq. 4, the .rst term is the likelihood of observation given the LDS model, 
the second term re.ects the transition between two adjacent LDS . Considering L and H as hidden variables, 
we can use the EM [11] algorithm to solve the above maximum likelihood prob­lem. The algorithm is looped 
until it converges to a local optimum. E-step: An inference process is used to obtain segmentation points 
H and segment labels L. Details are in Appendix A.  M-step: Model parameters Tare updated by .tting 
LDS . Details are provided in Appendix B.  The transition matrix Mij is set by counting the labels of 
seg-Ns ments: Mij = d(lk-1 =i)d(lk =j). The matrix M is k=2 Nt then normalized such that Mij =1. j=1 
 We take a greedy approach to incrementally initialize our model. First, we use Tmin frames to .t an 
LDS i, and incrementally label the subsequent frames to segment iuntil the .tting error is above a given 
threshold. Then all existing LDS (from 1to i) learnt from all preceding segments (possibly more than 
i) are tested on the re­maining unlabeled Tmin frames, and the best-.t LDS is chosen. If the smallest 
.tting error exceeds the given threshold, i.e., none of those LDS .ts the observation well, we introduce 
a new LDS and repeat the above process until the entire sequence is processed. 4.1 Discussion In the 
learning process, the user needs to specify a threshold of model .tting error. Once the threshold is 
given, the number of textons Nt is automatically determined by the above initialization step. The bigger 
the threshold, the longer the segments, and the fewer the number of textons. Model selection methods 
[3] such as BIC (Bayesian Information Criteria) or MDL (Minimum Descrip­tion Length) can also be used 
to learn Nt automatically. Another important parameter that the user needs to determine is Tmin. Tmin 
must be long enough to capture the local dynamics of motion. In our system, we have chosen Tmin to be 
approximately one second, corresponding to most beats in the disco music of the dance sequence. What 
we have obtained in the learning process are segment la­bels, segmentation points, textons, and texton 
distribution. For the purpose of synthesis, a texton should also include an initial state distribution 
P(X1). X1 can be regarded as the initial or key poses of a texton. Because our dynamics model is second 
order, we use the .rst two frames x1,x2 of each segment to represent the key poses X1. Figure 3 shows 
the two-level motion texture model. Since we may have labeled several segments for an LDS, we rep­resent 
P(X1)in a nonparametric way. In other words, we simply keep all the starting poses X1 of the segments 
which are labeled by the same LDS. 1. Initialization: Generate the .rst two poses {x1,x2}by sampling 
the initial state distribution Pi(X1).  2. Iterate for t=3,4,...  (a) Draw samples from the noise term 
vt, (b) Compute xt by the dynamics model (Eq. 10),  (c) Synthesize yt by projecting xt to the motion 
space (Eq. 11).   Figure 4: Texton synthesis by sampling noise.  5 Synthesis with Motion Texture 
With the learnt motion texture, new motions can be synthesized. Moreover, we can edit the motion interactively, 
both at the texton level and at the distribution level. 5.1 A Two-step Synthesis Algorithm Motion texture 
decouples global nonlinear dynamics (transition matrix) from local linear dynamics (textons). Accordingly, 
we de­velop a two-step approach to synthesize new motions. First, a tex­ton path needs to be generated 
in the state space. A straightfor­ward approach is to randomly sample the texton distribution, so that 
we can obtain an in.nitely long texton sequence. A more inter­esting way is to allow the user to edit 
the texton path interactively. Given two textons and their associated key poses, for instance, our algorithm 
can generate a most likely texton sequence that passes through those key poses (see Section 5.2). Once 
we have the texton sequence, the second step in synthesis is conceptually straightforward. In principle, 
given a texton and its key poses (.rst two frames to be exact), a motion sequence can be synthesized 
frame by frame with the learnt LDS and sampled noise. However, the prediction power of LDS decreases 
after some critical length of the sequence as LDS approaches its steady state. This is why we propose 
in Section 5.4 a constrained texton synthesis algorithm that preserves the same dynamics of the given 
texton, with two additional frames at the end of the synthesized segment. Because we can use the key 
poses of the texton next to the one we synthesize, a smooth motion transition between two neighboring 
textons can be achieved. 5.2 Texton Path Planning Given two motion textons Tu and Tv, the goal of path 
planning is to ¯¯ .nd a single best path, ¯S1S2 ...St), which starts .= {¯n}(n=Nat S¯1 = Tu and ends 
at S¯n = Tv. Depending on the application, we propose two different approaches. 5.2.1 Finding the Lowest 
Cost Path In this approach, we favor multiple good transitions from Tu to Tv. Since the transition matrix 
is de.ned on a .rst-order Markov chain, the best path is equivalent to .¯= argmax P(S1S2 ...Sn|S1 = Tu,Sn 
= Tv,M) . =arg max P(TuS2)P(S2S3) ···P(Sn-1Tv) . = -arg min (log P(TuS2)+log P(S2S3)+ . ...+log P(Sn-1Tv)). 
(5) If we consider each texton as a vertex, and the negative Log proba­bility as the weight associated 
with the edge between two vertices, 1. Initialization: (a) Generate the .rst two poses {xi1,xi2}by sampling 
the initial state distribution Pi(X1). x1 = xi1,x2 = xi2. (b) Similarly, generate {xj1,xj2}from Pj (X1). 
Then the end constraints (xl-1,xl) are obtained by reprojecting {xj1,xj2}(Eq. 7). (c) For t=2 ...(l-1), 
draw samples from the noise term vt and form vector b(Eq. 9).  2. Synthesis: (a) Synthesize x3:l-2 by 
solving Eq. 8. (b) Synthesize y1:l by projecting x1:l to the motion space.  Figure 5: Texton synthesis 
with constrained LDS. the transition matrix forms a weighted, directed graph G. Then the shortest path 
problem in Eq. 5 can be ef.ciently solved in O(N2) time by Dijkstra s algorithm [9]. Since each texton 
is represented by a single LDS, the self­connecting vertices in graph G will appear at most once when 
we seek for the optimal texton path by Dijkstra s algorithm. In or­der to enrich the synthesized motion, 
we further repeat each texton on the path according to its self-transition probability. Speci.cally, 
we randomly sample the transition probability P(Sj |Si) and repeat texton iuntil a different texton is 
sampled. 5.2.2 Specifying the Path Length Due to limited training data, interpolation between two motion 
tex­tons may result in a long path. An alternative way of texton path planning is to specify the path 
length. In this case, we need to trade­off cost and length. The best path between Tu and Tv with length 
L (L<..¯.) can be found by a dynamic programming algorithm [36] in O(LNs 2) time.  5.3 Texton Synthesis 
by Sampling Noise Once we have the texton path, we can generate a new motion se­quence by synthesizing 
motion for all the textons. The .rst task is to synthesize motion for a single texton i. A simple but 
effective approach is to draw samples from the white noise vt (see Appendix B) frame by frame. The key 
poses of texton i are the two starting poses, represented by x1 and x2. The .nal synthesis result can 
be generated by projecting xt from the state space to motion space yt. The algorithm is summarized in 
Figure 4. In theory, an in.nitely long sequence can be synthesized from the given texton after initialization 
and by sampling noise. How­ever, the synthesized motion will inevitably depart from the origi­nal motion 
as time progresses, as shown by the difference between Figures 7(a) and 7(b). This behavior is due to 
the fact that LDS learns only locally consistent motion patterns. Moreover, the syn­thesis errors will 
accumulate (Figure 7(b)) as the hidden variable xt propagates. Noise-driven synthesis has been widely 
used in animation. For example, Perlin-noise [31] has been used for procedural anima­tion [32]. Noises 
are also used for animating cyclic running mo­tion [4], dynamic simulation [18] and animating by multi-level 
sam­pling [35]. Our approach is similar to Soatto s ARMA model [40] that can generate dynamic texture 
by sampling noise, when video frames instead of .gure joints are considered.  Figure 6: Representation 
of observation and state variables. We model the human .gure with 19 joint angles and a global translation. 
After re-parameterizing the joint angles with exponential maps, we construct the observation variable 
Yt with a 60-dimensional vector. The state variable Xt is only 12 ~15-dimensional because it represents 
the subspace of Yt after SVD. (a) (b) (c) Figure 7: Comparison between constrained and unconstrained 
synthesis with a single texton. (a) Original motion. (b) Synthesized motion without end constraints. 
The dynamics deviate from the original one as time progresses. (c) Synthesized motion with end constraints. 
The original dynamics are kept. The last two frames in (a) are chosen as the end constraints. 5.4 Texton 
Synthesis with Constrained LDS We preserve texton dynamics by setting the end constraints of a synthesized 
segment. Since we have kept key poses (starting two poses) for each texton, we can incorporate those 
of the following texton (e.g., the right next one in the texton path) as hard constraints into the synthesis 
process. Let Si and Sj be the two adjacent tex­tons in the motion path, and {xi1,xi2}and {xj1,xj2}be 
the cor­responding key poses. Rearranging the dynamic equation in Eq. 10 (Appendix B), we have x [] t-1 
- A2 -A1 Ixt = D+ Bnt (6) x t+1 where I is the identity matrix. In order to achieve a smooth transi­tion 
from Si to Sj , we set the following hard constraints x1 = xi1,x2 = xi2 xl-1 = CiT Cj xj1,xl = CiT Cj 
xj2 (7) Note that we need to re-project the end constraints since the ob­servation model Ct (Eq. 1) 
is switched between motion textons. Then the in-between frames x3:l-2 =[x3,x4,···,xl-2]T (lis the length 
of the synthesized texton i) can be synthesized by solving a block-banded system of linear equations: 
AX = b (8) where . I -Ai1 I . . -Ai2 -Ai1 . A = . . . . . . I . . . 0 . . . . . . . . . -Ai2 -Ai1 I 
. . . 0 -Ai2 -Ai1 -Ai2 (9) . . Ai1xi2 + Ai2xi1 + Di + Biv2 Ai2x2 + Di + Biv3 . . . Di + Biv4 . . . b 
= . . . . . . . . Di + Bivl-3 . . . -xl-1 + Di + Bivl-2 Ai1xl-1 - xl + Di + Bivl-1 The algorithm is 
summarized in Figure 5. A by-product of the constrained synthesis is smooth transition between two textons 
because the two starting poses of the second texton are guaranteed from the synthesis processes of both 
the .rst and the second textons.  6 Experimental Results We have captured more than 20 minutes of dance 
motion of a professional dancer (performing mostly disco) at high frequency (60Hz) as our training data. 
We chose dance motion in our study because dancing is representative, complex and exhibits stochas­tic 
behavior with repeated patterns. It took approximately 4 hours to learn the motion texture of 49800 frames 
on an Intel Pentium IV 1.4GHz computer with 1G memory. A total of 246 textons are found after learning. 
The length of the textons ranges from 60 to 172 frames. Synthesizing a texton takes only 25ms to 35ms 
because it only involves solving a block-banded system of linear equations. Therefore, we can synthesize 
the character motion in real-time. 6.1 Dealing with High-dimensional Motion Data To deal with the high 
dimensionality of human motion, one must simplify characters, as shown in [33]. K-means clustering and 
PCA are also used in [5, 41] to model the complex deformation of human body. However, all these methods 
failed to .nd the intrinsically low-dimensional subspace of the motion patterns embedded in the high-dimensional 
nonlinear data space. In our system, we model a complex human .gure with 19 joints (57 rotation angles) 
plus its global position (3 translations). For global translation, we compute the displacement because 
we need to accumulate the subsequent translation to the previous frame for synthesis. For 3D rotation, 
we use exponential maps [16, 23] in­stead of Euler angles, unit quaternions [39], or rotation matrices. 
Using exponential maps [16, 23] for representing joint rotation an­gles is essential because they are 
locally linear. Singularities in ex­ponential maps are avoided in our system since the rotation change 
at each step in a texton is small (obviously less than p). The observation Yt is thus a 60-dimensional 
vector, with a 57-dimensional locally reparameterized exponential map and a 3-dimensional translation 
displacement. The state variables Xt should be chosen to be of low dimensionality and highly correlated 
with the observation Yt. Using a locally linear exponential map and the translation displacement, we 
are able to apply SVD on Yt and represent Xt by 12 ~ 15 (depending on the speci.c texton) most signi.cant 
principal vectors of Yt.  (a) (b) Figure 8: Synthesis with two adjacent textons. (a) Synthesizing two 
adjacent textons independently results in a jump at the transition. (b) By setting the starting poses 
of the second texton as the end constraints of the .rst texton, we achieve smooth motion at the transition. 
Pay attention to the difference between the ending frame of the .rst texton and the starting frame of 
the second texton in both (a) and (b), shown as key frames in the middle. (a) (b) (c) Figure 9: (a) 
Original texton. (b) and (c) are synthesized textons perturbed by noise. Notice there are differences 
in the intermediate key poses of (b) and (c). The synthesized textons have the same content as the original 
texton, but different .ne details. 6.2 Examples We demonstrate our approach with a number of synthesis 
and edit­ing examples, also shown in the accompanying videotape. Constrained versus unconstrained texton 
synthesis. Figure 7 shows the results of synthesized motion from a single texton. In this 68-frame motion, 
the dancer changes her orientation from left to right, which represents a swing dance motion (Figure 
7(a)). Given the learnt LDS and the starting two poses, we can synthesize a mo­tion sequence by simply 
sampling noise (Figure 7(b)). The syn­thesized motion looks similar to the original dynamics, but gradu­ally 
deviates as time progresses. By adding constraints of ending frames, we can ensure that the synthesized 
sequence (Figure 7(c)) has similar dynamics to the original. The least-squares solver took approximately 
27ms to synthesize this sequence. Synthesis with two adjacent textons. Figure 8 illustrates that a smooth 
transition can be achieved between two adjacent textons. Because we represent P (X1) in a non-parametric 
way, the starting pose of the second texton may be rather different from the ending pose of the .rst 
texton. Synthesizing these two textons separately results in a jump in the sequence (Figure 8(a)). We 
solve the prob­lem of preserving smooth transitions by applying the starting poses of the second texton 
as the end constraints of the .rst texton (Fig­ure 8(b)). Noise-driven synthesis with different .ne details. 
Figure 9 shows that by varying noise slightly, we can generate two different motions from the same texton. 
Yet the dynamics of two sequences look perceptually similar. Extrapolating new dynamics. The learnt motion 
texton can be generalized to synthesize novel motion. In the motion shown in Figure 10(a), the dancer 
waves her left arm once. From the texton of 127 frames, we can synthesize a longer sequence of 188 frames 
that contains new motion (the dancer waves her arm twice, as shown in Figure 10(d)). Comparison to simple 
linear interpolation with the same constraints is shown in Figure 10(c). Editing a texton. Motion texture 
can be edited at the texton level with precise pose changes. Figure 11 shows that after an intermedi­ate 
frame in a texton sequence is edited, we obtain a new sequence similar to the original one, without the 
need for modifying any other frames. Virtual Choreographer. Figure 1 shows that a sequence of 320 frames 
can be automatically generated given the starting and end­ing frames, and the learnt motion texture. 
This is similar to what a choreographer does. The synthesis has two steps: texton path generation, and 
motion synthesis from textons. Ballroom demo. By randomly sampling motion texture, we can synthesize 
an in.nitely long dance sequence. We present in the videotape a character dancing to the music.  7 Discussion 
Summary. In this paper, we have proposed a two-level statistical model, called motion texture, to capture 
complex motion dynam­ics. Motion texture is represented by a set of motion textons and their distribution. 
Local dynamics are captured by motion textons using linear dynamic systems, while global dynamics are 
modeled by switching between the textons. With the learnt motion texture, we can synthesize and edit 
motion easily. Limitations. Although our approach has been effective on gen­erating realistic and dynamic 
motion, there remain several areas for improvement. First, because we calculate the texton distribution 
by counting how many times a texton is switched to another, our ap­proach is best suited for motions 
consisting of frequently repeated patterns such as disco dance. The synthesized motion may lack global 
variations when the training data is limited. In order to enrich the synthesis variation, we have perturbed 
the dynamics model by Gaussian noise. We did not incorporate any physical model into the synthesis algorithm. 
So there is no guaran­tee that the synthesized motion is physically realistic in the absolute sense. 
However, since the LDS preserve the motion dynamics very well, our algorithm captures the essential properties 
of the original motion. Although our algorithm allows users to edit the motion at the texton level, the 
edited pose can not deviate from the original one too much (as shown in Fig 11). Otherwise the additional 
hard con­straint may contaminate the synthesized texton. Another shortcom­ing of our algorithm is that 
interacting with environment objects is not taken into consideration. Nevertheless, our algorithm provides 
a good initialization of an animation sequence, which can be further improved by other animation tools. 
 (a) (c) (b) (d) Figure 10: (a) Original texton (127 frames). (b) Synthesized texton by LDS (127 frames). 
(c) Linear time warping of the original texton (188 frames) has a slow motion effect. (d) Synthesized 
motion by LDS (188 frames). Our synthesis algorithm generalizes the learnt dynamics by extrapolating 
new motions. Notice the difference between the trajectories in (c) and (d). See the accompanying video 
for comparison. (a) (b) Figure 11: Editing a texton. (a) Original texton. (b) Synthesized texton by 
modifying an intermediate frame, but without changing any other frames. The speci.ed frame is used as 
an additional hard constraint to synthesize the texton.  Acknowledgement We would like to thank Ying-Qing 
Xu for the fruitful discussion on animation and help on motion data capture. We are grateful for the 
extremely valuable help on Siggraph video preparation provided by Yin Li and Gang Chen. Timely proofreading 
by Steve Lin is highly appreciated. Finally, the authors thank Siggraph reviewers for their constructive 
comments. References [1] O. Arikan and D. Forsyth. Interactive motion generation from examples. In Proceedings 
of ACM SIGGRAPH 02, 2002. [2] Z. Bar-Joseph, R. El-Yaniv, D. Lischiniski, and M. Werman. Texture movies: 
Statistical learning of time-varying textures. IEEE Transactions on Visualiza­tion and Computer Graphics, 
7(1):120 135, 2001. [3] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, Oxford, 
1995. [4] B. Bodenheimer, A. Shleyfman, and J. Hodgins. The effects of noise on the perception of animated 
human running. In Computer Animation and Simulation 99, Eurographics Animation Workshop, pages 53 63, 
1999. [5] R. Bowden. Learning statistical models of human motion. In IEEE Workshop on Human Modeling, 
Analysis and Synthesis, CVPR, 2000. [6] M. Brand and A. Hertzmann. Style machines. In Proceedings of 
ACM SIG-GRAPH 00, pages 183 192, 2000. [7] C. Bregler. Learning and recognizing human dynamics in video 
sequences. In Int. Conf. on Computer Vision and Pattern Rocognition, pages 568 574, 1997. [8] A. Bruderlin 
and L. Williams. Motion signal processing. In Proceedings of ACM SIGGRAPH 95, pages 97 104, 1995. [9] 
T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms. The MIT Press, 1997. [10] 
J. de Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. In Proceedings 
of ACM SIGGRAPH 97, pages 361 368, 1997. [11] N. M. Dempster, A. P. Laird, and D. B. Rubin. Maximum likelihood 
from incomplete data via the EM algorithm. J. R. Statist. Soc. B, 39:185 197, 1977. [12] A. Efros and 
W. Freeman. Image quilting for texture synthesis and transfer. In Proceedings of ACM SIGGRAPH 01, pages 
341 346, 2001. [13] A. W. Fitzgibbon. Stochastic rigidity: Image registration for nowhere-static scenes. 
In IEEE Int. Conf. on Computer Vision, pages 662 669, 2001. [14] A. Galata, N. Johnson, and D. Hogg. 
Learning variable length Markov mod­els of behaviour. Computer Vision and Image Understanding, 81(3):398 
413, 2001. [15] M. Gleicher. Retargetting motion to new characters. In Proceedings of ACM SIGGRAPH 98, 
pages 33 42, 1998. [16] F. S. Grassia. Practical parameterization of rotations using the exponential 
map. Journal of Graphics Tools, 3(3):29 48, 1998. [17] C. E. Guo, S. C. Zhu, and Y. N. Wu. Visual learning 
by integrating descriptive and generative methods. In Int. Conf. Computer Vision, 2001. [18] J. K. Hodgins, 
W. L. Wooten, D. C. Brogan, and J. F. O Brien. Animating human athletics. In Proceedings of ACM SIGGRAPH 
95, pages 71 78, 1995. [19] M. Isard and A. Blake. Condensation -conditional density propagation for 
visual tracking. Int l J. Computer Vision, 28(1):5 28, 1998. [20] B. Julesz. Textons, the elements of 
texture perception and their interactions. Nature, 290:91 97, 1981. [21] L. Kovar, M. Gleicher, and F. 
Pighin. Motion graphs. In Proceedings of ACM SIGGRAPH 02, 2002. [22] J. Lee, J. Chai, P. Reisma, and 
J. Hodgins. Interactive control of avartas ani­mated with human motion data. In Proceedings of ACM SIGGRAPH 
02, 2002. [23] J. Lee and S. Y. Shin. A hierarchical approach to interactive motion editing for human-like 
.gures. In Proceedings of ACM SIGGRAPH 99, pages 39 48, 1999. [24] T. Leung and J. Malik. Recognizing 
surfaces using three-dimensional textons. In Int. Conf. on Computer Vision, 1999. [25] L. Liang, C. Liu, 
Y. Xu, B. Guo, and H.-Y. Shum. Real-time texture synthe­sis by patch-based sampling. Technical Report 
MSR-TR-2001-40, Microsoft Research, 2001. [26] L. Ljung. System Identi.cation -Theory for the User. Prentice 
Hall, Upper Saddle River, N.J., 2nd edition, 1999. [27] J. Malik, S. Belongie, J. Shi, and T. Leung. 
Textons, contours and regions: Cue integration in image segmentation. In Int. Conf. Computer Vision, 
pages 918 925, 1999. [28] B. North, A. Blake, M. Isard, and J. Rittscher. Learning and classi.cation 
of complex dynamics. IEEE Transactions on Pattern Analysis and Machine Intel­ligence, 22(9):1016 1034, 
Sep. 2000. [29] V. Pavlovi´ c, J. M. Rehg, T. J. Cham, and K. P. Murphy. A dynamic Bayesian network approach 
to .gure tracking using learned dynamic models. In IEEE International Conference on Computer Vision, 
1999. [30] V. Pavlovi´ c, J. M. Rehg, and J. MacCormick. Impact of dynamic model learn­ing on classi.cation 
of human motion. In IEEE International Conference on Computer Vision and Pattern Recognition, 2000. [31] 
K. Perlin. An image synthesizer. In Computer Graphics (Proceedings of ACM SIGGRAPH 85), pages 287 296, 
1985. [32] K. Perlin and A. Goldberg. Improv: A system for scripting interactive actors in virtual worlds. 
In Proceedings of ACM SIGGRAPH 96, pages 205 216, 1996. [33] Z. Popovic and A. Witkin. ´Physically based 
motion transformation. In Pro­ceedings of ACM SIGGRAPH 99, pages 11 20, 1999. [34] K. Pullen and C. Bregler. 
http://graphics.stanford.edu/~pullen/motion texture. [35] K. Pullen and C. Bregler. Motion capture assisted 
animation: Texturing and synthesis. In Proceedings of ACM SIGGRAPH 02, 2002. [36] L. Rabiner. A tutorial 
on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257 
285, February 1989. [37] A. Sch¨ odl, R. Szeliski, D. H. Salesin, and I. Essa. Video textures. In Proceed­ings 
of ACM SIGGRAPH 00, pages 489 498, 2000. [38] H. J. Shin, J. Lee, M. Gleicher, and S. Y. Shin. Computer 
puppetry: An importance-based approach. ACM Transactions on Graphics, 20(2):67 94, April 2001. [39] K. 
Shoemake. Animating rotation with quaternion curves. In Computer Graph­ics (Proceedings of ACM SIGGRAPH 
85), pages 245 254, 1985. [40] S. Soatto, G. Doretto, and Y. N. Wu. Dynamic textures. In IEEE International 
Conference on Computer Vision, pages 439 446, 2001. [41] L. M. Tanco and A. Hilton. Realistic synthesis 
of novel human movements from a database of motion capture examples. In IEEE Workshop on Human Motion, 
2000. [42] M. Unuma, K. Anjyo, and R. Takeuchi. Fourier principles for emotion-based human .gure animation. 
In Proceedings of ACM SIGGRAPH 95, pages 91 96, 1995. [43] A. Witkin and Z. Popovic.´Motion warping. 
In Proceedings of ACM SIG-GRAPH 95, pages 105 108, 1995. [44] S. C. Zhu, C. E. Guo, Y. Wu, and Y. Wang. 
What are textons. In Proc. of European Conf. on Computer Vision (ECCV), 2002. A. Inference algorithm 
for H and L In inference we use current parameters of textons to recognize the motion sequence. More 
speci.cally, we divide the motion sequence into a sequence of concatenated segments and label each segment 
to a texton. The optimal solution could be derived by maximizing likelihood in Eq. 4. We can ef.ciently 
compute globally optimal segmentation points H = {h2,...,hNs } and segment labels L = {l1,...,lNs } by 
a dynamic programming algorithm. The details of the algorithm are given by the following. We use Gn(t) 
to represent the maximum value of the likelihood derived from dividing the motion sequence ending at 
frame t into a concatenated sequence of n segments. En(t) and Fn(t) are used to represent the label and 
the beginning point of the last segment of the sequence to achieve Gn(t). 1. Initialization G1(t)= max 
P(Y1:t|.i), 1=i=Nt E1(t) = arg max P(Y1:t|.i),Tmin = t = T. i T 2. Loop while 2 = n = , n · Tmin = t 
= T Tmin Gn(t)= max [Gn-1(b - 1)P(Yb:t|.i)Mli] 1=i=Nt (n-1)·Tmin<b=(t-Tmin) En(t),Fn(t) = argmax [Gn-1(b 
- 1)P(Yb:t|.i)Mli] i,b where l = En-1(b - 1). 3. Final solution G(T)= max Gn(T). T 1=n= Tmin Ns =arg 
max Gn(T). n 4. Backtrack the segment points and labels hNs+1 = T +1,lNs = ENs (T),h1 =1 hn = Fn(hn+1 
-1),ln-1 = En-1(hn -1),Ns = n> 1. For T frames and Nt textons, the complexity is O(NtT2). B. Fitting 
an LDS Given a segment of an observation sequence, we can learn the model parameters of a linear dynamic 
system (LDS). In order to capture richer dynamics (velocity and accel­eration), we use a second-order 
linear dynamic system: Dynamics Model: xt+1 = A1xt + A2xt-1 + D + Bvt (10) Observation Model: yt = Ctxt 
+ Wt (11) where vt ~ N(0,1), Wt ~ N(0,R), and R is the covariance matrix. Note that this model could 
also be written in the standard [][] xt A1 A2 form of Eq. 1 by setting Xt = , At = I 0 , Vt ~ xt-1 [ 
][] DBT B 0 N(,). 0 00 For any linear dynamic system, the choice of its model pa­rameters is not unique. 
For instance, by choosing X = TX, A = TAT-1 , C = CT-1, in which T is an arbitrary full rank square matrix, 
we will obtain another linear dynamic system which generates exactly the same observation. In order to 
.nd a unique so­lution, canonical model realizations need to be considered, as sug­gested in [40]. And 
a closed form approximated estimation of the model parameters could be derived as follows [26]: 1. Observation 
Model 1 . We calculate the SVD of the observa­tion sequence Y1:T , [U,S,V ]= SV D(Y1:T ), and set: C 
= U, X1:T = SV T . (12) 2. Dynamics Model. The maximum likelihood estimation of A1,A2,D,B is given by: 
-1 R1,1 R2,1 [A1,A2]=[R0,0,R0,1] · R1,2 R2,2 D = 1 T - 2 Q0 - 2 AiQi (13) i=1 2 BBT =1 R0,0 - AiRi,0 
T - 2i=1 where Qi,Ri,j is: T Qi = Xt-i t=3 T Ri,j Xt-i(Xt-j )j . = T - 1 QiQT T - 2 t=3 In learning, 
we may need to .t an LDS on several segments. For this purpose, we can concatenate such segments into 
a sequence and apply the same algorithm except drop some terms on boundaries of the segments when Qi 
and Rij are calculated. 1We do not incorporate Wt in the learning process because the observa­tion noise 
is not used for synthesis.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566605</article_id>
		<sort_key>473</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Motion graphs]]></title>
		<page_from>473</page_from>
		<page_to>482</page_to>
		<doi_number>10.1145/566570.566605</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566605</url>
		<abstract>
			<par><![CDATA[In this paper we present a novel method for creating realistic, controllable motion. Given a corpus of motion capture data, we automatically construct a directed graph called a <i>motion graph</i> that encapsulates connections among the database. The motion graph consists both of pieces of original motion and automatically generated transitions. Motion can be generated simply by building walks on the graph. We present a general framework for extracting particular graph walks that meet a user's specifications. We then show how this framework can be applied to the specific problem of generating different styles of locomotion along arbitrary paths.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation with constraints]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P339849</person_id>
				<author_profile_id><![CDATA[81100314305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lucas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kovar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin-Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39038559</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin-Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48022732</person_id>
				<author_profile_id><![CDATA[81100026067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fr&#233;d&#233;ric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pighin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARIKAN, O., AND FORSYTHE, D. 2002. Interactive motion generation from examples. In Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BOWDEN, R. 2000. Learning statistical models of human motion. In IEEE Workshop on Human Modelling, Analysis, and Synthesis, CVPR 2000, IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BRAND, M., AND HERTZMANN, A. 2000. Style machines. In Proceedings of ACM SIGGRAPH 2000, Annual Conference Series, ACM SIGGRAPH, 183-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>241078</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND CALVERT, T. 1996. Knowledge-driven, interactive animation of human running. In Graphics Interface, Canadian Human-Computer Communications Society, 213-221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND WILLIAMS, L. 1995. Motion signal processing. In Proceedings of ACM SIGGRAPH 95, Annual Conference Series, ACM SIGGRAPH, 97-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for physics-based character animation. In Proceedings of ACM SIGGRAPH 2001, Annual Conference Series, ACM SIGGRAPH, 251-260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>377412</ref_obj_id>
				<ref_obj_pid>376890</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GALATA, A., JOGNSON, N., ANDHOGG, D. 2001. Learning variable-length markov models of behavior. Computer Vision and Image Understanding Journal 81, 3, 398-413.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargeting motion to new characters. In Proceedings Of ACM SIGGRAPH 98, Annual Conference Series, ACM SIGGRAPH, 33-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364400</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 2001. Motion path editing. In Proceedings 2001 ACM Symposium on Interactive 3D Graphics, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O'BRIEN, J. F. 1995. Animating human athletics. In Proceedings of ACM SIGGRAPH 95, Annual Conference Series, ACM SIGGRAPH, 71-78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545277</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[KOVAR, L., GLEICHER, M., AND SCHREINER, J. 2002. Footskate cleanup for motion capture editing. Tech. rep., University of Wisconsin, Madison.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274990</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[LAMOURET, A., AND PANNE, M. 1996. Motion synthesis by example. Computer animation and Simulation, 199-212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like figures. In Proceedings ofACM SIGGRAPH 99, Annual Conference Series, ACM SIGGRAPH, 39-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LEE, J., CHAI, J., REITSMA, P. S. A., HODGINS, J. K., AND POLLARD, N. S. 2002. Interactive control of avatars animated with human motion data. In Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LEE, J. 2000. A hierarchical approach to motion analysis and synthesis for articulated figures. PhD thesis, Department of Computer Science, Korea Advanced Institute of Science and Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LI, Y., WANG, T., AND SHUM, H.-Y. 2002. Motion texture: A two-level statistical model for character motion synthesis. In Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MIZUGUCHI, M., BUCHANAN, J., AND CALVERT, T. 2001. Data driven motion transitions for interactive games. In Eurographics 2001 Short Presentations.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>823423</ref_obj_id>
				<ref_obj_pid>822088</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MOLINA-TANCO, L., AND HILTON, A. 2000. Realistic synthesis of novel human movements from a database of motion capture examples. In Proceedings of the Workshop on Human Motion, IEEE Computer Society, 137-142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MULTON, F., FRANCE, L., CANI, M.-P., AND DEBUNNE, G. 1999. Computer animation of human walking: a survey. The Journal of Visualization and Computer Animation 10, 39-54. Published under the name Marie-Paule Cani-Gascuel.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND GOLDBERG, A. 1996. Improv: A system for scripting interactive actors in virtual worlds. In Proceedings of ACM SIGGRAPH 96, ACM SIGGRAPH, 205-216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1995. Real time responsive animation with personality. IEEE Transactions on Visualization and Computer Graphics 1, 1 (Mar.), 5-15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872897</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2000. Animating by multi-level sampling. In IEEE Computer Animation Conference, CGS and IEEE, 36-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566608</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2002. Motion capture assisted animation: Texturing and synthesis. In Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237229</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., GUENTER, B., BODENHEIMER, B., AND COHEN, M. F. 1996. Efficient generation of motion transitions using spacetime constraints. In Proceedings of ACM SIGGRAPH 1996, Annual Conference Series, ACM SIGGRAPH, 147-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., COHEN, M., AND BODENHEIMER, B. 1998. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics and Application 18, 5, 32-40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SCH&#214;DL, A., SZELISKI, R., SALESIN, D., AND ESSA, I. 2000. Video textures. In Proceedings of ACM SIGGRAPH 2000, Annual Conference Series, ACM SIGGRAPH, 489-498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383288</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SUN, H. C., AND METAXAS, D. N. 2001. Automating gait animation. In Proceedings of ACM SIGGRAPH 2001, Annual Conference Series, ACM SIGGRAPH, 261-270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[WASHBURN, D. 2001. The quest for pure motion capture. Game Developer (December).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618477</ref_obj_id>
				<ref_obj_pid>616049</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WILEY, D., AND HAHN, J. 1997. Interpolation synthesis of articulated figure motion. IEEE Computer Graphics and Application 17, 6, 39-45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND POPOVI&#262;, Z. 1995. Motion warping. In Proceedings of ACM SlGGRAPH 95, Annual Conference Series, ACM SIGGRAPH, 105-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Graphs Lucas Kovar Michael Gleicher* Fr´ed´eric Pighin University of Wisconsin-Madison University 
of Wisconsin-Madison University of Southern California  Abstract In this paper we present a novel method 
for creating realistic, con­trollable motion. Given a corpus of motion capture data, we au­tomatically 
construct a directed graph called a motion graph that encapsulates connections among the database. The 
motion graph consists both of pieces of original motion and automatically gener­ated transitions. Motion 
can be generated simply by building walks on the graph. We present a general framework for extracting 
par­ticular graph walks that meet a user s speci.cations. We then show how this framework can be applied 
to the speci.c problem of gen­erating different styles of locomotion along arbitrary paths. CR Categories: 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; Keywords: motion synthesis, 
motion capture, animation with con­straints 1 Introduction Realistic human motion is an important part 
of media like video games and movies. More lifelike characters make for more immer­sive environments 
and more believable special effects. At the same time, realistic animation of human motion is a challenging 
task, as people have proven to be adept at discerning the subtleties of human movement and identifying 
inaccuracies. One common solution to this problem is motion capture. However, while motion capture is 
a reliable way of acquiring realistic human motion, by itself it is a technique for reproducing motion. 
Motion capture data has proven to be dif.cult to modify, and editing tech­niques are reliable only for 
small changes to a motion. This limits the utility of motion capture if the data on hand isn t suf.ciently 
* e-mail:{kovar,gleicher}@cs.wisc.edu e-mail:pighin@ict.usc.edu Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Institute for Creative Technologies 
similar to what is desired, then often there is little that can be done other than acquire more data, 
a time-consuming and expensive pro­cess. This in particular is a problem for applications that require 
motion to be synthesized dynamically, such as interactive environ­ments. Our goal is to retain the realism 
of motion capture while also giving a user the ability to control and direct a character. For example, 
we would like to be able to ask a character to walk around a room without worrying about having a piece 
of motion data that contains the correct number of steps and travels in the right directions. We also 
need to be able to direct characters who can perform multiple actions, rather than those who are only 
capable of walking around. This paper presents a method for synthesizing streams of motions based on 
a corpus of captured movement while preserving the qual­ity of the original data. Given a set of motion 
capture data, we com­pile a structure called a motion graph that encodes how the captured clips may be 
re-assembled in different ways. The motion graph is a directed graph wherein edges contain either pieces 
of original mo­tion data or automatically generated transitions. The nodes then serve as choice points 
where these small bits of motion join seam­lessly. Because our methods automatically detect and create 
transi­tions between motions, users needn t capture motions speci.cally designed to connect to one another. 
If desired, the user can tune the high-level structure of the motion graph to produce desired degrees 
of connectivity among different parts. Motion graphs transform the motion synthesis problem into one 
of selecting sequences of nodes, or graph walks. By drawing upon algorithms from graph theory and AI 
planning, we can extract graph walks that satisfy certain properties, thereby giving us control over 
the synthesized motions. To demonstrate the potential of our approach, we introduce a sim­ple example. 
We were donated 78.5 seconds of motion capture, or about 2400 frames of animation, of a performer randomly 
walking around with both sharp and smooth turns. Since the motion was donated, we did not carefully plan 
out each movement, as the liter­ature suggests is critical to successful application of motion capture 
data [Washburn 2001]. From this data we constructed a motion graph and used an algorithm described later 
in this paper to extract motions that travelled along paths sketched on the ground. Charac­teristic movements 
of the original data like sharp turns were auto­matically used when appropriate, as seen in Figure 1. 
It is possible to place additional constraints on the desired motion. For example, we noticed that part 
of the motion had the character sneaking around. By labelling these frames as special, we were able to 
specify that at certain points along the path the character must only use sneaking movements, and at 
other parts of the motion it must use normal walking motions, as is also shown in Figure 1.  Figure 
1: The top images show original motion capture data; two are walking motions and one is a sneaking motion. 
The black curves show the paths travelled by the character. The bottom images show new motion generated 
by a motion graph built out of these examples plus their mirror images. Images 1 and 2 show the result 
of having the motion graph .t walking motion to the smooth yellow paths. The black curve is the actual 
position of the center of mass on each frame. Image 3 shows motion formed by having the character switch 
from walking to sneaking halfway down the path. The remainder of this paper is organized as follows. 
In Section 2 we describe related work. In Section 3 we describe how a motion graph is constructed from 
a database of motion capture. In Section 4 we set forth a general framework for extracting motion from 
the motion graph that meets user speci.cations. Section 5 discusses the speci.c problem of generating 
movements along a path and how it is handled in our framework. We conclude in Section 6 with a discussion 
of the scalability of our approach to large data sets and potential future work. 2 Related Work Much 
previous work with motion capture has revolved around editing individual clips of motion. Motion warping 
[Witkin and Popovi´c 1995] can be used to smoothly add small changes to a mo­tion. Retargeting [Gleicher 
1998; Lee and Shin 1999] maps the motion of a performer to a character of different proportions while 
retaining important constraints like footplants. Various signal pro­cessing operations [Bruderlin and 
Williams 1995] can be applied to motion data. Our work is different from these efforts in that it involves 
creating continuous streams of motion, rather than modi­fying speci.c clips. One strategy for motion 
synthesis is to perform multi-target blends among a set of examples, yielding a continuous space of parame­terized 
motion. Wiley and Hahn [1997] used linear interpolation to create parameterizations of walking at various 
inclinations and reaching to various locations. Rose et al. [1998] used radial ba­sis functions to blend 
among clips representing the same motion performed in different styles. These works have a focus comple­mentary 
to ours: while they are mainly concerned with generating parameterizations of individual clips, we are 
concerned with con­structing controllable sequences of clips. Another popular approach to motion synthesis 
is to construct statis­tical models. Pullen and Bregler [2000] used kernel-based proba­bility distributions 
to synthesize new motion based on the statistical properties of example motion. Coherency was added to 
the model by explicitly accounting for correlations between parameters. Bow­den [2000], Galata et al. 
[2001], and Brand and Hertzmann [2000] all processed motion capture data by constructing abstract states 
which each represent entire sets of poses. Transition probabilities between states were used to drive 
motion synthesis. Since these statistical models synthesize motion based on abstractions of data rather 
than actual data, they risk losing important detail. In our work we have tighter guarantees on the quality 
of generated mo­tion. Moreover, these systems did not focus on the satisfaction of high-level constraints. 
We generate motion by piecing together example motions from a database. Numerous other researchers have 
pursued similar strate­gies. Perlin [1995] and Perlin and Goldberg [1996] used a rule­based system and 
simple blends to attach procedurally generated motion into coherent streams. Faloutsos et al. [2001] 
used sup­port vector machines to create motion sequences as compositions of actions generated from a 
set of physically based controllers. Since our system involves motion capture data, rather than proce­dural 
or physically based motion, we require different approaches to identifying and generating transitions. 
Also, these systems were mainly concerned with appropriately generating individual transi­tions, whereas 
we address the problem of generating entire motions (with many transitions) that meet user-speci.ed criteria. 
Lamouret and van de Panne [1996] developed a system that used a database to extract motion meeting high-level 
constraints. However, their system was applied to a simple agent with .ve degrees of freedom, whereas 
we generate motion for a far more sophisticated charac­ter. Molina-Tanco and Hilton [2000] used a state-based 
statistical model similar to those mentioned in the previous paragraph to re­arrange segments of original 
motion data. These segments were attached using linear interpolation. The user could create motion by 
selecting keyframe poses, which were connected with a high­probability sequence of states. Our work considers 
more general and sophisticated sets of constraints. Work similar to ours has been done in the gaming 
industry to meet the requirements of online motion generation. Many companies use move trees [Mizuguchi 
et al. 2001], which (like motion graphs) are graph structures representing connections in a database 
of motion. However, move trees are created manually short motion clips are collected in carefully scripted 
capture sessions and blends are cre­ated by hand using interactive tools. Motion graphs are constructed 
automatically. Also, move trees are typically geared for rudimen­tary motion planning ( I want to turn 
left, so I should follow this transition ), as opposed to more complicated objectives. The generation 
of transitions is an important part of our approach. Early work in this area was done by Perlin [1995], 
who presented a simple method for smoothly interpolating between two clips to cre­ate a blend. Lee [2000] 
de.ned orientation .lters that allowed these blending operations to be performed on rotational data in 
a more principled fashion. Rose et al. [1996] presented a more complex method for creating transitions 
that preserved kinematic constraints and basic dynamic properties. Our main application of motion graphs 
is to control a character s locomotion. This problem is important enough to have received a great deal 
of prior attention. Because a character s path isn t generally known in advance, synthesis is required. 
Procedural and physically based synthesis methods have been developed for a few activities such as walking 
[Multon et al. 1999; Sun and Metaxas 2001] and running [Hodgins et al. 1995; Bruderlin and Calvert 1996]. 
While techniques such as these can generate .exible motion paths, the current range of movement styles 
is limited. Also, these methods do not produce the quality of motion attainable by hand animation or 
motion capture. While Gleicher [2001] presented a method for editing the path traversed in a clip of 
motion capture, it did not address the need for continuous streams of motion, nor could it choose which 
clip is correct to .t a path (e.g. that a turning motion is better when we have a curved path). Our basic 
approach detecting transitions, constructing a graph, and using graph search techniques to .nd sequences 
satisfying user demands has been applied previously to other problems. Sch¨odl et al. [2000] developed 
a similar method for synthesizing seamless streams of video from example footage and driving these streams 
according to high-level user input. Since writing this paper, we have learned of similar work done concurrently 
by a number of research groups. Arikan and Forsythe [2002] constructed from a motion database a hierarchi­cal 
graph similar to ours and used a randomized search algorithm to extract motion that meets user constraints. 
Lee et al. [2002] also constructed a graph and generated motion via three user interfaces: a list of 
choices, a sketch-based interface similar to what we use for path .tting (Section 5), and a live video 
feed. Pullen and Bre­gler [2002] keyframed a subset of a character s degrees of freedom and matched small 
segments of this keyframed animation with the lower frequency bands of motion data. This resulted in 
sequences of short clips forming complete motions. Li et al [2002] generated a two-level statistical 
model of motion. At the lower level were lin­ear dynamic systems representing characteristic movements 
called textons , and the higher level contained transition probabilities among textons. This model was 
used both to generate new motion based on user keyframes and to edit existing motion. 3 Motion Graph 
Construction In this section, we de.ne the motion graph structure and the proce­dure for constructing 
it from a database of clips. A clip of motion is de.ned as a regular sampling of the charac­ter s parameters, 
which consist of the position of the root joint and quaternions representing the orientations of each 
joint. We Figure 2: Consider a motion graph built from two initial clips. (top) We can trivially insert 
a node to divide an initial clip into two smaller clips. (bottom) We can also insert a transition joining 
either two different initial clips or different parts of the same initial clip. also allow clips (or, 
more generally, sets of frames) to be anno­tated with other information, such as descriptive labels ( 
walking, karate ) and constraint information (left heel must be planted on these frames). A motion graph 
is a directed graph where all edges correspond to clips of motion. Nodes serve as choice points connecting 
these clips, i.e., each outgoing edge is potentially the successor to any incoming edge. A trivial motion 
graph can be created by placing all the initial clips from the database as arcs in the graph. This cre­ates 
a disconnected graph with 2n nodes, one at the beginning and end of each clip. Similarly, an initial 
clip can be broken into two clips by inserting a node, since the later part of the motion is a valid 
successor to the earlier part (see Figure 2). A more interesting graph requires greater connectivity. 
For a node to have multiple outgoing edges, there must be multiple clips that can follow the clip(s) 
leading into the node. Since it is unlikely that two pieces of original data are suf.ciently similar, 
we need to create clips expressly for this purpose. Transitions are clips designed such that they can 
seamlessly connect two segments of original data. By introducing nodes within the initial clips and inserting 
transition clips between otherwise disconnected nodes, we can create a well­connected structure with 
a wide range of possible graph walks (see Figure 2). Unfortunately, creating transitions is a hard animation 
problem. Imagine, for example, creating a transition between a run and a back.ip. In real life this would 
require several seconds for an ath­lete to perform, and the transition motion looks little like the mo­tions 
it connects. Hence the problem of automatically creating such a transition is arguably as dif.cult as 
that of creating realistic mo­tion in the .rst place. On the other hand, if two motions are close to 
each other then simple blending techniques can reliably gener­ate a transition. In light of this, our 
strategy is to identify portions of the initial clips that are suf.ciently similar that straightforward 
blending is almost certain to produce valid transitions. The remainder of this section is divided into 
three parts. First we describe our algorithm for detecting a set of candidate transition points. In the 
following two sections we discuss how we select among these candidate transitions and how blends are 
created at the chosen transition points. Finally, we explain how to prune the graph to eliminate problematic 
edges. 3.1 Detecting Candidate Transitions As in our system, motion capture data is typically represented 
as vectors of parameters specifying the root position and joint rota­tions of a skeleton on each frame. 
One might attempt to locate transition points by computing some vector norm to measure the difference 
between poses at each pair of frames. However, such a simple approach is ill-advised, as it fails to 
address a number of important issues: 1. Simple vector norms fail to account for the meanings of the 
parameters. Speci.cally, in the joint angle representation some parameters have a much greater overall 
effect on the character than others (e.g., hip orientation vs. wrist orienta­tion). Moreover, there is 
no meaningful way to assign .xed weights to these parameters, as the effect of a joint rotation on the 
shape of the body depends on the current con.guration of the body. 2. A motion is de.ned only up to 
a rigid 2D coordinate trans­formation. That is, the motion is fundamentally unchanged if we translate 
it along the .oor plane or rotate it about the ver­tical axis. Hence comparing two motions requires identifying 
compatible coordinate systems. 3. Smooth blends require more information than can be obtained at individual 
frames. A seamless transition must account not only for differences in body posture, but also in joint 
veloci­ties, accelerations, and possibly higher-order derivatives.  Our similarity metric incorporates 
each of these considerations. To motivate it, we note that the skeleton is only a means to an end. In 
a typical animation, a polygonal mesh is deformed according to the skeleton s pose. This mesh is all 
that is seen, and hence it is a natural focus when considering how close two frames of animation are 
to each other. For this reason we measure the distance between two frames of animation in terms of a 
point cloud driven by the skeleton. Ideally this point cloud is a downsampling of the mesh de.ning the 
character. gg To calculate the distance D( i, !j) between two frames i and !j, we consider the point 
clouds formed over two windows of frames of user-de.ned length k, one bordered at the beginning by g 
i and the other bordered at the end by !j. That is, each point cloud is the composition of smaller point 
clouds representing the pose at each frame in the window. The use of windows of frames effectively incorporates 
derivative information into the metric, and is similar to the approach in [Sch¨odl et al. 2000]. The 
size of the g windows are the same as the length of the transitions, so D( i, !j) is affected by every 
pair of frames that form the transition. We use a value of k corresponding to a window of about a third 
of a second in length, as in [Mizuguchi et al. 2001] g The distance betweeni and !j may be calculated 
by computing a weighted sum of squared distances between corresponding points pi and p'i in the two point 
clouds. To address the problem of .nd­ing coordinate systems for these point clouds (item 2 in the above 
list), we calculate the minimal weighted sum of squared distances given that an arbitrary rigid 2D transformation 
may be applied to the second point cloud: ' min wi.pi - T ,x0,z0 pi.2 (1) ,x0,z0 i where the linear transformation 
T ,x0,z0 rotates a point p about the y (vertical) axis by e degrees and then translates it by (x0, z0). 
The Figure 3: An example error function for two motions. The entry at (i, j) contains the error for 
making a transition from the ith frame of the .rst motion to the jth frame of the second. White values 
correspond to lower errors and black values to higher errors. The colored dots represent local minima. 
index is over the number of points in each point cloud. The weights wi may be chosen both to assign more 
importance to certain joints (e.g., those with constraints) and to taper off towards the end of the window. 
This optimization has a closed-form solution: e = arctan Li wi(xiz' i - x' izi) - 1 i wi (xz' - x' z) 
Li wi(xix' i + ziz' i) - 1 i wi (xx' + zz') (2) x0 = 1 Li wi (x - x' cos(e) - z' sine) (3) z0 = 1 Li 
wi (z + x' sin(e) - z' cos e) (4) where x = Li wixi and the other barred terms are de.ned similarly. 
We compute the distance as de.ned above for every pair of frames in the database, forming a sampled 2D 
error function. Figure 3 shows a typical result. To make our transition model more com­pact, we .nd all 
the local minima of this error function, thereby ex­tracting the sweet spots at which transitions are 
locally the most opportune. This tactic was also used in [Sch¨odl et al. 2000]. These local minima are 
our candidate transition points. 3.2 Selecting Transition Points A local minimum in the distance function 
does not necessarily im­ply a high-quality transition; it only implies a transition better than its neighbors. 
We are speci.cally interested in local minima with small error values. The simplest approach is to only 
accept local minima below an empirically determined threshold. This can be done without user intervention. 
However, often users will want to set the threshold themselves to pick an acceptable tradeoff between 
having good transitions (low threshold) and having high connectiv­ity (high threshold). Different kinds 
of motions have different .delity requirements. For example, walking motions have very exacting requirements 
on the transitions people have seen others walk nearly every day since birth and consequently have a 
keen sense of what a walk should look like. On the other hand, most people are less familiar with bal­let 
motions and would be less likely to detect inaccuracies in such motion. As a result, we allow a user 
to apply different thresholds to different pairs of motions; transitions among ballet motions may have 
a higher acceptance threshold than transitions among walking motions. 3.3 Creating Transitions g If 
D( i, !j) meets the threshold requirements, we create a tran­ gg sition by blending frames i to i+k-1 
with frames !j-k+1 to !j, inclusive. The .rst step is to apply the appropriate aligning 2D transformation 
to motion !. Then on frame p of the transition (0 < p < k) we linearly interpolate the root positions 
and perform spherical linear interpolation on joint rotations: Rp = a( p)R +[1 - a( p)]R (5) i+pj-k+1+p 
ii qp = slerp(qi , q , a( p)) (6) i+pj-k+1+p i where Rp is the root position on the pth transition frame 
and qis p the rotation of the ith joint on the pth transition frame. To maintain continuity we choose 
the blend weights a( p) accord­ing to the conditions that a( p)= 1 for p <-1, a( p)= 0 for p ? k, and 
that a( p) has C1 continuity everywhere. This requires p + 1 p + 1 a( p)= 2()3 - 3()2 + 1, -1 < p < k 
(7) kk Other transition schemes, such as [Rose et al. 1996], may be used in place of this one. The use 
of linear blends means that constraints in the original mo­tion may be violated. For example, one of 
the character s feet may slide when it ought to be planted. This can be corrected by using constraint 
annotations in the original motions. We treat constraints as binary signals: on a given frame a particular 
constraint either ex­ists or it does not. Blending these signals in analogy to equations 5 g and 6 amounts 
to using the constraints from in the .rst half of the transition and the constraints from !in the second 
half. In this manner each transition is automatically annotated with constraint information, and these 
constraints may later be enforced as a post­processing step when motion is extracted form the graph. 
We will discuss constraint enforcement in more detail in the next section. Descriptive labels attached 
to the motions are carried along into transitions. Speci.cally, if a transition frame is a blend between 
a frame with a set of labels L1 and another frame with a set of labels L2, then it has the union of these 
labels L1 U L2. Figure 4: A simple motion graph. The largest strongly connected component is [1, 2, 
3, 6, 7, 8]. Node 4 is a sink and 5 is a dead end. 3.4 Pruning The Graph In its current state there 
are no guarantees that the graph can syn­thesize motion inde.nitely, since there may be nodes (called 
dead ends) that are not part of any cycle (see Figure 4). Once such a node is entered there is a bound 
on how much additional motion can be generated. Other nodes (called sinks) may be part of one or more 
cycles but nonetheless only be able to reach a small fraction of the total number of nodes in the graph. 
While arbitrarily long motion may still be generated once a sink is entered, this motion is con­.ned 
to a small part of the database. Finally, some nodes may have incoming edges such that no outgoing edges 
contain the same set of descriptive labels. This is dangerous since logical discontinuities may be forced 
into a motion. For example, a character currently in a boxing motion may have no choice but to transition 
to a ballet motion. To address these problems, we prune the graph such that, starting from any edge, 
it is possible to generate arbitrarily long streams of motion of the same type such that as much of the 
database as possible is used. This is done as follows. Every frame of original data is associated with 
a (possibly empty) set of labels. Say there are n unique sets. For each set, form the subgraph consisting 
of all edges whose frames have exactly this set of labels. Compute the strongly connected components 
(SCCs) of this subgraph, where an SCC is a maximal set of nodes such that there is a connecting graph 
walk for any ordered pair of nodes (u, v). The SCCs can be computed in O(V + E) time using an algorithm 
due to Tarjan. We eliminate from this subgraph (and hence the original motion graph) any edge that does 
not attach two nodes in the largest SCC. Once this process is completed for all n label sets, any nodes 
with no edges are discarded. A warning is given to the user if the largest SCC for a given set of labels 
contains below a threshold number of frames. Also, a warning is given if for any ordered pair of SCCs 
there is no way to transition from the .rst to the second. In either case, the user may wish to adjust 
the transition thresholds (Section 3.2) to give the graph greater connectivity.  4 Extracting Motion 
By this stage we have .nished constructing the motion graph. Af­ter describing exactly how a graph walk 
can be converted into dis­playable motion, we will consider the general problem of extracting motion 
that satis.es user constraints. Our algorithm involves solv­ing an optimization problem, and so we conclude 
this section with some general recommendations on how to pose the optimization. 4.1 Converting Graph 
Walks To Motion Since every edge on the motion graph is a piece of motion, a graph walk corresponds to 
a motion generated by placing these pieces one after another. The only issue is to place each piece in 
the cor­rect location and orientation. In other words, each frame must be transformed by an appropriate 
2D rigid transformation. At the start of a graph walk this transformation is the identity. Whenever we 
exit a transition edge, the current transformation is multiplied by the transformation that aligned the 
pieces of motion connected by the transition (Section 3.1). As noted in Section 3.3, the use of linear 
blends to create transitions can cause artifacts, the most common of which is feet that slide when they 
ought to be planted. However, every graph walk is au­tomatically annotated with constraint information 
(such as that the foot must be planted). These constraints are either speci.ed directly in the original 
motions or generated as in Section 3.3, depending on whether the frame is original data or a transition. 
These constraints may be satis.ed using a variety of methods, such as [Gleicher 1998] or [Lee and Shin 
1999]. In our work we used the method described in [Kovar et al. 2002]. 4.2 Searching For Motion We 
are now in a position to consider the problem of .nding motion that satis.es user-speci.ed requirements. 
It is worth .rst noting that only very special graph walks are likely to be useful. For exam­ple, while 
a random graph walk will generate a continuous stream of motion, such an algorithm has little use other 
than an elaborate screen saver. As a more detailed example, consider computing an all-pairs shortest 
graph walk table for the graph. That is, given a suitable metric say, time elapsed or distance travelled 
 we can use standard graph algorithms like Floyd-Warshall to .nd for each pair of nodes u and v the connecting 
graph walk that minimizes the metric. With this in hand we could, for example, generate the mo­tion that 
connects one clip to another as quickly as possible. This is less useful than it might appear at .rst. 
First, there are no guaran­tees that the shortest graph walk is short in an absolute sense. In our larger 
test graphs (between a few and several thousand nodes) the average shortest path between any two nodes 
was on the order of two seconds. This is not because the graphs were poorly connected. Since the transitions 
were about one-third of a second apiece, this means there were on average only .ve or six transitions 
separat­ing any two of the thousands of nodes. Second, there is no control over what happens during the 
graph walk we can t specify what direction the character travels in or where she ends up. More generally, 
the sorts of motions that a user is likely to be in­terested in probably don t involve minimizing metrics 
as simple as total elapsed time. However, for complicated metrics there is typ­ically no simple way of 
.nding the globally optimal graph walk. Hence we focus instead on local search methods that try to .nd 
a satisfactory graph walk within a reasonable amount of time. We now present our framework for extracting 
graph walks that con­form to a user s speci.cations. We cast motion extraction as a search problem and 
use branch and bound to increase the ef.ciency of this search. The user supplies a scalar function g(w, 
e) that eval­uates the additional error accrued by appending an edge e to the existing path w, which 
may be the empty path /0. The total error f (w) of the path is de.ned as follows: n f (w)= f ([e1,... 
, en]) = g([e1,... , ei-1], ei) (8) i=1 where w is comprised of the edges e1,... , en. We require g(w, 
e) to be nonnegative, which means that we can never decrease the total error by adding more edges to 
a graph walk. In addition to f and g, the user must also supply a halting condi­tion indicating when 
no additional edges should be added to a graph walk. A graph walk satisfying the halting condition is 
called com­plete. The start of the graph walk may either be speci.ed by the user or chosen at random. 
Our goal is .nd a complete graph walk w that minimizes f . To give the user control over what sorts of 
motions should be considered in the search, we allow restrictions on what edges may be appended to a 
given walk w. For example, the user may decide that within a par­ticular window of time a graph walk 
may only contain sneaking edges. A na¨ive solution is to use depth-.rst search to evaluate f for all 
complete graph walks and then select the best one. However, the number of possible graph walks grows 
exponentially with the av­erage size of a complete graph walk. To address this we use a branch and bound 
strategy to cull branches of the search that are in­capable of yielding a minimum. Since g(w, e) by assumption 
never decreases, f (w) is a lower bound on f (w+ v) for any v, where w+ v is the graph walk composed 
of v appended to w. Thus we can keep track of the current best complete graph walk wopt and immediately 
halt any branch of the search for which the graph walk s error ex­ceeds f (wopt ). Also, the user may 
de.ne a threshold error . such that if f (w) < ., then w is considered to be good enough and the search 
is halted. Branch and bound is most successful when we can attain a tight lower bound early in the search 
process. For this reason it is worth­while to have a heuristic for ordering the edges we explore out 
of a particular node. One simple heuristic is to order the children greedily that is, given a set of 
unexplored children c1,... , cn, we search the one that minimizes g(w, ci). While branch and bound reduces 
the number of graph walks we have to test against f , it does not change the fact that the search process 
is inherently exponential it merely lowers the effective branching factor. For this reason we generate 
a graph walk incre­mentally. At each step we use branch and bound to .nd an optimal graph walk of n frames. 
We retain the .rst m frames of this graph walk and use the .nal retained node as a starting point for 
another search. This process continues until a complete graph walk is gen­erated. In our implementation 
we used values of n from 80 to 120 frames (2 23 to 4 seconds) and m from 25 to 30 frames (about one second). 
Sometimes it is useful to have a degree of randomness in the search process, such as when one is animating 
a crowd. There are a cou­ple of easy ways to add randomness to the search process without sacri.cing 
a good result. The .rst is to select a start for the search at random. The second is retain the r best 
graph walks at the end of each iteration of the search and randomly pick among the ones whose error is 
within some tolerance of the best solution. 4.3 Deciding What To Ask For Since the motion extracted 
from the graph is determined by the function g, it is worth considering what sorts of functions are likely 
to produce desirable results. To understand the issues involved, we consider a simple example. Imagine 
we want to lay down two clips on the .oor and create a motion that starts at the .rst clip and ends at 
the second. Both clips must end up in the speci.ed position and orientation. We can formally state this 
problem as follows: given a starting node N in the graph and a target edge e, .nd a graph walk Figure 
5: The above motion was generated using the search algorithm discussed in this section. The halting condition 
was to play a speci.c clip of two kicking motions. The error of a complete graph walk (which necessarily 
ended with the kicking clip) was determined by how far away this kicking clip was from being in a particular 
position and orientation. The character spends approximately seven seconds making minute adjustments 
to its orientation in an attempt to better align itself with the .nal clip. The highlighted line shows 
the the path of the target clip in its desired position and orientation. that ends with e such that the 
transformation T applied to e is as close as possible to a given transformation T' . What one will re­ceive 
is a motion like in Figure 5, where the initial clip is a walking motion and the .nal clip is a kick. 
The character turns around in place several times in an attempt to better line up with the target clip. 
While it s conceivable that given a larger database we would have found a better motion, the problem 
here is with the function we passed into the search algorithm. First, it gives no guidance as to what 
should be done in the middle of the motion; all that matters is that the .nal clip be in the right position 
and orientation. This means the character is allowed to do whatever is possible in order to make the 
.nal .t, even if the motion is nothing that a real person would do. Second, the goal is probably more 
speci.c than neces­sary. If it doesn t matter what kick the character does, then it should be allowed 
to choose a kick that doesn t require such effort to aim. More generally, there are two lessons we can 
draw from this ex­ample. First, g should give some sort of guidance throughout the entire motion, as 
arbitrary motion is almost never desirable. Sec­ond, g should be no more restrictive than necessary, 
in order to give the search algorithm more goals to seek. Note the tradeoff here guiding the search toward 
a particular result must be balanced against unduly preventing it from considering all available options. 
 5 Path Synthesis We have cast motion extraction as an optimization problem, and we have given some 
reasons why the formulation of this optimiza­tion can be dif.cult. To demonstrate that it is nonetheless 
possible to come up with optimization criteria that allow us to solve a real problem, we apply the preceding 
framework to path synthesis. This problem is simple to state: given a path P speci.ed by the user, gen­erate 
motion such that the character travels along P. In this section we present our algorithm for path synthesis, 
present results, and discuss applications of the technique. 5.1 Implementing Path Synthesis Given the 
framework in the previous section, our only tasks are to de.ne an error function g(w, e) and appropriate 
halting criteria. The basic idea is to estimate the actual path P' travelled by the character during 
a graph walk and measure how different it is from P. The graph walk is complete when P' is suf.ciently 
long. A simple way to determine P' is to project the root onto the .oor at each frame, forming a piecewise 
linear curve1 . Let P(s) be the point on P whose arc-length distance from the start of P is s. The ith 
frame of the graph walk, wi, is at some arc length s(wi) from the start of P' . We de.ne the corresponding 
point on P as the point at the same arc length, P(s(wi)). For the jth frame of e, we calculate the squared 
distance between P' (s(e j)) and P(s(e j)). g(w, e) is the sum of these errors: n g(w, e)= .P' (s(ei)) 
- P(s(ei)).2 (9) i=1 Note that s(ei) depends on the total arc length of w, which is why this equation 
is a function of w as well as e. The halting condition for path synthesis is when the current total length 
of P' meets or exceeds that of P. Any frames on the graph walk at an arc length longer than the total 
length of P are mapped to the last point on P. The error function g(w, e) was chosen for a number of 
reasons. First, it is ef.cient to compute, which is important in making the search algorithm practical. 
Second, the character is given incentive to make de.nite progress along the path. If we were to have 
re­quired the character to merely be near the path, then it would have no reason not to alternate between 
travelling forwards and back­wards. Finally, this metric allows the character to travel at what­ever 
speed is appropriate for what needs to be done. For example, a sharp turn will not cover distance at 
the same rate as walking straight forward. Since both actions are equally important for ac­curate path 
synthesis, it is important that one not be given undue preference over the other. One potential problem 
with this metric is that a character who stands still will never have an incentive to move forward, as 
it can accrue zero error by remaining in place. While we have not en­countered this particular problem 
in practice, it can be countered by requiring at least a small amount of forward progress a on each frame. 
More exactly, we can replace in Equation 9 the function s(ei) with t(ei)= max(t(ei-1)+ s(ei) - s(ei-1), 
t(ei-1)+ a). Typically the user will want all generated motion to be of a single type, such as walking. 
This corresponds to con.ning the search to the subgraph containing the appropriate set of descriptive 
labels. More interestingly, one can require different types of motion on dif­ferent parts of the path. 
For example, one might want the character to walk along the .rst half of the path and sneak down the 
rest. The necessary modi.cations to accomplish this are simple. We will consider the case of two different 
motion types; the generalization to higher numbers is trivial. We divide the original path into two smaller 
adjoining paths, P1 and P2, based on where the transition from type T1 to type T2 is to occur. If the 
character is currently .tting P2, then the algorithm is identical to the single-type case. If the character 
is .tting P1, then we check to see if we are a threshold distance from the end of P1. If not, we continue 
to only consider edges of type T1. Otherwise we allow the search to try both edges of type T1 and T2; 
in the latter case we switch to .tting P2. Note that we only allow this switch to occur once on any given 
graph walk, which prevents the resulting motion from randomly switching between the two actions. 5.2 
Results While the examples shown in Figure 1 suggest that our technique is viable, it perhaps isn t surprising 
that we were able to .nd accu­rate .ts to the given paths. As shown in the upper portion of the 1In our 
implementation we de.ned the path as a spline approximating this piecewise linear path, although this 
has little impact on the results. .gure, the input motion had a fair amount of variation, including straight-ahead 
marches, sharp turns, and smooth changes of curva­ture. However, our algorithm is still useful when the 
input database is not as rich. Refer to Figure 6. We started with a single 12.8­second clip of an actor 
sneaking along the indicated path. To stretch this data further, we created a mirror-image motion and 
then built a motion graph out of the two. From these we were able to construct the new motions shown 
at the bottom of the .gure, both of which are themselves approximately 13 seconds in length. Figure 7 
shows .ts to a more complicated path. The .rst example uses walking motions and the second uses martial 
arts motions; the latter demonstrates that our approach works even on motions that are not obviously 
locomotion. For the walking motion, the total computation time was nearly the same as the length of the 
generated animation (58.1 seconds of calculation for 54.9 seconds animation). The martial arts motion 
is 87.7 seconds long and required just 15.0 seconds of computation. In general, in our test cases the 
duration of a generated motion was either greater than or approximately equal to the amount of time needed 
to produce it. Both motion graphs had approximately 3000 frames (100 seconds) of animation. Finally, 
Figure 8 shows paths containing constraints on the allow­able motion type. In the .rst section of each 
path the character is required to walk, in the second it must sneak, and in the third it is to perform 
martial arts moves. Not only does the character fol­low the path well, but transitions between action 
types occur quite close to their speci.ed locations. This example used a database of approximately 6000 
frames (200 seconds). All examples were computed on a 1.3GHz Athlon. For our largest graph (about 6000 
frames), approximately twenty-.ve minutes were needed to compute the locations of all candidate transitions 
points. Approximately .ve minutes of user time were required to select transition thresholds, and it 
took less than a minute to calcu­late blends at these transitions and prune the resulting graph. 5.3 
Applications Of Path Synthesis Directable locomotion is a general enough need that the preceding algorithm 
has many applications. Interactive Control. We can use path synthesis techniques to give a user interactive 
control over a character. For example, when the user hits the left arrow key the character might start 
travelling east. To accomplish this, we can use the path .tting algorithm to .nd the sequence of edges 
starting from our current location on the graph that best allow the character to travel east. The .rst 
edge on the resulting graph walk is the next clip that will be played. This pro­cess may then be repeated. 
To make this practical, we can precom­pute for every node in the graph a sequence of graph walks that 
.t straight-line paths in a sampling of directions (0 degrees, 30 de­grees, ... ). The .rst edges on 
these paths are then stored for later use; they are the best edges to follow given the direction the 
char­acter is supposed to travel in. High-Level Keyframing. If we want a character to perform certain 
actions in a speci.c sequence and in speci.c locations, we can draw a path with subsections requiring 
the appropriate action types. This allows us to generate complex animations without the tedium of manual 
keyframing. For this reason we term this process high­level keyframing the user generates an animation 
based on what should be happening and where. Motion Dumping. If an AI algorithm is used to determine 
that a character must travel along a certain path or start performing cer­tain actions, the motion graph 
may be used to dump motion on top of the algorithm s result. Hence motion graphs may be used as a back-end 
for animating non-player characters in video games and interactive environments the paths and action 
types can be speci.ed by a high-level process and the motion graph would .ll in the details. Crowds. 
While our discussion so far has focused on a single char­acter, there s no reason why it couldn t be 
applied to several char­acters in parallel. Motion graphs may be used as a practical tool for crowd generation. 
For example, a standard collision-avoidance algorithm could be used to generate a path for each individual, 
and the motion graph could then generate motion that conforms to this path. Moreover, we can use the 
techniques described at the end of Section 4.2 to add randomness to the generated motion.  6 Discussion 
In this paper we have presented a framework for generating realis­tic, controllable motion through a 
database of motion capture. Our approach involves automatically constructing a graph that encapsu­lates 
connections among different pieces of motion in the database and then searching this graph for motions 
that satisfy user con­straints. We have applied our framework to the problem of path synthesis. As we 
had limited access to data, our largest examples used a database of several thousand frames of motion. 
While we believe this was suf.cient to show the potential of our method, a character with a truly diverse 
set of actions might require hundreds or thou­sands of times more data. Hence the scalability of our 
framework bears discussion. The principle computational bottleneck in graph construction is locating 
candidate transitions (Section 3.1). This re­quires comparing every pair of the F frames in the database 
and therefore involves O(F2) operations. However, this calculation is trivial to parallelize, and distances 
between old frames needn t be recomputed if additions are made to the database. It is the exception rather 
than the rule that two pieces of motion are suf.ciently similar that a transition is possible, and hence 
motion graphs tend to be sparse. In our experience the necessary amount of storage is approximately proportional 
to the size of the database. The number of edges leaving a node in general grows with the size of the 
graph, meaning the branching factor in our search algorithm may grow as well. However, we expect that 
future motion graphs will be larger mainly because the character will be able to perform more actions. 
That is, for example, having increasing amounts of walking motion isn t particularly useful once one 
can direct a char­acter along nearly any path. Hence the branching factor in a par­ticular subgraph will 
remain stationary once that subgraph is suf­.ciently large. We anticipate that typical graph searches 
will be restricted to one or two subgraphs, and so we expect that the search will remain practical even 
for larger graphs. We conclude with a brief discussion of future work. One limita­tion of our approach 
is that the transition thresholds must be spec­i.ed by hand, since (as discussed in Section 3.2) different 
kinds of motions have different .delity requirements. Setting thresholds in databases involving many 
different kinds of motions may be overly laborious, and so we are investigating methods for automating 
this process. A second area of future work is to incorporate parameter­izable motions [Wiley and Hahn 
1997; Rose et al. 1998] into our system, rather than having every node correspond to a static piece of 
motion. This would add .exibility to the search process and po­tentially allow generated motion to better 
satisfy user constraints. Finally, we are interested in applying motion graphs to problems other than 
path synthesis.  Figure 6: The leftmost image shows the original motion and its re.ection and the following 
images show motion generated by our path synthesis algorithm. The thick yellow lines were the paths to 
be .t and the black line is an approximation of the actual path of the character. Note how we are able 
to accurately .t nontrivial paths despite the limited variation in the path of the original motion. 
Figure 7: The left image shows a walking motion generated to .t to a path that spells Hello in cursive. 
The right image shows a karate motion .t to the same path. The total calculation time for the walking 
motion was 58.1 seconds and the animation itself is 54.9 seconds. The 87.7-second karate motion was computed 
in just 15.0 seconds. All computation was done on a 1.3gHz Athlon. Figure 8: These images are both .ts 
to paths wherein the character is required to walk, then sneak, and .nally perform martial arts moves. 
The desired transition points are indicated by where the curve changes color. Note that the character 
both .ts the path accurately and switches to the appropriate motion type close to the desired location. 
Acknowledgements We would like to acknowledge Andrew Gardner, Alex Mohr, and John Schreiner for assisting 
in video production, proofreading, and other technical matters. We also thank the University of Southern 
California s School of Film and Television for their support and the reviewers for their many useful 
suggestions. Our work was made possible through generous motion data donations from Spec­trum Studios 
(particularly Demian Gordon), House of Moves, and The Ohio State University. This work was supported 
in part by NSF grants CCR-9984506 and IIS-0097456, the U.S. Army2, the Wisconsin Alumni Research Fund 
s University Industrial Relations program, equipment donations from IBM, NVidia, and Intel, and software 
donations from Discreet, Alias/Wavefront, and Pixar.  References ARIKAN, O., AND FORSYTHE, D. 2002. 
Interactive motion generation from exam­ples. In Proceedings of ACM SIGGRAPH 2002, Annual Conference 
Series, ACM SIGGRAPH. BOWDEN, R. 2000. Learning statistical models of human motion. In IEEE Work­shop 
on Human Modelling, Analysis, and Synthesis, CVPR 2000, IEEE Computer Society. BRAND, M., AND HERTZMANN, 
A. 2000. Style machines. In Proceedings of ACM SIGGRAPH 2000, Annual Conference Series, ACM SIGGRAPH, 
183 192. BRUDERLIN, A., AND CALVERT, T. 1996. Knowledge-driven, interactive animation of human running. 
In Graphics Interface, Canadian Human-Computer Communi­cations Society, 213 221. BRUDERLIN, A., AND WILLIAMS, 
L. 1995. Motion signal processing. In Pro­ceedings of ACM SIGGRAPH 95, Annual Conference Series, ACM 
SIGGRAPH, 97 104. FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for 
physics-based character animation. In Proceedings of ACM SIG-GRAPH 2001, Annual Conference Series, ACM 
SIGGRAPH, 251 260. GALATA, A., JOGNSON, N., AND HOGG, D. 2001. Learning variable-length markov models 
of behavior. Computer Vision and Image Understanding Journal 81, 3, 398 413. GLEICHER, M. 1998. Retargeting 
motion to new characters. In Proceedings 0f ACM SIGGRAPH 98, Annual Conference Series, ACM SIGGRAPH, 
33 42. GLEICHER, M. 2001. Motion path editing. In Proceedings 2001 ACM Symposium on Interactive 3D Graphics, 
ACM. HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O BRIEN, J. F. 1995. Animating human athletics. 
In Proceedings of ACM SIGGRAPH 95, Annual Con­ference Series, ACM SIGGRAPH, 71 78. KOVAR, L., GLEICHER, 
M., AND SCHREINER, J. 2002. Footskate cleanup for motion capture editing. Tech. rep., University of Wisconsin, 
Madison. LAMOURET, A., AND PANNE, M. 1996. Motion synthesis by example. Computer animation and Simulation, 
199 212. LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like 
.gures. In Proceedings of ACM SIGGRAPH 99, Annual Conference Series, ACM SIGGRAPH, 39 48. LEE, J., CHAI, 
J., REITSMA, P.S.A.,HODGINS, J. K., AND POLLARD, N. S. 2002. Interactive control of avatars animated 
with human motion data. In Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM SIGGRAPH. 
LEE, J. 2000. A hierarchical approach to motion analysis and synthesis for articulated .gures. PhD thesis, 
Department of Computer Science, Korea Advanced Institute of Science and Technology. LI, Y., WANG, T., 
AND SHUM, H.-Y. 2002. Motion texture: A two-level statistical model for character motion synthesis. In 
Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM SIGGRAPH. 2This paper does not necessarily 
re.ect the position or the policy of the Government, and no of.cial endorsement should be inferred MIZUGUCHI, 
M., BUCHANAN, J., AND CALVERT, T. 2001. Data driven motion transitions for interactive games. In Eurographics 
2001 Short Presentations. MOLINA-TANCO, L., AND HILTON, A. 2000. Realistic synthesis of novel human movements 
from a database of motion capture examples. In Proceedings of the Workshop on Human Motion, IEEE Computer 
Society, 137 142. MULTON, F., FRANCE, L., CANI, M.-P., AND DEBUNNE, G. 1999. Computer animation of human 
walking: a survey. The Journal of Visualization and Computer Animation 10, 39 54. Published under the 
name Marie-Paule Cani-Gascuel. PERLIN, K., AND GOLDBERG, A. 1996. Improv: A system for scripting interac­tive 
actors in virtual worlds. In Proceedings of ACM SIGGRAPH 96, ACM SIG-GRAPH, 205 216. PERLIN, K. 1995. 
Real time responsive animation with personality. IEEE Transac­tions on Visualization and Computer Graphics 
1, 1 (Mar.), 5 15. PULLEN, K., AND BREGLER, C. 2000. Animating by multi-level sampling. In IEEE Computer 
Animation Conference, CGS and IEEE, 36 42. PULLEN, K., AND BREGLER, C. 2002. Motion capture assisted 
animation: Textur­ing and synthesis. In Proceedings of ACM SIGGRAPH 2002, Annual Conference Series, ACM 
SIGGRAPH. ROSE, C., GUENTER, B., BODENHEIMER, B., AND COHEN, M. F. 1996. Ef.cient generation of motion 
transitions using spacetime constraints. In Proceedings of ACM SIGGRAPH 1996, Annual Conference Series, 
ACM SIGGRAPH, 147 154. ROSE, C., COHEN, M., AND BODENHEIMER, B. 1998. Verbs and adverbs: Multidi­mensional 
motion interpolation. IEEE Computer Graphics and Application 18, 5, 32 40. SCH ODL¨ , A.,SZELISKI, R., 
SALESIN, D., AND ESSA, I. 2000. Video textures. In Proceedings of ACM SIGGRAPH 2000, Annual Conference 
Series, ACM SIG-GRAPH, 489 498. SUN, H. C., AND METAXAS, D. N. 2001. Automating gait animation. In Proceedings 
of ACM SIGGRAPH 2001, Annual Conference Series, ACM SIGGRAPH, 261 270. WASHBURN, D. 2001. The quest for 
pure motion capture. Game Developer (Decem­ber). WILEY, D., AND HAHN, J. 1997. Interpolation synthesis 
of articulated .gure motion. IEEE Computer Graphics and Application 17, 6, 39 45. WITKIN, A., AND POPOVI 
C´ , Z. 1995. Motion warping. In Proceedings of ACM SIGGRAPH 95, Annual Conference Series, ACM SIGGRAPH, 
105 108.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566606</article_id>
		<sort_key>483</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Interactive motion generation from examples]]></title>
		<page_from>483</page_from>
		<page_to>490</page_to>
		<doi_number>10.1145/566570.566606</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566606</url>
		<abstract>
			<par><![CDATA[There are many applications that demand large quantities of natural looking motion. It is difficult to synthesize motion that looks natural, particularly when it is people who must move. In this paper, we present a framework that generates human motions by cutting and pasting motion capture data. Selecting a collection of clips that yields an acceptable motion is a combinatorial problem that we manage as a randomized search of a hierarchy of graphs. This approach can generate motion sequences that satisfy a variety of constraints automatically. The motions are smooth and human-looking. They are generated in real time so that we can author complex motions interactively. The algorithm generates multiple motions that satisfy a given set of constraints, allowing a variety of choices for the animator. It can easily synthesize multiple motions that interact with each other using constraints. This framework allows the extensive re-use of motion capture data for new purposes.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation with constraints]]></kw>
			<kw><![CDATA[clustering]]></kw>
			<kw><![CDATA[graph search]]></kw>
			<kw><![CDATA[human motion]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Graph and tree search strategies</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010205.10010207</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies->Discrete space search</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205.10010210</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies->Game tree search</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14194108</person_id>
				<author_profile_id><![CDATA[81100558890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Okan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arikan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027977</person_id>
				<author_profile_id><![CDATA[81100502370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Forsyth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>525960</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BISHOP, C. M. 1995. Neural Networks for Pattern Recognition. Clarendon Press, Oxford.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BOWDEN, R., 2000. Learning statistical models of human motion.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BRAND, M., AND HERTZMANN, A. 2001. Style machines. In Proceedings of SIGGRAPH 2000, 15-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618563</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BROGAN, D. C., METOYER, R. A., AND HODGINS, J. K. 1998. Dynamically simulated characters in virtual environments. IEEE Computer Graphics & Applications 18, 5, 58-69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344882</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHENNEY, S., AND FORSYTH, D. A. 2000. Sampling plausible solutions to multi-body constraint problems. In Proceedings of SIGGRAPH 2000, 219-228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHOI, M. G., LEE, J., AND SHIN, S. Y. 2000. A probabilistic approach to planning biped locomotion with prescribed motions. Tech. rep., Computer Science Department, KAIST.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DEMPSTER, W., AND GAUGHRAN, G. 1965. Properties of body segments based on size and weight. In American Journal of Anatomy, vol. 120, 33-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A., AND LEUNG, T. K. 1999. Texture synthesis by non-parametric sampling. In ICCV (2), 1033-1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FORTNEY, V. 1983. The kinematics and kinetics of the running pattern of two-, four- and six-year-old children. In Research Quarterly for Exercise and Sport, vol. 54(2), 126-135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311538</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FUNGE, J., TU, X., AND TERZOPOULOS, D. 1999. Cognitive modeling: Knowledge, reasoning and planning for intelligent characters. In Proceedings of SIGGRAPH 1999, 29-38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>377412</ref_obj_id>
				<ref_obj_pid>376890</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GALATA, A., JOHNSON, N., AND HOGG, D. 2001. Learning variable length markov models of behaviour. In Computer Vision and Image Understanding (CVIU) Journal, vol. 81, 398-413.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GERSHO, A., AND GRAY, R. 1992. Vector Quantization and signal compression. Kluwer Academic Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GILKS, W., RICHARDSON, S., AND SPIEGELHALTER, D. 1996. Markov Chain Monte Carlo in Practice. Chapman and Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargetting motion to new characters. In Proceedings of SIGGRAPH 1998, vol. 32, 33-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218411</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GRZESZCZUK, R., AND TERZOPOULOS, D. 1995. Automated learning of muscle-actuated locomotion through control abstraction. In Proceedings of SIGGRAPH 1995, 63-70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280816</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[GRZESZCZUK, R., TERZOPOULOS, D., AND HINTON, G. 1998. Neuroanimator: Fast neural network emulation and control of physics based models. In Proceedings of SIGGRAPH 1998, 9-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HEEGER, D. J., AND BERGEN, J. R. 1995. Pyramid-Based texture analysis/synthesis. In Proceedings of SIGGRAPH 1995, 229-238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258822</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K., AND POLLARD, N. S. 1997. Adapting simulated behaviors for new characters. In Proceedings of SIGGRAPH 1997, vol. 31, 153-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J., WOOTEN, W., BROGAN, D., AND O'BRIEN, J., 1995. Animated human athletics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274990</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[LAMOURET, A., ANDVAN DE PANNE, M. 1996. Motion synthesis by example. In Eurographics Computer Animation and Simulation '96, 199-212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LATOMBE, J. P. 1999. Motion planning: A journey of robots, molecules, digital actors, and other artifacts. In International Journal of Robotics Research, vol. 18, 1119-1128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-likefigures. In Proceedings of SIGGRAPH 1999, 39-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LEE, J., CHAI, J., REITSMA, P., HODGINS, J., AND POLLARD, N. 2002. Interactive control of avatars animated with human motion data. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[LI, Y., WANG, T., AND SHUM, H. Y. 2002. Motion texture: A two-level statistical model for character motion synthesis. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>630558</ref_obj_id>
				<ref_obj_pid>630313</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[MATARIC, M. J. 2000. Getting humanoids to move and imitate. In IEEE Intelligent Systems, IEEE, 18-24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[MCMAHON, T. 1984. Muscles, Reflexes and Locomotion. PhD thesis, Princeton University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>823423</ref_obj_id>
				<ref_obj_pid>822088</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[MOLINA-TANCO, L., AND HILTON, A. 2000. Realistic synthesis of novel human movements from a database of motion capture examples. In Workshop on Human Motion (HUMO'00), 137-142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[NELSON, R., BROOKS, C., AND N. PIKE. 1977. Biomechanical comparison of male and female distance runners. In Annals of the NY Academy of Sciences, vol. 301, 793-807.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>521378</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[O'ROURKE, J. 1998. Computational Geometry in C. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>930744</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[POPOVlC, Z. 1999. Motion Transformation by Physically Based Spacetime Optimization. PhD thesis, Carnegie Mellon University Department of Computer Science.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872897</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2000. Animating by multi-level sampling. In Computer Animation 2000, 36-42. ISBN 0-7695-0683-6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566608</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2002. Motion capture assisted animation: Texturing and synthesis. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237229</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., GUENTER, B., BODENHEIMER, B., AND COHEN, M. F. 1996. Efficient generation of motion transitions using spacetime constraints. In Proceedings of SIGGRAPH 1996, vol. 30, 147-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[SCHODL, A., SZELISKI, R., SALESIN, D., AND ESSA, I. 2000. Video textures. In Proceedings of SIGGRAPH 2000, 489-498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[VEACH, E., AND GUIBAS, L. J. 1997. Metropolis light transport. In Proceedings of SIGGRAPH 1997, vol. 31, 65-76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND POPOVIC, Z. 1995. Motion warping. In Proceedings of SIGGRAPH 1995, 105-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Motion Generation from Examples Okan Arikan D. A. Forsyth University of California, Berkeley 
 Abstract There are many applications that demand large quantities of natural looking motion. It is dif.cult 
to synthesize motion that looks nat­ural, particularly when it is people who must move. In this paper, 
we present a framework that generates human motions by cutting and pasting motion capture data. Selecting 
a collection of clips that yields an acceptable motion is a combinatorial problem that we manage as a 
randomized search of a hierarchy of graphs. This ap­proach can generate motion sequences that satisfy 
a variety of con­straints automatically. The motions are smooth and human-looking. They are generated 
in real time so that we can author complex mo­tions interactively. The algorithm generates multiple motions 
that satisfy a given set of constraints, allowing a variety of choices for the animator. It can easily 
synthesize multiple motions that inter­act with each other using constraints. This framework allows the 
extensive re-use of motion capture data for new purposes. CR Categories: I.2.7 [Arti.cial Intelligence]: 
Problem Solv­ing, Control Methods and Search Graph and tree search strate­gies I.3.7 [COMPUTER GRAPHICS 
]: Three-Dimensional Graph­ics and Realism Animation Keywords: Motion Capture, Motion Synthesis, Human 
motion, Graph Search, Clustering, Animation with Constraints 1 Introduction Motion is one of the most 
important ingredients of CG movies and computer games. Obtaining realistic motion usually involves key 
framing, physically based modelling or motion capture. Creating natural looking motions with key framing 
requires lots of effort and expertise. Although physically based modelling can be applied to simple systems 
successfully, generating realistic motion on a com­puter is dif.cult, particularly for human motion. 
A standard so­lution is motion capture: motion data for an approximate skeletal hierarchy of the subject 
is recorded and then used to drive a recon­struction on the computer. This allows other CG characters 
to be animated with the same motions, leading to realistic, human look­ing motions for use in movies 
or games. The biggest drawbacks of motion capture are: 1. Most motion capture systems are very expensive 
to use, be­cause the process is time consuming for actors and technicians and motion data tends not to 
be re-used. Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to 
make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 2. It is very hard to obtain motions that do exactly what the ani­mator wants. 
Satisfying complex timed constraints is dif.cult and may involve many motion capture iterations. Examples 
include being at a particular position at a particular time ac­curately or synchronizing movement to 
a background action that had been shot before. In order to make motion capture widely available, the 
motion data needs to be made re-usable. This may mean using previous motion capture data to generate 
new motions so that certain re­quirements are met, transferring motions from one skeletal con.g­uration 
to another so that we can animate multiple .gures with the same motion without it looking funny , or 
changing the style of the motion so that the directors can have higher level control over the motion. 
There are three natural stages of motion synthesis: 1. Obtaining motion demands involves specifying constraints 
on the motion, such as the length of the motion, where the body or individual joints should be or what 
the body needs to be doing at particular times. These constraints can come from an interactive editing 
system used by animators, or from a computer game engine itself. 2. Generating motion involves obtaining 
a rough motion that satis.es the demands. In this paper, we describe a technique that cuts and pastes 
bits and pieces of example motions to­gether to create such a motion. 3. Post processing involves .xing 
small scale offensive arti­facts. An example would involve .xing the feet so that they do not penetrate 
or slide on the ground, lengthening or short­ening strides and .xing constraint violations.  In this 
paper, we present a framework that allows synthesis of new motion data meeting a wide variety of constraints. 
The synthe­sized motion is created from example motions at interactive speeds. 2 Related Work In the 
movie industry, motion demands are usually generated by animators. However, automatic generation of motion 
demands is required for autonomous intelligent robots and characters [Funge et al. 1999]. An overview 
of the automatic motion planning can be found in [Latombe 1999; O Rourke 1998]. Generating motion largely 
follows two threads: using examples and using controllers. Example based motion synthesis draws on an 
analogy with texture synthesis where a new texture (or motion) that looks like an example texture (or 
motion example) needs to be syn­thesized [Efros and Leung 1999; Heeger and Bergen 1995]. Pullen and Bregler 
used this approach to create cyclic motions by sampling motion signals in a signal pyramid [2000]. They 
also used a sim­ilar approach to fetch missing degrees of freedom in a motion from a motion capture database 
[Pullen and Bregler 2002]. The sam­pling can also be done in the motion domain to pick clips of mo­tions 
to establish certain simple constraints [Lamouret and van de Panne 1996; Schodl et al. 2000]. A roadmap 
of all the motion ex­amples can be constructed and searched to obtain a desired motion [Choi et al. 2000; 
Lee et al. 2002; Kovar et al. 2002]. The clips in this roadmap can also be parameterized for randomly 
sampling different motion sequences [Li et al. 2002]. The motion signals can also be clustered. The resulting 
Markov chain can be searched using dynamic programming to .nd a motion that connects two keyframes [Molina-Tanco 
and Hilton 2000] or used in a variable length Markov model to infer behaviors [Galata et al. 2001] or 
di­rectly sampled from to create new motions [Bowden 2000]. This is similar to our work. However, our 
clustering method does not operate on body con.gurations and our probabilistic search strat­egy is more 
effective than dynamic programming as it will be ex­plained below. Types of probabilistic search algorithms 
have also been used in physically based animation synthesis [Chenney and Forsyth 2000] and rendering 
[Veach and Guibas 1997]. Controller based approaches use physical models of systems and controllers that 
produce outputs usually in the form of forces and torques as a function of the state of the body. These 
controllers can be designed speci.cally to accomplish particular tasks [Brogan et al. 1998; Hod­gins 
et al. 1995] or they can be learned automatically using statis­tical tools [Grzeszczuk and Terzopoulos 
1995; Grzeszczuk et al. 1998; Mataric 2000]. The motion data can also be post processed to .x problems 
such as feet sliding on the ground or some constraints not being satis­.ed [Gleicher 1998; Lee and Shin 
1999; Popovic 1999; Rose et al. 1996]. This usually involves optimization of a suitable displace­ment 
function on the motion signal. Different body sizes move ac­cording to different time scales, meaning 
that motion cannot simply be transferred from one body size to another; modifying motions appropriately 
is an interesting research problem [Hodgins and Pol­lard 1997]. 3 Synthesis as Graph Search We assume 
there is a set of N motion sequences forming our dataset, each belonging to the same skeletal con.guration. 
Every motion is discretely represented as a sequence of frames each of which has the same M degrees of 
freedom. This is required to be able to compare two motions and to be able to put clips from dif­ferent 
motion sequences together. We write the i th frame of s th motion as si. 3.1 Motion Graph The collection 
of motion sequences could be represented as a di­rected graph. Each frame would be a node. There would 
be an edge from every frame to every frame that could follow it in an accept­able splice. In this graph, 
there would be (at least) an edge from the k th frame to the k +1 th frame in each sequence. This graph 
is not a particularly helpful representation because it is extremely large we can easily have tens of 
thousands of nodes and hun­dreds of thousands of edges and it obscures the structure of the sequences. 
Instead, we collapse all the nodes (frames) belonging to the same motion sequence together. This yields 
a graph G where the nodes of G are individual motion sequences and there is an edge from s to t for every 
pair of frames where we can cut from s to t. Since edges connect frames, they are labelled with the frames 
in the incident nodes (motion sequences) that they originate from and they point to. We also assume that 
the edges in G are attached a cost value which tells us the cost of connecting the incident frames. If 
cutting from one sequence to another along an edge introduces a discontinuous motion, then the cost attached 
to the edge is high. Appendix A introduces the cost function that we used. The collapsed graph still 
has the same number of edges. For an edge e from si to tj, let f romMotion(e)= s, Path  Edge 1  Edge 
5 Edge 2 Edge 4 Edge 3 Corresponding Motion Time Figure 1: We wish to synthesize human motions by 
splicing to­gether pieces of existing motion capture data. This can be done by representing the collection 
of motion sequences by a directed graph (top). Each sequence becomes a node; there is an edge between 
nodes for every frame in one sequence that can be spliced to a frame in another sequence or itself. A 
valid path in this graph represents a collection of splices between sequences, as the middle shows. We 
now synthesize constrained motion sequences by searching appro­priate paths in this graph using a randomized 
search method. toMotion(e)=t, f romFrame(e)=i, toFrame(e)= j and cost(e) be the cost associated with 
the edge (de.ned in Appendix A). In this setting, any sequence of edges e1 ···en where toMotion(ei)= 
f romMotion(ei+1) and toFrame(ei) < f romFrame(ei+1), .i, 1=i < n is a valid path and de.nes a legal 
sequence of splices. (.g­ure 1). 3.2 Constraints We wish to construct paths in the motion graph that 
satisfy con­straints. Many constraints cannot be satis.ed exactly. For example, given two positions, 
there may not be any sequence of frames in the collection that will get us from the .rst position to 
the second position exactly. We de.ne hard constraints to be those that can (and must) be satis.ed exactly. 
Typically, a hard constraint involves using a particular frame in a particular time slot. For example, 
in­stead of considering all valid paths, we can restrict ourselves to valid paths that pass through particular 
nodes at particular times. This way, we can constrain the moving .gure to be at a speci.c pose at a speci.c 
time. This enables us to search for motions such as jumping, falling, or pushing a button at a particular 
time. A soft constraint cannot generally be met exactly. Instead we score sequences using an objective 
function that re.ects how well the constraint has been met and attempt to .nd extremal sequences. One 
example is the squared distance between the position of the constraint and the actual position of the 
body at the time of the constraint. Example soft constraints include: 1. The total number of frames should 
be a particular number. 2. The motion should not penetrate any objects in the environ­ment. 3. The body 
should be at a particular position and orientation at a particular time. 4. A particular joint should 
be at a particular position (and maybe having a speci.c velocity) at a speci.c time. 5. The motion should 
have a speci.ed style (such as happy or energetic) at a particular time.  Finding paths in the motion 
graph that satisfy the hard con­straints and optimize soft constraints involves a graph search. Un­fortunately, 
for even a small collection of motions, the graph G has a large number of edges and straightforward search 
of this graph is computationally prohibitive. The main reason is the need to enu­merate many paths. There 
are, in general, many perfectly satisfac­tory motions that satisfy the constraints equally well. For 
example, if we require only that the person be at one end of a room at frame 0 and near the other end 
at frame 5000, unless the room is very large, there are many motions that satisfy these constraints. 
  4 Randomized Search The motion graph is too hard to search with dynamic programming as there are many 
valid paths that satisfy the constraints equally well. There may be substantial differences between equally 
valid paths in the example above, whether you dawdle at one side of the room or the other is of no signi.cance. 
This suggests summa­rizing the graph to a higher level and coarser presentation that is easier to search. 
Branch and bound algorithms are of no help here, because very little pruning is possible. In order to 
search the graph G in practical times, we need to do the search at a variety of levels where we do the 
large scale mo­tion construction .rst and then tweak the details so that the mo­tion is continuous and 
satis.es the constraints as well as possible. Coarser levels should have less complexity while allowing 
us to ex­plore substantially different portions of the path space. In such a representation, every level 
is a summary of the one .ner level. Let G. G. . G. .···. Gn . G be such a hierarchical represen­tation 
where G. is the coarsest level and G is the .nest. We will .rst .nd a path in G. and then push it down 
the hierarchy to a path in G for synthesis. 4.1 Summarizing the Graph All the edges between two nodes 
s and t can be represented in a matrix Pst . The (i, j) th entry of Pst contains the weight of the edge 
connecting si to tj and in.nity if there is no such edge. In the appendix A, we give one natural cost 
function C(si,tj) for edge weights. We now have: C(si,tj) if there is an edge from si to tj = (Pst )ij 
8 otherwise. The cost function explained in section A causes the P matrices to have non-in.nite entries 
to form nearly elliptical groups (.gure 2). This is due to the fact that if two frames are similar, most 
probably their preceding and succeeding frames also look similar. In order to summarize the graph, we 
cluster the edges of G. We now have G, whose nodes are the same as the nodes of G, and whose edges represent 
clusters of edges of G in terms of their f romFrame and toFrame labels. We require that, if there is 
a cut between two sequences represented by an edge between two nodes in G, there be at least one edge 
between the corresponding nodes in Walking , frame i Clustering  Figure 2: Every edge between two 
nodes representing different mo­tion clips can be represented as a matrix where the entries corre­spond 
to edges. Typically, if there is one edge between two nodes in our graph, there will be several, because 
if it is legal to cut from one frame in the .rst sequence to another in the second, it will usu­ally 
also be legal to cut between neighbors of these frames. This means that, for each pair of nodes in the 
graph, there is a matrix representing the weights of edges between the nodes. The i, j th entry in this 
matrix represents the weight for a cut from the i th frame in the .rst sequence to the j th frame in 
the second sequence. The weight matrix for the whole graph is composed as a collection of blocks of this 
form. Summarizing the graph involves compress­ing these blocks using clustering. G. If this were not 
the case, our summary would rule out potential paths. In order to insure that this condition holds and 
because the graph is very large, we cluster edges connecting every pair of nodes in G separately. We 
cluster unconnected edge groups of G from the P matrices (de.ned between every pair of nodes) using k-means 
[Bishop 1995]. The number of clusters is chosen as ma joraxislength minoraxislength for each group where 
the axis lengths refer to the ellipse that .ts to the cluster (obtained through Principal Component Analysis). 
The nodes of G. are the same as the nodes of G. The edges con­necting nodes in G. are cluster centers 
for clusters of edges connect­ing corresponding nodes in G. The centers are computed by taking the average 
of the edges in terms of f romFrame, toFrame and cost values. At this point, every edge in G. represents 
many edges in G. We would like to have a tree of graph representations whose root is G, and whose leaves 
are G. We use k-means clustering to split each cluster of edges in half at each intermediate level and 
obtain a hierarchical representation G. G. . G. . ··· . Gn . G for the original graph G. This is an instance 
of Tree-Structured Vector Quantization [Gersho and Gray 1992]. Thus, in our summarized graph G, each 
edge is the root of a binary tree and represents all the edges in close neighborhood in terms of the 
edge labels. Note that the leaf edges are the edges in the original graph and intermediate edges are 
the averages of all the leaf edges beneath them. A path in G represents a sequence of clips; so does 
a path in G, but now the positions of the clip boundaries are quantized, so there are fewer paths.  
4.2 Searching the Summaries While searching this graph, we would like to be able to generate dif­ferent 
alternative motions that achieve the same set of constraints. During the search, we need to .nd paths 
close to optimal solutions but do not require exact extrema, because they are too hard to .nd. This motivates 
a random search. We used the following search strat­egy: 1. Start with a set of n valid random seed paths 
in the graph G. 2. Score each path and score all possible mutations 3. Where possible mutations are: 
 (a) Delete some portion of the path and replace it with 0 or 1 hops. (b) Delete some edges of the path 
and replace them with their children  4. Accept the mutations that are better than the original paths 
 5. Include a few new valid random seed paths 6. Repeat until no better path can be generated through 
muta­tions  Intuitively the .rst mutation strategy replaces a clip with a (hope­fully) better one and 
the second mutation strategy adjusts the de­tailed position of cut boundaries. Since we start new random 
seed paths at every iteration, the algorithm does not get stuck at a local optimum forever. Section 4.2.2 
explains these mutations in more detail. Hard constraints are easily dealt with; we restrict our search 
to paths that meet these constraints. Typically hard constraints specify the frame (in a particular node) 
to be used at a particular time. We do this by ensuring that seed paths meet these constraints, and mutations 
do not violate them. This involves starting to sample the random paths from the hard constraint nodes 
and greedily adding sequences that get us to the next hard constraint if any. Since the path is sampled 
at the coarse level, a graph search can also be per­formed between the constraint nodes. At every iteration 
we check if the proposed mutation deletes a motion piece that has a hard con­straint in it. Such mutations 
are rejected immediately. Note that here we assume the underlying motion graph is connected. Section 
4.2.1 explains the constraints that we used in more detail. Notice that this algorithm is similar to 
MCMC search (a good broad reference to application of MCMC is [Gilks et al. 1996]). However, it is dif.cult 
to compute proposal probabilities for the mutations we use, which are strikingly successful in practice. 
This is an online algorithm which can be stopped at anytime. This is due to the fact that edges in intermediate 
graphs G···Gn also represent connections and are valid edges. Thus we do not have to reach the leaf graph 
G to be able to create a path (motion sequence). We can stop the search iteration, take the best path 
found so far, and create a motion sequence. If the sequence is not good enough, we can resume the search 
from where we left off to get better paths through mutations and inclusion of random paths. This allows 
an intuitive computation cost vs. quality tradeoff. 4.2.1 Evaluating a Path Since during the search 
all the paths live in a subspace implied by the hard constraints, these constraints are always satis.ed. 
Given a sequence of edges e1 ···en, we score the path using the imposed soft constraints. For each constraint, 
we compute a cost where the cost is indicative of the satisfaction of the constraint. Based on the scores 
for each of the constraints, we weight and sum them to create a .nal score for the path (The S function 
in equation 1). We also add the sum of the costs of the edges along the path to make sure we push the 
search towards paths that are continuous. The weights can be manipulated to increase/decrease the in.uence 
of a particular soft constraint. We now have an expression of the form: n S(e1 ···en)=wc *. cost(ei)+wf 
*F +wb *B +wj *J (1) i=1 Figure 3: The two mutations are: deleting some portion of the path (top-left, 
crossed out in red) and replacing that part with another set of edges (top-right), and deleting some 
edges in the path (bottom­left) and replacing deleted edges with their children in our hierarchy (bottom-right) 
Where wc,wf ,wb and wj are weights for the quality (continuity) of the motion, how well the length of 
the motion is satis.ed, how well the body constraints are satis.ed and how well the joints con­straints 
are de.ned. We selected these weights such that an error of 10 frames increases the total score the same 
amount as an error of 30 centimeters in position and 10 degrees in orientation. The scores F, B and J 
are de.ned as: 1. F: For the number of frame constraints, we compute the squared difference between the 
actual number of frames in the path and the required number of frames. 2. B: For body constraints, we 
compute the distance between the position and orientation of the constraint versus the ac­tual position 
and orientation of the torso at the time of the constraint and sum the squared distances. The position 
and orientation of the body at the constraint times are found by putting the motion pieces implied by 
the subsequent edges together (.gure 1). This involves taking all the frames of motion toMotion(ei) between 
frames f romFrame(ei+1) and toFrame(ei)and putting the sequence of frames starting from where the last 
subsequence ends or from the .rst body con­straint if there is no previous subsequence. Note that we 
re­quire that we have at least two body constraints enforcing the position/orientation of the body at 
the beginning of the syn­thesized motion (so that we know where to start putting the frames down) and 
at the end of the synthesized motion. The .rst body constraint is always satis.ed, because we always 
start putting the motions together from the .rst body con­straint. 3. J: For joint constraints, we compute 
the squared distance be­tween the position of the constraint and the position of the constrained joint 
at the time of the constraint and sum the squared distance between the two. To determine the con.gu­ration 
of the body at the time at which the constraint applies, we must assemble the motion sequence up to the 
time of the constraint; in fact, most of the required information such as the required transformation 
between start and end of each cut is already available in the dataset.   4.2.2 Mutating a Path We implemented 
two types of mutations which can be performed quickly on an active path. Domain of smoothing Figure 
4: In the synthesized motion, discontinuities in orientation are inevitable. We deal with these discontinuities 
using a form of localized smoothing. At the top left, a discontinuous orientation signal, with its discontinuity 
shown at the top right. We now con­ struct an interpolant to this discontinuity, shown on the bottom 
right and add it back to the original signal to get the continuous version shown on the bottom left. 
Typically, discontinuities in orientation are suf.ciently small that no more complex strategy is necessary. 
1. Replace a sequence by selecting two edges ei and ei+j where 0 = j = n - i, deleting all the edges 
between them in the path and connecting the unconnected pieces of the path us­ing one or two edges in 
the top level graph G. (if possible). Since in the summarized graph, there are relatively fewer edges, 
we can quickly .nd edges that connect the two un­ connected nodes by checking all the edges that go out 
from toMotion(ei), and enumerating all the edges that reach to Figure 5: Body constraints allow us to 
put checkpoints on the motion: in the .gure, the arrow on the right denotes the required starting position 
and orientation and the arrow on the left is the re­quired ending position and orientation. All constraints 
are also time stamped forcing the body to be at the constraint at the time stamp. For these two body 
constraints, we can generate many motions that satisfy the constraints in real-time.  f romMotion(ei+j) 
and generate a valid path. Note that we Figure 6: We can use multiple checkpoints in a motion. In this 
enumerate only 0 or 1 hop edges (1 edge or 2 edge connec-.gure, the motion is required to pass through 
the arrow (body con­ tions respectively). straint) in the middle on the way from the right arrow to the 
left. 2. Demoting two edges to their children and replacing them with one of their children if they can 
generate a valid path. Doing this mutation on two edges simultaneously allows us to compensate for the 
errors that would happen if only one of them was demoted. y(f )= . . . ... 0 f < d - s 1 2 * (f -ds +s 
)2 d - s = f < d - 12 * (f -d+s )2 +2 * (f -d+s )- 2 d = f = d +s ss 0 f > d +s We check every possible 
mutation, evaluate them and take the best few. Since the summary has signi.cantly fewer edges than the 
orig­inal graph, this step is not very expensive. If a motion sequence can­ not generate a mutation whose 
score is lower that itself, we decide that the current path is a local minimum in the valid path space 
and record it as a potential motion. This way, we can obtain multiple motions that satisfy the same set 
of constraints.  4.2.3 Creating and Smoothing the Final Path We create the .nal motion by taking the 
frames between as the smoothing function that gives the amount of displacement for every frame f , where 
d is the frame of the discontinuity and s if the smoothing window size (in our case 30). To make sure 
that we interpolate the body constraints (i.e. having a particular position/orientation at a particular 
frame), we take the difference between the desired constraint state, subtract the state at the time of 
the constraint and distribute this difference uniformly over the portion of the motion before the time 
of the constraint. Note that these smoothing steps can cause artifacts like feet penetrating or sliding 
on the ground. However, usually the errors made in terms toFrame(ei) and f romFrame(ei+1) from each motion 
toMotion(ei) where 1 = i < n (.gure 1). This is done by ro­tating and translating every motion sequence 
so that each piece starts from where the previous one ended. In general, at the frames corresponding 
to the edges in the path, we will have C0 of constraints and the discontinuities are so small that they 
are un­ noticeable.  4.3 Authoring Human Motions discontinuities, because of the .nite number of motions 
sampling an in.nite space. In practice these discontinuities are small and we can distribute them within 
a smoothing window around the discontinuity. We do this by multiplying the magnitude of the discontinuity 
by a smoothing function and adding the result back to the signal (.gure 4). We choose the smoothing domain 
to be ±30 frames (or one second of animation) around the discontinuity and Using iterative improvements 
of random paths, we are able to syn­thesize human looking motions interactively. This allows interac­tive 
manipulation of the constraints. This is important, because mo­tion synthesis is inherently ambiguous 
as there may be multiple mo­tions that satisfy the same set of constraints. The algorithm can .nd these 
local minimum motions that adhere to the same constraints. The animator can choose between them or all 
the different motions Figure 7: In addition to body constraints, joint constraints can be used to further 
assign checkpoints to individual joints. In this .gure, the head of the .gure is also constrained to 
be high (indicated by the blue line), leading to a jumping motion.  can be used to create a variety 
in the environment. Since the al­gorithm is interactive, the animator can also see the ambiguity and 
guide the search by putting extra constraints (.gure 6). Currently, we can constrain the length of the 
motion, the body s position and orientation at a particular frame (.gure 5,6), a joint (e.g. head, hand) 
to a particular state at a particular frame (.gure 7), or constrain the entire body s pose at a particular 
frame (.g­ure 8). Notice that we can synthesize multiple interacting motions independently using hard 
constraints (.gure 9); we simply select the poses, position and orientation at which the .gures interact 
and this framework .lls in the missing motion, in a sense, interpolat­ing the constraints. These are 
only a few of the constraints that can be implemented. As long as the user speci.es a cost function that 
evaluates a motion and attaches a score that is indicative of the animator s satisfaction with the path, 
many more constraints can be implemented. For example, if the motions in our database are marked with 
their individual stylistic attributes, we can also con­strain the style of the desired motion by penalizing 
motions that do not have the particular style. In a computer game environment, we can constrain the synthesized 
motion to avoid obstacles in the envi­ronment. In such a case, body position/orientation constraints 
can also come from an underlying path planner. Thus, given high level goals (such as going from point 
A to point B, say) human looking motions can be generated automatically.   5Results We have presented 
a framework that allows interactive synthesis of natural looking motions that adhere to user speci.ed 
constraints. We assess our results using four criteria. Firstly, the motion looks human. Secondly, the 
motions generated by the method do not have unnatural artifacts such as slipping feet on the ground or 
jerky movement. Third, the user speci.ed constraints are satis.ed, i.e. the motion passes through the 
required spot at the required time, or the character falls to a particular position (.gure 8). Finally, 
motions are generated interactively typically depending on the quality of the path desired, an acceptable 
300 frame motion is found in between 3 and 10 seconds on an average PC (Pentium III at 800 Mhz). This 
speed allows interactive motion authoring. For exam­ple, we generated the real-time screen captures in 
the attached video using a dataset of 60-80 unorganized, short (below 300 frames each) motion capture 
fragments. The average precomputation time speci.c activities. Here, we constrain the end of the motion 
to be lying .at on the ground at a particular position/orientation and time. Our framework generates 
the required tipping and tumbling motion in real-time. required for this many motions (computing the 
motion graph) is 5 hours on the same computer. On average, the results shown in the video contain 3-30 
motion pieces cut from the original motions. This framework is completely automatic. Once the input mo­tions 
are selected, the computation of the hierarchic motion graph does not require any user intervention and 
the resulting representa­tion is searched in real-time. For many kinds of constraints the motion synthesis 
problem is underconstrained; there are many possible combinations of motion pieces that achieve the same 
set of constraints. Randomized search is well suited to .nd many different motions that satisfy the con­straints. 
On the other hand, some constraints, may not be met by any motion. In this case, randomized search will 
try to minimize our objective motion and .nd the closest motion. For example, if the user asks for 100 
meters in 5 seconds, the algorithm will tend to put fast running motions together but not necessarily 
satisfying the constraints. Similarly, if the set of motions to begin with do not form a connected graph, 
the algorithm will perform searches con.ned to the unconnected graphs. If there are hard constraints 
in different unconnected components, we will not even be able to .nd starting seed paths. From this perspective, 
the selection of the database to work with is important. In our system, we used 60-100 football motions 
that have a strong bias towards motions that run forward. However, as the attached video suggest, the 
randomized search has no problem .nding rare motions that turn back to satisfy the constraints. The motion 
databases that we used were unorga­nized except that we excluded football warming up and tackling motions 
unless they were desired (.gure 9). The randomized search scales linearly as a function of the database 
size with a very small constant. We have tried datasets of 50-100 motions without a noticeable change 
in the running time of the algorithm. The linearity in the running time comes from the linear increase 
in the number of alternative mutations at every step. Note that as the database size gets larger, the 
constant t (Appendix A) that is used to create the edges can get lower since more mo­tions mean that 
we expect to .nd better connections between mo­tions, decreasing the number of edges. This will lead 
to a sublinear increase in the running time. The framework can work on any motion dataset: it can be 
created by traditional key framing, physically based modelling or motion capture. For example, we can 
take the motion data for Woody who may well have been key-framed, from Toy Story and create new Woody 
motions automatically. The framework is also appli­cable to non-human motion synthesis. For example, 
this framework can be used to generate control signals for robots to achieve a par­ticular task by generating 
the motion graph for previously known motion-control signal pairs. During the synthesis we can not only 
synthesize the .nal robot motion but also the associated control sig­nals that achieve speci.c goals. 
Since the generated motions are obtained by putting pieces of motions in the dataset, the resulting motions 
will also carry the underlying style of the data. This way, we can take the motion data for one character, 
and produce more motions with the intrinsic style of the character. 6 Future Work During the construction 
of the .nal motion, better ways of smooth­ing between adjacent motions could be used to improve realism 
[Popovic 1999]. Using better post processing, motions could also be synthesized on non-uniform surfaces 
which the current frame­work cannot handle. Additional post processing may involve phys­ically based 
modelling to make sure the synthesized motions are also physically correct. Automatic integration of 
higher level stylistic constraints could be incorporated into the framework, avoiding the arduous job 
of labelling every motion with the intrinsic style by hand. By analyz­ing patterns in the motion dataset, 
we might also infer these styles or obtain higher level descriptions [Brand and Hertzmann 2001]. The 
synthesized motions are strictly bound to the motions that were available in the original dataset. However, 
it is conceivable that the motions that are very close to the dataset could also be incorporated in the 
synthesizable motions using learned stylistic variations. The integrity of the original dataset directly 
effects the quality of the synthesized motion. For example, if the incoming motion dataset does not contain 
any turning left motions, we will not be able to synthesize motions that involve turning left . An automatic 
way of summarizing the portions of the possible human motions space that have not been explored well 
enough by the dataset could improve the data gathering and eventually the synthesized motions. This could 
also serve as a palette for artists: some portions of the precomputed motion graph can be paged in and 
out of memory de­pending on the required motion. For example, the animator could interactively select 
the motions that need to be used during the syn­thesis, and only the portion of the motion graph involving 
the de­sired motions could be loaded. This would give animators a tool whereby they can select the set 
of motions to work with in advance and the new motions will be created only from the artist selected 
set. Furthermore this encourages comprehensive re-use of motion data. 7 Acknowledgements This research 
was supported by Of.ce of Naval Research grant no. N00014-01-1-0890, as part of the MURI program. We 
would like to thank Electronic Arts for supplying us with the motion data. A Appendix: Similarity Metric 
We de.ne the torso coordinate frame to be the one where the body stands centered at origin on the xz 
plane and looks towards the pos­itive z axis. Any point p in the torso coordinate frame can be trans­ 
formed to the global coordinate frame by T (si)+ p , where R(si)·T (si)is the 3 ×1 translation of the 
torso and R(si)is the 3 ×1 rota­ tion of the torso and R(si)represents the rotation matrix associated 
with the rotation. We wish to have a weight on edges of the motion graph (section 3.1) that encodes the 
extent to which two frames can follow each other. If the weight of an edge is too high, it is dropped 
from the graph. To compute the weight of an edge, we use the difference between joint positions and velocities 
and the difference between the torso velocities and accelerations in the torso coordinate frame. Let 
P(si)be a 3 ×n matrix of positions of n joints for si in torso coordinate frame. Equation 2 gives us 
the difference in joint posi­tion and body velocity. Dsi,tj =[(P(si)-P(tj))(|T (si)|-|T (tj)|). (|R(si)|-|R(tj)|)] 
(2) We then de.ne the normalizing matrices O and L in equation 3 and 4. O =maxs,i(|DT si,siDsi,si+1 |) 
(3) D L =maxs,i(|DT |) (4) si,si si,si+1 Then the cost function function in equation 5 is used to relate 
si to tj. C(si,tj)=trace(Dsi,tjMO-1DT +D. TL-1DT ) (5) si,tj si,tj si,tj Where diagonal (n +2)×(n +2) 
matrices M and T are used to weight different joints differently. For example, position differ­ences 
in feet are much more noticeable than position differences of hands because the ground provides a comparison 
frame. We have found M and T matrices empirically by trying different choices. Unfortunately, de.ning 
a universal cost metric is a hard problem. The metric de.ned above produces visually acceptable results. 
Using this cost metric, we create edges from si to tj where C(si,tj)< t. For an edge e from si to tj, 
we set cost(e)=C(si,tj). t is a user speci.ed quality parameter that in.uences the number of edges in 
G. We have .xed this value so that cuts created between motions along the edges do not have visible artifacts. 
Note that an error that is visible on a short person may not be visible on an ex­tremely large person. 
Thus, in theory, the weights must be adjusted from person to person. However, in practice, possible size 
variation of adult people is small enough that we used the same weights for different people without 
creating a visible effect.  References BISHOP, C. M. 1995. Neural Networks for Pattern Recognition. 
Clarendon Press, Oxford. BOWDEN, R., 2000. Learning statistical models of human motion. BRAND, M., AND 
HERTZMANN, A. 2001. Style machines. In Proceedings of SIG-GRAPH 2000, 15 22. BROGAN,D.C., METOYER, R. 
A., AND HODGINS, J. K. 1998. Dynamically simu­ lated characters in virtual environments. IEEE Computer 
Graphics &#38; Applications 18, 5, 58 69. CHENNEY, S., AND FORSYTH, D. A. 2000. Sampling plausible solutions 
to multi­body constraint problems. In Proceedings of SIGGRAPH 2000, 219 228. CHOI, M. G., LEE, J., AND 
SHIN, S. Y. 2000. A probabilistic approach to planning biped locomotion with prescribed motions. Tech. 
rep., Computer Science Depart­ ment, KAIST. DEMPSTER,W., AND GAUGHRAN, G. 1965. Properties of body segments 
based on size and weight. In American Journal of Anatomy, vol. 120, 33 54. EFROS, A. A., AND LEUNG, T. 
K. 1999. Texture synthesis by non-parametric sam­pling. In ICCV (2), 1033 1038. FORTNEY, V. 1983. The 
kinematics and kinetics of the running pattern of two-, four­ and six-year-old children. In Research 
Quarterly for Exercise and Sport, vol. 54(2), 126 135.  Figure 9: Using combinations of constraints, 
we can generate multiple motions interacting with each other. Here, the skeletons are constrained to 
be tackling each other (using hard constraints) at speci.ed times and speci.ed positions/orientations. 
We can then generate the motion that connects the sequence of tackles. FUNGE, J., TU, X., AND TERZOPOULOS, 
D. 1999. Cognitive modeling: Knowledge, reasoning and planning for intelligent characters. In Proceedings 
of SIGGRAPH 1999, 29 38. GALATA, A., JOHNSON, N., AND HOGG, D. 2001. Learning variable length markov 
models of behaviour. In Computer Vision and Image Understanding (CVIU) Jour­nal, vol. 81, 398 413. GERSHO, 
A., AND GRAY, R. 1992. Vector Quantization and signal compression. Kluwer Academic Publishers. GILKS,W., 
RICHARDSON, S., AND SPIEGELHALTER, D. 1996. Markov Chain Monte Carlo in Practice. Chapman and Hall. GLEICHER, 
M. 1998. Retargetting motion to new characters. In Proceedings of SIGGRAPH 1998, vol. 32, 33 42. GRZESZCZUK, 
R., AND TERZOPOULOS, D. 1995. Automated learning of muscle­actuated locomotion through control abstraction. 
In Proceedings of SIGGRAPH 1995, 63 70. GRZESZCZUK, R., TERZOPOULOS, D., AND HINTON, G. 1998. Neuroanimator: 
Fast neural network emulation and control of physics based models. In Proceedings of SIGGRAPH 1998, 9 
20. HEEGER, D. J., AND BERGEN, J. R. 1995. Pyramid-Based texture analysis/synthesis. In Proceedings of 
SIGGRAPH 1995, 229 238. HODGINS, J. K., AND POLLARD, N. S. 1997. Adapting simulated behaviors for new 
characters. In Proceedings of SIGGRAPH 1997, vol. 31, 153 162. HODGINS, J., WOOTEN,W., BROGAN, D., AND 
O BRIEN, J., 1995. Animated human athletics. KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. 
In Proceedings of SIGGRAPH 2002. LAMOURET, A., AND VAN DE PANNE, M. 1996. Motion synthesis by example. 
In Eurographics Computer Animation and Simulation 96, 199 212. LATOMBE, J. P. 1999. Motion planning: 
A journey of robots, molecules, digital actors, and other artifacts. In International Journal of Robotics 
Research, vol. 18, 1119 1128. LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion 
editing for human-like.gures. In Proceedings of SIGGRAPH 1999, 39 48. LEE, J., CHAI, J., REITSMA,P., 
HODGINS, J., AND POLLARD, N. 2002. Interactive control of avatars animated with human motion data. In 
Proceedings of SIGGRAPH 2002. LI,Y., WANG,T., AND SHUM, H. Y. 2002. Motion texture: A two-level statistical 
model for character motion synthesis. In Proceedings of SIGGRAPH 2002. MATARIC, M. J. 2000. Getting humanoids 
to move and imitate. In IEEE Intelligent Systems, IEEE, 18 24. MCMAHON, T. 1984. Muscles, Re.exes and 
Locomotion. PhD thesis, Princeton University Press. MOLINA-TANCO, L., AND HILTON, A. 2000. Realistic 
synthesis of novel human movements from a database of motion capture examples. In Workshop on Human Motion 
(HUMO 00), 137 142. NELSON, R., BROOKS, C., AND N.PIKE. 1977. Biomechanical comparison of male and female 
distance runners. In Annals of the NY Academy of Sciences, vol. 301, 793 807. POPOVIC, Z. 1999. Motion 
Transformation by Physically Based Spacetime Optimiza­tion. PhD thesis, Carnegie Mellon University Department 
of Computer Science. PULLEN, K., AND BREGLER, C. 2000. Animating by multi-level sampling. In Computer 
Animation 2000, 36 42. ISBN 0-7695-0683-6. PULLEN, K., AND BREGLER, C. 2002. Motion capture assisted 
animation: Texturing and synthesis. In Proceedings of SIGGRAPH 2002. ROSE, C., GUENTER, B., BODENHEIMER, 
B., AND COHEN, M. F. 1996. Ef.cient generation of motion transitions using spacetime constraints. In 
Proceedings of SIGGRAPH 1996, vol. 30, 147 154. SCHODL, A., SZELISKI, R., SALESIN, D., AND ESSA, I. 2000. 
Video textures. In Proceedings of SIGGRAPH 2000, 489 498. VEACH, E., AND GUIBAS, L. J. 1997. Metropolis 
light transport. In Proceedings of SIGGRAPH 1997, vol. 31, 65 76. WITKIN, A., AND POPOVIC, Z. 1995. Motion 
warping. In Proceedings of SIG-GRAPH 1995, 105 108. O ROURKE, J. 1998. Computational Geometry in C. Cambridge 
University Press.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566607</article_id>
		<sort_key>491</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Interactive control of avatars animated with human motion data]]></title>
		<page_from>491</page_from>
		<page_to>500</page_to>
		<doi_number>10.1145/566570.566607</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566607</url>
		<abstract>
			<par><![CDATA[Real-time control of three-dimensional avatars is an important problem in the context of computer games and virtual environments. Avatar animation and control is difficult, however, because a large repertoire of avatar behaviors must be made available, and the user must be able to select from this set of behaviors, possibly with a low-dimensional input device. One appealing approach to obtaining a rich set of avatar behaviors is to collect an extended, unlabeled sequence of motion data appropriate to the application. In this paper, we show that such a motion database can be preprocessed for flexibility in behavior and efficient search and exploited for real-time avatar control. Flexibility is created by identifying plausible transitions between motion segments, and efficient search through the resulting graph structure is obtained through clustering. Three interface techniques are demonstrated for controlling avatar motion using this data structure: the user selects from a set of available choices, sketches a path through an environment, or acts out a desired motion in front of a video camera. We demonstrate the flexibility of the approach through four different applications and compare the avatar motion to directly recorded human motion.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[avatars]]></kw>
			<kw><![CDATA[human motion]]></kw>
			<kw><![CDATA[interactive control]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[virtual environments]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P138132</person_id>
				<author_profile_id><![CDATA[81100429553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jehee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37025092</person_id>
				<author_profile_id><![CDATA[81100205074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jinxiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39047820</person_id>
				<author_profile_id><![CDATA[81100543322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[S. A.]]></middle_name>
				<last_name><![CDATA[Reitsma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14028670</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207020</person_id>
				<author_profile_id><![CDATA[81100495142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nancy]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Pollard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARIKAN, O., AND FORSYTH, D. A. 2002. Interactive motion generation from examples. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BADLER, N. I., HOLLICK, M., AND GRANIERI, J. 1993. Real-time control of a virtual human using minimal sensors. Presence 2, 82-86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>379692</ref_obj_id>
				<ref_obj_pid>379437</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BEN-ARIE, J., PANDIT, P., AND RAJARAM, S. S. 2001. Design of a digital library for human movement. In Proceedings of the first ACM/IEEE-CS International Conference on Digital Libraries, 300-309.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218405</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLUMBERG, B. M., AND GALYEAN, T. A. 1995. Multi-level direction of autonomous creatures for real-time virtual environments. In Proceedings of SIGGRAPH 95, 47-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>281021</ref_obj_id>
				<ref_obj_pid>280953</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BLUMBERG, B. 1998. Swamped! Using plush toys to direct autonomous animated characters. In SIGGRAPH 98 Conference Abstracts and Applications, 109.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BOWDEN, R. 2000. Learning statistical models of human motion. In IEEE Workshop on Human Modelling, Analysis and Synthesis, CVPR2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BRADLEY, E., AND STUART, J. 1997. Using chaos to generate choreographic variations. In Proceedings of the Experimental Chaos Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BRAND, M., AND HERTZMANN, A. 2000. Style machines. In Proceedings of SIGGRAPH 2000, 183-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851583</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[BRAND, M. 1999. Shadow puppetry. In IEEE International Conference on Computer Vision, 1237-1244.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND CALVERT, T. W. 1989. Goal-directed, dynamic animation of human walking. In Computer Graphics (Proceedings of SIGGRAPH 89), vol. 23, 233-242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>241078</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND CALVERT, T. 1996. Knowledge-driven, interactive animation of human running. In Graphics Interface '96, 213-221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND WILLIAMS, L. 1995. Motion signal processing. In Proceedings of SIGGRAPH 95, 97-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383315</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J., VILHJ&#193;LMSSON, H. H., AND BICKMORE, T. 2001. Beat: The behavior expression animation toolkit. In Proceedings of SIGGRAPH 2001, 477-486.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>332075</ref_obj_id>
				<ref_obj_pid>332051</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J. 2000. Embodied conversational interface agents. Communications of the ACM, 4 (April), 70-78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>352172</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[CHI, D. M., COSTA, M., ZHAO, L., AND BADLER, N. I. 2000. The emote model for effort and shape. In Proceedings of SIGGRAPH 2000, 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>301152</ref_obj_id>
				<ref_obj_pid>301136</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[CHOPRA-KHULLAR, S., AND BADLER, N. I. 1999. Where to look? Automating attending behaviors of virtual human characters. In Proceedings of the third annual conference on Autonomous Agents, 16-23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. The virtual stuntman: dynamic characters with a repertoire of autonomous motor skills. Computers & Graphics 25, 6 (December), 933-953.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for physics-based character animation. In Proceedings of SIGGRAPH 2001, 251-260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[FRALEY, C., AND RAFTERY, A. E. 1998. How many clusters? Which clustering method? Answers via model-based cluster analysis. Computer Journal 41, 8, 578-588.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>377412</ref_obj_id>
				<ref_obj_pid>376890</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[GALATA, A., JOHNSON, N., AND HOGG, D. 2001. Learning variable length markov models of behaviour. Computer Vision and Image Understanding (CVIU) Journal 81, 3 (March), 398-413.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253321</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1997. Motion editing with spacetime constraints. In 1997 Symposium on Interactive 3D Graphics, ACM SIGGRAPH, 139-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargeting motion to new characters. In Proceedings of SIGGRAPH 98, 33-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508129</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 2001. Comparing constraint-based motion editing methods. Graphical Models 63, 2, 107-123.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O'BRIEN, J. F. 1995. Animating human athletics. In Proceedings of SIGGRAPH 95, 71-78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[HU, M. K. 1962. Visual pattern recognition by moment invariants. IRE Transactions on Information Theory 8, 2, 179-187.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274990</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[LAMOURET, A., AND VAN DE PANNE, M. 1996. Motion synthesis by example. In EGCAS '96: Seventh International Workshop on Computer Animation and Simulation, Eurographics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237231</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[LASZLO, J. F., VAN DE PANNE, M., AND FIUME, E. L. 1996. Limit cycle control and its application to the animation of balancing and walking. In Proceedings of SIGGRAPH 96, 155-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like figures. In Proceedings of SIGGRAPH 99, 39-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[LI, Y., WANG, T., AND SHUM, H.-Y. 2002. Motion texture: A two-level statistical model for character synthesis. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274982</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[MOLET, T., BOULIC, R., AND THALMANN, D. 1996. A real-time anatomical convertex for human motion capture. In EGCAS '96: Seventh International Workshop on Computer Animation and Simulation, Eurographics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246810</ref_obj_id>
				<ref_obj_pid>1246807</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[MOLET, T., AUBEL, T., GAPIN, T., CARION, S., AND LEE, E. 1999. Anyone for tennis? Presence 8, 2, 140-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>823423</ref_obj_id>
				<ref_obj_pid>822088</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[MOLINA TANCO, L., AND HILTON, A. 2000. Realistic synthesis of novel human movements from a database of motion capture examples. In Proceedings of the Workshop on Human Motion, 137-142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[OXFORD METRIC SYSTEMS, 2002. www.vicon.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND GOLDBERG, A. 1996. Improv: A system for scripting interactive actors in virtual worlds. In Proceedings of SIGGRAPH 96, 205-216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1995. Real time responsive animation with personality. IEEE Transactions on Visualization and Computer Graphics 1, 1 (Mar.), 5-15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872897</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2000. Animating by multi-level sampling. In Computer Animation 2000, IEEE CS Press, 36-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566608</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2002. Motion capture assisted animation: Texturing and synthesis. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[ROSALES, R., ATHITSOS, V., SIGAL, L., AND SCLAROFF, S. 2001. 3D hand pose reconstruction using specialized mappings. In IEEE International Conference on Computer Vision, 378-385.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., COHEN, M. F., AND BODENHEIMER, B. 1998. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics & Applications 18, 5 (September - October), 32-40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[SARCOS, 2002. www.sarcos.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[SCH&#214;DL, A., SZELISKI, R., SALESIN, D. H., AND ESSA, I. 2000. Video textures. In Proceedings of SIGGRAPH 2000, 489-498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246992</ref_obj_id>
				<ref_obj_pid>1246990</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[SEMWAL, S., HIGHTOWER, R., AND STANSFIELD, S. 1998. Mapping algorithms for real-time control of an avatar using eight sensors. Presence 7, 1, 1-21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649172</ref_obj_id>
				<ref_obj_pid>645315</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[SIDENBLADH, H., BLACK, M. J., AND SIGAL, L. 2002. Implicit probabilistic models of human motion for synthesis and tracking. In European Conference on Computer Vision (ECCV).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383288</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[SUN, H. C., AND METAXAS, D. N. 2001. Automating gait animation. In Proceedings of SIGGRAPH 2001, 261-270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[TARJAN, R. 1972. Depth first search and linear graph algorithms. SIAM Journal of Computing 1, 146-160.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[UNUMA, M., ANJYO, K., AND TAKEUCHI, R. 1995. Fourier principles for emotion-based human figure animation. In Proceedings of SIGGRAPH 95, 91-96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618477</ref_obj_id>
				<ref_obj_pid>616049</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[WILEY, D., AND HAHN, J. K. 1997. Interpolation synthesis of articulated figure motion. IEEE Computer Graphics and Applications 17, 6 (November), 39-45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A. P., AND POPOVI&#262;, Z. 1995. Motion warping. In Proceedings of SIGGRAPH 95, 105-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[WOOTEN, W. L., AND HODGINS, J. K. 1996. Animation of human diving. Computer Graphics Forum 15, 1, 3-14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Control of Avatars Animated with Human Motion Data Jehee Lee Jinxiang Chai Paul S. A. Reitsma 
Carnegie Mellon University Carnegie Mellon University Brown University Abstract Real-time control of 
three-dimensional avatars is an important problem in the context of computer games and virtual environ­ments. 
Avatar animation and control is dif.cult, however, because a large repertoire of avatar behaviors must 
be made available, and the user must be able to select from this set of behaviors, possibly with a low-dimensional 
input device. One appealing approach to obtaining a rich set of avatar behaviors is to collect an extended, 
unlabeled sequence of motion data appropriate to the application. In this paper, we show that such a 
motion database can be prepro­cessed for .exibility in behavior and ef.cient search and exploited for 
real-time avatar control. Flexibility is created by identifying plausible transitions between motion 
segments, and ef.cient search through the resulting graph structure is obtained through clustering. Three 
interface techniques are demonstrated for controlling avatar motion using this data structure: the user 
selects from a set of avail­able choices, sketches a path through an environment, or acts out a desired 
motion in front of a video camera. We demonstrate the .exibility of the approach through four different 
applications and compare the avatar motion to directly recorded human motion. CR Categories: I.3.7 [Three-Dimensional 
Graphics and Realism]: Animation Virtual reality Keywords: human motion, motion capture, avatars, virtual 
envi­ronments, interactive control 1 Introduction The popularity of three-dimensional computer games 
with human characters has demonstrated that the real-time control of avatars is an important problem. 
Two dif.culties arise in animating and controlling avatars, however: designing a rich set of behaviors 
for the avatar, and giving the user control over those behaviors. De­signing a set of behaviors for an 
avatar is dif.cult primarily due to the real-time constraint, especially if we wish to make use of rel­atively 
unstructured motion data for behavior generation. The raw material for smooth, appealing, and realistic 
avatar motion can be provided through a large motion database, and this approach is fre­quently used 
in video games today. Preparing such a database, how­ {jehee|jchai|jkh}@cs.cmu.edu, {psar|nsp}@cs.brown.edu 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00  Figure 1: Real-time avatar control in our system. (Top) The user controls the avatar s motion 
using sketched paths in maze and rough terrain environments. (Bottom left) The user selects from a number 
of choices in a playground environment. (Bottom right) The user is controlling the avatar by performing 
a motion in front of a camera. In this case only, the avatar s motion lags the user s input by several 
seconds. ever, requires substantial manual processing and careful design so that the character s behavior 
matches the user s expectations. Such databases currently tend to consist of many short, carefully planned, 
labeled motion clips. A more .exible and more broadly useful ap­proach would allow extended, unlabeled 
sequences of motion cap­ture data to be exploited for avatar control. If such unstructured data is used, 
however, searching for an appropriate motion in an on-line fashion becomes a signi.cant challenge. Providing 
the user with an intuitive interface to control the avatar s motion is dif.cult because the character 
s motion is high dimensional and most of the available input devices are not. In­put from devices such 
as mice and joysticks typically indicates a position (go to this location), velocity (travel in this 
direction at this speed) or behavior (perform this kick or pick up this object). This input must then 
be supplemented with autonomous behaviors and transitions to compute the full motion of the avatar. Control 
of individual degrees of freedom is not possible for interactive envi­ronments unless the user can use 
his or her own body to act out or pantomime the motion. In this paper, we show that a rich, connected 
set of avatar be­haviors can be created from extended, freeform sequences of mo­tion, automatically organized 
for ef.cient search, and exploited for real-time avatar control using a variety of interface techniques. 
The motion is preprocessed to add variety and .exibility by creating connecting transitions where good 
matches in poses, velocities, and contact state of the character exist. The motion is then clustered 
into groups for ef.cient searching and for presentation in the interfaces. A unique aspect of our approach 
is that the original motion data and the generalization of that data are closely linked; each frame of 
the original motion data is associated with a tree of clusters that cap­tures the set of actions that 
can be performed by the avatar from that speci.c frame. The resulting cluster forest allows us to take 
advan­tage of the power of clusters to generalize the motion data without losing the actual connectivity 
and detail that can be derived from that data. This two-layer data structure can be ef.ciently searched 
at run time to .nd appropriate paths to behaviors and locations spec­i.ed by the user. We explore three 
different interfaces to provide the user with intuitive control of the avatar s motion: choice, sketch, 
and per­formance (.gure 1). In choice interfaces, the user selects among a number of options (directions, 
locations, or behaviors) every few seconds. The options that are presented to the user are selected from 
among the clusters created during the preprocessing of the motion data. In the sketching interface, the 
user speci.es a path through the environment by sketching on the terrain, and the data structure is searched 
to .nd motion sequences that follow that path. In per­formance interfaces, the user acts out a behavior 
in front of a video camera. The best .t for his or her motion is then used for the avatar, perhaps with 
an intervening segment of motion to provide a smooth transition. For all three interface techniques, 
our motion data struc­ture makes it possible to transform possibly low-dimensional user input into realistic 
motion of the avatar. We demonstrate the power of this approach through examples in four environments 
(.gure 1) and through comparison with directly recorded human motion in similar environments. We note 
that the vision-based interface, due to the higher dimensional nature of the input, gives the most control 
over the details of the avatar s mo­tion, but that the choice and sketch interfaces provide the user 
with simple techniques for directing the avatar to achieve speci.c goals.  2 Background The behaviors 
required for animating virtual humans range from very subtle motions such as a slight smile to highly 
dynamic, whole body motions such as diving or running. Many of the applications envisioned for avatars 
have involved interpersonal communication and as a result, much of the research has focused on the subtle 
as­pects of the avatar s appearance and motion that are essential for communication: facial expressions, 
speech, eye gaze direction, and emotional expression [Cassell 2000; Chopra-Khullar and Badler 1999]. 
Because our focus is on applications in which whole body actions are required and subtle communication 
is not, we review only the research related to whole body human motion. Animated human .gures have been 
driven by keyframed mo­tion, rule-based systems [Bruderlin and Calvert 1989; Perlin 1995; Bruderlin and 
Calvert 1996; Perlin and Goldberg 1996; Chi et al. 2000; Cassell et al. 2001], control systems and dynamics 
[Hodgins et al. 1995; Wooten and Hodgins 1996; Laszlo et al. 1996; Falout­sos et al. 2001a; Faloutsos 
et al. 2001b], and, of course, motion capture data. Motion capture data is the most common technique 
in commercial systems because many of the subtle details of human motion are naturally present in the 
data rather than having to be in­troduced via domain knowledge. Most research on handling motion capture 
data has focused on techniques for modifying and varying existing motions. See Gleicher [2001] for a 
survey. This need may be partially obviated by the growing availability of signi.cant quan­tities of 
data. However, adaptation techniques will still be required for interactive applications in which the 
required motions cannot be precisely or completely predicted in advance. A number of researchers have 
shared our goal of creating new motion for a controllable avatar from a set of examples. For sim­ple 
behaviors like reaching and pointing that can be adequately spanned by a data set, straightforward interpolation 
works remark­ably well [Wiley and Hahn 1997]. Several groups explored methods for decomposing the motion 
into a behavior and a style or emo­tion using a Fourier expansion [Unuma et al. 1995], radial basis functions 
[Rose et al. 1998] or hidden Markov models with simi­lar structure across styles [Brand and Hertzmann 
2000]. Other re­searchers have explored introducing random variations into motion in a statistically 
reasonable way: large variations were introduced using chaos by Bradley and Stuart [1997] and small variations 
were introduced using a kernel-based representation of joint probability distributions by Pullen and 
Bregler [2000]. Domain speci.c knowl­edge can be very effective: Sun and Metaxas [2001] used principles 
from biomechanics to represent walking motion in such a way that it could be adapted to walking on slopes 
and around curves. Lamouret and van de Panne [1996] implemented a system that was quite similar to ours 
albeit for a far simpler character, a hopping planar Luxo lamp. A database of physically simulated motion 
was searched for good transitions, based on the state of the character, local terrain, and user preferences. 
The selected hop is then adapted to match the terrain. A number of researchers have used statistical 
models of human motion to synthesize new animation sequences. Galata and her col­leagues [2001] use variable 
length hidden Markov models to al­low the length of temporal dependencies to vary. Bowden [2000] uses 
principle component analysis (PCA) to simplify the motion, K-means clustering to collect like motions, 
and a Markov chain to model temporal constraints. Brand and Hertzmann s system al­lowed the reconstruction 
of a variety of motions statistically derived from the original dataset. Li et al. [2002] combine low 
level, noise driven motion generators with a high level Markov process to gen­erate new motions with 
variations in the .ne details. All of these systems used generalizations of the motion rather than the 
original motion data for synthesis, which runs the risk of smoothing out sub­tle motion details. These 
systems also did not emphasize control of the avatar s motion or behavior. Recent research efforts have 
demonstrated approaches similar to ours in that they retain the original motion data for use in synthesis. 
Sidenbladh and her colleagues [2002] have developed a probabilis­tic model for human motion tracking 
and synthesis of animations from motion capture data that predicts each next frame of motion based on 
the preceeding d frames. PCA dimensionality reduction combined with storage of motion data fragments 
in a binary tree help to contain the complexity of a search for a matching motion fragment in the database. 
Pullen and Bregler [2002] allow an an­imator to keyframe motion for a subset of degrees of freedom of 
the character and use a motion capture library to synthesize motion for the missing degrees of freedom 
and add texture to those that were keyframed. Kovar and his colleagues [2002] generate a graph structure 
from motion data and show that branch and bound search is very effective for controlling a character 
s motion for constraints such as sketched paths where the motion can be constructed incre­mentally. Their 
approach is similar to our sketch-based interface when no clustering is used. It has the advantage over 
our sketch­based interface, however, that the locomotion style (or other labeled characteristics of the 
motion) can be speci.ed for the sketched path. Arikan and Forsyth [2002] use a hierarchy of graphs to 
represent connectivity of a motion database and perform randomized search to identify motions that satisfy 
user constraints such as motion du­ration and pose of the body at given keyframes. One advantage of their 
work over similar techniques is .exibility in the types of constraints that can be speci.ed by the user. 
To our knowledge the .rst paper to be published using this general approach was by Molina-Tanco and Hilton 
[2000], who created a system that can be controlled by the selection of start and ending keyframes. Prepro­cessing 
of the motion data included PCA dimensionality reduction and clustering. The user-speci.ed keyframes 
are identi.ed in par­Figure 2: A subject wearing retro-re.ective markers in the motion capture laboratory. 
 ticular clusters, a connecting path of clusters is found via dynamic programming, and the most probable 
sequence of motion segments passing through these clusters is used to generate the details of the new 
motion. Their underlying data structure is similar to ours, al­though our use of cluster trees offers 
more .exibility in paths avail­able to the avatar at a given frame. In the area of interface techniques 
for controlling avatars, most successful solutions to date have given the character suf.cient au­tonomy 
that it can be directed with a low dimensional input. Be­sides the standard mouse or joystick, this input 
may come from vision (e.g. [Blumberg and Galyean 1995]) or from a puppet (e.g. [Blumberg 1998]). Control 
at a more detailed level can be provided if the user is able to act out a desired motion. The infrared 
sensor-based mocap games Mocap Boxing and Police 911 by Konami are an interesting commercial example 
of this class of interface. The user s motion is not precisely matched, although the impact of the user 
s motion on characters in the environment is essential for game play. In the re­search community, a number 
of groups have explored avatar control via real time magnetic motion capture systems (e.g. [Badler et 
al. 1993] [Semwal et al. 1998] [Molet et al. 1996] [Molet et al. 1999]). Alternatives to magnetic motion 
capture are available for capturing whole body motion in real time optically [Oxford Metric Systems 2002] 
or via an exoskeleton [Sarcos 2002]. Vision-based interfaces are appealing because they allow the user 
to move unencumbered by sensors. Vision data from a sin­gle camera, however, does not provide complete 
information about the user s motion, and a number of researchers have used motion capture data to develop 
mappings or models to assist in reconstruc­tion of three-dimensional pose and motion from video (e.g. 
[Ros­ales et al. 2001] [Brand 1999]). In the area of database retrieval, Ben-Arie and his colleagues 
[2001] use video data to index into a database of human motions that was created from video data and 
show that activity classes can be discriminated based on a sparse set of frames from a query video sequence. 
In our approach, the prob­lem of controlling an avatar from vision is more similar to database retrieval 
than pose estimation in the sense that the system selects an action for the avatar from among a .nite 
number of possibilities. 3 Human Motion Database The size and quality of the database is key to the 
success of this work. The database must be large enough that good transitions can be found as needed 
and the motion must be free of glitches and other characteristic problems such as feet that slip on the 
ground. The human motion data was captured with a Vicon optical motion capture system. The system has 
twelve cameras, each of which is capable of recording at 120Hz with images of 1000x1000 res­olution. 
We used a marker set with 43 14mm markers that is an adaptation of a standard biomechanical marker set 
with additional markers to facilitate distinguishing the left side of the body from the right side in 
an automatic fashion. The motions were captured in a working volume for the subject of approximately 
8 x24 . A Figure 3: Two layer structure for representing human motion data. The lower layer retains the 
details of the original motion data, while the higher layer generalizes that data for ef.cient search 
and for presentation of possible actions to the user. subject is shown in the motion capture laboratory 
in .gure 2. We captured subjects performing several different sets of mo­tions: interacting with a step 
stool (stepping on, jumping over, walking around, and sitting on), walking around an empty envi­ronment 
(forwards, backwards, and sideways), walking over poles and holes rough terrain, and swinging and climbing 
on a piece of playground equipment. Each subject s motion is represented by a skeleton that includes 
his or her limb lengths and joint range of motion (computed automatically during a calibration phase). 
Each motion sequence contains trajectories for the position and orienta­tion of the root node (pelvis) 
as well as relative joint angles for each body part. For the examples presented here, only one subject 
s mo­tion was used for each example. The motion database contains a single motion (about 5 minutes long) 
for the step stool example, 9 motions for walking around the environment, 26 motions on the rough terrain 
and 11 motions on the playground equipment. The motion is captured in long clips (an average of 40 seconds 
excluding the step stool motion) to allow the subjects to perform natural transitions between behaviors. 
Our representation of the motion data does not require hand segmenting the motion data into individual 
actions as that occurs naturally as part of the clustering of the data in preprocessing. Contact with 
the environment is an important perceptual fea­ture of motion, and the database must be annotated with 
contact information for generation of good transitions between motion seg­ments. The data is automatically 
processed to have this informa­tion. The system determines if a body segment and an environment object 
are in contact by considering their relative velocity and prox­imity. For instance, feet are considered 
to be on the ground if one of their adjacent joints (either the ankle or the toe) is suf.ciently close 
to the ground and its velocity is below some threshold. 4 Data Representation Human motion is typically 
represented either in a form that pre­serves the original motion frames or in a form that generalizes 
those frames with a parametric or probabilistic model. Both representa­tions have advantages; the former 
allows details of the original mo­tion to be retained for motion synthesis, while the latter creates 
a simpler structure for searching through or presenting the data. Our representation attempts to capture 
the strengths of both by combin­ing them in a two-layer structure (.gure 3). The higher layer is a statistical 
model that provides support for the user interfaces by clustering the data to capture similarities among 
character states. The lower layer is a Markov process that creates new motion se­quences by selecting 
transitions between motion frames based on the high-level directions of the user. A unique aspect of 
our data structure is the link between these layers: trees of accessible clus­ters are stored in the 
lower layer on a frame by frame basis, pro­viding a high-level and correct description of the set of 
behaviors achievable from each speci.c motion frame. The next two sections describe the details of the 
two layers and the link between these layers, beginning with the lower-level Markov process. 4.1 Lower 
Layer: Markov Process We model motion data as a .rst-order Markov process, inspired by the work on creating 
non-repeating series of images presented in Sch¨ odl et al. [2000]. The transition from one state to 
the next of a .rst-order Markov process depends only on the current state, which is a single frame of 
motion. The Markov process is represented as a matrix of probabilities with the elements Pij describing 
the probability of transitioning from frame i to frame j. As in Sch¨odl et al. [2000], the probabilities 
are estimated from a measure of similarity between frames using a exponential function: Pij . exp(-Di,j-1/s), 
(1) where Di,j-1 represents the distance between frame i and frame j - 1, and s controls the mapping 
between the distance measure and the probability of transition. The distance function is computed as 
Dij = d(pi,pj )+ .d(vi,vj ). (2) The .rst term d(pi,pj ) describes the weighted differences of joint 
angles, and the second term d(vi,vj ) represents the weighted dif­ferences of joint velocities. Parameter 
. weights velocity differ­ences with respect to position differences. The velocity term helps to preserve 
the dynamics of motion by, for example, discriminating between similar poses in forward walk and backward 
walk. In our implementation, Euclidean differences are used for veloc­ities. Position differences are 
expressed as m -1 .2 d(pi,pj )= .pi,0 - pj,0.2 + wk. log q (3) j,k qi,k k=1 where pi,0 . R3 is the translational 
position of the character at frame i, qi,k . S3 is the orientation of joint k with respect to its parent 
in frame i, and joint angle differences are summed over m rotational joints. The value of log(qa -1 qb) 
is a vector v such that a v rotation of 2.v. about the axis .v. takes a body from orientation qa to orientation 
qb. Important joints are selected manually to deter­mine weights; weights wk are set to one for joints 
at the shoulders, elbows, hips, knees, pelvis, and spine. Weights are set to zero for joints at the neck, 
ankle, toes and wrists, which have less impact on visible differences between poses. The matrix of probabilities 
Pij computed using equation 1 is dense and requires O(n 2) storage space for n motion frames. Be­cause 
the motion database is large (4000 to 12000 frames depending on the application), O(n 2) storage space 
may be prohibitive. Many of the transitions are of low probability and pruning them not only reduces 
the required storage space but also improves the quality of the resulting motion by avoiding too frequent 
transitions. We use four rules to prune the transitions (table 1). The .rst rule is based on the observation 
that contact is a key element of a motion. It prunes transitions between motion segments with dissimilar 
contact states and is described below. The second rule sets all probabilities be­low a user-speci.ed 
threshold to zero. The third rule favors the Table 1: The number of transition edges remaining after 
pruning edges present in the lower layer Markov model of motion for our examples. The total number of 
frames indicates initial database size for each example, and the number of possible edges is the square 
of this number. Edges are pruned based on contact conditions, based on a probability threshold, to eliminate 
similar transitions, and to remove edges that do not fall entirely within the largest strongly connected 
component (SCC) of the graph. total # of Pruning Criteria frames contact likelihood similarity SCC Maze 
8318 394284 32229 N/A 31469 Terrain 12879 520193 60130 N/A 59043 Step Stool 4576 30058 4964 N/A 4831 
Playground 5971 984152 37209 7091 5458 best transition among many similar transitions by selecting local 
maxima in the transition matrix and setting the probabilities of the others to zero. The fourth rule 
eliminates edges that are not fully contained in the single largest connected component of the graph 
as described below. Pruning based on contact. Pruning based on contact is done by examining contact states 
of the two motion segments at the transi­tion. A transition from frame i to frame j is pruned if frames 
i and j -1 or frames i+1 and j are in different contact states. For exam­ple, a transition is not allowed 
from a pose where the foot is about to leave the ground to another pose where the foot is about to touch 
the ground even if the con.gurations are similar. In practice, in all of our examples except the playground, 
this rule is made even more strict; transitions are allowed only during a contact change, and this same 
contact change must occur from frame i to frame i +1 and from frame j - 1 to frame j. Pruning of similar 
transitions is not necessary when this stricter contact pruning rule is in force. Be­cause contact change 
is a discrete event, no similar, neighboring sets of transitions exist to be pruned. Avoiding dead ends. 
As pointed out by Sch¨ odl et al. [2000], transitions might lead to a portion of motion that has no exits. 
They suggested avoiding dead ends by predicting the anticipated future cost of a transition and giving 
high cost to dead ends. Although their method works well in practice, it does not guarantee that dead 
ends will never be encountered. We instead .nd the strongly connected subcomponent of the directed graph 
whose nodes are the frames of the motion and whose edges are the non-zero transitions. We imple­mented 
Tarjan s algorithm [Tarjan 1972], with running time linear in the number of nodes, to .nd all the strongly 
connected subcom­ponents. In our experiments, the algorithm usually found one large strongly connected 
component and a number of small components most of which consist of a single node. We set the transition 
proba­bilities to zero if the transitions leave the largest strongly connected component. 4.1.1 Blending 
Transitions Although most transitions should introduce only a small disconti­nuity in the motion, a transition 
might be noticeable if the motion sequence simply jumped from one frame to another. Instead the system 
modi.es the motion sequence after the transition to match the sequence before the transition. If the 
transition is from frame i to frame j, the frames between j - 1 and j + b - 1 are modi.ed so that the 
pose and velocity at frame j - 1 is matched to the pose and velocity at frame i. Displacement mapping 
techniques are used to preserve the .ne details of the motion [Witkin and Popovi´c 1995; Bruderlin and 
Williams 1995]. In our experiments, blend interval b ranges from 1 to 2 seconds, depending on the example. 
We main­tain a buffer of motion frames during the blend interval. If a transi­tion happens before the 
previous transition has completed, motion frames in the buffer are used instead of the original motion 
frames for the overlap between the current and previous blend intervals. Although blending avoids jerkiness 
in the transitions, it can cause undesirable side effects such as foot sliding when there is contact 
between the character and the environment during the blend interval. This problem can be addressed by 
using constraint-based motion editing techniques such as [Gleicher 1997; Gleicher 1998; Lee and Shin 
1999]. We use the hierarchical motion .tting algo­rithm presented by Lee and Shin [1999]. A good set 
of contact constraints, expressed as a desired trajec­tory (both translation and rotation) for each contacting 
body, must be provided to the motion .tting algorithm. These constraints are obtained from one of the 
two motion sequences involved in the blend. Suppose the transition is from frame i to frame j, and a 
body is in contact with the environment between frame k and frame l. If there is an overlap between the 
blending interval and the contact interval [k, l], we establish the constraint based on the following 
cases: In Case 1, the constraint lies over the start of the blending interval and between frame i and 
frame l the foot should follow the tra­jectory of the motion sequence before the transition. Similarly 
in Case 2 the constraint lies over the end of the blending interval and for frame k to frame j + b - 
1 the trajectory is taken from the mo­tion sequence after the transition. In Case 3, the contact interval 
is contained within the blending interval and the trajectory of the foot can be taken from either side. 
Our implementation chooses the closer side. In Case 4, the constraint lies over both boundaries and there 
is no smooth transition. In this situation, the system allows the foot to slip or disables the transition 
by setting the correspond­ing probability to zero. 4.1.2 Fixed and Relative Coordinate Systems The motion 
in the database is stored as a position and orientation for the root (pelvis) in every frame, along with 
joint angles in the form of orientation of each body with respect to its parent. The position and orientation 
of the root segment at frame i can be represented in a .xed, world coordinate system or as a relative 
translation and rotation with respect to the previous frame of motion (frame i - 1). The decision of 
whether to represent the root in a .xed or relative coordinate system affects the implementation of Markov 
process for human motion data. With a .xed coordinate system, transitions will only oc­cur between motion 
sequences that are located nearby in three­dimensional space, while the relative coordinate system allows 
tran­sitions to similar motion recorded anywhere in the capture region. The relative coordinate system 
effectively ignores translation on the horizontal plane and rotation about the vertical axis when consider­ing 
whether two motions are similar or not. The decision as to which coordinate system is most appropri­ate 
depends on the amount of structure in the environment created for the motion capture subjects: highly 
structured environments al­lowed less .exibility than unstructured environments. In our exper­iments, 
we used a .xed coordinate system for the playground and step stool examples, because they involved interaction 
with .xed objects. We used a relative coordinate system for the maze exam­ple, where motion data was 
captured in an environment with no ob­stacles. For the uneven terrain example, we ignored vertical transla­tion 
of the ground plane, allowed rotations about the vertical axis in integer multiples of 90 degrees, and 
allowed horizontal translations that were close to integer multiples of block size. This arrangement 
provided .exibility in terrain height and extent while preserving step locations relative to the discontinuities 
in the terrain.  4.2 Higher Layer: Statistical Models While the lower layer Markov model captures motion 
detail and provides the avatar with a broad variety of motion choices, the re­sulting data structure 
may be too complex for ef.cient search or clear presentation in a user interface. The higher layer is 
a gener­alization of the motion data that captures the distribution of frames and transitions. Our method 
for generalizing the data is based on cluster analysis. Clusters are formed from the original motion 
data. These clusters capture similarities in motion frames, but they do not capture the connections between 
frames (.gure 4). To capture these connections, or the tree of choices available to the avatar at any 
given motion frame, we construct a data structure called a clus­ter tree at each motion frame. The entire 
higher layer is then called a cluster forest. 4.2.1 Cluster Analysis Cluster analysis is a method for 
sorting observed values into groups. The data are assumed to have been generated from a mixture of probabilistic 
distributions. Each distribution represents a different cluster. Fraley and Raftery [1998] provides an 
excellent survey on cluster analysis. In our application, we use the isometric Gaussian mixture model 
in which each cluster is an isometric multivariate Gaussian distribution with variable standard deviation. 
Our goal is to capture similar motion states within the same cluster. Motion state for frame i (referred 
to below as observation xi) is a vector containing root position pi,0, weighted root orientation qi,0, 
and weighted orientations qi,k of all bodies k with respect to their parents as follows: xi =[pi,0 w0 
log(qi,0) w1 log(qi,1) ... wm log(qi,m)]T (4) where weights wk for the shoulders, elbows, hips, knees, 
pelvis, and spine are set to one in our examples and all other weights are set to zero, as in equation 
3. Specifying root orientation requires careful selection of the reference frame to avoid the singularity 
in the expression log(qi,0) when qi,0 is a rotation of 2p radians about any axis. We select a reference 
orientation q from a series of root orientations {qi,0} such that it minimizes the distance to the far­thest 
orientation in {qi,0}. The distance between two orientations is -1 -1 computed as d(qa,qb) = min(. log(qa 
qb)., . log(qa (-qb)).). Given a set of observations (motion frames) x =(x1, ··· , xN ), let fk(xi|.k) 
be the density of xi from the k-th cluster, where .k are the parameters (the mean and the variance of 
the Gaus­sian distribution) of the k-th cluster. The Expectation Maximiza­tion (EM) algorithm is a general 
method to .nd the parameters . =(.1, ··· ,.K ) of clusters that maximize the mixture log­likelihood NK 
L(.k,tk,zik|x)= zik log (tkfk(xi|.k)) , (5) i=1 k=1 where tk is the prior probability of each cluster 
k, and zik is the posterior probability that observation xi belongs to the k-th cluster. Given initial 
values for the parameters of clusters, the EM algorithm iterates between the expectation step in which 
zik are computed from the current parameters and maximization step in which the parameters are updated 
based on the new values for zik (See [Fraley and Raftery 1998] for details). For cluster analysis, we 
need to choose the number K of clus­ters. We use the Bayesian information criterion (BIC) [Fraley and 
Figure 4: Motion data preprocessing. (A) The motion database ini­tially consists of a number of motion 
clips containing many frames. (B) Many frame-to-frame transitions are created to form a directed graph. 
(C) Similar poses are clustered into groups. (D) To capture connections across cluster boundaries, we 
construct a cluster tree for each motion frame by traversing the graph to identify clusters that are 
reachable within a given depth (time) bound (6 transitions in this .gure). When a transition (thick arrow) 
crossing cluster bound­aries is encountered, a new node is added to the cluster tree. Note that frames 
within the same cluster may have different cluster trees. Raftery 1998] of a mixture model M = {.k,tk,zik} 
to estimate the number of clusters: BIC(M) = 2L(M|x) - mM log N, (6) where L(M|x) is the maximized mixture 
log-likelihood for the model, and mM is the number of independent parameters to be es­timated in the 
model (the number of clusters × the dimension of the clusters). A larger value of BIC(M) provides stronger 
evidence for the model. To estimate the number of clusters, we run the EM algorithm for a series of models 
with different numbers of clusters and choose the highest value of BIC(M). The result of expectation 
maximization depends greatly on the initial values for the parameters. A typical method is to repeat 
the EM algorithm with different (randomly generated) initial con­ditions and select the best result that 
gives the maximum log­likelihood. An alternative method is to select several keyframes by hand and take 
those frames as initial values for the means of clus­ters. This is the approach we used in practice. 
In our examples, 15 to 25 keyframes were selected by hand to provide a starting point for the EM algorithm. 
This small amount of manual input appeared to result in much better statistical models. 4.2.2 Cluster 
Forest After the motion frames are clustered, we construct a cluster forest to encode the choices available 
to the avatar. The cluster forest con­sists of a collection of cluster trees representing sets of avatar 
be­haviors. Each frame in the database has its own cluster tree, which re.ects the behaviors immediately 
available to the avatar from that frame. This arrangement allows the interface techniques to consider 
only relevant behaviors. For example, if the avatar is far from the step stool, the behaviors that involve 
interacting with the step stool are not relevant although they may become important as the avatar moves 
nearer. A cluster tree for a given motion frame r is computed as follows. The algorithm begins with a 
one-node tree consisting of the cluster to which r belongs. When a new frame j is visited from frame 
i that belongs to the k-th cluster, the algorithm checks if frame j belongs to the k-th cluster. If not, 
the cluster including frame j is added to the tree as a child of the current node. In either case, the 
children of frame j are traversed recursively. The recursion terminates when the depth of the spanning 
tree reaches a given maximum depth (time bound). In the examples presented here, the time bound was set 
to 15 seconds. Figure 4 shows some simple example cluster trees. Cluster trees are constructed for all 
frames that have multiple out-going transitions. Frames with a single out-going transition are assigned 
to the .rst cluster tree reachable by traversing the lower layer graph from that frame. We note that 
because a cluster tree is assigned to each frame in the lower layer graph and because there are no dead 
ends in the lower layer graph, there will be no dead ends created in the resulting higher layer structure. 
 4.2.3 Cluster Paths and Most Probable Motions A cluster path is a path from the root of a cluster tree 
to one of its leaves. If the cluster tree for a frame has k leaves, it will contain k cluster paths, 
each representing a collection of actions available to the avatar. For any given cluster path, we can 
.nd the most probable se­quence of motion frames. This sequence of frames is one of a gen­erally large 
number of possible motions that pass through the given cluster path, but it provides a concrete representation 
of that cluster path that is a useful reference for the interface techniques used to control the avatar. 
Suppose that cluster path p, associated with frame s0, consists of the sequence of clusters (.1,.2, ··· 
,.p). The most probable sequence of motion frames s =(s0,s1, ··· ,sN ) through the se­quence p can be 
de.ned in such a way that it maximizes N P (s, p)= P (s)P (p|s)= P (si-1 . si)P (.(si)|si), (7) i=1 where 
.(si) is the cluster to which frame si belongs. The most probable path can be found by traversing all 
possible paths that start from s0, are contained within p, and are within a given time bound. The joint 
probability P (s, p) is evaluated for each of these paths, and the most probable one is selected. 4.2.4 
Practical Use of Clustering Clustering was not used in all of our examples; the playground and step stool 
examples made use of clustering, while the maze and rough terrain examples did not. We found that the 
value of cluster­ing depended on both the richness of the dataset and on the type of interface used. 
For sketch-based interfaces in the maze and rough terrain examples, clustering was not necessary, and 
we could simply search the lower layer graph for the motion that best matched the sketch. For the choice-based 
interface in the playground example, clustering was needed to limit the number of options presented to 
Figure 5: (Top to bottom) Maze, terrain, and playground exam­ples. The left column shows the avatar being 
controlled in syn­thetic environments, and the right column shows the original data captured in our motion 
capture studio. Avatar motion in the maze and terrain environments is controlled using the sketch-based 
inter­face, and avatar motion in the playground environment is controlled using choice. the user. For 
the vision-based interface used in the step stool exam­ple, clustering was required for ef.ciency in 
matching user actions to possible avatar actions. User actions were compared against tens of paths in 
a cluster tree instead of millions of paths in the lower layer graph.  5 Controlling the Avatar Users 
of our system can control an avatar interactively through in­terfaces based on choice, sketching of paths, 
and performance (.g­ure 5). This section describes the interfaces we explored and the process of mapping 
from user input to avatar motion. 5.1 Choice In a choice-based interface, the user is continuously presented 
with a set of actions from which to choose (.gure 5, bottom). As the avatar moves, the display changes 
so that the choices remain ap­propriate to the context. When the user selects an action, the avatar performs 
that action. Whenever there is no currently selected ac­tion, the avatar s motion is probabilistically 
determined. One of the primary requirements for the choice-based interface is that the display should 
be uncluttered to avoid confusing the user. Figure 6: Windows in space and time can be used to constrain 
the set of clusters considered for presentation of possible actions to the user. In this .gure, the pink 
nodes are selected, because they fall in the intersection of the time bounds and the space bounds set 
by the user. The two rightmost of these nodes, the leaves of the selected subtree, will be portrayed 
as possible avatar actions. Figure 7: The sketch interface for the maze example. (Top left) Mo­tion 
data was recorded from a small, empty capture region in which a motion capture subject walked around. 
(Bottom left) Through graph search, we select a series of motion segments that are con­nected through 
the lower layer graph. (Right) These motion seg­ments are translated and rotated to match the sketched 
path in an environment with obstacles. Note that paths with collisions were eliminated during the search, 
and the selected path differs signi.­cantly from the sketched path where necessary to avoid the obstacle. 
In practice, this means that roughly three or four actions should be presented to the user at any given 
time. We use the cluster tree at the current frame to obtain a small set of actions for display that 
are typically well-dispersed. By default, leaves of the currently ac­tive cluster tree are portrayed 
as example poses. Additional visual information such as the path that would be traced by the charac­ter 
root or footprints along that path can provide more detail about available actions when desired. The 
user has some ability to con­trol how many and which actions are displayed by setting bounds on lookahead 
time and bounds on the desired location of the avatar in the environment (.gure 6). 5.2 Sketching The 
sketch-based interface allows the user to draw paths on the screen using a mouse. The two-dimensional 
paths are pro­jected onto the surfaces of environment objects to provide three­dimensional coordinates. 
In our examples, the sketched paths are assumed to represent the trajectory of the center of mass projected 
onto the ground plane. We have implemented two different ways to determine avatar motion from a sketched 
path. In the .rst ap­proach, used in the step stool environment (.gure 8, top right), the most appropriate 
action currently available to the avatar is selected. In the second approach, used in the maze and rough 
terrain envi­ronments (.gure 5, top and middle), avatar motion is constructed incrementally. Our .rst 
approach to sketch-based avatar control exploits the fact that it is often possible to .nd a good match 
to a sketched path based on the cluster tree at the current frame. For a given cluster tree, the Figure 
8: Step stool example. (Top left) Choice-based interface. (Top right) Sketch-based interface. (Middle 
and bottom left) The user performing a motion in front of a video camera and her sil­houette extracted 
from the video. (Middle and bottom right) The avatar being controlled through the vision-based interface 
and the rendered silhouette that matches the user s silhouette. actions available are represented by 
cluster paths through that tree. We compute a score for each cluster path m based on the most prob­able 
motion sm as follows. First, the center of mass of the character is projected onto the surfaces of environment 
objects for each frame in sm. Second, the distance between the resulting center of mass path and the 
sketched path is computed using dynamic time warp­ing (described in [Bruderlin and Williams 1995]). This 
distance is the score assigned to cluster path m, and the cluster path with the lowest score is selected 
as the best match to the user sketch. Our second approach allows us to track longer paths and to use 
a sketch-based interface in an environment where clustering has not been performed. In either case, the 
sketched path is considered as a sequence of goal positions to be traversed, and our tracking al­gorithm 
is based on best-.rst search through the lower layer graph. The following simple objective function is 
used: E(p)= .pg - p.- c((pg - p) · u), (8) where pg and p are the goal position and the actual position 
of the avatar respectively, and u is a vector of the avatar s facing direction. The .rst term of the 
objective function rewards motion toward the goal. The second term rewards facing directions that point 
toward the goal. At each time instance, our algorithm traverses a .xed number of frames to maintain a 
constant rate of motion. Note that this approach could result in local minima, where the avatar can no 
longer make progress toward the goal. The possibility of local min­ima is not a practical problem, however, 
because the user is doing the high-level planning and has already identi.ed an appropriate path. 5.3 
Vision In our vision-based interface, the user acts out the desired motion in front of a camera. Visual 
features are extracted from video and used to determine avatar motion (.gure 8, middle and bottom). The 
cost of an avatar s action is measured based on the difference between visual features present in rendered 
versions of that action and visual features present in the video. Visual features for a single video 
image are obtained from a sil­houette of the human body, which is extracted from the image us­ing foreground 
segmentation. We use nine different features drawn from this silhouette. The .rst seven features are 
the Hu moments [Hu 1962], which are used because they yield reasonable shape dis­crimination in a scale 
and rotation invariant manner. The remaining two features are the coordinates of the central location 
of the sil­houette region in the image plane. When measured over time, these coordinates provide information 
about translational motion of the user. Visual features for a motion frame are obtained during prepro­cessing 
of the motion database. Because we do not know in advance the orientation of the user with respect to 
the camera, each motion frame in the database is rendered from a number of different camera viewpoints. 
In our examples, 18 different camera viewpoints were used. Visual features are then extracted from the 
rendered images just as for real camera images and stored with each motion frame. For avatar control, 
we maintain a buffer of video frames as the user performs. For every time step of avatar motion, the 
video buffer is compared to the motions available to the avatar. If the avatar is currently at frame 
i, a cost is computed for each cluster path m associated with frame i and for each camera viewpoint using 
dynamic timewarping. The cost of cluster path m is the minimum cost over all camera viewpoints. The cluster 
path with minimum cost determines the avatar s next motion, and the process repeats. In our examples, 
the system selects an action for the avatar based on the current video buffer every 0.1 seconds as the 
avatar moves through the environment. Vision-based action selection can be done in real-time, but se­lecting 
an action for the avatar requires matching avatar actions in the future to user actions in the past, 
which means that there will always be some lag between user motion and avatar motion. In our examples, 
this lag is currently three seconds.  6 Results All of the motion data used in our experiments was collected 
at the rate of 120 frames/second and then down-sampled to 15 frames/second for real-time display. In 
the maze and terrain exam­ples, motion data are represented in a relative coordinate system. In the other 
two examples, a .xed coordinate system is employed. Maze. The maze example in .gure 5 (top) shows a path 
sketched by a user and an avatar being controlled to follow the path and avoid obstacles. The avatar 
is animated with a motion database of 9 min­utes (4.5 minutes duplicated by mirror re.ection) in which 
our mo­tion capture subject walked in a straight line, stopped and turned, side-stepped, and walked backwards. 
This example demonstrates that the avatar can select an appropriate motion at any time depend­ing on 
user input and the environment. For example, side steps are selected when it passes through a narrow 
passage. To assess the results, we recorded a subject moving in an en­vironment with physical obstacles 
and then compared her motion to the motion of the avatar moving in a similar environment (.g­ure 9). 
The database for the avatar s motion was recorded from a different subject in an environment without 
obstacles. The avatar s motion lacks context-dependent details of the recorded motion (for example, in 
the recorded motion the subject looked front and back to see if she would clear the obstacles, but the 
motion database did Figure 9: Comparison to directly recorded motion data. (Left) A subject passing through 
obstacles in an environment laid out to match the virtual world. (Middle) Recorded human motion. (Right) 
Avatar s motion created from the database. not include this head movement), but is still acceptable 
as a natural human motion. Terrain. For the rough terrain example, we recorded motions of about 15 minutes 
duration on small sections of rough terrain con­structed from ten wooden blocks (24 by 24 by 7.5 inches) 
with three height levels: the ground level, one block height, and two block height (.gure 5, middle). 
Those motions are suf.cient to allow an avatar to navigate in an extended rough terrain environment with 
much more height variation. The user can either sketch a path or mark a target position to direct the 
avatar. Playground. For the playground example (.gure 5, bottom), 7 minutes of motion were collected, 
and the motion data was grouped into 25 clusters. The playground example demonstrates that the avatar 
can be controlled in a complex environment in which dy­namic, full-body interaction is required. Although 
the sketch in­terface is .exible and intuitive, drawing a path was not appropriate for avatar control 
in a playground environment because many dis­tinct actions can be mapped to the same projected path. 
Instead, a choice-based interface was used, with key poses providing suf.­cient information for the user 
to select the desired action. Step stool. In the step stool example, a 5-minute sequence of motion was 
captured in which our motion capture subject walked around, stepped on, sat on, and jumped over the step 
stool. This data was grouped into 14 clusters. All three interfaces were tested for the step stool example 
(.gure 8). The advantage of the choice­based interface is that users are informed about the options available 
at any time. The sketch-based interface is very easy to use and re­sponds instantly to the user input 
by .nding a matching action and updating the display while the path is being drawn. Sometimes, the sketch-based 
interface was not able to distinguish different styles; jumping over and stepping over the step stool 
result in similar cen­ter of mass paths when they are projected on the ground. For the vision-based interface, 
the user performs desired motions in front of a high speed video camera that can capture 60 images per 
sec­ond. The silhouettes of the captured images are extracted from the video and compared to the preprocessed 
silhouettes of the avatar to select a matching action. The silhouette comparison usually can discriminate 
different styles. If the user acts out a motion that is not available in the database, the system selects 
motion that looks similar. For example, if the user jumps down from the step stool, no exact matching 
motion is available, and the system generates motion where the avatar simply steps down. 7 Discussion 
We have presented a two layer structure that allows a database of unsegmented, extended motion sequences 
to be used effectively to control an avatar with three quite different interfaces. Although we have not 
yet attempted to assess the effectiveness of the interfaces through user studies, our intuition is that 
each interface plays a use­ful role and that the best interface may well be determined by the application. 
The choice-based interface does a good job of show­ing the user what the options are and therefore may 
be most useful in applications such as a complicated but slow moving video game where the user might 
not be aware of all possibilities. However this interface is only effective for relatively slow moving 
behaviors as the user must have the time to make a decision before the op­tion is no longer valid. Sketching 
a path is a natural way to guide the avatar around the space but does not generalize to much more complicated 
environments where the two-dimensional mouse path projected to three dimensions does not adequately specify 
what the avatar should do. The vision-based interface allows the most complete control over the motion 
but an interface that requires whole body motions will certainly not be appropriate for all applications. 
The latency of the current system would be a factor in many applications and may be necessary if a precise 
match among many similar actions is re­quired. However, in applications such as ours with fairly small 
numbers of similar actions or where precise matches are not re­quired, we believe that additional video 
cameras and the compu­tation of more sophisticated visual cues such as motion and edge detection should 
allow us to reduce latency substantially. Although the motion databases used here are relatively large 
from a research perspective, they are much smaller than motion databases used in video games. Because 
our databases are of lim­ited size, we need to be careful to retain generality wherever possi­ble by 
using a local coordinate frame to express the motion of the character root, for example. A larger database 
for any of our ex­amples would result in more variety and .exibility in the avatar s motions. As database 
size grows, the ability to rapidly search for an appropriate action will become more and more critical. 
The two­layer structure proposed here where the cluster tree at each frame provides a generalization 
of the set of actions available to the avatar at that frame should scale well to larger databases, because 
the data structure stored at each frame contains only clusters immedi­ately relevant to the avatar and 
grows in size only with the branch­ing factor in transitions between those clusters. Keeping database 
size manageable is a concern, however, and determining when a database has suf.cient variety for a given 
space of motions is an open and interesting research problem. The Markov process on the lower level of 
the data representation has proven to be very effective for databases of the sizes we have explored. 
Smaller databases might not span the space of possible motions suf.ciently to provide the necessary transitions. 
A much larger database might require more aggressive pruning of transi­tions to prevent the computation 
time of the preprocessing step from scaling as the square of the number of frames. We made somewhat arbitrary 
decisions in deciding when to use cluster trees as part of the search process. The maze and rough terrain 
examples do not use clustering at all. This decision was partially motivated by the fact that clustering 
in a relative coordinate system is more dif.cult than clustering in a .xed coordinate system, and we 
found that clustering was not in fact necessary when the sketch-based interface was used. We assume, 
however, that as the size of the database increases further, clustering will become more important. The 
effectiveness of clustering for avatar control is in.uenced by the distribution of motion capture data 
collected. If we captured action A 100 times and action B once, it would be dif.cult to expect the EM 
algorithm to identify two separate actions. We found that it was important to ensure that actions were 
captured several times; our subjects were always asked to repeat their actions at least three times. 
We picked the initial member of each cluster by hand. Although this was not a burdensome chore, it does 
require that the application designer know the motion database suf.ciently well to select widely dispersed 
poses as cluster seeds. It should be possible to automate this process. The quality of clustering without 
hand seeding did improve with computation time spent, but estimation of reliably good clusters required 
several hours even for the small datasets. The three interfaces and four behavior domains shown clearly 
demonstrate the power of a large database of unsegmented motion recorded in extended sequences. We anticipate 
that as the size of publically available motion databases grows, we will see very com­pelling examples 
developed that build on statistical techniques for preprocessing the data such as those presented here. 
  Acknowledgments The authors would like to thank Rory and Justin Macey for their assistance collecting 
and cleaning the motion capture data. We would also like to thank all of our mo­tion capture subjects. 
This work was supported in part by NSF EIA CADRE Grant #0196217. References ARIKAN, O., AND FORSYTH, 
D. A. 2002. Interactive motion generation from exam­ples. In Proceedings of SIGGRAPH 2002. BADLER, N. 
I., HOLLICK, M., AND GRANIERI, J. 1993. Real-time control of a virtual human using minimal sensors. Presence 
2, 82 86. BEN-ARIE, J., PANDIT, P., AND RAJARAM, S. S. 2001. Design of a digital library for human movement. 
In Proceedings of the .rst ACM/IEEE-CS International Conference on Digital Libraries, 300 309. BLUMBERG, 
B. M., AND GALYEAN, T. A. 1995. Multi-level direction of au­tonomous creatures for real-time virtual 
environments. In Proceedings of SIG-GRAPH 95, 47 54. BLUMBERG, B. 1998. Swamped! Using plush toys to 
direct autonomous animated characters. In SIGGRAPH 98 Conference Abstracts and Applications, 109. BOWDEN, 
R. 2000. Learning statistical models of human motion. In IEEE Workshop on Human Modelling, Analysis and 
Synthesis, CVPR2000. BRADLEY, E., AND STUART, J. 1997. Using chaos to generate choreographic varia­tions. 
In Proceedings of the Experimental Chaos Conference. BRAND, M., AND HERTZMANN, A. 2000. Style machines. 
In Proceedings of SIG-GRAPH 2000, 183 192. BRAND, M. 1999. Shadow puppetry. In IEEE International Conference 
on Computer Vision, 1237 1244. BRUDERLIN, A., AND CALVERT, T. W. 1989. Goal-directed, dynamic animation 
of human walking. In Computer Graphics (Proceedings of SIGGRAPH 89), vol. 23, 233 242. BRUDERLIN, A., 
AND CALVERT, T. 1996. Knowledge-driven, interactive animation of human running. In Graphics Interface 
96, 213 221. BRUDERLIN, A., AND WILLIAMS, L. 1995. Motion signal processing. In Proceed­ings of SIGGRAPH 
95, 97 104. CASSELL, J., VILHJ ´ ALMSSON, H.H., AND BICKMORE,T.2001.Beat:Thebehavior expression animation 
toolkit. In Proceedings of SIGGRAPH 2001, 477 486. CASSELL, J. 2000. Embodied conversational interface 
agents. Communications of the ACM, 4 (April), 70 78. CHI, D. M., COSTA, M., ZHAO, L., AND BADLER, N. 
I. 2000. The emote model for effort and shape. In Proceedings of SIGGRAPH 2000, 173 182. CHOPRA-KHULLAR, 
S., AND BADLER, N. I. 1999. Where to look? Automating attending behaviors of virtual human characters. 
In Proceedings of the third annual conference on Autonomous Agents, 16 23. FALOUTSOS, P., VAN DE PANNE, 
M., AND TERZOPOULOS, D. 2001. The virtual stuntman: dynamic characters with a repertoire of autonomous 
motor skills. Com­puters &#38; Graphics 25, 6 (December), 933 953. FALOUTSOS, P., VAN DE PANNE, M., AND 
TERZOPOULOS, D. 2001. Composable controllers for physics-based character animation. In Proceedings of 
SIGGRAPH 2001, 251 260. FRALEY, C., AND RAFTERY, A. E. 1998. How many clusters? Which clustering method? 
Answers via model-based cluster analysis. Computer Journal 41, 8, 578 588. GALATA, A., JOHNSON, N., AND 
HOGG, D. 2001. Learning variable length markov models of behaviour. Computer Vision and Image Understanding 
(CVIU) Journal 81, 3 (March), 398 413. GLEICHER, M. 1997. Motion editing with spacetime constraints. 
In 1997 Symposium on Interactive 3D Graphics, ACM SIGGRAPH, 139 148. GLEICHER, M. 1998. Retargeting motion 
to new characters. In Proceedings of SIG-GRAPH 98, 33 42. GLEICHER, M. 2001. Comparingconstraint-basedmotioneditingmethods. 
Graphical Models 63, 2, 107 123. HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O BRIEN, J. F. 1995. 
Animating human athletics. In Proceedings of SIGGRAPH 95, 71 78. HU, M. K. 1962. Visual pattern recognition 
by moment invariants. IRE Transactions on Information Theory 8, 2, 179 187. KOVAR, L., GLEICHER, M., 
AND PIGHIN, F. 2002. Motion graphs. In Proceedings of SIGGRAPH 2002. LAMOURET, A., AND VAN DE PANNE, 
M. 1996. Motion synthesis by example. In EGCAS 96: Seventh International Workshop on Computer Animation 
and Simu­lation, Eurographics. LASZLO, J. F., VAN DE PANNE, M., AND FIUME, E. L. 1996. Limit cycle control 
and its application to the animation of balancing and walking. In Proceedings of SIGGRAPH 96, 155 162. 
LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like .gures. 
In Proceedings of SIGGRAPH 99, 39 48. LI, Y., WANG, T., AND SHUM, H.-Y. 2002. Motion texture: A two-level 
statistical model for character synthesis. In Proceedings of SIGGRAPH 2002. MOLET, T., BOULIC, R., AND 
THALMANN, D. 1996. A real-time anatomical con­verter for human motion capture. In EGCAS 96: Seventh International 
Workshop on Computer Animation and Simulation, Eurographics. MOLET, T., AUBEL, T., GAPIN, T., CARION, 
S., AND LEE, E. 1999. Anyone for tennis? Presence 8, 2, 140 156. MOLINA TANCO, L., AND HILTON, A. 2000. 
Realistic synthesis of novel human movements from a database of motion capture examples. In Proceedings 
of the Workshop on Human Motion, 137 142. OXFORD METRIC SYSTEMS, 2002. www.vicon.com. PERLIN, K., AND 
GOLDBERG, A. 1996. Improv: A system for scripting interactive actors in virtual worlds. In Proceedings 
of SIGGRAPH 96, 205 216. PERLIN, K. 1995. Real time responsive animation with personality. IEEE Transac­tions 
on Visualization and Computer Graphics 1, 1 (Mar.), 5 15. PULLEN, K., AND BREGLER, C. 2000. Animating 
by multi-level sampling. In Computer Animation 2000, IEEE CS Press, 36 42. PULLEN, K., AND BREGLER, C. 
2002. Motion capture assisted animation: Texturing and synthesis. In Proceedings of SIGGRAPH 2002. ROSALES, 
R., ATHITSOS, V., SIGAL, L., AND SCLAROFF, S. 2001. 3D hand pose reconstruction using specialized mappings. 
In IEEE International Conference on Computer Vision, 378 385. ROSE, C., COHEN, M. F., AND BODENHEIMER, 
B. 1998. Verbs and adverbs: Mul­tidimensional motion interpolation. IEEE Computer Graphics &#38; Applications 
18, 5 (September -October), 32 40. SARCOS, 2002. www.sarcos.com. SCH ¨ ODL, A., SZELISKI, R., SALESIN, 
D. H., AND ESSA, I. 2000. Video textures. In Proceedings of SIGGRAPH 2000, 489 498. SEMWAL, S., HIGHTOWER, 
R., AND STANSFIELD, S. 1998. Mapping algorithms for real-time control of an avatar using eight sensors. 
Presence 7, 1, 1 21. SIDENBLADH,H., BLACK, M.J., AND SIGAL,L.2002.Implicitprobabilisticmodels of human 
motion for synthesis and tracking. In European Conference on Computer Vision (ECCV). SUN, H. C., AND 
METAXAS, D. N. 2001. Automating gait animation. In Proceedings of SIGGRAPH 2001, 261 270. TARJAN, R. 
1972. Depth .rst search and linear graph algorithms. SIAM Journal of Computing 1, 146 160. UNUMA, M., 
ANJYO, K., AND TAKEUCHI, R. 1995. Fourier principles for emotion­based human .gure animation. In Proceedings 
of SIGGRAPH 95, 91 96. WILEY, D., AND HAHN, J. K. 1997. Interpolation synthesis of articulated .gure 
motion. IEEE Computer Graphics and Applications 17, 6 (November), 39 45. WITKIN, A. P., AND POPOVI C´, 
Z. 1995. Motion warping. In Proceedings of SIG-GRAPH 95, 105 108. WOOTEN, W. L., AND HODGINS, J. K. 1996. 
Animation of human diving. Computer Graphics Forum 15, 1, 3 14.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566608</article_id>
		<sort_key>501</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Motion capture assisted animation]]></title>
		<subtitle><![CDATA[texturing and synthesis]]></subtitle>
		<page_from>501</page_from>
		<page_to>508</page_to>
		<doi_number>10.1145/566570.566608</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566608</url>
		<abstract>
			<par><![CDATA[We discuss a method for creating animations that allows the animator to sketch an animation by setting a small number of keyframes on a fraction of the possible degrees of freedom. Motion capture data is then used to enhance the animation. Detail is added to degrees of freedom that were keyframed, a process we call texturing. Degrees of freedom that were not keyframed are synthesized. The method takes advantage of the fact that joint motions of an articulated figure are often correlated, so that given an incomplete data set, the missing degrees of freedom can be predicted from those that are present.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
			<kw><![CDATA[motion texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382443</person_id>
				<author_profile_id><![CDATA[81100069123]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katherine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pullen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15019977</person_id>
				<author_profile_id><![CDATA[81100027036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bregler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARIKAN, O., AND FORSYTH, D. A. 2002. Interactive motion generation from examples. Proc. SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BODENHEIMER, B., SHLEYFMAN, A., AND HODGINS, J. 1999. The effects of noise on the perception of animated human running. Computer Animation and Simulation '99, Eurographics Animation Workshop (September), 53-63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BONET, J. S. D. 1997. Multiresolution sampling procedure for analysis and synthesis of texture images. proc. SIGGRAPH 1997, 361-368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BRAND, M., AND HERTZMANN, A. 2000. Style machines. proc. SIGGRAPH 2000, 183-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311537</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BRAND, M. 1999. Voice puppetry. Proc. SIGGRAPH 1999, 21-28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794493</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BREGLER, C. 1997. Learning and recognizing human dynamics in video sequences. Proc. CVPR, 569-574.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND WILLIAMS, L. 1995. Motion signal processing. proc. SIGGRAPH 1995, 97-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344882</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CHENNEY, S., AND FORSYTH, D. A. 2000. Sampling plausible solutions to multi-body constraint problems. proc. SIGGRAPH 2000, 219-228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>352172</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CHI, D., COSTA, M., ZHAO, L., AND BADLER, N. 2000. The emote model for effort and shape. proc. SIGGRAPH 2000, 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253321</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1997. motion editing with spacetime constraints. 1997 Symposium on Interactive 3D graphics, 139-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1998. Retargeting motion to new characters. proc. SIGGRAPH 1998, 33-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HEEGER, D. J., AND BERGEN, J. R. 1995. Pyramid-based texture analysis/sysnthesis. proc. SIGGRAPH 1995, 229-238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J., WOOTEN, W. L., BROGAN, D. C., AND O'BRIEN, J. F. 1995. Animating human athletics. proc. SIGGRAPH 1995, 229-238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. Proc. SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like figures. proc. SIGGRAPH 1999, 39-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508128</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LEE, J., AND SHIN, S. Y. 2001. A coordinate-invariant approach to multiresolution motion analysis. Graphical Models 63, 2, 87-105.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LEE, J., CHAI, J., REITSMA, P. S. A., HODGINS, J. K., AND POLLARD, N. S. 2002. Interactive control of avatars animated with human motion data. Proc. SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LI, Y., WANG, T., AND SHUM, H. 2002. Motion texture: A two-level statistical model for character motion synthesis. Proc. SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND GOLDBERG, A. 1996. Improv: a system for scripting interactive actors in virtual reality worlds. proc. SIGGRAPH 1996, 205-216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1985. An image synthesizer. Computer Graphics 19, 3, 287-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[POPOVIC, Z., AND WITKIN, A. 1999. Physically based motion transformation. proc. SIGGRAPH 1999, 159-168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872897</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PULLEN, K., AND BREGLER, C. 2000. Animating by multi-level sampling. IEEE Computer Animation Conference, 36-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SCHODL, A., SZELISKI, R., SALESIN, D. H., AND ESSA, I. 2000. Video textures. Proc. SIGGRAPH 00, 489-498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[UNUMA, M., ANJYO, K., AND TEKEUCHI, R. 1995. Fourier principles for emotion-based human figure animation. proc. SIGGRAPH 1995, 91-96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. 1988. Spacetime constraints. Computer Graphics 22, 159-168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND POPOVIC, Z. 1995. Motion warping. proc. SIGGRAPH 1995, 105-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Capture Assisted Animation: Texturing and Synthesis Katherine Pullen Christoph Bregler Stanford 
University Stanford University Abstract We discuss a method for creating animations that allows the 
anima­tor to sketch an animation by setting a small number of keyframes on a fraction of the possible 
degrees of freedom. Motion capture data is then used to enhance the animation. Detail is added to de­grees 
of freedom that were keyframed, a process we call texturing. Degrees of freedom that were not keyframed 
are synthesized. The method takes advantage of the fact that joint motions of an artic­ulated .gure are 
often correlated, so that given an incomplete data set, the missing degrees of freedom can be predicted 
from those that are present. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and 
Realism Animation; J.5 [Arts and Humantities]: performing arts Keywords: animation, motion capture, motion 
texture, motion synthesis 1 Introduction As the availability of motion capture data has increased, there 
has been more and more interest in using it as a basis for creating com­puter animations when life-like 
motion is desired. However, there are still a number of dif.culties to overcome concerning its use. As 
a result, most high quality animations are still created by keyfram­ing by skilled animators. Animators 
usually prefer to use keyframes because they allow precise control over the actions of the character. 
However, creating a life-like animation by this method is extremely labor intensive. If too few key frames 
are set, the motion may be lacking in the detail we are used to seeing in live motion (.gure 1). The 
curves that are generated between key poses by computer are usually smooth splines or other forms of 
interpolation, which may not represent the way a live human or animal moves. The animator can put in 
as much detail as he or she wants, even to the point of specifying the position at every time, but more 
detail requires more time and effort. A second reason keyframing can be extremely labor intensive is 
that a typical model of an articulated .gure has over 50 degrees of freedom, each of which must be painstakingly 
keyframed. Motion capture data, on the other hand, provides all the detail and nuance of live motion 
for all the degrees of freedom of a char­acter. However, it has the disadvantage of not providing for 
full Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00  Figure 1: Comparison of keyframed data and motion capture data for root y translation for walking. 
(a) keyframed data, with keyframes indicated by red dots (b) motion capture data. In this ex­ample, the 
keyframed data has been created by setting the minimum possible number of keys to describe the motion. 
Notice that while it is very smooth and sinusoidal, the motion capture data shows ir­regularities and 
variations. These natural .uctuations are inherent to live motion. A professional animator would achieve 
such detail by setting more keys. control over the motion. Motion capture sessions are labor inten­sive 
and costly, and if the actor does not do exactly what the anima­tor had in mind or if plans change after 
the motion capture session, it can be dif.cult and time consuming to adapt the motion capture data to 
the desired application. A more subtle problem with motion capture data is that it is not an intuitive 
way to begin constructing an animation. Animators are usually trained to use keyframes, and will often 
build an animation by .rst making a rough animation with few keyframes to sketch out the motion, and 
add complexity and detail on top of that. It is not easy or convenient for an animator to start creating 
an animation with a detailed motion he or she did not create and know every aspect of. We propose a method 
for combining the strengths of keyframe animation with those of using motion capture data. The animator 
begins by creating a rough sketch of the scene he or she is creating by setting a small number of keyframes 
on a few degrees of free­dom. Our method will use the information in motion capture data to add detail 
to the degrees of freedom that were animated if de­sired, a process we call adding texture to the motion. 
Degrees of freedom that were not keyframed at all are synthesized. The result is an animation that does 
exactly what the animator wants it to, but has the nuance of live motion. 2 Related Work There has been 
a great deal of past research in a number of different areas that are related to our project. We divide 
this work into four main categories that are described below. 2.1 Variations in Animation Many other 
researchers before us have made the observation that part of what gives a texture its distinctive look, 
be it in cloth or in motion, are variations within the texture. These variations are of­ten referred 
to as noise, and one of the earliest papers to address this topic was in image texture synthesis, where 
random variability was added to textures with the Perlin-noise function [Perlin 1985]. These ideas were 
later applied to animations [Perlin and Goldberg 1996]. Other researchers have created motion of humans 
running using dynamical simulations [Hodgins et al. 1995] and applied hand crafted noise functions [Bodenheimer 
et al. 1999]. Statistical vari­ations in motion were extracted directly from data by sampling kernel-based 
probability distributions in [Pullen and Bregler 2000]. Here we also create animations that exhibit natural 
variations, in this case because they are inherent to the fragments of motion cap­ture data that we use 
in texturing and synthesis. 2.2 Signal Processing There are a number of earlier studies in which researchers 
in both texture synthesis and motion studies have found it to be useful to look at their data in frequency 
space. In image texture synthesis, one of the earliest such approaches divided the data into multi-level 
Laplacian pyramids, and synthetic data were created by a histogram matching technique [Heeger and Bergen 
1995]. This work was fur­ther developed by DeBonet [1997], in which the synthesis takes into account 
the fact that the higher frequency bands tend to be condi­tionally dependent upon the lower frequency 
bands. We incorporate a similar approach, but applied to motion data. In animation, Unuma et al. [1995] 
use fourier analysis to manipulate motion data by performing interpolation, extrapolation, and transi­tional 
tasks, as well as to alter the style. Bruderlin and Williams [1995] apply a number of different signal 
processing techniques to motion data to allow editing. Lee and Shin [2001] develop a mul­tiresolution 
analysis method that guarantees coordinate invariance for use in motion editing operations such as smoothing, 
blending, and stitching. Our work relates to these animation papers in that we also use frequency bands 
as a useful feature of the data, but we use them to synthesize motion data. 2.3 Motion Editing Many 
techniques have been proposed that start with existing mo­tions, often obtained from motion capture data, 
and vary the mo­tions to adapt to different constraints while preserving the style of the original motion. 
Witkin and Popovic [1995] developed a method in which the motion data is warped between keyframe-like 
constraints set by the animator. The spacetime constraints method originally created by Witkin and Kass 
[1988] was developed to al­low the animator to specify constraints such as feet positions of a character, 
and then solve for these constraints by minimizing the difference from the original data [Gleicher 1997]. 
In further work, this method was applied to adapt a set of motion data to characters of different sizes 
[Gleicher 1998], and combined with a multiresolution approach for interactive control of the re­sult 
[Lee and Shin 1999]. Physics were included in the method of Popovic and Witkin [1999], in which the editing 
is performed in a reduced dimensionality space. We also are interested in adapting motion capture data 
to differ­ent situations. However, rather than starting with the live data, we start with a sketch created 
by the animator of what the .nal result should be, and .t the motion capture data onto that framework. 
As a result, it can be used to create motions substantially different from what was in the original data. 
 2.4 Style and synthesis Numerous other projects besides ours have addressed the problem of synthesizing 
motions or altering pre-existing motions to have a particular style. A Markov chain monte carlo algorithm 
was used to sample multiple animations that satisfy constraints for the case of multi-body collisions 
of inanimate objects [Chenney and Forsyth 2000]. In work with similar goals to ours but applied to image­based 
graphics, other researchers [Schodl et al. 2000] develop the concept of a video texture, which enables 
a user to begin with a short video clip and then generate an in.nite amount of similar looking video. 
Monte carlo techniques are used to address the stochastic nature of the texture, and appropriate transitions 
are found in the motion to create a loop. The method was applied to example mo­tions that contain both 
a repetitive and stochastic component, such as .re or a .ag blowing in the wind. In other projects, a 
common method of representing data has been to use mixtures of Gaussians and hidden Markov models. Bre­gler 
[1997] has used them to recognize full body motions in video sequences, and Brand [1999] has used them 
to synthesize facial an­imations from example sets of audio and video. Brand and Hertz­mann [2000] have 
also used hidden Markov models along with an entropy minimizations procedure to learn and synthesize 
motions with particular styles. Our approach differs in that we strive to keep as much of the information 
in the motion capture data intact as pos­sible, by directly using fragments of real data rather than 
general­izing it with representations that may cause some of the .ne detail to be lost. In other interesting 
work, Chi and her colleagues [Chi et al. 2000] presented work with similar goals to ours, in that they 
were seeking to create a method that allows animators to enhance the style of pre-existing motions in 
an intuitive manner. They made use of the principles of Laban Movement Analysis to create a new interface 
for applying particular movement qualities to the motion. More recently, there have been a number of 
projects aimed to­ward allowing an animator to create new animations based on mo­tion capture data. For 
example, in the work of Li et al. [2002], the data was divided into motion textons, each of which could 
be modelled by a linear dynamic system. Motions were synthesized by considering the likelihood of switching 
from one texton to the next. Other researchers developed a method for automatic motion generation at 
interactive rates [Arikan and Forsyth 2002]. Here the animator sets high level constraints and a random 
search algorithm is used of .nd appropriate pieces of motion data to .ll in between. In closely related 
work, the concept of a motion graph is de.ned to enable one to control a characters s locomotion [Kovar 
et al. 2002]. The motion graph contains original motion and automatically gen­erated translations, and 
allows a user to have high level control over the motions of the character. In the work of [Lee et al. 
2002], a new technique is developed for controlling a character in real time using several possible interfaces. 
The user can choose from from a set of possible actions, sketch a path on the screen, or act out the 
motion in front of a video camera. Animations are created by searching through a motion data base using 
a clustering algorithm. Any of the above techniques would be more appropriate to use than ours in the 
case where the user has a large data base of motions and wants high level control over the actions of 
the character. Our project is geared more toward an animator who may have a limited set of data of a 
particular style, and who wants to have .ne control over the motion using the familiar tools of keyframing. 
 Figure 2: Correlation between joint angles. Shown is the ankle angle versus the hip angle for human 
walking data. The fact that this plot has a de.nite form demonstrates that the angles are related to 
each other.  3 Methods In human and animal motion, there are many correlations between joint actions. 
These correlations are especially clear for a repetitive motion like walking. For example as the right 
foot steps forward, the left arm swings forward, or when the hip angle has a certain value, the knee 
angle is most likely to fall within a certain range. We can see those correlations graphically with a 
plot such as that shown in .gure 2, where we plot the ankle angle as a function of hip angle for some 
human walking data. The fact that the plot has a speci.c shape, a skewed horseshoe shape in this case, 
indicates that there is a relationship between the angles. These relationships hold true for more complex 
motions as well, but may be more local in time, speci.c to a particular action within a motion data set. 
In our method we take advantage of these rela­tionships to synthesize degrees of freedom that have not 
been ani­mated. Similarly, we can add detail to a degree of freedom that has been animated by synthesizing 
only the higher frequency bands, a process we refer to as texturing. The animator must provide the following 
information: (1) which joint angles should be textured (2) which joint angles should be synthesized (3) 
which joint angles should be used to drive the mo­tion in each case. For example, suppose an animator 
sketches out a walk by animating only the legs and wants to synthesize the upper body motions. A good 
choice for the angles to drive the animation would be the hip and knee x angles (where we de.ne the x 
axis as horizontal, perpendicular to the direction of walking) because they de.ne the walking motion. 
These data are broken into fragments, and used to .nd fragments of the motion capture data with hip and 
knee x angles similar to what has been created by keyframing. The corresponding fragments of motion capture 
data for the upper body motion can then be used to animate the upper body of the computer character. 
To achieve this task, we require a method to determine what con-Figure 3: Frequency analysis. Shown are 
bands 2-7 of a Laplacian Pyramid decomposition of the left hip angle for dance motions from both keyframing 
and motion capture. One band, shown with a red dashed line, is chosen for the matching step stitutes 
a matching region of data. The problem is complicated by the fact that the keyframed data may be of a 
different time scale from the real data. In addition, the ends of the fragments we choose must join together 
smoothly to avoid high frequency glitches in the motion. We address these issues in our method, which 
we divide into the following steps: (1) frequency analysis (2) matching (3) path .nding and (4) joining. 
In the following explanation, we will use the example of using the left hip and left knee x angles to 
synthesize upper body mo­tions. These degrees of freedom will be referred to as the matching angles. 
Also note that we de.ne keyframed data as the data at ev­ery time point that has been generated in the 
animation after setting the keyframes. An example of such data is in .gure 1a. 3.1 Frequency Analysis 
In order to separate different aspects of the motion, the .rst step is to divide the data (both keyframed 
and motion capture) into frequency bands (.gure 3). For a joint that has already been animated, we may 
only want to alter the mid to high frequency range, leaving the overall motion intact. For a degree of 
freedom that has not been animated, we may wish to synthesize all of the frequency bands.  3.2 Matching 
Matching is at the heart of our method. It is the process by which fragments of data from the keyframed 
animation are compared to fragments of motion capture data to .nd similar regions. To begin this step, 
a low frequency band of one of the joints is chosen, in our example the left hip angle. The results are 
not highly dependent upon which frequency band is chosen, as long as it is low enough to provide information 
about the overall motion. For example in .gure 3 we illustrate chosing band 6 of the Laplacian pyramid, 
but chosing band 4 or 5 also yields good results. Band 7 is too low, as can be seen by the lack of structure 
in the curve, and band 3 is too high, as it does not re.ect the overall motion well enough. We .nd the 
locations in time where the .rst derivative of the chosen band of one of the matching angles changes 
sign. The real and keyframed data of all of the matching angles of that band (the Figure 4: Breaking 
data in to fragments. The bands of the keyframed data and motion capture data shown with red dashed lines 
in .gure 3 are broken into fragments where the sign of the .rst derivative changes. (a) keyframed data. 
(b) motion capture data. (c) keyframed data broken in to fragments. (d) motion cap­ture data broken into 
fragments.  left hip and left knee x angles, in our example) are broken into frag­ments at those locations 
(.gure 4). Note that in the .gures we il­lustrate the process for just one of the matching angles, the 
hip, but actually the process is applied to all of the matching angles simul­taneously. We also match 
the .rst derivative of the chosen band of each of these angles. Including the .rst derivatives in the 
matching helps choose fragments of real data that are more closely matched not only in value but in dynamics 
to the keyframed data. Note that the sign change of the .rst derivative of only one of the angles is 
used to determine where to break all of the data corresponding to the matching angles into fragments, 
so that all are broken at the same locations. All of the fragments of keyframed data in the chosen frequency 
band and their .rst derivatives are stepped through one by one, and for each we ask which fragment of 
real data is most similar (.g­ure 5). To achieve this comparison, we stretch or compress the real data 
fragments in time by linearly resampling them to make them the same length as the keyframed fragment. 
In the motion capture data, there are often unnatural poses held for relatively long periods of time 
for calibration purposes. To avoid chosing these fragments, any real fragment that was originally more 
than 4 times as long as the fragment of keyframed data being matched is rejected. We .nd the sum of squared 
differences between the keyframed fragment being matched and each of the real data fragments, and keep 
the K closest matches. As we save fragments of the matching angles, we also save the corresponding fragments 
of original motion cap­ture data (not just the frequency band being matched) for all of the angles to 
be synthesized or textured (.gure 6). At this point, it is sometimes bene.cial to include a simple scale 
factor. Let A be the m × n matrix of values in the keyframed data being matched, where m is the number 
of matching angles and n is the length of those fragments. Let M be the m × n matrix of one of the K 
choices of matching fragments. Then to scale the data, we look for the scale factor s that minimizes 
.Ms - A.. The factor s is then multiplied by all of the data being synthesized. In practice such a scale 
factor is useful only in a limited set of cases, because it assumes a linear relationship between the 
magnitude of the match-Figure 5: Matching. Each keyframed fragment is compared to all of the motion capture 
fragments, and the K closest matches are kept. Shown is the process of matching the .rst fragment shown 
in .g­ure 4 (c). (a) The keyframed fragment to be matched. (b) The keyframed fragment, shown in a thick 
blue line, compared to all of the motion capture fragments, shown in thin black lines. (c) Same as (b), 
but the motion captured fragments have been stretched or compressed to be the same length as the keyframed 
fragment. (d) Same as (c), but only the 5 closest matches are shown. ing angles and the magnitude of 
the rest of the angles, which is not usually likely to be true. However, it can improve the resulting 
animations for cases in which the keyframed data is similar to the motion capture data, and the action 
is fairly constrained, such as walking. More fragments than just the closest match are saved because 
there is more to consider than just how close the data fragment is to the original. We must take into 
consideration which fragments come before and after. We would like to encourage the use of con­secutive 
chunks of data as described in the next section.  3.3 Path .nding Now that we have the K closest matches 
for each fragment, we must choose a path through the possible choices to create a single data set. The 
resulting animation is usually more pleasing if there are sections in time where fragments that were 
consecutive in the data are used consecutively to create the path. As a result, our algorithm considers 
the neighbors of each fragment, and searches for paths that maximize the use of consecutive fragments. 
For each join between fragments, we create a cost matrix, the ijth component of which gives the cost 
for joining fragment i with fragment j. A score of zero is given if the fragments were consec­utive in 
the original data, and one if they were not. We .nd all of the possible combinations of fragments that 
go through the points of zero cost. This technique is easiest to explain using an example, which is diagrammed 
in .gure 7. Suppose we had 4 fragments of synthetic data to match, and saved 3 nearest matches. In the 
illustration we show that for fragment 1 of the keyframed data, the best matches were to fragments 4, 
1, and 3 of the real data, and for fragment 2 of the keyframed data the closest matches were to fragments 
5, 7, and 2 of the real data, and so on. We have drawn lines between fragments to indicate paths of zero 
cost. Here there are three best Figure 6: Matching and synthesis. (a) The .ve closest matches for a series 
of fragments of keyframed data is shown. The keyframed data is shown with a thick blue line, the matching 
motion capture fragments are shown with thin black lines. (b) An example of one of the angles being synthesized 
is shown, the lowest spine joint angle rotation about the x axis. The .ve fragments for each section 
come from the spine motion capture data from the same location in time as the matching hip angle fragments 
shown in plot (a). (c) An ex­ample of a possible path through the chosen spine angle fragments is shown 
with a thick red line.  choices. One is fragment 4, 5, 6, and 2 from the real data. In this case we 
choose fragment 2 of the real data to match the fourth frag­ment of keyframed data rather than 8 or 5 
because it was originally the closest match. A second possible path would be 4, 5, 4, and 5, and a third 
would be 1, 2, 4, 5. All three would yield two instances of zero cost. An example of an actual path taken 
through fragments chosen by matching is shown in .gure 6c. Note that for z instances of zero cost, there 
can be no greater than z paths to consider, and in fact will usually be far less because the instances 
can be linked up. In our example (.gure 7) there were four instances of zero cost, but only three possible 
paths that min­imize the cost. The P best paths (where P is a parameter set by the animator) are saved 
for the animator to look at. All are valid choices and it is an artistic decision which is best. In practice 
we found that saving roughly 1/10 the total number of fragments produced good results. Saving too many 
matches re­sulted in motions that were not coordinated with the rest of the body, and saving too few 
did not allow for suf.cient temporal co­herence when seeking a path through the fragments.  3.4 Joining 
Now that we have the best possible paths, the ends may still not quite line up in cases where the fragments 
were not originally con­secutive. For example, in .gure 6c we show an example of data after matching 
and choosing the paths. To take care of these dis­continuities, we join the ends together by the following 
process. For fragment i, we de.ne the new endpoints as follows. The new .rst point will be the mean between 
the .rst point of fragment i and the last point of fragment i - 1. (Note that there is overlap between 
the ends of the fragments; if the last point of fragment i is placed at time t, the .rst point of fragment 
i + 1 is also at time t.) The new last point of fragment i will be the mean between the last point of 
fragment i and the .rst point of fragment i + 1. Figure 7: Choosing a path by maximizing the instances 
of con­secutive fragments. In the table we show a hypothetical example of a case where four keyframed 
fragments were matched, and the K = 3 closest matches of motion capture fragments were kept for each 
keyframed fragment. The matches at the tops of the columns are the closest of the 3 matches. Blue lines 
are drawn between frag­ments that were consecutive in the motion capture data, and the cost matricies 
between each set of possible matches are shown below. The next step is to skew the fragment to pass through 
the new endpoints. To achieve this warping, we de.ne two lines, one that passes through the old endpoints, 
and one that passes through the new endpoints. We subtract the line that passes through the old endpoints 
and add the line that passes through the new endpoints to yield the shifted fragment. The process is 
diagramed in .gure 8. In order to further smooth any remaining discontinuity, a quadratic function is 
.t to the join region from N points away from the joint point to within 2 points of the join point, where 
N is a parameter. A smaller value of N keeps the data from being altered too greatly from what was in 
the motion capture data, and a larger value more effectively blends between different fragments. In prac­tice 
we found a N from 5-20 to be effective, corresponding to 0.2 -0.8 seconds. The resulting quadratic is 
blended with the original joined data using a sine squared function as follows. De.ne the blend function 
f as pt f =(cos + 1)2 (1) 2N where N is half the length of the shortest of the fragments that are to 
be joined and t is the time, shifted to be zero at the join region. If we de.ne q as the quadratic function 
we obtained from the .t, and m as the data after matching, then the data s after smoothing is s(t)= f 
(t)q(t)+(1 - f (t))m(t). (2) An example of this process is shown in .gure 9.  4 Experiments We tested 
our method on several different situations, three of which are described below. All of these examples 
are presented on the accompanying video tape. 4.1 Walking A short animation of two characters walking 
toward each other, slowing to a stop, stomping, and crouching was created using Figure 9: Smoothing at 
the join point. A close up of the join be­tween fragments 1 and 2 from .gure 8 is shown with a red solid 
line. (a) The quadratic .t using the points on either side of the join point (as described in the text) 
is shown with a black dashed line.  (b) The data after blending with the quadratic .t is shown with 
a blue dashed line. keyframes. Keyframes were set only on the positions of the root (not the rotations) 
and feet. Inverse kinematics were used on the feet at the ankle joint, as is customary in keyframe animation 
of ar­ticulated .gures. The joint angles for the hips and knees were read out afterwards for use in texturing 
and synthesis. Each character s motion was enhanced using a different motion capture data set of walking 
motion. The two data sets each con­sisted of walking with roughly a single step size, but each exhibited 
a very different style of walking. One was a relatively normal walk, but rather bouncy, and the other 
was of a person imitating a drag queen and was quite stylized, containing unusual arm and head gestures. 
The length of each data set was 440 time points at 24 frames per second, or about 18 seconds worth of 
data. A Lapla­cian pyramid was used for the frequency analysis. The 4th highest band was used for matching. 
For texturing, bands 2-3 were synthe­sized, and for synthesis, all bands 2 and lower. The very highest 
frequency band tended to add only undesirable noise to the motion. The upper body degrees of freedom 
could successfully be syn­thesized using a number of different combinations for the matching angles. 
For example, both hip x angles; the left hip x and left knee x angle; or the right hip x and right knee 
x all gave good results. The most pleasing results were obtained by using data from the left hip and 
left knee x angles during the stomp (the character stomps his left foot) and data from both hips for 
the rest of the animation. Scaling after matching also improved the results in this case, for example 
when the character slows down and comes to a stop, scal­ing caused the motion of the body and arm motions 
to reduce in coordination with the legs. The method does not directly incorporate hard constraints, so 
we used the following method to maintain the feet contact with the .oor. First the pelvis and upper body 
motions were synthesized. Since altering the pelvis degrees of freedom causes large scale mo­tions of 
the body, inverse kinematic constraints were subsequently applied to keep the feet in place on the .oor. 
This new motion was used for texturing the lower body motion during times the feet were not in contact 
with the .oor. The motion of the characters was much more life-like after en­hancement. The upper body 
moved in a realistic way and responded appropriately to the varying step sizes and the stomp, even though 
these motions were not explicit in the motion capture data. In ad­dition, the style of walking for each 
character clearly came from the data set used for the enhancement. Some example frames are shown in .gure 
10.  Figure 11: Example frames from animations of the otter charac­ter. On the top row are some frames 
from the original keyframed animation, while on the bottom are the corresponding frames after texturing. 
 4.2 Otter Character Although we have focussed on the idea of .lling in missing degrees of freedom by 
synthesis or adding detail by texturing, the method can also be used to alter the style of an existing 
animation that al­ready has a large amount of detail in it. To test this possibility, we used an otter 
character that had been animated by keyframe animation to run. Using the motion capture sets of walking 
described above, we could affect the style of the character s run by texturing the upper body motions, 
using the hip and knee angles as the matching angles. The effect was particularly noticeable when using 
the drag queen walk for texturing, the otter character picked up some of the head bobs and asymmetrical 
arm usage of the motion capture data. Some example frames are shown in .gure 11. 4.3 Modern Dance In 
order to investigate a wider range of motions than those related to walking or running, we turned to 
modern dance. Unlike other forms of dance such as ballet or other classical forms, modern does not have 
a set vocabulary of motions, and yet it uses the whole body at its full range of motion. Thus it provides 
a situation where the correlations between joints will exist only extremely locally in time, and a stringent 
test of our method. A modern dance phrase was animated by sketching only the lower body and root motions 
with keyframes. Motion capture data of several phrases of modern dance was collected, and a total of 
1097 time points (24 frames per second) from 4 phrases was used. The upper body was synthesized, and 
the lower body textured. The same method for maintaining feet contact with the .oor that was described 
above for the walking experiments was used here. The frequency analysis was the same as for the walking, 
except that the 6th highest band was used for matching. A lower frequency band was used because the large 
motions in the dance data set tended to happen over longer times than the steps in walking. The results 
were quite successful here, especially for synthesis of the upper body motions. The motions were full 
and well coor­dinated with the lower body, and looked like something the dancer who performed for the 
motion capture session could have done, but did not actually do. Some example frames are shown in .gure 
12. The best results were obtained by using all of the hip and knee an­gles as the matching angles, but 
some good animations could also be created using fewer degrees of freedom. In these experiments, the 
effects of choosing different paths through the matched data be­came especially noticeable. Because of 
the wide variation within the data, different paths yielded signi.cantly different upper body motions, 
all of which were well coordinated with the lower body.  5 Conclusion and Future Work Presently the 
two main methods by which the motions for computer animations are created are by using keyframes and 
by using motion capture. The method of keyframes is labor intensive, but has the ad­vantage of allowing 
the animator precise control over the actions of the character. Motion capture data has the advantage 
of providing a complete data set with all the detail of live motion, but the animator does not have full 
control over the result. In this work we present a method that combines the advantages of both methods, 
by allowing the animator to control an initial rough animation with keyframes, and then .ll in missing 
degrees of freedom and detail using the in­formation in motion capture data. The results are particularly 
strik­ing for the case of synthesis. One can create an animation of only the lower body, and given some 
motion capture data, automatically create life-like upper body motions that are coordinated with the 
lower body. One drawback of the method as it currently stands is that it does not directly incorporate 
hard constraints. As a result the textur­ing cannot be applied to cases where the feet are meant to remain 
in contact with the .oor, unless it were combined with an inverse kinematic solver in the animation package 
being used. Currently we are working to remedy this de.ciency. Another active area of research is to 
determine a more funda­mental method for breaking the data into fragments. In this work we used the sign 
change of the derivative of one of the joint an­gles used for matching, because it is simple to detect 
and often rep­resents a change from one movement idea to another. The exact choice of where to break 
the data into fragments is not as important as it may seem. What is important is that both the keyframed 
and real data are broken at analogous locations, which is clearly the case with our method. The method 
could be made more ef.cient by de­tecting more fundamental units of movement that may yield larger fragments. 
However, due to the complexity of human motion, this problem is a challenging one, and an ongoing topic 
of research. On the surface, another drawback of the method may appear to be the need to choose a particular 
frequency band for the matching step. However, the choice is not dif.cult to make, and in fact the re­sults 
are not highly dependent upon the choice. Any low frequency band that provides information about the 
overall motion will pro­vide good results. The resulting animations sometimes vary from one another depending 
upon which frequency band is chosen as slightly different regions of data are matched, but more often 
they are quite similar. If too high a band is chosen, however, the result­ing animation has an uncoordinated 
look to it, as the overall motion is not accurately represented in the matching step. Similarly, another 
drawback of the method may appear to be that the animator must specify which degrees of freedom to use 
as the matching angles. However, if one has spent some time keyframing a character, it is quite easy 
in practice to specify this information. The most simplistic approach is to simply use all of the degrees 
of freedom that the animator has sketched out the motion for with keyframes. In many cases, however, 
fewer degrees of freedom can be speci.ed, and equally good results can be obtained. If the motion has 
less variation, such as walking, the results will still be pleasing if fewer angles are chosen as the 
matching angles. In fact it is fas­cinating how correlated the motions of the human body are. Given only 
the data in two angles, such as the hip and knee x angles of one leg, one can synthesize the rest of 
the body. However, for a motion with more variation in it, such as dancing, it is better to include more 
angles, to ensure good choices during matching. If fewer joints are used for matching in this case, some 
of the result­ing paths may still be good results, but others may appear somewhat uncoordinated with 
the full body. In fact, the goal of this project was not to create a completely automatic method, but 
to give the animator another tool for incor­porating the information in motion capture data into his 
or her cre­ations. Different choices of the matching angles can yield different results and provide the 
animator with different possibilities to use in the .nal animation. Another source of different motions 
comes from examining different paths through the best matches. The ani­mator has the option of looking 
at several possibilities and making an artistic decision which is best. Ultimately we hope that methods 
such as this one will further allow animators to take advantage of the bene.ts of motion capture data 
without sacri.cing the control they are used to having when keyframing. 6 Acknowledgments Special thanks 
to Rearden Steel studios for providing the motion capture data, and to Electronic Arts for providing 
the otter model. References ARIKAN, O., AND FORSYTH, D. A. 2002. Interactive motion generation from 
examples. Proc. SIGGRAPH 2002. BODENHEIMER, B., SHLEYFMAN, A., AND HODGINS, J. 1999. The effects of noise 
on the perception of animated human run­ning. Computer Animation and Simulation 99, Eurographics Animation 
Workshop (September), 53 63. BONET, J. S. D. 1997. Multiresolution sampling procedure for analysis and 
synthesis of texture images. proc. SIGGRAPH 1997, 361 368. BRAND, M., AND HERTZMANN, A. 2000. Style machines. 
proc. SIGGRAPH 2000, 183 192. BRAND, M. 1999. Voice puppetry. Proc. SIGGRAPH 1999, 21 28. BREGLER, C. 
1997. Learning and recognizing human dynamics in video sequences. Proc. CVPR, 569 574. BRUDERLIN, A., 
AND WILLIAMS, L. 1995. Motion signal pro­cessing. proc. SIGGRAPH 1995, 97 104. CHENNEY, S., AND FORSYTH, 
D. A. 2000. Sampling plausible solutions to multi-body constraint problems. proc. SIGGRAPH 2000, 219 
228. CHI, D., COSTA, M., ZHAO, L., AND BADLER, N. 2000. The emote model for effort and shape. proc. SIGGRAPH 
2000, 173 182. GLEICHER, M. 1997. motion editing with spacetime constraints. 1997 Symposium on Interactive 
3D graphics, 139 148. GLEICHER, M. 1998. Retargeting motion to new characters. proc. SIGGRAPH 1998, 33 
42. HEEGER, D. J., AND BERGEN, J. R. 1995. Pyramid-based texture analysis/sysnthesis. proc. SIGGRAPH 
1995, 229 238. HODGINS, J., WOOTEN, W. L., BROGAN, D. C., AND O BRIEN, J. F. 1995. Animating human athletics. 
proc. SIGGRAPH 1995, 229 238. KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. Proc. SIGGRAPH 
2002. LEE, J., AND SHIN, S. Y. 1999. A hierarchical approach to inter­active motion editing for human-like 
.gures. proc. SIGGRAPH 1999, 39 48. LEE, J., AND SHIN, S. Y. 2001. A coordinate-invariant approach to 
multiresolution motion analysis. Graphical Models 63, 2, 87 105. LEE, J., CHAI, J., REITSMA, P. S. A., 
HODGINS, J. K., AND POLLARD, N. S. 2002. Interactive control of avatars animated with human motion data. 
Proc. SIGGRAPH 2002. LI, Y., WANG, T., AND SHUM, H. 2002. Motion texture: A two-level statistical model 
for character motion synthesis. Proc. SIGGRAPH 2002. PERLIN, K., AND GOLDBERG, A. 1996. Improv: a system 
for scripting interactive actors in virtual reality worlds. proc. SIG-GRAPH 1996, 205 216. PERLIN, K. 
1985. An image synthesizer. Computer Graphics 19, 3, 287 296. POPOVIC, Z., AND WITKIN, A. 1999. Physically 
based motion transformation. proc. SIGGRAPH 1999, 159 168. PULLEN, K., AND BREGLER, C. 2000. Animating 
by multi-level sampling. IEEE Computer Animation Conference, 36 42. SCHODL, A., SZELISKI, R., SALESIN, 
D. H., AND ESSA, I. 2000. Video textures. Proc. SIGGRAPH 00, 489 498. UNUMA, M., ANJYO, K., AND TEKEUCHI, 
R. 1995. Fourier principles for emotion-based human .gure animation. proc. SIG-GRAPH 1995, 91 96. WITKIN, 
A., AND KASS, M. 1988. Spacetime constraints. Com­puter Graphics 22, 159 168. WITKIN, A., AND POPOVIC, 
Z. 1995. Motion warping. proc. SIGGRAPH 1995, 105 108.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566609</section_id>
		<sort_key>509</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Lighting and appearance]]></section_title>
		<section_page_from>509</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P338142</person_id>
				<author_profile_id><![CDATA[82158642957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Marschner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566610</article_id>
		<sort_key>509</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Homomorphic factorization of BRDF-based lighting computation]]></title>
		<page_from>509</page_from>
		<page_to>516</page_to>
		<doi_number>10.1145/566570.566610</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566610</url>
		<abstract>
			<par><![CDATA[Several techniques have been developed to approximate Bidirectional Reflectance Distribution Functions (BRDF) with acceptable quality and performance for realtime applications. The recently published <i>Homomorphic Factorization</i> by McCool et al. is a general approximation approach that can be used with various setups and for different quality requirements.In this paper we propose a new technique based on the Homomorphic Factorization. Instead of approximating the BRDF, our technique factorizes the full lighting computation of an isotropic BRDF in a global illumination scenario. With this method materials in complex lighting situations can be simulated with only two textures by using commonly available computation capabilities of current graphics hardware.The new technique can also be considered as a generalized approach to several environment map prefiltering techniques. Existing prefiltering techniques are usually limited to specific BRDFs or require advanced hardware capabilities like 3D texturing. With the factorization only common 2D textures are required.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[reflectance & shading model]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[rendering hardware]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Measurement</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382446</person_id>
				<author_profile_id><![CDATA[81100283918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lutz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Latta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences Wedel, Wedel, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP1200023900</person_id>
				<author_profile_id><![CDATA[81100640932]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kolb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Applied Sciences Wedel, Wedel, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192246</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BANKS, D. 1994. Illumination in diverse codimensions. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 28, ACM, 327-334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311553</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., OLANO, M., AND NEMEC, P. 1999. Reflection space image based rendering. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 33, ACM, 165-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHEN, W.-C., GRZESZCZUK, R., AND BOUGUET, J.-Y. 2001. Light field mapping: Hardware-accelerated visualization of surface light fields. In SIGGRAPH 2001 Course Notes "Acquisition and Visualization of Surface Light Fields".]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COOK, R., AND TORRANCE, K. 1981. A reflectance model for computer graphics. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 15, ACM, 307-316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CORNELL UNIVERSITY, 2002. Program of computer graphics online resource: Reflectance data. http://www.graphics.cornell.edu/online/measurements/reflectance/index. html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., 2002. Online resources. http://www.debevec.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>901892</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A. 1992. Filtering normal maps and creating multiple surfaces. Tech. Rep. TR-92-41, Department of Computer Science, University of British Columbia, Vancouver, BC, Canada.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A. 1992. Normal distribution functions and multiple surfaces. In Proc. Graphics Interface, Workshop on Local Illumination, Morgan Kaufmann, Canadian Human-Computer Communications Society, 45-52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>902023</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A. 1995. Separating reflection functions for linear radiosity. In EUROGRAPHICS Rendering Workshop, Eurographics, 296-305.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FREUND, R. W., AND NACHTIGAL, N. M. 1992. QMR: a quasi-minimal residual method for non-Hermitian linear systems. In Iterative Methods in Linear Algebra, R. Beauwens and P. de Groen, Eds. Elsevier Science Publishers, 151-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285310</ref_obj_id>
				<ref_obj_pid>285305</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W., AND SEIDEL, H.-P. 1998. View-independent environment maps. In Eurographics/SIGGRAPH Workshop on Graphics Hardware, Addison-Wesley, 39-45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W., AND SEIDEL, H.-P. 1999. Realistic, hardware-accelerated shading and lighting. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 33, ACM, 171-178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND McCOOL, M.-D. 1999. Hardware rendering with bidirectional reflectances. Tech. rep., Department of Computer Science, University of Waterloo.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312153</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND MCCOOL, M.-D. 1999. Interactive rendering with arbitrary BRDFs using separable approximations. In EUROGRAPHICS Rendering Workshop, Eurographics, 253-253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732274</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND McCOOL, M. D. 2000. Approximation of glossy reflection with prefiltered environment maps. In Proc. Graphics Interface, Morgan Kaufmann, Canadian Human-Computer Communications Society, 119-126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732274</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., VAZQUEZ, P., HElDRICH, W., AND SEIDEL, H.-P. 2000. A unified approach to prefiltered environment maps. In EUROGRAPHICS Rendering Workshop, Eurographics, 185-196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E. P., AND WILLEMS, Y. D. 1994. Using the modified phong BRDF for physically based rendering. Tech. Rep. CW197, Dept. of Computer Science, Leuven, Belgium.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E., FOO, S.-C., TORRANCE, K., AND GREENBERG, D. 1997. Non-linear approximation of reflectance functions. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 31, ACM, 117-126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND HANRAHAN, P. 1996. Lightfield rendering. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 30, ACM, 31-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LINDHOLM, E., KILGARD, M., AND MORETON, H. 2001. A user-programmable vertex engine. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 35, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383276</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[McCOOL, M.-D., ANG, J., AND AHMAD, A. 2001. Homomorphic factorization of BRDFs for high-performance rendering. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 35, ACM, 171-178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MILLER, G., AND HOFFMAN, R. 1984. Illumination and reflection maps: Simulated objects in simulated and real environments. In SIGGRAPH 1984 Course Notes "Advanced Computer Graphics Animation".]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PHONG, B. T. 1975. Illumination for computer generated pictures. Communications of the ACM 18, 6, 311-317.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SGI, 2002. OpenGL extension registry. http://oss.sgi.com/projects/ogl-sample/registry.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Homomorphic Factorization of BRDF-based Lighting Computation Lutz Latta and Andreas Kolb Faculty of 
Media-Information Science University of Applied Sciences Wedel, Wedel, Germany Abstract Several techniques 
have been developed to approximate Bidirec­tional Re.ectance Distribution Functions (BRDF) with acceptable 
quality and performance for realtime applications. The recently published Homomorphic Factorization by 
McCool et al. is a gen­eral approximation approach that can be used with various setups and for different 
quality requirements. In this paper we propose a new technique based on the Homo­morphic Factorization. 
Instead of approximating the BRDF, our technique factorizes the full lighting computation of an isotropic 
BRDF in a global illumination scenario. With this method materi­als in complex lighting situations can 
be simulated with only two textures by using commonly available computation capabilities of current graphics 
hardware. The new technique can also be considered as a generalized ap­proach to several environment 
map pre.ltering techniques. Exist­ing pre.ltering techniques are usually limited to speci.c BRDFs or 
require advanced hardware capabilities like 3D texturing. With the factorization only common 2D textures 
are required. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, 
Shading, Shadowing, and Texture I.3.3 [Computer Graphics]: Picture/Image Generation Keywords: Illumination, 
Re.ectance &#38; Shading Model, Render­ing, Rendering Hardware, Texture Mapping  1 Introduction Several 
techniques for integrating more realistic material and light­ing models in realtime rendering based on 
Bidirectional Re.ectance Distribution Functions (BRDF) have been developed in recent years. These approaches 
build upon the BRDF lighting function  Lo(. o)=BRDF. i,. oLi(. i)cos(.i) d. i (1) O which describes 
the relation between incoming light Li from direc­tion . i =(.i,fi) over the hemisphere O and the outgoing 
light Lo in direction . o. The symbol . represents the elevation angle, whereas f stands for the angle 
in the (local) tangent plane. According to Lambert s law Li(. i)cos(.i)d. i describes the amount of light 
per Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 unit area arriving at the point on the surface of the illuminated ob­ject. Note that . o is taken 
from the hemisphere O since for viewing directions below the horizon no re.ection occurs. The computation 
required for Equation (1) is quite complex and can not be implemented ef.ciently during realtime rendering. 
A major research effort focuses on the approximation of the high-dimensional BRDF using several lower 
dimensional textures (see e.g. [Kautz and McCool 1999a; McCool et al. 2001]). These techniques assume 
a relatively simple lighting environment consist­ing of a few point light sources, reducing Equation 
(1) to a discrete sum. Usually several textures are generated that approximate cer­tain materials for 
a single point light source. Utilizing standard tex­ture mapping techniques, these textures realize a 
lookup table func­tionality for BRDF values. However, once for each light source the reconstruction of 
the BRDF and mapping of the resulting textures is required. Thus these approaches are not suitable for 
complex lighting environments. Other publications, e.g. [Heidrich and Seidel 1999; Kautz and McCool 2000], 
discuss the integration of more complex lighting environments. They use a simpli.ed version of the BRDF 
itself in order to be able to pre-compute environment maps that are used to approximate an object s re.ectance 
in realtime. The computation of these environment maps usually incorporates .ltering applied to the initial 
environment lighting data. The .lter actually replaces the complex and general BRDF description of an 
object s re.ectance behavior, thereby restricting the BRDF to special function classes. This paper introduces 
a technique that combines both major re­search directions stated above. Our approach uses the BRDF sep­aration 
introduced by [McCool et al. 2001], but in contrast to their original simpli.cation of Equation (1) we 
separate the complete lighting function. The major difference to [McCool et al. 2001] is the use of global 
parameterizations instead of local ones. We in­troduce several possible global parameterizations to be 
used in the approximation of the lighting function. Our technique allows the integration of arbitrary 
BRDFs in arbitrary lighting environments in realtime applications using standard textures mapping. The 
new technique is currently restricted to isotropic BRDFs and static lighting environments. Compared to 
pre.ltering tech­niques our method does involve more computational effort in the pre-processing phase. 
In section 2 we give an overview of existing works on BRDF approximation and pre.ltering techniques. 
Section 3 describes our approach in detail. First we summarize McCool et al. s Homomor­phic Factorization 
approach which we use as approximation tech­nique. Then we give a detailed description of the new technique 
to approximate the lighting function (1), including aspects of pa­rameterizations, sampling the lighting 
function and rendering. Our results are presented in section 4, and section 5 states several future research 
directions in this context. 2 Prior Work In this section we give a brief overview of the various approaches 
to make the BRDF-based lighting Equation (1) accessible to realtime rendering. These techniques either 
restrict the lighting environment and focus on the BRDF approximation (section 2.1) or use simple classes 
of BRDF functions and focus on the integration of more complex lighting environments using pre.ltering 
techniques (sec­tion 2.2). Additionally in section 2.3 some recent approaches in the area of surface 
light .elds that relate to our results are summarized. 2.1 Texture based BRDF Approximation A texture 
based BRDF approximation technique applicable in re­altime rendering was developed by [Heidrich and Seidel 
1999] to pre-compute several analytical BRDFs into textures. Based on this work, [Kautz and McCool 1999a] 
propose a gen­eralized approach for the approximation of arbitrary BRDFs. They expand a technique introduced 
by [Fournier 1995] to approximate the BRDF with a sum of products of two 2D functions using Singu­lar 
Value Decomposition (SVD). The function values are stored in textures tj,1,tj,2 yielding the following 
approximation: J BRDF(. i,. o) . tj,1 p1(. i,. o) tj,2 p2(. i,. o) (2) j=1 where p1,p2 appropriately 
map the four-dimensional parameter space onto a two-dimensional one. For J = 1 [Kautz and McCool 1999a] 
propose the accelerated Normalized Decomposition tech­nique. The restriction to pairs of factors with 
the same parameters is removed by the Homomorphic Factorization algorithm introduced by [McCool et al. 
2001]. They approximate the BRDF by the prod­uct of J two-dimensional functions tj, each pre-computed 
into a texture. Their general parameterization can be stated as J BRDF(. i,. o) . tjp j(. i,. o)(3) 
j=1 The approximation is computed by taking the logarithm on both sides of the equation, and then solving 
the resulting linear problem with an iterative solver. 2.2 Pre.ltering Techniques for the approximation 
of complex lighting effects are often based on environment maps. Generally, these methods gen­erate a 
2-dimensional .lter kernel from a BRDF. In many cases the BRDF is isotropic and radially symmetric w.r.t. 
the re.ected view­ing direction. The application of the .lter to an environment map approximates the 
lighting computation with the illumination based on the original environment map. For an overview of 
the most com­mon pre.ltering techniques see [Kautz et al. 2000]. [Heidrich and Seidel 1999] build upon 
the well known Phong il­lumination model [Phong 1975]. They use Phong lobes to approx­imate isotropic 
BRDFs. Phong lobes are radially symmetric w.r.t. the re.ected viewing direction and do not depend upon 
the view elevation. This technique stores the diffuse and specular terms in separate textures. Additionally 
a Fresnel term is included at run­time. [Kautz and McCool 2000] developed a pre.ltering technique that 
uses arbitrary shaped, radially symmetric lobes. Using a single lobe approximation, 3D textures are needed 
to store the approximated lighting functions yielding glossy environment maps. For simple BRDFs it is 
also possible to use a 2D texture. Furthermore an ap­proximation technique based on several lobes is 
introduced, where each lobe has to be rendered separately. [Cabral et al. 1999] propose a technique using 
a set of pre.ltered environment maps. Each of the maps is computed for a .xed point of view. For an arbitrary 
viewpoint the precomputed environment maps of the three closest points of view are warped and blended 
to compute an approximation of the environment map used for render­ing. [Kautz et al. 2000] introduced 
a uni.ed approach to the pre.lter­ing techniques proposed by [Miller and Hoffman 1984], [Heidrich and 
Seidel 1999], [Kautz and McCool 2000] and [Cabral et al. 1999]. Based on this generalized point of view 
an approximation of the anisotropic BRDF by [Banks 1994] is given. Additionally [Kautz et al. 2000] introduce 
a hardware accelerated approach to realize the different pre.ltering processes. 2.3 Surface Light Fields 
In contrast to a BRDF, a surface light .eld f (r,s,.,f) describes the radiance of a surface point with 
parameters (r,s) in viewing direction . =(.,f). One major task in the context of light .elds is rendering 
an object, for which f is known, in realtime (e.g. see [Levoy and Hanrahan 1996]). [Chen et al. 2001] 
propose a texture based approximation which is very similar to the BRDF approximation technique introduced 
by [Kautz and McCool 1999a]. They also use a SVD to approximate the light .eld data yielding J f (r,s,.,f) 
 . tj,1(r,s)tj,2(.,f) (4) j=1 which is, in spite of the parameterization, the same as Equation (2). Our 
approach has something in common with Chen et al. s pro­cedure since they also apply a BRDF approximation 
technique to a different lighting problem.  3 Lighting Function Factorization In this section we describe 
in detail our technique to approximate the lighting function (LF). We .rst discuss the Homomorphic Fac­torization 
(HF) technique introduced by [McCool et al. 2001] and describe the modi.cations needed to approximate 
the complete lighting function. Further sections focus on parameterization as­pects (section 3.2), the 
sampling of the lighting function (section 3.3) and rendering details (section 3.4). The main goal of 
our approach is the factorization of the com­plete lighting function Lo(v ,n , t)= (5) BRDF . ( l,n , 
t),. (v ,n ,t ) Li( l) n · l d l O where Li( l) describes the amount of incoming light from direction 
l. The two vectors n and t describe the normal and a tangent of the surface. The function . (a ,n ,t 
) computes the spherical coordinates for a vector a relative to the coordinate frame de.ned by {n ,t,n 
× t}. All other vectors are given in world space. Note that in contrast to the representation of the 
BRDF in Equa­tion (1), we use world coordinates in Equation (5). Thus we have to assume that all vectors 
lie on the whole sphere not only the hemi­sphere O. The integration is again on the hemisphere since 
n · l = 0 is a meaningful assumption. Applying the Homomorphic Factorization algorithm with the lighting 
function for isotropic BRDFs, simpli.es Equation (5) since isotropic BRDFs do not depend on the tangent 
t. Replacing t by a function t(n ), which computes a valid tangent for a given normal yields Lo(v ,n 
)= (6) BRDF . ( l,n , t(n )),. (v ,n , t(n )) Li( l) n · l d l O which depends only on two directional 
vectors: the viewing direc­tion and the surface normal. Both can be expressed in spherical coordinates 
with two angles. Therefore the function has four de­grees of freedom, just like a position-invariant 
BRDF. We compute the tangent vector as x - (x · n ) n t(n )= Ix - (x · n ) n I where x denotes the horizontal 
direction in viewing coordinates. This leads to singularities as n x , placing them in visually unim­portant 
regions. 3.1 Approximation The approximation of the lighting function is based on the Ho­momorphic Factorization 
approach introduced by [McCool et al. 2001]. Analogous to the HF approximation in Equation (3) we ap­proximate 
the lighting function in Equation (6) in world coordinates using normal and re.ection vectors. We derive 
the general parame­terization for isotropic materials as J Lo(v ,n ) . tj p j(v , n ) j=1 For now, 
we use the following effective and simple parameteri­zation Lo(v ,n ) t1(n )t2(r ) (7) v which is based 
on the normal and the viewing direction re.ected at the normal r = 2 (n · v )n -v (see also section 3.2 
for other param­ v eterizations). Both functions, t1 and t2, can be pre-computed into textures using 
a mapping of a direction vector onto texture coordinates. Several such texture mappings exist in environment 
mapping tech­niques, e.g. spherical, parabolic or cubic maps. The HF algorithm works by taking the logarithm 
of both sides of Equation (7) yielding ¯ Lo(v ,n ) t¯1(n )+t¯2(r ) v Here log(a) is written as ¯a (for 
more detailed description see [Mc-Cool et al. 2001]). Now the approximation can be expressed as a linear 
equation system. In general a number of LF samples are used to .ll the constant vector of the equation 
system, and all the texels of the two approximation textures are unpacked into the solution vector. The 
coef.cient matrix of the equation system now maps each LF sample (one per row) to the appropriate texels. 
To achieve sub-pixel precision, the exact texture coordinates derived from the directions of the current 
sample are used to compute bilinear weights for the four surrounding texel positions. In short the whole 
equation system can be written as t¯1 ¯ Lo = A1A2 t¯2 This system is probably under-constrained. On 
the one hand some texels might not be mapped according to the texture mapping technique, e.g. texels 
outside the inner circle in case of parabolic maps [Heidrich and Seidel 1998]. On the other hand depending 
on the sampling technique some texels representing valid directions might not be constrained by any LF 
sample. To insure that all texels are constrained a Laplace operator is introduced which adds addi­tional 
dependencies to the equation system. ... . ¯A1 A2 Lo t¯1 . 0 . = ..L10 .(8)t¯2 0 0 .L2 where the scalar 
. controls the in.uence of the Laplacian. Note that the Laplace operator will smooth the resulting texture, 
thus possibly increases the overall approximation error. We use the Quasi-Minimal Residual (QMR) algorithm 
[Freund and Nachtigal 1992] to iteratively solve the equation system. The coef.cient matrix might be 
quite large. Since there are only a few texels mapped in each row of the matrix, it is quite sparse and 
can either be stored in a compressed format or computed on the .y dur­ing the solving process. The computation 
time of iterative solvers heavily depend on the starting vector. The initial vector is taken as the average 
of all data implied by the LF samples to each texel. In cases where a texel has no constraints, a weighted 
sum of LF samples closest to the consid­ered texel is computed, using the inverse distances as weights. 
We further discuss the choice of sampling directions in section 3.3. Beside the use of a different (global) 
parameterization, the need to span a whole sphere instead of a hemisphere also distinguishes our approach 
from the original HF method. [McCool et al. 2001] use parabolic maps (see [Heidrich and Seidel 1998]) 
during the approximation. The whole sphere could be described using two parabolic maps. But since nearly 
22% of the texture area remain unused, we prefer cube maps to describe the LF function. Another advantage 
of cube maps lies in the application of the Laplacian. [McCool et al. 2001] state, that the Laplacian 
should ideally be applied to the BRDF, i.e. to the lighting function in our situation. Since this is 
relatively expensive, the Laplacian is applied to the texture. Adopted to our situation, the Laplacian 
would have to be applied to both parabolic maps independently. This, however, may lead to discontinuities 
for directions (. i or . o) close to the horizon, i.e. directions that are represented by texels close 
to the in­ner circles of the parabolic maps. For cube maps the neighborhood of texels for adjacent spacial 
directions can easily be established (see Figure 1). Figure 1: The Laplace kernel applied to several 
cube map texels, each representing a different spacial direction. 3.2 Other Parameterizations The above 
normal-re.ection parameterization (NR) with two tex­tures, one dependent on the normal and the other 
dependent on the re.ection vector, is only one of several possible parameterizations. The use of both 
these vectors is motivated by the properties of the lighting function with simple BRDFs. In general, 
the texture de­pendence on the normal vector tends to approximate the amount of light resulting from 
the diffuse part of the BRDF, and the re.ection vector texture will contain the specular highlights. 
Using the Phong BRDF or BRDFs with a rotational symme­try around the re.ection vector (as in the pre.ltering 
technique by [Kautz and McCool 2000]) the parameterization can achieve opti­mal results. Other more complex 
isotropic BRDFs will be approxi­mated as well as possible during the solver run. In addition to the two 
textures above a third one can be used that is dependent on the viewer direction. This normal-re.ection­view 
parameterization (NRV) results in slightly less statistical error, but does not improve signi.cantly 
the visual appearance with most BRDFs. Alternatively two additional textures can be used that work simi­larly 
to the Gram-Schmidt halfangle-difference (GSHD) parameter­ization of the BRDF separation introduced by 
[Kautz and McCool 1999a]. The halfangle vector h is usually computed between the viewing direction and 
the light direction. In the global illumina­tion scenario a dominant light direction ldom is used instead 
of the direction to a single light source. The resulting halfangle vector h is transformed to the local 
surface coordinate frame {n ,t (n ),b}, where b = n × t (n ) is the orthogonal complement. .. h I· t(n 
)v h I = ldom + h = . h I· b . v ldom + h I· n In our global lighting approach the difference vector 
d resembles the dominant light direction in the local surface frame: . n) . t( ldom · . . d = b ldom 
· n ldom · This normal-re.ection-halfangle-difference parameterization (NRHD) is most useful if light 
comes predominantly from one general direction (e.g. from the sun). In the extreme case of having light 
coming only from one direction, the quality of the approximation is similar to a BRDF factorization with 
the GSHD parameterization. In other situations the light from other directions than the dominant one 
will be approximated as well as possible. 3.3 Sampling the Lighting Function Sampling of the lighting 
function involves two major issues: which samples to consider in the factorization and how to compute 
these samples based on incoming light information Li( l) (see Equation (6)). When using measured BRDFs, 
only samples that represent di­rections for which the BRDF is known can be used. To solve this problem, 
unknown BRDF values have to be interpolated from nearby samples. Because interpolation on a four-dimensional 
func­tion is costly, we simply use McCool s original Homomorphic Fac­torization technique to approximate 
the BRDF and use the recon­structed BRDF in the LF integral. This reconstruction can be done with .oating 
point precision and does not need to be quantized for storage in textures as in the case of hardware 
accelerated BRDF rendering. The computation of a LF sample requires the evaluation of the in­tegral in 
Equation (6). In our case the incoming light function Li( l) is discretely and regularly de.ned as a 
cubic environment map. A simple variant of the integration might simply sum up the dis­crete lighting 
information weighted with the related BRDF value. Actually, this is incorrect, since a texel covers a 
non-constant area on the environmental sphere (see Figure 2). environmental sphere z y x texture normal 
ntex projected pixel pixel position vector p pixel center .y .x Figure 2: Setup for computing the texel 
weights for the lighting integration for the environment map at z = -1. A better approach takes the relevance 
of an environmental texel into account, since boundary texels are less important than texels in the environment 
map s center. The projected texel area w.r.t the area of the unit sphere should be used as additional 
weight. This yields the following sampling equation (the BRDF arguments have been simpli.ed): .BRDF(v 
, l)Li(pe( l))g(pe( l)) n · l l Lo(v ,n )= (9) .g(pe( l)) l where pe maps the directional vector l to 
cube map texture coordi­nates and g is the projected texel area. To derive the weight g we assume for 
simpli.cation, that the textures of the cubic environment map are placed at x = ±1,y = ±1 and z = ±1 
and that they are parameterized over [-1, 1]2 (see Figure 2). Considering the environment map located 
at z = -1 and the differential texel with center (x0,y0), the projected texel area on the environmental 
sphere with radius 1 is given by -p dxdy dxdy Ip I· r2 =(x2 + y2 + 1)3/2 ntex 00 where p =(x0,y0,-1) 
is the position vector of the texel center, n tex =(0,0, 1) is the environment map s normal and r is 
the distance of the texel center to the origin, i.e. r = Ip I. Applying this result to a texel with extension 
.x and .y in x- and y- direction respectively, we get .x .y y0+ g(x0, y0)= dxdy (10) x0+ 2 21 .x .y x0- 
y0- (x2 + y2 + 1)3/2 22 Since the analytical solution to Equation (10) involves the evalua­tion of several 
trigonometric functions we work with the following approximation without signi.cant impact on the results: 
 -p .x.y .x.y g(x0, y0) · n tex= (11) Ip I r2 (x2 + y2 + 1)3/2 00 Figure 3 shows the function in Equation 
(11) for weighting the texels. Figure 3: The weight function used for integrating the lighting sam­ples 
for cubic maps. Due to extreme BRDF values or light environment values some LF samples might get very 
large. To avoid precision problems in the factorization step we clamp the LF sample values. Theoretically 
we could clamp to [0, 1] since, in contrary to BRDF approximation, damping through the n · l factor does 
not occur. However, we found that allowing slightly greater values up to 4 leads to better approxi­mation 
results. The selection of LF samples used in the factorization should map as regularly as possible to 
the texels in the resulting textures used to approximate the lighting equation (6). To achieve this, 
the in­verse mappings p-j 1 , j = 1,2 are applied to all texels. The related directions are then used 
to compute the LF samples, if the direc­tional combinations is valid. Actually, we use v instead of r 
which v makes no difference for the regularity of the resulting LF samples w.r.t. the textures. 3.4 
Rendering The reconstruction of the factorization result in current graphics hardware raises several 
problematic issues. While the factorization can be done with high precision .oating point values, the 
resulting textures have to be converted to low precision (e.g. 8 bit) integer representation. In what 
follows, we will regard the hardware texture processing pipeline as a .xed-point arithmetic unit working 
with values in the range [0,1]. The factorization textures need to retain as much precision as possible 
during the LF reconstruction. A .rst step to achieve this goal is the normalization of the textures to 
the maximum value in each texture. To correct for the normalization a constant correc­tion color is multiplied 
with the textures during the rendering. This correction color can be included in the computation as constant 
per­vertex color. If the correction value appears to be greater than one, it can not be represented by 
a color. To reach an optimal result in this case, the correction value should be split into two factors. 
One below one, that is used as per-vertex color, and another constant value that is multiplied as late 
as possible in the texture pipeline, in order to avoid premature clamping. Most current graphics hard­ware 
allows this post-scaling with constant values of 1, 2 or 4 (e.g. OpenGL extension GL_ARB_texture_env_combine, 
also part of OpenGL 1.3, see [SGI 2002]). Recent programmable hardware allows even higher powers of two, 
though 4 is usually an acceptable compromise as higher values lead to a loss of precision in the lower 
bit ranges. Sometimes these scaling factors are not suf.cient to restore the original intensity range 
of the map. In such cases the textures need to be pre-scaled before the rendering, possibly immediately 
after the factorization. The pre-scaling needs to .nd an acceptable com­promise between avoiding contouring 
artifacts in lower intensity areas and risking clamping of higher intensities during the texture pipeline 
computations. Due to our experience this compromise has to be found for each factorization result individually. 
Currently we use an interactive adjustment of the scaling factors which allows immediate visual quality 
judgement. Now some notes on the rendering performance of the method. On most current graphics hardware 
the two textures required for the NR parameterization can be applied in a single pass. The ren­dering 
then differs from a standard Phong-lit material only at the vertex processing and the texture application. 
The vertex process­ing needs to compute the two texture coordinate sets instead of the diffuse and specular 
Phong computation. For the texture coordi­nate computation both normal and viewing direction are required 
in world coordinates. The normal is usually needed for Phong light­ing in eye coordinates. This transformation 
can now be replaced by the world coordinates computation. The viewing direction has to be transformed 
and then used for the computation of the re.ection vector. Using a fully programmable vertex processor 
(e.g. with ver­tex programs/shaders as described by [Lindholm et al. 2001]) the required number of instructions 
per vertex is nearly equal to Phong lighting, especially when using cube maps, because the computed vectors 
can be used immediately as texture coordinates. If no pro­grammable vertex processor is available, one 
can apply appropriate texture coordinate generation functionality (e.g. in OpenGL with GL_NORMAL_MAP 
and GL_REFLECTION_MAP) in combination with texture matrix transformations to map the directional vectors 
in world coordinates. The application of two textures might be the performance bottle necks in most situations 
with limited pixel processing and memory transfer resources for each object in a scene. Since the textures 
are often quite small or can be used at a low mip-map level due to their smoothness, memory transfers 
should be cacheable in most texture caches. Therefore rendering the objects sorted by texture should 
have an even higher priority than usual. Note that standard tech­niques to generate mip-map levels might 
lead to incorrect recon­struction results, as the .ltering can cause the loss of highlights and non-linear 
properties. This can be avoided by using mip-map tex­tures generated during a multi-resolution factorization 
as described in the next section. However even a theoretical incorrect mip-map .ltering should give a 
reasonable approximation and reduces alias­ing artifacts signi.cantly.  4 Results Our algorithm has 
been tested with several isotropic BRDFs and light environments. The analytical Cook-Torrance [Cook and 
Tor­rance 1981] and Modi.ed Phong [Lafortune and Willems 1994] BRDF have been used as well as a reconstruction 
of the measured BRDF Cayman , a highly specular car lacquer [Cornell Univer­sity 2002]. Among the used 
light environments is the synthetical environment map Loch by Jeff Heath (also used in [Kautz et al. 
2000]) and high-dynamic range light probe measurements in St. Pe­ter s Basilica, Rome and the Uf.zi Gallery, 
Florence by [Debevec 2002]. The computational effort for the approximation falls into two categories: 
Sampling the lighting function, i.e. computing the in­tegral over the incoming light for each sample, 
and performing the factorization. The LF sample locations have been determined by reverse­projection 
of the texels (see section 3.3). Having a cube map with 64 texels per edge, leads to about 1 (64×64×6)2 
texels (the squar­ 2 ing is due to two textures and the 1/2 is due to invalid normal-view combinations), 
which is just over 300 million samples. This is gen­erally above current workstation computation and 
memory capabil­ities. To reduce the number of samples we use a virtual texture size of 12 or 16 steps 
per edge, resulting in several hundred thousand to a millon samples. This texture size is only used during 
sampling, not for the real textures during the factorization itself, leading to regu­larly but not densely 
mapped texels. Problems that are due to this lower sampling density can be resolved, in most cases, by 
using a multi-resolution algorithm, starting a factorization with a cube map edge size of 16. Then the 
result of this factorization is interpolated to a map of twice the size and used as starting solution 
for another factorization run, again with the same samples as in the .rst run. This can be repeated until 
the desired map size (e.g. 64) is reached. The environment map representing the incoming light function 
is usually scaled to a resolution between 202 and 642 per cube face. This leads to several thousand light 
samples. For each LF sample only about half of the light samples need to be considered due to invalid 
combinations of the normal and light direction. For each light value an appropriate BRDF value needs 
to be evaluated, re­sulting in a total of several hundred of millions to a billion BRDF samples taken 
during the LF sampling. This usually consumes the most computational effort of the approximation process. 
On a cur­rent AMD Athlon 1.4 GHz workstation this takes between a quarter and a full hour in our research 
implementation. The effort of the factorization itself depends on the size of the coef.cient matrix and 
the convergence of the whole equation sys­tem. While the coef.cient matrix depends only on the number 
of LF samples and the texture size, the convergence behavior cannot be predicted easily and varies greatly 
for each BRDF and light en­vironment setup. On the aforementioned system the factorization usually takes 
between 10 and 30 minutes. Note that all values given here apply to high-quality approximations, for 
previewing purposes considerably smaller map sizes and sample numbers need to be taken, thereby immediately 
accelerating the process. 4.1 Examples Here are some examples of our test cases. Figure 4 shows the fac­torization 
of the LF with the Cook-Torrance BRDF (set up to simu­late copper) in a lighting environment de.ned by 
the high dynamic range environment map of the Uf.zi Gallery, shown in the top-left corner as a unfolded 
cube map. On the right hand side, the two resulting textures are shown. The top one is parameterized 
by the normal vector, while the middle one is parameterized by the re­.ection vector. Below them the 
normalization correction color is displayed (see section 3.4). A rendered teapot with applied textures 
is shown in the bottom-left corner. To be able to judge the quality of the approximation the exact lighting 
computation with the copper BRDF has been carried out for each vertex on a very .ne tessellated model 
(almost per-pixel computation) in Figure 5. It can be seen that the approximation cannot capture all 
properties of the BRDF completely and the typi­cal Cook-Torrance BRDF color shift at the edge of the 
highlight is slightly less intense in Figure 4. To test the factorization results with a highly specular, 
almost mirror-like BRDF, the Modi.ed Phong BRDF has been used with a specular exponent of 400. The incoming 
light is sampled from the environment map of St. Peter s Basilica. Figure 6 shows the results with the 
same layout as above. The previous examples use real, measured light environments as incoming light function. 
In order to use standard images with only a limited dynamic range, it is advisable to enhance highlights 
while sampling the environment map. In Figure 7 the factorization of a Cook-Torrance BRDF (with gold 
setup) has been conducted with the arti.cial, rendered Loch environment map. At the direction of  Figure 
5: Teapot with exact lighting computation the sun and its re.ection in the sea, the map has been made 
brighter by several powers of ten to simulate the much higher intensity of light arriving from these 
directions. Finally Figure 8 shows an example using a measured BRDF, Cayman car lacquer. The light environment 
is de.ned by a high dynamic range light measurement of a forest scene.  4.2 Error Measurements Several 
error measurements have been taken for the above shown factorizations. The error measurements in Table 
1 are abbreviated as follows: Average (Avg) and maximum (Max) absolute (Abs) er­ror and relative (Rel) 
error, all between the exact measurements and the reconstruction from double precision .oating point 
values. RMS I is the root mean squares error of the same difference, while RMS II is the error of the 
reconstruction from 8-bit integer textures. All values are the arithmetic averages over the RGB components. 
The errors have been measured with the same samples that have been used for the factorization.  Figure 
6: Highly specular Modi.ed Phong material with St. Peters Basilica environment map Material Environ. 
Param. Copper Uf.zi NR Gold Loch NR Gold Loch NRV Phong St.Pet. NR Cayman Forest NR Avg Abs Max Abs Avg 
Rel Max Rel RMS I RMS II 0.1298 0.2246 3.3554 36.9487 0.3073 0.9881 0.0726 0.1015 3.0664 3.9212 0.1887 
0.3290 0.0724 0.0982 3.0965 3.8807 0.1892 0.3319 0.1626 0.3518 3.9328 19.2405 0.2895 0.4774 0.0460 1.0804 
0.3120 35.8461 0.0635 0.1022 Table 1: Error measurements Besides the four examples shown in section 
4.1 also values for the gold BRDF with the NRV parameterization are given. The vi­sual appearance does 
not differ from the NR parameterization and the error measurements show only small differences as well. 
Sur­prisingly the RMS error is even slightly higher, which might be caused by the higher in.uence of 
the Laplacian in the equation sys­tem due to three textures rather than two. Some of the error measurements, 
especially the maximum rela­tive error are quite high. This has also been observed by [McCool et al. 
2001] with the BRDF factorization and can be attributed to large peaks in the data. The measured light 
environments with high­dynamic range lead to very high intensity differences in the lighting function 
after the computation with a highly specular BRDF. The synthetical environment map with less dynamic-range 
has consid­erably lower maximum error values. For quality judgements the original BRDF factorization 
tech­nique has been compared to a LF factorization with a single-light source. Using a Cook-Torrance 
BRDF the BRDF factorization has a RMS error of 0.0435, the LF with normal-re.ection parameters 0.1110, 
the LF with the additional halfangle-difference parameters 0.0781. Due to the different application of 
the techniques this com­parison has only limited signi.cance. Of course, we would like to compare our 
technique with other environment map pre.ltering techniques, especially [Kautz and McCool 2000]. Unfortunately, 
no comparable data sets are cur­rently available. Figure 7: Gold material with Loch environment map 
 5 Future Work Our technique offers a uni.ed approach to the simulation of com­plex lighting and material. 
Since we separate the complete lighting, the incoming light situation needs to be static to a certain 
extent in order to be visually plausible. A rather simple improvement is the use of additional dynamic 
light sources based on the standard Phong model. Alternatively dynamic light sources with the original 
BRDF factorization might be added during rendering. As stated above, the pre-computational costs to approximate 
the lighting function is relatively high. Both, computing the LF sam­ples and the .nal factorization, 
e.g. solving the linear system, con­tribute to this costs. One could attempt to solve the linear system 
once by .nding a pseudo inverse matrix to the one in Equation (8). Changing the environment or the BRDF 
would still need the com­putation of the lighting function, but the factorization would be very cheap. 
However, such a global solution will not give as good ap­proximations as the ones computed for each individual 
BRDF and incoming light function. Trying to reduce the effort needed to sample the lighting func­tion 
using Monte Carlo-like approaches might help to decrease the computing time for the LF sampling. Another 
principle direction of future research is the introduction of anisotropic materials. This would involve 
the factorization of the lighting equation (5), forcing us to add another texture. The major problem 
lies in the general use of world coordinates in the lighting computation. Thus a technique has to be 
found to map the local tangent vector to world coordinates. Acknowledgments We found it very helpful 
to have the chance to discuss our research results with Jan Kautz and his colleagues of the Computer 
Graph­ics Group at the Max-Planck Institut für Informatik in Saarbrücken, Germany and the Computer Graphics 
Group at the Saarland Univer­sity. Also we would like to thank Michael McCool for his sugges­tions on 
further optimization and improvements of our approach. Figure 8: Measured Cayman BRDF with forest environment 
map References BANKS, D. 1994. Illumination in diverse codimensions. In ACM Proceedings SIGGRAPH, Addison 
Wesley, vol. 28, ACM, 327 334. CABRAL, B., OLANO, M., AND NEMEC, P. 1999. Re.ection space image based 
rendering. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 33, ACM, 165 170. CHEN, W.-C., GRZESZCZUK, 
R., AND BOUGUET, J.-Y. 2001. Light .eld mapping: Hardware-accelerated visualization of sur­face light 
.elds. In SIGGRAPH 2001 Course Notes "Acquisition and Visualization of Surface Light Fields". COOK, R., 
AND TORRANCE, K. 1981. A re.ectance model for computer graphics. In ACM Proceedings SIGGRAPH, Addison 
Wesley, vol. 15, ACM, 307 316. CORNELL UNIVERSITY, 2002. Program of com­puter graphics online resource: 
Re.ectance data. http://www.graphics.cornell.edu/online/ measurements/reflectance/index.html. DEBEVEC, 
P., 2002. Online resources. http://www.debevec.org. FOURNIER, A. 1992. Filtering normal maps and creating 
multiple surfaces. Tech. Rep. TR-92-41, Department of Computer Sci­ence, University of British Columbia, 
Vancouver, BC, Canada. FOURNIER, A. 1992. Normal distribution functions and multiple surfaces. In Proc. 
Graphics Interface, Workshop on Local Illumi­nation, Morgan Kaufmann, Canadian Human-Computer Com­munications 
Society, 45 52. FOURNIER, A. 1995. Separating re.ection functions for linear ra­diosity. In EUROGRAPHICS 
Rendering Workshop, Eurograph­ics, 296 305. FREUND, R. W., AND NACHTIGAL, N. M. 1992. QMR: a quasi-minimal 
residual method for non-Hermitian linear sys­tems. In Iterative Methods in Linear Algebra, R. Beauwens 
and P. de Groen, Eds. Elsevier Science Publishers, 151 154. HEIDRICH, W., AND SEIDEL, H.-P. 1998. View-independent 
environment maps. In Eurographics/SIGGRAPH Workshop on Graphics Hardware, Addison-Wesley, 39 45. HEIDRICH, 
W., AND SEIDEL, H.-P. 1999. Realistic, hardware­accelerated shading and lighting. In ACM Proceedings 
SIG-GRAPH, Addison Wesley, vol. 33, ACM, 171 178. KAUTZ, J., AND MCCOOL,M.-D.1999.Hardwarerenderingwith 
bidirectional re.ectances. Tech. rep., Department of Computer Science, University of Waterloo. KAUTZ, 
J., AND MCCOOL, M.-D. 1999. Interactive rendering with arbitrary BRDFs using separable approximations. 
In EU-ROGRAPHICS Rendering Workshop, Eurographics, 253 253. KAUTZ, J., AND MCCOOL, M. D. 2000. Approximation 
of glossy re.ection with pre.ltered environment maps. In Proc. Graph­ics Interface, Morgan Kaufmann, 
Canadian Human-Computer Communications Society, 119 126. KAUTZ, J., VAZQUEZ, P., HEIDRICH, W., AND SEIDEL, 
H.- P. 2000. A uni.ed approach to pre.ltered environment maps. In EUROGRAPHICS Rendering Workshop, Eurographics, 
185 196. LAFORTUNE, E. P., AND WILLEMS, Y. D. 1994. Using the mod­i.ed phong BRDF for physically based 
rendering. Tech. Rep. CW197, Dept. of Computer Science, Leuven, Belgium. LAFORTUNE, E., FOO, S.-C., TORRANCE, 
K., AND GREEN-BERG, D. 1997. Non-linear approximation of re.ectance functions. In ACM Proceedings SIGGRAPH, 
Addison Wesley, vol. 31, ACM, 117 126. LEVOY, M., AND HANRAHAN, P. 1996. Light.eld rendering. In ACM 
Proceedings SIGGRAPH, Addison Wesley, vol. 30, ACM, 31 42. LINDHOLM, E., KILGARD, M., AND MORETON, H. 
2001. A user-programmable vertex engine. In ACM Proceedings SIG-GRAPH, Addison Wesley, vol. 35, ACM. 
MCCOOL, M.-D., ANG, J., AND AHMAD, A. 2001. Homomor­phic factorization of BRDFs for high-performance 
rendering. In ACM Proceedings SIGGRAPH, Addison Wesley, vol. 35, ACM, 171 178. MILLER, G., AND HOFFMAN,R.1984.Illuminationandre.ection 
maps: Simulated objects in simulated and real environments. In SIGGRAPH 1984 Course Notes "Advanced Computer 
Graphics Animation". PHONG, B. T. 1975. Illumination for computer generated pictures. Communications 
of the ACM 18, 6, 311 317. SGI, 2002. OpenGL extension registry. http://oss.sgi.com/projects/ogl­ sample/registry. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566611</article_id>
		<sort_key>517</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Frequency space environment map rendering]]></title>
		<page_from>517</page_from>
		<page_to>526</page_to>
		<doi_number>10.1145/566570.566611</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566611</url>
		<abstract>
			<par><![CDATA[We present a new method for real-time rendering of objects with complex isotropic BRDFs under distant natural illumination, as specified by an environment map. Our approach is based on spherical frequency space analysis and includes three main contributions. Firstly, we are able to theoretically analyze required sampling rates and resolutions, which have traditionally been determined in an ad-hoc manner. We also introduce a new compact representation, which we call a <i>spherical harmonic reflection map (SHRM),</i> for efficient representation and rendering. Finally, we show how to rapidly prefilter the environment map to compute the <i>SHRM</i>---our frequency domain prefiltering algorithm is generally orders of magnitude faster than previous angular (spatial) domain approaches.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[complexity analysis]]></kw>
			<kw><![CDATA[environment maps]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[signal-processing]]></kw>
			<kw><![CDATA[spherical harmonics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14019965</person_id>
				<author_profile_id><![CDATA[81100019585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ravi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramamoorthi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BASRI, R., AND JACOBS, D. 2001. Lambertian reflectance and linear subspaces. In International Conference on Computer Vision, 383-390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLINN, J., AND NEWELL, M. 1976. Texture and reflection in computer generated images. Communications of the ACM 19, 542-546.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., MAX, N., AND SPRINGMEYER, R. 1987. Bidirectional reflection functions from surface bump maps. In SIGGRAPH 87, 273-281.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311553</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., OLANO, M., AND NEMEC, P. 1999. Reflection space image based rendering. In SIGGRAPH 99, 165-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DANA, K., GINNEKEN, B., NAYAR, S., AND KOENDERINK, J. 1999. Reflectance and texture of real-world surfaces. ACM Transactions on Graphics 18, 1 (January), 1-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GREENE, N. 1986. Environment mapping and other applications of world projections. IEEE Computer Graphics & Applications 6, 11, 21-29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364402</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[HAKURA, Z., SNYDER, J., AND LENGYEL, J. 2001. Parameterized environment maps. In ACM symposium on interactive 3D graphics, 203-208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J., AND KAY, T. 1989. Rendering fur with three dimensional textures. In SIGGRAPH 89, 271-280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312153</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND MCCOOL, M. 1999. Interactive rendering with arbitrary BRDFs using separable approximations. In EGRW 99, 247-260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., AND MCCOOL, M. 2000. Approximation of glossy reflection with pre-filtered environment maps. In Graphics Interface, 119-126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732274</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J., V&#193;ZQUEZ, P., HElDRICH, W., AND SEIDEL, H. 2000. A unified approach to prefiltered environment maps. In EGRW 00, 185-196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J., AND VAN DOORN, A. 1998. Phenomenological description of bidirectional surface reflection. JOSA A 15, 11, 2903-2912.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E., FOO, S., TORRANCE, K., AND GREENBERG, D. 1997. Non-linear approximation of reflectance functions. In SIGGRAPH 97, 117-126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MACROBERT, T. 1948. Spherical harmonics; an elementary treatise on harmonic functions, with applications. Dover Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T., GELB, D., AND WOLTERS, H. 2001. Polynomial texture maps. In SIGGRAPH 01, 519-528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383829</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MARSCHNER, S., WESTIN, S., LAFORTUNE, E., TORRANCE, K., AND GREENBERG, D. 2000. Image-Based BRDF measurement including human skin. In EGRW 00, 139-152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383276</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MCCOOL, M., ANG, J., AND AHMAD, A. 2001. Homomorphic factorization of BRDFs for high-performance rendering. In SIGGRAPH 01, 171-178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MILLER, G., AND HOFFMAN, C. 1984. Illumination and reflection maps: Simulated objects in simulated and real environments. SIGGRAPH 84 Advanced Computer Graphics Animation seminar notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>269080</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MOHLENKAMP, M. 1999. A fast transform for spherical harmonics. The Journal of Fourier Analysis and Applications 5, 2/3, 159-184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[NISHINO, K., SATO, Y., AND IKEUCHI, K. 1999. Eigen-texture method: Appearance compression based on 3D model. In CVPR 99, 618-624.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383275</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PROUDFOOT, K., MARK, W., TZVETKOV, S., AND HANRAHAN, P. 2001. A real-time procedural shading system for programmable graphics hardware. In SIGGRAPH 01, 159-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383317</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND HANRAHAN, P. 2001. An efficient representation for irradiance environment maps. In SIGGRAPH 01,497-500.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND HANRAHAN, P. 2001. On the relationship between radiance and irradiance: Determining the illumination from images of a convex lambertian object. JOSA A 18, 10, 2448-2459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND HANRAHAN, P. 2001. A signal-processing framework for inverse rendering. In SIGGRAPH 01, 117-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[RUSINKIEWICZ, S. 1998. A new change of variables for efficient BRDF representation. In EGRW 98, 11-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SCHR&#214;DER, P., AND SWELDENS, W. 1995. Spherical wavelets: Texture processing. In EGRW 95, 252-263.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122739</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SILLION, F., ARVO, J., WESTIN, S., AND GREENBERG, D. 1991. A global illumination solution for general reflectance distributions. In SIGGRAPH 91, 187-196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[TORRANCE, K., AND SPARROW, E. 1967. Theory for off-specular reflection from roughened surfaces. JOSA 57, 9, 1105-1114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WESTIN, S., ARVO, J., AND TORRANCE, K. 1992. Predicting reflectance functions from complex surfaces. In SIGGRAPH 92, 255-264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344925</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WOOD, D., AZUMA, D., ALDINGER, K., CURLESS, B., DUCHAMP, T., SALESIN, D., AND STUETZLE, W. 2000. Surface light fields for 3D photography. In SIGGRAPH 00, 287-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Frequency Space Environment Map Rendering Ravi Ramamoorthi Pat Hanrahan Stanford University *  Figure 
1: These images, showing many different lighting conditions and BRDFs, were each rendered at approximately 
30 frames per second using our Spherical Harmonic Re.ection Map (SHRM) representation. From left to right, 
a simpli.ed microfacet BRDF, krylon blue (using McCool et al. s reconstruction from measurements at Cornell), 
orange and velvet (CURET database), and an anisotropic BRDF (based on the Kajiya-Kay model). The environment 
maps are the Grace Cathedral, St. Peter s Basilica, the Uf.zi gallery, and a Eucalyptus grove, courtesy 
Paul Debevec. The armadillo model is from Venkat Krishnamurthy. Abstract We present a new method for 
real-time rendering of objects with complex isotropic BRDFs under distant natural illumination, as speci.ed 
by an environment map. Our approach is based on spheri­cal frequency space analysis and includes three 
main contributions. Firstly, we are able to theoretically analyze required sampling rates and resolutions, 
which have traditionally been determined in an ad-hoc manner. We also introduce a new compact representation, 
which we call a spherical harmonic re.ection map (SHRM), for ef.cient representation and rendering. Finally, 
we show how to rapidly pre.lter the environment map to compute the SHRM our frequency domain pre.ltering 
algorithm is generally orders of mag­nitude faster than previous angular (spatial) domain approaches. 
CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Environment Maps Keywords: 
Environment Maps, Image-Based Rendering, Signal-Processing, Complexity Analysis, Spherical Harmonics. 
 1 Introduction Our goals are real-time rendering with complex natural illumina­tion and realistic, possibly 
measured, BRDFs. The closest previous work is that of Cabral et al. [1999], who extended standard environ­ment 
maps by interactively warping and combining a sparse 2D set of prerendered images. These precomputed 
images were obtained by pre.ltering the environment map, i.e. integrating the product of the BRDF and 
lighting over the visible (upper) hemisphere for each image pixel, with each pixel corresponding to a 
particular sur­face normal direction. Subsequently, [Kautz and McCool 2000; Kautz et al. 2000] proposed 
alternative implementations and im­proved pre.ltering methods. This paper introduces a new frequency 
space paradigm for pre­.ltering and rendering environment mapped images with general *(ravir,hanrahan)@graphics.stanford.edu 
 Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 isotropic BRDFs. Our approach is based on recent theoretical re­sults by Basri and Jacobs [2001] 
and Ramamoorthi and Hanra­han [2001b; 2001c], wherein they formalize the notion of re.ection as a spherical 
convolution of the illumination and BRDF. We show that frequency space analysis allows for setting sampling 
rates ac­curately, and enables compact representations. Further, just as im­age convolutions are often 
computed in the Fourier rather than the spatial domain, pre.ltering is more ef.cient in frequency rather 
than angular space. Our main contributions are: Theoretical analysis of sampling rates and resolutions: 
Most previous work has determined re.ection map resolutions, or the number of re.ection maps required, 
in an ad-hoc manner. By using a signal-processing framework, we are able to perform error analy­sis, 
that allows us to set sampling rates and resolutions accurately. Ef.cient representation and rendering 
with Spherical Har­monic Re.ection Maps: We introduce spherical harmonic re­.ection maps (SHRMs) as a 
compact representation. Instead of a single color, each pixel stores coef.cients of a spherical harmonic 
expansion encoding view-dependence of the re.ection map. A key insight that emerges from the theoretical 
analysis is that for almost all BRDFs, a very low order spherical harmonic expansion suf­.ces. Thus, 
SHRMs can be evaluated in real-time for rendering. Further, they are signi.cantly more compact and accurate 
than pre­vious methods [Cabral et al. 1999; Kautz and McCool 2000] that use an explicit 1D or 2D set 
of images. Fast pre.ltering: One of the drawbacks of current environment mapping techniques is the signi.cant 
computational time required for pre.ltering, which can run into hours, and preclude the use of these 
approaches in applications involving lighting and material de­sign, or dynamic lighting. We introduce 
new pre.ltering methods based on spherical harmonic transforms, and show both empiri­cally, and analytically 
by computational complexity analysis, that our algorithms are orders of magnitude faster than previous 
work. We present a complete theoretical analysis and practical algo­rithm pipeline, incorporating all 
three contributions. It is also pos­sible to separately (incrementally) incorporate any one (or two) 
of the improvements into previous methods.  2 Related Work Angular space environment map rendering has 
a long history in graphics, including early work by Blinn and Newell [1976], Miller and Hoffman [1984], 
and Greene [1986]. Hakura et al. [2001] pro­pose location and geometry-dependent environment maps for 
local re.ections. Our goals are different in that we want to capture the ef­fects of complex BRDFs and 
use any object geometry, but it should be possible in future to combine the methods for local re.ections 
RN,RGlobal incident, normal, viewing, re.ected directions L,RV,R R.i,R.o Local incident, outgoing (viewing) 
directions (a, ß) Elevation, azimuthal angles for R N (or R ) Ra,ß Rotation operator for surface orientation 
(a, ß) (.i,fi) Local incident elevation and azimuthal angles (R.i) (.o,fo) Local outgoing elevation and 
azimuthal angles (R.o) (. o,fo) Global outgoing angles (RV ) d.i, . (Differential) hemisphere of integration 
L, B Incident, Re.ected radiance ., . BRDF, BRDF multiplied by cosine of incident angle Llm,. l,. lpq 
Coef.cients in spherical harmonic expansion of L,. Blm,Blmpq Coef.cients in basis-function expansion 
of B Bpq(a, ß) Coef.cients in SHRM Dl (a) Matrix for rotating spherical harmonics mnYlm(., f) Spherical 
Harmonic basis function Y * (., f) Complex Conjugate of Spherical Harmonic lmflm(.) Normalized . dependence 
of Ylm s, s Phong exponent, surface roughness (microfacet) p .l 4p/(2l +1) v I -1 F Maximum order l of 
coef.cients . lpq,Blmpq P Maximum order p in spherical harmonic expansion S Angular resolution (S>F) 
T Number of images in angular space (T>P) W Angular width of BRDF Error (unaccounted energy in approximation) 
Ca, Cf Angular, frequency domain computational costs Table 1: Notation used in the paper. and complex 
BRDFs. Most recently, we [Ramamoorthi and Hanra­han 2001a] applied spherical harmonic analysis to irradiance 
envi­ronment maps for Lambertian objects. This paper generalizes that approach to general isotropic materials. 
As noted by Cabral et al. [1999], environment mapping can be viewed as re.ection-space image-based rendering, 
and is there­fore related to a number of IBR methods like surface light .elds [Nishino et al. 1999; Wood 
et al. 2000]. A surface light .eld stores the outgoing radiance distribution for each point on a geo­metric 
model. We store the re.ected radiance distribution for each normal direction, allowing our representation 
to be mapped on to any object geometry. Our representation is essentially equivalent to the surface light 
.eld of a sphere, or an orientation light .eld. Our work also relates to recent research on hardware 
render­ing with factored BRDFs [Kautz and McCool 1999; McCool et al. 2001]. However, these methods require 
the BRDF to at least ap­proximately satisfy a particular factored form. These previous methods also do 
not support complex illumination. We use spherical harmonics the analogue on the sphere to the Fourier 
basis, i.e. sines and cosines, on the line or circle. Spherical harmonics were .rst used in graphics 
in early work by Cabral et al. [1987], and later by Sillion et al. [1991] and Westin et al. [1992]. 
 3 Preliminaries In this section, we .rst discuss the re.ection equation and introduce the basic framework 
for our method. We then describe our BRDF parameterization, and discuss previous 4D function representa­tions. 
Table 1 summarizes notation used in the paper, and appendix A gives the main spherical convolution formulae 
[Ramamoorthi and Hanrahan 2001b; Ramamoorthi and Hanrahan 2001c]. Assumptions: We make a number of simplifying 
assumptions common in real-time rendering in general, and environment map­ping in particular. Speci.cally, 
we assume distant illumination and isotropic BRDFs, and restrict ourselves to direct lighting, ignoring 
interre.ection and self-shadowing. We will also not explicitly con­sidered textured objects, but it is 
easy to use texture-mapping to modulate the net re.ectance, simply by multiplying the texture and the 
re.ected light .eld computed by our methods. Re.ection Equation: Given our assumptions, the re.ected 
light .eld can be written simply as a function of the surface orienta­tion N and outgoing direction .No 
with respect to the local surface: Z B(N ; N.o)= L(N ; .Ni).(N.i,N.o)(N.i · Nz) d.i (1) . Here, L is 
the incident radiance, . is the BRDF, and B is the re­.ected light .eld. N corresponds to the global 
coordinates of the surface normal. N.i and N.o are local incident and outgoing direc­tions, with N.i 
· Nz being the cosine of the incident angle (in lo­cal coordinates, the surface normal is the Nz vector). 
Under the assumption of distant illumination, L depends only on the global light direction, obtained 
by applying a rotation to the local incident direction. We now rewrite equation 1 using the spherical 
angular parameters (a, ß) for N , (.i,fi) for .Ni, and (.o,fo) for .No. Fi­ nally, we assume isotropy, 
and de.ne . = . cos .i. Z ; B(a, ß;.o,fo)= LRa,ß (.i,fi). (.i,.o, | fo - fi |)d.i (2) . When the viewer 
is distant, it is often useful to rewrite the re.ected light .eld in terms of the global viewing direction 
VN=(. o,fo). .. R-1 (.o,fo)= a,ß . o,fo ...... B a,ß; . o,fo = B a,ß; Ra,ß -1 . o,fo (3) Our general 
approach (and that of previous work [Cabral et al. 1999; Kautz and McCool 2000]) is to represent the 
incident lighting L by an environment map. The environment map is pre.ltered to compute some representation 
of B (or B ), followed by interactive rendering with this representation. The rest of this paper covers 
a number of issues that must be addressed. First, we must .nd the appropriate (re)parameterization for 
B and . . Next, we must determine how to represent B in a compact manner suitable for interactive rendering. 
For this, it is important to analyze the required sampling rates and output reso­lutions. Finally, we 
must determine how to ef.ciently compute our representation of B, i.e. rapidly pre.lter the environment 
map. An overview of our entire algorithm pipeline is shown in .gure 3. 3.1 Reparameterization by central 
BRDF direction Our goal is to reparameterize the BRDF and re.ected light .eld so that they become relatively 
simple and compact, and possibly lower-dimensional functions. Reparameterizing also allows us to eliminate 
the warping step required by Cabral et al. [1999]. Consider .rst the special case of radially symmetric 
or 1D BRDFs, where the BRDF consists of a single symmetric lobe of .xed shape, whose orientation depends 
only on a well-de.ned cen­tral direction CN. In other words, the BRDF is given by a 1D func­tion u as 
. = u(CN· LN). Examples are Lambertian . = N · LNand Phong . =(RN· LN)s models. If we reparameterize 
by CN, the BRDF NN becomes a function of only 1 variable (.i with cos .i = C · L) in­stead of 3. Further, 
the re.ected light .eld can be represented sim­ply by a 2D re.ection map B(a, ß) parameterized by CN=(a, 
ß). Note that we will often use N R, the re.ection of the viewing direc­tion about the surface normal, 
as a synonym for CNsince that is the most common case; however our analysis applies generally. For general 
BRDFs, the radial symmetry property does not hold precisely, so they cannot be reduced exactly to 1D 
functions, nor can B be written simply as a 2D re.ection map. Nevertheless, a reparameterization by the 
re.ection vector still yields compact N forms. As can be seen for the N · H model shown in the lower 
part of .gure 2, most of the variation in the BRDF is still over only a single variable (.i) after reparameterization, 
while there is very little variation over .o (or f) for .xed .i. Further, most of the variation in B 
remains over (a, ß), with only low-frequency variation over the other two variables (.o,fo). R   ing, 
ease of error analysis, and speed for computation (pre.ltering). Explicit tabular representation: We 
may simply tabulate B(a, ß, .o,fo) on a 4D grid. Cabral et al. [1999] use a sparse 2D set of standard 
2D re.ection maps. However, a very large amount of data will be required to accurately tabulate a 4D 
light .eld. Cabral et al. [1999] use only 12 re.ection maps, trading accu­racy for compactness. Kautz 
and McCool [2000] approximate the BRDF as a 2D function . = u(.i,.o) having no azimuthal depen- Figure 
3: An overview of our entire pipeline. S and T are angular resolutions, while F and P are orders of the 
spherical harmonic expansions. for angular resolutions while F and P stand for orders of the spher­ical 
harmonic expansions, which are determined using the theoret­ical analysis in section 5. The inputs to 
the algorithm are tabulated values of the lighting L(., f) and 3D isotropic BRDF . (.i,.o,f). We then 
use our fast pre.ltering algorithm, described in detail in section 6, to compute the SHRM. This is done 
by .rst computing the spherical harmonic lighting coef.cients Llm and BRDF coef­.cients . lpq. We then 
use the spherical frequency-space convolu­tion formula [Ramamoorthi and Hanrahan 2001c], which may be 
viewed as the frequency domain analog of equation 2, to compute re.ected light .eld coef.cients Blmpq. 
Blmpq =.lLlm. lpq (5) Finally, we expand Blmpq to generate the SHRM, as per equa­tion 14 in appendix 
A. PBp XX B(a, ß, .o,fo)= Bpq(a, ß)Ypq(.o,fo) (6) p=0 q=-p In this equation, Bpq(a, ß)is one coef.cient 
in the SHRM, Ypq is the spherical harmonic of order p and index q, and PB = 0is the maximum order of 
the expansion, with the SHRM containing a total of (PB +1)2 terms. Figure 4 illustrates the idea behind 
SHRMs. Each pixel (a, ß)in a re.ection (cube)map has a partic­ular distribution of outgoing radiance 
B(a, ß, .o,fo). This distri­bution is encoded by the SHRM as a spherical harmonic expansion in (.o,fo), 
with coef.cients Bpq(a, ß). For the special case of radially symmetric 1D BRDFs, there is no dependence 
on (.o,fo) after reparameterization, so we need only the DC or constant term B00(a, ß), and the SHRM 
reduces to a simple 2D re.ection map. Figure 4: The idea behind SHRMs. Each pixel (a, ß) in the re.ection 
cubemap has some distribution of re.ected light. This distribution is encoded as a low-order spher­ical 
harmonic expansion in (.o,fo) for every pixel, with coef.cients Bpq(a, ß).To avoid clutter, this diagram 
uses only a 3x3 resolution in the re.ection map and shows the outgoing distribution for only four of 
the pixels. So far, we have considered local SHRMs, depending on local outgoing angles (.o,fo), which 
are different for each (a, ß).It is often convenient to assume the viewer is distant and compute global 
SHRMs, dependent on a global viewing direction (. o,f o). PBp XX B (a, ß, . o,f o)= B pq(a, ß)Ypq(. o,f 
o) (7) p=0 q=-p The advantage of equation 7 over equation 6 lies in ease of eval­uation for rendering, 
since Ypq(. o,f o)can be evaluated once per frame, instead of per pixel. In fact, we will show in section 
7.5 that this allows global SHRMs to be rendered using a single dynamic re.ection map, with standard 
re.ection mapping hardware. We still need to know how to determine global SHRM coef.­cients B pq(a, ß). 
The spherical convolution formula in equation 5 applies only to local SHRMs. However, we may rotate coef.cients 
to compute the global SHRM. We make use of equations 3 and 6, with the subscript q changed to s for later 
convenience. PB p ()XX(()) B a, ß; . o,f o= Bps(a, ß)Yps R-1 . o,f o a,ß p=0 s=-p From this expression, 
we determine the modi.ed (rotated) coef.­cients of the global SHRM separately for each orientation (a, 
ß), using the spherical harmonic rotational formulae. Dp is a matrix that expresses how rotation about 
the y axis transforms spherical harmonics into other spherical harmonics of the same order p. p ..X Dp 
-Iqß Bpq(a, ß)= sq(-a)eBps(a, ß) (8) s=-p Advantages: SHRMs are a hybrid form, midway between a pure 
coef.cient-based approach, and an explicit tabular represen­tation. We believe this is a good point in 
the design space, and our representation has the following signi.cant advantages: Compact, Ef.cient 
and Accurate: A key insight from the theoretical analysis is that for essentially all BRDFs, a very low 
value of PB (usually = 3) suf.ces for high accuracy. This is the formal basis for using a low order spherical 
harmonic expansion in the SHRM, and ensures that our representation is very compact and accurate compared 
to previous approaches, as well as being ef.cient to evaluate for real-time rendering.  Error analysis 
and number of coef.cients/resolutions:  Unlike for other compression and factorization techniques, the 
error analysis in section 5 does not .rst require compu­tation of a dense 4D re.ected light .eld, and 
allows us to eas­ily determine the correct order PB of the spherical harmonic expansion and the resolutions 
of the re.ection maps. Rapid computation: In section 6, we show how the SHRM can be computed using frequency 
domain pre.ltering, orders of magnitude faster than previous approaches.   5 Analysis of sampling rates/resolutions 
In this section, we present our framework for analyzing the required sampling rates, i.e. the number 
of coef.cients needed in our spher­ical harmonic expansions. At the end of the section, we will justify 
the SHRM representation based on our analysis. The sampling rates will depend on the frequency content 
of the lighting and BRDF. Figure 5 shows spheres rendered with progres­sively blurred illumination (along 
the y axis) and a progressively more diffuse BRDF (along the x axis). It can be seen that the high­est 
frequencies in the re.ected light .eld are determined approxi­mately by the minimum of the highest frequencies 
in the lighting and BRDF. This is not surprising, since we may view the BRDF as a low pass .lter acting 
on the lighting signal. As summarized in .gure 3, we assume the input lighting L(., f) is represented 
on an SL ×SL grid, where SL is the grid angular res­olution, and that the 3D isotropic BRDF . (.i,.o,f)is 
represented on a grid of size S. × T. × T. where S. is the angular resolution with respect to .i and 
T. is the angular resolution with respect to (.o,f). For simplicity, we will consider the lighting and 
BRDF to be represented in latitude-longitude form, i.e. simply as tabulated values on an equally-spaced 
grid 0= . = p and 0= f = 2p. Our pre.ltering algorithm computes the lighting coef.cients Llm to order 
FL (i.e. FL is the maximum value of l) and BRDF coef.cients . lpq to orders F. and P. (i.e. l = F. and 
p = P. ). The light .eld coef.cients Blmpq are computed to orders FB and PB. Finally, we generate the 
SHRM Bpq(a, ß), with the angular size in (a, ß)being SB × SB, and the spherical harmonic expan­sion up 
to order PB. Radially symmetric 1D BRDFs can be seen as special cases of this framework with T. =1, and 
P. =PB =0. 5.1 Order of spherical harmonic expansions We now analyze required orders F and P in our 
spherical harmonic expansions. First, consider the lighting. The total energy is ZZ 8 l p 2p XX L2 (., 
f)sin.d.df = | Llm |2 .=0 f=0 l=0 m=-l We can estimate the error E in an order FL spherical harmonic 
expansion by considering the fraction of the total lighting energy captured. To obtain an accuracy 1- 
E, we require that FLl ZZ p 2p XX L2 | Llm |2= (1- E)(., f)sin.d.df .=0 f=0 l=0 m=-l  0 5 1015 Order 
Figure 6: Accuracy (1-) versus frequency F for an order F approximation of the re.ected light .eld B, 
and estimates of that accuracy obtained by taking the minimum error for BRDF and lighting, and by using 
the conservative bound based on residual  energy. We have not separately plotted using the BRDF error 
only, as this gives almost Figure 5: Renderings with different lighting and BRDF conditions. The highest 
exactly the same curve as taking the minimum error for BRDF and lighting. BRDF frequency decreases frequency 
in the images is approximately the minimum of the highest frequencies in the lighting and BRDF. Speci.cally, 
all spheres inside a yellow delimiter look similar. Given L(.,f) and FL, it is easy to determine the 
error E, and check if it is below threshold. Conversely, if we .x E, we can com­pute FL as the minimum 
frequency for which the above equation holds. The number of coef.cients required measures the frequency 
width of the signal, and Emeasures the missing (residual) informa­tion. A similar method may be used 
for analyzing the BRDF. The remaining issue is how to combine the information for light­ing and BRDFs 
to determine appropriate orders for the re.ected light .eld B. We list below two possible approaches. 
 Minimum of orders or errors: Consider the case where E= 0 for either the lighting or BRDF, i.e. one 
or both is bandlim­ited. The re.ected light .eld is then exactly reproduced by using an expansion to 
order (FB,PB)=(min(FL,F. ),P. ). This formalizes the intuition that we need to sample densely enough 
to catch the highest frequency present simultaneously in both the lighting signal and BRDF .lter. This 
analysis does not apply rigorously when neither signal is bandlimited, but taking the minimum of orders 
for a given error Eis still a good heuristic. Conversely, for a given order of expansion, we can estimate 
the error EB =min(EL,E. ). Since the lighting signal usually contains substantial high fre­quency content, 
while the BRDF acts as a low-pass .lter, this method often reduces simply to capturing 1 - Eof the BRDF 
energy, i.e. choosing FB,PB = F. ,P. or setting EB = E. .  Bound residual energy: For completeness, 
we discuss a more rigorous numerical scheme, which can be proven to give conservative estimates. The 
scheme is based on bounding the residual unaccounted for energy in the re.ected light .eld. One disadvantage 
of this method is that, unlike the previous method, we .rst need to actually calculate the coef.cients 
Blmpq of the re.ected light .eld. Thus, this method is most useful as a .nal sanity check on the validity 
of the earlier heuristic. The mathematical details are in appendix B.  We use a simple example to illustrate 
these methods. For a particular illumination (the Grace Cathedral), and a Phong BRDF (exponent s =32), 
we computed approximations to the light­ing, BRDF, and re.ected light .eld for increasing values of order 
F = FL = F. = FB. Since the BRDF is radially symmetric, P = P. = PB =0. We also computed the re.ected 
light .eld accurately, by using a very high order F =30, so we could de­termine the errors of lower-order 
approximations. Figure 6 plots the accuracy (top curve) of an order F approximation of B, as well as 
estimates of this accuracy obtained by taking the minimum of BRDF and light errors at order F, and by 
bounding the residual energy. We see that both accuracy estimates are conservative but fairly tight, 
especially for small errors or high accuracies (at higher frequencies). Further, taking the minimum of 
lighting and BRDF errors is almost always equivalent simply to using the BRDF error. Therefore, we choose 
the simplest approach of using the BRDF error, requiring E. be lower than a user-selected tolerance. 
 5.2 Justi.cation for SHRM representation We seek to determine the best point in the spectrum of time/space 
or angular/frequency tradeoffs. For this, we must understand how to relate angular space resolutions 
S and T to frequency-space or­ders F and P. As a simple illustration, consider irradiance maps from Lambertian 
BRDFs. It has been shown [Basri and Jacobs 2001; Ramamoorthi and Hanrahan 2001b] that an order 2 spheri­cal 
harmonic expansion suf.ces. However, a 3 × 3 irradiance map will clearly be inadequate. In practice, 
irradiance maps are usually represented at angular resolutions higher than 16 × 16. Experi­menting with 
different resolutions, we have found that in general, one requires S ~ 10F (and T ~ 10P). Therefore, 
a signi.cantly more compact size for B is obtained using spherical harmonic coef.cients rather than an 
explicit 4D tabular representation. The other extreme in the spectrum of time-space tradeoffs using a 
purely coef.cient-based approach is also usually1 undesirable. Ef.cient rendering of 2D re.ection maps 
having high frequency content, such as specular re.ection maps from Phong BRDFs, is dif.cult directly 
from the spherical harmonic expansion, since O(F2) terms must be added per pixel, with F generally larger 
than 10. Rendering the 4D light .eld purely from coef.cients is even harder, requiring O(F2P2) terms. 
Hence, we believe an intermediate representation, allowing for both compact representation, and fast 
rendering, is optimal. In or­der to determine the best representation for B, we must know the common 
values for orders F and P (and hence resolutions S and T). Our results in section 7 show that for practically 
all currently available analytic and measured BRDFs, values of F = 30 and P = 5 suf.ce for an accuracy 
greater than 90%. Therefore, it is best to encode the view dependence (.o,fo) as a compact (and eas­ily 
evaluated) spherical harmonic expansion consisting of (P+1)2 terms, while explicitly representing the 
high-frequency dependence on (a,ß). This is the approach taken by SHRMs, where each pixel (a,ß) stores 
coef.cients Bpq(a,ß) of an order P spherical har­monic expansion.  6 Pre.ltering We now describe our 
ef.cient frequency space pre.ltering algo­rithms to create the SHRM and ef.ciently implement the pipeline 
in .gure 3. We will present an analysis of the computational com­plexity of our algorithms, and end this 
section by validating our conclusions on the Phong BRDF. 6.1 Key steps and insights Our pre.ltering method 
has two main components. First, we must ef.ciently convert between angular and frequency space descrip­tions. 
Second, we must ef.ciently compute coef.cients of the re­.ected light .eld from lighting and BRDF coef.cients. 
Both com­ponents can be performed rapidly because of the insights below. 1For very diffuse BRDFs (F. 
and P. both very small), a purely coef.cient-based approach may be acceptable. The most notable example 
is the Lambertian BRDF (F. =2, P. =0), where a 9 term spherical harmonic expansion suf.ces [Ra­mamoorthi 
and Hanrahan 2001a]. The algorithm itself is just a direct three step ef.cient implemen­tation of the 
pipeline of .gure 3. Implementation details, and the time complexities of the various steps, are found 
in appendix C. Linear time complexity of convolution formula: The re­.ected light .eld coef.cients Blmpq 
can be computed in time lin­ear in the number of output coef.cients Blmpq simply by applying the spherical 
convolution formula in equation 5. Fast conversion to and from spherical harmonics: We still need to 
convert from an angular space representation of L(.,f) and . (.i,.o,f) to the spherical harmonic coef.cients, 
as well as generate the SHRM from Blmpq. As an example, consider computation of lighting coef.cients 
Llm. For any l, mwe have ZZ p 2p Llm = lm(.,f)sin .d.df L(.,f)Y * .=0 f=0 The cost of performing this 
integral is O(SL2 ). Since we must do this for all coef.cients, it would appear the total cost would 
be O(FL2SL2 ). In fact, we can amortize the cost to compute all the coef.cients in O(FLSL2 ) time by 
writing the spherical harmonics as products of functions in . and f, and then separating the com­putations 
in . and f. The basic idea is to compute in succession: Z 2p -Imf .m,. : Lm(.)= L(.,f)e df f=0 Zp .l,m 
: Llm = Lm(.)flm(.)sin .d. .=0 Here, the spherical harmonic Ylm(.,f)= flm(.)e Imf . The .rst step involves 
a loop over (2FL+1)SL elements, each step of which requires numerical integration by adding together 
SL values. Thus, the cost is O(FLSL2 ). A similar argument shows that the second step takes time of O(FL2SL). 
Since FL <SL, the .rst step dom­inates, and the total complexity is O(FLSL2 ). Fast spherical har­monic 
transform methods [Mohlenkamp 1999], analogous to the Fast Fourier Transform, may reduce the cost further2 
to logarith­mic in FL, i.e O(SL 2 log2 SL) ~ O(SL 2 log2 FL). However, these methods are complicated, 
and although asymptotically faster, have relatively large constant cost factors. Therefore, they are 
unlikely to be signi.cantly faster for the relevant low frequencies F ~ 30. 6.2 Computational complexity 
The cost of previous angular domain algorithms is O(WSL2 ) per pixel in the output, since they perform 
a hemispherical integral for each pixel. Here, W is the fraction of the illumination pixels that need 
be considered, corresponding to the angular width of the BRDF, with W . 0 for a mirror, and W =1/2 if 
one considers the entire visible hemisphere. In appendix C, we derive the cost for our frequency space 
algorithm3, which is much lower, being O(FB) or O(PB) per pixel. Table 3 summarizes our main results. 
Type Angular Frequency Cost /pixel Cost /pixel 1D BRDF WS2 LS2 B WS2 L FBS2 B FB SHRM FBP2 BS2 B FB 3D 
BRDF WS2 LT2 BS2 B WS2 L PBT2 BS2 B PB Table 3: Computational complexity of pre.ltering. We show both 
total costs and costs per pixel. The angular costs correspond to hemispherical integration, with W being 
the BRDF angular width. The frequency space costs are for our method. Radially Symmetric 1D BRDFs: The 
output re.ection map size is SB × SB. Standard hemispherical integration is quadratic in the output size, 
since we must examine O(WSL2 ) illumination pixels per output pixel and4 SL = SB. By contrast, our frequency 
space pre.ltering algorithm requires only O(FB) time per output pixel. Since FB « SB <SL, this method 
is only slightly super­linear, being substantially sub-quadratic in the output size. 2Since SL ~ 10FL, 
log SL ~ log FL. Also note that simply using an FFT in step 1 will not suf.ce, since step 2 does not 
have logarithmic complexity. 3If we were to use fast spherical harmonic transform methods, the asymptotic 
com­plexity per output pixel would be O(log2 FB) or O(log2 PB) instead. 4One way to compute FB is min(FL,F. 
) so FB = FL and SB = SL. SHRM creation: The SHRM is a new representation, not cre­ated by traditional 
pre.ltering algorithms. As for 1D BRDFs, the cost of our method is O(FB) per output pixel per coef.cient. 
3D BRDFs, explicit re.ection maps: In order to compare to angular domain methods, we must produce the 
same output. We can use the SHRM to explicitly compute a set of TB ×TB re.ection maps (with TB ~ 10PB 
), similar to the explicit representations of Cabral et al. [Cabral et al. 1999] or Kautz and McCool 
[Kautz and McCool 2000]5. The cost of traditional pre.ltering remains O(WSL2 ) per output pixel. On the 
other hand, our method takes O(PB) time per pixel. Since PB = 3 in most cases, it can be re­garded a 
constant. Hence, our method is quasi-linear in the output size. This is a speedup of three to four orders 
of magnitude the difference between near-interactive computation in a few seconds, and pre.ltering times 
in hours. 6.3 Validation with Phong BRDF In this subsection, we validate our theoretical computational 
com­plexity analysis on the simple radially symmetric Phong model. In this case, P. = PB =0 and the SHRM 
reduces to a standard 2D re.ection map B(a,ß). In the results section, we show timings, including for 
more general 3D isotropic BRDFs. The normalized and reparameterized Phong BRDF is de.ned by s+1 s . = 
cos .i 2p where cos s .i =(RN· LN)s . BRDF coef.cients . l can be derived analytically [Ramamoorthi and 
Hanrahan 2001c], and an accurate approximation is .. l2 . l .-l 1 exp - 2s In appendix D, we show that 
the order F= F. corresponding to v B v error E is FB = -slog E. Ignoring the constant - log E, the pre.ltering 
cost of our frequency space algorithm is therefore = O(FBSB2 )= O(S2 v s ) (9) CfB Appendix D derives 
a formula for the angular width W of the BRDF, and shows that the angular domain pre.ltering cost is 
.. SL2 S2 LS2 Ca = O(WS2 B)= O B (10) s We note that the frequency space cost Cf increases with increas­ 
v ing Phong exponent as s, while the angular space cost Ca de­creases with increasing Phong exponent 
as 1/s. This is entirely ex­pected, since sharp specular surfaces (large s) have a BRDF which is very 
local in the angular domain but requires a large number of coef.cients to represent in the frequency 
domain. Conversely, rough surfaces (small s) are very easily handled in the frequency domain, but their 
BRDFs have a large angular width. Therefore, for s<s *, our frequency domain methods are to be preferred 
and for s>s *, conventional angular domain techniques are preferable. s * can be found by equating equations 
9 and 10. v ** 4/3 S2 s * ~ SL2 S2 . s L BB/s ~ S What does this mean numerically? Assume a small size 
of SL = 100. We then obtain s * 464. Therefore, in essentially all practi­cal cases of interest, our 
frequency domain algorithm is superior to conventional angular domain methods, often by one to two orders 
of magnitude. Of course, the actual numerical value for s * depends on the constant cost factors associated 
with the respective imple­mentations. Our empirical tests, discussed in section 7.4, show that the practical 
value of s * is actually even higher than predicted.  7 Results We have tested our method using a number 
of different lighting conditions and BRDFs. This section reports our main results. 5Since their representation 
is 3D, we should compute only TB re.ection maps. View 1 Difference image View 2 Difference image P = 
0 87.6% P = 1 94.8% P = 2 98.0% P = 3 99.0% exact 100% Figure 7: Comparing images obtained with different 
values for P for a simpli.ed microfacet BRDF model with surface roughness s =0.2. These images correspond 
to two particular views, i.e. values of (. o,f o). The percentages are fractions of the total energy 
(1 - ) of the BRDF captured for that P , which we use as a conservative estimate of the accuracy of the 
re.ected light .eld. The exact images at the bottom were computed by a full hemispherical angular-space 
integral for each image pixel. For this and subsequent .gures, the difference images are not ampli.ed, 
and we used FB =30 and SB = 128. 7.1 Number of coef.cients for analytic BRDFs The practical values of 
the orders in the spherical harmonic expan­sion of the BRDF F. and P. (and hence FB and PB) will depend 
on the form of the BRDF, with slowly varying BRDFs requiring fewer coef.cients. As a basic test to determine 
reasonable empiri­cal values, we considered three general analytic BRDFs. Microfacet: Consider a simpli.ed 
microfacet [Torrance and Sparrow 1967] model, -(.h/s) . = 1 2 e 2 (11) 4ps -1(NN· N where .h =cos H)is 
the angle between the normal and the half-angle vector. Approximations to re.ection maps with different 
values of P = P. = PB are shown in .gure 7. As expected, the accuracy improves as we use higher values 
of P. Speci.cally, P = 2 suf.ces to produce very accurate results, with the BRDF error E<.03. Recall 
from section 5.1 that we use the BRDF accuracy as a conservative estimate of the accuracy of the re.ected 
light .eld. For this BRDF, F = F. = FB is given approximately by F ~ s-1, and ranges from 10 to 30 for 
common values of s ~ 0.1. These values of F and P are typical for most BRDFs. In general, P is very small, 
while F is usually much larger. Lafortune BRDF: We also tested the model of [Lafortune et al. 1997], 
with coef.cients obtained from the skin measurements of Marschner et al. [2000]. Although the behavior 
is more interesting, with much stronger specularities exhibited toward grazing angles, a value of P =4still 
suf.ces for an error E<.03. Kajiya-Kay model: Finally, we tried the Kajiya-Kay [1989] model, which is 
an anisotropic variant of the Phong BRDF, and depends on incident and outgoing angles with respect to 
the tan­gent vector. As discussed in section 3.1, we may reparameterize by the tangent vector, to derive 
. =cos s(.i - .o). While this paper does not consider general 4D anisotropic BRDFs, we can handle the 
Kajiya-Kay BRDF, since it is mathematically analogous to a (2D) isotropic BRDF after reparameterization. 
Unlike for ordinary 61-Moss 60-Cracker_b 59-Cracker_a 58-Tree-Bark 57-Peacock-Feather 56-Wood_b 55-Orange-Peel 
54-Wood_a 53-Plant 52-White-Bread 51-Corn-Husk 50-Concrete_c 49-Concrete_b 48-Brown-Bread 47-Stones 46-Cotton 
45-Concrete_a 44-Linen 43-Salt-Crystals 42-Corduroy 41-Brick_b 40-Straw 39-Human-Skin 38-Ribbed-Paper 
37-Brick_a 36-Limestone 35-Spheres 34-Slate_b 33-Slate_a 32-(14-zoomed) 31-(12-zoomed) 30-(11-zoomed) 
29-(2-zoomed) 28-Crumpled-Paper 27-Insulation 26-Loofa 25-Quarry-Tile 24-Rabbit-Fur 23-Lettuce-Leaf 22-Lambswool 
21-Sponge 20-Styrofoam 19-Rug_b 18-Rug_a 17-Rough-Tile 16-Cork 15-Foil 14-Roof-Shingle 13-Artificial-Grass 
12-Rough-Paper 11-Plaster_b 10-Plaster_a 9-Frosted-Glass 8-Pebbles 7-Velvet 6-Sandpaper 5-Leather 4-Rough-Plastic 
3-Terrycloth 2-Polyester 1-Felt 0 102030405060708090 100 AccuracyFigure 8: Accuracy of a spherical harmonic 
BRDF approximation for all 61 BRDFs in the CURET database. We show 6 values of P. ranging from 0 to 5 
from left to right. The low orders for P. are shown with light gray diamonds, while a black circle shows 
the highest order P. =5. Note that the rightmost circle corresponds to an accuracy greater than 90% in 
56 of the 61 rows. Phong-like BRDFs, we cannot apply any further re.ective reparam­eterization. Therefore, 
the value of P required is large (we found P =8for s=32and E<.03). However, there is no azimuthal de­pendence, 
so we require only P+1terms Bp0(a,ß)in the SHRM instead of (P+1)2 (i.e. q=0, with no dependence on fo). 
Hence, the SHRM is still a very ef.cient and compact representation. 7.2 Number of coef.cients for measured 
BRDFs To further evaluate the accuracy of our approximations, we used the CURET database [Dana et al. 
1999]. This database consists of 61 BRDFs and BTFs, corresponding to a variety of materials. For each 
sample, there are 205 BRDF measurements, which may be interpolated by .tting order 8 Zernike polynomials 
to create a complete BRDF description [Koenderink and van Doorn 1998]. Figure 8 is a bar chart showing, 
for each of the 61 samples, the accuracy of a BRDF approximation6 with F. =30 and values of P. ranging 
from 0 to 5. In 56 cases, the accuracy for F. =30 and P. =5 was greater than 90% (E< 0.1), and was usually 
signi.cantly higher (in most cases, E<.05 for P. =3). The remaining 5 examples (9-frosted glass, 23-lettuce 
leaf, 33-slate a, 41-brick b, 57-peacock feather) were all signi.cantly anisotropic. Therefore, we conclude 
that for almost all BRDFs of interest, an order PB = 5 suf.ces for the SHRM, with F = 30. In fact, for 
most BRDFs, a quadratic or cubic (second or third order with PB =2or 3) spherical harmonic expansion 
in the SHRM suf.ces. 6We reparameterized all BRDFs by the re.ection vector. Our results demonstrate that 
this reparameterization is suitable even if the BRDF is not primarily re.ective, or consists of both 
diffuse and specular components. The specular components are compactly represented, while the diffuse 
components are low frequency anyway. 7.3 SHRM accuracy We now compare images created with SHRMs to the 
correct image, and to previous approaches. First, .gure 9 compares our method to Kautz and McCool s [2000] 
3D texture-mapping technique7, where they approximate the BRDF and hence, the re.ected light .eld as 
having no azimuthal dependence. For the relatively complex velvet BRDF (CURET database) in .gure 9, their 
approximation introduces large errors, while the SHRM with PB =5 is accurate. Figure 10 compares our 
approach to the correct image and Cabral s icosahedral interpolation. For sharply-varying BRDFs, such 
as those exhibiting strong near-grazing specularities, or com­plex anisotropic behavior, Cabral s approximation 
can lead to large errors, while our approach still gives accurate results. Correct image SHRM 2D BRDF 
(Kautz) Difference images Figure 9: Comparing the correct image on the left to those created using 
SHRMs (middle) and the 2D BRDF approximation of Kautz and McCool (right). Correct image SHRM Cabral 
 7.4 Speed of pre.ltering We .rst consider Phong BRDFs, experimentally validating the the­oretical conclusions 
of section 6.3. For our frequency domain algo­ v rithm, we used E = .01, conservatively setting FB =1+ 
6s.For the angular domain, we were more aggressive, setting E = .05. The resolution SL and SB of the 
inputs and .nal results were 128, i.e. we generated output Phong re.ection maps at 128×128 resolution. 
According to the theory, this is an appropriate resolution for s =32 and s =64 (i.e. FB 12), and is 
therefore the most suitable single resolution for the entire range of Phong exponents. The numerical 
running times reported in table 4 obviously depend on our imple­mentation and hardware. However, we believe 
the ratio in running times of angular and frequency domain methods is quite represen­tative. Furthermore, 
the timing data can be .t almost precisely to the theoretical predictions of equations 9 and 10. The 
results indi­cate that our pre.ltering method is usually two orders of magnitude faster than angular-space 
methods, and that Phong BRDFs can be pre.ltered at close to real-time rates using our approach. Exponent 
s Time (sec) Ratio Angular Frequency (Ang/Freq) 8 67.28 0.081 830.6 16 38.03 0.114 333.6 32 21.80 0.159 
137.1 64 11.94 0.227 52.6 128 7.17 0.328 21.9 256 3.55 0.461 7.7 512 2.28 0.686 3.3 Table 4: Comparison 
of timings of angular and frequency-space pre.ltering for different values of the Phong exponent s. The 
timings are on a 1.4GHz Pentium IV. image. If hardware multitexturing support is available, we may represent 
the spherical harmonics Ypq(.o,fo) and the local SHRM coef.cients Bpq(a, ß) by 2D texture maps. Since 
we are reparam­eterizing by the re.ection vector, we will sometimes also refer to Bpq(a, ß) as re.ection 
maps. If PB =2, there would be 9 terms in the SHRM, corresponding to a total of 18 texture maps. We would 
then use graphics hardware to accumulate 9 terms, with each term being the product of two texture maps, 
i.e. Bpq(a, ß)Ypq(.o,fo). Since this algorithm is essentially that previously used for ren­dering factored 
BRDFs [Kautz and McCool 1999; McCool et al. 2001], the same code can now be easily adapted for arbitrary 
isotropic BRDFs and complex illumination. A simpler approach is possible when the viewer can be assumed 
distant, using the global SHRM in equation 7. The spherical har-  Figure 10: Comparing the correct image 
to those created using SHRMs and icosa­hedral interpolation (Cabral s method). We see that the SHRM image 
is accurate, while Cabral s approximation is inadequate for sharp near-grazing re.ections (top), and 
for complex BRDFs like the anisotropic Kajiya-Kay model (bottom). In our approach, the theoretical analysis 
can be used to system­atically trade off accuracy for compactness and ef.ciency. Speci.­cally, if Kautz 
and McCool s [2000] approximation of 2D BRDFs with no azimuthal dependence suf.ces (q =0), we get a3D 
SHRM with only PB +1 terms instead of (PB +1)2. If Cabral et al. s [1999] icosahedral set of 12 re.ection 
maps suf.ces, we can use a very small number of terms (PB =1 or 2) in the SHRM. 7We use their single 
lobe model, with the BRDF being an arbitrary 2D function . = u(.i,.o). This is essentially equivalent 
to setting q =0 in the local SHRM, using only the azimuthally independent terms. monics Ypq(. o,f o) 
need be evaluated only once per frame, for given viewpoint (. o,f o), instead of at each vertex or pixel. 
In fact, it is possible to render the scene using only a single re.ection map­ping pass. The key idea 
is to explicitly sum equation 7 to create a single dynamic 2D re.ection map B (a, ß), which is updated 
for every frame, i.e. each new viewpoint (. o,f o). PBp XX B (a, ß)= B pq(a, ß)Ypq(. o,f o) (12) p=0 
q=-p Our implementation extends the Stanford real-time pro­grammable shading system [Proudfoot et al. 
2001] to render with global SHRMs using equation 12. An advantage of our approach is that standard re.ection 
maps can be upgraded to SHRMs with no change in the external shader programs. Internally, we simply up­date 
the re.ection map for each frame. We compute equation 12 in software, which allows us to easily consider 
high-dynamic range, and avoids hardware precision and clamping issues. In the .gures, the high-dynamic 
range backgrounds are tone-mapped, but the ob­jects themselves are computed and shaded using a linear 
scale. We used a 1.4 GHz Pentium IV running Linux, with an NVIDIA Geforce2 GTS graphics card, for our 
tests. The re.ection (cube)map B (a, ß) was computed at a resolution of 64 × 64 × 6, which is an appropriate 
resolution for most BRDFs, i.e. F ~ 20. Since there is real-time cube mapping hardware, the major cost 
is that for computing equation 12 in software per frame. We are able to achieve frame rates of approximately 
30 frames per second, with real-time speeds even in scenes with multiple SHRMs. Figure 1 and the accompanying 
videotape show a number of examples.  8 Conclusions and Future Work We have presented new frequency-space 
algorithms for real-time rendering of complex isotropic BRDFs under arbitrary distant il­lumination, 
and have validated our approach using many different BRDFs and lighting conditions. Our contributions 
include theo­retical analysis that allows us to precisely determine the orders of our spherical harmonic 
expansions, the new compact and ef.cient SHRM representation for the re.ected light .eld, and very fast 
pre­.ltering algorithms based on spherical harmonic transforms. We have integrated the three contributions 
into a complete frequency­space pipeline, as per .gure 3. However, it is also easy to con­vert between 
SHRMs and previous explicit representations, as dis­cussed in appendix C. Therefore, the contributions 
of this paper are relatively independent, and can also be incorporated separately. There are several 
interesting similarities and differences between the SHRM and surface light .eld representations. In 
fact, the SHRM can be seen as a surface light .eld on a sphere. The main advantage is that the SHRM is 
independent of geometry. The dis­advantage is that we do not capture effects due to spatially varying 
illumination, which includes non-uniform lighting, interre.ection and self-shadowing. Since surface light 
.elds are so large, they must be compressed. SHRMs expose the structure of the re.ected light .eld in 
terms of the frequency properties of the illumination and BRDF. It is therefore interesting to compare 
spherical harmonic basis functions to PCA-based compression and factorization meth­ods used for BRDFs 
[Kautz and McCool 1999] and surface light .elds [Nishino et al. 1999; Wood et al. 2000]. The main advan­tage 
of SHRMs is that the theoretical analysis gives insight into the intrinsic complexity of the re.ected 
light .eld. This allows us to di­rectly compute the SHRM with the right order and resolution. On the 
other hand, PCA-based methods .nd an optimal basis to repre­sent a data set, assuming no a priori knowledge, 
and are also usually more expensive. Standard PCA requires a dense 4D re.ected light .eld as input, and 
an expensive singular-value decomposition. To ef.ciently create a more compact .nal representation, it 
it might be possible to run PCA directly on the SHRM. One drawback of synthetic IBR is the long time 
required for precomputation, which precludes dynamic lighting or interactive manipulation of material 
properties. Our new pre.ltering method takes an important step in addressing this problem for environment 
maps, and may be adapted in the future to rapidly compute syn­thetic surface light .elds. For the special 
case of radially sym­metric BRDFs, Kautz et al. [2000] have proposed using hardware­assisted 2D image 
convolution. However, while BRDFs are shift­invariant .lters on the spherical domain, they are not shift-invariant 
in the plane, since projection on to a 2D image introduces distor­tion [Kautz et al. 2000], and may lead 
to inconsistencies for in­stance, rotating the lighting may not correspond simply to rotating the pre.ltered 
image. Our pre.ltering algorithm can be viewed as spherical image processing on the incident illumination, 
con­volving it with the BRDF .lter. Our speedups are not surpris­ing, given that planar image convolutions 
are often more ef.ciently computed in the Fourier domain. Other approaches to speed up pre.ltering are 
hierarchical methods [Kautz et al. 2000] and spher­ical wavelets [Schr¨oder and Sweldens 1995]. However, 
there is no wavelet or hierarchical convolution formula, so frequency domain methods are more appropriate 
for environment mapping. In summary, natural illumination and accurate BRDFs are of growing importance 
in interactive applications, and this paper has presented a complete frequency-space pipeline to enable 
this. Acknowledgements: Kekoa Proudfoot and Bill Mark helped us with de­tails of the Stanford Real-Time 
Programmable Shading system, enabling us to imple­ment SHRMs within its framework. Thanks also to Li-Yi 
Wei and Olaf Hall-Holt for reading early drafts. This work was supported in part by a Hodgson-Reed Stanford 
graduate fellowship and NSF ITR grant #0085864 Interacting with the Visual World. References BASRI, 
R., AND JACOBS, D. 2001. Lambertian re.ectance and linear subspaces. In International Conference on Computer 
Vision, 383 390. BLINN, J., AND NEWELL, M. 1976. Texture and re.ection in computer generated images. 
Communications of the ACM 19, 542 546. CABRAL, B., MAX, N., AND SPRINGMEYER, R. 1987. Bidirectional re.ection 
functions from surface bump maps. In SIGGRAPH 87, 273 281. CABRAL, B., OLANO, M., AND NEMEC, P. 1999. 
Re.ection space image based rendering. In SIGGRAPH 99, 165 170. DANA, K., GINNEKEN, B., NAYAR, S., AND 
KOENDERINK, J. 1999. Re.ectance and texture of real-world surfaces. ACM Transactions on Graphics 18, 
1 (January), 1 34. GREENE, N. 1986. Environment mapping and other applications of world projections. 
IEEE Computer Graphics &#38; Applications 6, 11, 21 29. HAKURA, Z., SNYDER, J., AND LENGYEL, J. 2001. 
Parameterized environment maps. In ACM symposium on interactive 3D graphics, 203 208. KAJIYA, J., AND 
KAY, T. 1989. Rendering fur with three dimensional textures. In SIGGRAPH 89, 271 280. KAUTZ, J., AND 
MCCOOL, M. 1999. Interactive rendering with arbitrary BRDFs using separable approximations. In EGRW 99, 
247 260. KAUTZ, J., AND MCCOOL, M. 2000. Approximation of glossy re.ection with pre­.ltered environment 
maps. In Graphics Interface, 119 126. KAUTZ, J., V ´ AZQUEZ,P., HEIDRICH,W., AND SEIDEL, H. 2000. A uni.ed 
ap­proach to pre.ltered environment maps. In EGRW 00, 185 196. KOENDERINK, J., AND VAN DOORN, A. 1998. 
Phenomenological description of bidirectional surface re.ection. JOSA A 15, 11, 2903 2912. LAFORTUNE, 
E., FOO, S., TORRANCE, K., AND GREENBERG, D. 1997. Non-linear approximation of re.ectance functions. 
In SIGGRAPH 97, 117 126. MACROBERT, T. 1948. Spherical harmonics; an elementary treatise on harmonic 
functions, with applications. Dover Publications. MALZBENDER,T., GELB, D., AND WOLTERS, H. 2001. Polynomial 
texture maps. In SIGGRAPH 01, 519 528. MARSCHNER, S., WESTIN, S., LAFORTUNE, E., TORRANCE, K., AND GREEN-BERG, 
D. 2000. Image-Based BRDF measurement including human skin. In EGRW 00, 139 152. MCCOOL, M., ANG, J., 
AND AHMAD, A. 2001. Homomorphic factorization of BRDFs for high-performance rendering. In SIGGRAPH 01, 
171 178. MILLER, G., AND HOFFMAN, C. 1984. Illumination and re.ection maps: Simulated objects in simulated 
and real environments. SIGGRAPH 84 Advanced Computer Graphics Animation seminar notes. MOHLENKAMP, M. 
1999. A fast transform for spherical harmonics. The Journal of Fourier Analysis and Applications 5, 2/3, 
159 184. NISHINO, K., SATO,Y., AND IKEUCHI, K. 1999. Eigen-texture method: Appearance compression based 
on 3D model. In CVPR 99, 618 624. PROUDFOOT, K., MARK,W., TZVETKOV, S., AND HANRAHAN, P. 2001. A real­time 
procedural shading system for programmable graphics hardware. In SIG-GRAPH 01, 159 170. RAMAMOORTHI, 
R., AND HANRAHAN, P. 2001. An ef.cient representation for irradiance environment maps. In SIGGRAPH 01, 
497 500. RAMAMOORTHI, R., AND HANRAHAN, P. 2001. On the relationship between radi­ance and irradiance: 
Determining the illumination from images of a convex lam­bertian object. JOSA A 18, 10, 2448 2459. RAMAMOORTHI, 
R., AND HANRAHAN, P. 2001. A signal-processing framework for inverse rendering. In SIGGRAPH 01, 117 128. 
RUSINKIEWICZ, S. 1998. A new change of variables for ef.cient BRDF representa­tion. In EGRW 98, 11 22. 
SCHR ¨ ODER,P., AND SWELDENS, W. 1995. Spherical wavelets: Texture processing. In EGRW 95, 252 263. SILLION,F., 
ARVO, J., WESTIN, S., AND GREENBERG, D. 1991. A global illumi­nation solution for general re.ectance 
distributions. In SIGGRAPH 91, 187 196. TORRANCE, K., AND SPARROW, E. 1967. Theory for off-specular re.ection 
from roughened surfaces. JOSA 57, 9, 1105 1114. WESTIN, S., ARVO, J., AND TORRANCE, K. 1992. Predicting 
re.ectance functions from complex surfaces. In SIGGRAPH 92, 255 264. WOOD, D., AZUMA, D., ALDINGER, K., 
CURLESS, B., DUCHAMP,T., SALESIN, D., AND STUETZLE, W. 2000. Surface light .elds for 3D photography. 
In SIG-GRAPH 00, 287 296.  Appendix A: Spherical signal processing We summarize the important spherical 
signal processing results derived by Ra­ mamoorthi and Hanrahan [2001b; 2001c]. This analysis uses spherical 
harmon­ics [MacRobert 1948], denoted here by Ylm(., f)= flm(.)e Imf. We .rst con­sider the expansion 
of the BRDF. Here, f = fo - fi, and . lq,pq = .l-q,p-q = . lpq. Uq is a constant = 1 when q =0and 2 otherwise. 
88 min(l,p) XXX* . (.i,fi,.o,fo)= . lq,pq Ylq(.i,fi)Ypq(., fo) l=0 p=0 q=- min(l,p) 88 min(l,p) XXX . 
(.i,.o,f)= Uq. lpqflq(.i)fpq(.o)cosqf l=0 p=0 q=0 ZZZ pp 2p . lpq = K(.i,.o,f) .(.i,.o,f)d.id.odf .i=0 
.o=0 f=0 K(.i,.o,f)=2pflq(.i)fpq(.o)sin.i sin.o cosqf (13) The re.ected light .eld B(a, ß, .o,fo), de.ned 
by equation 2 in the angular do­main, may now be computed as follows in the frequency domain: 8 l 8 p 
-1 l Imß l XXXX B(a, ß, .o,fo)= Blmpq.Dmq(a)eYpq(.o,fo) l=0 m=-lp=0 q=-p Blmpq =.lLlm. lpq To avoid needing 
the constants .l and .-1, we will simply (re)de.ne the coef­ l .cients Blmpq = Llm. lpq in what follows. 
We may then expand the coef.cients Blmpq to compute the local SHRM Bpq(a, ß). 8 l .. XX l Imß Bpq(a, 
ß)= Blmpq D(a)e mq l=0 m=-l 8 p XX B(a, ß, .o,fo)= Bpq(a, ß)Ypq(.o,fo) (14) p=0 q=-p  Appendix B: Bound 
on residual energy Consider an expansion to order F with errors for lighting and BRDF given by L and 
. . Denote the total BRDF and lighting energies by . tot and Ltot. Since the re.ected light .eld coef.cients 
are simply a product of lighting and BRDF terms, the worst case for the residual energy occurs when it 
is all concentrated in mode F +1. This residual energy, denoted by Bres, and a conservative error estimate 
B are Bres =.F +1 (LLtot)(. . tot) Bres B = PP Fl l=0 m=-l |Blm|2 +Bres This is for the radially symmetric 
case; in general, we simply use Blmpq in place of Blm. Note that B tends to 0as Bres tends to 0. But 
the latter quantity is a product of . and L, and therefore always tends to 0 as F increases. Appendix 
C: Pre.ltering Algorithm Step 1. Compute lighting and BRDF coef.cients: We .rst compute the spherical 
harmonic coef.cients of the BRDF using the following three step algorithm that ef.ciently implements 
equation 13. Z 2p . q (.i,.o)=2p. (.i,.o,f)cosqf df 0 Z p . pq(.i)= . q (.i,.o)fpq(.o)sin.o d.o 0 Z p 
. lpq = . pq(.i)flq (.i)sin.i d.i 0 The computational costs of the three terms in the above sequence 
are, respectively, O(P. S. T. 2), O(P 2S. T. ), and O(F. P. 2S. ). Since P. <T. , the .rst term . .. 
dominates the second, and the net cost is OP. S. (T. 2 +F. P. ) . For most non radially-symmetric BRDFs, 
T2 >F. P. (for instance, use T. ~ 10, F. ~ 20and . P. ~ 3), so the .rst term dominates and the total 
cost is O(P. S. T. 2). If our error tolerance . is satis.ed, we see how far P. can be reduced to still 
satisfy the error tolerance, and then also reduce F. as much as possible. We can then set FB and PB according 
to the minimal values of F. and P. . If the error is initially larger than , we repeat the algorithm 
with larger values for F. and P. . Since computing BRDF coef.cients is not the dominant algorithm cost, 
this recomputation does not signi.cantly affect the total time, nor does using large initial values for 
F. and P. . Finally, we compute the lighting coef.cients in time O(FB S2 ). Note that we Lhave already 
determined FB , so we are not required to consider higher frequencies for the lighting, which is why 
we use FB instead of FL. Step 2. Find re.ected light .eld coef.cients: We now .nd Blmpq by directly using 
equation 5 in time O(F2 P 2 ). BB Step 3. Compute SHRM: We now compute the local SHRM by ef­.ciently 
implementing equation 14. From this, we can compute the global SHRM B pq(a, ß)using equation 8. FB Xl 
mq Bmpq(a)= BlmpqD(a) l=|m| X FB Imß Bpq(a, ß)= Bmpq(a)e m=-FB The costs of the two terms in the above 
sequence are O(F2 P 2 SB) and BBO(FBP 2 S2 ). Since SB >FB, the net cost is O(FBP 2 S2 ). The cost for 
BBBB this step is also the dominant cost for the entire algorithm. Radially symmetric BRDFs: For the 
special case of BRDFs like Lam­bertian and Phong models, T. =1and P. = PB =0. Technically, the complexity 
formulae above should use P +1instead of P , to yield meaningful results for radi­ally symmetric BRDFs. 
For these models, step 1 takes time of O(F. S. )to compute BRDF coef.cients . l, and time O(FB S2 ) to 
compute lighting coef.cients Llm. L Step 2 takes O(F2 ) time. Finally, the SHRM in step 3 includes only 
the constant B term and is therefore a simple re.ection map B(a, ß), computed in time O(FBS2 ). B The 
dominant cost here is to convert to and from spherical harmonic representations. Assuming we downsample 
the environment map if necessary so SL ~ SB, the total time is O(FB S2 )or O(FB)per output image pixel. 
B Conversion between SHRMs and explicit forms: It is pos­sible to incorporate the pre.ltering and rendering 
phases of our algorithm separately into existing systems. SHRMs may be created from explicit representations 
simply by .tting coef.cients or integrating. If the implementer wants to use only our fast pre­.ltering 
method, but render using previous explicit representations, they can compute tabular representations 
from SHRMs. Cabral s twelve prerendered re.ection maps may be computed very rapidly using equation 12, 
with (. o,f o)set to vertices of an icosahedron. Kautz and McCool s [2000] 3D texture is computed by 
expanding PB X B(a, ß, .o)= Bp0(a, ß)Yp0(.o) p=0 This takes time O(PB)per output texel. Using fast conversion 
methods, we can also explicitly generate TB × TB re.ection maps (a full 4D light .eld) in time O(PB ) 
per output pixel, for a total cost of O(PBT 2 S2 ). BB Appendix D: Costs for Phong BRDF In the frequency 
domain, the order F =FB =F. for an error . is found by F 8 X2 X2 . =(1- ) . Z ll l=0 l=0 F Z8 -l2/s -l2/s 
ledl (1- ) ledl 00 -F 2/s 1- e 1- . p F -s log. In the angular domain, we may truncate the BRDF, so 
(1 - ) of the angular domain energy lies in .i = . *. We .nd the angular width W and . * by ii ZZ . * 
p/2 is s cos .i sin.i d.i = (1- ) cos .i sin.i d.i 00 s+1 * 1- cos . =1- i * 1/(s+1) cos. = i Z . * 
. . 2p i 1 1/(s+1) W = sin.i d.i =1- 4p 2 0 We now assume s is large (1/s . 0) and perform a Taylor series 
expansion: .... .. log. - log 1/(s+1) W ~ 1- =1- 1+ s +1 s  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566612</article_id>
		<sort_key>527</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments]]></title>
		<page_from>527</page_from>
		<page_to>536</page_to>
		<doi_number>10.1145/566570.566612</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566612</url>
		<abstract>
			<par><![CDATA[We present a new, real-time method for rendering diffuse and glossy objects in low-frequency lighting environments that captures soft shadows, interreflections, and caustics. As a preprocess, a novel global transport simulator creates functions over the object's surface representing transfer of arbitrary, low-frequency incident lighting into <i>transferred radiance</i> which includes global effects like shadows and interreflections from the object onto itself. At run-time, these transfer functions are applied to actual incident lighting. Dynamic, local lighting is handled by sampling it close to the object every frame; the object can also be rigidly rotated with respect to the lighting and vice versa. Lighting and transfer functions are represented using low-order spherical harmonics. This avoids aliasing and evaluates efficiently on graphics hardware by reducing the shading integral to a dot product of 9 to 25 element vectors for diffuse receivers. Glossy objects are handled using matrices rather than vectors. We further introduce functions for radiance transfer from a dynamic lighting environment through a preprocessed object to neighboring points in space. These allow soft shadows and caustics from rigidly moving objects to be cast onto arbitrary, dynamic receivers. We demonstrate real-time global lighting effects with this approach.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Monte Carlo techniques]]></kw>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[shadow algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P225694</person_id>
				<author_profile_id><![CDATA[81100524617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter-Pike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sloan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022735</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48023369</person_id>
				<author_profile_id><![CDATA[81100167784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344954</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGRAWALA, M, RAMAMOORTHI, R, HEIRICH, A, AND MOLL, L, Efficient Image-Based Methods for Rendering Soft Shadows, SIGGRAPH '00, 375-384.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AIREY, J, ROHLF, J, AND BROOKS, F, Towards Image Realism with Interactive Update Rates in Complex Virtual Building Environments,1990 Symposium on Interactive 3D Graphics, 24(2), 41-50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>504790</ref_obj_id>
				<ref_obj_pid>504789</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ASHIKHMIN, M, AND SHIRLEY, P, Steerable Illumination Textures, ACM Transactions on Graphics, 2(3), to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B, MAX, N, AND SPRINGMEYER, R, Bidirectional Reflection Functions from Surface Bump Maps, SIIGRAPH '87, 273-281.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311553</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B, OLANO, M, AND NEMEC, P, Reflection Space Image Based Rendering, SIGGRAPH '99, 165-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COHEN, M, AND WALLACE, J, Radiosity and Realistic Image Synthesis, Academic Press Professional, Cambridge, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COOK, R, PORTER, T, AND CARPENTER, L, Distributed Ray Tracing, SIGGRAPH '84, 137-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P, Rendering Synthetic Objects into Real Scenes: Bridging Traditional and Image-Based Graphics with Global Illumination and High Dynamic Range Photogaphy, SIGGRAPH '98, 189-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P, HAWKINS, T, TCHOU, C, DUIKER, H, SAROKIN, W, AND SAGAR, M, Acquiring the Reflectance Field of a Human Face, SIGGRAPH 2000, 145-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DOBASHI, Y, KANEDA, K, NAKATANI, H, AND YAMASHITA, H, A Quick Rendering Method Using Basis Functions for Interactive Lighting Design, Eurographics '95, 229-240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122723</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DORSEY, J, SILLION, F, AND GREENBERG, D, Design and Simulation of Opera Lighting and Projection Effects, SIGGRAPH '91, 41-50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D'ZMURA, M, Shading Ambiguity: Reflection and Illumination. In Computational Models of Visual Processing (1991), Landy and Movshon, eds., MIT Press, Cambridge, 187-207.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[EDMONDS, A, Angular Momentum in Quantum Mechanics, Princeton University, Princeton, 1960.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GREENE, N, Environment Mapping and Other applications of World Projections, IEEE CG&A, 6(11):21-29, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GREGER, G, SHIRLEY, P, HUBBARD, P, AND GREENBERG, D, The Irradiance Volume, IEEE Computer Graphics And Applications, 6(11):21-29, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732281</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HAKURA, Z, AND SNYDER, J, Realistic Reflections and Refractions on Graphics Hardware with Hybrid Rendering and Layered Environment Maps, Eurographics Workshop on Rendering, 2001, 289-300.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P, AND AKELEY, K, The Accumulation Buffer: Hardware Support for High-Quality Rendering, SIGGRAPH '90, 309-318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W, LENSCH, H, COHEN, M, AND SEIDEL, H, Light Field Techniques for Reflections and Refractions, Eurographics Rendering Workshop 99, 195-375.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W, SEIDEL H, Realistic, Hardware-Accelerated Shading and Lighting, SIGGRAPH '99, 171-178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344984</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W, DAUBERT, K, KAUTZ, J, AND SEIDEL, H, Illuminating Micro Geometry based on Precomputed Visibility, SIGGRAPH '00, 455-464.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H, Global Illumination using Photon Maps, Eurographics Workshop on Rendering 1996, 21-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H, MARSCHNER, S, LEVOY, M, AND HANRAHAN, P, A Practical Model for Subsurface Light Transport, SIGGRAPH '01, '511-518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383838</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J, AND MCCOOL, M, Interactive Rendering with Arbitrary BRDFs using Separable Approximations, Eurographics Workshop on Rendering 99,.247-260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732274</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[KAUTZ, J, VAZQUEZ, P, HEIDRICH, W, AND SEIDEL, H, A Unified Approach to Prefiltered Environment Maps, Eurographics Workshop on Rendering 2000, 185-196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J, The Rendering Equation, SIGGRAPH '86, 143-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383834</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[KEATING, B, AND MAX, N, Shadow Penumbras for Complex Objects by Depth-Dependent Filtering of Multi-Layer Depth Images, Eurographics Rendering Workshop, 1996, pp.205-220.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258769</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[KELLER, A, Instant Radiosity, SIGGRAPH '97, 49-56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[LINDE, Y, BUZO, A, AND GRAY, R, An algorithm for Vector Quantizer Design, IEEE Transactions on Communication COM-28, 1980,84-95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[LOKOVIC, T, AND VEACH, E, Deep Shadow Maps, SIGGRAPH '00, pp.385-392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T, GELB, D, AND WOLTERS, H, Polynomial Texture Maps, SIGGRAPH '01, 519-528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>23954</ref_obj_id>
				<ref_obj_pid>23944</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[MAX, N, Horizon Mapping: Shadows for Bump-Mapped Surfaces, The Visual Computer, July 1998, 109-117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192244</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[MILLER, G, Efficient Algorithms for Local and Global Accessibility Shading, SIGGRAPH '94, 319-326.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[NIMEROFF, J, SIMONCELLI, E, AND DORSEY, J, Efficient Re-rendering of Natural Environments, Eurographics Workshop on Rendering 1994, 359-373.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383317</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R, AND HANRAHAN, P, An Efficient Representation for Irradiance Environment Maps, SIGGRAPH '01, 497-500.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[REEVES, W, SALESIN, D, AND COOK, R, Rendering Antialiased Shadows with Depth Maps, SIGGRAPH '87, '283-291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[SEGAL, M, KOROBKIN, C, VAN WIDENFELT, R, FORAN, J, AND HAEBERLI, P, Fast Shadows and Lighting Effects Using Texture Mapping, SIGGRAPH '92, '249-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[SCHR&#214;DER, P, AND SWELDENS, W, Spherical Wavelets: Efficiently Representing the Sphere, SIGGRAPH '95, '161 - 172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122739</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[SILLION, F, ARVO, J, WESTIN, S, AND GREENBERG, D, A Global Illumination Solution for General Reflectance Distributions, SIGGRAPH '91, 187-196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280927</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[SOLER, C, AND SILLION, F, Fast Calculation of Soft Shadow Textures Using Convolution, SIGGRAPH '98, '321-332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>892215</ref_obj_id>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[TEO, P, SIMONCELLI, E, AND HEEGER, D, Efficient Linear Re-rendering for Interactive Lighting Design, October 1997 Report No. STAN-CS-TN-97-60, Stanford University, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[WARD, G, RUBINSTEIN, F, AND CLEAR, R, A Ray Tracing Solution for Diffuse Interreflection, SIGGRAPH '88, '85-92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[WESTIN, S, ARVO, J, TORRANCE, K, Predicting Reflectance Functions from Complex Surfaces, SIGGRAPH '92, 255-264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L, Casting Curved Shadows on Curved Surfaces, SIGGRAPH '78, 270-274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[ZARE, R, Angular Momentum: Understanding Spatial Aspects in Chemistry and Physics, Wiley, New York, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environments 
 Peter-Pike Sloan Jan Kautz John Snyder Microsoft Research Max-Planck-Institut für Informatik Microsoft 
Research ppsloan@microsoft.com jnkautz@mpi-sb.mpg.de johnsny@microsoft.com Abstract We present a new, 
real-time method for rendering diffuse and glossy objects in low-frequency lighting environments that 
cap­tures soft shadows, interreflections, and caustics. As a preprocess, a novel global transport simulator 
creates functions over the object s surface representing transfer of arbitrary, low-frequency incident 
lighting into transferred radiance which includes global effects like shadows and interreflections from 
the object onto itself. At run-time, these transfer functions are applied to actual incident lighting. 
Dynamic, local lighting is handled by sampling it close to the object every frame; the object can also 
be rigidly rotated with respect to the lighting and vice versa. Lighting and transfer functions are represented 
using low-order spherical harmonics. This avoids aliasing and evaluates efficiently on graphics hardware 
by reducing the shading integral to a dot product of 9 to 25 element vectors for diffuse receivers. Glossy 
objects are handled using matrices rather than vectors. We further introduce functions for radiance transfer 
from a dynamic lighting environment through a preprocessed object to neighboring points in space. These 
allow soft shadows and caustics from rigidly moving objects to be cast onto arbitrary, dynamic receivers. 
We demonstrate real-time global lighting effects with this approach. Keywords: Graphics Hardware, Illumination, 
Monte Carlo Techniques, Rendering, Shadow Algorithms. 1. Introduction Lighting from area sources, soft 
shadows, and interreflections are important effects in realistic image synthesis. Unfortunately, general 
methods for integrating over large-scale lighting environ­ments [8], including Monte Carlo ray tracing 
[7][21][25], rad­iosity [6], or multi-pass rendering that sums over multiple point light sources [17][27][36], 
are impractical for real-time rendering. Real-time, realistic global illumination encounters three difficul­ties 
 it must model the complex, spatially-varying BRDFs of real materials (BRDF complexity), it requires 
integration over the hemisphere of lighting directions at each point (light integration), and it must 
account for bouncing/occlusion effects, like shadows, due to intervening matter along light paths from 
sources to receiv­ers (light transport complexity). Much research has focused on extending BRDF complexity 
(e.g., glossy and anisotropic reflec­tions), solving the light integration problem by representing incident 
lighting as a sum of directions or points. Light integra­tion thus tractably reduces to sampling an analytic 
or tabulated BRDF at a few points, but becomes intractable for large light sources. A second line of 
research samples radiance and pre­convolves it with kernels of various sizes [5][14][19][24][34]. This 
solves the light integration problem but ignores light trans­port complexities like shadows since the 
convolution assumes the incident radiance is unoccluded and unscattered. Finally, clever techniques exist 
to simulate more complex light transport, espe­cially shadows. Light integration becomes the problem; 
these techniques are impractical for very large light sources. Our goal is to better account for light 
integration and light trans­port complexity in real-time. Our compromise is to focus on low- Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 Figure 1: Precomputed, unshadowed irradiance from [34] (left) vs. our precomputed transfer (right). 
The right model can be rendered at 129Hz with self-shadowing and self-interreflection in any lighting 
environment. frequency lighting environments, using a low-order spherical harmonic (SH) basis to represent 
such environments efficiently without aliasing. The main idea is to represent how an object scatters 
this light onto itself or its neighboring space. To describe our technique, assume initially we have 
a convex, diffuse object lit by an infinitely distant environment map. The object s shaded response to 
its environment can be viewed as a transfer function, mapping incoming to outgoing radiance, which in 
this case simply performs a cosine-weighted integral. A more complex integral captures how a concave 
object shadows itself, where the integrand is multiplied by an additional transport factor representing 
visibility along each direction. Our approach is to precompute for a given object the expensive transport 
simulation required by complex transfer functions like shadowing. The resulting transfer functions are 
represented as a dense set of vectors or matrices over its surface. Meanwhile, incident radiance need 
not be precomputed. The graphics hard­ware can dynamically sample incident radiance at a number of points. 
Analytic models, such as skylight models [33] or simple geometries like circles, can also be used. By 
representing both incident radiance and transfer functions in a linear basis (in our case, SH), we exploit 
the linearity of light transport to reduce the light integral to a simple dot product between their coefficient 
vectors (diffuse receivers) or a simple linear transform of the lighting coefficient vector through a 
small transfer matrix (glossy receivers). Low-frequency lighting envi­ronments require few coefficients 
(9-25), enabling graphics hardware to compute the result in a single pass (Figure 1, right). Unlike Monte-Carlo 
and multi-pass light integration methods, our run-time computation stays constant no matter how many 
or how big the light sources, and in fact relies on large-scale, smooth lighting to limit the number 
of SH coefficients necessary. We represent complex transport effects like interreflections and caustics 
in the transfer function. Since these are simulated as a preprocess, only the transfer function s basis 
coefficients are affected, not the run-time computation. Our approach handles both surface and volume-based 
geometry. With more SH coeffi­cients, we can even handle glossy (but not highly specular) receivers as 
well as diffuse, including interreflection. 25 coeffi­cients suffice for useful glossy effects. In addition 
to transfer from a rigid object to itself, called self-transfer, we generalize the technique to neighborhood-transfer 
from a rigid object to its neighboring space, allowing cast soft shadows, glossy reflections, and caustics 
on dynamic receivers, see Figure 7. Overview As a preprocess, a global illumination simulator is run 
over the model that captures how it shadows and scatters light onto itself. The result is recorded as 
a dense set of vectors (diffuse case) or matrices (glossy case) over the model. At run-time (Figure 2), 
incident radiance is first projected to the SH basis. The model s field of transfer vectors or matrices 
is then applied to the lighting s coefficient vector. If the object is diffuse, a transfer vector at 
each point on the object is dotted with the lighting s coefficients to produce correctly self-scattered 
shading. If the object is glossy, a transfer matrix is applied to the lighting coefficients to produce 
the coefficients of a spherical function representing self-scattered incident radiance at each point. 
This function is convolved with the object s BRDF and then evaluated at the view-dependent reflec­tion 
direction to produce the final shading.  2. Related Work Scene relighting precomputes a separate global 
illumi­nation solution per light source as we do; linear combinations of the results then provide limited 
dy­namic effects. Early work [2][11] adjusts intensities of a fixed set of sources and is not intended 
to fit general lighting environments. Nimeroff, et al. [33] precompute a steerable function basis for 
general skylight illumi­nation on a fixed view. Their basis, essentially the spherical monomials, is 
related to the SH by a linear glossy surface self-transfer transformation and thus shares some of its 
properties Figure 2: Self-Transfer Run-Time Overview. Red signifies positive SH coefficients (e.g., rotational 
invariance) but not others (e.g., or-and blue, negative. For a diffuse surface (top row), the SH lighting 
coefficients (on the thonormality). Teo, et al. [40] generalize to non-infinite left) modulate a field 
of transfer vectors over the surface (middle) to produce the final sources, using principal component 
analysis to reduce result (right). A transfer vector at a particular point on the surface represents 
how the the basis set. Our work differs by computing a transfer surface responds to incident light at 
that point, including global transport effects like self-shadowing and self-interreflection.. For a glossy 
surface (bottom row), there is a field over the object s surface in 3D rather than over a matrix at 
each point on the model instead of a vector. This matrix transforms the light­ fixed 2D view to allow 
viewpoint changes. Dobashi, et ing coefficients into the coefficients of a spherical function representing 
transferred al. [10] use the SH basis and transfer vector fields over radiance. The result is convolved 
with the model s BRDF kernel and evaluated at the surfaces to allow viewpoint change but restrict lighting 
 view-dependent reflection direction R to yield the result at one point on the model.  changes to the 
directional intensity distribution of an existing set of non-area light sources in diffuse scenes. Debevec, 
et al. [9] relight faces using a directional light basis. Real-time rendering requires a fixed view. 
 Shadow maps, containing depths from the light source s point of view, were first used by Williams [43] 
to simulate point light source shadows. Many extensions of the basic technique, some suitable for real-time 
rendering, have since been described: percentage-closer filtering [35], which softens shadow edges, layered 
depth maps [26] and layered attenuation maps [1], which more accurately simulate penumbra shape and falloff, 
and deep shadow maps [29], which generalize the technique to partially transparent and volume geometry. 
All these techniques assume point or at least localized light sources; shadowing from larger light sources 
has been handled by multi-pass rendering that sums over a light source decomposition into points or small 
sources [17][27][36]. Large light sources become very expensive. Another technique [39] uses FFT convolution 
of occluder projec­tions for soft shadowing with cost independent of light source size. Only shadows 
between pre-segmented clusters of objects are handled, making self-shadows on complex meshes difficult. 
Finally, accessibility shading [32] is also based on precomputed global visibility, but is a scalar quantity 
that ignores changes in lighting direction. Methods for nonlocal lighting on micro-geometry include the 
horizon map [4][31], which efficiently renders self-shadowing from point lights. In [20], this technique 
is tailored to graphics hardware and generalized to diffuse interreflections, though interreflection 
change due to dynamic lighting is still not real­time. By precomputing a higher-dimensional texture, 
polynomial texture maps [30] allow real-time interreflection effects as well as shadowing. A similar 
approach using a steerable basis for direc­tional lighting is used in [3]. Like our approach, these methods 
precompute a simple representation of a transfer function, but one based on directional light sources 
and thus requiring costly multi­pass integration to simulate area lights. We compute self-transfer directly 
on each preprocessed 3D object rather than mapping it with 2D micro-geometry textures, allowing more 
global effects. Finally, our neighborhood transfer extends these ideas to cast shadows, caustics, and 
reflections. Caching onto diffuse receivers is useful for accelerating global illumination. Ward et. 
al. [41] perform caching to simulate diffuse interreflection in a ray tracer. Photon maps [21] also cache 
but perform forward ray tracing from light sources rather than backwards from the eye, and handle specular 
bounces in the transport (as does our approach). We apply this caching idea to real-time rendering, but 
cache a transfer function parameterized by a SH lighting basis rather than scalar irradiance. Precomputed 
transfer using light-field remapping [18] and dynamic ray tracing [16] has been used to achieve highly 
specular reflections and refractions. We apply a similar precomputed, per­object decomposition but designed 
instead for soft shadows and caustics on nearly diffuse objects in low-frequency lighting. Irradiance 
volumes [15] allow movement of diffuse receivers in precomputed lighting. Unlike our approach, lighting 
is static and the receiver s effect on itself and its environment is ignored. Spherical harmonics have 
been used to represent incident radi­ance and BRDFs for offline rendering and BRDF inference [4] [38][42]. 
Westin, et al. [42] use a matrix representation for 4D BRDF functions in terms of the SH basis identical 
to our transfer matrix. But rather than the BRDF, we use it to represent global and spatially varying 
transport effects like shadows. The SH basis has also been used to solve ambiguity problems in computer 
vision [12] and to represent irradiance for rendering [34].  3. Review of Spherical Harmonics Definition 
Spherical harmonics define an orthonormal basis over the sphere, S, analogous to the Fourier transform 
over the 1D circle. Using the parameterization s = ( x, y, z ) = (sinq cosj , sinq sinj , cosq ) , the 
basis functions are defined as m mim || Y (, )= Ke j P m (cs )q ,l N ,-l £ qj om £ l l ll where Plm 
are the associated Legendre polynomials and Klm are the normalization constants m )! (2l + 1) (l - Klm 
= . 4p (l + m )! The above definition forms a complex basis; a real-valued basis is given by the simple 
transformation m mm Ï 2 Re(Yl ), m > 0 Ï 2 Kl cos(mj )Pl (cosq ), m > 0 m Ô m Ô m -m yl =Ì 2 Im(Yl 
), m < 0 =Ì 2 Kl sin(-mj )Pl (cosq ), m < 0 Ô 0 Ô 00 Y , m = 0 KP (cosq ), m = 0 l ll ÔÓ ÔÓ Low values 
of l (called the band index) represent low-frequency basis functions over the sphere. The basis functions 
for band l reduce to polynomials of order l in x, y, and z. Evaluation can be done with simple recurrence 
formulas [13][44]. Projection and Reconstruction Because the SH basis is or­thonormal, a scalar function 
f defined over S can be projected into its coefficients via the integral lm = f ()sylm () f sds (1) Ú 
These coefficients provide the n-th order reconstruction function n -1 l ] ()= flm ylm () fs ÂÂ s (2) 
l = 0 m=-l which approximates f increasingly well as the number of bands n increases. Low-frequency signals 
can be accurately represented with only a few SH bands. Higher frequency signals are bandlim­ited (i.e., 
smoothed without aliasing) with a low-order projection. Projection to n-th order involves n 2 coefficients. 
It is often con­venient to rewrite (2) in terms of a singly-indexed vector of projection coefficients 
and basis functions, via 2 ]n ()= i () (3) fs Â i fys i =1 where i=l(l+1)+m+1. This formulation makes 
it obvious that evaluation at s of the reconstruction function represents a simple dot product of the 
n 2-component coefficient vector fi with the vector of evaluated basis functions yi(s). Basic Properties 
A critical property of SH projection is its rotational invariance; that is, given g ()= f Qs )where Q 
is an s ( () arbitrary rotation over S then ] g ()= f ( () (4) ]s Qs ) This is analogous to the shift-invariance 
property of the 1D Fourier transform. Practically, this property means that SH projection causes no aliasing 
artifacts when samples from f are collected at a rotated set of sample points. Orthonormality of the 
SH basis provides the useful property that given any two functions a and b over S, their projections 
satisfy 2 b n asb() () s= ii . Ú sdÂ ab (5) i =1 In other words, integration of the product of bandlimited 
functions reduces to a dot product of their projection coefficients. Convolution We denote convolution 
of a circularly symmetric kernel function h(z) with a function f as *. hfNote that h must be circularly 
symmetric (and hence can be defined as a simple function of z rather than s) in order for the result 
to be defined on S rather than the higher-dimensional rotation group SO(3). Projection of the convolution 
satisfies m 4p 0 m 00 m hf hf = a hf . (6) ( *)l = ll lll 21 l+ In other words, the coefficients of the 
projected convolution are simply scaled products of the separately projected functions. Note that because 
h is circularly symmetric about z, its projection coefficients are nonzero only for m=0. The convolution 
property provides a fast way to convolve an environment map with a hemispherical cosine kernel, defined 
as hz() = max(z,0) , to get an irradiance map [34], for which the hl0 are given by an analytic formula. 
The convolution property can also be used to produce prefiltered environment maps with narrower kernels. 
Product Projection Projection of the product of a pair of spheri­cal functions cs()=asbs () () where 
a is known and b unknown can be viewed as a linear transformation of the projection coeffi­cients bj 
via a matrix a : ci = Ú a()( jj ()) i ()ds s bys ys ( Ú )( Ú) (7) ()() ()dsb= a () ()()db= = asysysysysyssab 
ij jkijk jij j where summation is implied over the duplicated j and k indices. Note that a is a symmetric 
matrix. The components of a can be computed by integrating the triple product of basis functions using 
recurrences derived from the well-known Clebsch-Gordan series [13][44]. It can also be computed using 
numerical integration without SH-projecting the function a beforehand. Note that the product s order 
n projection involves coefficients of the two factor functions up to order 2n-1. ] Rotation A reconstruction 
function rotated by Q, fQ()) , can ( s be projected into SH using a linear transformation of f s projec­tion 
coefficients, fi. Because of the rotation invariance property, this linear transformation treats the 
coefficients in each band independently. The most efficient implementation is achieved via a zyz Euler 
angle decomposition of the rotation Q, using a fairly complicated recurrence formula [13][44]. Because 
we deal only with low-order functions, we have implemented their explicit rotation formulas using symbolic 
integration. 4. Radiance Self-Transfer Radiance self-transfer encapsulates how an object O shadows and 
scatters light onto itself. To represent it, we first parameterize incident lighting at points p O , 
denoted Lp(s), using the SH basis. Incident lighting is therefore represented as a vector of n 2 coefficients 
(Lp)i. We sample the lighting dynamically and sparsely near the surface, perhaps at only a single point. 
The assumption is that lighting variation over O not due to its own presence is small (see Section 6.1). 
We also precompute and store densely over O transfer vectors or matrices. A transfer vector (Mp)i is 
useful for diffuse surfaces and repre­sents a linear transformation on the lighting vector producing 
scalar exit radiance, denoted Lp ¢ , via the inner product 2 n L¢= Â( M )( L ) . (8) p pi pi i =1 (a) 
unshadowed (b) shadowed (c) interreflected Figure 3: Diffuse Surface Self-transfer. In other words, each 
component of (Mp)i represents the linear influence that a lighting basis function (Lp)i has on shading 
at p. A transfer matrix (Mpi)jis useful for glossy surfaces and repre­sents a linear transformation on 
the lighting vector which produces projection coefficients for an entire spherical function of transferred 
radiance L¢() p s rather than a scalar; i.e., 2 n ()pi = ()j()L . L¢ Â Mpipj (9) j=1 The difference between 
incident and transferred radiance is that L p ¢()s includes shadowing/scattering effects due to the presence 
of O while Lp(s) represents incident lighting assuming O was removed from the scene. Components of (Mpi)j 
represent the linear influence of the j-th lighting coefficient of incident radiance (Lp)j to the i-th 
coefficient of transferred radiance (). The next L¢ pi sections derive transfer vectors for diffuse surfaces 
and transfer matrixes for glossy surfaces due to self-scattering on O. 4.1 Diffuse Transfer [transfer 
vector for known normal] First assume O is diffuse. The simplest transfer function at a point p O represents 
unshadowed diffuse transfer, defined as the scalar function () s ds TDU (L p )= ( rp ) L p sH Np () p 
Ú producing exit radiance which is invariant with view angle for diffuse surfaces. Here, r p is the object 
s albedo at p, Lp is the incident radiance at p assuming O was removed from the scene, N is the object 
s normal at p, and Hs = max(N i s,)is the () 0 pNp p cosine-weighted, hemispherical kernel about Np. 
By SH­projecting Lp and HNp separately, equation (5) reduces TDU to an inner product of their coefficient 
vectors. We call the resulting factors the light function, Lp, and transfer function, Mp. In this pDU 
s Np (). first simple case, M ()= Hs Because Np is known, the SH-projection of the transfer function 
DU ) ( M pi can be precomputed, resulting in a transfer vector. In fact, storing is unnecessary because 
a simple analytic formula yields it given Np. Because M pDU is inherently a low-pass filter, second-order 
projection (9 coefficients) provides good accuracy in an arbitrary (even non-smooth) lighting environment 
[34]. To include shadows, we define shadowed diffuse transfer as TDS (L p )= ( rp ) L p sH Np () p s 
ds () sV () p Ú where the additional visibility function, Vsp ()Æ {0,1}, equals 1 when a ray from p in 
the direction s fails to intersect O again (i.e., is unshadowed). As with unshadowed transfer, we decompose 
this integral into two functions, using an SH-projection of Lp and the transfer function DS s sV () M 
() =H () s . (10) be inaccurate even for smooth lighting environments since Vp can create higher-frequency 
lighting locally, e.g., by self-shadowing pinholes . 4-th or 5-th order projection provides good results 
on typical meshes in smooth lighting environments. Finally, to capture diffuse interreflections as well 
as shadows, we define interreflected diffuse transfer as ()= ()+ ( rp L()Hs -V() TL TL ) s()1 ( s) ds 
DIp DS p p p Np p Ú where L p ()is the radiance from O itself in the direction s. The s difficulty is 
that unless the incident radiance emanates from an infinitely-distant source, we don t actually know 
L p () s given the incident radiance only at p because Lp depends on the exit radiance of points arbitrarily 
far from p and local lighting varies over O. If lighting variation is small over O then L p is well­approximated 
as if O were everywhere illuminated by Lp. TDI thus depends linearly on Lp and can be factored as in 
the previous two cases into a product of two projected functions: one light­dependent and the other geometry-dependent. 
Though precomputed interreflections must make the assumption of spatially invariant incident lighting 
over O, simpler shadowed transfer need not. The difference is that shadowed transfer de­pends only on 
incident lighting at p, while interreflected transfer depends on many points qpover O at which L q p 
L p . Thus, p as long as the incident radiance field is sampled finely enough (Section 6.1), local lighting 
variation can be captured and shad­owed transfer will be correct. The presence of L makes it hard to 
explicitly denote the transfer function for interreflections, M p s . We will see how to com- DI () pute 
its projection coefficients numerically in Section 5. 4.2 Glossy Transfer [transfer matrix for unknown 
direction] Self-transfer for glossy objects can be defined similarly, but generalizes the kernel function 
to depend on a (view-dependent) reflection direction R rather than a (fixed) normal N. Analogous to the 
H kernel from before, we model glossy reflection as the kernel G(s,R,r) where a scalar r defines the 
glossiness or broad­ness of the specular response. We believe it is possible to handle arbitrary BRDFs 
as well using their SH projection coefficients [38] but this remains for future work. We can then define 
the analogous three glossy transfer functions for the unshadowed, shadowed, and interreflected cases 
as T ( ,,) = L sGsRr ds L Rr () (,,) GUp p Ú (L Rr , ,) = () (,,) () s T L sGsRrV sd GSpp p Ú T (,,) 
= s LRr T (L)+ L() (, ,)1 s GsRr ( -V() ) ds GIp GS pp p Ú which output scalar radiance in direction 
R as a function of Lp and R, quantities both unknown at precomputation time. Since trans­fer is no longer 
solely a function of s, it can t be reduced to a simple vector of SH coefficients Instead of parameterizing 
scalar transfer by R and r, a more useful decomposition is to transfer the incident radiance Lp(s) into 
a whole sphere of transferred radiance, denoted L p ¢ () s . Assuming the glossy kernel G is circularly 
symmetric about R (i.e., a simple Phong-like model) L p ¢() can then be convolved s with r * ()=G( s,(001, 
) Gz ,,)r and evaluated at R to produce the final p Np p Separately SH-projecting Lp and Mp again reduces 
the integral in TDS to an inner product of coefficient vectors. Transfer is now nontrivial; we precompute 
it using a transport simulator (Section 5), storing the resulting transfer vector (Mp)i at many points 
p over O. Unlike the previous   (a) unshadowed (b) shadowed (c) interreflected case, second-order projection 
of M DS may p Figure 4: Glossy Surface Self-transfer. result (see bottom of Figure 2, and further details 
in Section 6). Transfer to Lp ¢ can now be represented as a matrix rather than a vector. For example, 
glossy shadowed transfer is MGS (,Ls)= L (s)(Vs) (11) pp pp a linear operator on Lp whose SH-projection 
can be represented as the symmetric matrix V p via equation (7). Even with smooth lighting, more SH 
bands must be used for L¢ as O s glossiness p increases; non-square matrices (e.g., 25×9) mapping low­frequency 
lighting to higher-frequency transferred radiance are useful under these conditions. For shadowed glossy 
transfer (but not interreflected), an alternative still uses a vector rather than a matrix to represent 
MpGS by computing the product of Vp with Lp on-the-fly using the tabulated triple product of basis functions 
in equation (7). We have not yet implemented this alternative. 4.3 Limitations and Discussion An important 
limitation of precomputed transfer is that material properties of O influencing interreflections in TDI 
and TGI (like albedo or glossiness) must be baked in to the preprocessed transfer and can t be changed 
at run-time. On the other hand, the simpler shadowed transfer without interreflection does allow run­time 
change and/or spatial variation over O of the material proper­ties. Error arises if blockers or light 
sources intrude into O s convex hull. O can only move rigidly, not deform or move one component relative 
to the whole. Recall also the assumption of low lighting variation over O required for correct interreflections. 
Finally, note that diffuse transfer as defined produces radiance after leaving the surface, since it 
has already been convolved with the cosine-weighted normal hemisphere, while glossy transfer produces 
radiance incident on the surface and must be convolved with the local BRDF to produce the final exit 
radiance. It s also possible to bake in a fixed BRDF for glossy O, making the convo­lution with G unnecessary 
at run-time but limiting flexibility.  5. Precomputing Radiance Self-Transfer As a preprocess, we perform 
a global illumination simulation over an object O using the SH basis over the infinite sphere as emitters. 
Our light gathering solution technique is a straightforward adapta­tion of existing approaches [7][25] 
and could be accelerated in many ways; its novelty lies in how it parameterizes the lighting and collects 
the resulting integrated transfers. Note that all integrated transfer coefficients are signed quantities. 
The simulation is parameterized by an n-th order SH projection of the unknown sphere of incident light 
L; i.e., by n 2 unknown coefficients Li. Though the simulation results can be computed independently 
for each Li using the SH basis function yi(s) as an emitter, it is more efficient to compute them all 
at once. The infinitely-distant sphere L will be replaced at run-time by the actual incident radiance 
field around O, Lp. An initial pass simulates direct shadows from paths leaving L and reaching sample 
points p O . In subsequent passes, interreflec­tions are added, representing paths from L that bounce 
a number of times off O before arriving at p (Lp, LDp, LDDp, etc.). In each pass, energy is gathered 
to every sample point p. Large emitters (i.e., low-frequency SH basis) make a gather more efficient then 
a shooting-style update [6]. Note that this idea of caching onto diffuse (or nearly diffuse) receivers 
is not new [21][41]. To capture the sphere of directions at sample points p O , we generate a large (10k-30k), 
quasi-random set of directions {sd}, sd S . We also precompute evaluations for all the SH basis functions 
at each sd. The sd are organized in hierarchical bins formed by refining an initial icosahedron with 
1.2 bisection into equal-area spherical triangles (1.4 subdivision does not lead to equal area triangles 
on the sphere as it does in the plane). We use 6 to 8 subdivision levels, creating 512 to 2048 bins. 
Every bin at each level of the hierarchy contains a list of the sd within it. In the first pass, for 
each p O , we cast shadow rays in the hemisphere about p s normal Np, using the hierarchy to cull directions 
outside the hemisphere. We tag each direction sd with an occlusion bit, 1 -Vs() , indicating whether 
sd is in the hemi­ pd sphere and intersects O again (i.e., is self-shadowed by O). An occlusion bit is 
also associated with the hierarchical bins, indicat­ing whether any sd within it is occluded. Self-occluded 
directions and bins are tagged so that we can perform further interreflection passes on them; completely 
unoccluded bins/samples receive only direct light from the environment. For diffuse surfaces, at each 
point p O we further compute the transfer vector by SH-projecting Mp from (10). For glossy sur­faces, 
we compute the transfer matrix by SH-projecting M from p (11). In either case, the result represents 
the radiance collected at p, parameterized by L. SH-projection to compute the transfers is performed 
by numerical integration over the direction samples sd, summing into an accumulated transfer using the 
following rules: diffuse: ( )0( ) () () () pi p p d N d i dM Vs H s ys rp+= glossy: 0( ) () () () pij 
p d j d i d+= Vs ys ys M Transfer integration over sd [shadow pass, iteration 0] The superscript 0 refers 
to the iteration number. The vector Mp or matrix Mp at each point p is initialized to 0 before the shadow 
pass, which then sums over all sd at every p. The rules are derived using equation (1) for diffuse transfer 
integration, and equation (7) for glossy transfer integration. Later interreflection passes traverse 
the bins having the occlusion bit set during the shadow pass. Instead of shadow rays, they shoot rays 
that return trans­fer from exiting illumination on O. If p the ray (p,sd) intersects another point q 
O (where q is closest to p), we sample the radiance exiting from q in the direction sd. The following 
update rules are used, where the superscript b is the bounce pass iteration: diffuse: ( ) ( ) 1( ) 1 
()( ) () b b pi p p d qi N dM Vs M H s rp -+= - glossy: ( ) ( )* 1 ( ) 1 ( ) ( ) ( ) reflect( , ) ( ) 
q b pij p d b k r k qkj k d q i d k Vs G y sN ys a -+= -Ê -Á Ë ¯Â M M Transfer integration over sd [interreflection 
passes, iteration b] As in the shadow pass, we begin by initializing transfer vectors or matrices to 
0 before accumulating transfer over directions sd. The diffuse rules are derived from the definition 
of TDI and equation (1); glossy rules from the definition of TGI and equations (6) and (7). The middle 
factor in the glossy transfer definition represents radiance emanating from q back to p from the previous 
bounce pass, b-1. Since M q stores incident radiance, it must be con­volved with O s BRDF at q to obtain 
exiting radiance in the -sd direction, yielding a summation over k. Recall that a k is the k-th convolution 
coefficient, expressed in singly-indexed notation. The reflect operator simply reflects its first vector 
argument with respect to its second. We observe that equation (7) implies (Mpi)jis a symmetric matrix 
for shadowed glossy transfer since it is formed by the product of two spherical functions; this is untrue 
for interreflected glossy transfer. Interreflection passes are repeated until the total energy of a given 
pass falls below a threshold. For typical materials, it diminishes quite rapidly. The sum of transfers 
from all bounce passes then accounts for interreflections. Our implementation simulates diffuse and glossy 
transfer at the same time. A simple enhancement to this simulation allows mirror-like surfaces within 
O. We do not record transfers on such surfaces. Instead, a ray striking a mirrored surface is always 
reflected and then propagated until a non-mirrored surface is reached. Thus our paths at successive iterations 
can be represented as (L[S] * p, ** *** L[S] D[S] p, L[S] D[S] D[S] p, etc.), where D is a diffuse or 
glossy bounce and S is a specular one. This captures caustics onto diffuse or glossy receivers that respond 
dynamically to lighting change (Figure 9). 6. Run-time Rendering of Radiance Transfer We now have a model 
O capturing radiance transfer at many points p over its surface, represented as vectors or matrices. 
Rendering O requires the following steps at run-time: 1. compute incident lighting {LPi} at one or more 
sample points Pi near O in terms of the SH basis, 2. rotate these LPi to O s coordinate frame and blend 
them (see below) to produce a field of incident lighting Lp over O, and 3. perform a linear transformation 
on (Lp)i at each point p on O to obtain exit radiance. This requires a dot product with (Mp)i for diffuse 
surfaces (equation (8)), or a matrix-vector multiplication with (Mpi)jfor glossy surfaces (equation (9)). 
 4. Glossy surfaces need a final step in which the radiance vector resulting from step 3 is convolved 
with O s BRDF at p, and then evaluated at the view-dependent reflection direction R.  Step 1 can load 
a precomputed environment map, evaluate ana­lytic lighting models in software, or sample radiance using 
graphics hardware. Rotation for Step 2 is outlined in Section 3, and is done once per object, not for 
each p. It is necessary be­cause transfer is stored using a common coordinate system for O. If O is rigidly 
moving, it is more efficient to rotate the few radi­ance samples in LPi to align with O than it is to 
rotate O s many transfer functions. We currently perform this rotation in software. For diffuse surfaces, 
a simple implementation of step 3 is to store the transfer vector per vertex and perform the dot product 
in a vertex shader. The transfer vectors can also be stored in texture maps rather than per-vertex and 
evaluated using a pixel shader. Since the coefficients are signed quantities not always in the [-1,1] 
range, DirectX 8.1 pixel shaders (V1.4) or their OpenGL counter­part (extension by ATI) must be used, 
since they provide a larger range of [-8,8]. Our pixel shader needs 8 instructions to perform the dot-product 
and stores LP s coefficients in constant registers. For colored environments or simulation of color bleeding 
on O, three passes are required, each performing a separate dot-product for the r, g, and b channels. 
Otherwise a single pass suffices. For glossy self-transfer, we perform the matrix transform from equation 
(9) in software because the transfer matrix is too big to be manipulated in either current vertex or 
pixel shaders. The result is ()¢ the SH coefficients of transferred radiance at points L pi p over O. 
Then in a pixel shader, we perform a convolution with a simple cosine-power (Phong lobe) kernel for G 
* and evaluate the result in the reflection direction R. The result can be written 22 nn Ê * Âai Gi 
Á Â(Mp )( L p ) j yi ()R (12) ij i =1 Ë j =1 ¯ We evaluate SH-projections up to n=5 on graphics hardware. 
6.1 Spatial Sampling of the Incident Radiance Field A simple and useful approach for dynamically sampling 
incident radiance is to sample it at O s center point. To handle local lighting variation over O, a more 
accurate technique samples incident lighting at multiple points (Figure 5). A good set of sample points 
can be obtained using the ICP (iterated closest (a) single sample (b) ICP points (c) multiple samples 
Figure 5: ICP can be used to precompute good locations for sampling the incident radiance field over 
an object. Note the improved locality of lighting in (c) compared to (a) when the lighting is sampled 
at the 8 points in (b) rather than at the object center. point) algorithm [28] as a preprocess, given 
a desired number of points as input. This produces a representative set of points Pi near O and distributed 
uniformly over it where incident lighting can be sampled at run-time. We can also precompute coefficients 
at each p over O that blend contribution from each of the resulting sampled radiance spheres LPi to produce 
an incident radiance field over O, denoted previously by Lp. 6.2 Sampling SH Radiance on Graphics Hardware 
Graphics hardware is useful to capture the radiance samples {LPi} in a dynamic scene. To do this, 6 images 
are rendered from each Pi corresponding to the 6 faces of the cube map spherical parame­terization. O 
itself should be removed from these renderings. Cube map images can then be projected to their SH coefficients 
using the integral in equation (1), as was done in [4]. For efficiency, we precompute textures for the 
basis functions mm weighted by differential solid angle, ()= ysds() Bs () s, each ll evaluated over the 
cube map parameterization for s. The result­ing integral then becomes a simple dot product of the captured 
samples of LP(s) with the textures lm (). Bs Ideally, this computation would be performed on the graphics 
hardware. Precision issues and inability to do inner products in hardware force us to read back the sampled 
radiance images and project them in software. In this case, it is important to reduce the resolution 
of read-back images as much as possible. Low-order SH projection can be computed with very low­resolution 
cube maps, assuming they have been properly bandlim­ited. For example, spherical signals already bandlimited 
to 6-th order can be projected using six 4×4 images with about 0.3% average-case squared error and about 
1% worst-case squared error, where error is normalized by assuming unit-power signals (i.e., signals 
whose integrated square over the sphere is 1).1 For 6×8×8 maps, this error reduces to 0.003% mean and 
0.02% worst­case. Unfortunately, typical signals aren t spherically bandlimited. Another analysis shows 
that, assuming continuous bilinear recon­struction over the sampled 2D images, projection to 6-th order 
using 6×8×8 images yields 0.7% and 2% average and worst-case squared error, while 6×16×16 yields 0.2% 
and 0.5% squared error, and 6×32×32 yields 0.05% and 0.1% squared error. We extract 6×16×16 images from 
the hardware. As is always true in point-sampled rendering, aliasing of the 2D images is still a problem 
because the above analysis uses bilinear reconstruction from point samples as the reference. To reduce 
aliasing, we supersample the cube map images by a factor of 2 in each dimen­sion, and do a box-filtered 
decimation in hardware before reading back and projecting. The basis function textures are also super­sampled 
and decimated in the same way as a preprocess. A radiance sample, including read-back and SH projection, 
takes about 1.16ms on a PIII-933 PC with an ATI Radeon 8500. 1 More precisely, average-case error is 
the integrated squared difference between the reference and reconstruction signals, averaged over all 
unit-power signals. Worst­case error is the same integrated error, but for the worst-case unit-power 
signal.  Figure 6: Volumetric self-transfer captures how this cloud model shadows itself. Lighting can 
be changed in real-time (first row). The same model can also be placed in other environments (second 
row). 7. Self-Transfer for Volumetric Models Self-transfer on volumetric data uses the same framework 
as surfaces. The resulting precomputed model allows run-time changes to the lighting, with correct shadowing 
and interreflec­tions in any low-frequency lighting environment (Figure 6). Our simple simulator currently 
works only for diffuse volumes. As with surface transfer, a preprocessing step simulates lighting on 
the volume using the SH basis functions as emitters.  For shadowed transfer without interreflection 
(i.e., direct shadowing), we gather energy from the emitter to every voxel p of the volume, attenuated 
by its path through the volume. The required numeri­cal integration over directions sd can be expressed 
as 0 + ( (Mpi) +=Ap( Æp Dsd) ys d) i where A( p Æ q) is the volume s integrated attenuation along the 
path from p to q, and D is the distance until the ray (p,sd) exits the volume. To include interreflections, 
we traverse every voxel p and forward-scatter its transfer along random directions sd. The transfer is 
deposited to all voxels q along sd until exiting the volume, using the rule bb-1 ( M ) +=Ap Æq)( ( M 
) qi pi More passes over the volume produce further indirect bounces. Rendering is performed in the traditional 
way: by drawing slices through the 3D volume in back to front order using alpha blending to account for 
transparency. Each slice is a 2D image containing samples of the transfer vector. A pixel shader computes 
the dot­product between the lighting s coefficients and the transfer vec­tor s required to shade each 
slice. 8. Radiance Neighborhood-Transfer Neighborhood-transfer precomputes an object O s influence on 
its neighboring environment with respect to parameterized, low­frequency lighting. Transport simulation 
is identical to that for self-transfer in Section 5, but takes place with respect to points in a 3D space 
surrounding O, not on it. At run-time, an arbitrary receiver R can be placed in this volume to capture 
shadows, reflections, and caustics cast by O onto R without knowing R in advance. For example, a moving 
vehicle O can cast shadows over a terrain R (Figure 7). Cast shadows and lighting also respond to lighting 
change; for example, moving the lights move soft shadows on R. This generalizes irradiance volumes [15] 
by accounting for glossy transfer and allowing dynamic lighting. Because R is unknown during the precomputation 
step, O s neighborhood volume must store a transfer matrix rather than a vector. This is true even for 
diffuse receivers, because we do not know in advance what R s normal will be. Our current imple­mentation 
precomputes the transfer matrix M p at each point within a simple 3D grid surrounding O. At run-time, 
we perform the matrix transform from equation (9) in software at each point Figure 7: Neighborhood transfer 
captures how this hang glider blocks light to a volume of points below it. This allows cast soft shadow 
onto a bumpy terrain as the glider moves. in the volume and upload the result to the graphics hardware. 
The result is a volume texture containing coefficients of transferred radiance ()L¢ which is applied 
to R. pi Then in a pixel shader this transferred radiance is used to light the receiver. A diffuse receiver 
convolves the radiance with the cosine weighted hemisphere H * using equation (6) and then evaluates 
the resulting SH projection at R s normal vector. Glossy receivers perform equation (12). Receivers having 
precomputed self-transfer raise the difficulty that O and R do not share a common coordinate system. 
Thus, one of the two object s dense set of transfer samples must be dynamically rotated to align with 
the other s. The SH-rotation operation required is currently impractical for hardware evalua­tion. Improving 
hardware should soon ease this difficulty. Compared to self-transfer, neighborhood-transfer incurs some 
additional approximation errors. Cast shadow or light from multiple neighborhood-transfer objects onto 
the same receiver is hard to combine. Local lighting variation not due to O or R s presence is also a 
problem; lighting must be fairly constant across O s entire neighborhood to provide accurate results. 
In particular, errors such as missing shadows will result when objects besides O and R intrude into O 
s neighborhood. O s neighborhood must also be large enough to encompass any cast shadow or light it may 
cast on R. Nevertheless, neighborhood transfer captures effects impossible to obtain in real-time with 
previous methods. 9. Results Full statistics are summarized in Table 1. We achieve real-time performance 
for all models except the transfer matrix ones (tea­pot, buddha, glider). For these models, multiplication 
with 25x25 or 9x25 transfer matrices over the surface in software forms the bottleneck. Real-time results 
can be achieved even for these models after first fixing the light (allowing the view to be moved) or 
the view (allowing the light to be changed) and represent the second and third render rate entries in 
the table after slashes. The reason is that fixing either the view (which then fixes the reflection vector 
R) or the light Lp reduces the computation in (12) to a simple dot product, which can then be done in 
hardware. Fixed light rendering is slower than fixed view because the fixed light mode requires evaluation 
of the SH basis at a changing view vector, followed by a dot product, while fixed view requires only 
the dot product and is identical in performance to diffuse transfer. model transfer type transfer shape 
transfer sampling preproc. time render rate head (fig 1) DS 25-M 50k ver. mesh 1.1h 129 bird (fig 4) 
DS 25-M 50k ver. mesh 1.2h 125 ring (fig 9) DS 25-M 256x256 grid 8m 94 buddha_d (fig 11) DS 25-M 50k 
ver. mesh 2.5h 125 buddha_g (fig 11) GS 25x25-M 50k ver. mesh 2.5h 3.6/16/125 tyra_d (fig 11) DS 25-M 
100k ver. mesh 2.4h 83 tyra_g (fig 11) GS 25x25-M 100k ver. mesh 2.4h 2.2/9.4/83 teapot (fig 5) GS 25x25-M 
150k ver. mesh 4.4h 1.7/7.7/49 cloud (fig 6) DV 25-M 32x32x32 vol. 15m 40 glider (fig 7) N 9x25-M 64x64x8 
vol. 3h 4/120/4 Table 1: Results. Transfer types are DS=diffuse surface self-transfer, GS=glossy surface 
self-transfer, DV=diffuse volume self-transfer, and N=neighborhood transfer. Timings are on a 2.2GHz 
Pentium 4 with ATI Radeon 8500 graphics card. Render rates are in Hz. Rendering quality can be judged 
from images in this paper (Fig­ures 1 and 3-12) all of which were computed with the PC graphics hardware. 
Self-shadowing and interreflection effects are convinc­ing and robust. No depth tolerances are required 
to prevent self­shadowing artifacts as they are with the standard shadow buffer technique [43]. Even 
when the lighting contains very high fre­quencies (Figure 9, top row of Figure 10, and Figure 12), pleasing 
images are produced without temporal artifacts but with some blurring of self-shadows; the result looks, 
and indeed is, identical to blurring the incident lighting. Figure 10 compares shadowing results across 
different SH orders. Small light sources (top row) require more bands; larger ones are approximated very 
well with fewer bands. Using up to the quartic band (fifth order with 25 coefficients) provides good 
results and is a good match to today s graphics hardware. Note that quality is not dictated solely by 
how much energy is ignored in SH­projecting the lighting diffuse and glossy objects are effective low-pass 
filters of incident radiance. With self-transfer effects though, the extent of this low-pass filtering 
depends on the ob­ject s geometry, varies spatially, and typically requires more than third order (quadratics), 
unlike unshadowed diffuse transfer [34]. Because of its rotational invariance (equation (4)), we consider 
the SH basis especially useful for our low-frequency lighting application compared to alternatives like 
spherical wavelets [37]. When dynamically sampling incident radiance, this property eliminates aliasing 
which would otherwise produce temporal artifacts, like shading wobble , if projected to other bases with 
the same number of coefficients. Ringing or Gibbs phenomenon (oscillatory undershoot and overshoot in 
the reconstruction function) can be a problem when the lighting environment has significant energy near 
its highest represented band [10][42]. We notice ringing artifacts only on very simple models such as 
the ones in Figures 9 and 10; artifacts are masked on complex meshes. Of course, reducing lighting frequency 
by attenuating higher frequency bands, called windowing , also reduces ringing (see Figure 10, columns 
f and g). 10. Conclusions and Future Work Much important shading variation over a complex object such 
as a human face is due to itself. Precomputed radiance self-transfer is a general method for capturing 
the occlusion and scattering effects an object has on itself with respect to any low-frequency lighting 
environment. When the actual incident lighting is substituted at run-time, the resulting model provides 
global illumination effects like soft shadows, interreflections, and caustics in real-time. Using graphics 
hardware, incident lighting can be sampled every frame and at multiple points near the object allowing 
dynamic, local lighting. Neighborhood-transfer generalizes the concept by recording transfer over 3D 
space, allowing cast soft shadows and caustics onto arbitrary receivers. In future work, we want to apply 
precomputed transfer to more sophisticated transport models, especially subsurface scattering [22]. We 
believe the smoothness of exiting radiance produced by this model makes it particularly suitable for 
SH-parameterized transfer. It would also be valuable to combine existing shadowing techniques with ours, 
by decomposing the scene s lighting into high and low frequency terms. Compression of transfer fields 
is an important but unaddressed problem. Extension to deformable objects like human characters could 
be achieved by parameteriz­ing the precomputed self-transfer in the same way as the deformation, assuming 
the number of independent degrees of freedom remains manageable. Finally, we are interested in tracking 
fast-improving PC graphics hardware so that all compu­tation, including transfer matrix transforms and 
SH-rotation, may eventually be performed on the GPU. Acknowledgements: Thanks to Paul Debevec for his 
light probes (http://www.debevec.org), to the Stanford University Computer Graphics Laboratory for the 
happy Buddha model (http://www-graphics.stanford.edu/data/3Dscanrep), and Cyberware for the tyrannosaur 
model. Jason Mitchell and Michael Doggett of ATI and Matthew Papakipos of NVidia kindly provided graphics 
hardware. We also wish to acknowledge Michael Cohen for early discussions and Charles Boyd and Hans-Peter 
Seidel for support. References [1] AGRAWALA, M, RAMAMOORTHI, R, HEIRICH, A, AND MOLL, L, Efficient Image-Based 
Methods for Rendering Soft Shadows, SIGGRAPH 00, 375-384. [2] AIREY, J, ROHLF, J, AND BROOKS, F, Towards 
Image Realism with Interactive Update Rates in Complex Virtual Building Environments,1990 Symposium on 
Interactive 3D Graphics, 24(2), 41-50. [3] ASHIKHMIN, M, AND SHIRLEY, P, Steerable Illumination Textures, 
ACM Transactions on Graphics, 2(3), to appear. [4] CABRAL, B, MAX, N, AND SPRINGMEYER, R, Bidirectional 
Reflection Functions from Surface Bump Maps, SIIGRAPH 87, 273-281. [5] CABRAL, B, OLANO, M, AND NEMEC, 
P, Reflection Space Image Based Render­ing, SIGGRAPH 99, 165-170.. [6] COHEN, M, AND WALLACE, J, Radiosity 
and Realistic Image Synthesis, Aca­demic Press Professional, Cambridge, 1993. [7] COOK, R, PORTER, T, 
AND CARPENTER, L, Distributed Ray Tracing, SIGGRAPH 84, 137-146. [8] DEBEVEC, P, Rendering Synthetic 
Objects into Real Scenes: Bridging Tradi­tional and Image-Based Graphics with Global Illumination and 
High Dynamic Range Photogaphy, SIGGRAPH 98, 189-198. [9] DEBEVEC, P, HAWKINS, T, TCHOU, C, DUIKER, H, 
SAROKIN, W, AND SAGAR, M, Acquiring the Reflectance Field of a Human Face, SIGGRAPH 2000, 145-156. [10] 
DOBASHI,Y, KANEDA, K,NAKATANI, H, AND YAMASHITA, H, A Quick Render­ing Method Using Basis Functions for 
Interactive Lighting Design, Eurographics 95, 229-240. [11] DORSEY, J, SILLION, F, AND GREENBERG, D, 
Design and Simulation of Opera Lighting and Projection Effects, SIGGRAPH 91, 41-50. [12] D ZMURA, M, 
Shading Ambiguity: Reflection and Illumination. In Computa­tional Models of Visual Processing (1991), 
Landy and Movshon, eds., MIT Press, Cambridge, 187-207. [13] EDMONDS, A, Angular Momentum in Quantum 
Mechanics, Princeton Univer­sity, Princeton, 1960. [14] GREENE, N, Environment Mapping and Other applications 
of World Projec­tions, IEEE CG&#38;A, 6(11):21-29, 1986. [15] GREGER, G., SHIRLEY,P, HUBBARD, P, AND 
GREENBERG, D, The Irradiance Volume, IEEE Computer Graphics And Applications, 6(11):21-29, 1986. [16] 
HAKURA, Z, AND SNYDER, J, Realistic Reflections and Refractions on Graphics Hardware with Hybrid Rendering 
and Layered Environment Maps, Eurograph­ics Workshop on Rendering, 2001, 289-300. [17] HAEBERLI, P, AND 
AKELEY, K, The Accumulation Buffer: Hardware Support for High-Quality Rendering, SIGGRAPH 90, 309-318. 
[18] HEIDRICH,W, LENSCH, H, COHEN, M, AND SEIDEL, H, Light Field Techniques for Reflections and Refractions, 
Eurographics Rendering Workshop 99,195-375. [19] HEIDRICH,W, SEIDEL H, Realistic, Hardware-Accelerated 
Shading and Lighting, SIGGRAPH 99, 171-178. [20] HEIDRICH,W, DAUBERT, K, KAUTZ, J, AND SEIDEL, H, Illuminating 
Micro Geometry based on Precomputed Visibility, SIGGRAPH 00, 455-464. [21] JENSEN, H, Global Illumination 
using Photon Maps, Eurographics Workshop on Rendering 1996, 21-30. [22] JENSEN, H, MARSCHNER,S, LEVOY, 
M, AND HANRAHAN, P, A Practical Model for Subsurface Light Transport, SIGGRAPH 01, 511-518. [23] KAUTZ, 
J, AND MCCOOL, M, Interactive Rendering with Arbitrary BRDFs using Separable Approximations, Eurographics 
Workshop on Rendering 99,.247-260. [24] KAUTZ, J, VAZQUEZ,P, HEIDRICH, W, AND SEIDEL, H, A Unified Approach 
to Pre­filtered Environment Maps, Eurographics Workshop on Rendering 2000, 185-196. [25] KAJIYA, J, The 
Rendering Equation, SIGGRAPH 86, 143-150. [26] KEATING, B, AND MAX, N, Shadow Penumbras for Complex Objects 
by Depth-Dependent Filtering of Multi-Layer Depth Images, Eurographics Rendering Workshop, 1996, pp.205-220. 
[27] KELLER, A, Instant Radiosity, SIGGRAPH 97, 49-56. [28] LINDE,Y, BUZO, A, AND GRAY, R, An algorithm 
for Vector Quantizer Design, IEEE Transactions on Communication COM-28, 1980,84-95. [29] LOKOVIC, T, 
AND VEACH, E, Deep Shadow Maps, SIGGRAPH 00, pp.385-392. [30] MALZBENDER,T, GELB, D, AND WOLTERS, H, 
Polynomial Texture Maps, SIGGRAPH 01, 519-528. [31] MAX, N, Horizon Mapping: Shadows for Bump-Mapped 
Surfaces, The Visual Computer, July 1998, 109-117. [32] MILLER, G, Efficient Algorithms for Local and 
Global Accessibility Shading, SIGGRAPH 94, 319-326. [33] NIMEROFF,J, SIMONCELLI, E, AND DORSEY, J, Efficient 
Re-rendering of Natural Environments, Eurographics Workshop on Rendering 1994, 359-373. [34] RAMAMOORTHI, 
R, AND HANRAHAN, P, An Efficient Representation for Irradiance Environment Maps, SIGGRAPH 01, 497-500. 
[35] REEVES, W, SALESIN, D, AND COOK, R, Rendering Antialiased Shadows with Depth Maps, SIGGRAPH 87, 
283-291. [36] SEGAL, M, KOROBKIN, C, VAN WIDENFELT, R, FORAN, J, AND HAE-BERLI, P, Fast Shadows and Lighting 
Effects Using Texture Mapping, SIGGRAPH 92, 249-252. [37] SCHRÖDER, P, AND SWELDENS, W, Spherical Wavelets: 
Efficiently Representing the Sphere, SIGGRAPH 95, 161-172. [38] SILLION, F, ARVO, J, WESTIN, S, AND GREENBERG, 
D, A Global Illumination Solution for General Reflectance Distributions, SIG-GRAPH 91, 187-196. [39] 
SOLER, C, AND SILLION, F, Fast Calculation of Soft Shadow Textures Using Convolution, SIGGRAPH 98, 321-332. 
[40] TEO, P, SIMONCELLI, E, AND HEEGER, D, Efficient Linear Re-rendering for Interactive Lighting Design, 
October 1997 Report No. STAN-CS­TN-97-60, Stanford University, 1997. [41] WARD, G, RUBINSTEIN, F, AND 
CLEAR, R, A Ray Tracing Solution for Diffuse Interreflection, SIGGRAPH 88, 85-92. [42] WESTIN, S,ARVO, 
J, TORRANCE, K, Predicting Reflectance Functions from Complex Surfaces, SIGGRAPH 92, 255-264. [43] WILLIAMS, 
L, Casting Curved Shadows on Curved Surfaces, SIG-GRAPH 78, 270-274. [44] ZARE, R, Angular Momentum: 
Understanding Spatial Aspects in Chemistry and Physics, Wiley, New York, 1987. 0° 20° 40° Figure 9: Real-time, 
dynamic caustics and shadows using diffuse self-transfer. The ring model is rendered with a traditional 
environment map; the ground plane was rendered using pre­computed self-transfer results from a caustic 
simulation combining both the specular ring and a diffuse ground (Section 5). A 25 com­ponent transfer 
vector was recorded over a 256x256 grid on the ground plane. These two images were generated by rotating 
an acquired environment around the model. A frame rate of 130Hz is obtained in our implementation . 
    (g) windowed (h) ray traced (n=26)  (a) diffuse, unshadowed (b) diffuse, interreflected (c) 
glossy, unshadowed (d) glossy, interreflected (a) no interreflections (shadowed transfer) (b) 1-bounce 
interreflections (c) 2-bounce interreflections   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566613</article_id>
		<sort_key>537</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Interactive global illumination in dynamic scenes]]></title>
		<page_from>537</page_from>
		<page_to>546</page_to>
		<doi_number>10.1145/566570.566613</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566613</url>
		<abstract>
			<par><![CDATA[In this paper, we present a system for interactive computation of global illumination in dynamic scenes. Our system uses a novel scheme for caching the results of a high quality pixel-based renderer such as a bidirectional path tracer. The Shading Cache is an object-space hierarchical subdivision mesh with lazily computed shading values at its vertices. A high frame rate display is generated from the Shading Cache using hardware-based interpolation and texture mapping. An image space sampling scheme refines the Shading Cache in regions that have the most interpolation error or those that are most likely to be affected by object or camera motion.Our system handles dynamic scenes and moving light sources efficiently, providing useful feedback within a few seconds and high quality images within a few tens of seconds, without the need for any pre-computation. Our approach allows us to significantly outperform other interactive systems based on caching ray-tracing samples, especially in dynamic scenes. Based on our results, we believe that the Shading Cache will be an invaluable tool in lighting design and modelling while rendering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Monte Carlo techniques]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[parallel computing]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[rendering systems]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382455</person_id>
				<author_profile_id><![CDATA[81100250004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Parag]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tole]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43116383</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14016782</person_id>
				<author_profile_id><![CDATA[81100010986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Walter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166137</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AUPPERLE, L., AND HANRAHAN, P. 1993. A hierarchical illumination algorithm for surfaces with glossy reflection. Proceedings of SIGGRAPH 93 (August), 155-162. ISBN 0-201-58889-7. Held in Anaheim, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>336417</ref_obj_id>
				<ref_obj_pid>336414</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BALA, K., DORSEY, J., AND TELLER, S. 1999. Radiance interpolants for accelerated bounded-error ray tracing. ACM Transactions on Graphics 18, 3 (July), 213-256. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300551</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BASTOS, R., HOFF, K., WYNN, W., AND LASTRA, A. 1999. Increased photorealism for interactive architectural walkthroughs. 1999 ACM Symposium on Interactive 3D Graphics (April), 183-190. ISBN 1-58113-082-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192195</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BISHOP, G., FUCHS, H., MCMILLAN, L., AND ZAGIER, E. J. S. 1994. Frameless rendering: Double buffering considered harmful. Proceedings of SIGGRAPH 94 (July), 175-176. ISBN 0-89791-667-0. Held in Orlando, Florida.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122737</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E., RUSHMEIER, H. E., MILLER, G., AND TURNER, D. 1991. A progressive multi-pass method for global illumination. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4 (July), 165-174. ISBN 0-201-56291-X. Held in Las Vegas, Nevada.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97894</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E. 1990. Incremental radiosity: An extension of progressive radiosity to an interactive image synthesis system. In Computer Graphics (Proceedings of SIGGRAPH 90), vol. 24, 135-144. ISBN 0-201-50933-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L., CARPENTER, L., AND CATMULL, E. 1987. The reyes image rendering architecture. In Computer Graphics (Proceedings of SIGGRAPH 87), no. 4, 95-102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DARSA, L., AND COSTA, B. 1996. Multiresolution representation and reconstruction of adaptively sampled images. Proceedings of SIBGRAPI 96 (October), 321-328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253308</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DIEFENBACH, P. J., AND BADLER, N. I. 1997. Multi-pass pipeline rendering: Realism for dynamic environments. 1997 Symposium on Interactive 3D Graphics (April), 59-70. ISBN 0-89791-884-3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258772</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DRETTAKIS, G., AND SILLION, F. X. 1997. Interactive update of global illumination using a line-space hierarchy. Proceedings of SIGGRAPH 97 (August), 57-64. ISBN 0-89791-896-7. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732305</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DUMONT, R., PELLACINI, F., AND FERWERDA, J. A. 2001. A perceptually-based texture caching algorithm for hardware-based rendering. In Rendering Techniques 2001: 12th Eurographics Workshop on Rendering, Eurographics, 249-256. ISBN 3-211-83709-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GORAL, C. M., TORRANCE, K. E., GREENBERG, D. P., AND BATTAILE, B. 1984. Modelling the interaction of light between diffuse surfaces. Computer Graphics (Proceedings of SIGGRAPH 84) 18, 3 (July), 213-222. Held in Minneapolis, Minnesota.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GRANIER, X., AND DRETTAKIS, G. 2001. Incremental updates for rapid glossy global illumination. Computer Graphics Forum 20, 3, 268-277. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732134</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GRANIER, X., DRETTAKIS, G., AND WALTER, B. 2000. Fast global illumination including specular effects. Rendering Techniques 2000: 11th Eurographics Workshop on Rendering (June), 47-58. ISBN 3-211-83535-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280888</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GUO, B. 1998. Progressive radiance evaluation using directional coherence maps. Proceedings of SIGGRAPH 98 (July), 255-266. ISBN 0-89791-999-8. Held in Orlando, Florida.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>365104</ref_obj_id>
				<ref_obj_pid>355588</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HALTON, J., AND WELLER, G. 1964. Algorithm 247: Radical inverse quasi-random point sequence. Comm. ACM, 701-702.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. 1991. A rapid hierarchical radiosity algorithm. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4 (July), 197-206. ISBN 0-201-56291-X. Held in Las Vegas, Nevada.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311551</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HART, D., DUTR&#201;, P., AND GREENBERG, D. P. 1999. Direct illumination with lazy visibility evaluation. Proceedings of SIGGRAPH 99 (August), 147-154. ISBN 0-20148-560-5. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732139</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[HOLZSCHUCH, N., CUNY, F., AND ALONSO, L. 2000. Wavelet radiosity on arbitrary planar surfaces. In Rendering Techniques 2000: 11th Eurographics Workshop on Rendering, Eurographics, 161-172. ISBN 3-211-83535-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W. 1996. Global illumination using photon maps. Eurographics Rendering Workshop 1996 (June), 21-30. ISBN 3-211-82883-4. Held in Porto, Portugal.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. T. 1986. The rendering equation. Computer Graphics (Proceedings of SIGGRAPH 86) 20, 4 (August), 143-150. Held in Dallas, Texas.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>337722</ref_obj_id>
				<ref_obj_pid>337680</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LARSON, G. W., AND SIMMONS, M. 1999. The holodeck ray cache: An interactive rendering system for global illumination in non-diffuse environments. ACM Transactions on Graphics 18, 4 (October), 361-368. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[LINDHOLM, E., KILGARD, M. J., AND MORETON, H. 2001. A user-programmable vertex engine. Proceedings of SIGGRAPH 2001 (August), 149-158. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383276</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MCCOOL, M. D., ANG, J., AND AHMAD, A. 2001. Homomorphic factorization of brdfs for high-performance rendering. Proceedings of SIGGRAPH 2001 (August), 171-178. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383284</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[MYSZKOWSKI, K., TAWARA, T., AKAMINE, H., AND SEIDEL, H.-P. 2001. Perception-guided global illumination solution for animation rendering. Proceedings of SIGGRAPH 2001 (August), 221-230. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300537</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[PARKER, S., MARTIN, W., SLOAN, P.-P. J., SHIRLEY, P., SMITS, B., AND HANSEN, C. 1999. Interactive ray tracing. 1999 ACM Symposium on Interactive 3D Graphics (April), 119-126. ISBN 1-58113-082-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344812</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PELLACINI, F., FERWERDA, J. A., AND GREENBERG, D. P. 2000. Toward a psychophysically-based light reflection model for image synthesis. Proceedings of SIGGRAPH 2000 (July), 55-64. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731973</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F. P., LISCHINSKI, D., AND SALESIN, D. 1997. Progressive previewing of ray-traced images using image plane discontinuity meshing. Eurographics Rendering Workshop 1997 (June), 115-126. ISBN 3-211-83001-4. Held in St. Etienne, France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311543</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[RAMASUBRAMANIAN, M., PATTANAIK, S. N., AND GREENBERG, D. P. 1999. A perceptually based physical error metric for realistic image synthesis. Proceedings of SIGGRAPH 99 (August), 73-82. ISBN 0-20148-560-5. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344886</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SCHAUFLER, G., DORSEY, J., DECORET, X., AND SILLION, F. X. 2000. Conservative volumetric visibility with occluder fusion. Proceedings of SIGGRAPH 2000 (July), 229-238. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[SEGAL, M., KOROBKIN, C., VAN WIDENFELT, R., FORAN, J., AND HAEBERLI, P. E. 1992. Fast shadows and lighting effects using texture mapping. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 249-252. ISBN 0-201-51585-7. Held in Chicago, Illinois.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[SILLION, F. X., DRETTAKIS, G., AND SOLER, C. 1995. A clustering algorithm for radiance calculation in general environments. Eurographics Rendering Workshop 1995 (June), 196-205. Held in Dublin, Ireland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732132</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[SIMMONS, M., AND S&#201;QUIN, C. H. 2000. Tapestry: A dynamic mesh-based display representation for interactive rendering. Rendering Techniques 2000: 11th Eurographics Workshop on Rendering (June), 329-340. ISBN 3-211-83535-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[SMITS, B. E., ARVO, J. R., AND SALESIN, D. H. 1992. An importance-driven radiosity algorithm. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 273-282. ISBN 0-201-51585-7. Held in Chicago, Illinois.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351653</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[STAMMINGER, M., SCHEEL, A., GRANIER, X., PEREZ-CAZORLA, F., DRETTAKIS, G., AND SILLION, F. X. 1999. Efficient glossy global illumination with interactive viewing. Graphics Interface '99 (June), 50-57. ISBN 1-55860-632-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732275</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[STAMMINGER, M., HABER, J., SCHIRMACHER, H., AND SEIDEL, H.-P. 2000. Walkthroughs with corrective texturing. Rendering Techniques 2000: 11th Eurographics Workshop on Rendering (June), 377-390. ISBN 3-211-83535-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732111</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[ST&#220;RZLINGER, W., AND BASTOS, R. 1997. Interactive rendering of globally illuminated glossy scenes. Eurographics Rendering Workshop 1997 (June), 93-102. ISBN 3-211-83001-4. Held in St. Etienne, France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275485</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[TELLER, S., BALA, K., AND DORSEY, J. 1996. Conservative radiance interpolants for ray tracing. Eurographics Rendering Workshop 1996 (June), 257-268. ISBN 3-211-82883-4. Held in Porto, Portugal.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300783</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., HODGINS, J. K., AND GUENTER, B. K. 1999. Two methods for display of high contrast images. 56-94. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383823</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[UDESHI, T., AND HANSEN, C. 1999. Towards interactive, photorealistic rendering of indoor scenes: A hybrid approach. Eurographics Rendering Workshop 1999 (June). Held in Granada, Spain.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[VEACH, E., AND GUIBAS, L. 1994. Bidirectional estimators for light transport. In Fifth Eurographics Workshop on Rendering, 147-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[WALD, I., SLUSALLEK, P., BENTHIN, C., AND WAGNER, M. 2001. Interactive rendering with coherent ray tracing. Computer Graphics Forum 20, 3, 153-164. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258766</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[WALTER, B., ALPPAY, G., LAFORTUNE, E. P. F., FERNANDEZ, S., AND GREENBERG, D. P. 1997. Fitting virtual lights for non-diffuse walkthroughs. Proceedings of SIGGRAPH 97 (August), 45-48. ISBN 0-89791-896-7. Held in Los Angeles, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>256158</ref_obj_id>
				<ref_obj_pid>256157</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[WALTER, B., HUBBARD, P. M., SHIRLEY, P., AND GREENBERG, D. F. 1997. Global illumination using local linear density estimation. ACM Transactions on Graphics 16, 3 (July), 217-259. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383819</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[WALTER, B., DRETTAKIS, G., AND PARKER, S. 1999. Interactive rendering using the render cache. Eurographics Rendering Workshop 1999 (June). Held in Granada, Spain.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. 1994. The radiance lighting simulation and rendering system. Proceedings of SIGGRAPH 94 (July), 459-472. ISBN 0-89791-667-0. Held in Orlando, Florida.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[WHITTED, T. 1980. An improved illumination model for shaded display. Communications of the ACM 23, 6 (June), 343-349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383842</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[WILLMOTT, A., HECKBERT, P. S., AND GARLAND, M. 1999. Face cluster radiosity. In Eurographics Rendering Workshop 1999, Springer Wein / Eurographics, Granada, Spain.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383748</ref_obj_id>
				<ref_obj_pid>383745</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[YEE, H., PATTANAIK, S., AND GREENBERG, D. P. 2001. Spatiotemporal sensitivity and visual attention for efficient rendering of dynamic environments. ACM Transactions on Graphics 20, 1 (January), 39-65. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Global Illumination in Dynamic Scenes Parag Tole Fabio Pellacini Bruce Walter Donald P. 
Greenberg Program of Computer Graphics, Cornell University . Abstract In this paper, we present a system 
for interactive computation of global illumination in dynamic scenes. Our system uses a novel scheme 
for caching the results of a high quality pixel-based renderer such as a bidirectional path tracer. The 
Shading Cache is an object­space hierarchical subdivision mesh with lazily computed shading values at 
its vertices. A high frame rate display is generated from the Shading Cache using hardware-based interpolation 
and texture mapping. An image space sampling scheme re.nes the Shading Cache in regions that have the 
most interpolation error or those that are most likely to be affected by object or camera motion. Our 
system handles dynamic scenes and moving light sources ef.ciently, providing useful feedback within a 
few seconds and high quality images within a few tens of seconds, without the need for any pre-computation. 
Our approach allows us to signi.cantly outperform other interactive systems based on caching ray-tracing 
samples, especially in dynamic scenes. Based on our results, we believe that the Shading Cache will be 
an invaluable tool in lighting design and modelling while rendering. CR Categories: I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism; I.3.3 [Computer Graphics]: Picture/Image Generation; I.3.2 [Computer 
Graphics]: Graphics Systems; Keywords: Rendering, Ray Tracing, Parallel Computing, Render­ing Systems, 
Illumination, Monte Carlo Techniques 1 Introduction Interactive computation of global illumination is 
of major impor­tance in the .eld of computer graphics, especially when applied to engineering and design. 
Effects such as soft shadows, diffuse inter­re.ections and caustics are important cues for the human 
visual system. All these effects can be correctly simulated using global illumination algorithms such 
as path tracing [Kajiya 1986] or the RADIANCE system [Ward 1994], but only at a huge computational cost. 
Needless to say, these techniques are not suitable for interac­tive applications. Modern graphics hardware 
can render complex environments at interactive rates. The realism of hardware-based approaches can be 
increased by using pre-computed radiosity textures, environment maps and sophisticated pixel and vertex 
shaders [Lindholm et al. .580 Rhodes Hall, Ithaca, NY 14853. WWW: http://www.graphics.cornell.edu/ E-mail: 
.parag,fabio,bjw,dpg.@graphics.cornell.edu Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 2001]. However, these approaches limit the choice of 
shading algo­rithms. In addition, the amount of pre-computation or hand-tuning required is often quite 
signi.cant. Inspite of these drawbacks, hard­ware rendering remains the only viable choice for interactive 
ap­plications, primarily because of its ef.ciency in performing hidden surface removal and texture mapping. 
Recently, interactive walkthroughs of global illumination have become possible by clever caching of ray-traced 
samples [Walter et al. 1999; Simmons and S´equin 2000; Stamminger et al. 2000] or by ef.ciently updating 
partially pre-computed radiosity solu­tions [Drettakis and Sillion 1997; Granier and Drettakis 2001]. 
However, these techniques provide only limited interaction; large­scale scene manipulation or the movement 
of primary or bright sec­ondary light sources typically results either in a loss of interactivity or 
a prolonged degradation in image quality. Applications such as lighting design require interactive global 
illumination solutions while allowing unrestricted manipulation of the scene including the lights. Interactive 
walkthroughs with pro­gressively re.ned global illumination solutions are also useful in such applications. 
In addition, it is also important to have scala­bility with processing power and .exibility in the choice 
of global illumination algorithms. In order to meet these goals, we have designed a novel scheme for 
caching high quality samples generated by pixel-based render­ing algorithms such as path tracing and 
then interpolating between these samples using graphics hardware. Our Shading Cache is a hi­erarchical 
subdivision mesh attached to each primitive in the scene, with lazily evaluated shading values at the 
mesh vertices. It is re­.ned by choosing locations in the image plane that require more accuracy. The 
selection of these locations is made using a prior­ity map, which captures the likelihood of error due 
to interpolation, view-dependence and object motion. We also introduce a novel .ood .lling operator for 
ef.ciently capturing the high frequency detail in the image to supplement the priority based sampling. 
Fi­nally, we use graphics hardware to ef.ciently interpolate between the shading samples for polygonal 
or curved surfaces. By using the hardware for image reconstruction, we ensure that the scene geom­etry 
and textures are accurately reproduced at all times, even when the shading values are only available 
at a very low resolution. The entire process is illustrated in Figure 1. While the idea of image reconstruction 
from a sparse set of high quality samples has been previously explored [Walter et al. 1999; Simmons and 
S´equin 2000; Stamminger et al. 2000], we believe that our approach offers signi.cant improvements in 
speed and quality, especially in dynamic scenes. The reason for our superior performance is the use of 
object space caching and interpolation, combined with a superior sampling scheme that re.nes the mesh 
only in regions of high importance and interpolates in other regions. After a discussion of previous 
work in Section 2, we give an overview of our system in Section 3 and the sample selection pro­cess in 
Section 4. We then show how moving objects and view dependent illumination can be ef.ciently handled 
in Section 5. In Section 6, we provide performance statistics and in Section 7, we compare our system 
to other interactive global illumination tech­niques. We conclude with a discussion of the advantages 
and limi­tations of our work and future directions for research. Update of Shading Cache Shading Cache 
at t n-1 Priority Map Selected Samples Shading Cache at t n Update of camera and object motion Figure 
1: The Shading Cache at time ....is used to generate a display representation which is drawn using graphics 
hardware. The Shading Cache is updated asynchronously of the display by .rst computing a priority map 
and then using it to select additional samples. The resulting mesh at time ..is used to compute a new 
display representation which is used starting from frame .... 2 Previous work Interactive rendering: 
Current interactive rendering methods are mostly based on graphics hardware. Hardware rendering can be 
made more realistic by adding shadows [Segal et al. 1992], specular re.ections and refractions [Diefenbach 
and Badler 1997] and general BRDF models [McCool et al. 2001]. Udeshi and Hansen [1999] employ parallel 
graphics hardware for shadows and CPUs for computing indirect illumination, thus extending the hard­ware 
shading model to include approximate one-bounce indirect lighting. Alternatively, an approximate representation 
of the global illumination can be pre-computed and viewed interactively using graphics hardware, as in 
[Walter et al. 1997a; Stamminger et al. 1999; Bastos et al. 1999; St¨urzlinger and Bastos 1997]. In the 
con­text of ray-tracing, Parker et al. [1999] and Wald et al. [2001] used low level optimizations and 
parallelism to show that interactive ray­tracing of complex scenes is feasible on today s computers, 
albeit with simple shading models. The Holodeck data structure [Lar­son and Simmons 1999] caches rays 
in order to provide an inter­active walkthrough, but it supports a very restricted object motion paradigm. 
Radiosity based methods: The radiosity method [Goral et al. 1984] can be used to pre-compute view-independent 
radiosity val­ues which can be viewed interactively. Extensions have been pro­posed for view-dependent 
illumination [Chen et al. 1991; Aupperle and Hanrahan 1993] and view-driven re.nement [Aupperle and Hanrahan 
1993; Smits et al. 1992], but they do not provide interac­tive rates. On the other hand, several researchers 
have demonstrated that a radiosity solution can be updated rapidly after localized ob­ject motion, see 
for example, [Chen 1990; Drettakis and Sillion 1997]. However, the image quality resulting from such 
systems may not be high because of the dif.culty in reproducing disconti­nuities such as sharp shadows 
using radiosity. Recently, Granier and Drettakis [2001] used a combination of hierarchical radiosity 
with clustering [Sillion et al. 1995] and par­ticle tracing [Jensen 1996; Walter et al. 1997b] to simulate 
global illumination including non-diffuse effects. In their system, diffuse scenes can be visualized 
using graphics hardware, while the Ren­derCache [Walter et al. 1999] is used for scenes containing non­diffuse 
surfaces. Their system even updates the global illumination solution on the diffuse surfaces at near-interactive 
rates when an ob­ject is moved, but they report pre-computation times ranging from 35 minutes to over 
an hour. In addition, their technique is likely to require a signi.cant amount of time to update the 
illumination after large changes to the scene such as moving light sources. Caching schemes: The RADIANCE 
system [Ward 1994] pro­duces high quality images off-line by caching lazily computed dif­fuse inter-re.ections 
and interpolating between the cached values. Radiance interpolants [Teller et al. 1996; Bala et al. 1999] 
cache radiance in 4-D line space, exploiting spatial and temporal coher­ence by using these interpolants 
over several frames. However, they assume the Whitted ray tracing model [Whitted 1980] for deriving the 
physical error bounds used to re.ne the interpolants. Moreover, their primary goal is to generate images 
within a speci.ed error tolerance, so the frame rate may drop when additional interpolants need to be 
constructed to satisfy the given error bounds. Reprojection-based schemes, inspired by frameless render­ing 
[Bishop et al. 1994], are most closely related to our work. Wal­ter et al. [1999] cache the results of 
previous images in a Ren­derCache and use reprojection to reduce the number of pixels that need to be 
computed per frame. The RenderCache also supports simple object motions. The Tapestry system [Simmons 
and S´equin 2000] uses an image-plane Delaunay mesh to reduce the visibil­ity artifacts of the RenderCache. 
However, this does not eliminate all the geometric artifacts. In particular, geometric edges have to 
be reconstructed using a very large number of point samples and even then, may be distorted when the 
camera moves. Stamminger et al. [2000], on the other hand, improve a pre-computed radiosity solution 
by using object-local or camera-local corrective textures. The radiosity solution together with the corrective 
textures are then displayed without any geometric artifacts using graphics hardware. However, using camera-local 
projective textures introduces repro­jection artifacts in the shading. Also, object-local textures must 
be of an extremely high resolution in order to reconstruct sharp shading features such as hard shadow 
boundaries. Finally, Tapestry and Corrective Texturing are primarily meant to provide global illu­mination 
walkthroughs and extending these systems to handle dy­namic scenes seems non-trivial. Image reconstruction 
from a sparse sample set and adaptive sampling: An entire image may be reconstructed from a sparse set 
of samples to enable rapid previewing [Darsa and Costa 1996]. The reconstruction may be improved by pre-processing 
the geometry [Pighin et al. 1997]. However, such pre-processes are too expensive for interactive applications. 
Computation of static images or ani­mation sequences can be accelerated using adaptive re.nement and 
perceptually-driven sampling [Guo 1998; Ramasubramanian et al. 1999; Myszkowski et al. 2001; Yee et al. 
2001]. But the resulting improvement in ef.ciency is not suf.cient to achieve interactivity.  3 System 
Overview Computation of global illumination is very slow, so in order to pro­vide a high frame rate, 
our system decouples the refresh of camera view and object motion from the update of shading values. 
This de­coupling is achieved by caching recently computed shading values in an object-space mesh data 
structure called the Shading Cache. The data structure for the Shading Cache is a hierarchical patch 
tree, similar to that used in hierarchical radiosity [Hanrahan et al. 1991]. Our system can handle all 
locally parameterizable surfaces; the cur­rent implementation supports triangles, quadrilaterals and 
bicubic curved surfaces. Each patch in the mesh stores the last computed shading values for its vertices. 
During initialization, the mesh for each geometric primitive is set to be a single un-subdivided patch. 
The shading values for these root patches are not pre-computed, but evaluated lazily when required for 
generating a good image. When a patch is selected for update, we either evaluate the shading values for 
its vertices or subdivide the patch into four children and evaluate the shading values for their vertices. 
For non-diffuse surfaces, the exi­tant radiance for the current viewing direction is used as the shading 
value, while for diffuse surfaces, the irradiance is used. This dis­tinction allows us to use texture 
mapping hardware during image reconstruction. The main building blocks of the system are shown schematically 
in Figure 2. The Shading Cache is updated in a view-driven fashion in order to re.ne the rendered image. 
An ef.cient display repre­sentation is then generated for the Image Generator, which uses the graphics 
pipeline for image reconstruction, including texture mapping, Gouraud interpolation and hidden surface 
removal. The User Interaction Loop 1 runs asynchronously from the update of the Shading Cache. This allows 
us to refresh the camera view and to display moving objects at a high frame rate without errors in ge­ometry 
or texture, irrespective of the shading speed. The Sample Selector assigns priorities to each pixel depending 
on the estimated error in its currently displayed value and uses this priority map to select locations 
in the image plane for computing additional shading samples. These samples are then computed by the Sample 
Renderer using any suitable global illumination algorithm. This basic idea of decoupling the camera view 
update from shad­ing calculations has been explored in the RenderCache [Walter et al. 1999], Tapestry 
[Simmons and S´equin 2000] and Corrective Tex­turing [Stamminger et al. 2000]. Of these, only the corrective 
tex­turing approach reproduces scene geometry accurately. All three of them rely on point sampling to 
reproduce textures. Our system 1We have implemented the User Interaction loop using OpenGL always reproduces 
the scene geometry and textures accurately and the frame rate is not related to the shading speed or 
display resolu­tion (within the limits of graphics hardware). This results in higher quality images and 
greater interactivity. Our choice of a hierarchi­cal mesh data structure is superior to a regularly subdivided 
cor­rective texture because it can reproduce high frequency features such as hard shadows without using 
excessive texture memory. We describe our system in detail in the following three sections. Later, in 
Section 7, we provide additional comparisons between our sys­tem and the other caching schemes. Figure 
2: An Overview of the System 3.1 Updating the Shading Cache The Shading Cache must be updated in order 
to progressively re.ne the displayed image. This may involve computing regions of the Shading Cache that 
were not required for previous frames or those that have inaccurate shading values. In addition, a display 
represen­tation that can be ef.ciently handled by the graphics pipeline must be generated. The sequence 
of steps that perform these operations is described next. A patch identi.er map is drawn and read back 
by the Image Gen­erator when required by the update process. The Sample Selector goes over this ID map 
and evaluates the priority for each patch that is visible in the current view. These priorities are then 
used to se­lect image plane locations for re.nement. The process of assigning priorities and selecting 
samples is described in detail in Section 4. The patches that correspond to each of the sample locations 
are then determined using the ID map. A list of selected patches with the number of samples lying on 
each patch is created. For each patch in this list, we either update the shading values for its ver­tices 
or subdivide the patch and compute new shading values for the vertices of its children. Patches with 
more sample requests are subdivided faster than others until eventually all patches project to one pixel 
in the image plane. We use aging to limit the size of the Shading Cache and to pre­vent the frame rate 
from dropping to an unacceptable level. For this, patches that are visible in the current view are marked 
as seen . Then, if the cache size has grown above a given threshold, patches that are no longer useful 
for rendering the current or immediately foreseeable views are deleted. Thus, all children of a patch 
are deleted if (a) none of them has been seen in a user-speci.ed num­ber of frames, or, (b) the patch 
itself projects to less than one pixel in the given view. This not recently used replacement strategy 
is based on the assumption that recently seen patches are likely to be in view again in the near future 
and hence should be retained, whereas patches that have not been in view recently are likely to remain 
outside our view. 3.2 Sample Renderer Shading values for selected samples are computed by the Sample 
Renderer. In our implementation, we use area sampling of light sources to compute direct illumination 
with soft shadows and a bidi­rectional path tracer [Veach and Guibas 1994] to compute indirect illumination. 
All examples in this paper have been rendered using 100 area samples per light and between 400 and 1200 
samples for the indirect illumination. Sampling is performed using the Halton sequence [Halton and Weller 
1964]. With these settings, our ren­derer can shade 10 to 100 points per second on one 1.7GHz Pentium 
4 processor in moderately complex scenes. This compares to to half a million pixels in a 800 X 600 image. 
Our system is designed to scale with the processing power available for rendering samples. We have found 
the system to be usable on dual processor worksta­tions with one of the processors rendering the samples, 
scaling up to 16 processors or more. 3.3 Ef.cient display The Shading Cache is used to generate an ef.cient 
display repre­sentation consisting of Gouraud interpolated, texture-mapped poly­gons. For planar objects, 
the patches in the Shading Cache can be displayed as is without introducing visibility artifacts. However, 
doing so for curved surfaces results in geometric errors, such as tears in the interior and incorrect 
silhouettes. Therefore, we tessel­late the curved surfaces in a view-dependent fashion and display this 
tessellation instead of the Shading Cache. Note that the ver­tices of this tessellation are not shaded 
using the expensive sample renderer, but simply by interpolation from the Shading Cache. The display 
representation uses OpenGL vertex arrays for ren­dering. In order to prevent synchronization overhead 
between the user interaction loop and the update loop, we maintain two copies of the vertex arrays for 
double buffering. Thus, synchronization is required only when swapping the arrays. We perform incremental 
updates to the arrays by keeping track of which patches are affected in the current update cycle. This 
way, the overhead of maintaining the Shading Cache is minimal. To summarize, the Shading Cache is updated 
using a view-driven re.nement process and then used to generate a display representa­tion that can be 
displayed at a high frame rate without errors in geometry or texture.  4 Sample Selection Since the 
Sample Renderer can only render a very small number of pixels per frame, the pixels to render must be 
selected carefully in order to provide good image quality. We use a priority-based scheme similar to 
Simmons and S´equin [2000] to direct the image plane samples into regions where the object-space Shading 
Cache needs more accuracy. In addition, we also use a novel .ood .lling operator to quickly re.ne the 
Shading Cache in regions containing discontinuities or high gradients. The process of sample selection 
is illustrated in Figure 3. In Section 5, we extend this basic scheme to handle view-dependent illumination 
and dynamic scenes ef.ciently. 4.1 Estimation of interpolation error The priority of a patch is based 
on the estimated error in the shading values of the pixels that it projects to. We estimate this error 
as the difference between the maximum and minimum colors of the vertices of the patch, after tone-mapping 
with the s-shaped operator of Tumblin et al. [1999] with weights of ...., ....and .... for the red, green 
and blue channels. This is because the visual difference between two radiance values is linear in the 
difference of their tone-mapped values. The error metric is simply the estimated interpolation error 
of each pixel covered by this patch. Since geometric discontinuities are always accurately reproduced 
by our system, we do not need the depth term used by Simmons and S´equin [2000]. In addition, we also 
do not quantize and pack the priority into 8 bits. Instead we simply generate an id map by drawing the 
32-bit pointer of the patch into the color channel. This allows us to evaluate the priorities lazily 
for just the visible patches and also provides greater .exibility in choice of error metrics. For example, 
our system can support view­dependent perceptual error metrics similar to [Ramasubramanian et al. 1999; 
Yee et al. 2001; Dumont et al. 2001].  4.2 Sampling the priority map The patch priorities can be thought 
of as the likelihood of error in the value of a pixel covered by the patch. Therefore, we would like 
to sample the patches with the highest priorities. As correctly ob­served by Walter et al. [1999], sorting 
the priorities and selecting the highest priority patches will lead to a very poor spatial distribu­tion 
of samples in the image plane. To avoid this problem, we use a hit-and-test approach. We draw an ID map 
at the beginning of each update cycle and compute the priority of every patch in view. After this, we 
normalize the priorities to a 0 to 1 range and treat the priority of a patch as the probability of selecting 
it for accurate computation. In our hit-and-test approach, a random pixel in the id map is se­lected 
for testing. Since the priorities are normalized between 0 and 1, we simply generate a random number 
in that range and accept the pixel if its priority is greater than the random number. Thus, the to­tal 
probability of selecting a patch is proportional to its priority and its projected area in pixels. Note 
that our approach does not require the selection of an ad hoc threshold for accepting samples and is 
also guaranteed to eventually select all pixels in the image plane. Outlier priorities can be a problem 
in the normalization step, so in­stead of mapping all priorities between the minimum and maximum value 
linearly to a [0,1] range, we instead compute the average and variance of the priorities and map values 
that are 4 standard devia­tions above and below the average to 1 and 0 respectively. 4.3 Avoiding bias 
in the sampling If we simply use a difference-based metric (or even a contrast-based one), it is possible 
that certain patches might get a zero or very low priority if the shading values at their corners are 
almost equal. However, this does not necessarily mean that the radiance function is constant in the interior 
of those patches. So, in order to avoid bias in the sampling, we accept samples with a certain small 
probability, ., irrespective of their priority. Thus, the total priority of selecting a pixel is given 
by .......................... (1) We have found that .values from ....to ....work well in prac­tice; 
we used ....in all our examples. 4.4 Reconstructing shading discontinuities Since the probability of 
hitting a patch with random sampling is proportional to its projected area, small patches are very unlikely 
to be tested. However, since the sampling scheme is trying to direct samples into regions of high gradients, 
such regions inevitably con­tain a large number of small patches. Thus, the patches around the shading 
discontinuity that need to be re.ned quickly end up being rarely tested by the hit-and-test process. 
In order to improve the rate of convergence in these cases, we run a second image plane selection phase 
that tests the neighboring locations of the already selected high priority samples and accepts them if 
their priority is greater than the current sample. This can easily be done by a .ood .lling algorithm 
so that already visited regions will not be touched again. The .ood .ll is stopped when all neighboring 
pixels have priority below threshold. This second phase is similar in spirit to the one presented by 
Hart et al. [1999]. Hit and test Samples Final mesh Initial mesh    Floodfilled Samples Figure 3: 
Illustration of the sample selection algorithm. The sampling procedure starts with the assignment of 
priorities to the patches in the id map. The priority map is used to select patches for accurate computation 
according to three criteria as illustrated above. We found that this two phase approach of selecting 
samples be­haves much better than the sorting approach or the simple hit-and­test. This is because the 
.rst phase has the advantage of uniformly spreading the samples in the image plane (as for hit-and-test), 
and the second one ensures that regions with small, high priority patches will be highly sampled (as 
for sorting). Figure 3 illustrates the entire sample selection process. The left­most column shows the 
initial mesh and the corresponding image, which is used to compute the priority map. Samples selected 
using this map are shown in the third column, color coded to show the mechanism of acceptance: red for 
samples selected by the hit-and­test mechanism, green for random samples and yellow for .ood .lled samples. 
The .nal mesh and the resulting image are shown in the rightmost column.  5 View-dependent effects 
and object mo­tion When the shading values in a scene change with camera or ob­ject motion, it is necessary 
to direct sampling speci.cally into the regions that are most affected by such motion. In this sec­tion, 
we extend the basic priority assignment and sample selec­tion scheme of Section 4 to ef.ciently update 
view-dependent and motion-dependent shading. 5.1 View-dependent illumination The shading of non-diffuse 
objects changes with the viewpoint, and hence should be recomputed when the camera moves. How­ever, given 
the constraint of interactivity, we cannot afford to re­compute the entire image each frame. Instead, 
we use the mecha­nism of priorities described in the previous section to preferentially re-compute the 
illumination on non-diffuse objects by increasing their priorities. This is achieved by keeping track 
of the last time at which each patch in the Shading Cache was computed. Non-diffuse patches computed 
before the last camera motion are assigned an age priority proportional to the difference between the 
current time and their time of computation (measured in cycles of the Shading Cache update loop). The 
constant of proportionality is always at least equal to 1; it is higher for objects whose shading is 
expected to change more rapidly with viewpoint. For glossy objects, we select this constant using a heuristic 
based on the perceptually uniform gloss scale proposed in Pellacini et al. [2000]. Similar strategies 
can be constructed for other re.ection models. More sophisticated heuristics that take into account the 
change in the viewing direction can also be developed. However, our simple heuristic performed adequately 
in our tests; for example, notice the sample distribution for the glossy objects in Figure 4. Note that 
the estimated error and the age priority are added together before the normalization and bi­asing steps 
described in the previous section.  5.2 Motion-dependent shading Moving objects present a tougher challenge 
than camera motion. We know that when the camera moves, we should only recom­pute the shading for non-diffuse 
surfaces. However, when an ob­ject moves, we .rst need to detect a change in shading and then re-compute 
all the affected regions of the scene. We rely on the random sampling to detect large changes in shading 
caused by ob­ject motion. As soon as the shading values of one patch change, the interpolation error 
of its neighboring patches increases, thereby raising their priorities. In addition, objects that detect 
such a change in shading are also aged just like non-diffuse objects, and eventu­ally they are selected 
for re-computation. For objects with both view-dependent and motion-dependent shading, the age priority 
is chosen to be the maximum of the age priorities from view depen­dence and motion dependence. It is 
important to note that we update the position of the moving objects instantaneously. The shading changes 
resulting from the object motion are updated asynchronously from position changes. Thus, users can receive 
useful feedback quickly even when the shading cannot be updated fast enough. Initial image 3 sec after 
moving light Priority map Image after 10 sec Initial image 1 sec after moving chair Priority map Image 
after 4 sec Initial image 1 sec after camera motion Priority map Image after 5 sec Figure 4: Handling 
moving and non-diffuse objects: The top row shows a moving light source, the middle row shows a moving 
chair in an (only)indirectly lit scene and the bottom row shows view dependent illumination. 5.3 Providing 
fast updates When the shading of a large portion of the scene is changing due to camera or object motion, 
the slow Sample Renderer often can­not provide updates at a suf.cient rate. So in order to improve the 
quality of the image presented to the user, it is necessary to limit the number of patches that need 
to be re-computed. This is achieved by lowering the resolution of the Shading Cache on the objects whose 
shading has been found to be changing. The new resolution is de­termined based on a target cleaning up 
time, which is a user­speci.ed parameter. The resolution drop is greater when the target time is lower 
and vice versa. When the shading stops changing, we continue re.ning the Shading Cache up to a 1 pixel 
projected size, so that the image can converge at the original screen resolution.  6 Results We tested 
our system on the scenes shown in Figure 5. The .rst scene is a room with 1 area light and about 4000 
primitives (in­cluding 9 untessellated curved objects). The second scene has 1 area light and about 300 
primitives. The illumination in this scene is almost entirely indirect. The third scene is a pool hall 
with 11 point lights and about 10000 primitives (including 15 untessellated curved objects). For the 
purpose of these tests, the sample renderer used 100 samples per area light to compute direct illumination 
and 400 bidirectional samples for indirect illumination. There was no pre-computation other than building 
a hierarchical regular grid for accelerating ray-casts. We ran walkthrough sessions on 2 sets of hardware 
for timings. First, we used one dual 1.7GHz Pentium 4 workstation, and then we added 8 more dual Pentium 
4 workstations to render the samples. The display machine in both cases had a GeForce3 graphics card. 
Scene 1 Scene 2 Scene 3 Points rendered per update 70 72 61 Sample Selection (ms) Patch subdivision (ms) 
Curved surface tessellation (ms) Aging (ms) Updating display meshes (ms) Rendering time (ms) 50 .1 12 
.1 1 611 36 .1 0 .1 1 490 12 .1 9 .1 2 506 Time per Iteration (ms) Frame rate 675 45 529 60 533 40 Table 
1: Performance statistics: 1 rendering client The results of these tests are shown in Tables 1 and 2. 
Note that for the multi-processor version, the samples to be rendered are sent to the renderers right 
after the sample selection is completed and read back at the end of the update iteration. Thus, the total 
update time per iteration may be less than the sum of the individual steps. These results show that our 
system has very little overhead other than sampling and curved surface tessellation. Of the two, sam­pling 
is approximately 10% of the total time. Also, the GeForce3 graphics card supports view-dependent tessellation 
of curved sur­faces in hardware. This feature could be used to reduce the tessel­lation overhead. In 
addition to these timings, Figure 4 shows typical performance with 8 dual Pentium 4 rendering clients 
in the case of dynamic scenes. Our system can provide global illumination solutions in scenes with moving 
geometry and view-dependent lighting. The system is usable for interaction on a single workstation and 
is scalable with processing power. The overhead of maintaining and re.ning the Figure 5: Test scenes 
used for measuring performance Shading Cache is minimal and so most of the available processing power 
is devoted to the task of rendering high quality samples. In a typical walkthrough, the number of patches 
(in excess of the orig­inal geometry) is about 20,000. When the camera stops moving, this number goes 
up and may approach the number of pixels in the image. The memory usage per patch (including the double 
buffered display representation) is under 200 bytes. Caustics present a challenge to all global illumination 
renderers based on path tracing from the eye. Since we compute the shading Table 2: Performance statistics: 
8 dual Pentium 4 rendering clients of only one point at a time, we cannot compute caustics effectively 
using bidirectional path tracing. However, this is a limitation of the underlying sample renderer, not 
of our system. Lighting effects such as caustics can be simulated using a sample renderer that can ef.ciently 
render them. To demonstrate this, we wrote a caustic renderer that speci.cally samples the specular surfaces 
in the scene, shown in Figure 6. While this particular algorithm is not suitable for rendering caustics 
in complex scenes, it does show that our system can support different global illumination algorithms. 
For example, dynamically updated caustic textures [Granier et al. 2000; Granier and Drettakis 2001] could 
be used for rendering caustics in more complex scenes. Scene 1 Scene 2 Scene 3 Points rendered per update 
288 363 328 Sample Selection (ms) Patch subdivision (ms) Curved surface tessellation (ms) Aging (ms) 
Updating display meshes (ms) Rendering time including network overhead (ms) 48 .1 5 .1 1 325 12 1 0 1 
2 424 16 .1 8 1 2 420 Time per Iteration (ms) Frame rate 375 45 444 60 440 40  Figure 6: Rendering 
caustics: The .rst image is computed by area sampling the glossy surfaces. The second image is a screen 
shot 5 seconds after moving the ring. 7 Comparison with similar systems Our approach based on caching 
shading values in object space is similar to the RenderCache [Walter et al. 1999], Tapestry [Simmons 
and S´equin 2000] and Corrective Texturing [Stamminger et al. 1999]. Of these, the Tapestry data structure 
uses an image plane Delaunay triangulation, and it is unlikely that this approach can be applied ef.ciently 
to dynamic scenes. Corrective texturing, on the other hand, is primarily useful as a walkthrough technique 
because of its reliance on a pre-computed radiosity solution. Finally, the RenderCache is a general caching 
scheme that can be used with any pixel-based renderer and supports moving objects. Since the Ren­derCache 
is the only system which provides the same feature set as our system, we performed additional performance 
comparisons between the two. The author of the RenderCache paper has allowed us to use his original implementation 
for comparisons. This implementation has been hand optimized to run on dual Pentium 4 workstations and 
supports multiple rendering clients, similar to our system. We in­terfaced our rendering clients to the 
RenderCache and compared the two systems running on identical hardware (one dual 1.7GHz Pentium 4 for 
display and 8 dual Pentium 4 rendering clients). The equal time and equal quality comparisons in Figures 
7, 8 and 9 show that our system offers substantial performance improvement both in static and dynamic 
scenes. In order to compare different caching schemes, we extend the concept of the render mismatch ratio 
from Walter et al. [1999] and Shading Cache RenderCache Tapestry Corrective Texturing Hierarchical Radiosity 
Pre-computation No No No Yes Yes Normalized Mismatch Ratio 1000 10 1000 1000 N/A Display Frame Rate .30 
10-20 (at 512 .512) 2-10 2-10 .30 Dynamic scenes Yes Yes No No Yes Reprojection artifacts No Yes Yes 
In shading only No High frequency shading detail Yes Yes Yes Yes (using large textures) No Restrictions 
on geometry Locally parameterizable None None Parameterizable Parameterizable Table 3: Comparison between 
various interactive global illumination techniques de.ne the normalized mismatch ratio as the number 
of pixels in a frame divided by the number of new points rendered per second by the sample renderer. 
The normalized mismatch ratio is essen­tially the time in seconds to compute each pixel in the image, 
and is a characteristic of the sample renderer being used. In all of our scenes, the normalized mismatch 
ratio is between 250 and 2500 (using an image size of .......and 8 dual Pentium 4 render­ing clients). 
Our system is very responsive even at such high ratios, while the RenderCache is only effective at ratios 
near 10 (or lower). In addition to the caching-based schemes, hierarchical radios­ity solutions can also 
be updated at interactive rates after object motion [Drettakis and Sillion 1997]. An extension of this 
ap­proach is the hybrid hierarchical radiosity/particle tracing algorithm of Granier and Drettakis [2001] 
which can compute the correct lighting on diffuse surfaces including non-diffuse effects. Both of these 
systems require an initial radiosity solution to be computed and only illuminate the diffuse surfaces. 
Also, large changes to the scene such as moving a light source will cause a severe slowdown. Moreover, 
these techniques can only provide a coarse resolution at interactive rates and so the quality of images 
may be poor, es­pecially near shadow boundaries. Our system uses a hierarchical radiosity-like data structure, 
but the view-driven re.nement allows us to generate high quality images quickly even in the presence 
of sharp shadows and high frequency indirect illumination (see for ex­ample, Scene 2 in Figure 5). The 
comparison between the various interactive global illumi­nation techniques is summarized in Table 3. 
We believe that our approach based on the Shading Cache is the best suited for appli­cations such as 
interactive lighting design and modelling while ren­dering.  8 Discussion and Future Work The renderings 
produced by our system in the initial stages have aliasing artifacts in sharp shadows and specular re.ections, 
along with light and shadow leaks similar to radiosity. However, unlike traditional radiosity systems, 
these artifacts are quickly detected and removed by our sampler. Non-importance-based radiosity sys­tems 
typically need to use discontinuity meshing to remove these artifacts, whereas the Shading Cache merely 
needs to be subdivided to a level that is suf.cient for image plane anti-aliasing. All caching systems 
suffer from popping artifacts when the ren­dered samples are noisy; blending new values slowly with the 
old values may reduce the popping. New perceptual error metrics are required that take into account effects 
such as popping and light or shadow leaks. In addition, contrast-based adaptive sampling schemes are 
falsely led into regions of noise. In our current system, we have side-stepped this problem by using 
Quasi Monte Carlo sampling and setting the sampling rate high enough to reduce band­ing artifacts. However, 
noise can be easily handled by our sampler by storing a variance estimate in addition to the radiance 
estimate at each vertex. The variance can then be used to decide whether to reduce noise by re.ning the 
radiance estimate at the vertices or to reduce contrast by subdividing the mesh. Such a scheme would 
probably also be useful in the RenderCache and Tapestry systems. We do not claim to provide real-time 
global illumination. In­deed, that goal is still at least an order of magnitude away. At the same time, 
we do not stall object or camera motion while shading values are being updated. This way the user can 
at least have a smooth interface and a high frame rate. However, depending on the application, blocking 
all motion until a reasonable image is avail­able may be the right choice and can be easily incorporated 
into our system. Our current implementation can handle all parameterizable sur­faces. The parameterization 
does not have to be well-behaved; we have rendered surfaces with singularities such as spheres without 
dif.culty. In a manner similar to the REYES algorithm [Cook et al. 1987], our subdivision scheme can 
be easily extended to handle all locally parametric surfaces that can be split into parameterizable primitives, 
including subdivision surfaces of arbitrary topology. Al­ternatively, a hybrid approach of point sampling 
the non-parametric surfaces and using the Shading Cache representation elsewhere also seems promising. 
We have demonstrated the usability of our system in moderately complex models with up to 10,000 primitives 
and dif.cult illumina­tion conditions. For handling environments that are geometrically much more complex, 
the hardware side may need to use frustum culling or occlusion culling [Schau.er et al. 2000]. However, 
for the purpose of computing shading values, only the on-screen geo­metric complexity matters. Our method 
will not provide signi.cant speedups if the initial tessellation is large or irregular, since at least 
one sample per visible primitive is required to get a reasonable esti­mate of the image. A possible direction 
of research would be to fur­ther separate the shading representation from the geometry, similar to clustering 
approaches for radiosity [Sillion et al. 1995; Willmott et al. 1999; Holzschuch et al. 2000]. Note that 
even without clus­tering, our system will perform at least as well as point sampling. 8.1 Conclusion 
In this paper, we have described a novel caching and interpola­tion scheme for global illumination that 
provides interactivity even when per pixel shading costs are high. Spatial coherence is ex­ploited in 
the reconstruction of a single image from a sparse set of shaded samples and temporal coherence is exploited 
by caching shading values and reusing them for several frames. While our sys­tem is useful in walkthrough 
applications, our biggest contribution is in the area of interactive update of global illumination after 
dras­tic changes to the illumination in the scene, such as when moving the light sources. As a result, 
we believe that our system will be very useful in applications such as lighting design and modelling. 
  Acknowledgements Thanks to Philip Dutr´e, Steve Westin, Randy Fernando and the anonymous reviewers 
for their comments, and to Jeremy Selan for helping with the Poolhall model. This work was supported 
by the NSF Science and Technology Center for Computer Graphics and Scienti.c Visualization (ASC-8920219) 
and MRA parallel global illumination project (ASC-9523483), and performed using equip­ment generously 
donated by the Intel Corporation and NVIDIA Corporation. References AUPPERLE,L., AND HANRAHAN, P. 1993. 
A hierarchical illumination algorithm for surfaces with glossy re.ection. Proceedings of SIGGRAPH 93 
(August), 155 162. ISBN 0-201-58889-7. Held in Anaheim, California. BALA,K., DORSEY,J., AND TELLER, S. 
1999. Radiance interpolants for accelerated bounded-error ray tracing. ACM Transactions on Graphics 18, 
3 (July), 213 256. ISSN 0730-0301. BASTOS,R., HOFF,K., WYNN,W., AND LASTRA, A. 1999. Increased photorealism 
for interactive architectural walkthroughs. 1999 ACM Symposium on Interactive 3D Graphics (April), 183 
190. ISBN 1-58113-082-1. BISHOP,G., FUCHS,H., MCMILLAN,L., AND ZAGIER, E. J. S. 1994. Frameless rendering: 
Double buffering considered harmful. Proceedings of SIGGRAPH 94 (July), 175 176. ISBN 0-89791-667-0. 
Held in Orlando, Florida. CHEN,S. E., RUSHMEIER,H. E., MILLER,G., AND TURNER, D. 1991. A progres­sive 
multi-pass method for global illumination. Computer Graphics (Proceedings of SIGGRAPH 91) 25, 4 (July), 
165 174. ISBN 0-201-56291-X. Held in Las Vegas, Nevada. CHEN, S. E. 1990. Incremental radiosity: An extension 
of progressive radiosity to an interactive image synthesis system. In Computer Graphics (Proceedings 
of SIGGRAPH 90), vol. 24, 135 144. ISBN 0-201-50933-4. COOK,R. L., CARPENTER,L., AND CATMULL, E. 1987. 
The reyes image rendering architecture. In Computer Graphics (Proceedings of SIGGRAPH 87), no. 4, 95 
102. DARSA,L., AND COSTA, B. 1996. Multiresolution representation and reconstruction of adaptively sampled 
images. Proceedings of SIBGRAPI 96 (October), 321 328. DIEFENBACH,P. J., AND BADLER, N. I. 1997. Multi-pass 
pipeline rendering: Re­alism for dynamic environments. 1997 Symposium on Interactive 3D Graphics (April), 
59 70. ISBN 0-89791-884-3. DRETTAKIS,G., AND SILLION, F. X. 1997. Interactive update of global illumina­tion 
using a line-space hierarchy. Proceedings of SIGGRAPH 97 (August), 57 64. ISBN 0-89791-896-7. Held in 
Los Angeles, California. DUMONT,R., PELLACINI,F., AND FERWERDA, J. A. 2001. A perceptually-based texture 
caching algorithm for hardware-based rendering. In Rendering Techniques 2001: 12th Eurographics Workshop 
on Rendering, Eurographics, 249 256. ISBN 3-211-83709-4. GORAL,C. M., TORRANCE,K. E., GREENBERG,D. P., 
AND BATTAILE, B. 1984. Modelling the interaction of light between diffuse surfaces. Computer Graphics 
(Proceedings of SIGGRAPH 84) 18, 3 (July), 213 222. Held in Minneapolis, Min­nesota. GRANIER,X., AND 
DRETTAKIS, G. 2001. Incremental updates for rapid glossy global illumination. Computer Graphics Forum 
20, 3, 268 277. ISSN 1067-7055. GRANIER,X., DRETTAKIS,G., AND WALTER, B. 2000. Fast global illumination 
including specular effects. Rendering Techniques 2000: 11th Eurographics Work­shop on Rendering (June), 
47 58. ISBN 3-211-83535-0. GUO, B. 1998. Progressive radiance evaluation using directional coherence 
maps. Proceedings of SIGGRAPH 98 (July), 255 266. ISBN 0-89791-999-8. Held in Orlando, Florida. HALTON,J., 
AND WELLER, G. 1964. Algorithm 247: Radical inverse quasi-random point sequence. Comm. ACM, 701 702. 
HANRAHAN,P., SALZMAN,D., AND AUPPERLE, L. 1991. A rapid hierarchical radiosity algorithm. Computer Graphics 
(Proceedings of SIGGRAPH 91) 25,4 (July), 197 206. ISBN 0-201-56291-X. Held in Las Vegas, Nevada. HART,D., 
DUTR E´,P., AND GREENBERG, D. P. 1999. Direct illumination with lazy visibility evaluation. Proceedings 
of SIGGRAPH 99 (August), 147 154. ISBN 0-20148-560-5. Held in Los Angeles, California. HOLZSCHUCH,N., 
CUNY,F., AND ALONSO, L. 2000. Wavelet radiosity on arbitrary planar surfaces. In Rendering Techniques 
2000: 11th Eurographics Workshop on Rendering, Eurographics, 161 172. ISBN 3-211-83535-0. JENSEN, H. 
W. 1996. Global illumination using photon maps. Eurographics Render­ing Workshop 1996 (June), 21 30. 
ISBN 3-211-82883-4. Held in Porto, Portugal. KAJIYA, J. T. 1986. The rendering equation. Computer Graphics 
(Proceedings of SIGGRAPH 86) 20, 4 (August), 143 150. Held in Dallas, Texas. LARSON,G. W., AND SIMMONS, 
M. 1999. The holodeck ray cache: An interac­tive rendering system for global illumination in non-diffuse 
environments. ACM Transactions on Graphics 18, 4 (October), 361 368. ISSN 0730-0301. LINDHOLM,E., KILGARD,M. 
J., AND MORETON, H. 2001. A user-programmable vertex engine. Proceedings of SIGGRAPH 2001 (August), 149 
158. ISBN 1­58113-292-1. MCCOOL,M. D., ANG,J., AND AHMAD, A. 2001. Homomorphic factorization of brdfs 
for high-performance rendering. Proceedings of SIGGRAPH 2001 (August), 171 178. ISBN 1-58113-292-1. 
MYSZKOWSKI,K., TAWARA,T., AKAMINE,H., AND SEIDEL, H.-P. 2001. Perception-guided global illumination solution 
for animation rendering. Proceed­ings of SIGGRAPH 2001 (August), 221 230. ISBN 1-58113-292-1. PARKER,S., 
MARTIN,W., SLOAN,P.-P. J., SHIRLEY,P., SMITS,B., AND HANSEN, C. 1999. Interactive ray tracing. 1999 ACM 
Symposium on Interactive 3D Graphics (April), 119 126. ISBN 1-58113-082-1. PELLACINI,F., FERWERDA,J. 
A., AND GREENBERG, D. P. 2000. Toward a psychophysically-based light re.ection model for image synthesis. 
Proceedings of SIGGRAPH 2000 (July), 55 64. ISBN 1-58113-208-5. PIGHIN,F. P., LISCHINSKI,D., AND SALESIN, 
D. 1997. Progressive previewing of ray-traced images using image plane discontinuity meshing. Eurographics 
Render­ing Workshop 1997 (June), 115 126. ISBN 3-211-83001-4. Held in St. Etienne, France. RAMASUBRAMANIAN,M., 
PATTANAIK,S. N., AND GREENBERG, D. P. 1999. A perceptually based physical error metric for realistic 
image synthesis. Proceedings of SIGGRAPH 99 (August), 73 82. ISBN 0-20148-560-5. Held in Los Angeles, 
California. SCHAUFLER,G., DORSEY,J., DECORET,X., AND SILLION, F. X. 2000. Conser­vative volumetric visibility 
with occluder fusion. Proceedings of SIGGRAPH 2000 (July), 229 238. ISBN 1-58113-208-5. SEGAL,M., KOROBKIN,C., 
VAN WIDENFELT,R., FORAN,J., AND HAEBERLI, P. E. 1992. Fast shadows and lighting effects using texture 
mapping. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 249 252. ISBN 0-201­51585-7. Held 
in Chicago, Illinois. SILLION,F. X., DRETTAKIS,G., AND SOLER, C. 1995. A clustering algorithm for radiance 
calculation in general environments. Eurographics Rendering Workshop 1995 (June), 196 205. Held in Dublin, 
Ireland. SIMMONS,M., AND S´ 2000.EQUIN, C. H. Tapestry: A dynamic mesh-based dis­play representation 
for interactive rendering. Rendering Techniques 2000: 11th Eurographics Workshop on Rendering (June), 
329 340. ISBN 3-211-83535-0. SMITS,B. E., ARVO,J. R., AND SALESIN, D. H. 1992. An importance-driven radiosity 
algorithm. Computer Graphics (Proceedings of SIGGRAPH 92) 26,2 (July), 273 282. ISBN 0-201-51585-7. Held 
in Chicago, Illinois. STAMMINGER,M., SCHEEL,A., GRANIER,X., PEREZ-CAZORLA,F., DRETTAKIS, G., AND SILLION, 
F.X.1999.Ef.cientglossyglobalilluminationwithinteractive viewing. Graphics Interface 99 (June), 50 57. 
ISBN 1-55860-632-7. STAMMINGER,M., HABER,J., SCHIRMACHER,H., AND SEIDEL, H.-P. 2000. Walkthroughs with 
corrective texturing. Rendering Techniques 2000: 11th Eu­rographics Workshop on Rendering (June), 377 
390. ISBN 3-211-83535-0. ST ¨ URZLINGER,W., AND BASTOS, R. 1997. Interactive rendering of globally illu­minated 
glossy scenes. Eurographics Rendering Workshop 1997 (June), 93 102. ISBN 3-211-83001-4. Held in St. Etienne, 
France. TELLER,S., BALA,K., AND DORSEY, J. 1996. Conservative radiance interpolants for ray tracing. 
Eurographics Rendering Workshop 1996 (June), 257 268. ISBN 3-211-82883-4. Held in Porto, Portugal. TUMBLIN,J., 
HODGINS,J. K., AND GUENTER, B. K. 1999. Two methods for display of high contrast images. 56 94. ISSN 
0730-0301. UDESHI,T., AND HANSEN, C. 1999. Towards interactive, photorealistic rendering of indoor scenes: 
A hybrid approach. Eurographics Rendering Workshop 1999 (June). Held in Granada, Spain. VEACH,E., AND 
GUIBAS, L. 1994. Bidirectional estimators for light transport. In Fifth Eurographics Workshop on Rendering, 
147 162. WALD,I., SLUSALLEK,P., BENTHIN,C., AND WAGNER, M. 2001. Interactive rendering with coherent 
ray tracing. Computer Graphics Forum 20, 3, 153 164. ISSN 1067-7055. WALTER,B., ALPPAY,G., LAFORTUNE,E. 
P. F., FERNANDEZ,S., AND GREEN-BERG, D. P. 1997. Fitting virtual lights for non-diffuse walkthroughs. 
Proceedings of SIGGRAPH 97 (August), 45 48. ISBN 0-89791-896-7. Held in Los Angeles, California. WALTER,B., 
HUBBARD,P. M., SHIRLEY,P., AND GREENBERG, D. F. 1997. Global illumination using local linear density 
estimation. ACM Transactions on Graphics 16, 3 (July), 217 259. ISSN 0730-0301. WALTER,B., DRETTAKIS,G., 
AND PARKER, S. 1999. Interactive rendering using the render cache. Eurographics Rendering Workshop 1999 
(June). Held in Granada, Spain. WARD, G. J. 1994. The radiance lighting simulation and rendering system. 
Proceed­ings of SIGGRAPH 94 (July), 459 472. ISBN 0-89791-667-0. Held in Orlando, Florida. WHITTED, T. 
1980. An improved illumination model for shaded display. Communi­cations of the ACM 23, 6 (June), 343 
349. WILLMOTT,A., HECKBERT,P. S., AND GARLAND, M. 1999. Face cluster radios­ity. In Eurographics Rendering 
Workshop 1999, Springer Wein / Eurographics, Granada, Spain. YEE,H., PATTANAIK,S., AND GREENBERG, D. 
P. 2001. Spatiotemporal sensi­tivity and visual attention for ef.cient rendering of dynamic environments. 
ACM Transactions on Graphics 20, 1 (January), 39 65. ISSN 0730-0301. Shading Cache 15 sec RenderCache 
15 sec RenderCache 180 sec Figure 7: Equal time and equal quality comparison between our system and the 
RenderCache for glossy re.ections Shading Cache 10 sec Render Cache 10 sec Render Cache 120 sec Figure 
8: Equal time and equal quality comparison between our system and the RenderCache for diffuse inter-re.ections 
 Move bookshelf Initial Image (Shading Cache, from Fig. 8) Shading Cache 10 sec Move bookshelf Initial 
Image (Render Cache, from Fig. 8) Render Cache 10 sec Render Cache 360 sec Figure 9: Equal time and equal 
quality comparison between our system and the RenderCache in dynamic scenes 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566614</article_id>
		<sort_key>547</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[A lighting reproduction approach to live-action compositing]]></title>
		<page_from>547</page_from>
		<page_to>556</page_to>
		<doi_number>10.1145/566570.566614</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566614</url>
		<abstract>
			<par><![CDATA[We describe a process for compositing a live performance of an actor into a virtual set wherein the actor is consistently illuminated by the virtual environment. The Light Stage used in this work is a two-meter sphere of inward-pointing RGB light emitting diodes focused on the actor, where each light can be set to an arbitrary color and intensity to replicate a real-world or virtual lighting environment. We implement a digital two-camera infrared matting system to composite the actor into the background plate of the environment without affecting the visible-spectrum illumination on the actor. The color reponse of the system is calibrated to produce correct color renditions of the actor as illuminated by the environment. We demonstrate moving-camera composites of actors into real-world environments and virtual sets such that the actor is properly illuminated by the environment into which they are composited.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[image-based lighting]]></kw>
			<kw><![CDATA[matting and compositing]]></kw>
			<kw><![CDATA[radiosity]]></kw>
			<kw><![CDATA[reflectance and shading]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40023545</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC ICT, Marina del Rey, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14041802</person_id>
				<author_profile_id><![CDATA[81100089374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wenger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC ICT, Marina del Rey, CA and Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43119199</person_id>
				<author_profile_id><![CDATA[81100346087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tchou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC ICT, Marina del Rey, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382419</person_id>
				<author_profile_id><![CDATA[81100521605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gardner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC ICT, Marina del Rey, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133939</person_id>
				<author_profile_id><![CDATA[81100225969]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jamie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Waese]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC ICT, Marina del Rey, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117244</person_id>
				<author_profile_id><![CDATA[81100179328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC ICT, Marina del Rey, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALEXANDER, D. K., JONES, P. J., AND JENKINS, H. The artificial sky and heliodon facility at Cardiff University. In Proc. International Building Performance Simulation Association (IBPSA) (September 1999). (Abstract).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BEN-EZRA, M. Segmentation with invisible keying signal. In Proc. IEEE Conf. on Comp. Vision and Patt. Recog. (June 2000), pp. 568-575.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122729</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BORGES, C. F. Trichromatic approximation for computer graphics illumination models. In Computer Graphics (Proceedings of SIGGRAPH 91) (July 1991), vol. 25, pp. 101-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>320164</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BRINKMANN, R. The Art and Science of Digital Compositing. Morgan Kaufmann, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344844</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHUANG, Y.-Y., ZONGKER, D. E., HINDORFF, J., CURLESS, B., SALESIN, D. H., AND SZELISKI, R. Environment matting extensions: Towards higher accuracy and real-time capture. In Proceedings of SIGGRAPH 2000 (July 2000), pp. 121-130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In SIGGRAPH 98 (July 1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND SAGAR, M. Acquiring the reflectance field of a human face. Proceedings of SIGGRAPH 2000 (July 2000), 145-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P. E., AND MALIK, J. Recovering high dynamic range radiance maps from photographs. In SIGGRAPH 97 (August 1997), pp. 369-378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FAUGERAS, O. Three-Dimensional Computer Vision. MIT Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FIELDING, R. The Technique of Special Effects Cinematography, 4th ed. Hastings House, New York, 1985, ch. 11, pp. 290-321.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GAT, N. Real-time multi- and hyper-spectral imaging for remote sensing and machine vision: an overview. In Proc. 1998 ASAE Annual International Mtg. (Orlando, Florida, July 1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. T. The rendering equation. In Computer Graphics (Proceedings of SIGGRAPH 86) (Dallas, Texas, August 1986), vol. 20, pp. 143-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KOUDELKA, M., MAGDA, S., BELHUMEUR, P., AND KRIEGMAN, D. Image-based modeling and rendering of surfaces with arbitrary brdfs. In Proc. IEEE Conf. on Comp. Vision and Patt. Recog. (2001), pp. 568-575.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND HANRAHAN, P. Light field rendering. In SIGGRAPH 96 (1996), pp. 31-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T., GELB, D., AND WOLTERS, H. Polynomial texture maps. Proceedings of SIGGRAPH 2001 (August 2001), 519-528. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MATUSIK, W., BUEHLER, C., RASKAR, R., GORTLER, S. J., AND MCMILLAN, L. Image-based visual hulls. In Proc. SIGGRAPH 2000 (July 2000), pp. 369-374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MILLER, G. S., AND HOFFMAN, C. R. Illumination and reflection maps: Simulated objects in simulated and real environments. In SIGGRAPH 84 Course Notes for Advanced Computer Graphics Animation (July 1984).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[NIMEROFF, J. S., SIMONCELLI, E., AND DORSEY, J. Efficient re-rendering of naturally illuminated environments. Fifth Eurographics Workshop on Rendering (June 1994), 359-373.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[PORTER, T., AND DUFF, T. Compositing digital images. In Computer Graphics (Proceedings of SIGGRAPH 84) (Minneapolis, Minnesota, July 1984), vol. 18, pp. 253-259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[RAMAMOORTHI, R., AND HANRAHAN, P. A signal-processing framework for inverse rendering. In Proc. SIGGRAPH 2001 (August 2001), pp. 117-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237263</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R., AND BLINN, J. F. Blue screen matting. In Proceedings of SIGGRAPH 96 (August 1996), pp. 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[TCHOU, C., AND DEBEVEC, P. HDR Shop. Available at http://www.debevec.org/HDRShop, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[TSAI, R. A versatile camera calibration technique for high accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. IEEE Journal of Robotics and Automation 3, 4 (August 1987), 323-344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[VIDOR, Z. An infrared self-matting process. Society of Motion Picture and Television Engineers 69 (June 1960), 425-427.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[WINDOWS AND DAYLIGHTING GROUP. The sky simulator for daylighting studies. Tech. Rep. DA 188 LBL, Lawrence Berkeley Laboratories, January 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[WYSZECKI, G., AND STILES, W. S. Color Science: Concepts and Methods, Quantitative and Formulae, 2nd ed. Wiley, New York, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[ZONGKER, D. E., WERNER, D. M., CURLESS, B., AND SALESIN, D. H. Environment matting and compositing. Proceedings of SIGGRAPH 99 (August 1999), 205-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Lighting Reproduction Approach to Live-Action Compositing Paul Debevec Andreas Wenger Chris Tchou 
Andrew Gardner Jamie Waese Tim Hawkins University of Southern California Institute for Creative Technologies 
1 ABSTRACT We describe a process for compositing a live performance of an ac­tor into a virtual set 
wherein the actor is consistently illuminated by the virtual environment. The Light Stage used in this 
work is a two-meter sphere of inward-pointing RGB light emitting diodes fo­cused on the actor, where 
each light can be set to an arbitrary color and intensity to replicate a real-world or virtual lighting 
environ­ment. We implement a digital two-camera infrared matting system to composite the actor into the 
background plate of the environ­ment without affecting the visible-spectrum illumination on the ac­tor. 
The color reponse of the system is calibrated to produce correct color renditions of the actor as illuminated 
by the environment. We demonstrate moving-camera composites of actors into real-world environments and 
virtual sets such that the actor is properly illumi­nated by the environment into which they are composited. 
Keywords: Matting and Compositing, Image-Based Lighting, Ra­diosity, Global Illumination, Re.ectance 
and Shading 1 Introduction Many applications of computer graphics involve compositing, the process of 
assembling several image elements into a single image. An important application of compositing is to 
place an image of an actor over an image of a background environment. The result is that the actor, originally 
.lmed in a studio, will appear to be in either a separately photographed real-world location or a completely 
virtual environment. Most often, the desired result is for the actor and environment to appear to have 
been photographed at the same time in the same place with the same camera, that is, for the results to 
appear to be real. Achieving a realistic composite requires matching many aspects of the image of the 
actor and image of the background. The two elements must be viewed from consistent perspectives, and 
their boundary must match the contour of the actor, blurring appropri­ately when the actor moves quickly. 
The two elements need to exhibit the same imaging system properties: brightness response curves, color 
balance, sharpness, lens .are, and noise. And .nally, 1USC ICT, 13274 Fiji Way, Marina del Rey, CA, 90292 
Email: debevec@ict.usc.edu, aw@cs.brown.edu, tchou@ict.usc.edu, gard­ner@ict.usc.edu, jamie waese@cbc.ca, 
timh@ict.usc.edu. Andreas Wenger worked on this project during a summer internship at USC ICT while a 
student of computer science at Brown University. See also http://www.debevec.org/Research/LS3/ Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 Figure 1: Light Stage 3 focuses 156 red-green-blue LED lights to­ward an actor, who is .lmed simultaneously 
with a color camera and an infrared matting system. The device composites an actor into a background 
environment as illuminated by a reproduction of the light from that environment, yielding a composite 
with consis­tent illumination. In this image the actor is illuminated by light recorded in San Francisco 
s Grace Cathedral. the lighting of the two elements needs to match: the actor should ex­hibit the same 
shading, highlights, indirect illumination, and shad­ows they would have had if they had really been 
standing within the background environment. If an actor is composited into a cathedral, his or her illumination 
should appear to come from the cathedral s candles, altars, and stained glass windows. If the actor is 
walking through a science laboratory, they should appear to be lit by .uo­rescent lamps, blinking readouts, 
and indirect light from the walls, ceiling, and .oor. In wider shots, the actor must also photometri­cally 
affect the scene: properly casting shadows and appearing in re.ections. The art and science of compositing 
has produced many tech­niques for matching these elements, but the one that remains the most challenging 
has been to achieve consistent and realistic illumi­nation between the foreground and background. The 
fundamental dif.culty is that when an actor is .lmed in a studio, they are illu­minated by something 
often quite different than the environment into which they will be composited typically a set of studio 
lights and the green or blue backing used to distinguish them from the background. When the lighting 
on the actor is different from the lighting they would have received in the desired environment, the 
composite can appear as a collage of disparate elements rather than an integrated scene from a single 
camera, breaking the sense of re­alism and the audience s suspension of disbelief. Experienced practitioners 
will make their best effort to arrange the on-set studio lights in a way that approximates the positions, 
colors, and intensities of the direct illumination in the desired back­ground environment; however, the 
process is time-consuming and dif.cult to perform accurately. As a result, many digital composites require 
signi.cant image manipulation by a compositing artist to convincingly match the appearance of the actor 
to their background. Complicating this process is that once a foreground element of an actor is shot, 
the extent to which a compositor can realistically alter the form of its illumination is relatively restricted. 
As a result of these complications, creating live-action composites is labor inten­sive and sometimes 
fails to meet the criteria of realism desired by .lmmakers. In this paper, we describe a process for 
achieving realistic com­posites between a foreground actor and a background environment by directly illuminating 
the actor with a reproduction of the direct and indirect light of the environment into which they will 
be com­posited. The central device used in this process (Figure 1) consists of a sphere of one hundred 
and .fty-six inward-pointing computer­controlled light sources that illuminate an actor standing in the 
cen­ter. Each light source contains red, green, and blue light emitting diodes (LEDs) that produce a 
wide gamut of colors and intensities of illumination. We drive the device with measurements or simu­lations 
of the background environment s illumination, and acquire a color image sequence of the actor as illuminated 
by the desired environment. To create the composite, we implement an infrared matting system to form 
the .nal moving composite of the actor over the background. When successful, the person appears to ac­tually 
be within the environment, exhibiting the appropriate colors, highlights, and shadows for their new environment. 
 2 Background and Related Work The process of compositing photographs of people onto photo­graphic backgrounds 
dates back to the early days of photography, when printmakers would create combination prints by multiply­exposing 
photographic paper with different negatives in the dark­room [4]. This process evolved for motion picture 
photography, meeting with early success using rear projection in which a movie screen showing the background 
would be placed behind the actor and the two would be .lmed together. A front-projection approach developed 
in the 1950s (described in [10]) involved placing a retro­re.ective screen behind the actor, and projecting 
the background image from the vantage point of the camera lens to re.ect back to­ward the camera. The 
most commonly used process to date has been to .lm the actor in front of a blue screen, and process the 
.lm to produce both a color image of the actor and a black and white matte image showing the actor in 
white and the background in black. Using an optical printer, the background, foreground, and matte elements 
can be combined to form the composite of the actor in front of the background. The color difference method 
developed by Petro Vlahos (de­scribed in [21]) presented a technique for reducing blue spill from the 
background onto the actor and providing cleaner matte edges for higher quality composites. Variants of 
the blue screen process have used a second piece of .lm to record the matte rather than deriving it from 
a color image of the actor. A technique using infrared (or ul­traviolet) sensitive .lm [24] with a similarly 
illuminated backdrop allowed the mattes to be acquired using non-visible light, which avoided some of 
the problems associated with the background color spilling onto the actor. A variant of this process 
used a background of yellow monochromatic sodium light, which could be isolated by the matte .lm and 
blocked from the color .lm, producing both high-quality mattes and foregrounds. The optical printing 
process has been re.ned using digital image processing techniques, with a classic paper on digital compositing 
presented at SIGGRAPH 84 by Porter and Duff [19]. More re­cently, Smith and Blinn [21] presented a mathematical 
analysis of the color difference method and explained its challenges and vari­ants. They also showed 
that if the foreground element could be pho­tographed with two different known backings, then a correct 
matte for the object could be derived for any colored object without apply­ing the heuristic approximations 
used in other methods. The recent technique of Environment Matting [27] showed that by using a se­ries 
of patterned backgrounds, the object could be shown properly refracting and re.ecting the background 
behind it. Both of these lat­ter techniques generally require several images of the subject with different 
backgrounds, which poses a challenge for compositing a live performance using this method. [5] showed 
an extension of environment matting applied to live video. Our previous work [7] addressed the problem 
of replicating en­vironmental illumination on a foreground subject when composit­ing into a background 
environment. This work illuminated the per­son from a large number of incident illumination directions, 
and then computed linear combinations of the resulting images as in [18] to show the person as they would 
appear in a captured illu­mination environment [17, 6]. This synthetically illuminated im­age could then 
be composited over an image of the background, allowing the apparent light on the foreground object to 
be the same light as recorded in the real-world background. Again, this tech­nique (and subsequent work 
[15, 13]) requires multiple images of the foreground object which precludes its use for live-action sub­jects. 
In this work we extend the lighting reproduction technique in [7] to work for a moving subject by using 
an entire sphere of computer­controlled light sources to simultaneously illuminate the subject with a 
reproduction of a captured or computed illumination envi­ronment. To avoid the need for a colored backing 
(which might contribute to the illumination on the subject and place restrictions on the allowable foreground 
colors), we implement a digital version of Zoli Vidor s infrared matting system [24] using two digital 
video cameras to composite the actor into the environment by which they are being illuminated. A related 
problem in the .eld of building science has been to visualize architectural models of proposed buildings 
in controllable outdoor lighting conditions. One device for this purpose is a he­liodon, in which an 
arti.cial sun is moved relative to an architec­tural model, and another is a sky simulator, in which 
a hemispher­ical dome is illuminated by arrays of lights of the same color to simulate varying sky conditions. 
Examples of heliodons and sky simulators are at the College of Environmental Design at UC Berke­ley [25], 
the Building Technology Laboratory at the University of Michigan, and the School of Architecture at Cardiff 
University [1].  3 Apparatus This section describes the construction and design choices made for the 
structure of Light Stage 3, its light sources, the camera system, and the matte backing. 3.1 The Structure 
The purpose of the lighting structure is to position a large number of inward-pointing light sources, 
distributed about the entire sphere of incident illumination, around the actor. The structure of Light 
Stage 3 (Figure 1) is based on a once subdivided icosahedron, yielding 42 vertices, 120 edges, and 80 
faces. Placing a light source at each vertex and in the middle of each edge allows for 162 lights to 
be evenly distributed on the sphere. The bottom 5-vertex of the struc­ture and its adjoining edges were 
left out to leave room for a person, reducing the number of lights in the device to 156. The stage was 
designed to be two meters in diameter, a size chosen to .t a stan­dard room, to be large enough to .lm 
as wide as a medium close-up, to keep the lights far enough from the actor to appear distant, and close 
enough to provide suf.cient illumination. We mounted the sphere on a base unit 80cm tall to place the 
center of the sphere at the height of a tall person s head. To minimize re.ections onto the actor, the 
structure was painted matte black and placed in a dark­colored room. Figure 2: Color and Infrared Lights 
A detail of Light Stage 3 showing two of the 156 RGB LED lights and one of the six infrared LED light 
sources. At right is the edge of the infrared-re.ective backing for the stage, attached to the fronts 
of the lights with holes to let the lights shine through. The infrared lights, .agged with black tape 
to illuminate just the backing, produce light invisible to both the human eye and the color camera used 
in our system. They do, however, produce a response in the digital camera used to take this picture. 
 3.2 The Lights The lights we used were iColor MR lights from Color Kinetics cor­poration. Each iColor 
light (see Figure 2) consists of a mixture of eighteen red, green, and blue LEDs. Each light, at full 
intensity, produces approximately 20 lux on a surface one meter away. The beam spread of each light is 
slightly narrower than ideal, falling off to 50% of the maximum at twelve degrees from the beam center. 
We discus how we compensated for this falloff in Section 4. The lights receive their power and control 
signals from common wires, simplifying the wiring. The lights interface to the computer via the Universal 
Serial Bus allowing us to update all of the light colors at video frame rates. Each color component of 
each light is driven by an 8-bit values 0 through 255, with the resulting intensi­ties produced through 
pulse width modulation of the current to the LEDs. The rate of the modulation is suf.ciently fast to 
produce continuous shades to the human eye as well as to the color video camera. The color LEDs produce 
no infrared light, facilitating the infrared matting process and avoiding heat on the actor. We found 
156 lights to be suf.cient to produce the appearance of a continuous .eld of illumination as re.ected 
in both diffuse and specular skin (Fig. 3), a result consistent with the signal processing analysis of 
diffuse re.ection presented in [20]. For tight closeups, however, the lights are sparse enough that double 
shadows can be seen when two neighboring lights are used to approximate a sin­gle light halfway between 
them (Fig. 3(d)). The re.ection of the lights in extreme closeups of the eyes also reveals the discrete 
light sources (Fig. 3(f)), but for a medium closeup the lights blur to­gether realistically (Fig. 3(f)). 
We found the total illumination from the lights to be just adequate for getting enough illumination to 
the cameras, which suggests that even more lights could be used.  3.3 The Camera System The camera system 
(Figure 4) was designed to simultaneously cap­ture color and infrared images of the actor in the light 
stage. For these we chose a Sony DXC-9000 three-CCD color camera and a Uniq Vision UP-610 monochrome 
camera. The DXC-9000 was chosen to produce high quality, progressive scan color images at up to 60 frames 
per second, and the UP-610 was chosen for producing high quality, progressive scan monochrome images 
with good sen­sitivity in the near infrared (700-900 nm) spectrum asynchronously up to 110 frames per 
second. Since the UP-610 is also sensitive to visible light, we used a Hoya R72 infrared .lter to transmit 
only the infrared light to the UP-610. Conveniently, the DXC-9000 color  (a) (b) (c) (d) (e) (f) 
  Figure 3: Lighting Resolution (a) An actor illuminated by just one of the light sources. (b) A closeup 
of the shadows on the actor s face in (a). (c) The actor illuminated by two adjacent light sources, approximating 
a light halfway between them. (d) A closeup of the resulting double shadow. (e) The actor illuminated 
by all 156 lights, approximating an even .eld of illumination. (f) A closeup of the actor s eye in this 
.eld, showing the lights as individual point re­.ections. Inset: a detail of video image (e), showing 
that the light sources are numerous enough to appear as a continuous re.ection for video resolution medium 
closeups. camera had no response to infrared light, which is not true of all color video cameras. The 
DXC-9000 and UP-610 cameras were mounted at right an­gles on a board using two translating Bogen tripod 
heads, allowing the cameras to be easily adjusted forwards and backwards up to four inches. This adjustment 
allows the camera nodal points to be re-aligned for different lens con.gurations. A glass beam splitter 
was placed between the two cameras at a 45 degree angle, re.ect­ing 30% of the light toward the UP-610 
and transmitting 70% to the DXC-9000, which worked well for the relative sensitivities of the two cameras. 
Improved ef.ciency could be obtained by using a hot mirror to re.ect just the infrared light toward the 
infrared camera. Both cameras yield video-resolution 640 x480 pixel images; for future work it would 
be of interest to design a high-de.nition ver­sion of this system. Each camera is connected to a PC computer 
with a frame grabber for capturing images directly to memory. With 2GB of memory, the systems can record 
shots up to .fteen seconds long. We used the DXC-9000 s standard zoom lens at its widest setting (approximately 
a 40 degree .eld of view) in order to see as much of the environment behind the person as possible. For 
the UP-610 we used a 6mm lens with a 42 degree horizontal .eld of view, allowing it to contain the .eld 
of view of the DXC-9000. Ideally, a future version of the system would employ a single 4-channel camera 
sen­sitive to both infrared and RGB color light, allowing changing the focus, aperture, and zoom during 
a shot.  3.4 The Infrared Lights and Cloth The UP-610 camera is used to acquire a matte image for composit­ing 
the actor over the background. We wished to obtain the matte without restricting the colors in the foreground 
and without illumi­nating the actor with colored light from a blue or green backing. We solved both of 
these problems by implementing an infrared matting system wherein the subject is placed in front of a 
.eld of infrared il­lumination. We chose the infrared system over a polarization-based system [2] due 
to the relative ease of illuminating the background with infrared rather than polarized light. We used 
infrared rather than ultraviolet light due to the relative ease of obtaining infrared lights, cameras, 
and optics. And we chose the infrared system over a front-projection system due to the relative ease 
of constructing the backing and to facilitate making adjustments to the background image (for example, 
placing virtual actors in the background) after shooting. To provide the infrared .eld we searched for 
a backing material that would re.ect infrared light but not visible light; if the material were to re.ect 
visible light it would re.ect stray illumination onto the actor. To .nd this material, we used a Sony 
night shot cam­corder .tted with the Hoya IR-pass .lter. With this we found many black fabrics in a typical 
fabric store that re.ected a considerable proportion of incident infrared light (Figure 5). We chose 
one of the fabrics that was relatively dark, reasonably durable, and somewhat stretchy. We cut the fabric 
to cover twenty faces of the lighting structure, .lling the .eld of view of the video cameras with additional 
latitude to pan and tilt. The cloth was cut to be slightly smaller than its intended size so that it 
would stretch tightly, and holes were cut for the light sources to shine through. The fabric was attached 
to the fronts of the lights with Velcro. To light the backing, we attached six Clover Electronics infrared 
LED light sources (850nm peak wavelength) to the inside of the stage between the colored lights (Figure 
2). The cloth backing, illuminated by the infrared LEDs and seen from the infrared cam­era, is shown 
in Figure 6(a). Though darker than the cloth, enough Figure 4: The Camera System The camera system consists 
of a 3-CCD Sony DXC-9000 color video camera to record the actor (left) and a Uniq Vision UP-610 monochrome 
video camera to record the infrared matte. The cameras are placed with coincident nodal points using 
a 30R / 70T beam splitter.  (a) (b) Figure 5: Infrared Re.ection of Cloth (a) Six black cloth sam­ples 
illuminated by daylight, seen in the visible spectrum. (b) The re.ection of the same cloth materials 
under daylight in the near in­frared part of the spectrum. The top sample is the one used for the cloth 
backing in the light stage, just under it is a sample of Duva­teen, and the bottom four are various black 
T-shirts. light re.ects off the light sources to detect them as part of the back­ground.  4 System 
Calibration This section presents the procedures we performed to calibrate the color and intensity of 
the light sources, the geometric registration of the color and infrared video images, and the brightness 
consistency of the color and infrared video images. 4.1 Intensity and Color Calibration Our calibration 
process began by measuring the intensity response characteristics of both the light sources and the cameras, 
so that all of our image processing could be performed in relative radiometric units. We .rst measured 
the intensity response characteristics of the iColor MR LED lights using a Photo Research PR-650 spectro­radiometer, 
which acquires radiometric readings every 4nm from 380nm to 780nm. We iterated through the 8-bit values 
for each of the colors of the lights, taking a spectral measurement at each value. We found that the 
lights maintained the same spectrum at all levels of brightness and that they exhibited a nonlinear intensity 
response to their 8-bit inputs (Fig. 7(a)). We found that the intensity response for each channel could 
be modeled accurately by a gamma curve of the form L =z/255). where L is the relative light radiance, 
z is the eight-bit input to the light, and . =1.86. From this equation we map desired linear light levels 
to 8-bit light inputs as z =255L(1/.). The nonlinear response is a great bene.t since it dramatically 
increases each light s usable dynamic range: a fully illuminated light is more than ten thousand times 
as bright as a minimally illuminated one, and this allows us to make use of high dynamic range illumination 
environments. To ensure that the lights produced no ultraviolet or infrared light, we measured the emission 
spectra of the red, green, and blue light LEDs as shown in Figure 8. We calibrated the intensity response 
characteristics of the color channels of the DXC-9000 color video camera (Fig. 7(b)) and the monochrome 
UP-610 camera using the calibration technique in [8]. This allowed us to represent imaged pixel values 
using linear values in the range [0,1]for each channel. For the rest of this discussion all RGB triples 
for lights and pixels refer to such linear values. The light stage software control system (Fig. 9) takes 
as input a high dynamic range omnidirectional image of incident illumination [8, 6] in a longitude-latitude 
format; we have generally used images sampled to 128 x64 pixels in the PFM [22] .oating-point image format. 
We determine the 156 individual light colors from such an image as follows. First, the triangular structure 
of the light stage is projected onto the image to divide the image into triangular cells with one light 
at each vertex. The color for each light is determined as an average of the pixel values in the .ve or 
six triangles adjacent Color Kinetics iColor MR Intensity Response Sony DXC9000 Response Curve 1 1 0.9 
0.9 0.8 0.8 0.7 0.7 0.6 0.6 0.5 Exposure 0.5 Light    0.4 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 8-bit Color 
Value Pixel Value (a) (b) 0 (a) (b) Figure 7: Intensity Response Curves (a) The measured intensity 
response curve of the iColor MR lights. (b) The derived intensity response curve of the DXC-9000 color 
video camera. (c) (d)  (a) (b) Figure 8: Light Source and Skin Spectra (a) The emission spec­tra of 
the red, green, and blue LED lights, from 380nm to 700nm. (d) The spectral re.ectance of a patch of 
skin on a person s face.  (e) (f) Figure 6: The Matting Process (a) The cloth backing, illumi­nated 
by the six infrared light sources. The dark circular disks are the fronts of the RGB lights appearing 
through holes in the cloth. (b) An infrared image of the actor in the light stage. (c) The matte obtained 
by dividing image (b) by the clean plate in (a); the actor remains black and the background divides out 
to white. (d) The color image of the actor in a lighting environment. (e) A corre­sponding environment 
background image, multiplied by the matte in (c) leaving room for the actor. (f) A composite image formed 
by compositing (d) onto (e) using the matte (c). The actor s image has been adjusted for color balance 
and brightness falloff as described in Section 4. to its vertex. In each triangle, the pixels are weighted 
according to their Barycentric weights with respect to the light s vertex so that a pixel will contribute 
more light to closer vertices. To treat all light fairly, the contribution of each pixel is weighted 
by the solid angle that the pixel represents. As an example, an image of incident illumination with all 
pixels set to an RGB value of (0.25,0.5,0.75) sets each light to the color (0.25,0.5,0.75). An image 
with just one non-zero pixel illuminates either one, two, or three of the lights depending on whether 
the pixel s location maps to a vertex, edge, or face of the structure. If such a single-pixel lighting 
environment is rotated, different com­binations of one, two, and three neighboring lights will fade up 
and down in intensity so that the pixel s illumination appears to travel continuously with constant intensity. 
We calibrated the absolute intensity and color balance of our sys­tem by placing a white re.ectance standard 
11 into the middle of the light stage facing the camera. The re.ectance standard is close to Lambertian 
and approximately 99% re.ective across the visible spectrum, and we assume it to have a diffuse albedo 
.d =1 for our purposes. Suppose that the light stage could produce a completely even sphere of incident 
illumination with radiance L coming from all directions. Then, by the rendering equation [12], the radiance 
R of the standard would be: .. .d 1 R =L cos. d. =L cos. d. =L. .p .p This equation simply makes the 
observation that if a white dif­fuse object is placed within an even .eld of illumination, it will appear 
the same intensity and color as its environment. This ob­servation allows us to photometrically calibrate 
the light stage: for an incident illumination image with RGB pixel value P, we should send an RGB value 
to the lights L to produce a re.ected value R from the re.ectance standard as imaged by the color video 
camera where R =P. If we simply choose L =P, this will not necessarily be the case, since the spectral 
sensitivities of the CCDs in the DXC­9000 camera need not have any particular relationship to the spec­tral 
curves of the iColor LEDs. A basic result from color science [26], however, tells us that there exists 
a 3 x3 linear transformation matrix M such that if we choose L =MP we will have R =P for all P. We compute 
this color transformation M by sending a variety of colors Li distributed throughout the RGB color cube 
to the lights and observing the resulting colors Ri of the re.ectance standard imaged in the color video 
camera. Since we desire that R =P,we substitute R for P to obtain: L =MR. Since M is a 3 x3 matrix, we 
can solve for it in a least squares sense as long as we have at least three Li,Ri)color measurements. 
We accomplished this using the MATLAB numerical analysis soft­ware package by computing M =L.R, the package 
s notation for solving for M using the Singular Value Decomposition method. With M determined, we choose 
light colors L =MP to ensure that the intensity and color of the light re.ected by the standard matches 
the the color and intensity of any even sphere of incident illumination. For our particular color camera 
and LED lights, we found that the matrix M was nearly diagonal, with the off-diagonal elements being 
less than one percent of the magnitude of the diago­nal elements. Speci.cally, the red channel of the 
camera responded  Figure 9: The Lighting Control Program, showing the longi­tude/latitude image of 
incident illumination (upper right), the tri­angular resampling of this illumination to the light stage 
s lights (middle right), the light colors visualized upon the sphere of illu­mination (left), and a simulated 
sphere illuminated by the stage and composited onto the background (lower right). very slightly to the 
green of the lights, and the green channel re­sponded very slightly to the blue. Since the off-diagonal 
elements were so small, we in practice computed the color transformation from P to L as a simple scaling 
of the color channels of P based on the diagonals of the matrix M. We note that while this transformation 
corrects the appearance of the white re.ectance standard for all colors of incident illumination, it 
does not necessarily correct the appearance of objects with more complex re.ectance spectra. Fortunately, 
the spectral re.ectance of skin (Figure 8(d)) is relatively smooth, so calibrating the light colors based 
on the white re.ectance standard is a reasonable ap­proach. A spectral analysis of this lighting process 
is left as future work (see Section 6).  4.2 Matte Registration Our matte registration process involved 
.rst aligning the nodal points of the color and infrared cameras and then computing a 2D warp to compensate 
for the remaining misalignment. We measured the nodal points of each camera by placing the camera .at 
on a large horizontal piece of cardboard, and viewing a live video image of the camera output on the 
computer screen. We then used a long ruler to draw lines on the cardboard radiating out from the camera 
lens. For each line, we aligned the ruler so that its edge would ap­pear perfectly vertical in live image 
(Fig. 10a), at times moving the video window partially off the desktop to verify the straightness. We 
drew .ve lines for each camera (Fig. 10b) including lines for the left and right edges of the frame, 
a line through the center, and two intermediate lines, and also marked the position of the front of the 
camera lens. Removing the camera, we traced the radiating lines to their point of convergence behind 
the lens and noted the distance of the nodal point with respect to the lens front. Also, we measured 
the horizontal .eld of view of each lens using a protrac­tor. For this project, we assumed the center 
of projection for each camera to be at the center of the image; we realized that determin­ing such calibration 
precisely would not be necessary for the case of placing organic forms in front of backdrops. The camera 
focal lengths and centers of projection could also have been determined using image-based calibration 
techniques (e.g. [23]), but such tech­niques do not readily produce the position of the nodal point of 
the camera with respect to the front of the lens. We determined that for the UP-610 camera that the nodal 
point was 17mm behind the front of the lens. For the DXC-9000, the (a) (b) (a) The nodal point and .eld 
of view of each camera were measured by setting the camera on a horizontal piece of cardboard (a) and 
drawing converging lines on the cardboard corresponding to verti­cal lines in the camera s .eld of view. 
(b) The .eld of view was measured as the angle between the leftmost and rightmost lines; the nodal point, 
with respect to the front of the lens, was measured as the convergence of the lines. (a) (b) (c) Figure 
11: Geometric and Photometric Calibration (a) The cal­ibration grid placed in the light stage to align 
the infrared camera image to the color camera image (b) A white re.ectance standard imaged in an even 
sphere of illumination, used to determine the mapping between light colors and camera colors (c) A white 
card placed in the light stage illuminated by all of the lights to calibrate the intensity falloff characteristics. 
rays converged in a locus between 52mm and 58mm behind the front surface of the lens. Such behavior is 
not unexpected in a com­plex zoom lens, as complex optics need not exhibit an ideal nodal point. We chose 
an intermediate value of 55mm and positioned the two cameras 85mm and 47mm away from the front surface 
of the beam splitter respectively for the UP-610 and the DXC-9000, placing each camera s nodal point 
102mm behind the glass. Even with the lenses nodally aligned, the matte and color im­ages will not line 
up pixel for pixel (after horizontally .ipping the re.ected UP-610 image) due to differences in the two 
cameras ra­dial distortion, sensor placement, and the fact that infrared light may focus differently 
than visible light. We corrected for any such misalignments by photographing a checkerboard grid covering 
both .elds of view placed at the same distance from the camera as the actor (Fig. 11(a)). One of the 
central white tiles was marked with a dark spot, and the corners of this spot were indicated to the com­puter 
in each image to seed the automatic registration process. The algorithm convolves each image with a 4 
x4 image of an ideal checker corner, and takes the absolute value of the output, to pro­duce images where 
the checker corners appear as white dots on a black background. The subpixel location of the dot s center 
is deter­mined by .nding the brightest pixel in a small region around the dot and then .tting a quadratic 
function through it and its 4-neighbors. Using the seed square as a guide, the program searches spirally 
out­ward to calculate the remaining tile correspondences between the two images. With correspondences 
made between the checker corners in the two images, 2D homographies [9] are computed to map the pixels 
in each square in the color image to pixels in each square in the IR image. From these homographies, 
a u-v displacement map is created indicating the subpixel mapping between the color and IR image coordinates. 
With this map, each matte is warped to align with the visible camera image for each frame of the captured 
se­quence. In this work we ran the color camera at its standard rate of 59.94 frames per second, saving 
every other frame to RAM to produce 29.97fps video. We ran the matte camera at its full rate of 110 frames 
per second, and took weighted averages of the matte im­ages to produce 29.97fps mattes temporally aligned 
to the video. (The weight of each matte image was proportional to its tempo­ral overlap with the corresponding 
color image, and the sequences were synchronized with sub-frame accuracy by observing the posi­tion of 
a stick waved in front of the cameras at the beginning of the sequence.) Though this only approximated 
having simultaneously photographed mattes, the approach worked well even for relatively fast subject 
motion. Future work would be to drive the matte camera synchronized to the color camera, or ideally to 
use a single camera capable of detecting both RGB color and infrared light. 4.3 Image Brightness Calibration 
The iColor MR lights are somewhat directional and do not light the subject entirely evenly. We perform 
a .rst-order correction to this brightness falloff by placing a white forward-facing card in the center 
of the light stage covering the color camera s .eld of view. We then raise the illumination on all of 
the lights until just before the pixel values begin to saturate, and take an image of the card (Fig. 
11(c)). The color of the card image is scaled so that its brightest value is (1,1,1); images from the 
color camera are then corrected for brightness falloff by dividing these images by the normalized image 
of the card. While this calibration process signi.cantly improves the brightness consistency of the color 
images, it is not perfect since in actuality different lights will decrease in brightness differently 
depending on their orientation toward the card. 4.4 Matte Extraction and Compositing We use the following 
technique to derive a clean matte image for creating the composite. The matte background is not generally 
il­luminated evenly (Fig. 6(a)), so we also acquire a clean plate of the matte (Fig. 6(b)) without the 
actor for each shot. We then divide the pixel values of each matte image by the pixel values of the clean 
plate image (Fig. 6(c)). After this division, the silhouet­ted actor remains dark and the background, 
which does not change between the two images, becomes white, producing a clean matte. Pixels only partially 
covered by the foreground also divide out to represent proper ratios of foreground to background. We 
allow the user to specify a garbage matte [4] of pixels that should always be set to zero or to one in 
the event that stray infrared light falls on the actor or that the cloth backing does not fully cover 
the .eld of view. Using the clean matte, the composite image (Fig. 6(f)) is formed by multiplying the 
matte by the background plate (Fig. 6(e)) and then adding in the color image of the actor; the color 
image of the actor forms a pre-multiplied alpha image as the actor is pho­tographed on a black background 
(Fig. 6(d)). Since some of the light stage s lights may be in the .eld of view of the color camera, we 
do not add in pixels of the actor s image where the matte indi­cates there is no foreground element. 
Additionally, we disable the few lights which are nearly obscured by the actor to avoid having the actor 
s silhouette edge contain saturated pixels from crossing the lights. Improving the matting algorithm 
to correct for such a problem is an avenue for future work.  5 Results We performed two kinds of experiments 
with our light stage. For both, we worked to produce the effect of both the camera and the subject moving 
within the environment in order to best show the interaction of the subject with the illumination. 5.1 
Rotating Camera Composites For our .rst set of shots, we visualized our actors in differing om­nidirectional 
illumination environments available from the Light Probe Image Gallery at http://www.debevec.org/Probes/. 
For these shots we wished to look around in the environment as well as see the actor, so we planned the 
shots to show the camera rotating around the actor looking slightly upward. Since in our system the camera 
stays .xed, we built a manually operated rotation stage for the ac­tor to stand on, and out.tted it with 
an encoder to communicate its rotation angles to the lighting control program. Using these angles, the 
computer rotates the lighting environment to match the rotation of the actor, and records the encoder 
data so that a sequence of syn­chronized perspective background plates of the environment can be generated. 
Since the actor and background rotate together, the ren­dered effect is of the camera moving around the 
standing actor. Figure 12 shows frames from two ten-second rotation sequences of subjects illuminated 
by and composited into two different envi­ronments. As hoped, the lighting appears to be consistent between 
the actors and their backgrounds. In Fig. 12(b-d), the actor is il­luminated by light captured in UC 
Berkeley s Eucalyptus Grove in the late afternoon. The moving hair in (d) tests the temporal accu­racy 
of the matte. In Fig. 12(f-h), the actor is illuminated by an overcast strip of sky and indirect light 
from the collonade of the Uf­.zi gallery in Florence. In Fig. 12(j-l), the actor is illuminated by light 
captured in San Francisco s Grace Cathedral. In (j) and (k) the yellow altar, nearly behind the actor, 
provides yellow rim lighting on the actor s left and right sides. In (j-l), the bluish light from the 
overhead stained glass windows re.ects in the actor s forehead and hair. In all three cases, the mattes 
follow the actors well, and the illumination appears consistent between the background and fore­ground 
elements. There are also some artifacts in these examples. The (b-d) and (j-l) backgrounds are perspective 
reprojections of lighting environ­ments originally imaged as re.ections in a mirrored ball [17, 6], and 
as a result are lower in resolution than the image of the ac­tor. Also, since the environments are modeled 
as distant spheres of illumination, there is no three-dimensional parallax visible in the backgrounds 
as the camera rotates. Furthermore, the metallic gold stripes of the actor s shirt in (b-d) and (j-l) 
re.ect enough light from the infrared backing at grazing angles to appear as background pix­els in some 
frames of the composite. This shows that while infrared spill does not alter the actor s visible appearance, 
it can still cause errors in the matte.  5.2 Moving Camera Composite For our second type of shot, we 
composited the actor into a 3D model of the colonnade of the Parthenon with a moving camera. The model 
was rendered with late afternoon illumination from a photographically recorded sky [6] and a distant 
circular light source for the sun (Figure 13(a-c)). The columns on the left produced a space of alternating 
light and shadow, and the wall to the right holds red and blue banners providing colored indirect illumination 
in the environment. The background plates of the environment were rendered using a Monte Carlo global 
illumination program (the Arnold renderer by Marcos Fajardo) in order to provide physi­cally realistic 
illumination in the environment. After the background plates, a second sequence was rendered to record 
the omnidirectional incident illumination conditions upon the actor moving through the scene. We accomplished 
this by plac­ing an orthographic camera focused onto a virtual mirrored ball moving along the desired 
path of the actor within the scene (Fig­ure 13(d-f)). These renderings were saved as high dynamic range 
images to preserve the full range of illumination intensities inci­dent upon the ball. We then converted 
this 300-frame sequence of mirrored ball images into the longitude-latitude omnidirectional  (a) (b) 
(c) (d) (e) (f) (g) (h) (i) (j) (k) (l)  Figure 12: Rotation Sequence Results (a) A captured lighting 
environment acquired in the UC Berkeley Eucalyptus Grove in the late afternoon. (b-d) Three frames from 
a sequence of an actor illuminated by the environment in (a) and composited over a perspective projection 
of the environment. (e) A captured lighting environment taken between two stone buildings on a cloudy 
day. (f-h) Three composited frames from a sequence of an actor illuminated by (e). (i) A captured lighting 
environment inside a cathedral. (j-l) Three frames from a sequence of an actor illuminated by the environment 
in (i) and composited over a perspective projection of the environment. Rim lighting from the yellow 
altar and re.ections from the bluish stained glass windows can be seen in the actor s face and hair. 
images used by our lighting control program. The virtual camera in the sequence moved backwards at 4 
feet per second, and our actor used a treadmill to become familiar with the motions they would make at 
this pace. We then recorded our actor walking in place in our light stage at the same time that we played 
back the incident illumination sequence of the correspond­ing light in the environment, taking care to 
match the real camera system s azimuth, inclination, and focal length to those of the vir­tual camera. 
Figure 13(g-i) shows frames from the recorded sequence of the actor, Figure 13(j-l) shows the derived 
matte images, and Figure 13(m-o) shows the actor composited over the moving background. As hoped, the 
composite shows that the lighting on the actor dy­namically matches the illumination in the scene; the 
actor steps into light and shadow as she walks by the columns, and the side of her face facing the wall 
subtly picks up the indirect illumination from the red and blue banners as she passes by. The most signi.cant 
challenge in this example was to reproduce the particularly high dynamic range of the illumination in 
the en­vironment. When the sun was visible, the three lights closest to the sun s direction were required 
to produce ten times as much il­lumination as the other 153 lights combined. This required scaling down 
the total intensity of the reproduced environment so that the brightest lights would not be driven to 
saturation, making the other lights quite dim. While this remained a faithful reproduction of the illumination, 
it strained the sensitivity of the color video camera, re­quiring 12 decibels of gain and a fully open 
aperture to register the image. As a result, the image is mildly noisy and slightly out of fo­cus. Initial 
experiments using a single repositionable high-intensity light to act as the sun show promise for solving 
this problem.  6 Discussion and Future Work The capabilities and limitations of our lighting apparatus 
suggest a number of avenues for future versions of the system. To reduce the need to compensate for the 
brightness falloff, using lights with a wider spread or building a larger light stage to place the lights 
further from the actor would be desirable. To increase the light­ing resolution and total light output, 
the system could bene.t from both more lights and brighter lights. We note that in the limit, the light 
stage becomes a omnidirectional display device, presenting a panoramic image of the environment to the 
actor. In the current stage, the actor can not be illuminated in dappled light or partial shadow; the 
.eld of view of each light source covers the entire actor. In our colonnade example, we should actually 
have seen the actor s nose become shadowed before the rest of her face, but instead the shadowing happens 
all at once. A future version of the light stage could replace each light source with a video projec­tor 
to reproduce spatially varying incident .elds of illumination. In this limit, this light stage would 
become an immersive light .eld [14] display device, immersing the actor in a holographic reproduc­tion 
of the environment with full horizontal and vertical parallax. Recording the actor with a dense array 
of cameras would also allow arbitrary virtual viewpoints of the actor to be generated using the  (a) 
(b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n) (o)   Figure 13: Moving Sequence Results (a-c) 
Three frames of an animated background plate, rendered using the Arnold global illumination system. The 
camera moves backwards down the collonade of the Parthenon illuminated by late afternoon sun. (d-f) The 
corresponding light probe images formed by rendering an orthographic view of a re.ective sphere placed 
where the actor will be in the scene. The sphere goes into and out of shadow and re.ects the red and 
blue banners on the right wall of the gallery. (g-i) The corresponding frames of the actor from the live-action 
shoot, illuminated by the lighting in (d-f), before brightness falloff correction. (j-l) The processed 
mattes obtained from the infrared compositing system, with the background in black (m-o) The .nal composite 
of the actor into the background plates after brightness falloff correction. The actor begins near the 
red banner, moves into the shadow of a column, and emerges into sunlight near the blue banner. (The blue 
banner appears directly to the right in the lighting environment (f) but has not yet entered the .eld 
of view of the background plate in (c) and (o).) Comparing (m) to (o), it can be seen that the shadowed 
half of the actor s face receives subtle indirect illumination from the red and blue banners. light .eld 
rendering technique. References To create a general-purpose production tool, it would be desir­able to 
build a large-scale version of the device capable of illumi­nating whole bodies and multiple actors. 
Once we can see an entire person, however, it will become necessary to reproduce the shadows that the 
actor should cast into the environment. Having an approxi­mate 3D model of the actor s performance, perhaps 
obtained using a silhouette-based volumetric reconstruction algorithm (e.g. [16]) or one or more range 
imaging cameras (such as the 3DV Systems Z-Cam), could be useful for this purpose. The matting process 
could be improved by using a single camera to detect both the RGB and IR light; for this purpose a single-chip 
camera with an R,G,B,and IR .lter mosaic over each group of four pixels or a 4-chip camera using a prisms 
to send light to R, G, B, and IR image sensors could be constructed. In this work we calibrated the light 
stage making the common trichromatic approximation [3] to the interaction of incident illu­mination with 
re.ective surfaces. For illumination and surfaces with complex spectra, such as .uorescent lights and 
certain vari­eties of fabric, the material s re.ection of reproduced illumination in the light stage 
could be noticeably different that its actual appear­ance under the original lighting. This problem could 
be addressed through multispectral imaging of the incident illumination [11], and by illuminating the 
actor with additional colors of LEDs. Adding yellow and turquoise LEDs as a beginning would serve to 
round out our illumination s color gamut. Finally, we note that cinematographers often in fact usually 
 augment and modify environmental illumination with bounce cards, .ll lights, absorbers, re.ectors, and 
many other techniques to artis­tically manipulate the illumination on an actor; skilled use of these 
techniques can greatly increase the aesthetic and emotional impact of a shot without degrading its realism. 
Our lighting approach could facilitate such manipulations to the lighting; an avenue for future work 
is to collaborate with lighting artists to develop useful tools for performing this sort of virtual cinematography. 
 7 Conclusion In this paper we have presented a system for realistically composit­ing an actor s live 
performance into a background environment by illuminating the actor with a reproduction of the illumination 
of the light in the environment. Though not fully general, our device has produced results which we believe 
demonstrate potential use for the technique. We look forward to the continued development of the light 
stage technique and working with .lmmakers to explore how it can evolve into a useful production tool. 
Acknowledgements We thank Brian Emerson and Mark Brownlow for their 3D envi­ronment renderings, Maya 
Martinez for making our infrared cloth backing, Anshul Panday for rendering the light probe backgrounds, 
Gordie Haakstad, Kelly Richard, and Woody Omens, ASC for cine­matography consultations, Randal Kleiser 
and Alexander Singer for directorial guidance, Yikuong Chen, Rippling Tsou, Diane Suzuki, and Shivani 
Khanna for their 3D modeling, Diane Suzuki and Mark Brownlow for their illustrations, Elana Livneh and 
Emily DeGroot for their acting, Marcos Fajardo for the Arnold renderer used for our 3D models, Van Phan 
for editing our video, the anonymous review­ers for their helpful comments, and Dick Lindheim, Bill Swartout, 
Kevin Dowling, Andy van Dam, Andy Lesniak, Ken Wiatrak, Bob­bie Halliday, Linda Miller, Ayten Durukan, 
and Color Kinetics, Inc. for their additional help, support, and suggestions for this project. This work 
has been sponsored by the University of South­ern California, U.S. Army contract number DAAD19-99-D-0046, 
and TOPPAN Printing Co., Ltd. The content of this information does not necessarily re.ect the position 
or policy of the sponsors and no of.cial endorsement should be inferred. [1] ALEXANDER, D. K., JONES, 
P. J., AND JENKINS, H. The arti.cial sky and heliodon facility at Cardiff University. In Proc. International 
Building Performance Simulation Association (IBPSA) (September 1999). (Abstract). [2] BEN-EZRA, M. Segmentation 
with invisible keying signal. In Proc. IEEE Conf. on Comp. Vision and Patt. Recog. (June 2000), pp. 568 
575. [3] BORGES, C. F. Trichromatic approximation for computer graphics illumination models. In Computer 
Graphics (Proceedings of SIG-GRAPH 91) (July 1991), vol. 25, pp. 101 104. [4] BRINKMANN,R. The Art and 
Science of Digital Compositing. Mor­gan Kaufmann, 1999. [5] CHUANG, Y.-Y., ZONGKER, D. E., HINDORFF, 
J., CURLESS, B., SALESIN, D. H., AND SZELISKI, R. Environment matting exten­sions: Towards higher accuracy 
and real-time capture. In Proceedings of SIGGRAPH 2000 (July 2000), pp. 121 130. [6] DEBEVEC, P. Rendering 
synthetic objects into real scenes: Bridg­ing traditional and image-based graphics with global illumination 
and high dynamic range photography. In SIGGRAPH 98 (July 1998). [7] DEBEVEC,P., HAWKINS,T., TCHOU, C., 
DUIKER, H.-P., SAROKIN,W., AND SAGAR, M. Acquiring the re.ectance .eld of a human face. Proceedings of 
SIGGRAPH 2000 (July 2000), 145 156. [8] DEBEVEC, P. E., AND MALIK, J. Recovering high dynamic range radiance 
maps from photographs. In SIGGRAPH 97 (August 1997), pp. 369 378. [9] FAUGERAS,O. Three-Dimensional Computer 
Vision. MIT Press, 1993. [10] FIELDING,R. The Technique of Special Effects Cinematography, 4th ed. Hastings 
House, New York, 1985, ch. 11, pp. 290 321. [11] GAT, N. Real-time multi-and hyper-spectral imaging for 
remote sens­ing and machine vision: an overview. In Proc. 1998 ASAE Annual International Mtg. (Orlando, 
Florida, July 1998). [12] KAJIYA, J. T. The rendering equation. In Computer Graphics (Pro­ceedings of 
SIGGRAPH 86) (Dallas, Texas, August 1986), vol. 20, pp. 143 150. [13] KOUDELKA, M., MAGDA, S., BELHUMEUR,P., 
AND KRIEGMAN, D. Image-based modeling and rendering of surfaces with arbitrary brdfs. In Proc. IEEE Conf. 
on Comp. Vision and Patt. Recog. (2001), pp. 568 575. [14] LEVOY, M., AND HANRAHAN, P. Light .eld rendering. 
In SIG-GRAPH 96 (1996), pp. 31 42. [15] MALZBENDER,T., GELB, D., AND WOLTERS, H. Polynomial tex­ture 
maps. Proceedings of SIGGRAPH 2001 (August 2001), 519 528. ISBN 1-58113-292-1. [16] MATUSIK,W., BUEHLER, 
C., RASKAR, R., GORTLER, S. J., AND MCMILLAN, L. Image-based visual hulls. In Proc. SIGGRAPH 2000 (July 
2000), pp. 369 374. [17] MILLER, G. S., AND HOFFMAN, C. R. Illumination and re.ection maps: Simulated 
objects in simulated and real environments. In SIG-GRAPH 84 Course Notes for Advanced Computer Graphics 
Anima­tion (July 1984). [18] NIMEROFF, J. S., SIMONCELLI, E., AND DORSEY, J. Ef.cient re­rendering of 
naturally illuminated environments. Fifth Eurographics Workshop on Rendering (June 1994), 359 373. [19] 
PORTER,T., AND DUFF, T. Compositing digital images. In Computer Graphics (Proceedings of SIGGRAPH 84) 
(Minneapolis, Minnesota, July 1984), vol. 18, pp. 253 259. [20] RAMAMOORTHI, R., AND HANRAHAN, P. A signal-processing 
framework for inverse rendering. In Proc. SIGGRAPH 2001 (August 2001), pp. 117 128. [21] SMITH, A. R., 
AND BLINN, J. F. Blue screen matting. In Proceed­ings of SIGGRAPH 96 (August 1996), pp. 259 268. [22] 
TCHOU, C., AND DEBEVEC, P. HDR Shop. Available at http://www.debevec.org/HDRShop, 2001. [23] TSAI, R. 
A versatile camera calibration technique for high accu­racy 3d machine vision metrology using off-the-shelf 
tv cameras and lenses. IEEE Journal of Robotics and Automation 3, 4 (August 1987), 323 344. [24] VIDOR, 
Z. An infrared self-matting process. Society of Motion Pic­ture and Television Engineers 69 (June 1960), 
425 427. [25] WINDOWS AND DAYLIGHTING GROUP. The sky simulator for day­lighting studies. Tech. Rep. DA 
188 LBL, Lawrence Berkeley Labo­ratories, January 1985. [26] WYSZECKI, G., AND STILES,W. S. Color Science: 
Concepts and Methods, Quantitative and Formulae, 2nd ed. Wiley, New York, 1982. [27] ZONGKER, D. E., 
WERNER, D. M., CURLESS, B., AND SALESIN, D. H. Environment matting and compositing. Proceedings of SIG-GRAPH 
99 (August 1999), 205 214.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566615</section_id>
		<sort_key>557</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Shadows, translucency, and visibility]]></section_title>
		<section_page_from>557</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39026797</person_id>
				<author_profile_id><![CDATA[81100091740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gritz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Exluna, Inc.]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566616</article_id>
		<sort_key>557</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Perspective shadow maps]]></title>
		<page_from>557</page_from>
		<page_to>562</page_to>
		<doi_number>10.1145/566570.566616</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566616</url>
		<abstract>
			<par><![CDATA[Shadow maps are probably the most widely used means for the generation of shadows, despite their well known aliasing problems. In this paper we introduce <i>perspective shadow maps,</i> which are generated in normalized device coordinate space, i.e., after perspective transformation. This results in important reduction of shadow map aliasing with almost no overhead. We correctly treat light source transformations and show how to include all objects which cast shadows in the transformed space. Perspective shadow maps can directly replace standard shadow maps for interactive hardware accelerated rendering as well as in high-quality, offline renderers.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[frame buffer algorithms]]></kw>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[level of detail algorithms]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[shadow algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14106427</person_id>
				<author_profile_id><![CDATA[81100285418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stamminger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES - INRIA Sophia-Antipolis, France and Bauhaus-Universit&#228;t, Weimar, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026929</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES - INRIA Sophia-Antipolis, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. C. Crow. Shadow algorithms for computer graphics. Computer Graphics (Proc. of SIGGRAPH 77), 11(2):242-248, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383302</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Fernando, S. Fernandez, K. Bala, and D. P. Greenberg. Adaptive shadow maps. Proc. of SIGGRAPH 2001, pages 387-390, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes. Computer graphics, principles and practice, second edition. 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. Heckbert. Survey of Texture Mapping. IEEE Computer Graphics and Applications, 6(11):56-67, November 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[T. Lokovic and E. Veach. Deep shadow maps. Proc. of SIGGRAPH 2000, pages 385-392, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. S. Montrym, D. R. Baum, D. L. Dignam, and C. J. Migdal. Infinite-reality: A real-time graphics system. Proc. of SIGGRAPH 97, pages 293-302, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[nvidia. webpage. http://developer.nvidia.com/view.asp?IO=cedec_shadowmap.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344936</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[H. Pfister, M. Zwicker, J. van Baar, and M. Gross. Surfels: Surface elements as rendering primitives. Proceedings of SIGGRAPH 2000, pages 335-342, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves, D. H. Salesin, and R. L. Cook. Rendering antialiased shadows with depth maps. Computer Graphics (Proc. of SIGGRAPH 87), 21(4):283-291, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[S. Rusinkiewicz and M. Levoy. Qsplat: A multiresolution point rendering system for large meshes. Proc. of SIGGRAPH 2000, pages 343-352, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732280</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Stamminger and G. Drettakis. Interactive sampling and rendering for complex and procedural geometry. In S. Gortler and K. Myszkowski, editors, Rendering Techniques 2001 (12th Eurographics Workshop on Rendering), pages 151-162. Springer Verlag, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[K. Tadamura, X. Qin, G. Jiao, and E. Nakamae. Rendering optimal solar shadows with plural sunlight depth buffers. The Visual Computer, 17(2):76-90, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[S. Upstill. The RenderMan Companion. Addison-Wesley, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383299</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Wand, M. Fischer, I. Peter, F. Meyer auf der Heide, and W. Stra&#223;er. The randomized z-buffer algorithm: Interactive rendering of highly complex scenes. Proc. of SIGGRAPH 2001, pages 361-370, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[K. Weiler and K. Atherton. Hidden surface removal using polygon area sorting. Computer Graphics (Proc. of SIGGRAPH 77), 11(2):214-222, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Casting curved shadows on curved surfaces. Computer Graphics (Proc. of SIGGRAPH 78), 12(3):270-274, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617600</ref_obj_id>
				<ref_obj_pid>616014</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Woo, P. Poulin, and A. Fournier. A survey of shadow algorithms. IEEE Computer Graphics and Applications, 10(6):13-32, November 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perspective Shadow Maps Marc Stamminger and George Drettakis . REVES -INRIA Sophia-Antipolis, France 
   ... ... Figure 1: (Left) Uniform 512x512 shadow map and resulting image. (Right) The same with 
a perspective shadow map of the same size. Abstract Shadow maps are probably the most widely used means 
for the gen­eration of shadows, despite their well known aliasing problems. In this paper we introduce 
perspective shadow maps, which are gen­erated in normalized device coordinate space, i.e., after perspective 
transformation. This results in important reduction of shadow map aliasing with almost no overhead. We 
correctly treat light source transformations and show how to include all objects which cast shadows in 
the transformed space. Perspective shadow maps can directly replace standard shadow maps for interactive 
hardware ac­celerated rendering as well as in high-quality, of.ine renderers. CR Categories: I.3.3 [Computer 
Graphics]: Picture/Image Generation Bitmap and framebuffer operations; I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Color, shading, shadowing, and texture Keywords: Frame Buffer Algorithms, Graphics 
Hardware, Illumi­nation, Level of Detail Algorithms, Rendering, Shadow Algorithms 1 Introduction Shadows 
are an important element for any computer graphics im­age. They provide signi.cant visual cues, aiding 
the user to un­derstand spatial relationships in a scene and add realism to syn­thetic images. The challenge 
of generating shadows goes back to the early days of computer graphics, where various geometric al­gorithms 
such as those based on clipping [15], or shadow volumes ... Marc.Stamminger|George.Drettakis.@sophia.inria.fr 
The .rst author is now at the Bauhaus-Universit¨at, Weimar, Germany Copyright &#38;#169; 2002 by the 
Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of 
this work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 [1] were introduced. These 
methods often suffer from robustness problems due to the geometric computations required, and may also 
require non-trivial data structures or involve a signi.cant rendering overhead. In addition, such algorithms 
are usually a preprocess, and are thus best suited to static scenes. The introduction of shadow maps 
[16] marked an important step in the evolution of shadow algorithms. With shadow maps, we sim­ply render 
the scene from the viewpoint of the light source, creating a depth image. When rendering each pixel in 
the .nal image, the visible point is transformed into the light coordinate system, and a depth comparison 
is performed with the corresponding shadow map pixel to decide whether the point is hidden with respect 
to the light source. A single shadow map is suf.cient for directional lights and spot lights; omnidirectional 
point lights require several shadow maps to cover all emission directions. When considering point lights 
in this paper, we implicitly mean spot lights, but the ideas developed here transfer to omnidirectional 
point lights easily. Compared to shadow algorithms which require geometric calcu­lations prone to robustness 
problems, shadow maps are much more general. Due to their simplicity, they have been incorporated into 
graphics hardware, .rst on the In.nite Reality [6] and more recently on consumer-level graphics cards 
such as the NVIDIA GeForce3. The possibility to have shadows for large scenes makes shadow maps particularly 
interesting for rendering-intensive applications like video games. Shadow maps are also widely used in 
of.ine, high-quality rendering systems because of their simplicity and .ex­ibility. In the RenderMan 
standard, for example, shadow maps are the most widely used method to generate images with shadows [13]. 
However, as with any method based on a discrete buffer, shadow maps suffer from aliasing problems, if 
the shadow map resolution used is insuf.cient for the image being rendered. This is particu­larly true 
for scenes with a wide depth range, where nearby shadows need high resolution, whereas for distant shadows 
low resolution would be suf.cient. Recent shadow map approaches [12, 2] adapt shadow map resolution by 
using multiple shadow maps of vary­ing resolution. However, by using multiple shadow maps, several rendering 
passes are required, and due to the more involved data structures, the methods no longer map to hardware. 
In this paper, we present a novel approach to adapt shadow map resolution to the current view. By using 
a non-uniform parameter­ization, a single perspective shadow map is generated, providing high resolution 
for near objects, and decreased resolution as we move away from the viewpoint. An example is shown in 
Fig. 1. A standard shadow map (left) has insuf.cient resolution for nearby objects (center left). In 
contrast, in our perspective shadow map (center right), resolution varies such that nearby objects have 
suf.­cient shadow map resolution (right). Our shadow map projection can still be represented by a 4 .4 
matrix, so that it fully maps to hardware. It can easily be used to improve any interactive and of.ine 
rendering method based on shadow maps, with only marginal overhead for the computation of the optimized 
projection matrix. Perspective shadow maps often re­duce aliasing dramatically. In a few dif.cult cases, 
our parameteri­zation converges to standard uniform shadow maps. Our approach is view-dependent, requiring 
the generation of a new shadow map at each frame or at least after major camera movements. This is necessary 
for scenes containing moving objects anyhow, such as those used in video games or other interactive applications. 
In this case, we have no signi.cant speed penalty compared to a standard, .xed-resolution shadow map. 
1.1 Shadow Map Aliasing First, we brie.y formalize the aliasing problems of shadow maps for point light 
sources. Every pixel in the shadow map represents a sheared pyramid of rays, passing through a shadow 
map pixel of size ds xds on the (shadow) image plane (Fig. 2). In the follow­ing, we will call the reciprocal 
of ds the shadow map resolution. To increase shadow map resolution we decrease ds and vice versa. When 
the ray bundle through a shadow map pixel hits a surface at distance rs under angle a, the size of the 
intersection is approx­ . imately dsrs cos a. In the .nal image, this intersection with the surface has 
size: rs cosß d .ds ri cosa . where ß is the angle between the light source direction and the sur­face 
normal and ri is the distance from the camera to the intersec­  Figure 2: Shadow map projection quantities. 
For directional light sources, d is independent of rs, so d only . depends on ds ri. Analogously, for 
an orthogonal view d is propor­tional to dsrs. Shadow map undersampling appears when d is larger than 
the . image pixel size di. This can happen when dsrs ri becomes large, which typically happens when the 
user zooms into a shadow bound­ary so that single shadow map pixels become visible. We will call this 
perspective aliasing. Due to limited memory, the shadow map . resolution 1 ds can only be increased up 
to a certain limit in prac­tice. Perspective aliasing can be avoided by keeping the fraction . rs ri 
close to a constant. As we show in this paper, this is what is achieved when we compute the shadow map 
after perspective trans­formation. . When cos ßcos a becomes large, projection aliasing appears. This 
typically happens when the light rays are almost parallel to a surface, so that the shadow stretches 
along the surface. Projection aliasing is not treated by perspective shadow maps, since this would require 
a local increase in shadow map resolution. This is not possi­ble without much more involved data structures, 
and it would force us to abandon hardware acceleration. Note that there are also other sources of aliasing 
when using shadow maps, e.g., depth quantization, which we will not treat here. 1.2 Related Work The 
literature on shadow generation in computer graphics is vast, and far beyond the scope of this paper 
(see [17] for an old, but nice survey). We thus concentrate our review on previous work directly related 
to shadows maps and our novel approach. Reeves et al. [9] introduced the concept of percentage closer 
.l­tering. The idea is to .lter the shadows obtained from shadow maps by interpolating the binary result 
of the depth comparison instead of the depth values. The deep shadow maps of Lokovic et al. [5] extend 
the idea of .ltering shadow maps by storing approximate attenuation functions for each shadow map pixel, 
thus capturing self shadowing within hair, fur or smoke. Both approaches perform anti-aliasing in that 
they .lter undersampling artifacts, but they do not solve the problem of undersampling as such. Tadamura 
et al. [12] introduce multiple shadow maps with vary­ing resolution as a solution to the aliasing problem 
for large outdoor scenes. This solves the problem of perspective aliasing, but it does not map to hardware 
and is thus not appropriate for interactive ap­plications. An interesting improvement to traditional 
shadow maps is the adaptive shadow map method [2]. This approach replaces the .at shadow map with an 
adaptive, hierarchical representation, which is updated continuously during a walkthrough. For each frame, 
a .rst rendering pass is required to determine which parts of the hierarchi­cal shadow map need re.nement. 
This is achieved using OpenGL texture mip-mapping capabilities. Perspective and projection alias­ing 
are detected. The most critical parts of the shadow map are then rendered at higher resolution, read 
back, and inserted into the hier­archical shadow map structure. Oversampled parts of the hierarchy are 
truncated. The method is a software solution, since it requires the traversal and re.nement of a hierachical 
data structure instead of a shadow map, and thus cannot entirely map to hardware. Rapid view changes 
make it necessary to update a large number of nodes in the hierarchy, so either the frame rate drops 
or aliasing appears until the hierarchical structure has been updated. The generation of a new node essentially 
requires an entire rendering pass, albeit with a small part of the model, but with the full cost of a 
frame-buffer readback to update the data structure. Scenes with moving objects would require the generation 
of the entire hierarchical structure at each frame, which would require many rendering passes. Shadow 
maps are closely related to textures, so we assume the reader is familiar with the texturing and the 
projective mapping pro­cess. Most standard graphics textbooks treat these issues [3]. A particularly 
nice and thorough description can be found in [4]. 2 Overview Perspective shadow maps are computed in 
post-perspective space of the current camera, or, using standard graphics terminology, the space of normalized 
device coordinates [3]. In the traditional ren­dering pipeline, perspective is obtained by transforming 
the world to a perspectively distorted space, where objects close to the camera are enlarged and distant 
objects are shrunk (see Fig. 3). This map­ping is projective [4] and can thus be represented by a 4 .4 
ma­trix with homogeneous coordinates. It projects the current viewing frustum to the unit cube; the .nal 
image is generated by a parallel projection of this cube along z. The idea of perspective shadow maps 
is to .rst map the scene to post-perspective space and then generate a standard shadow map in this space 
by rendering a view from the transformed light source to the unit cube. We can work in post-perspective 
space al­most like in world space, with the exception of objects behind the viewer, which will be handled 
in Sect. 4. Because the shadow map sees the scene after perspective projection, perspective aliasing 
(see Sect. 1.1) is signi.cantly decreased, if not completely avoided. To explain this principle a special 
case is depicted in Fig. 3. The scene is illuminated by a vertical directional light source. In post­perspective 
space the shadow map is an orthogonal view from the top onto the unit cube and the .nal image is an orthogonal 
view from the front. Consequently, all shadow map pixels projected onto the ground in this scene have 
the same size in the image. Thus, in the sense of Sect. 1.1, perspective aliasing is completely avoided 
for this case.  3 Post-Perspective Light Sources To apply our new method, we .rst transform the scene 
to post­perspective space, using the camera matrix. We then transform the light source by the same matrix, 
and generate the shadow map. The different cases which arise are described next. 3.1 Directional Light 
Sources Directional light sources can be considered as point lights at in.n­ity. The perspective mapping 
can move these sources to a .nite position; the set of possible cases is shown in Fig. 3 and 4. A di­rectional 
light source parallel to the image plane remains at in.nity. All other directional light sources become 
point lights on the in.n­ity plane at z .f .n...f .n. .., where n and f are the near and far distance 
of the viewing frustum (see Fig. 3). A directional light source shining from the front becomes a point 
light in post-perspective space (Fig. 4, left), whereas a directional light source from behind is mapped 
to a inverted point light source (center). In this context, inverted means that all light rays for these 
light sources do not emanate, but converge to a single point. Such inverted point light sources can easily 
be handled as point lights with their shadow map depth test reversed, such that hit point furthest from 
the point source survive. The extreme cases are directional lights parallel (facing or opposite to) the 
view direction, which are mapped to a point light just in front of the viewer (right).  3.2 Point Light 
Sources The different settings for point light sources are shown in Fig. 5. Point light sources in front 
of the viewer remain point lights, (left).  Figure 5: Mapping of point lights in world space (top row) 
to post­perspective space (bottom row). A point light in front of the user remains a normal point light 
(left), a point light behind the viewer becomes inverted (right). Boundary cases are mapped to directional 
lights (center). Point lights behind the viewer are mapped beyond the in.nity plane and become inverted 
(right). Point lights on the plane through the view point which is perpendicular to the view direction 
(camera plane), however, become directional (center).  3.3 Discussion In post-perspective space, the 
.nal image is an orthogonal view onto the unit cube. Following our observations in Sect. 1.1, this means 
that perspective aliasing due to distance to the eye, ri, is avoided. However, if the light source is 
mapped to a point light in post-perspective space, aliasing due to the distance to the shadow map image 
plane, rs, can appear. The ideal case occurs when, after the perspective mapping, the light source is 
directional. Perspective aliasing due to rs is thus also avoided. This happens for: . a directional light 
parallel to the image plane (see Fig. 3), . a point light in the camera plane. The typical example is 
the miner s head-lamp, that is a point light just above the camera. It is known that this setting is 
also ideal for standard shadow maps [7]. With our parameterization, the offset can be arbi­trarily large, 
as long as the point remains in the camera plane. On the other hand, the less optimal cases appear when 
we obtain a point light with a large depth range in post-perspective space. The extreme example is a 
directional light parallel to the viewing direc­tion. In post-perspective space, this becomes a point 
light on the in­.nity plane on the opposite side of the viewer (see Fig. 4 right). In this worst case 
the two perspective projections with opposite view­ing directions cancel out mutually and we obtain a 
standard uniform shadow map. In general, for directional light sources the bene.t is maximal for a light 
direction perpendicular to the view direction. Since these are also parallel lights in post-perspective 
space, perspective aliasing is completely avoided in this case. Consider the smaller of the two angles 
formed between the light direction and the viewing direc­tion. The bene.t of our approach decreases as 
this angle becomes smaller, since we move further away from the ideal, perpendicu­lar case. If the angle 
is zero our parameterization corresponds to a uniform shadow map. For point lights, the analysis is harder, 
since the point light source also applies a perspective projection. Informally, the ad­vantage of our 
parameterization is largest when the point light is far away from the viewing frustum, so that it is 
similar to a paral­lel light, and if the visible illuminated region exhibits a large depth range. For 
the case of a miner s lamp, which is known to be ideal for uniform shadow maps [7], our parameterization 
again converges to the uniform setting. A common problem in shadow maps is the bias necessary to avoid 
self-occlusion or surface acne [7]. This problem is in­creased for perspective shadow maps, because objects 
are scaled non-uniformly. We use a constant offset in depth in shadow map space, which may require user 
adjustment depending on the scene.  4 Including all Objects Casting Shadows Up to now, we ignored an 
important issue: our shadow map is op­timized for the current viewing frustum. But the shadow map must 
contain all objects within that frustum plus all potential occluders outside the frustum that can cast 
a shadow onto any visible object. 4.1 World Space More formally, we de.ne the set of points that must 
appear in the shadow map as follows: Let S be an envelope of the scene objects, typically its bounding 
box. V is the viewing frustum and L the light source frustum. The light source is at position l (for 
directional lights L .R3 and l is at in.nity). First, we compute the convex hull M of V and l, so M contains 
all rays from points in V to l. From M we then remove all points outside the scene s bounding box and 
the light frustum: H .M .S .L (shown in yellow in Fig. 6 (right)). Figure 6: The current view frustum 
V , the light frustum L with light position l, and the scene bounding box S (left). M is obtained by 
extending V towards the light (center). Then M is intersected with S and L to obtain the set of points 
that need to be visible in the shadow map (right). In our implementation we use the computational geometry 
li­brary CGAL (www.cgal.org) to perform these geometric com­putations. As a result, the window of the 
shadow map is chosen so that all points in H are included. However, the shadow map will not see H , but 
its transformation to post-perspective space, so we have to consider how H changes by the perspective 
mapping. 4.2 Post-perspective Space Under projective transformations lines remain lines, but the order 
of points along a line can change. For perspective projections, this happens when a line intersects the 
camera plane, where the inter­section point is mapped to in.nity. As a result, we can quickly transform 
the convex set H to post-perspective space by transfor­mation of its vertices, as long as H is completely 
in front of the viewer. An example where this is not the case is shown in Fig. 7. Points behind the viewer 
had to be included, because they can cast shadows into the viewing frustum. However, in post-perspective 
space, these points are projected to beyond the in.nity plane. In this case, possible occluders can be 
on both sides of the in.nity plane.  Figure 7: The scene is lit by a directional light source coming 
from behind (top left). After perspective projection, an object behind the viewer is inverted and appears 
on the other side of the in.nity plane (lower left). To treat this we move the center of projection back 
(upper right), so that we are behind the furthermost point which can cast a shadow into the view frustum. 
After this, a standard shadow map in post-perspective space is suf.cient again (lower right). One solution 
would be to generate two shadow maps in this case. A .rst one as described previously, and a second one 
that looks beyond the in.nity plane. We avoid this awkward solution by a virtual camera shift. We virtually 
move the camera view point backwards, such that H lies entirely inside the transformed cam­era frustum; 
the near plane distance remains unchanged and the far plane distance is increased to contain the entire 
previous frustum. Note that this camera point displacement is only for the shadow map generation, not 
for rendering the image. By this, we modify the post-perspective space, resulting in de­creased perspective 
foreshortening. If we move the camera to in.n­ity, we obtain an orthogonal view with a post-perspective 
space that is equivalent to the original world space; the resulting perspective shadow map then corresponds 
to a standard uniform shadow map. Consequently, we can say that by moving back the camera, our parameterization 
degrades towards that of a standard shadow map. By this, perspective aliasing is reintroduced, however, 
in practice this is only relevant for extreme cases, so we found it preferable to the double shadow map 
solution. It is interesting to note that for the cases that are ideal for perspective shadow maps, no 
such shift is necessary, because H will always be completely in front of the camera. The shift distance 
will be signi.cant only if the light source is far behind the viewer. But in this case, our parameterization 
con­verges to the uniform shadow map anyway.  5 Point Rendering Point rendering (e.g., [8, 10, 11, 14]) 
has proven to be a very effec­tive means for rendering complex geometry. Points are particularly well 
suited for natural objects. However, for the generation of uni­form shadow maps point rendering loses 
most of its bene.ts. A very large, dense point set needs to be generated in order to render a high-resolution 
uniform shadow map. Frequent regeneration of the shadow map for dynamic scenes becomes highly inef.cient. 
On the other hand, our projective shadow map parameterization .ts nicely with the point rendering approaches 
presented in [14, 11], in the sense that the point sets generated by these methods can also be used for 
the rendering of the shadow map. The reason is that these approaches generate random point clouds, with 
point den­sities adapted to the distance to the viewer. In post-perspective space, these point clouds 
have uniform point density. Since per­spective shadow maps are rendered in this space, the shadow map 
can assume that point densities are uniform. It is thus easy to select the splat size when rendering 
the shadow map such that holes are avoided. 6 Implementation and Results We implemented perspective 
shadow maps within an inter­active rendering application using OpenGL hardware assisted shadow maps. 
The following results, and all sequences on the accompanying video, were obtained under Linux on a Compaq 
AP550 with two 1 GHz Pentium III processors and an NVIDIA GeForce3 graphics accelerator. We used the 
shadow map extensions GL SGIX depth texture and GL SGIX shadow. The shadow maps are rendered into p-buffers 
(GLX SGIX pbuffer) and are applied using register combiners (GL NV register combiners). In order to obtain 
optimal results from perspective shadow maps, we require that the near plane of the view be as far as 
possible. We achieve this by reading back the depth buffer after each frame, searching the minimal depth 
value, and adapting the near plane ac­cordingly. Due to the fast bus and RAM of our computer, the speed 
penalty for reading back the depth buffer is moderate, e.g., 10ms for reading back depth with 16 bits 
precision at 640 by 480 pixel resolution. The increase in quality is well worth this overhead. Fig. 1 
shows a chess board scene, lit by a spot light source. The left image shows a uniform shadow map, the 
center left image is the resulting rendering with well visible shadow map aliasing. The center right 
image was obtained with a perspective shadow map of the same size, which is shown on the right. Fig. 
8 shows Notre Dame in Paris, augmented with a crowd and trees. With point based rendering for the trees 
and the crowd, the views shown are still obtained at about 15 frames/sec. In the street scene in Fig. 
9 the cars and two airplanes are mov­ing. The .gure shows three snapshots from an interactive session; 
all images were obtained at more than ten frames per second with perspective shadow maps of size 1024 
.1024. Our .nal test scene is a complete ecosystem, consisting of hun­dreds of different plants and a 
few cows. The view shown in Fig. 10 contains more than 20 million triangles. We show the image without 
shadows (left), with shadows from a 1024 .1024 uniform shadow map (center) and with shadows from a projective 
shadow map of the same size (right). 7 Conclusions We have introduced perspective shadow maps, a novel 
parameter­ization for shadow maps. Our approach permits the generation of shadow maps with greatly improved 
quality, compared to standard uniform shadow maps. By choosing an appropriate projective map­ping, shadow 
map resolution is concentrated where appropriate. We have also shown how our method can be used for point 
rendering, thus allowing interactive display of very complex scenes with high­quality shadows. Perspective 
shadow maps can be used in interac­tive applications and fully exploit shadow map capabilities of recent 
graphics hardware, but they are also applicable to high-quality soft­ware renderers.      8 Acknowledgments 
The .rst author has been supported by a Marie-Curie postdoctoral Fellowship while doing this work. L. 
Kettner implemented the frus­tum intersection code in CGAL (www.cgal.org). Thanks to F. Durand for his 
very helpful comments. Most of the models in the result section were downloaded from www.3dcafe.comand 
www.help3d.com.  References [1] F. C. Crow. Shadow algorithms for computer graphics. Computer Graphics 
(Proc. of SIGGRAPH 77), 11(2):242 248, 1977. [2] R. Fernando, S. Fernandez, K. Bala, and D. P. Greenberg. 
Adaptive shadow maps. Proc. of SIGGRAPH 2001, pages 387 390, 2001. [3] J. D. Foley, A. van Dam, S. K. 
Feiner, and J. F. Hughes. Computer graphics, principles and practice, second edition. 1990. [4] P. Heckbert. 
Survey of Texture Mapping. IEEE Computer Graphics and Applications, 6(11):56 67, November 1986. [5] T. 
Lokovic and E. Veach. Deep shadow maps. Proc. of SIGGRAPH 2000, pages 385 392, 2000. [6] J. S. Montrym, 
D. R. Baum, D. L. Dignam, and C. J. Migdal. In.nite­reality: A real-time graphics system. Proc. of SIGGRAPH 
97, pages 293 302, 1997. [7] nvidia. webpage. http://developer.nvidia.com/ view.asp?IO=cedec shadowmap. 
[8] H. P.ster, M. Zwicker, J. van Baar, and M. Gross. Surfels: Surface elements as rendering primitives. 
Proceedings of SIGGRAPH 2000, pages 335 342, 2000. [9] W. T. Reeves, D. H. Salesin, and R. L. Cook. Rendering 
antialiased shadows with depth maps. Computer Graphics (Proc. of SIGGRAPH 87), 21(4):283 291, 1987. [10] 
S. Rusinkiewicz and M. Levoy. Qsplat: A multiresolution point ren­dering system for large meshes. Proc. 
of SIGGRAPH 2000, pages 343 352, 2000. [11] M. Stamminger and G. Drettakis. Interactive sampling and 
ren­dering for complex and procedural geometry. In S. Gortler and K. Myszkowski, editors, Rendering Techniques 
2001 (12th Euro­graphics Workshop on Rendering), pages 151 162. Springer Verlag, 2001. [12] K. Tadamura, 
X. Qin, G. Jiao, and E. Nakamae. Rendering optimal so­lar shadows with plural sunlight depth buffers. 
The Visual Computer, 17(2):76 90, 2001. [13] S. Upstill. The RenderMan Companion. Addison-Wesley, 1990. 
[14] M. Wand, M. Fischer, I. Peter, F. Meyer auf der Heide, and W. Straßer. The randomized z-buffer algorithm: 
Interactive rendering of highly complex scenes. Proc. of SIGGRAPH 2001, pages 361 370, 2001. [15] K. 
Weiler and K. Atherton. Hidden surface removal using poly­gon area sorting. Computer Graphics (Proc. 
of SIGGRAPH 77), 11(2):214 222, 1977. [16] L. Williams. Casting curved shadows on curved surfaces. Computer 
Graphics (Proc. of SIGGRAPH 78), 12(3):270 274, 1978. [17] A. Woo, P. Poulin, and A. Fournier. A survey 
of shadow algorithms. IEEE Computer Graphics and Applications, 10(6):13 32, November 1990.  Figure 
8: Notre Dame rendered with shadow maps of size 1024 .1024. The left image and the right halves of the 
center and right image were rendered with perspective shadow maps at about 15 frames per second. The 
left sides of the center and right image have been generated with uniform shadow maps of the same size. 
The rendering of the trees and the crowd is accelerated by point rendering. Figure 9: A street scene 
with moving cars and planes. The images were rendered at more than 10 frames per second with perspective 
shadow maps of size 1024 .1024. Note the different shadow details at varying distances. no shadows uniform 
shadow map perspective shadow map  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566617</article_id>
		<sort_key>563</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A user interface for interactive cinematic shadow design]]></title>
		<page_from>563</page_from>
		<page_to>566</page_to>
		<doi_number>10.1145/566570.566617</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566617</url>
		<abstract>
			<par><![CDATA[Placing shadows is difficult task since shadows depend on the relative positions of lights and objects in an unintuitive manner. To simplify the task of the modeler, we present a user interface for designing shadows in 3d environments. In our interface, shadows are treated as first-class modeling primitives just like objects and lights. To transform a shadow, the user can simply move, rescale or rotate the shadow as if it was a 2d object on the scene's surfaces.When the user transforms a shadow, the system moves lights or objects in the scene as required and updates the shadows in realtime during mouse movement. To facilitate interaction, the user can also specify constraints that the shadows must obey, such as never casting a shadow on the face of a character. These constraints are then verified in real-time, limiting mouse movement when necessary. We also integrate in our interface fake shadows typically used in computer animation. This allows the user to draw shadowed and non-shadowed regions directly on surfaces in the scene.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[human computer interaction]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[lighting design]]></kw>
			<kw><![CDATA[user interface design]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43116383</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382455</person_id>
				<author_profile_id><![CDATA[81100250004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Parag]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tole]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>272314</ref_obj_id>
				<ref_obj_pid>272313</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARZEL, R. 1997. Lighting controls for computer cinematography. Journal of Graphics Tools, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CALAHAN, S., 2000. Storytelling through Lighting, a Computer Graphics Perspective. In Advanced RenderMan, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147199</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CONNER, D. B., SNIBBE, S. S., HERNDON, K. P., ROBBINS, D. C., ZELEZNIK, R. C., ANDVAN DAMN, A., 1992. Three-Dimensional Widgets. In Proceedings of SIGGRAPH 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344938</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[GERSHBEIN, R. AND HANRAHAN P., 2000. A Fast Relighting Engine for Interative Cinematic Lighting Design. In Proc. of SIGGRAPH 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166136</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[KAWAI, J. K., PAINTER, J. S. AND COHEN, M. F., 1993. Radioptimiation --- Goal Based Rendering. In Proceedings of SIGGRAPH 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345073</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[PETROVIC, L., FUJITO, B., WILLIAMS, L. AND FINKELSTEIN, A., 2000. Shadows for Cel Animation. In Proceedings of SIGGRAPH 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147160</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[POULIN, P. AND FOURNIER, A., 1992. Lights from Highlights and Shadows. In Proc. of Symposium of 3D Interactive Graphics, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>792857</ref_obj_id>
				<ref_obj_pid>792756</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[POULIN, P., RATIB, K. AND JACQUES, M., 1997. Sketching Shadows and Highlights to Position Lights. In Proceedings of Computer Graphics Internation 97, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166135</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[SCHOENEMAN, C., DORSEY, J., SMITS, B., ARVO, J. AND GREENBERG, D. P., 1993. Painting with Light. In Proceedings of SIGGRAPH 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A User Interface for Interactive Cinematic Shadow Design Fabio Pellacini Parag Tole Donald P. GreenbergProgram 
of Computer Graphics, Cornell University ABSTRACT Placing shadows is difficult task since shadows depend 
on the relative positions of lights and objects in an unintuitive manner. To simplify the task of the 
modeler, we present a user interface for designing shadows in 3d environments. In our interface, shadows 
are treated as first-class modeling primitives just like objects and lights. To transform a shadow, the 
user can simply move, rescale or rotate the shadow as if it was a 2d object on the scene s surfaces. 
When the user transforms a shadow, the system moves lights or objects in the scene as required and updates 
the shadows in realtime during mouse movement. To facilitate interaction, the user can also specify constraints 
that the shadows must obey, such as never casting a shadow on the face of a character. These constraints 
are then verified in real-time, limiting mouse movement when necessary. We also integrate in our interface 
fake shadows typically used in computer animation. This allows the user to draw shadowed and non-shadowed 
regions directly on surfaces in the scene. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional 
Computer Graphics and Realism; I.3.6 [Computer Graphics]: Methodologies and Techniques Interactive Techniques. 
Keywords: Human Computer Interaction, Illumination, User Interface Design, Lighting Design 1. INTRODUCTION 
The patterns of light and shadows on the surfaces of objects are extremely important visual cues. In 
cinematic lighting design for movies, light designers carefully place lights and objects to specify the 
visual appearance of the scene and enhance storytelling [Calahan 2000]. Although so important, the task 
of designing shadows is extremely complex even for experienced users since shadows are affected by lights 
and objects positions in a very unintuitive manner. To simplify the task of light designers, we propose 
a new user interface that lets the user manipulate the shadows in the scene in real-time, while the system 
automatically adjusts lights and objects transformations as necessary. In our interface, shadows are 
treated as first-class modeling primitives just like objects and light sources. A shadow can be moved, 
rotated or scaled as if it was a 2d objects on the scene s surfaces, and similar to other objects in 
the scene. The mouse interface used is same as that used for object transformations in familiar graphics 
packages such as Maya or 3dsMax . For example, in order to place a shadow, the user simply clicks on 
the shadow, and then drags it to the desired location. In order to further simplify the task of placing 
shadows in a complex environment, we let the user place constraints on the shadows using an intuitive 
painting interface; for instance, the modeler may want to ensure that there is never a shadow on the 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 main character s face. These constraints are verified in realtime and enforced by limiting mouse 
movement when necessary. This allows the user to manipulate the shadows in complex environments more 
efficiently. In addition to real shadows caused by occlusion of light sources, fake shadows are used 
in cinematic lighting to enhance the visual appearance of the scene. We integrate fake shadows in our 
interface providing the user with the ability to draw shadowed and non-shadowed areas directly on the 
surfaces of the objects. The system automatically creates 3d cucaloris [Barzel 1997] that are updated 
appropriately when the lights are moved. We have implemented a prototype of our interface using shadow 
maps on commodity graphics hardware. Our prototype supports multiple lights, soft shadows and constraint 
validation in real-time. Since the only requirement to implement our interface is the support of hardware 
accelerated shadow maps, we can easily integrate our interface in relighting engines such as the one 
presented by Gershbein and Hanrahan [2000] to provide a complete light design solution for cinematic 
lighting. We believe that our shadow manipulation interface, combined with shadow constraints and fake 
shadows, provides the user with a powerful tool for shadow design for cinematic lighting. Our user interface 
for shadow manipulation is very intuitive since it takes advantage of users familiarity with standard 
interfaces for object transformations by treating the shadows as 2d primitives on the surfaces of the 
scene. 2. RELATED WORK Researchers in computer animation have tried to simplify lighting design interfaces. 
Poulin and Fournier [1992] presented an interface where the user interacts with the shadows by transforming 
the shadow volumes in a wireframe view. Although interesting, we believe that this interface is far less 
intuitive than manipulating the shadows themselves; also the shadow volume wireframe clutters the scene 
display, making it hard for the user to appreciate the scene s final look. In a following work, Poulin 
et al. [1997] presented a sketching interface to specify constraints on the shadows, which are then solved 
to compute the light positions. Unfortunately, it is almost impossible for the user to specify solvable 
constraints for complex scenes, thus limiting the usability of their system. Our constraints are specified 
in the same way, but instead of being solved, they just need to be verified so as to limit mouse movement 
when necessary. In the context of global illumination, Schoneman et al. [1993] presented an optimization 
technique based on a sketching metaphor, while Kawai et al. [1993] automatically set the light parameters 
based on a subjective impression of the scene illumination. Although useful, these systems cannot move 
the light sources. Also, none of these previous approaches allow the user to adjust object positions, 
which we do with the same interface as lights. 3. USER INTERFACE This section describes the user interface 
from the point of view of the modeler, leaving implementation details for the next section. In the rest 
of the paper, we will discuss only spotlights. Since omni lights and directional lights have fewer degrees 
of freedoms, they can be integrated using a subset of the user interface commands used for the spotlight 
case. 3.1 User interface paradigm Our physical user interface is based upon a traditional 2D input device 
such as a standard three-button mouse or a tablet. The interaction mode of the user with the scene is 
selected using modifier keys. Although important, the particular interface paradigm is not the main contribution 
of this paper; our algorithm can be integrated in other interface paradigms such as gestural systems, 
button widgets or gizmo widgets [Conner et al. 1992]. 3.2 Lighting operations 3.2.1 Overview When direct 
lighting illuminates a scene, hotspots and shadows define the patterns of the light on the surfaces of 
the objects. To completely specify the light patterns, the user has to be able to modify the position 
and size of the hotspot of each spotlight and the position, size and orientation of each shadow. While 
the hotspots depend only on the lights parameters, the shadows depend on the positions of lights as well 
as objects in the scene. When interacting with the shadows, the user chooses to either modify the light 
source or the object transformation by selecting the specific mode. Figure 1a shows the lighting operations 
provided in our interface together with the equations the system uses to update corresponding light and 
object transformations. 3.2.2 Shadow operations At any time during the light design process, our system 
presents the user with a rendering of the scene from an arbitrarily selectable camera position. By clicking 
on a shadow, the user selects the light-object pair that casts that shadow. Depending on the interaction 
mode, the user can move, scale or rotate the shadow as if it was a 2d object on the surfaces of the scene. 
The system figures out the correct transformation for either the light or the object. In case more than 
one object is casting the selected shadow, the system automatically chooses the one closest to the light; 
while in the presence of more than one light, the system selects the closest light as for the hotspot 
operations case. While this automatic selection seems to work well, the users have the option of overriding 
the automatic behavior and selecting the light-object pair. When changing the light parameters, the user 
can either move or rescale shadows. The user moves a shadow by dragging it on the surfaces of the objects 
in the scene. During mouse movement, the system moves the light source by rotating the ray connecting 
the picked point and the light around the closest point of the shadow­casting object to the light. To 
rescale a shadow, the user drags the mouse vertically while the system moves the light position along 
the line connecting the light position and the object center proportional to the amount of mouse movement. 
To simplify the light design task, it is important that the operations on shadows affect the appearance 
of the hotspots as little as possible. To ensure this, when the light position is being changed, we also 
update the direction of the light so that the position of the hotspot on the picked surface remains the 
same. We also would like to keep the size of the hotspot (as projected on the surfaces) the same. Unfortunately, 
this is not always possible on general surfaces; nonetheless we try to correct most of the size mismatch 
by resizing the hotspot angle as if the picked surface was a plane. The user can also change the position, 
size and rotation of the shadow by changing the position and rotation of the object casting the shadow. 
The interface to move and scale the shadow is the same as above. When moving the shadow, the system moves 
the object by rotating it around the light position. While resizing the shadow, the system moves the 
object along the line connecting the center of the object to the light position. To rotate the shadow, 
the user drags the mouse vertically, similarly to object rotation in commercial animation packages. The 
object is rotated around the line connecting the center of the object and the light position. The shadow 
operations on an object specify only four degrees of freedom. The remaining two are object rotations 
along two axes orthogonal to the line joining the light and the object. For a general object, these rotations 
affect the shape of the shadow in a completely unintuitive manner. We believe that adding these non­intuitive 
shape transformations as a shadow manipulation command would not help the users more than simply using 
standard object rotations (also provided in our prototype). 3.2.3 Additional shadow operations In the 
rescaling operation specified above, the position of the mouse on the screen does not represent the size 
of the shadow being rescaled (see Figure 1a). When finely tweaking the shadow, it would be helpful if 
simply dragging the shadow silhouette to a new position could rescale the shadow by that amount. In general 
this is not possible for an arbitrary silhouette and an arbitrary mouse movement. Nonetheless for small 
mouse movements, we can approximately achieve it. Depending on the selected action, the system moves 
the light or the object along the line passing though the light position and the object center. It can 
be shown, that the interface will behave as expected provided that the picked point during mouse drag 
is roughly in the plane containing the light position, the object center and the silhouette point picked 
by the initial mouse click. While the system tries to find the best solution for every possible mouse 
position, the shadow may not be in the expected position for large mouse movements. Our interface also 
supports soft shadows from area lights. The user can choose the shadow softness with the same mouse interface 
as rescaling the shadow. The system will rescale the size of the area light by the appropriate amount. 
 3.2.4 Hotspot operations The user can change the size and the position of the hotspot of the spotlights 
in the scene, by clicking on a hotspot and moving or rescaling it with the same mouse interface used 
for shadows. In the presence of multiple lights, the system automatically selects the light whose hotspot 
direction is closest to the line connecting the light position and the picked point. The user can choose 
to override the automatic behavior. When moving the hotspot, the system will compute an appropriate spotlight 
direction and while rescaling it, the hotspot angle will be modified. The manipulation of the hotspot 
is very similar to the one used by commercial animation packages, except that the hotspot is moved on 
the surface of objects, making it very simple for the user to shine a light on a specific location on 
a surface . Shadow and hotspot operations completely specify the light source s parameters. A spotlight 
has six degrees of freedom: 3 for position, 2 for direction and 1 for hotspot angle. The light and hotspot 
operations in our interface map these onto shadow position and size (3) and hotspot position and size 
(3). Since shadows and hotspots operations have the same interface as object transformations in commercial 
packages, the learning curve for new users is very short. In addition, since the hotspot operations do 
not affect the shadows, the design is simplified by making the shadow and hotspot interactions orthogonal. 
 3.3 Constraints When light designers are interacting with shadows in complex scenes, they often want 
to specify constraints that shadows have to obey during interaction. In our interface, the user can specify 
that an area of a surface should not change its shadow state by drawing the contour of that region on 
the objects. Drawing a constraint on a shadow has the effect of enforcing that the marked area will always 
remain in shadow by at least one object; while drawing the constraint on a lit area will make sure that 
no shadows will be in that area. While manipulating the shadows in the scene, the constraints are verified 
and the positions of objects and lights are updated only if all of the constraints are valid. Figure 
1b illustrates the interface for constraints. We believe that our interface to set constraints by painting 
is an intuitive way of specifying complex constraint behavior. Also, since we define a constraint as 
a set of points that cannot change their shadow state , we implicitly let the user specify only solvable 
constraints, thus avoiding the confusion of other interfaces where unsolvable constraints can be specified. 
 3.4 Shadow Cookies Shadow cookie [Barzel 1997] is the colloquial term for cucaloris, the technical term 
for an opaque card with cutouts used to block a light. In special effects production environments, cookies 
are used to add fake shadows from non-existent objects. Our interface allows the user to paint shadow 
cookies with the same painting metaphor used for constraints, shown in Figure 1c. The user can attach 
the cookie to the light source or to the world selecting the appropriate interaction mode. When the cookie 
is attached to the light, moving the light will not move the position of the cookie shadow on the surfaces 
of the objects. On the other hand, if the cookie is attached to the world, then moving the light will 
move the shadow. Also world-attached shadow cookies are first-class shadow objects and their shadows 
can be individually manipulated using the shadow-object mode. We also introduce light cookies that are 
used to subtract any existing shadows from their region (Figure 1c). They are drawn in the same way as 
shadow cookies and can also be attached to the light or to the world. World-attached light cookies can 
also be used to cut parts of the shadows cast by specific objects.  4. IMPLEMENTATION DETAILS Our lighting 
interface requires realtime updates of the rendered scene with shadows. We achieve this by using hardware 
shadow maps for computing shadows and hardware lighting for the light contributions. Our prototype runs 
at about 20 frames per second on a Pentium III 500 MHz with a GeForce 3 graphics board for scenes with 
4 light sources using a multipass tecnique. When computing shadows from area lights, the number of samples 
is reduced to 4 during mouse interaction, and the final image is computed when the interaction stops. 
During interaction, the system determines the 3d position of the point picked by the mouse by querying 
the depth buffer of the camera view. Also the system needs to determine which light­object pair is casting 
a shadow on the picked point. This is computed by querying the depth and id buffers of the given light 
s view. Since the two read back operations are required only at mouse click, there is no slowdown when 
dragging the mouse. Constraints are represented as the array of the 3d positions of the points on the 
contour of the constraint. This data structure makes it simple to specify constraints on surfaces of 
any shape. We verify a constraint by checking if any of the points in the constraint has changed its 
shadow state. To do this, we query the depth buffer of each light with the position of each point in 
the constraint. We represent cookies as 3d polygons created halfway between the light position and the 
picked point. Shadow cookies are drawn only in the shadow map pass, while light cookies are drawn in 
the stencil buffer, which is then used to set the stencil test to fail in these regions. Representing 
the cookies as 3d objects makes it very easy to use the rest of our interface to manipulate the cookies 
themselves. It would be much harder to modify the cookie later if we were to directly paint a matte in 
the shadow as is normally done in computer animation [Barzel 1997]. 5. CONCLUSIONS AND FUTURE WORK We 
presented a user interface for interactive cinematic shadow design consisting of an intuitive interface 
for shadow manipulation and a painting interface for shadow constraints and fake shadows. Our interface 
can be easily integrated in relighting engines such as [Gershbein and Hanraham 2000] to provide a more 
complete interactive lighting design solution. We believe that our interface to manipulate shadows is 
intuitive to use; treating shadows as 2d objects on the surfaces of the scene allows us to leverage the 
user s familiarity with object transformations. We also believe that the ability to draw intuitive constraints 
simplifies the design process for accurate shadows in complex lighting situations. We would like to extend 
our interface in several directions. First, we would like to try to integrate our shadow manipulation 
interface in other paradigms such as widgets [Conner 1992] and sketching that have proven to be very 
fruitful in surface modeling. We would also like to extend our shadow interface to support more complex 
light parameters such as the one presented in [Barzel 1997] to provide a better user interface for a 
more complete light design solution. For the fake shadow components, we would like to investigate if 
the behavior of more complex cookie representations such as [Petrovic et al. 2000] can also be made more 
intuitive to use for the user. We would also like to test how our system scales with the complexity of 
the scene. First, updating the shadows in realtime may not be possible in some cases; integrating the 
use of simplified geometry for shadow computation might make the system more usable. Furthermore, an 
extremely complex environment could have very complicated overlapping shadow patterns that could make 
the selection of light-object pair cumbersome. While our informal tests suggest that our interface scales 
well with up to four lights and hundreds of objects, more formal user studies are needed to verify the 
scalability when the complexity is dramatically increased. Also, in the presence of very complex shadow 
patterns, it might be useful to specify constraints relatively to specific light-object pairs. While 
our constraint validation system currently supports these constraints, we would like to extend our constraint 
painting metaphor to support these more complex constraint specifications. Finally, we would like to 
extend our interface to support animated scenes. Currently the user can design the shadows for animated 
sequences by lighting a few selected frames separately and using keyframe interpolation for the remaining 
frames; fruitful extensions could be the use of shadow movements to define the animated parameters of 
the lights directly and the specification and validation of constraints for the entire animation sequence. 
 ACKNOWLEDGMENTS We would like to thank Randy Fernando, James Ferwerda, Moreno Piccolotto and Bruce 
Walter for their comments. This work was supported by the NSF Science and Technology Center for Computer 
Graphics and Scientific Visualization (ASC-8920219) and performed using equipment generously donated 
by Intel Corporation and NVidia Corporation. BIBLIOGRAPHY BARZEL, R. 1997. Lighting controls for computer 
cinematography. Journal of Graphics Tools, 1997. CALAHAN, S., 2000. Storytelling through Lighting, a 
Computer Graphics Perspective. In Advanced RenderMan, 2000. CONNER, D.B., SNIBBE, S.S., HERNDON, K.P., 
ROBBINS, D.C., ZELEZNIK, R.C., AND VAN DAMN, A., 1992. Three-Dimensional Widgets. In Proceedings of SIGGRAPH 
1992. GERSHBEIN, R. AND HANRAHAN P., 2000. A Fast Relighting Engine for Interative Cinematic Lighting 
Design. In Proc. of SIGGRAPH 2000. KAWAI, J.K., PAINTER, J.S. AND COHEN, M.F., 1993. Radioptimiation 
Goal Based Rendering. In Proceedings of SIGGRAPH 1993. PETROVIC, L., FUJITO, B., WILLIAMS, L. AND FINKELSTEIN, 
A., 2000. Shadows for Cel Animation. In Proceedings of SIGGRAPH 2000. POULIN, P. AND FOURNIER, A., 1992. 
Lights from Highlights and Shadows. In Proc. of Symposium of 3D Interactive Graphics, 1992. POULIN, P., 
RATIB, K. AND JACQUES, M., 1997. Sketching Shadows and Highlights to Position Lights. In Proceedings 
of Computer Graphics Internation 97, 1997. SCHOENEMAN, C., DORSEY, J., SMITS, B., ARVO, J. AND GREENBERG, 
D.P., 1993. Painting with Light. In Proceedings of SIGGRAPH 1993. Figure 1: a) Lighting interaction 
modes. b) Constraints. c) Cookies. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566618</article_id>
		<sort_key>567</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Robust epsilon visibility]]></title>
		<page_from>567</page_from>
		<page_to>575</page_to>
		<doi_number>10.1145/566570.566618</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566618</url>
		<abstract>
			<par><![CDATA[Analytic visibility algorithms, for example methods which compute a subdivided mesh to represent shadows, are notoriously unrobust and hard to use in practice. We present a new method based on a generalized definition of extremal stabbing lines, which are the extremities of shadow boundaries. We treat scenes containing multiple edges or vertices in degenerate configurations, (e.g., collinear or coplanar). We introduce a robust &#949; method to determine whether each generalized extremal stabbing line is blocked, or is touched by these scene elements, and thus added to the line's generators. We develop robust blocker predicates for polygons which are smaller than &#949;. For larger &#949; values, small shadow features merge and eventually disappear. We can thus robustly connect generalized extremal stabbing lines in degenerate scenes to form shadow boundaries. We show that our approach is consistent, and that shadow boundary connectivity is preserved when features merge. We have implemented our algorithm, and show that we can robustly compute analytic shadow boundaries to the precision of our chosen &#949; threshold for non-trivial models, containing numerous degeneracies.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D visibility]]></kw>
			<kw><![CDATA[epsilon visibility]]></kw>
			<kw><![CDATA[illumination]]></kw>
			<kw><![CDATA[robust visibility predicates]]></kw>
			<kw><![CDATA[shadow algorithms]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382433</person_id>
				<author_profile_id><![CDATA[81100625118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Florent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duguet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES - INRIA Sophia-Antipolis, France and Ecole Nationale Sup&#233;rieure des T&#233;l&#233;communications (ENST), Paris]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026929</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[REVES - INRIA Sophia-Antipolis, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344954</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Agrawala, R. Ramamoorthi, A. Heirich, and L. Moll. Efficient image-based methods for rendering soft shadows. In ACM SIGGRAPH 2000, Annual Conference Series, pages 375-384, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>336417</ref_obj_id>
				<ref_obj_pid>336414</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[K. Bala, J. Dorsey, and S. Teller. Radiance interpolants for accelerated bounded-error ray tracing. ACM Transactions on Graphics, 18(3):213-256, July 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97896</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. T. Campbell, III and D. S. Fussell. Adaptive mesh generation for global diffuse illumination. Computer Graphics (Proc. SIGGRAPH '90), 24:155-164, August 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147159</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[N. Chin and S. Feiner. Fast object-precision shadow generation for areal light sources using BSP trees. In Computer Graphics (1992 Symposium on Interactive 3D Graphics), volume 25, pages 21-30, March 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[F. C. Crow. Shadow algorithms for computer graphics. Computer Graphics (Proc. SIGGRAPH 77), 11(2):242-248, July 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192207</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Drettakis and E. Fiume. A fast shadow algorithm for area light sources using back projection. In ACM SIGGRAPH 94, Annual Conference Series, pages 223-230, July 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134027</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[T. Duff. Interval arithmetic and recursive subdivision for implicit functions and constructive solid geometry. Computer Graphics (Proc. SIGGRAPH'92), 26(2):131-138, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[F. Durand. 3D Visibility: analytical study and applications. PhD thesis, Universit&#233; Joseph Fourier, Grenoble I, July 1999. http://www-imagis.imag.fr.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258785</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[F. Durand, G. Drettakis, and C. Puech. The Visibility Skeleton: A Powerful and Efficient Multi-Purpose Global Visibility Tool. In ACM SIGGRAPH 97 (Los Angeles, CA), Annual Conference Series, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>318012</ref_obj_id>
				<ref_obj_pid>318009</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[F. Durand, G. Drettakis, and C. Puech. Fast and accurate hierarchical radiosity using global visibility. ACM Trans. on Graphics, 18(2):128-170, Apr 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>80992</ref_obj_id>
				<ref_obj_pid>80983</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Z. Gigus and J. Malik. Computing the aspect graph for the line drawings of polyhedral objects. IEEE Trans. Pattern Analysis and Machine Intelligence, 12(2), February 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[E. A. Haines. Shaft culling for efficient ray-traced radiosity. In Photorealistic Rendering in Comp. Graphics, pages 122-138. Springer Verlag, 1993. Proc. 2nd EG Workshop on Rendering (Barcelona, 1991).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[P. Heckbert. Discontinuity meshing for radiosity. Proc. Third Eurographics Workshop on Rendering, Bristol, pages 203-226, May 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J. J. Koenderink and A. J. van Doorn. The internal representation of solid shape with respect to vision. Biological Cybernetics, 32(4):211-216, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732133</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[L. Leblanc and P. Poulin. Guaranteed occlusion and visibility in cluster hierarchical radiosity. In Rendering Techniques 2000, (Proc. 11th Eurographics Workshop on Rendering 2000), pages 89-100, June 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. Lischinski, F. Tampieri, and D. P. Greenberg. Discontinuity meshing for accurate radiosity. IEEE CGA, 12(6):25-39, November 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[nvidia. webpage. http://developer.nvidia.com/view.asp?IO=cedec_stencil.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>73857</ref_obj_id>
				<ref_obj_pid>73833</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[D. Salesin, L. Guibas, and J. Stolfi. Epsilon geometry: Building robust algorithms from imprecise computations. In Annual Symposium on Computational Geometry, 1989. Saarbrucken, West Germany.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134024</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J. M. Snyder. Interval analysis for computer graphics. Computer Graphics (Proc. SIGGRAPH'92), 26(2):121-130, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280927</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[C. Soler and F. X. Sillion. Fast calculation of soft shadow textures using convolution. In ACM SIGGRAPH'98, Annual Conference Series, pages 321-332, Jul 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192210</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A. J. Stewart and S. Ghali. Fast computation of shadow boundaries using spatial coherence and backprojections. In ACM SIGGRAPH 94, Annual Conference Series, pages 231-238, July 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[S. Teller. Visibility Computations in Densely Occluded Polyhedral Environments. PhD thesis, UC Berkeley, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[S. J. Teller. Computing the antipenumbra of an area light source. Computer Graphics (Proc. SIGGRAPH 92), 26(4):139-148, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[K. Weiler and K. Atherton. Hidden surface removal using polygon area sorting. Computer Graphics (Proc. SIGGRAPH 77), 11(2):214-222, July 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Casting curved shadows on curved surfaces. Computer Graphics (Proc. SIGGRAPH 78), 12(3):270-274, August 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Robust Epsilon Visibility Florent Duguet and George Drettakis REVES -INRIA Sophia-Antipolis, France, 
http://www-sop.inria.fr/reves/.  Figure 1: (Far left) A Dandelion model of 62,000 polygons. Using robust 
e visibility, the output shadow mesh has 270,000 polygons for e =10.4. From left to right: shadows on 
the leaves and ground (top view) for e =10.4, and e =10.2, resulting in 220,000 polygons, with no visible 
difference. Rightmost two images: zoom of the ground for e =10.4 and e =10.2; some missing features are 
now visible. Abstract Analytic visibility algorithms, for example methods which compute a subdivided 
mesh to represent shadows, are notoriously unrobust and hard to use in practice. We present a new method 
based on a generalized de.nition of extremal stabbing lines, which are the extremities of shadow boundaries. 
We treat scenes containing mul­tiple edges or vertices in degenerate con.gurations, (e.g., collinear 
or coplanar). We introduce a robust e method to determine whether each generalized extremal stabbing 
line is blocked, or is touched by these scene elements, and thus added to the line s generators. We develop 
robust blocker predicates for polygons which are smaller than e. For larger e values, small shadow features 
merge and even­tually disappear. We can thus robustly connect generalized extremal stabbing lines in 
degenerate scenes to form shadow boundaries. We show that our approach is consistent, and that shadow 
boundary connectivity is preserved when features merge. We have imple­mented our algorithm, and show 
that we can robustly compute an­alytic shadow boundaries to the precision of our chosen e threshold for 
non-trivial models, containing numerous degeneracies. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Color, shading, shadowing, and texture I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Visible line/surface algorithms Keywords: Illumination, Shadow Algorithms, 3D Visibility, 
Ro­bust Visibility Predicates, Epsilon Visibility. .{Florent.Duguet|George.Drettakis}@sophia.inria.fr. 
The .rst author is also af.liated to the Ecole Nationale Sup´erieure des T´el´ecommunications (ENST), 
Paris. Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make 
digital or hard copies of part or all of this work for personal or classroom use is granted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for components of this work owned by others than 
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on 
servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 1 Introduction Computing visibility is central for many aspects of computer 
graph­ics. From the most basic visible surface determination, to shadows and global illumination calculations, 
a major part of computational effort is spent on visibility. Although discrete approximations or sampling 
such as the z-buffer or ray-casting are often used in prac­tice, analytic methods in which a continuous 
and precise solution is computed, can be very useful. In particular, for shadow cal­culations [16, 21, 
6], or global illumination [10], the importance and utility of analytic visibility methods has been demonstrated. 
For virtual environments or games, graphics engines can now han­dle large polygon counts, even for low-end 
platforms. In certain cases, for games engines or virtual reality, texture memory may be a scarce resource, 
precluding its use for shadow representation. As a result, subdividing the input geometry into (partially) 
shadowed and lit sub-polygons using one of these methods could well be the best solution for displaying 
high-quality shadows for these appli­cations. Note that such subdivision is a view-independent solution 
with shadows, with no additional shadow processing per frame. We will refer to this subdivision of the 
input scene as a shadow mesh in what follows. The shadow mesh can be created either for hard shad­ows 
(such as from a directional or point source) or for soft shadows from an area source. In practice however, 
analytic methods have not been used be­cause they suffer from robustness problems, algorithmic complex­ity 
and/or memory restrictions. These problems render them unus­able for the type of scene used in virtual 
reality, games or other in­teractive applications. Such scenes are geometrically complex, and typically 
contain a large number of degeneracies: objects touch, edges are often aligned or coplanar etc. In addition, 
since .oating point numbers are used both for the modeling phase and visibility computation, below a 
certain threshold, small features of the model or of the resulting shadows can lead geometric algorithms 
to fail. Object connectivity is not always given in such models and can­not always be reconstructed, 
and intersecting polygons are often present in the models. All of these properties can lead to robustness 
problems for geometric algorithms. 1.1 Motivation To create shadow meshes, in particular for soft shadows, 
disconti­nuity meshing approaches [13, 6, 16] intersect shadow boundary surfaces, or swaths, with scene 
geometry. This approach is in­evitably unrobust for large and degenerate scenes, since numerical problems 
quickly lead to loss of connectivity in the shadow mesh, or result in geometrical errors due to small 
features. We can observe however that shadow boundaries are delimited by extremal stabbing lines [23]. 
Two examples are shown in red in Fig. 2 (a). One way to generate shadow boundaries is to com­pute extremal 
stabbing lines, and then join them to form the actual swaths which constitute shadow boundaries. A simpler 
problem is thus solved, since in essence we only perform line-or ray-casting. This results in a stabler 
approach, as was shown in the Visibility Skeleton [9]. For non-degenerate scenes, the number of possible 
con.gurations of connectivity between extremal stabbing lines and swaths is .nite and small. As a result, 
Durand et al., [9], devel­oped a catalog of swaths and their neighboring lines to establish connectivity, 
based on a small set of non-degenerate con.gurations of edges, vertices and faces. The result of this 
construction, among other applications, is the shadow mesh. (b) Figure 2: (a) Shadow boundaries are 
delimited by extremal stabbing lines (in red). (b) In complex models, stabbing lines may contain multiple 
collinear vertices. (c) If vertices are almost collinear, we choose to consider them as being on the 
same line. Unfortunately, this approach will fail for complex scenes, such as those encountered in games 
or VR applications. Consider Fig. 2(b), where vertices of several leaves of a tree are collinear. Build­ing 
shadow adjacencies based on a .nite set of con.gurations is no longer possible, since a catalog capable 
of treating all cases would be in.nitely big. Despite treatment of certain cases for the test scenes 
used in [10], a complete solution is lacking, as men­tioned by the authors ([10], pages 164 166). This 
fact is also clearly demonstrated by the small size and speci.c (i.e., custom­built) nature of the test 
scenes used in all previous analytic ap­proaches [6, 21, 16, 9, 10], in which the largest test scenes 
used were under 2,000 polygons. If edges or vertices are almost aligned or coplanar (see Fig. 2(c)) problems 
will appear, since resulting small shadow mesh features become sources of numerical insta­bility, such 
as small, sliver triangles, edge side-test problems etc. Finally, many models are inaccurate, and may 
contain intersecting polygons or lack connectivity information. To use the catalog ap­proach, expensive 
and numerically unstable preprocessing would have to be performed to re-facetize the input scene based 
on these intersections, and then to reconstruct all connectivity information around edges. 1.2 Contributions 
To robustly compute shadow boundaries for real-world scenes, we develop an approach based on generalized 
extremal stabbing lines. Candidate extremal stabbing lines are proposed based on native generators for 
non-degenerate con.gurations [9], i.e., VV , VEE and E4, with V a vertex and E an edge. We attach additional 
generators to the candidate line, using e methods to robustly de­termine whether a single feature (edge 
or vertex) is a generator or a blocker. The set of attached generators ensures that degenerate stab­bing 
lines are correctly constructed, and allows us to robustly estab­lish shadow boundary connectivity. We 
show that our e method is consistent, by avoiding undesired propagation of e contact and by ensuring 
consistent shadow-boundary connectivity. The use of e methods requires special attention when encounter­ing 
faces (polygons), smaller than the e threshold. We introduce robust blocker predicates both for the case 
when face connectivity is available, and for models lacking this information. We also treat models with 
intersecting polygons as input. The resulting structure is suf.cient for the generation of a shadow mesh. 
An interesting advantage of our methods is that when the value of e used in the robust computation is 
increased, small shadow features will disappear, reducing the size of the shadow mesh. These methods 
have allowed us to implement robust shadow mesh computation for soft and directional shadow boundaries 
for complex, real-world scenes, such as those shown in Fig. 1, 14 19.  2 Previous Work Very early work 
in computer graphics attempted to compute shad­ows using polygon clipping methods [24]. These were the 
precur­sors of analytic visibility methods which have inspired our work. It is beyond the scope of this 
paper to survey all work on visibil­ity. An excellent survey of the whole spectrum of visibility meth­ods 
in computer graphics can be found in F. Durand s Ph.D. thesis [8]. For directional and point light sources, 
shadow volumes [5] allow the computation of shadow boundaries. They suffer how­ever from robustness problems 
and computational expense, both in the preprocess and for rendering since they are very polygon-.ll intensive 
[17]. Various other methods have been developed, for ex­ample the methods of Campbell and Fussell [3] 
or Chin and Feiner [4] based on BSP-trees, which also run into robustness or mem­ory problems due to 
the geometric operations and data structures involved. Discontinuity meshing approaches were introduced 
by Heckbert [13] and Lischinski et al. [16]. These methods compute shadow discontinuity surfaces (line 
swaths), by intersecting them with the scene, with the resulting robustness problems previously discussed 
in Sect. 1.1. The ideas developed in the original aspect graph vision literature [14, 11], and introduced 
to graphics by Teller [23] have led to algorithms computing the complete discontinuity mesh [6, 21]. 
These include the backprojection data structure representing the visible part of the source for rapid 
computation of penumbral lighting. These methods suffer from the same robustness problems. We have already 
mentioned the Visibility Skeleton [9], which computes extremal stabbing lines by casting them into the 
scene, and uses a .nite catalog to establish swath connectivity. This ap­proach was used to compute shadows, 
but also in a global illumi­nation algorithm which adapts the radiosity mesh using perceptual criteria 
to insert discontinuities [10]. Many approximate or discrete methods have been developed for shadow computation 
(e.g., shadow maps [25]) and ray-tracing is ev­idently one of the most widespread point-sampling approaches 
used in practice. For soft shadows, convolution methods [20] give con­vincing results, which may contain 
some error however, depending on the con.guration. All these methods use a discrete buffer, and thus 
resolution and resulting aliasing are the predominant problems. More recent work [1] presents an image-based 
approach compris­ing an ef.cient interactive solution with limited control of sampling and an expensive, 
view-dependent ray-tracing solution. We adopt an e geometry approach [18], albeit without perturbing 
the input data, using interval arithmetic to robustly treat degenera­cies for our visibility computations. 
Interval approaches have been extensively used in computer graphics, for example for implicit functions 
and CSG [7] or by Snyder [19] for a variety of problems such as ray-tracing, interference detection etc. 
To our knowledge, e methods have only been used to a limited extent for 3D visibility or shadow problems, 
notably by Bala et al. [2] for ray-tracing.  3 Generalized Visibility Events As mentioned in the introduction, 
we will treat scenes containing degeneracies, such as those used in real-world applications. We .rst 
de.ne our new framework, which is based on a general de.nition of extremal stabbing lines (ESL) and line 
swaths. This de.nition overcomes the limitations of a catalog of visibility events [9], since it will 
not depend on a .nite enumeration of speci.c con.gurations. We .rst de.ne the basic entities we use. 
A generator is an edge or a vertex of the scene 1. For edges we distinguish silhouette edges with respect 
to a given line direction, in the traditional sense of the term, i.e., that the edge is attached to a 
single face, or that one of the two faces connected to the edge is back-facing with respect to the line. 
A silhouette vertex is de.ned depending on its local blocking properties, i.e., whether it blocks an 
extremal stabbing line or not, depending on its neighboring faces. This is de.ned in detail in Section 
4.1 (see also Fig. 6).   Figure 3: An extremal stabbing line l1, with generators V1,V2,E1,E2, and 
up and down blockers blockup and blockdown, and a swath between ESL l1 and ESL l2, sharing E2 and V2. 
A blocker is a face, or non-silhouette edge or vertex touched by a line. A generalized extremal stabbing 
line (ESL) is a data structure, containing the corresponding line equation (also called supporting line), 
the set of associated generators, and its up and down blocker. A degenerate ESL, l1, its generators and 
blockers are shown in Fig. 3. The limits of a critical line swath, or simply swath, are its up and down 
blockers, and the start and end ESL s (see Fig. 3). To establish shadow boundaries, we need to connect 
stabbing lines with swaths. Since we can no longer use a .nite catalog, we will de.ne when it is possible 
to connect pairs of ESL s. The intuition is that there needs to be a suf.cient number of shared generators 
to de.ne a planar (e.g., edge-vertex) or quadric (edge­edge-edge) swath joining the two ESL s. The process 
we use is procedural, and detailed in Section 4.3. In what follows we consider visibility events between 
an emit­ter and a receiver pair. This is a logical choice for shadow com­putations, and allows us to 
de.ne distances and sizes in a mean­ingful manner for our e algorithms. This can be seen as a lazy 1In 
contrast to [9], we do not consider faces as generators. In the context of a .nite catalog, such a consideration 
is appropriate, since it allows a more concise presentation of possible con.gurations. evaluation of 
parts of the visibility skeleton [9]. In theory, the Visi­bility Skeleton could be computed using our 
algorithm, if we com­puted and stored all emitter-receiver pairs. This would however be impractical for 
any scene of reasonable size, due to the (at least) quadratic complexity in memory. Our e approach is 
based on dis­tance, de.ned across the shaft formed by the emitter and receiver. Thus merging operations 
vary depending on the chosen receiver­emitter pair; querying the entire structure would require special 
manipulation. However, for all the applications using the visibil­ity skeleton to date [9, 10], (shadows, 
discontinuity mesh, global illumination), emitter-receiver pair computations suf.ce, so we do not consider 
this restriction signi.cant.  4 Enumeration and Validation of ESL s and Swaths The .rst step in our 
approach is the enumeration and validation of extremal stabbing lines (ESL). We construct ESL s, associating 
all appropriate generators as we proceed, as well as start and end blockers, if they exist. The second 
step is the identi.cation and validation of swaths connecting ESL pairs. ESL s are cast into the scene 
to identify potential generators. We will use e methods to consistently treat almost coplanar or almost 
collinear events, during this process. Before the enumer­ation and validation, we present the speci.cs 
of our e methods. 4.1 e methods While casting lines, we need to identify whether a scene element, face 
(polygon), edge or vertex interacts with the line. If there is an interaction, it can be either a blocking 
interaction, i.e., we consider that the line is blocked, or can be a generator interaction, i.e., the 
line grazes the element, for example an edge, and thus we consider that the edge is a generator of the 
line. Consistent attribution of generators is a key to correct connectivity of shadow boundaries, since 
ESL s are connected by looping over generators. We perform all calculations to determine whether an interaction 
exists, and its type, in .oating point arithmetic, up to a precision of a pre-determined e threshold. 
 (a)(b) ( ) Figure 4: (a) A fat line or fat ray, (b) A fat vertex (c) A fat edge. The e parameter is 
given in the same units as the model of the scene. Thus if the scene is de.ned for example in meters, 
e will also be de.ned in meters. Thus, the algorithm reacts as expected to scaling operations, i.e., 
if the entire scene is scaled by a coef.cient of say 2, then the result will be the same in the topological 
sense, as if e was divided by 2. We use an interval arithmetic approach with .oating point val­ues for 
upper and lower bounds. We de.ne e contact in a natural manner: two elements are in e contact, if and 
only if their distance is below e. We say that an edge or vertex of the scene is in inter­action with 
a line if and only if its distance to the line is below e. For a vertex p and line we project the vertex 
onto the line to .nd the closest point q, and for a line and an edge, p and q are the pair of closest 
points of the two lines. An equivalent de.nition is that a Figure 5: (a) No-stab, the line does not interact 
with the triangle (b) Full-stab, the line is blocked by the triangle (c) Edge-stab, the line touches 
the edge but nothing else (d) Vertex-stab the line touches the vertex but nothing else (e) Multi-stab: 
the fat line touches multiple elements.  vertex or an edge interacts with the line if it hits the associated 
fat element, i.e., sphere or shaft. This e contact approach is central to our algorithm. As a result 
of our e approach, scene elements are transformed. A line is transformed into a shaft, which we call 
a fat line or fat ray (Fig. 4(a)). This shaft is the cylinder de.ned by the spheres of any two points 
centered on the line. A vertex is a sphere of radius e, which we will call a fat vertex, Fig. 4(b), and 
an edge is a cylindrical shaft (similar in spirit to that of [12]) between the two spheres of its extremal 
vertices, which we will call a fat edge, Fig. 4(c). Since faces are only considered as blockers in the 
context of our approach, they remain as is. There are .ve kinds of interactions between a line and a 
face, shown in Fig. 5. The .rst two cases, no-stab and full-stab, are han­dled trivially (Fig. 5(a) and 
(b)), since the .rst case results in no in­teraction, and the second results in the line being blocked. 
The third and fourth (Fig. 5(c) and (d)) are followed by a blocker/generator test described below, in 
order to check if the element is a blocker or a generator for the line. The last case is more complex 
and is the essence of our e approach (Fig. 5(e)). In Section 5, we introduce techniques to robustly determine 
whether this con.guration blocks the line. The blocker/generator tests for edges and vertices proceed 
as fol­lows. Since the line may not pass exactly through the edge, for an edge-stab we form a plane . 
with the edge (points A and B in Fig. 6(a)) and normal to a direction perpendicular to the line with 
its origin on the edge. We can always de.ne this plane since we do not have a vertex-stab, and thus the 
edge is not on the line. We then compute the position of the faces attached to the edge with respect 
to this plane. If faces are present on both sides, the edge is a blocker, otherwise, the edge is a generator 
(Fig. 6(b)). This is equivalent to determining whether we have a silhouette edge with respect to the 
line. For the vertex stab, let . be a plane orthogonal to the line, for instance a plane passing through 
the vertex. We compute the or­thogonal projections of the faces surrounding the vertex onto .. Each face 
around the vertex can be seen as a slice of a pie. We then merge the slices in contact. If the slices 
merge into a whole pie, then the vertex is a blocker, otherwise, it is a generator (see Fig. 6(c)). 
 4.2 Enumeration and Validation of ESL s We have now de.ned the framework for generalized visibility 
events and our e visibility approach, which are the basic elements required to de.ne our new, robust, 
algorithm. We start with the enu­meration of non-degenerate or generic ESL s, that is VV , VEE and E4, 
as de.ned in previous work [23, 9]. For VV and VEE the line equation is de.ned as in [9]. For E4 ESL 
s we adopt the method previously proposed by Teller [22]. An alternative would be to use the bisection 
approach described in [9]. The choice of which generators are considered as native depends on the order 
in which we evaluate them: We demonstrate below that  (a)(b) ( ) Figure 6: (a) Determining whether 
an edge is a blocker for an edge­stab (b) Top, the edge is blocked, bottom the edge is a generator (c) 
Top the vertex is a generator, bottom, the vertex is a blocker. a consistent result is obtained with 
our method independently of the order of evaluation. The algorithm proceeds with an ESL casting process. 
First, generic ESL s are formed by enumeration of native generators. For each such generic line, we propose 
a candidate ESL and cast it through the scene. The ESL-casting process has a dual function: we .nd and 
attach all additional generators to form the appropri­ate set attached to a degenerate ESL, and we test 
visibility of the stabbing line to decide whether the candidate ESL is valid. Finally, redundant ESL 
s are also eliminated. 4.2.1 ESL Casting A candidate ESL is de.ned by a line and a set of native generators 
(VV , VEE, E4). The ESL casting process is very similar to a tradi­tional ray-casting algorithm, but 
is based on fat rays as described previously. The casting process identi.es the elements, either face, 
edge or vertex, which interact with the ESL, as described in the pre­vious section. The fat ray emanates 
from the emitter, and we visit elements in order of increasing distance from the origin of the fat ray. 
This is achieved cheaply by the traversal routine of the accel­eration structure described below; a sort 
is performed on elements in the interior of a cell of the acceleration structure. For each such element, 
we test if it is a blocker, using the e stab­bing process. If it is, we .ag the element as the down blocker 
of the extremal stabbing line, and the casting process is stopped. If not, we add the element to the 
set S of stabbed elements associated with the ESL, which is initially empty. Note that the native generators 
are not contained in S at the outset of the casting process. Once the ray is stopped, we compare the 
set of stabbed elements S, and the set of native generators N.If N cS, then all the na­tive generators 
are touched by the candidate ESL, which is then validated. The set of stabbed elements also constitutes 
the set of generators associated with the ESL. This process is illustrated in Fig. 7. To accelerate swath 
validation, references to the ESL s are stored on the scene edges and vertices.  Figure 7: The ESL casting 
process. (a) Before casting, the line is formed by the native generators v1 and v2, S is empty and the 
line originates at the emitter. (b) The ESL encounters the generator v1, and in this con.guration, the 
ESL is invalidated by the face f . (c) Here, the ESL encounters generators v1,v2,e1,e2. This is a valid 
ESL, with down blocker f , since S contains the native generators. Figure 8: Elimination of redundant 
ESL s. (a) The native generators of l1 are v1 and v2, and we add v3 and v4. (b) Native generators of 
l2 are v2 and v3, and we add v1. l1 and l2 satisfy the combinatorial criterion and the topological criterion, 
and thus l2 is eliminated. (c) Native generators of l3 are v3 and v4, and we add v2. Even though the 
combinatorial criterion is satis.ed with respect to l1, the topological criterion is not, and thus l3 
is maintained as a separate ESL. We use standard acceleration techniques for fat-ray casting. Faces are 
simply added to the acceleration structure cells if they are either inside the cell or at a distance 
less than e. We use a recur­sive grid for faces and subdivide based on a 3 n criterion, where n is the 
number of polygons at each level. Most other acceleration techniques for ray tracing could also be used 
for our ESL casting.  4.2.2 Elimination of Redundant ESL s As mentioned above, we consider the order 
in which we traverse generators to be irrelevant. For this to give a consistent result, we need a method 
which will eliminate redundant ESL s when they are suf.ciently close to an existing ESL, with respect 
to the e chosen. This elimination step must be done carefully, to avoid propagation of e across generators, 
which would result in the merge of lines which in reality are geometrically far apart. We thus de.ne 
a consistent elimination method for ESL s, based on two criteria: a combinatorial and a topological criterion. 
An ESL lc satis.es the combinatorial criterion with respect to another ESL le, if and only if the set 
of its native generators is com­pletely contained in the set of generators of the other ESL le. An ESL 
lc satis.es the topological criterion with respect to an­other ESL le, if and only if the distance between 
the two supporting lines is less than e. The distance between two lines used for this test is de.ned 
with respect to the emitter-receiver shaft. In particular it is de.ned as the maximum distance (over 
the shaft), between any point of one line with respect to the other line. If an ESL satis.es both criteria 
with respect to another, existing ESL, then it is redundant and is eliminated. The entire process is 
illustrated in Fig. 8. Note that the test can be performed before the actual ESL cast­ing. We thus compare 
the candidate line to existing ESL s pre­venting unnecessary computation implied by the actual casting 
al­gorithm. We only search a subset of existing ESL s, since we .rst need to satisfy the .rst criterion. 
 4.3 Enumeration and Validation of Line Swaths Once we have generated all the ESL s, we create the swaths 
which join them, if required by the application. Since we have foregone the catalog, we use a procedural 
construction. Swath creation in­volves two steps, enumeration, to determine whether a swath can exist 
between two ESL s, and validation, which veri.es visibility with respect to the rest of the scene. Swaths 
are found by looping over all edges and vertices in the scene. We .rst loop over all pairs of edges EE 
to treat the special case of coplanar edges. We also collect any other edges which are coplanar to this 
pair; We discuss details of this case later. At the same time we .nd any potential EEE swaths. We then 
loop over all V and E pairs. For each potential swath, EEE or EV , we examine the extremal stabbing lines 
shared by each generator, E or V . If there is a shared pair of ESL s, we create a candidate swath, which 
is then tested for visibility. We need to validate the candidate swath, that is treat visibility with 
respect to the other elements of the scene. In the catalog ap­proach [9], this step is not necessary, 
since it is included in the local connectivity information. An example of a swath which must be discarded 
is shown in Fig. 9. Since all ESL s have been computed, there cannot be a visibility discontinuity along 
the swath, otherwise it would be represented by an ESL. Thus, to validate the swath, it suf.ces to sample 
visibility by a line in the middle of the line set, that is the line through the point in the middle 
of the edge portion concerned by the swath, which we call the midline. The midline is then cast in the 
same manner as an ESL into the scene, using the acceleration structures. The swath is valid if the midline 
is not blocked before reaching the receiver. A special case occurs for coplanar edges, as was pointed 
out in [13]. This special case corresponds to a line set of dimension 2, as opposed to other swaths (EEE 
and EV ) which are of dimension 1, since they only have one degree of freedom (along the E in an EV for 
example). We .rst collect all the ESL s contained in the plane, and then .nd the extremal lines, in the 
sense of the emitter­receiver pair, shown in red in Fig. 10. We then sort the ESL s across the generating 
emitter edge, and perform the visibility test for each Figure 9: Swath validation. (a) The swath ev is 
valid since the mid­line does not touch any other object. (b) The swath e2v is invalid, since the midline 
is blocked by the face f , and the receiver has not been encountered.  individual swath separately. 
For the shadow application, we create a line segment on the receiver with the boundary ESL s, we then 
calculate the intersections of all other ESL s, and insert the shadow mesh segments sorted from the start 
to the end boundary ESL s. 4.3.1 Swath Connectivity Consistency To demonstrate consistency, we adopt 
the approach of the visibility skeleton [9], in which ESL s are the nodes of a graph, connected by the 
swaths which are arcs. We want to demonstrate that if we merge a certain number of nodes together, due 
to our e approach, the graph remains consistent. By consistent, we mean that all arcs which could potentially 
exist in the new con.guration are preserved. Consider the example shown in Fig. 11 (left). If we increase 
e suf.ciently, l1 and l2 will merge, Fig. 11 (right). Once the merge has taken place, the graph becomes 
as shown in Fig. 11, upper right. In practice, all generators of l2 are attached to l1, assuming that 
l1 was created .rst. We refer to the new merged node as lm. We need to show that all arcs which were 
connected to all the nodes before merging are taken into account. Arcs which connected the two merged 
nodes no longer exist. We need to show that arcs originating at another node and arriving at one of the 
nodes merged into lm, will now be connected to lm. This is simple to show, since the generator set of 
the merged node lm is the union of the generator sets of all constituent nodes (l1 and l2). Consider 
any given external node le (e.g., l3) which was linked to one of the constituent nodes before the merge. 
The merged node lm thus contains the necessary generators and we are able to create an arc between le 
and lm, when looping over generators for swath enumeration. When creating swaths between merged nodes, 
an edge is nec­essarily shared between them. This edge is used for the midline computation, thus guaranteeing 
a consistent result, since it will re­.ect the original structure of the shadow boundary.   5 Robust 
Blocker Predicates for e Visibility The use of fat lines, or an e approach, implies that all geomet­ric 
computation is performed up to a certain precision. Once the threshold is chosen some elements may be 
of size smaller than the given e. For instance, consider the example shown in Figure 12(a) where line 
l interacts with many small faces. Since the faces are smaller than the e chosen, they should not be 
considered individu­ally. (b) The multi-face does not block the fat line, since it is not con­tained 
in the multi-face contour. In particular, we need a robust way of determining whether the line is blocked 
by multiple faces. For this, we develop two solutions: the multi-face structure, which assumes that the 
scenes contain faces with connectivity in­formation; and the blocker-fan, which treats multiple interactions 
without connectivity information, and handles surface contact. If the original model does not contain 
connectivity information we reconstruct it as much as possible, by .nding vertices at the same position 
and determining faces sharing an edge. The blocker­fan is used for all remaining cases, due either to 
numerical impreci­sion or touching faces. 5.1 The Multi-face Structure If an ESL has multiple interactions 
with a face (e.g., Fig. 5(e) or Fig. 12(a)), we apply the multi-face approach. The multi-face is a data 
structure containing the faces interacting with the fat line, and the contour of these faces used for 
the blocker test. The idea of the multi-face is related to the application of face-clustering to Figure 
13: Blocker-fan: (Left) Perspective view of the unconnected faces encountered by the line. (Middle) The 
slices, on a plane per­pendicular to the line. The red lines show the angular slices. (Right) View of 
the slice along the line direction to verify depth overlap. Upper row: The line is not blocked, both 
due to depth and to slice coverage. Middle row: The line is blocked, since it satis.es both criteria. 
Lower row: Special case of surface contact. The valid region is de.ned by the surfaces in contact. visibility 
presented by Leblanc and Poulin [15], but is developed and used in a very different context. For each 
surrounding face which is touched by the line, in the e sense (i.e., distance less than e), we add the 
face to the multi-face. All connected faces touched by the line are added if the edge is not a silhouette 
with respect to the line. Once the set is complete, we identify the boundary edges of this set. To determine 
if the multi-face blocks a given fat line, we check if the projection of the boundary edges onto a plane 
orthogonal to the line is a complete loop around the line. If it is, then the multi­face is a blocker; 
Otherwise, it is not. In addition, the line must be entirely contained in the loop in the e sense (see 
Fig. 12). This test is performed by traversing the neighboring faces to the line, adjacent to the face(s) 
interacting with the line. 5.2 The Blocker-fan structure For unconnected polygons we use a different 
approach. We collect a fan of independent polygons encountered along the line of sight of the fat ray, 
as it is propagated through the scene. The elements encountered are either edges or vertices, and they 
form slices with respect to the shaft, as shown in red in Figure (13, middle column). We also consider 
an e depth for each slice, in the direction of the line. The depth is taken around the point of intersection 
of the edge or the vertex with the line. As with the multi-face, we project the faces onto a perpendicular 
plane. For each face, we take the edges outside the fat line, and create angular slices (see Fig. 13). 
If the slices collected cover a whole section of the shaft, and they overlap in depth, then we have a 
blocker. Note that, as with fat-ray casting (see Sec. 4.2.1), we visit elements in order of distance 
on the line. The blocker fan also handles surface contact. In particular for touching objects and a line 
passing between them, we perform the same slice operation (see Fig. 13, lower row), and verify that the 
slices overlap in depth. Note that the blocker fan works to­gether with fat ray casting, so that the 
routines must be called by the casting process.  6 Implementation and Results We have implemented a 
system based on the algorithms presented above. In addition to the methods described in the previous 
sec­tions, we compute intersections between polygons. Instead of re­facetizing the input polygons along 
these discontinuities, we con­sider them as special generators and blockers, rather than true polygonal 
elements. They are thus attached to the original input geometry. We have also implemented an octree acceleration 
structure for the hourglass formed by two edges when looping over generators as described in [9]. All 
the results and tests we present have been run on scenes con­taining unmodi.ed objects we have either 
found on the internet or which are used in real applications. All timings are on a Pentium III 1 Ghz 
PC under Linux, with a GNU C++ implementation.  The example scenes we use are a Dandelion (Dand) (Fig. 
1), a Cactus (Fig. 15), Vizzy the skeleton (Fig. 16), the Agave plant (Fig. 19 right), the Big scene 
(Fig. 14), which is the largest scene tested, and a very degenerate aligned cube scene (Fig. 17). For 
soft shadows we have also used the Tree of Big , containing 33,000 input polygons (Fig. 19, middle). 
6.1 Shadow Computation To compute shadow boundaries for directional light sources we only need to enumerate 
ESL s in the direction of the light source, and we only have planar swaths. We use all algorithms developed; 
that is the general de.nition of ESL, e methods, multiface, blocker fan and intersecting polygons. Results 
for directional light sources are shown in Fig. 14 16. We have also computed a solution for an extremely 
degenerate model, which is a cube of cubes, in which everything is aligned, shown in Figure 17. This 
computation took 20 minutes. To compute soft shadows, we compute the ESL s and the swaths for each such 
emitter-receiver pair, as described previously. Note that we only compute visibility events with one 
generator on the source; in the full discontinuity mesh [6, 21] other events also exist, but we choose 
to ignore them. We made this choice for computa­tional ef.ciency and because the impact of these events 
on shadows is usually minimal. The algorithm could easily be adapted to com­pute them all, at the cost 
of an expensive additional search for ESL s within the shaft.  To create images with shadows, we insert 
all discontinuity lines into a Constrained Delaunay Triangulation (CDT), and then sample the light source. 
Nonetheless, standard constrained Delaunay trian­gulation packages are numerically unrobust. For soft 
shadows, the CDT packages we tried failed for very complex models. We show however the discontinuity 
edges for an area light source computed by our algorithm, without actually computing a CDT and the result­ing 
shadows. We computed the discontinuity edges on all objects for the Vizzy and Tree models. Due to memory 
limitations of our current implementation, we restricted the computation to the .oor only for Agave. 
These results are shown in Figure 19. For the rose (1,700 polygons) and the bunny (69,000 polygons) 
models, the CDT was able to produce a soft shadow mesh, Fig. 18. Scene/e Vizzy/1e .4 Cactus/1e .4 Dand/1e 
.4 Agave/1e .4 Big1e .4   polys 11,000 34,000 61,000 114,000 121,000 time 0m35s 1h10m 2h40m 0h04m 
0h10m smesh 31,600 428,000 287,000 153,000 329,500 ESL 4,300 45,000 75,000 20,000 19,000 dESL 535 19,600 
7,700 5,000 4,300 mface 254 17,000 1,200 7,700 3,100 Table 1: Statistics for test scenes with directional 
shadows, polys: number of input polygons, time: time to compute shadow mesh, smesh: size of shadow mesh, 
ESL: number of generic ESL s, dESL: number of degenerate ESL s, mface: number of ESL s implying multi-faces. 
 6.2 Statistics As we can see from Table 1, we can treat models of signi.cant complexity (from 11,000 
to 121,000 input polygons), and the re­sulting shadow meshes multiply the polygon count by 2-5 times 
for a directional source; this a reasonable overhead for high-quality shadows. The Cactus is an exception 
where we have a factor of 13 increase. For soft shadows, the shadow mesh for the rose is 49,500 polygons 
and required 2h, while for the bunny it is about 95,000 polygons and required 3h; the respective times 
for computing the discontinuity edges only were 8m and 1h46m. Our soft shadow computation could be signi.cantly 
optimized by using a hierarchi­cal data structure on the scene graph and using more appropriate acceleration 
structures. For the discontinuity edges only, the times are 1h43m for Vizzy, 17h30m for Tree and 2h40m 
for Agave (.oor only). Other than the unoptimized implementation, the reason for the high computational 
cost for Tree is also the large number of E4 events (91,000) compared for example to Vizzy (446). It 
is also in­teresting to note the number of swaths, 464,000 for Tree compared to 34,000 for Vizzy. e 
.3 8e .4 e Agave 3e .3 Vizzy e .2 Dand 2e .3     polys 114,000 114,000 11,000 11,000 61,000 time 
0h15m 0h06m 1m45s 0m39s 1h48m smesh 139,000 146,000 22,600 29,600 221,000 ESL 8700 15,300 1,250 3,500 
55,000 dESL 7400 6,000 1,800 969 19,400 mface 7600 5,000 2,179 645 7,700 Table 2: Additional statistics 
for test scenes for varying e. It is interesting to note the much higher number of ESL s for the Cactus 
which has multiple interactions between spikes, compared to the Vizzy model which is quite sparse and 
does not contain mul­tiple interactions. The percentage of degenerate ESL s varies, but is always more 
than 10%, and in some cases the number of degen­erate ESL s is at the same level as the non-degenerate 
lines, even for small e (see Table 2) which underlines the importance of our approach. Similarly, the 
number of multi-face operations on the various models shows that the systematic treatment we propose 
is indispensable. The running times do not always decrease with the increase in e, due to the potential 
additional complexity of certain steps of our algorithm such as the blocker-fan computation.  7 Discussion 
and Conclusion We have presented a novel, systematic approach to robust shadow boundary computation on 
real-world models. Our method .rst re­de.nes visibility events in a generalized framework, thus taking 
into account typical degenerate con.gurations, notably collinear and coplanar scene features. We then 
develop an algorithm to ro­bustly compute generalized extremal stabbing lines using e meth­ods, and a 
procedural approach to connect shadow boundaries. We demonstrated that our approach is consistent both 
in terms of the usage of e and in preservation of shadow boundary connectivity. Finally, we present robust 
blocker predicates for e computations, both for connected models, and models which do not have con­nectivity 
information. We also handle intersecting polygons. Our results show that we can robustly compute both 
directional and soft shadow boundaries on complex models.  Our method greatly improves the usability 
of analytic shadow al­gorithms for real scenes. We consider the idea of consistent feature merging to 
be of particular interest, since it may lead to an interest­ing development of hierarchical visibility 
methods. Currently, the multi-face approach requires connectivity information. If connec­tivity information 
is missing, interesting problems arise, since, for large e values we may wish to merge small features 
which are rel­atively far apart, such as the bones of the Vizzy model Fig. 16. We .nd this line of research 
interesting, and inevitably dependent on application requirements in terms of output. Evidently, our 
approach is limited by the inherent computational complexity of visibility events. It is known that the 
worst-case com­putational complexity of the full discontinuity mesh is O.n4 .for the E4 ESL s, with n 
the number of objects/edges, multiplied by mesh construction cost which can be O.n2 .on each object. 
In cases where the number of E4 events is large, such as the Tree example, this effect becomes signi.cant, 
although the worst case bounds are still very pessimistic. In our approach ESL enumeration and elim­ination 
are brute-force, but cheap, while the more expensive ESL validation step is output dependent. Developing 
optimal algorithms is an interesting research direction, and should follow the spirit of sweep approaches 
(e.g., [21]). As with any robust algorithm, it is interesting to discuss pos­sible cases of failure. 
The use of e predicates means that we will never block an ESL which should not be blocked. For the same 
rea­son however, we may enumerate and validate an ESL which should have been blocked. This is not problematic 
since no swath will be connected to this ESL, and therefore the output will remain consis­tent. In its 
current form, we do not guarantee topological consis­tency of shadows for very large e values; shadows 
simply disappear after a certain point. More sophisticated application-dependent ap­proaches could be 
developed to guide the way simpli.ed swaths are created. Our approach could be directly applied to the 
global illumination method of [10], since all operations are performed in that method on a link basis, 
which is exactly equivalent to the emitter-receiver pair we use here.  Acknowledgments Thanks to P. 
Poulin for reading an early version and M. Stamminger for his many comments and for making the video. 
This work was partially supported by the INRIA Action de Recherche Coop´erative (ARC Vis3D) on 3D Visibility. 
The initial ideas leading to this work originated in F. Durand s Ph.D. thesis. We would like to thank 
him for the many fruitful discussions and exchanges. References [1] M. Agrawala, R. Ramamoorthi, A. 
Heirich, and L. Moll. Ef.cient image-based methods for rendering soft shadows. In ACM SIGGRAPH 2000, 
Annual Confer­ence Series, pages 375 384, July 2000. [2] K. Bala, J. Dorsey, and S. Teller. Radiance 
interpolants for accelerated bounded­error ray tracing. ACM Transactions on Graphics, 18(3):213 256, 
July 1999. [3] A. T. Campbell, III and D. S. Fussell. Adaptive mesh generation for global diffuse illumination. 
Computer Graphics (Proc. SIGGRAPH 90), 24:155 164, August 1990. [4] N. Chin and S. Feiner. Fast object-precision 
shadow generation for areal light sources using BSP trees. In Computer Graphics (1992 Symposium on Interactive 
3D Graphics), volume 25, pages 21 30, March 1992. [5] F. C. Crow. Shadow algorithms for computer graphics. 
Computer Graphics (Proc. SIGGRAPH 77), 11(2):242 248, July 1977. [6] G. Drettakis and E. Fiume. A fast 
shadow algorithm for area light sources using back projection. In ACM SIGGRAPH 94, Annual Conference 
Series, pages 223 230, July 1994. [7] T. Duff. Interval arithmetic and recursive subdivision for implicit 
functions and constructive solid geometry. Computer Graphics (Proc. SIGGRAPH 92), 26(2):131 138, July 
1992. [8] F. Durand. 3D Visibility: analytical study and applications. PhD thesis, Univer­sit´e Joseph 
Fourier, Grenoble I, July 1999. http://www-imagis.imag.fr. [9] F. Durand, G. Drettakis, and C. Puech. 
The Visibility Skeleton: A Powerful and Ef.cient Multi-Purpose Global Visibility Tool. In ACM SIGGRAPH 
97 (Los Angeles, CA), Annual Conference Series, August 1997. [10] F. Durand, G. Drettakis, and C. Puech. 
Fast and accurate hierarchical radiosity using global visibility. ACM Trans. on Graphics, 18(2):128 170, 
Apr 1999. [11] Z. Gigus and J. Malik. Computing the aspect graph for the line drawings of poly­hedral 
objects. IEEE Trans. Pattern Analysis and Machine Intelligence, 12(2), February 1990. [12] E. A. Haines. 
Shaft culling for ef.cient ray-traced radiosity. In Photorealistic Rendering in Comp. Graphics, pages 
122 138. Springer Verlag, 1993. Proc. 2nd EG Workshop on Rendering (Barcelona, 1991). [13] P. Heckbert. 
Discontinuity meshing for radiosity. Proc. Third Eurographics Workshop on Rendering, Bristol, pages 203 
226, May 1992. [14] J. J. Koenderink and A. J. van Doorn. The internal representation of solid shape 
with respect to vision. Biological Cybernetics, 32(4):211 216, 1979. [15] L. Leblanc and P. Poulin. Guaranteed 
occlusion and visibility in cluster hierar­chical radiosity. In Rendering Techniques 2000, (Proc. 11th 
Eurographics Work­shop on Rendering 2000), pages 89 100, June 2000. [16] D. Lischinski, F. Tampieri, 
and D. P. Greenberg. Discontinuity meshing for ac­curate radiosity. IEEE CGA, 12(6):25 39, November 1992. 
[17] nvidia. webpage. http://developer.nvidia.com/ view.asp?IO=cedec stencil. [18] D. Salesin, L. Guibas, 
and J. Stol.. Epsilon geometry: Building robust algo­rithms from imprecise computations. In Annual Symposium 
on Computational Geometry, 1989. Saarbrucken, West Germany. [19] J. M. Snyder. Interval analysis for 
computer graphics. Computer Graphics (Proc. SIGGRAPH 92), 26(2):121 130, July 1992. [20] C. Soler and 
F. X. Sillion. Fast calculation of soft shadow textures using convo­lution. In ACM SIGGRAPH 98, Annual 
Conference Series, pages 321 332, Jul 1998. [21] A. J. Stewart and S. Ghali. Fast computation of shadow 
boundaries using spatial coherence and backprojections. In ACM SIGGRAPH 94, Annual Conference Series, 
pages 231 238, July 1994. [22] S. Teller. Visibility Computations in Densely Occluded Polyhedral Environ­ments. 
PhD thesis, UC Berkeley, 1992. [23] S. J. Teller. Computing the antipenumbra of an area light source. 
Computer Graphics (Proc. SIGGRAPH 92), 26(4):139 148, July 1992. [24] K. Weiler and K. Atherton. Hidden 
surface removal using polygon area sorting. Computer Graphics (Proc. SIGGRAPH 77), 11(2):214 222, July 
1977. [25] L. Williams. Casting curved shadows on curved surfaces. Computer Graphics (Proc. SIGGRAPH 
78), 12(3):270 274, August 1978.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566619</article_id>
		<sort_key>576</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A rapid hierarchical rendering technique for translucent materials]]></title>
		<page_from>576</page_from>
		<page_to>581</page_to>
		<doi_number>10.1145/566570.566619</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566619</url>
		<abstract>
			<par><![CDATA[This paper introduces an efficient two-pass rendering technique for translucent materials. We decouple the computation of irradianceat the surface from the evaluation of scattering inside the material.This is done by splitting the evaluation into two passes, where the first pass consists of computing the irradiance at selected points on the surface. The second pass uses a rapid hierarchical integration technique to evaluate a diffusion approximation based on the irradiance samples. This approach is substantially faster than previous methods for rendering translucent materials, and it has the advantage that it integrates seamlessly with both scanline rendering and global illumination methods. We show several images and animations from our implementation that demonstrate that the approach is both fast and robust, making it suitable for rendering translucent materials in production.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[BSSRDF]]></kw>
			<kw><![CDATA[diffusion theory]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[light transport]]></kw>
			<kw><![CDATA[realistic image synthesis]]></kw>
			<kw><![CDATA[reflection models]]></kw>
			<kw><![CDATA[subsurface scattering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14219695</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31028592</person_id>
				<author_profile_id><![CDATA[81322489561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Juan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buhler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/Dream Works]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[APPEL, A. 1985. An efficient program for many-body simulations. SIAM Journal of Scientific Statistical Computing 6, 85-103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHANDRASEKHAR, S. 1960. Radiative Transfer. Oxford Univ. Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DEBEVEC, P., 1999. St. Peter's Basilica (www.debevec.org/probes/).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DORSEY, J., EDELMAN, A., JENSEN, H. W., LEGAKIS, J., AND PEDERSEN, H. K. 1999. Modeling and rendering of weathered stone. In Proceedings of SIGGRAPH'99, 225-234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FURUTSO, K. 1980. Diffusion equation derived from space-time transport equation. J. Opt. Soc. Am 70, 360.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GEMERT, M., JACQUES, S., STERENBORG, H., AND STAR, W. 1989. Skin optics. IEEE Trans. on Biomedical Eng. 16, 1146-1156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GROENHUIS, R. A., FERWERDA, H. A., AND BOSCH, J. J. T. 1983. Scattering and absorption of turbid materials determined from reflection measurements. 1: Theory. Applied Optics 22, 2456-2462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND KRUEGER, W. 1993. Reflection from layered surfaces due to subsurface scattering. In Computer Graphics (SlGGRAPH'93 Proceedings), 165-174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HENYEY, L., AND GREENSTEIN, J. 1941. Diffuse radiation in the galaxy. Astrophysics Journal 93, 70-83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[ISHIMARU, A. 1978. Wave Propagation and Scattering in Random Media, vol. 1. Academic Press, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W., MARSCHNER, S. R., LEVOY, M., AND HANRAHAN, P. 2001. A practical model for subsurface light transport. In Proceedings of SIGGRAPH 2001, 511-518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W. 1996. Global illumination using photon maps. In Rendering Techniques '96, Springer Wien, X. Pueyo and P. Schr&#246;der, Eds., 21-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J., ANDVAN DOORN, A. 2001. Shading in the case of translucent objects. In Proceedings of SPIE, vol. 4299, 312-320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[NICODEMUS, F. E., RICHMOND, J. C., HSIA, J. J., GINSBERG, I. W., AND LIMPERIS, T. 1977. Geometric considerations and nomenclature for reflectance. Monograph 161, National Bureau of Standards (US), Oct.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344824</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PHARR, M., AND HANRAHAN, P. 2000. Monte carlo evaluation of non-linear scattering equations for subsurface reflection. In Proceedings of SIGGRAPH 2000, 75-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[STAM, J. 1995. Multiple scattering as a diffusion process. In Eurographics Rendering Workshop 1995, Eurographics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[TURK, G. 1992. Re-tiling polygonal surfaces. In Computer Graphics (SIGGRAPH '92 Proceedings), vol. 26, 55-64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J., RUBINSTEIN, F. M., AND CLEAR, R. D. 1988. A ray tracing solution for diffuse interreflection. In Computer Graphics (SIGGRAPH '88 Proceedings), vol. 22, 85-92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>70464</ref_obj_id>
				<ref_obj_pid>70459</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WYMAN, D. R., PATTERSON, M. S., AND WILSON, B. C. 1980. Similarity relations for anisotropic scattering in monte carlo simulations of deeply penetrating neutral sparticle. J. Comp. Physics 81, 137-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Rapid Hierarchical Rendering Technique for Translucent Materials Henrik Wann Jensen Juan Buhler Stanford 
University PDI/DreamWorks Abstract This paper introduces an ef.cient two-pass rendering technique for 
translucent materials. We decouple the computation of irradiance at the surface from the evaluation of 
scattering inside the material. This is done by splitting the evaluation into two passes, where the .rst 
pass consists of computing the irradiance at selected points on the surface. The second pass uses a rapid 
hierarchical integration technique to evaluate a diffusion approximation based on the irra­diance samples. 
This approach is substantially faster than previous methods for rendering translucent materials, and 
it has the advan­tage that it integrates seamlessly with both scanline rendering and global illumination 
methods. We show several images and anima­tions from our implementation that demonstrate that the approach 
is both fast and robust, making it suitable for rendering translucent materials in production. Keywords: 
Subsurface scattering, BSSRDF, re.ection models, light transport, diffusion theory, global illumination, 
realistic image synthesis 1 Introduction Translucent materials are frequently encountered in the natural 
world. Examples include snow, plants, milk, cheese, meat, human skin, cloth, marble, and jade. The degree 
of translucency may vary, but the characteristic appearance is distinctly smooth and soft as a result 
of light scattering inside the objects, a process known as sub­surface scattering. Subsurface scattering 
diffuses the scattered light and blurs the effect of small geometric details on the surface, soft­ening 
the overall look. In addition, scattered light can pass through translucent objects; this is particularly 
noticeable when the objects are lit from behind. To render these phenomena and capture the true appearance 
of translucent materials it is therefore necessary to simulate subsurface scattering. Traditionally subsurface 
scattering has been approximated as Lambertian diffuse re.ection. This was later improved by Hanra­han 
and Krueger [1993] with an analytic term for single scattering in order to account for important directional 
effects. They also pro­posed a method for simulating subsurface scattering by tracing pho­tons through 
the material, but in the end they used a BRDF (Bidirec­tional Re.ectance Distribution Function [Nicodemus 
et al. 1977]) to represent the .nal model. A BRDF only accounts for scattering at a single point, and 
it cannot be used to simulate light transport Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 within the material between different points on the surface. 
This requires treating the material as a participating medium with a sur­face. This was done by Dorsey 
et al. [1999] who used photon map­ping to simulate subsurface scattering in weathered stone. Pharr and 
Hanrahan [2000] introduced the concept of scattering equations and demonstrated how this concept could 
be used to simulate sub­surface scattering more ef.ciently than traditional Monte Carlo ray tracing. 
More recently, Koenderink and van Doorn [2001] and Jensen et al. [2001] proposed modeling the scattering 
of light in translu­cent materials as a diffusion process. The diffusion approxima­tion works particularly 
well in highly scattering media where tra­ditional Monte Carlo ray tracing becomes very expensive [Stam 
1995]. Jensen et al. [2001] suggested a simple analytical dipole dif­fusion approximation and found this 
model to be in good agreement with measurements of light scattered from translucent materials. They used 
this approximation to formulate a complete BSSRDF (Bidirectional Scattering Surface Re.ectance Distribution 
Func­tion [Nicodemus et al. 1977]), which relates outgoing radiance at a point to incident .ux at all 
points on the surface. Finally, they evaluate the BSSRDF by sampling the incident .ux on the surface. 
The BSSRDF approximation [Jensen et al. 2001] is much faster than Monte Carlo photon tracing. However, 
since it requires sam­pling the incident .ux distribution at the surface, it is still more expensive 
to evaluate than a traditional BRDF. It is particularly ex­pensive for highly translucent materials where 
light can scatter a long distance within the material. Another dif.culty with the ap­proach is that it 
only includes internal scattering in the material due to direct illumination from the light sources. 
It is not obvious how to extend the sampling technique to include global illumination as well. In this 
paper we introduce a fast and general two-pass rendering technique for translucent materials. Our approach 
is based on two key ideas. The .rst idea is to decouple of the computation of the incident illumination 
from the evaluation of the BSSRDF by using a two-pass approach. In the .rst pass, we compute the irradiance 
at selected points on the surface, and in the second pass we evaluate a diffusion approximation using 
the pre-computed irradiance sam­ples. The second idea is to use a rapid hierarchical evaluation of the 
diffusion approximation using the pre-computed irradiance sam­ples. This approach is substantially faster 
than directly sampling the BSSRDF since it only evaluates the incident illumination once at a given surface 
location, and it is particularly ef.cient for highly translucent materials where sampling the BSSRDF 
is costly. To evaluate the irradiance, we can use standard rendering techniques including scanline rendering 
and global illumination methods. This means that we can compute the effects of indirect illumination 
on translucent materials. Furthermore, our results do not suffer from any high-frequency Monte Carlo 
sampling noise since the hierar­chical evaluation is deterministic. This is a great advantage for ani­mations 
where this type of noise is particularly noticeable. Another contribution of this paper is a reformulation 
of the scat­tering parameters for translucent materials. We show how the in­trinsic scattering properties 
of translucent materials can be com­puted from two intuitive parameters: a diffuse re.ectance and an 
average scattering distance. Finally, we show several results from our implementation of the method in 
a scanline renderer as well as a Monte Carlo ray tracer. Our results indicate that the hierarchical evaluation 
technique is fast and robust, and capable of rendering images and animations of translucent objects in 
complex lighting environments.  2 Light Diffusion in Translucent Materials The scattering of light within 
a medium is described by the radiative transport equation [Chandrasekhar 1960]: (.w·V)L(x, w.)= -stL(x, 
w.)+ ssLi(x, w.)+ s(x, w.). (1) Here, L is the radiance, s is a source term, ss is the scattering coef.cient, 
sa is the absorption coef.cient, st is de.ned as sa +ss, and Li is the in-scattered radiance: ''' Li(x, 
w.)=p(w., w.)L(x, w.)dw.. (2) 4p The phase function, p, speci.es the spherical distribution of the h 
scattered light. It is normalized, p(.,ww.')dw.' =1, and we 4p assume it only depends on the cosine of 
the scattering angle, p(.,ww.')= p(.w· .w'). The mean cosine, g, of the scattering an­gle is: g =p(w.'. 
· w')dw'. ., w)(w..(3) 4p The value of g . [-1, 1] indicates the type of scattering in the medium. g 
=0 is isotropic scattering, g< 0 is backwards scat­tering and g> 0 is forward scattering. Most translucent 
materials are strongly forward scattering with g> 0.7 (skin for example has 0.7 <g< 0.9 [Gemert et al. 
1989]). Such strongly peaked phase functions are costly to simulate in media with multiple scat­tering 
since the probability of sampling in the direction of the light sources will be low in most situations. 
The dif.culty of sampling further increases with the distance to the light sources. In this case we can 
bene.t from a powerful technique known as the similarity of moments [Wyman et al. 1980], which allows 
us to change the scattering properties of the medium without signi.cantly in.uenc­ing the actual distribution 
of light. Speci.cally, we can modify the medium to have isotropic scattering (g =0) by changing the scat­tering 
coef.cient to ss ' = (1 - g)ss, (4) where ss'is the reduced scattering coef.cient. The absorption co­ef.cient 
remains unchanged, and we get the reduced extinction co­ef.cient st'= ss'+ sa. Equation 1 is a .ve-dimensional 
integro-differential equation, and even in media with isotropic scattering it is in most cases dif­.cult 
to solve. One approach is to expand radiance into a trun­cated series of spherical harmonics. For this 
purpose we divide the radiance into two components: the unscattered radiance, Lu, and the scattered (diffuse) 
radiance, Ld. The unscattered radi­ance is reduced as a function of the distance traveled through the 
medium [Ishimaru 1978]: -s .x Lu(x +.x, w.)= e tLu(x, w.). (5) The average distance at which the light 
is scattered, the mean-free path, is eu =1/st'. The diffusion approximation uses the .rst four terms 
of the spherical harmonic expansion to represent Ld: 3 Ld(x, w.) Ft(x)+ E(x) · w(6)pw.. 4 The 0th-order 
spherical harmonic, the radiant .uence, Ft, is h Ft(x)= Ld(x, w.')dw.', and the 3 terms of the 1st­ 4p 
ww order spherical harmonic, the vector irradiance, E, is E = h Ld(x, w.').w'dw.'. Note that Ld cannot 
be purely diffuse as this 4p would result in zero .ux within the medium. Instead Ld is approx­imated 
as being mostly diffuse, but with a preferential direction (as indicated by Ew) to the overall .ow of 
the .ux. The diffusion approximation is particularly effective in highly scattering media at some distance 
from the light sources as well as in regions with rapidly changing scattering properties. This is due 
to the natural smoothing resulting from multiple scatter­ing [Stam 1995]. More precisely, the diffusion 
approximation has been shown [Furutso 1980] to be accurate when sa/st « 1 - g 2 . Applying the diffusion 
approximation (Equation 6) to the radia­tive transport equation (Equation 1) yields the diffusion equation 
(the details of the derivation can be found in [Ishimaru 1978]): 11 V2Ft(x)= saFt(x) - S0(x)+ ' V· Sw1(x). 
(7)3st ' st Here, S0 and S1 represents the 0th-and the 1st-order spherical har­monics expansion of the 
source term, similar to the expansion for diffuse radiance. The diffusion equation can be solved analytically 
for special cases [Ishimaru 1978], or by using a multigrid approach [Stam 1995]. In the case of translucent 
materials, we are interested in the outgoing radiance at the material surface as a function of the incoming 
radiance. Jensen et al. [2001] use a dipole diffusion ap­proximation for a point source in a semi-in.nite 
medium. The point source is an approximation of an incoming beam of light for which it is assumed that 
all light scatters at a depth of one mean-free path below the surface. The dipole diffusion approximation 
results in the following expression for the radiant exitance, Mo, at surface location xo due to incident 
.ux, Fi(xi), at xi: a'-strdr -strdv ee dMo(xo)= dFi(xi) C1 + C2 , (8) d2 d2 4prv where 11 C1 = zrstr 
+ and C2 = zvstr + . (9)drdv ' s' Here, a= s/st'is the reduced albedo, str =v 3sast'is the effective 
transport extinction coef.cient, dr = r2 + zr 2 is the v distance to the real light source, dv = r2 + 
zv 2 is the distance to the virtual source, r = ||xo - xi|| is the distance from xo to the point of illumination, 
and zr = eu and zv = eu(1 + 4/3A) are the distance from the the dipole lights to the surface (shown in 
Figure 2). Finally, the boundary condition for mismatched in­terfaces is taken into account by the A 
term which is computed as A = (1+ Fdr)/(1 - Fdr), where the diffuse Fresnel term, Fdr is approximated 
from the relative index of refraction . by [Groenhuis et al. 1983]: 1.440 0.710 Fdr = - + +0.668 + 0.0636.. 
(10) .2 . In addition to Equation 8 the BSSRDF includes a single scatter­ing term (see [Jensen et al. 
2001] for the details). 2.1 The Importance of Multiple Scattering The diffuse term is the most costly 
to sample for translucent materi­als since it depends on lighting from a large fraction of the material 
surface. We can approximate the average distance, ed =1/str,  Figure 1: These graphs show the effect 
of increasing the scattering albedo of the material. The left graph shows the average scatter­ing distance 
for diffuse radiance divided by the mean-free path for single scattering (for g =0.9) as predicted by 
Equation 11 and es­timated using a Monte Carlo photon simulation. The graph on the right shows the fraction 
of the radiant exitance that is due to mul­tiple scattering (estimated with a Monte Carlo photon simulation). 
The important thing to notice in the two graphs is that the diffuse radiance scatters much further, and 
that it becomes increasingly im­portant as the albedo gets closer to one. along the surface that the 
diffused radiance scatters by assuming that the exponential term dominates in Equation 8. By dividing 
this distance with the mean-free path, es =1/st, of single-scattered light, we can estimate the relative 
scattered distance of the two within the medium: ed st 1 == . (11) es str 3(1 - a)(1 - ga) Note how the 
ratio depends only on the albedo, a, and the scat­tering anisotropy, g. Figure 1 shows a plot of this 
equation and a comparison with a Monte Carlo photon simulation. For the pho­ton simulation, we traced 
photons from random directions towards a translucent material and recorded the average distance at which 
the photons left the surface again after scattering inside the mate­rial. This distance divided by es 
is shown in the graph. For the simulation we used the Henyey-Greenstein phase function [Henyey and Greenstein 
1941] and the photons are scattered using the ap­proach described by Hanrahan and Krueger [1993]. Despite 
sev­eral assumptions about the average scattering distance, it can be seen that the predictions of Equation 
11 are surprisingly accurate. For both simulations the ratio rapidly increases as the albedo ap­proaches 
one as a consequence of the increasing number of scat­tering events. From the measurements in [Jensen 
et al. 2001] we can see that all of the materials have an albedo close to one. As an example, red wavelengths 
in skim milk (assuming g =0.9) have a scattering albedo of a 0.9998, which gives a ratio ed/es 129. 
This means that the average distance traveled by diffuse radiance is 129 times larger than the average 
distance traveled by unscattered radiance. In effect this means that single scattering is substantially 
more localized than diffuse scattering. The importance of multiple scattering increases with the albedo 
of the material. To further investigate how important multiple scat­tered light is for translucent materials, 
we performed another Monte Carlo photon simulation. In this simulation we traced photons from random 
directions towards the surface scattering medium. At the surface we recorded the radiant exitance from 
the photons that scat­tered in the medium. We used an index of refraction of 1.3 for the medium (the 
results are very similar for other values). Two impor­tant parameters for the medium are the scattering 
anisotropy and the scattering albedo. The right graph in Figure 1 shows the fraction of the radiant exitance 
from the material due to multiple scattered light as a function of the albedo. Note, how the fraction 
gets close to 1.0 for the forward scattering material, and close to 0.9 for a material with isotropic 
scattering.  3 A Two-Pass Technique for Evaluating the Diffusion Approximation As shown in the previous 
section, the radiant exitance from highly scattering translucent materials is dominated by photons that 
have scattered multiple times inside the material. Jensen et al. [2001] compute the contribution from 
multiple scattering by sampling the irradiance at the material surface and evaluating the diffusion ap­proximation 
 in effect convolving the re.ectance pro.le predicted by the diffusion approximation with the incident 
illumination. Even though the diffusion approximation is a very effective way of ap­proximating multiple 
scattering, this sampling technique becomes expensive for highly translucent materials. The reason for 
this is that the sampled surface area grows and needs more samples as the material becomes more translucent. 
The key idea for making this process faster is to decouple the computation of irradiance from the evaluation 
of the diffusion ap­proximation. This makes it possible to reuse irradiance samples for different evaluations 
of the diffusion equation. For this pur­pose, we use a two-pass approach in which the .rst pass consists 
of computing the irradiance at selected points on the surface, and the second pass is evaluating the 
diffusion approximation using the precomputed irradiance values. For the second pass we exploit the decreasing 
importance of distant samples and use a rapid hierarchi­cal integration technique. Pass 1: Sampling the 
Irradiance To obtain the sample locations on the surface of a piece of geometry we use Turk s point repulsion 
algorithm [Turk 1992], which pro­duces a uniform sampling of points on a polygon mesh. We do not change 
(retile) our mesh as we only need the point locations. To en­sure an accurate evaluation of the diffusion 
approximation we need enough points to account for several factors including the geometry, the variation 
in the lighting, the scattering properties of the material, and the integration technique. We use the 
mean-free path, eu, as the maximum distance between the points on the surface. The approxi­mate number 
of points that we use for a given object then becomes A/(pe2 ), where A is the surface area of the object. 
This is a con­ u servative estimate, since anything below the mean-free path will be blurred by multiple 
scattering. However, the sample density should not be much lower since this will result in low-frequency 
noise in the reconstruction of the diffusion approximation. Note that our re­construction does not require 
a uniform sampling since we weight each sample point by the area associated with the point. It would 
be possible to use other approaches that sample more densely around discontinuities in the irradiance 
or the geometry. With each sample point we store the location, the area associ­ated with the point (in 
the case of uniform sampling, this is sim­ply the surface area divided by the number of points), and 
a com­puted irradiance estimate. Since the irradiance is computed at a surface location we can use standard 
rendering techniques includ­ing methods that account for global illumination (such as photon mapping 
[Jensen 1996] and irradiance caching [Ward et al. 1988]). Pass 2: Evaluating the Diffusion Approximation 
The diffusion approximation can be evaluated directly (using Equa­tion 8) by summing the contribution 
from all the irradiance samples. However, this approach is costly since most objects have several thousand 
irradiance samples. Another, strategy would be to only consider nearby important points. This approach 
would work, but it could potentially leave out important illumination, and for accurate evaluations it 
would still need hundreds of irradiance sam­ples (e.g. our sampling produces roughly 300 samples within 
a disc with a radius of 10 mean-free paths). Instead we use a hierarchi­cal evaluation technique which 
takes into account the contribution from all irradiance samples by clustering distant samples to make 
this evaluation fast. The exponential shaped fall-off in the diffusion approximation makes the hierarchical 
approach very ef.cient. The concept is similar to the hierarchical approaches used for N-body problems 
[Appel 1985]. Several different hierarchical structures can be used for the ir­radiance samples. We use 
an octree in our implementation. Each node in the tree stores a representation of illumination in all 
its child nodes: the total irradiance, Ev, the total area represented by the points, Av, and the average 
location (weighted by the irradiance) of w the points, Pv. To increase ef.ciency we allow up to 8 irradiance 
samples in a leaf voxel. The total radiant exitance .ux at a location, x, is computed by traversing the 
octree from the root. For each voxel we check if it is small enough or if it is a leaf node that it can 
be used directly; otherwise all the child nodes of the voxel are evaluated recursively. If the point 
x is inside the voxel then we always evaluate the child nodes. For all other voxels we need an error 
criterion that speci­.es the desired accuracy of the hierarchical evaluation. One option would be to 
compute the potential contribution from the voxel and decide based on this estimate if the voxel should 
be subdivided. Unfortunately, this is not trivial and simply using the center of the points in the voxel 
is not a good approximation for nearby large voxels. Instead we use a much simpler criterion that is 
both fast to evaluate and that works well in practice. We base our criteria for subdividing a voxel on 
an approximation of the maximum solid angle, .., spanned by the points in the voxel: Av .. = . (12) ||wx 
- Pwv||2 To decide if a voxel should be subdivided we simply compare .. to a value . which controls the 
error. If .. is larger than . then the children of the voxel are evaluated; otherwise the voxel is used 
directly. Another option would be to check the solid angle of the voxel itself; however, using the area 
of the points makes the evalu­ation faster, since we can use the clustered values for large voxels with 
just a few irradiance samples (e.g. large voxels that just barely intersect the surface). The radiant 
exitance due to a voxel is evaluated using the clus­tered values for the voxel, or if it is a leaf-voxel 
by summing the contribution from each of the points in the voxel. The radiant ex­itance, Mo,p(x) at x 
from a given irradiance sample is computed using the dipole diffusion approximation dMo(x) Mo,p(x)= Fdt(x) 
EpAp, (13) a ' dFi(Pwp) where Pp is the location of the irradiance sample(s), Ep is the irradiance at 
that location, and Ap is the area of the location. w dMo(||x - Pp||2)/(a ' dFi(Pwp)) is computed using 
Equation 8. Notice that we scale each irradiance sample by the diffuse Fres­nel transmittance, Fdt =1 
- Fdr (Fdr is computed using Equa­tion 10). This is necessary when approximating the irradiance by the 
dipole source. We could have scaled the contribution from each of the sample rays used to compute the 
irradiance by the true Fres­nel transmittance, but by using the diffuse (Lambertian) assumption we can 
bene.t from fast rendering techniques for diffuse materials (e.g. caching techniques such as photon maps 
[Jensen 1996] and irradiance caching [Ward et al. 1988]). The dipole approximation for an irradiance 
sample is illustrated in Figure 2. Note that this approximation has been derived assuming a semi-in.nite 
medium. In the presence of complex geometry (e.g. curved surfaces or thin geometry) we use the same techniques 
as Jensen et al. [2001] to ensure numerical stability. The result of traversing and evaluating the voxels 
is an estimate of the total (diffuse) radiant exitance, Mo at x, which we convert into radiance, Lo: 
 Figure 2: For each point sample we use the dipole diffusion approx­imation to compute the radiant exitance. 
Ft(x, w.) Mo(x) Lo(x, w.)= . (14) Fdr(x) p We scale the contribution by the Fresnel transmittance, Ft 
to ac­count for re.ection and transmission at the surface. Since, the dif­fusion approximation already 
includes a diffuse Fresnel transmit­tance we divide by Fdr. Alternatively, we could omit the Fresnel 
terms and assume a diffuse radiance. 4 Reparameterizing the BSSRDF One dif.culty in simulating subsurface 
scattering is that it is dif.­cult to predict the resulting appearance from a given combination of absorption 
and scattering coef.cients (since their effect is highly non-linear). In this section, we will outline 
a simple technique for reparameterizing the BSSRDF into using intuitive translucency and re.ectivity 
parameters. These parameters are already present in the computations in the form of the diffuse mean 
free path ed and the diffuse re.ectance of the material, and they are suf.cient for com­puting the scattering 
and absorption properties of the material. First, using the diffuse re.ection coef.cient (see [Jensen 
et al. 2001]), we solve for the reduced albedo of the material: vv a ' - 43 A 3(1-a ) - 3(1-a ) Rd = 
1+ e e. (15) 2 This equation is not easily invertible, but it has a simple monotonic shape in the valid 
region a ' . [0 : 1], and we use a few iterations of a simple secant root .nder to compute a ' . We know 
the effective transport coef.cient, str 1/ed, and given the reduced albedo we can .nd the reduced extinction 
coef.­cient: str ' str = 3(1 - a ' ) -. st = (16) s ' t 3(1 - a') Finally, this gives us the absorption 
and the reduced scattering coef­.cients: ss ' = a ' st ' and sa = st ' - ss' . If the scattering anisotropy, 
g, is given then the real extinction and scattering coef.cients can be computed as well. 5 Results In 
this section we present several results from our implementation of the rendering technique. We have used 
two different systems to implement the model: A Monte Carlo ray tracer with support for global illumination, 
and a modi.ed a-buffer renderer used in production. Our timings were recorded on a dual 800MHz Pentium 
3 for images with a width of 1024 pixels and 4 samples per pixel. The .rst example include several renderings 
of a translucent marble teapot as shown in Figure 3. All of these images were rendered with the Monte 
Carlo ray tracer. The left column shows a comparison with the BSSRDF sampling technique by Jensen et 
al. [2001], and our hierarchical technique under the same lighting conditions (for this comparison we 
use the BRDF approximation for the single scattering term). The important thing to notice is that the 
two images are practically indistinguishable except for a small amount of sampling noise in the BSSRDF 
image. This shows that the hierarchical approach is capable of matching the output of the BSSRDF for 
a translucent material. However, the hierarchi­cal technique took just 7 seconds to render (including 
1 second to compute the irradiance values at the samples), while the BSS-RDF sampling took 18 minutes 
 a factor of 154 speedup in this case. The speedup will be even more dramatic for objects that are more 
translucent. The top image in the right column shows a glossy teapot illuminated by a high dynamic range 
environment map [De­bevec 1999]. To enhance the translucency effect we made the en­vironment black behind 
the camera. The render time for the image without glossy re.ection is 7 seconds (the rendering time including 
glossy re.ection is 40 seconds). The precomputation time for the irradiance samples (sampling the environment 
map) was roughly 1 minute. This scene would be extremely costly to render using the BSSRDF sampling approach, 
since it would involve picking a point on the light source and then sampling in the direction of the 
teapot a process where the probability of generating good samples is very low. Finally, the lower right 
image shows the 150,000 sample locations on the teapot that we used for all images. Our second example 
in Figure 4 shows the classic Cornell box global illumination scene with a translucent box. This image 
was rendered using Monte Carlo ray tracing we used photon map­ping [Jensen 1996] to speed up the computation 
of global illumi­nation. Note the light scattering through the box, and the color bleeding in the scene. 
In the precomputation of the indirect illu­mination for the irradiance samples on the translucent box 
we used a diffuse assumption in order to account for multiple re.ections between the box and the walls. 
However we do account for translu­cency when evaluating color bleeding on the wall. For scenes where 
translucency is important for the indirect illumination of the translu­cent elements (e.g. light bleeding 
through a leaf onto a white .oor which then re.ects back on the leaf) a multi-pass approach could be 
used where the indirect illumination is computed recursively for the translucent materials. The rendering 
time for the image was 55 seconds, and the precomputation of the irradiance for 20,000 points on the 
box was 21 seconds. Our third example in Figure 6 shows the lower half of a face model rendered using 
a standard skin shader (on the left) and us­ing a skin shader with support for translucency (on the right). 
This face model was built before the translucency shader was developed. It uses several textures, which 
we apply by scaling the 250,000 ir­radiance samples with .ltered texture values (the .lter support is 
equal to the area associated with each irradiance sample). This ap­proach made it possible to replace 
the previous skin shader with a translucent version. The added translucency vastly increases the realism 
of the model as it softens the appearance of the skin and nat­urally simulates effects such as the color 
bleeding around the nose. The rendering time using the standard skin shader was 16 minutes while the 
translucent skin shader took 20 minutes (including gener­ating the sample points). A signi.cant advantage 
of our approach is that it works with all the standard lights used in production such as .ll lights, 
rim lights and key lights. This natural integration of the translucency shader in the production framework 
made it a nat­ural choice for the main character in Sprout (a short animation). Translucency helps depict 
the small size of the character as shown in Figure 5. 6 Conclusion and Future Work We have presented 
an ef.cient two-pass rendering technique for translucent materials. We combine a point sampling of the 
irradi­ance on the surface with a fast hierarchical evaluation of a diffusion approximation. Our approach 
is particularly ef.cient for highly translucent materials where the BSSRDF sampling [Jensen et al. 2001] 
becomes costly, and it integrates seamlessly with both scan­line rendering and global illumination methods. 
Our results demon­strate how the technique is both fast and robust making it suitable for rendering translucent 
materials in production of computer ani­mations. Future improvements include extending the approach to 
translu­cent materials with a visible internal 3D structure. It would also be useful to investigate the 
accuracy of the dipole diffusion approxi­mation in the presence of complex geometry. Another interesting 
path to explore is interactive rendering of translucent materials. This could be done by further simplifying 
the evaluation technique so that it can be implemented directly on programmable graphics hardware. 7 
Acknowledgments Thanks to the SIGGRAPH reviewers and to Maryann Simmons, Mike Cammarano, Pat Hanrahan, 
and Marc Levoy for helpful com­ments. The .rst author was supported by NSF ITR (IIS-0085864). The second 
author was supported by PDI/DreamWorks and by the PDI R&#38;D group and thanks them for supporting the 
development and publication of these techniques.  References APPEL, A. 1985. An ef.cient program for 
many-body simulations. SIAM Journal of Scienti.c Statistical Computing 6, 85 103. CHANDRASEKHAR, S. 1960. 
Radiative Transfer. Oxford Univ. Press. DEBEVEC, P., 1999. St. Peter s Basilica (www.debevec.org/probes/). 
DORSEY, J., EDELMAN, A., JENSEN, H. W., LEGAKIS, J., AND PEDERSEN, H. K. 1999. Modeling and rendering 
of weathered stone. In Proceedings of SIG-GRAPH 99, 225 234. FURUTSO, K. 1980. Diffusion equation derived 
from space-time transport equation. J. Opt. Soc. Am 70, 360. GEMERT, M., JACQUES, S., STERENBORG, H., 
AND STAR, W. 1989. Skin optics. IEEE Trans. on Biomedical Eng. 16, 1146 1156. GROENHUIS, R. A., FERWERDA, 
H. A., AND BOSCH, J. J. T. 1983. Scattering and absorption of turbid materials determined from re.ection 
measurements. 1: Theory. Applied Optics 22, 2456 2462. HANRAHAN, P., AND KRUEGER, W. 1993. Re.ection 
from layered surfaces due to subsurface scattering. In Computer Graphics (SIGGRAPH 93 Proceedings), 165 
174. HENYEY, L., AND GREENSTEIN, J. 1941. Diffuse radiation in the galaxy. Astro­physics Journal 93, 
70 83. ISHIMARU, A. 1978. Wave Propagation and Scattering in Random Media, vol. 1. Academic Press, New 
York. JENSEN, H. W., MARSCHNER, S. R., LEVOY, M., AND HANRAHAN, P. 2001. A practical model for subsurface 
light transport. In Proceedings of SIGGRAPH 2001, 511 518. JENSEN, H. W. 1996. Global illumination using 
photon maps. In Rendering Tech­niques 96, Springer Wien, X. Pueyo and P. Schr¨ oder, Eds., 21 30. KOENDERINK, 
J., AND VAN DOORN, A. 2001. Shading in the case of translucent objects. In Proceedings of SPIE, vol. 
4299, 312 320. NICODEMUS, F. E., RICHMOND, J. C., HSIA, J. J., GINSBERG, I. W., AND LIMPERIS, T. 1977. 
Geometric considerations and nomenclature for re.ectance. Monograph 161, National Bureau of Standards 
(US), Oct. PHARR, M., AND HANRAHAN, P. 2000. Monte carlo evaluation of non-linear scat­tering equations 
for subsurface re.ection. In Proceedings of SIGGRAPH 2000, 75 84. STAM, J. 1995. Multiple scattering 
as a diffusion process. In Eurographics Rendering Workshop 1995, Eurographics. TURK, G. 1992. Re-tiling 
polygonal surfaces. In Computer Graphics (SIGGRAPH 92 Proceedings), vol. 26, 55 64. WARD, G. J., RUBINSTEIN, 
F. M., AND CLEAR, R. D. 1988. A ray tracing solution for diffuse interre.ection. In Computer Graphics 
(SIGGRAPH 88 Proceedings), vol. 22, 85 92. WYMAN, D. R., PATTERSON, M. S., AND WILSON, B. C. 1980. Similarity 
relations for anisotropic scattering in monte carlo simulations of deeply penetrating neutral particles. 
J. Comp. Physics 81, 137 150.  BSSRDF: sampled evaluation -18 minutes Illumination from a HDR environment 
 BSSRDF: hierarchical evaluation -7 seconds The sample locations on the teapot Figure 3: A translucent 
teapot. On the left we compare our hierarchical BSSRDF evaluation (bottom) to a sampled BSSRDF (top). 
The top right image shows the teapot in a HDR environment, and the bottom right shows the 150,000 sample 
points on the teapot.  Figure 6: A textured face model lit by three light sources (key, .ll, and rim). 
The left image shows the result using the skin shader that was used in the movie Shrek , and the right 
image shows the result after adding our simulation of translucency to this shader. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566620</section_id>
		<sort_key>582</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Soft things]]></section_title>
		<section_page_from>582</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP77034195</person_id>
				<author_profile_id><![CDATA[81407594555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INP Grenoble]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566621</article_id>
		<sort_key>582</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[DyRT]]></title>
		<subtitle><![CDATA[dynamic response textures for real time deformation simulation with graphics hardware]]></subtitle>
		<page_from>582</page_from>
		<page_to>585</page_to>
		<doi_number>10.1145/566570.566621</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566621</url>
		<abstract>
			<par><![CDATA[In this paper we describe how to simulate geometrically complex, interactive, physically-based, volumetric, dynamic deformation models with <i>negligible main CPU costs.</i> This is achieved using a <i><b>Dy</b>namic <b>R</b>esponse <b>T</b>exture,</i> or <b>DyRT,</b> that can be mapped onto any conventional animation as an optional rendering stage using commodity graphics hardware. The DyRT simulation process employs precomputed modal vibration models excited by rigid body motions. We present several examples, with an emphasis on bone-based character animation for interactive applications.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P69488</person_id>
				<author_profile_id><![CDATA[81100415142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[James]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P67322</person_id>
				<author_profile_id><![CDATA[81100642559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Pai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Baraff and A. Witkin. Dynamic Simulation of Non-penetrating Flexible Bodies. In Computer Graphics (SIGGRAPH 92 Conference Proceedings), pages 303-308, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Baraff and A. Witkin. Large Steps in Cloth Simulation. In SIGGRAPH 98 Conference Proceedings, pages 43-54, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808573</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Barr. Global and Local Deformations of Solid Primitives. In Computer Graphics (SIGGRAPH 84 Conference Proceedings), volume 18, pages 21-30, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Basdogan. Real-time Simulation of Dynamically Deformable Finite Element Models Using Modal Analysis and Spectral Lanczos Decomposition Methods. In Medicine Meets Virtual Reality (MMVR'2001), pages 46-52, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74358</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. E. Chadwick, D. R. Haumann, and R. E. Parent. Layered Construction of Deformable Animated Characters. In Computer Graphics (SIGGRAPH 89 Conference Proceedings), volume 23, pages 243-252, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383262</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Debunne, M. Desbrun, A. Barr, and M.-P. Cani. Dynamic real-time deformations using space and time adaptive sampling. In SIGGRAPH O1 Conference Proceedings, pages 31-36, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G. Dhondt and K. Wittig. CalculiX: A Free Software Three-Dimensional Structural Finite Element Program.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Friedmann and A. Pentland. Distributed physical simulation. In Third Eurographics Workshop on Animation and Simulation, pages 1-17, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74335</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Gourret, N. Magnenat-Thalmann, and D. Thalmann. Simulation of Object and Human Skin Deformations in a Grasping Task. In Computer Graphics (SIGGRAPH 89 Conference Proceedings), volume 23, pages 21-29, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>936163</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[D. L. James. Multiresolution Green's Function Methods for Interactive Simulation of Large-scale Elastostatic Objects and Other Physical Systems in Equilibrium. PhD thesis, Institute of Applied Mathematics, University of British Columbia, Vancouver, British Columbia, Canada, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311542</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. L. James and D. K. Pai. ARTDEFO: Accurate Real Time Deformable Objects. In SIGGRAPH 99 Conference Proceedings, pages 65-72, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Y. Lee, D. Terzopoulos, and K. Walters. Realistic Modeling for Facial Animation. In SIGGRAPH 95 Conference Proceedings, pages 55-62, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. Lehoucq, D. Sorensen, and C. Yang. ARPACK Users' Guide: Solution of large scale eigenvalue problems with implicitly restarted Arnoldi methods. Technical report, Comp. and Applied Mathematics, Rice Univ., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[E. Lindholm, M. J. Kilgard, and H. Moreton. A User-Programmable Vertex Engine. In SIGGRAPH 2001 Conference Proceedings, pages 149-158, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134085</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. Metaxas and D. Terzopoulos. Dynamic Deformation of Solid Primitives with Constraints. In Computer Graphics (SIGGRAPH 98 Conference Proceedings), volume 26, pages 309-312, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>561828</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. M. Murray, Z. Li, and S. S. Sastry. A Mathematical Introduction to Robotic Manipulation. CRC Press, Inc., 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383321</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. O'Brien, P. Cook, and G. Essl. Synthesizing Sounds from Physically Based Motion. In SIGGRAPH 01 Conference Proceedings, pages 529-536, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. F. O'Brien and J. K. Hodgins. Graphical modeling and animation of brittle fracture. In SIGGRAPH 99 Conference Proceedings, pages 111-120, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74355</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. Pentland and J. Williams. Good Vibrations: Modal Dynamics for Graphics and Animation. In Computer Graphics (SIGGRAPH 89 Conference Proceedings), volume 23, pages 215-222, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[G. Picinbono, H. Delingette, and N. Ayache. Non-linear and anisotropic elastic soft tissue models for medical simulation. In ICRA2OO1: IEEE International Conference on Robotics and Automation, Seoul Korea, May 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J. Schoberl. NETGEN - An advancing front 2D/3D-mesh generator based on abstract rules. Comput. Visual. Sci, 1:41-52, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. A. Shebana. Theory of Vibration, Volume II: Discrete and Continuous Systems. Springer-Verlag, New York, NY, first edition, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Stam. Stochastic Dynamics: Simulating the Effects of Turbulence on Flexible Structures. Computer Graphics Forum, 10(3), 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>265600</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[K. Steiglitz. A Digital Signal Processing Primer with Applications to Digital Audio and Computer Music. Addison-Wesley, New York, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and K. Fleischer. Deformable models. The Visual Computer, 4:306-331, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[K. van den Doel, P. G. Kry, and D. K. Pai. Foley Automatic: Physically-based Sound Effects for Interactive Simulations and Animations. In SIGGRAPH 01 Conference Proceedings, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258833</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Wilhelms and A. V. Gelder. Anatomically Based Modeling. In SIGGRAPH 97 Conference Proceedings, pages 173-180, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>565650</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and W. Welch. Fast Animation and Control of Nonrigid Structures. In Computer Graphics (SIGGRAPH 90 Conference Proceedings), pages 243-252, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[O. C. Zienkiewicz. The Finite Element Method. McGraw-Hill Book Company (UK) Limited, Maidenhead, Berkshire, England, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DyRT: Dynamic Response Textures for Real Time Deformation Simulation with Graphics Hardware Doug L. 
James and Dinesh K. Pai Department of Computer Science, University of British Columbia Abstract In this 
paper we describe how to simulate geometrically com­plex, interactive, physically-based, volumetric, 
dynamic de­formation models with negligible main CPU costs. This is achieved using a Dynamic Response 
Texture, or DyRT, that can be mapped onto any conventional animation as an optional rendering stage using 
commodity graphics hard­ware. The DyRT simulation process employs precomputed modal vibration models 
excited by rigid body motions. We present several examples, with an emphasis on bone-based character 
animation for interactive applications. 1 Introduction In this paper we present an e.cient rendering 
technique for simulating real time dynamic deformations for appli­cations such as character animation. 
This is achieved us­ing a Dynamic Response Texture, or DyRT, that can be mapped onto any conventional 
animation (motion capture or keyframe or rigid body dynamics simulation) as an op­tional rendering stage. 
This is because the complexity of rendering deformations using DyRT is comparable to light­ing the object. 
Therefore, every deformable object, large or small, can be rendered with realistic dynamic deformation 
responses, in real time, on commodity graphics hardware. The physical realism of DyRT is due to the use 
of precom­puted modal analyses [22] of dynamic elastic models com­puted using, e.g., the Finite Element 
Method (FEM) [29]. These systems typically have a few clearly dominant dy­namic deformation modes that 
enable us to produce con­vincing realizations on commodity graphics cards. This is achieved using vertex 
programs [14] that perform the per­vertex linear superpositions necessary to compute displace­ment and 
normal vectors. A second key component of a DyRT is the use of rigid motion transfer functions for rigid 
(bone) motion input de­pendence, so that DyRTs respond physically and are not just canned vibrations. 
This is in contrast to, e.g., several nVidia vertex program demos [14], such as warp, which, although 
extremely useful in context, have limited physical foundations. In the remainder of this paper we describe 
the foundations for and the process of applying DyRT to objects, with a Copyright &#38;#169; 2002 by 
the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all 
of this work for personal or classroom use is granted without fee provided that copies are not made or 
distributed for commercial advantage and that copies bear this notice and the full citation on the first 
page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with 
credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, 
requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., 
fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 particular 
emphasis on bone-based character animation. 1.1 Related Work Signi.cant work has been done on simulating 
dynamic de­formable objects, in areas such as human body modeling and interactive simulation. Despite 
the large amount of pioneer­ing work on deformation [25, 28, 15, 1], there continue to be exciting new 
applications [18, 17] and improvements in simulation e.ciency [2, 6]. Numerous examples of human body 
modeling exist in the literature with particular areas of interest being deforma­tions of skin and muscles 
[27, 9], faces [12], and layered models [5]. Support exists in commercial animation pack­ages, such as 
Maya, for simulating tissue dynamics. There have also been signi.cant recent developments for interactive 
dynamic tissue simulation, especially for force feedback ap­plications such as surgical simulation [6, 
20]. Despite these advances, the simulation of transient vibration responses for secondary animation 
remains largely absent from the tra­ditional character animation pipeline, and especially so in video 
games. Of particular interest for graphics hardware are data­driven deformation models based on linear 
superposition of precomputable global deformation bases [3], which in­clude space warping methods such 
as FFD. While such mod­els can provide fast simulation and constraint handling for physically-based dynamic 
[19, 28] and also static [11, 10] deformable models, we are primarily interested in their amenability 
to graphics hardware simulation [14]. For simulating free vibrations of elastic models with modest amplitudes, 
global deformation bases based on Karhunen-Loeve expansions from modal analysis provide the optimal description 
[22, 8]. First introduced to the graphics community by the pioneering work of Pentland and Williams [19, 
8], more recently they have been used for in­teractive applications involving precomputed or measured 
modal data: stochastic simulation of tree-like structures [23], force feedback [4], and contact sound 
simulation [26]. Our contribution: This is the .rst paper to show how to simulate geometrically complex, 
interactive, physically­based, volumetric, dynamic deformation models in real time with negligible main 
CPU costs. We do so with precomputed modal vibration models stored in graphics hardware mem­ory and driven 
by a handful of inputs de.ned by rigid body motion.   2 Background on Modal Vibration Models We brie.y 
summarize the necessary background on modal vibration analysis here, and refer the reader to a suitable 
text [22]. The linear elastodynamic equation for a .nite element model [29], M¨u + C .u + Ku = F, (1) 
describes the displacements u=u(t) of N nodes within a vol­ume. The displacement .eld u is expanded in 
a modal dis­placement basis u(t)= <q(t) (2) where < denotes the model s modal matrix, a matrix whose 
ith column <:i represents the ith mode shape, and q = q(t) are the corresponding modal amplitudes, i.e., 
qi is the modal amplitude of mode shape <:i. An important property is that the modal matrix < is independent 
of time, and completely characterized by values at mesh vertices. Substituting (2) into (1) and premultiplying 
by <T yields Mqq¨+ Cqq.+ Kqq = Q (3) in which Mq = <TM< = diag(mi) (4) Kq = <TK< = diag(ki) (5) Cq = 
<TC< (6) <T Q = F (7) where all of Mq and Kq are diagonal matrices, but for general damping Cq is dense. 
If we make the common assumption of proportional (Rayleigh) damping C = aM + aK = Cq = diag(ami + aki) 
then the system of ODEs are completely decoupled by the modal transformation. This allows the motions 
due to indi­vidual modes to be computed independently and combined by linear superposition. The system 
of decoupled ordinary di.erential equations may be written as 2 Qi ¨qi +2 iwiq.i + wi qi = ,i =1..n, 
(8) mi where the undamped natural frequency of vibration is . ki wi = (in radians) (9) mi and the dimensionless 
modal damping factor is () ci 1 a i = =+ awi . (10) 2miwi 2 wi We are interested in underdamped systems 
for which visible damped vibration occurs, and this corresponds to i E (0, 1). See Figure 1 for example 
mode shapes and frequencies. <:1 (W1 =1.00) <:2 (W2 " 1.12) <:3 (W3 " 1.25) <:4 (W4 " 1.44) Figure 1: 
Dominant low frequency mode shapes of the belly model represent bulk translation and rotation. RGB colors 
correspond to XYZ displacement magnitudes. Finally, for a system starting from rest at t =0 the solution 
for the ith mode due to forcing Qi(t) is t - i i(t- ) Qi(. ) qi(t)=e sin wdi(t - . ) d. (11) miwdi 0 
where the observed damped natural frequency is . wdi = wi 1 - i 2 . (12) 3 Exciting Modes with Rigid 
Motions Our goal is to produce realistic modal deformations auto­matically from a conventional bone-based 
animation speci.­cation, for instance using motion capture data or rigid body dynamics simulation. Suppose 
the motion of a rigid body, the bone, is speci.ed as a homogenous transformation ma­ () e p trix R(t)= 
, where e is a rotation matrix. We now 01 describe how to compute the correct modal forcing function 
Qi(t) for a deformable object, the .esh, attached to a bone such as depicted in Figure 2. We describe 
how to deal with joints between bones in Sec. 4.2.  Figure 2: Modeling of a thigh .nite element model 
using a skeleton and CSG operations. The velocity of rigid body is represented by its linear ve­locity 
. and angular velocity w. We can therefore view veloc­ .)T ity as a 6 × 1 twist or spatial velocity1 
vector . = wT .T . The velocity, .rj , of a material point at rj is then given by r.j =[w]rj + . =( -[rj 
] I ) ., (13) where [w] is the standard skew-symmetric matrix of the cross product w×, and I is a 3-by-3 
identity matrix. Using a simple Euler discretization, with constant time step size h, the acceleration 
of the material point at discrete time step k is 11 (k)(k)(k-1) (k)(k-1) r¨j (.rj -r.j )= ( -[rj ] I 
)(.-.). (14) hh Higher order discretizations are similar. De.ning . -[r1 ] I -[r2 ] I f =. (. (. . . 
. -[rp] I we have the acceleration of points on the body as 1 (k)(k)(k-1) ¨r f(.- .). (15) h When viewed 
relative to a coordinate frame attached to the bone, which is accelerating, the D Alembert force is2 
(k)(k)(k)(k-1) F= Mr¨j =1 Mf(.- .). (16) h This is the forcing function for the vibration in (1). The 
primary modal forcing term Qi/mi in (11) is therefore M-1(k) 1 <-1f(.(k) - .(k-1) q Q=), (17) h def (k)(k-1) 
= H(.- .). (18) 1 Here we use the traditional kinematic terminology from screw theory. We refer the reader 
to any standard mathematical treat­ment on kinematics, such as [16], for more details 2 Coriolis forces 
are negligible here and have been omitted. We call H = (1/h)<-1f the rigid motion transfer matrix. It 
maps changes in spatial velocity to modal forces that lead to modal vibrations. It can be precomputed 
in advance of a simulation and stored. In practice, the forces may be .ltered, e.g., scaled and clamped, 
to avoid extremely large excitations from abrupt motion changes or resonant forcing. Finally, we need 
to perform the time-domain convolution of (11). This can be performed e.ciently in discrete time using 
a small IIR digital .lter [24, 26]: (k)(k-1) 2(k-2) qi =2.i cos .iqi - .i qi (19) (k-1) 2[.i cos(.i + 
ßi) - .2 i cos(2.i + ßi)] Qi + 3wiwdi mi where .i =exp(- iwih), .i = wdih and ßi =arcsin i.  4 Special 
Considerations 4.1 Normal Calculation Unlike the displaced vertex positions which can be computed in 
parallel on a per-vertex basis, vertex normals are compli­cated by the requirement of neighbouring vertex 
informa­tion. Therefore DyRT objects include an approximate ver­tex normal correction obtained by linearizing 
the ith vertex s deformed normal ni about the undeformed value ni,  ni = n i +Nimqm (20) m where Nim 
is the ith vertex s normal correction for mode m. Details are given in Appendix A. While corrected normals 
can further increase visual real­ism (see Figure 3), the added cost of per-vertex memory for each mode 
s normal correction should be weighed against other vertex memory requirements. In practice, correcting 
normals only for particular modes, such as the dominant and/or torsional modes, is a fair trade-o.. 
  Undeformed Deformed without Deformed with Figure 3: Normal correction bene.ts are illustrated using 
the lowest torsional deformation mode of the thigh model: (Left) undeformed, (Middle) deformed without 
normal correction, and (Right) with normal correction computed.  4.2 Matrix Palette Skinning with DyRT 
DyRT provides minimal complications for traditional hardware character animation. Using vertex program 
hard­ware for indexed matrix palette skinning vertex programs, as in [14] (see their jester example), 
static display lists are used for each DyRT mapped object. In our examples, per-vertex data exists not 
only for vertex position, normal, color, texture coords, and 4 matrix index/weight pairs, but also for 
DyRT values: one for each mode s displacement and any normal correction. Due to current vertex memory 
con­straints, each vertex is vibrated by only one DyRT object (with multiple modes, as described in §5.1), 
but multiple layered (or blended) DyRTs could be used in the future, or at the cost of fewer modes or 
normal corrections per DyRT. In the vertex program, modal deformations are per­formed before the vertex 
blending stage, and require at most (2m + 2) extra instructions for m modes (using all normal corrections); 
in our DyRT Man example m =5 so that only 12 instructions are added and the vertex program remains fast 
(see Appendix B).  5 Process Details 5.1 Precomputation 1. Acquire articulated character geometry. 
2. For each deformable body part, e.g., thigh,  Use surface model to de.ne a closed volume to be .lled 
with elastic material.  Generate a volumetric .nite element mesh, e.g., using a tetrahedral mesh generation 
package such as NETGEN [21].  Fix the .nite element model s boundary vertices where you do not desire 
deformation, e.g., along bones and seams.  De.ne material properties such as sti.ness, com­pressibility 
and density.  Compute and save the dominant modes frequen­cies and volumetric mode shapes < using a 
modal analysis package, e.g., CalculiX [7] uses the excel­lent ARPACK eigenvalue solver [13].  Build 
an m-mode DyRT object consisting of  m modal model natural frequencies wi;  m modal shape functions 
<:1..m interpolated onto the original character geometry;  m normal perturbation maps N:1..m com­puted 
on the character geometry;  m IIR digital convolution .lters from (19);  the m-by-6 transfer matrix 
H from (18).    5.2 Runtime Computations For each animation time step, k, and each DyRT object: 1. 
Obtain the rigid bone transform and estimate the spa­tial velocity twist, .(k-1) . 2. For each mode 
i =1..m:  (k-1) Compute the modal forcing term Qi /mi using the rigid motion transfer matrix from (18). 
 Perform the time domain IIR .lter convolution of  (19) to obtain qi (k) . 3. Bind and enable appropriate 
DyRT vertex program and set vertex program constants: modal coe.cients, qi (k) , and current bone transforms 
(See Appendix B). 4. Call static display list for this body part.   6 Results Our .rst example applies 
DyRT to a character animated us­ing indexed matrix palette skinning vertex programs. The humanoid mesh 
used was exported from Curious Labs Poser and converted to 17,980 quadrilateral faces and 17,953 ver­tices. 
Following the described process, we constructed 3 DyRT objects: matching thigh models based on a 10,000 
10-node tetrahedral element .nite element model, and an abdominal model with 30,000 elements. Precomputation 
times were only a couple of minutes for each DyRT, and much larger models could be used. The .nal character 
was animated with House of Moves motion capture. In our second example, we apply DyRT to secondary tis­sue 
in a laparoscopic surgical simulation. In this setting, DyRT helps increase scene realism while allowing 
the main CPU to focus on simulating more complex tissue models in­volved in user contact interactions. 
Dynamic deformations are inherently di.cult to portray in paper format, however examples in the accompanying 
video (see Figure 4) illustrate the subtle yet signi.cant im­pact DyRT can have on scene realism. All 
examples run in real time, at approximately 60 FPS, on a PC with a GeForce3 graphics card; throughout 
the simulation the run­time cost to the main CPU is negligible. Figure 4: Examples from video: (Left) 
A jumping motion that leads to signi.cant thigh and belly vibrations; (Right) DyRT applied to tissue 
in a surgical simulation. 7 Summary and Conclusion We have illustrated the process by which DyRT can 
be used to simulate geometrically complex, volumetric, physically­based, dynamic deformation models with 
negligible main CPU costs by exploiting commodity graphics hardware. Given our results, we believe that 
DyRT-based secondary animation is an e.cient technique to increase the level of realism in modern real 
time applications. Acknowledgements: We would like to thank the reviewers for their helpful suggestions, 
and Edward M. Lichten M.D. for the texture map image in our laparoscopic surgery example. A Computation 
of Normal Correction We .rst show how to approximate the face normal for a deformed triangle. Consider 
an undeformed triangle with vertices (p0,p1,p2), a single mode m with amplitude qand shape function vertex 
dis­ m placements (u0,u1,u2), so that the deformed triangle has coordinates (p0 + qu0,p1 + qu1,p2 + qu2). 
Let U =(p1 - p0), V =(p2 - p0), mmm iU =(u1 - u0), iV =(u2 - u0), U' = U + iU and V ' = V + iV . For 
su.ciently small values of qthe face normal is m [] U' × V ' U × V iU × V + U × iV n ' = " + q (21) m 
IU' × V ' IIU × V IIU × V I B Vertex Program for DyRT # Load vertex pi into R1 and add 5 modal corrections: 
MOV R1, v[OPOS]; # R1 = pi MAD R1, c[DyRT ].xxxw, v[5], R1; # R1 += q1<i1 MAD R1, c[DyRT ].yyyw, v[6], 
R1; # R1 += q2<i2 MAD R1, c[DyRT ].zzzw, v[7], R1; # R1 += q3<i3 MAD R1, c[DyRT+1].xxxw, v[8], R1; # 
R1 += q4<i4 MAD R1, c[DyRT+1].yyyw, v[9], R1; # R1 += q5<i5 # Load normal ni into R2 and add 5 modal 
corrections: MOV R2, v[NRML]; # R2 = ni MAD R2, c[DyRT ].xxxw, v[10], R2; # R2 += q1Ni1 MAD R2, c[DyRT 
].yyyw, v[11], R2; # R2 += q2Ni2 MAD R2, c[DyRT ].zzzw, v[12], R2; # R2 += q3Ni3 MAD R2, c[DyRT+1].xxxw, 
v[13], R2; # R2 += q4Ni4 MAD R2, c[DyRT+1].yyyw, v[14], R2; # R2 += q5Ni5 # Bone-weighted Vertex Blending: 
.... # Transform and Lighting: ....   References [1] D. Baraff and A. Witkin. Dynamic Simulation of 
Non-penetrating Flexible Bodies. In Computer Graphics (SIGGRAPH 92 Conference Proceedings), pages 303 
308, 1992. [2] D. Baraff and A. Witkin. Large Steps in Cloth Simulation. In SIGGRAPH 98 Conference Proceedings, 
pages 43 54, 1998. [3] A. Barr. Global and Local Deformations of Solid Primitives. In Computer Graphics 
(SIGGRAPH 84 Conference Proceedings), volume 18, pages 21 30, 1984. [4] C. Basdogan. Real-time Simulation 
of Dynamically Deformable Finite El­ement Models Using Modal Analysis and Spectral Lanczos Decomposition 
Methods. In Medicine Meets Virtual Reality (MMVR 2001), pages 46 52, 2001. [5] J.E. Chadwick, D.R. Haumann, 
and R.E. Parent. Layered Construction of Deformable Animated Characters. In Computer Graphics (SIGGRAPH 
89 Conference Proceedings), volume 23, pages 243 252, 1989. [6] G. Debunne, M. Desbrun, A. Barr, and 
M.-P. Cani. Dynamic real-time deformations using space and time adaptive sampling. In SIGGRAPH 01 Conference 
Proceedings, pages 31 36, 2001. [7] G. Dhondt and K. Wittig. CalculiX: A Free Software Three-Dimensional 
Structural Finite Element Program. [8] M. Friedmann and A. Pentland. Distributed physical simulation. 
In Third Eurographics Workshop on Animation and Simulation, pages 1 17, 1992. [9] J. Gourret, N. Magnenat-Thalmann, 
and D. Thalmann. Simulation of Ob­ject and Human Skin Deformations in a Grasping Task. In Computer Graphics 
(SIGGRAPH 89 Conference Proceedings), volume 23, pages 21 29, 1989. [10] D.L. James. Multiresolution 
Green s Function Methods for Interactive Simulation of Large-scale Elastostatic Objects and Other Physical 
Systems in Equilibrium. PhD thesis, Institute of Applied Mathematics, University of British Columbia, 
Vancouver, British Columbia, Canada, 2001. [11] D.L. James and D.K. Pai. ArtDefo: Accurate Real Time 
Deformable Objects. In SIGGRAPH 99 Conference Proceedings, pages 65 72, 1999. [12] Y. Lee, D. Terzopoulos, 
and K. Walters. Realistic Modeling for Facial Ani­mation. In SIGGRAPH 95 Conference Proceedings, pages 
55 62, 1995. [13] R. Lehoucq, D. Sorensen, and C. Yang. ARPACK Users Guide: Solution of large scale eigenvalue 
problems with implicitly restarted Arnoldi methods. Technical report, Comp. and Applied Mathematics, 
Rice Univ., 1997. [14] E. Lindholm, M.J.Kilgard, and H. Moreton. A User-Programmable Vertex Engine. In 
SIGGRAPH 2001 Conference Proceedings, pages 149 158, 2001. [15] D. Metaxas and D. Terzopoulos. Dynamic 
Deformation of Solid Primitives with Constraints. In Computer Graphics (SIGGRAPH 92 Conference Proceedings), 
volume 26, pages 309 312, 1992. [16] R.M. Murray, Z. Li, and S.S. Sastry. A Mathematical Introduction 
to Robotic Manipulation. CRC Press, Inc., 1994. [17] J. O Brien, P. Cook, and G. Essl. Synthesizing Sounds 
from Physically Based Motion. In SIGGRAPH 01 Conference Proceedings, pages 529 536, 2001. [18] J.F. O 
Brien and J.K. Hodgins. Graphical modeling and animation of brittle fracture. In SIGGRAPH 99 Conference 
Proceedings, pages 111 120, 1999. [19] A. Pentland and J. Williams. Good Vibrations: Modal Dynamics for 
Graph­ics and Animation. In Computer Graphics (SIGGRAPH 89 Conference Proceedings), volume 23, pages 
215 222, 1989. [20] G. Picinbono, H. Delingette, and N. Ayache. Non-linear and anisotropic elastic soft 
tissue models for medical simulation. In ICRA2001: IEEE Interna­tional Conference on Robotics and Automation, 
Seoul Korea, May 2001. [21] J. Schoberl. NETGEN -An advancing front 2D/3D-mesh generator based on abstract 
rules. Comput.Visual.Sci, 1:41 52, 1997. [22] A.A. Shabana. Theory of Vibration, Volume II: Discrete 
and Continuous Systems. Springer Verlag, New York, NY, first edition, 1990. [23] J. Stam. Stochastic 
Dynamics: Simulating the Effects of Turbulence on Flexible Structures. Computer Graphics Forum, 16(3), 
1997. [24] K. Steiglitz. A Digital Signal Processing Primer with Applications to Digital Audio and Computer 
Music. Addison-Wesley, New York, 1996. [25] D. Terzopoulos and K. Fleischer. Deformable models. The Visual 
Computer, 4:306 331, 1988. [26] K. van den Doel, P.G. Kry, and D.K. Pai. FoleyAutomatic: Physically-based 
Sound Effects for Interactive Simulations and Animations. In SIGGRAPH 01 Conference Proceedings, 2001. 
[27] J. Wilhelms and A.V. Gelder. Anatomically Based Modeling. In SIGGRAPH 97 Conference Proceedings, 
pages 173 180, 1997. [28] A. Witkin and W. Welch. Fast Animation and Control of Nonrigid Struc­tures. 
In Computer Graphics (SIGGRAPH 90 Conference Proceedings), pages 243 252, 1990. [29] O. C. Zienkiewicz. 
The Finite Element Method. McGraw-Hill Book Company (UK) Limited, Maidenhead, Berkshire, England, 1977. 
where the quantity in square brackets is the .at-shaded normal cor­rection. For smooth shading, normals 
can be averaged over vertex ad­jacent faces to obtain the ith per-vertex normal correction Nim from (20). 
Alternate approaches using .nite di.erences are also possible. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566622</article_id>
		<sort_key>586</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Interactive skeleton-driven dynamic deformations]]></title>
		<page_from>586</page_from>
		<page_to>593</page_to>
		<doi_number>10.1145/566570.566622</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566622</url>
		<abstract>
			<par><![CDATA[This paper presents a framework for the skeleton-driven animation of elastically deformable characters. A character is embedded in a coarse volumetric control lattice, which provides the structure needed to apply the finite element method. To incorporate skeletal controls, we introduce line constraints along the bones of simple skeletons. The bones are made to coincide with edges of the control lattice, which enables us to apply the constraints efficiently using algebraic methods. To accelerate computation, we associate regions of the volumetric mesh with particular bones and perform locally linearized simulations, which are blended at each time step. We define a hierarchical basis on the control lattice, so for detailed interactions the simulation can adapt the level of detail. We demonstrate the ability to animate complex models using simple skeletons and coarse volumetric meshes in a manner that simulates secondary motions at interactive rates.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[deformation]]></kw>
			<kw><![CDATA[physically-based animation]]></kw>
			<kw><![CDATA[physically-based modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P362262</person_id>
				<author_profile_id><![CDATA[81100524410]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Capell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P362261</person_id>
				<author_profile_id><![CDATA[81100201058]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Seth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Green]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17009957</person_id>
				<author_profile_id><![CDATA[81100301736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duchamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P310471</person_id>
				<author_profile_id><![CDATA[81100620346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AUBEL, A., AND THALMANN, D. 2000. Realistic deformation of human body shapes. In Proceedings of Computer Animation and Simulation 2000, 125-135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BANK, R. E. 1996. Hierarchical bases and the finite element method, vol. 5 of Acta Numerica. Cambridge University Press, Cambridge, 1-43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D., AND WITKIN, A. 1992. Dynamic simulation of non-penetrating flexible bodies. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2, 303-308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D., AND WITKIN, A. 1998. Large steps in cloth simulation. In Proceedings of SIGGRAPH 98, 43-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718982</ref_obj_id>
				<ref_obj_pid>647241</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BRO-NIELSEN, M., AND COTIN, S. 1996. Real-time volumetric deformable models for surgery simulation using finite elements and condensation. Computer Graphics Forum (Proceedings of Eurographics '96) 15, 3, 57-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CAPELL, S., GREEN, S., CURLESS, B., DUCHAMP, T., AND POPOVI&#262;, Z. 2002. A multiresolution framework for dynamic deformations. University of Washington, Department of Computer Science and Engineering, Technical Report 02-04-02.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CIRAK, F., AND ORTIZ, M. 2001. Fully c1-conforming subdivision elements for finite deformation thin-shell analysis. International Journal for Numerical Methods in Engineering 51, 7 (July), 813-833.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEBUNNE, G., DESBRUN, M., BARR, A., AND CANI, M.-P. 1999. Interactive multiresolution animation of deformable models. Eurographics Workshop on Animation and Simulation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383262</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DEBUNNE, G., DESBRUN, M., CANI, M.-P., AND BARR, A. H. 2001. Dynamic real-time deformations using space & time adaptive sampling. In Proceedings of SIGGRAPH 2001, 31-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351638</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., SCHR&#214;DER, P., AND BARR, A. 1999. Interactive animation of structured deformable objects. Graphics Interface '99 (June), 1-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614374</ref_obj_id>
				<ref_obj_pid>614267</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 1997. Dynamic free-form deformations for animation synthesis. IEEE Transactions on Visualization and Computer Graphics 3, 3 (July-Sept.), 201-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199410</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., AND COHEN, M. F. 1995. Hierarchical and variational geometric modeling with wavelets. Symposium on Interactive 3D Graphics, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74335</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GOURRET, J.-P., THALMANN, N. M., AND THALMANN, D. 1989. Simulation of object and human skin deformations in a grasping task. Computer Graphics (Proceedings of SIGGRAPH 89) 23, 3 (July), 21-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566578</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GRINSPUN, E., KRYSL, P., AND SCHR&#214;DER, P. 2002. Charms: A simple framework for adaptive simulation. To appear in the Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134036</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HSU, W. M., HUGHES, J. F., AND KAUFMAN, H. 1992. Direct manipulation of free-form deformations. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 177-184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311542</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[JAMES, D. L., AND PAI, D. K. 1999. Artdefo - accurate real time deformable objects. Proceedings of SIGGRAPH 99 (August), 65-72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237281</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KOCH, R. M., GROSS, M. H., CARLS, F. R., VON B&#220;REN, D. F., FANKHAUSER, G., AND PARISH, Y. 1996. Simulating facial surgery using finite element methods. Proceedings of SIGGRAPH 96 (August), 421-428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344862</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J. P., CORDNER, M., AND FONG, N. 2000. Pose space deformation: A unified approach to shape interpolation and skeleton-driven deformation. In Proceedings of SIGGRAPH 2000, 165-172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364343</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LI, X., WOON, T. W., TAN, T. S., AND HUANG, Z. 2001. Decomposing polygon meshes for interactive applications. In ACM Symposium on Interactive 3D Graphics, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237247</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MACCRACKEN, R., AND JOY, K. I. 1996. Free-form deformations with lattices of arbitrary topology. Computer Graphics (Proceedings of SIGGRAPH 96) 30, 181-188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134085</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[METAXAS, D., AND TERZOPOULOS, D. 1992. Dynamic deformation of solid primitives with constraints. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 309-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74355</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PENTLAND, A., AND WILLIAMS, J. 1989. Good vibrations: Modal dynamics for graphics and animation. Computer Graphics (Proceedings of SIGGRAPH 89) 23, 3 (July), 215-222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>710679</ref_obj_id>
				<ref_obj_pid>646923</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PICINBONO, G., DELINGETTE, H., AND AYACHE, N. 2000. Real-time large displacement elasticity for surgery simulation: Non-linear tensor-mass model. In Proceedings of the Third International Conference on Medical Robotics, Imaging and Computer Assisted Surgery: MICCAI 2000, 643-652.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378524</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[PLATT, J. C., AND BARR, A. H. 1988. Constraint methods for flexible models. Computer Graphics (Proceedings of SIGGRAPH 88) 22, 4 (August), 279-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[PRENTER, P. M. 1975. Splines and Variational Methods. John Wiley and Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[ROTH, S. H. M., GROSS, M. H., TURELLO, S., AND CARLS, F. R. 1998. A bernstein-b&#233;zier based approach to soft tissue simulation. Computer Graphics Forum 17, 3, 285-294.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SEDERBERG, T. W., AND PARRY, S. R. 1986. Free-form deformation of solid geometric models. Computer Graphics (Proceedings of SIGGRAPH 86) 20, 4 (Aug.), 151-160.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SHABANA, A. 1998. Dynamics of Multibody Systems. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SINGH, K., AND KOKKEVIS, E. 2000. Skinning characters using Surface-Oriented Free-Form deformations. In Proceedings of the Graphics Interface 2000, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364382</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SLOAN, P.-P. J., ROSE, C. F., AND COHEN, M. F. 2001. Shape by example. In Symposium on Interactive 3D Graphics, 135-144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286071</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[STOLLNITZ, E. J., DEROSE, T. D., AND SALESIN, D. H. 1996. Wavelets for Computer Graphics: Theory and Applications. Morgan Kaufmann, San Francisco, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>282217</ref_obj_id>
				<ref_obj_pid>280953</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[TEICHMANN, M., AND TELLER, S. 1998. Assisted articulation of closed polygonal models. In Computer Animation and Simulation '98, 87-101.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling Inelastic deformation: Viscoelasticity, plasticity, fracture. Computer Graphics (Proceedings of SIGGRAPH 88) 22, 4 (August), 269-278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617444</ref_obj_id>
				<ref_obj_pid>616002</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND WITKIN, A. 1988. Physically based models with rigid and deformable components. IEEE Computer Graphics and Applications 8, 6 (Nov.), 41-51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 1987. Elastically deformable models. Computer Graphics (Proceedings of SIGGRAPH 87) 21, 4 (July), 205-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258833</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[WILHELMS, J., AND GELDER, A. V. 1997. Anatomically based modeling. In Proceedings of SIGGRAPH 97, 173-180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>565650</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND WELCH, W. 1990. Fast animation and control of nonrigid structures. Computer Graphics (Proceedings of SIGGRAPH 90) 24, 4 (August), 243-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Skeleton-Driven Dynamic Deformations Steve Capell Seth Green Brian Curless Tom Duchamp Zoran 
Popovic´ University of Washington Abstract This paper presents a framework for the skeleton-driven animation 
of elastically deformable characters. A character is embedded in a coarse volumetric control lattice, 
which provides the structure needed to apply the .nite element method. To incorporate skele­tal controls, 
we introduce line constraints along the bones of sim­ple skeletons. The bones are made to coincide with 
edges of the control lattice, which enables us to apply the constraints ef.ciently using algebraic methods. 
To accelerate computation, we associate regions of the volumetric mesh with particular bones and perform 
locally linearized simulations, which are blended at each time step. We de.ne a hierarchical basis on 
the control lattice, so for detailed interactions the simulation can adapt the level of detail. We demon­strate 
the ability to animate complex models using simple skeletons and coarse volumetric meshes in a manner 
that simulates secondary motions at interactive rates. CR Categories: I.3.5 [Computer Graphics]: Computational 
Ge­ometry and Object Modeling Physically Based Modeling I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Animation; Keywords: animation, deformation, physically-based animation, physically-based 
modeling 1 Introduction Physical simulation is central to the process of creating realis­tic character 
animations. In the .lm industry, animators require detailed control of the motion of their characters, 
but creating physically-based secondary motions is dif.cult and time consum­ing to do by hand. Recently, 
techniques have been developed for automatically simulating these secondary motions. These methods are 
built atop skin, muscle, and bone models and can generate de­tailed, dynamic motions. However, constructing 
these models is time consuming, and the simulations are computationally expen­sive. By contrast, in video 
game or virtual reality applications where interactivity is critical, character animation is built atop 
much sim­pler models. The shapes are composed of convenient primitives and are controlled by line segment 
based skeletons. Deformations of body parts are purely kinematically driven, using, e.g., blended coordinate 
frames. Incorporating realistic physically-based dynam­ics using the .lm industry s approach is currently 
impractical. Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to 
make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 In this paper, we attempt to bring dynamic simulation into the realm of real-time, 
skeleton-driven animation. The challenge is to .nd the right combination of physical principles, geometric 
mod­eling, computational tools, and simplifying assumptions that yield compelling animations at interactive 
rates. Our approach is based on the equations of motion of elastic solids, simulated in a .nite element 
setting. The volumetric .nite element mesh need only be speci.ed coarsely, subject to the re­quirement 
that it encompass the geometric model on which simula­tion will be performed. This last requirement is 
necessary in order to ensure complete integration over the interior of the object. In fact, as long as 
the interior of the object is well-de.ned, simulation of its elastic deformation is possible regardless 
of the the surface representation or complexity. The volumetric mesh we choose is not restricted to a 
regular grid; rather, it is comprised of elements such as tetrahedra and hexahe­dra. This .exibility 
permits construction of meshes that conform better to the surface of the object, improving simulation 
quality. In addition, to support adaptive level of detail during simulation, we construct a hierarchical 
basis, which allows detail to be introduced or removed as needed. Since our ultimate goal is simulation 
of skeletally controlled characters, our framework supports line constraints, where lines correspond 
to bones. In order to incorporate these constraints eas­ily, we require the volumetric mesh to contain 
edges coincident with the bones. Finally, to achieve interactive rates, we linearize the equations of 
motion, solve them over volumetric regions associated with each bone, and blend the deformations where 
regions overlap. We believe that this work makes a number of contributions. Our crafting of the function 
space in order to make constraint han­dling easier is, to our knowledge, novel. We introduce blended 
local linearization of nonlinear equations, in the context of de­formable animated characters. We generalize 
a method of solving constraints using linear subspace projection. We also introduce a constraint that 
allows one-dimensional bones to behave as three­dimensional bones. Finally, we believe our most important 
contri­bution is putting together a collection of techniques that allows us to interactively animate 
arbitrary shapes with skeletal controls while generating realistic dynamic deformations. 2 Related Work 
Probably the most common technique for deforming articulated characters is to de.ne the position of the 
surface geometry as a function of an underlying skeletal structure or set of control pa­rameters. Recent 
advances in this area can be found in the work of Lewis et al. [2000], Singh and Kokkevis [2000], and 
Sloan et al. [2001]. Our work builds on the notion of skeletal control, but within a physically-based 
framework. In the late 1980 s, Terzopoulos et al. pioneered the .eld of physically-based deformable models 
for computer graphics. Using Lagrangian equations of motion and .nite differences they simu­lated elastic 
[1987] and inelastic [1988] behaviors, combined with a rigid body motion term to compensate for instabilities 
with stiff bodies [1988]. Much of the research that followed sought to add more sophisti­cated constraint 
solvers, accelerate the solutions under a variety of approximations, and add stability to permit larger 
timesteps. Platt and Barr [1988] improved on existing constraint methods (e.g., the penalty method) by 
introducing reaction constraint and augmented Lagrangian constraint approaches. Applying these constraints 
to already complex simulations, however, was not a step toward inter­activity. Pentland and Williams 
[1989] simpli.ed the problem by solv­ing for the vibrational modes of a body and keeping only the lower 
frequency modes and obtained realtime simulations of physically plausible deforming bodies. Witkin and 
Welch [1990] used La­grangian dynamics to solve for low-order polynomial, global de­formations, coupled 
with constraints enforced through Lagrange multipliers. Baraff and Witkin [1992] later extended this 
method for better handling of non-penetration constraints. Finally, Metaxas and Terzopoulos [1992] combined 
Witkin and Welch s global de­formation framework with local .nite element surface deformations and Lagrange 
multiplier constraints to animate superquadric sur­faces. In each of these examples, while achieving 
interactive rates, the deformations were substantial approximations to detailed volu­metric deformations 
and were not demonstrated on complex shapes. To accelerate computations, some hierarchical methods have 
also been employed. Terzopoulos et al. [1988] use a multigrid solver for a surface-based inelastic simulation. 
The approach of Metaxas and Terzopoulos [1992] is analogous to a two level simu­lation that uses global 
deformations at the coarse level and .nite ele­ments for .ner surface deformations. More recently, Debunne 
et al. [1999] built an octtree of particles that interact according to Lam´ e s equation, resulting in 
interactive simulations. The particles are sim­ulated using an explicit Euler solver that steps each 
particle adap­tively in time and at differing spatial resolutions. To animate a sur­face, the particles 
are linked to each surface point by a weighting scheme. Their approach was recently extended to use unstructured 
tetrahedral hierarchies [Debunne et al. 2001]. To add stability to computations, implicit solvers have 
proven to be quite effective. Terzopoulos et al. [1987; 1988] used semi­implicit solvers in their initial 
work. Baraff and Witkin [1998] used an implicit scheme to permit large timesteps in notoriously unstable 
cloth simulations. Desbrun et al. [1999], also working with cloth models, showed that the implicit solution 
method acts as a .lter that stabilizes stiff systems. They also add a rotation term to preserve angular 
momentum and a correction term after each time step to simulate nonlinear elasticity. Free-form deformation 
(FFD), introduced by Sederberg and Parry [1986], is also closely related to our work. FFD involves embedding 
an object in a domain that is more easily parameterized than the object itself. The main advantages of 
FFD are that arbitrary objects can be easily deformed and the space of deformations can be crafted independently 
of the representation and resolution of the object. Since its introduction, the .exibility of FFD has 
been im­proved by introducing lattices of arbitrary topology [MacCracken and Joy 1996], and dynamic free-form 
deformation has been in­troduced to apply FFD to animation [Faloutsos et al. 1997]. Our framework builds 
on FFD by also embedding the object in a coarse control lattice. But unlike the work of Faloutsos et 
al., where a diag­onal stiffness matrix was used, we use the principles of continuum elasticity to compute 
the dynamics of the object being deformed. Another approach to fast, physically-based deformations is 
to solve quasi-static solutions, i.e., compute the equilibrium state of the system given forces and constraints, 
and then animate by ad­justing the forces and constraints over time. Gourret et al. [1989] explored such 
a technique for volumetric .nite elements, and James and Pai [1999] developed an interactive boundary 
element solution under the assumption of constant material properties inside the vol­ume. Other quasi-static 
approaches have also been favored for sur­  Figure 1: (a) An object O . R3 instrumented with a skeletal 
complex S and a control lattice K = K0. Functions on O are de.ned in terms of K, which forms a neighborhood 
of the object. A trilinear basis function is associated with each vertex of K (includ­ing the red vertices 
on the skeleton). (b) The control lattice subdivided once to form K1 (which includes the black, red, 
and green vertices and edges). Additional basis functions are associated with newly introduced vertices 
(shown in green). gical planning and simulation [Bro-Nielsen and Cotin 1996; Koch et al. 1996; Roth et 
al. 1998]. Quasi-static solutions, however, are approximations that do not capture the true dynamics 
of motion. In some cases, interactivity has been achieved without resort­ing to signi.cant simpli.cations 
in the dynamic model. Recently, Picinbono et al. [2000] described an interactive surgery simulation using 
a nonlinear .nite element method. They were able to achieve interactive rates for a virtual liver composed 
of about 2000 tetrahe­dra. However, more complex objects such as the ones used in our work would require 
many more tetrahedra. There are also a number of papers that approach the problem of deforming humans 
and animals using anatomical modeling [Aubel and Thalmann 2000; Wilhelms and Gelder 1997]. They differ 
from our approach in that we are more concerned with interactivity and the appearance of realism than 
actual anatomical modeling. Another possibility for deformation is to perform thin-shell com­putations. 
Recent work by Cirak and Ortiz [2001] demonstrates the use of subdivision elements for computing the 
dynamics of thin shells. However, thin shells are insuf.cient for modeling the inte­rior of solid objects. 
In the next section we introduce the basic mathematical and physical formulation that underlies our framework. 
We then de­scribe our simulation and control methodology (Section 4), discuss results (Section 5), and 
conclude (Section 6). 3 Formulation Each object that we wish to animate is represented as a domain O 
. R3. We make no assumptions about O other than we know its interior. We instrument each object with 
a skeleton, i.e. an anno­tated transformation hierarchy suitable for animation. We refer to each transformation 
in the hierarchy, as well as its associated origin point, as a joint. The skeleton de.nes a graph S, 
whose vertices correspond to joints and whose edges correspond to line segments between two joints that 
share a parent-child relationship. Joints and bones are located interior to the object, so the graph 
S is a piecewise linear subset S . O, that we call a skeletal complex (see Figure 1). Motion of the object 
is represented by a time dependent function p : O × R . R3:(x, t) .. p(x, t) . (1) Let pS : S × R . R3 
(2) denote the restriction of the map to S. Rigidity of the bones implies that pS is an isometry on each 
edge of S. In particular, pS is a piecewise linear function on S. Our goal is to solve for the dynamic 
motion of the object given the motion of the skeleton. Since we model the object as an elastic body, 
the function p(x, t) is then the solution of a system of partial differential equations, subject to the 
constraint p(x, t)= pS(x, t) for x . S. To solve the system numerically, we apply the .nite element method 
(see, e.g., [Prenter 1975]). We separate the map p(x, t) into a constant rest state r(x) and a dynamic 
displacement d(x, t), each of which is represented as a .nite sum. The rest state of the object is given 
by the identity map r : O . R3, which has the expansion: r(x)= ra fa(x)= ra fa(x)= x (3) a where the 
functions fa(x) are elements of a .nite basis B, and ra . R3. As demonstrated in the above equation, 
we use the Einstein summation convention throughout this paper: whenever a term contains the same index 
as both a subscript and a superscript, the term implies a summation over the range of that index. The 
displacement is expanded similarly: d(x, t)= qa(t) fa(x) . (4) where qa(t) . R3 are the dynamically evolving 
coef.cients that determine the deformation of the object over time. The state of the system is simply 
the sum of the rest state and the displacement: p(x, t)=(ra + qa(t))fa(x) (5) We represent the state 
of the body at time t as a column vector of generalized coordinates q = q(t) whose a-th component is 
the coef.cient qa(t) in Equation (4), and we model the dynamics of the body as a system of second order 
ordinary differential equations. The system is obtained by applying the .nite element method to the Lagrangian 
formulation of the equations of elasticity (see, e.g., [Shabana 1998]). In the remainder of the section 
we describe the basis B and formulate the .nite element problem. 3.1 The Hierarchical Basis In order 
to allow our simulations to adapt to local conditions, we employ a hierarchical basis. Such bases are 
well established as use­ful tools for numerical computation (see, e.g., [Bank 1996]). Our construction 
mirrors that of what are referred to as lazy wavelets in [Stollnitz et al. 1996]. The basis B is de.ned 
in terms of re­peated subdivision of a control lattice surrounding the object (see Figure 1). It is desirable 
that the control lattice conform to the shape of the object while being as coarse as possible. An advantage 
of us­ing an unstructured lattice instead of a regular grid to de.ne the de­formation function space 
is that the lattice can be tailored to .t the object. More precisely a control lattice K is a .nite union 
K = .iCi of convex cells Ci satisfying the following conditions: (i) For all i, j, i = j the intersection 
Ci,j = Ci . Cj is either empty or a face, edge, or vertex of both Ci and Cj. (ii) The edges of S are 
edges of cells of K.  (iii) The domain O is contained in the interior of K. (iv) For all i, each vertex 
of Ci has valence 3 (within Ci). Condition (iv) still allows a variety of cell shapes including hexa­hedra, 
tetrahedra and triangular prisms. We now show how to construct a collection of functions B = {fa} on 
K whose restriction to O is a linearly independent set of continuous functions on O. Let V0 . V1 . V2 
. ... be the nested sequence of function spaces described in appendix A.1. The set VJ consists of the 
piecewise trilinear functions on the complex KJ obtained from K by J hexahedral subdivisions. For each 
vertex a of K, positioned at xa, let fa denote the unique function in V0 such that fa(xa) = 1and fa(xb) 
= 0for b = a a vertex of K. Include fa in B if the restriction of fa to O is non-zero. Proceed inductively 
as follows. Let a be a vertex of KJ+1 that is not a vertex of KJ and let fa be the unique function in 
VJ+1 such that fa(a) = 1 and that vanishes at all other vertices of KJ+1. Include fa in B if its restriction 
to O is non-zero and if its restriction to S is zero. Although the elements of B are de.ned on all of 
K, we are only interested in their values on O; we will, therefore, interpret B as collection of functions 
on O. One can show that the set B is linearly independent set of functions, which we call the hierarchical 
basis. By construction fa .B for each joint vertex a . S, and the re­striction of fa to S is linear on 
each bone of S. Moreover if fa .B, for a not a joint vertex, then fa vanishes identically on S. Conse­quently, 
the function pS(x, t) can be written in the form L pS(x, t)= (ra + qa(t))fa(x) . (6) a.S and because 
fa(a) = 1, the vector (ra + qa(t)) is the location of the joint vertex a at time t. 3.2 Equations of 
Motion By virtue of Equation (5), we can express the kinetic energy T and elastic potential energy V 
as functions of q.and q, respectively, where q.denotes the time derivative of q. The equations of motion 
are then the Euler-Lagrange equations () d .T(q.) .V(q)+ Qext + - µq.=0 (7) dt .q..q where .T/.q.and 
.V/.q denote gradients with respect to q.and q, respectively. The term Qext is a generalized force arising 
from external body forces, such as gravity. The last term is a generalized dissipative force, added to 
simulate the effect of friction. We will now derive each of the .rst three terms of Equation 7, ultimately 
yielding a system of ODEs to be solved in generalized coordinates. The kinetic energy of a moving body 
is a generalization of the familiar 12 mv2: 1 11 Mab . T = .(x) p.· p.dO = qa · q.b (8) 2 O 2 Mab where 
.(x) is the mass density of the body, and = 1 .fafbdO. Equation (8) yields the formula O () d .T = Mq¨. 
(9) dt .q. The matrix M composed of the elements Mab is called the mass matrix. We discuss its computation 
in Section 3.3. The elastic potential energy of a body captures the amount of work required to deform 
the body from the rest state into the current con.guration. It is expressed in terms of the strain tensor 
and stress tensor. Strain is the degree of metric distortion of the body. A standard measure of strain 
is Green s strain tensor: .di .dj .dk .dl eij =++ dkl (10) .xj .xi .xi .xj The diagonal terms of the 
strain tensor represent normal deforma­tions while those off the diagonal capture shearing. Forces acting 
on the interior of a continuum appear in the form of the stress ten­sor, which is de.ned in terms of 
strain: . tij =2Gtr(e)dij + eij(11) 1 - 2. dij where tr(e)= eij. The constant G, called the shear modulus 
or modulus of rigidity, determines how hard the body resists deforma­tion. The coef.cient ., called Poisson 
s ratio, determines the extent to which strains in one direction are related to those perpendicular to 
it. This gives a measure of the degree to which the body preserves volume. The elastic potential energy 
V(q), which is analogous to the familiar de.nition of work as force times distance, is given by the formula 
1 . V = G tr2(e)+ dijdkleikejl dO (12) O 1 - 2. By combining Equations 4, 10, and 12 we can express the 
elastic potential V and its derivatives (with respect to q) as polynomial functions of q. The coef.cients 
of these polynomials are integrals that can be precomputed. Details are described in appendix A.2. .2V 
The matrix S = is referred to as the stiffness matrix. .q.q To add realism, we include the force of gravity 
in our formula­tion. Gravity is an example of a body force that affects all points inside the body. We 
treat gravity as a constant acceleration .eld speci.ed by the vector g. The gravitational potential energy 
is then the integral 11 Vg = . g · p = .fag · qa . (13) OO The generalized gravitational force is the 
gradient (1 ) .Vg Qga == .fag (14).qa O The above force can be interpreted as the familiar mg except 
that the mass term represents all of the mass associated with a particular basis function. 3.3 Numerical 
Integration In order to compute the gravity terms and the mass and stiffness matrices we precompute the 
integrals in equations (8), (14), and (29). The integration is done numerically using the following steps: 
1. Subdivide K to the desired level for numerical integration. 2. Compute the values of the basis functions 
at each vertex. 3. Tetrahedralize the domain. After subdividing once, the do­main is composed of only 
hexahedral cells. We then divide each of these cells into tetrahedra in order to approximate functions 
on the domain as piecewise linear. 4. Compute the integrals over each domain tetrahedron using piecewise 
linear approximations to the basis functions. If all four vertices of a tetrahedron fall outside the 
surface of the object, its contribution to the integrals is neglected.  With the integrals computed, 
equation (7) can now be solved using a nonlinear Newton-Raphson solver.  4 Skeletal Simulation The fully 
nonlinear elastic formulation described in the previous section is computationally expensive, and does 
not take into consid­eration the skeleton. In this section we introduce a set of techniques, tailored 
for fast skeleton-driven animation, that approximate the nonlinear dynamics. Figure 2: The upper left 
image shows an input model instrumented with a skeleton and local coordinate systems. The upper right 
image shows the model embedded in (half of) a control lattice. The lower left image shows how the skeleton 
coincides with edges and vertices of the control lattice. The lower right image shows the entire control 
lattice, as well as the division of the object into regions for local linearization. Each region is associated 
with one of the local coordinate systems in the upper left image. Note the color blending where regions 
overlap. 4.1 Instrumentation Prior to simulation, a model must be instrumented with a skeleton and control 
lattice. Although recent work by Teichmann and Teller addresses automated skeleton construction [1998], 
we currently let the animator specify the skeleton in order to achieve the desired level of control. 
We have implemented a simple system that allows a skeleton to be constructed manually in just a few minutes. 
The user creates a joint by clicking on the object with the mouse. If the ray through the mouse point 
(from the camera projection center) intersects the object at least twice, a joint is placed midway be­tween 
the .rst two intersections. This positioning scheme produces joints that are centrally located inside 
the object. Two joints can be selected to de.ne a bone, and with the selection of a root joint, a transformation 
hierarchy can be created automatically. We currently use a constructive procedure that allows the user 
to build the control lattice interactively by adding cells incremen­tally and repositioning the control 
vertices as needed. Several hours are required for an experienced user to create a moderately complex 
control lattice. The abundance of volumetric meshing schemes sug­gests that automatic creation of the 
control lattice is possible, and we hope to address this problem in the future. Figure 2 shows the skeleton 
and control mesh for a kangaroo model. 4.2 Solving the System Due to the computational expense of solving 
the full nonlinear equations of elasticity, we seek simpli.cations that make the equa­tions easier to 
solve. One possibility is to linearize the equations of motion at the beginning of each timestep as was 
done by Baraff and Witkin in their work on cloth simulation [1998]. In our experi­ence, simulations using 
this method are essentially indistinguish­able from results obtained using a nonlinear implicit method 
to solve the system, as long as the timestep is not so large as to al­low radical shape change during 
a single step. After applying their implicit solver to our formulation, the resulting equations are: 
.q = h(q.+ .v) (15) () .V - Qext (M - hµI + h2S).v = h µq.-- hSq.(16) .q where h is the timestep, µ is 
the damping coef.cient, I is the identity matrix, .v is the change in the velocity q.during the timestep, 
.q is the change in q during the timestep, M is the mass matrix, and S is the stiffness matrix. All quantities 
are evaluated at the beginning of the timestep. Equation (16) is a sparse linear system that can be solved 
for .v using a Conjugate Gradients (CG) solver. Then .v is substituted into equation (15) to obtain .q. 
 4.3 Bone Constraints In our framework the skeleton is controlled directly by keyframe data or some other 
source external to the dynamic simulation. From the viewpoint of the simulation, the skeleton is simply 
a compli­cated constraint. Because we have restricted the bones to lie along edges in the control lattice, 
and the basis is interpolating, it is es­pecially easy to handle the bone constraints algebraically. 
Each control point that lies on a bone corresponds to a component of .v that is known a priori, rather 
than having to be computed. Simpli­fying equation (16) to the form A.v = b, we can sort the variables 
into known (.vk and bk) and unknown quantities (.vu and bu) and form the following system: ( )( )() A11 
A12 .vk bu = (17) A21 A22 .vu bk The reason that some components of the vector b are now unknown arises 
from the fact that the external forces required to enforce the bone constraints are unknown, and they 
appear on the r.h.s. of equa­tion (16). In order to solve for .vu we simply solve the system: A22.vu 
= bk - A21.vk (18) The advantage of this approach is that adding skeletal constraints actually reduces 
the computational cost by shrinking the system that must be solved. 4.4 Linear Subspace Constraints 
Because we would like our objects to interact with other objects, position constraints are also important. 
The framework of Baraff and Witkin [1998] provides an elegant solution for particle systems. During each 
internal step of a CG solver, they project out certain components of .v corresponding to constrained 
particles. Here we show that this technique can be extended to include position constraints at any point 
in a continuous body. Position constraints in our framework are of the form: dc(t)= qafa(xc) (19) which 
simply says that the displacement at xc conforms to some known function dc. Evaluating equation (15) 
at xc results in: dc(t + h) - dc(t) .vafa(xc)= - q.afa(xc) (20) h The r.h.s. of the above equation is 
simply a constant ac that can be computed at the beginning of each timestep. If we accumulate the x, 
y, and z components of the 3-vectors ac into the n-vectors aa , where a .{x, y, z}, de.ne the matrix 
Cac = fa(xc), and separate .v into its x, y, and z components .va, equation (20) becomes: CT aa .v= a, 
a .{x, y, z} (21) So each constraint requires that .v be constant along three par­ticular directions 
in R3n . Maintaining the constraints involves the following steps: 1. At the beginning of each timestep, 
.v is initialized so that equation (21) holds. This is accomplished by computing the QR-decomposition 
of C and transforming equation (21) into RT baa a = a, .v= Qba, from which .v can be easily com­puted. 
Although QR-decomposition of an n × m matrix re­quires O(nm2) time, in our case the number of constraints 
m is typically small, so the computational cost is low. 2. Each column c of C has an associated projection 
matrix P = I - ccT /cT c, which, when applied to a vector, eliminates the component in the direction 
of c. These projectors are applied during CG such that incremental updates to .v are orthogonal to the 
vectors c, ensuring that equation (21) remains true (for details see [Baraff and Witkin 1998]). In our 
current framework, con.icting constraints can be detected during QR-decomposition and removed. In the 
future we hope to augment this method to solve over-constrained systems more ele­gantly, as was done 
for FFD by Hsu et al. [1992]. 4.5 Blended Local Linearization A major bottleneck in our system is the 
computation of the stiff­ness matrix at the beginning of each timestep (the elastic potential is a quartic 
function of q). A well-known simpli.cation is to lin­earize the strain tensor by dropping the last term 
in equation (10), which results in a quadratic elastic potential and thus a constant stiffness matrix 
(which is composed of the .rst three addends in equation (28)). As compared to other simpli.cations such 
as us­ing a mass-spring-based elastic potential, linearization of strain has the advantage that it is 
a very good approximation, but only when the deformation is small; for large deformations, severe distortions 
occur. A notable case for the linear strain model is when the object undergoes a large rigid rotation, 
coupled with a small deformation. While the elastic potential based on nonlinear strain does not penal­ize 
rotations, the linear strain model does, while failing to penalize certain shearing deformations. Terzopoulos 
et al. [1988] addressed this case by modeling the deformation relative to a frame of refer­ence that 
follows the gross motion of the object. Since the relative deformation is assumed to be small, the linearized 
strain is a rea­sonable approximation. This approach is common practice in the engineering literature, 
such as in the textbook of Shabana [1998], in which multibody systems composed of interconnected parts 
are considered. In such systems, the deformation of each part can be measured from a local reference 
con.guration that factors in the ro­tation of the part. As long as the deformation of each part is small 
relative to its rotated reference con.guration, the linear strain model is a good approximation. To apply 
these ideas to articulated characters, we .rst recognize that the soft tissues of vertebrates do not 
typically undergo large deformations relative to nearby bones. Based on this assertion, our approach 
is to divide the object into regions, each of which can be simulated using the linear strain model. The 
user divides the object into regions by assigning weights to the control vertices, forming a partition 
of unity over the object. A piece of the object can belong to a single region or can be di­vided fractionally 
among several regions. We encode the weights for region i in a diagonal square matrix Wi, where Wi is 
the weight aa associated with vertex a in region i. The lower right image in Fig­ure 2 shows a partitioned 
object, colored according to the region assignments. Our current system requires that the user select 
indi­vidual weights for each control vertex, but a more intuitive painting interface would be straightforward 
to implement. It would also be helpful to automate the task of region assignment (recent work by Li et 
al. [2001] may be adaptable to our problem domain). From the region assignments we form a cell complex 
Ki corre­sponding to region i: Ki = {C . K : . va . v(C), Waai > 0} (22) where v(C) is the set of control 
vertices on cell C. Each region has an associated function space: i fa B= { fa|Ki : fa .B, |Ki =0} (23) 
where fa|Ki denotes the restriction of fa to Ki . We de.ne a rect­angular matrix Qi to select the basis 
functions that have nonzero restrictions to region i. The element Qi .B ab = 1 if and only if fa corresponds 
to fb .Bi. The pseudocode for taking a single simu­lation step is: foreach region i do iii] . [Qir, Qiq, 
Qi . 1 [r, q, q.q] foreach a do ii ii 2 qa := qa - Ti(ra)+ ra end 3 Construct Ai and bi from equation 
(18) 4 Solve Ai.vi = bi end WiQiT i 5 .v . .v i 6q.. q.+ .v 7q . q + hq. Line 1 extracts the regional 
variables from the global system. Line 2 converts qi so that it corresponds not to displacement from 
the rest state, but to displacement from the rest state transformed according to the transformation of 
the bone coordinate system. The homogeneous transformation Ti, extracted from the current con.g­uration 
of the skeleton, represents the transformation of the bone from its rest position to its current position. 
But it is not enough to simply transform qi, because the transformation itself must be sub­tracted from 
qi. A displacement .eld dT that transforms the object x according to the transformation T(x), has the 
following form: dT + x = T(x) (24) It is from the above expression that line 2 is derived. Line 3 builds 
the linear system required to solve for the local equations of mo­tion, including the extraction of bone 
constraints, and line 4 solves the linear system using CG. Line 5 merges the solutions from each region, 
each weighted according to the user-assigned weights in Wi . Finally, the state of the global system 
is updated in lines 6 and 7.  4.6 Twist Constraint In natural creatures with three-dimensional bones, 
the .esh cannot twist (i.e. rotate) around the axis of the bone without causing the .esh to deform. Such 
deformations are resisted by emergent elas­tic forces, so the twisting is limited. But .esh can rotate 
about a line constraint without deforming. To avoid such unnaturally free movement, we introduce a soft 
constraint to penalize all displace­ment (not just deformation) within a .xed radius of the bones. We 
 Figure 3: The left image shows the kangaroo at rest. Brown spheres represent active basis functions. 
The cyan sphere represents a position constraint. On the right, the position constraint has been moved 
causing adaptation of the basis. The red spheres in the right image represent newly introduced detail 
coef.cients. denote this region Oß . O. The following potential describes the constraint: 1 1 U = d · 
ddO (25) 2 Oß The above potential is quadratic, so its Hessian is simply a constant that can be added 
to the stiffness matrix: 1 .2U fab = I f(26) .qa.qb Oß where I is a 3 × 3 identity matrix. The above 
constraint must be computed relative to the rigidly transformed bone, which .ts well into our local computation 
framework. 4.7 Adaptation Because we use a hierarchical basis, our simulator can add detail where needed. 
We apply the simple heuristic that detail is more helpful where there are large deformations (similar 
to, e.g., [De­bunne et al. 2001]). If the object is suf.ciently deformed over the support of a particular 
basis function, then all of the basis functions in the next .ner level with support overlapping the area 
of high dis­tortion are introduced into the simulation. Likewise, basis functions are removed when there 
is little deformation in their support. Each level of the basis has an associated threshold for determining 
when to re.ne and another for determining when to coarsen. As noted in [Debunne et al. 2001], a lower 
threshold is required for coarsen­ing than re.ning in order to prevent the simulation from oscillating 
between levels of resolution. Regardless of the criteria employed, adapting the basis is straightforward 
in our framework. Most of this simplicity comes from re.ning the basis, not the geometry, as was done 
by Gortler and Cohen [1995] and recently generalized by Grinspun et al. [2002]. For some .xed number 
of basis levels we precompute the mass and stiffness matrices and store them in a sparse data struc­ture. 
Adapting the basis simply corresponds to extracting and re­linquishing certain components from these 
matrices, which can be done very quickly. The resultant subsets of the basis are linearly independent 
regardless of which basis functions we choose. Fig­ure 3 shows adaptation of the kangaroo model. For 
more details regarding our adaptation methodology see [Capell et al. 2002].  5 Results The accompanying 
video shows the results of applying our frame­work to two triangle meshes that we acquired from the Internet. 
 Figure 4: Frames from an interactive animation. There is no noticeable warping due to strain linearization, 
and the different materials (e.g., ears, horns) behave distinctly. Figure 5: On the left is the global 
linear solution, which shows signi.cant warping when the cow turns its head to one side. In the center 
is the fully nonlinear solution. On the right is the blended local linear solution, which shows no noticeable 
warping of the head. A slight protrusion can be seen in the neck of the right image due to region blending. 
The control mesh for the kangaroo model has 448 cells and 177 vertices; the cow control mesh has 572 
cells and 214 vertices. On a 1 Ghz PC, both the cow and kangaroo animated at about 100 Hz using only 
the coarse basis functions, which is clearly within range for interactive applications (with adaptation, 
simulation time varies depending on the degree of adaptation required). Figure 4 shows frames of an animation 
of the cow model (using the coarse basis), which demonstrates the ability of our system to handle variable 
ma­terial properties; the ears .op around realistically while the horns stay rigid. This feature is possible 
to do interactively because the control mesh can be carefully crafted to respect material bound­aries, 
and because our computation of the stiffness matrix takes variable material properties into account. 
For our datasets, the blended local linear and global linear so­lutions required about the same amount 
of computation time. Yet the blended local linear solution produced much more pleasing re­sults, as demonstrated 
in Figure 5. The blended local linear solution looks similar to the fully nonlinear solution, while the 
global linear solution is badly warped. 6 Conclusion We have introduced a method for interactive simulation 
of de­formable bodies controlled by an underlying skeleton. By choosing a volumetric mesh that aligns 
with the bones, we are able to meet the bone constraints rapidly. We extend a fast constraint solver 
that works directly within an iterative solver. We also introduce a twist constraint that mimics the 
effects of three-dimensional bones when only one-dimensional bones are being modeled. Our method per­forms 
with the speed of simple linear-strain models of elasticity, but does not suffer from distortions arising 
from global linearity. There are many avenues for future work. We would like to au­tomatically generate 
skeletons and especially control lattices, the latter being the most labor intensive aspect of our framework. 
Our assumptions about small deformations break down near the joints. It may be possible to address this 
problem by using nonlinear elas­ticity near the joints. The deformations near joints might also be improved 
by speci.cally tailoring adaptation to the problem. Fi­nally, it would be convenient to include dynamic, 
not just fully con­strained, bones. Acknowledgments The authors would like thank Chris Twigg, Mira Dontcheva 
and Samantha Michel for their instrumental work in creating and converting Maya skeletal animations, 
and Shawn Bonham and Sean Smith for additional help. This work was sup­ported by the Animation Research 
Labs, Microsoft Research, NSF grants DMS-9803226 and CCR-0092970, and an Intel equipment donation. A 
Appendix A.1 Review of Trilinear Functions A trilinear function on the standard unit cube C3= {x =(x, 
y, z): 0 = x, y, z = 1} is a function of the form f (x, y, z)= a0+ a1x + a2y + a3z + a4xy + a5xz + a6yz 
+ a7xyz. The function f is determined by its values at the vertices of C3: let f(s) denote the hat function 
. f(s) = 1 - |s| 0 for |s| = 1 for |s| > 1. . and let f0(x, y, z)= f (x)f (y)f (z). Then L f (x)= fi,j,k 
f0(x - i, y - j, z - k) 0=i,j,k=1 where fi,j,k = f (i, j, k). It is easy to check that trilinear functions 
satisfy the following interpolation or hexahedral subdivision rules: (i) The value of f at the midpoint 
of an edge of C3 is the average of its values at the endpoints of the edge. (ii) The value of f at the 
centroid of a face of C3 is the average of its values at the corners of the face.  (iii) The value of 
f at the centroid of C3 is the average of its values at the eight vertices of C3. If we subdivide the 
unit cube into 8 sub-cubes in the standard way, we can use these subdivision rules to determine the value 
of f at the vertices of each sub-cube. Repeatedly subdividing and apply­ing the subdivision rules yields 
the value of f at each diadic point (i/2J , j/2J , k/2J ) of C3. Because the diadic points are dense 
in C3, the subdivision rules completely determine f from its values at the vertices of C3. More generally, 
starting with values of a function at the vertices of the standard cubic tiling of R3 and applying the 
subdivision rules to each cubic cell determines a piecewise trilinear 3 function on R. We can generalize 
this construction to de.ne piecewise trilinear functions on any control lattice in which the vertices 
of each 3-cell of K have valence 3. Starting with the values of f at the vertices of K, we infer its 
values at the centroid of every edge, face and 3-cell of K. This gives values of f at every vertex of 
the re.ned complex K1 obtained by subdivision (see [MacCracken and Joy 1996] for details). Because the 
vertices of each 3-cell of K have valence 3, the subdivided complex K1 has only hexahedral cells, so 
after one subdivision, the subdivision process behaves just as for cubes in R3. There is a corresponding 
nested sequence of function spaces V0 . V1 . V2 . ... de.ned on K. To de.ne VJ , subdivide J-times to 
obtain the complex KJ and specify values at each vertex of KJ . The subdivision rules then determine 
a function on all of K. Thus, each function in VJ , for J = 0,1,2 ... , is determined by its values at 
the vertices of K. A.2 Derivatives of Elastic Potential The gradient and Hessian of V from equation 
(12) are: 2Aca 1 qa + Aac 2 qa + Bac qa · Cadc · qb) Ccab .V +2qd qa +(qa = ((1 )) (1 ) (27) · Cacd · 
Ccad · qb) Cbac .qc +qa qd 2+ qa qd 2 +(qa 2 · qb) Dabcd · qe) Dadce +qd (qa 1+ qa (qd 2 () 2Acf + Afc 
· Cafc + IBfc +2I qa 12 1 () +2qd . Cfdc +2Ccfb · Cfcd . qb + I qd 11 2 () .2V +qa . Cacf · Ccfd + qa 
. Ccaf = 2+ I qd 2 2 (28) .qc.qf +Cfac . qa + Cbfc · qb) Dabcf . qb + I (qa 22 1 +2 (qd . qa) Dafcd + 
I (qd · qe) Dfdce 12 +(qa . qd) Dadcf +(qa . qe) Dafce 22 where I is a 3 × 3 identity matrix and J(()) 
Aab 4G. .fa . .fb = dO 1 O 1-2..x .x () Aab J.fa . .fb =4GdO 2 O .x .x J Bab .fa .fb =4G · dO O .x .x 
J(b ) Cabc 4G. .fa .f.fc 1= · dO (29) O 1-2..x .x .x J() Cabc 4G .fa .fb .fc 2= · dO O .x .x .x J(b )(d 
)Dabcd 4G. .fa .f.fc .f = ·· dO 1 O 1-2..x .x .x .x J()() Dabcd .fa .fb .fc .fd =4G ·· dO 2 O .x .x .x 
.x Note that Aab is a 3 × 3 matrix, Cabc is a 3-vector, and Bab and Dabcd ii i are scalar quantities. 
 References AUBEL, A., AND THALMANN,D.2000.Realisticdeformationofhumanbodyshapes. In Proceedings of 
Computer Animation and Simulation 2000, 125 135. BANK, R. E. 1996. Hierarchical bases and the .nite element 
method, vol. 5 of Acta Numerica. Cambridge University Press, Cambridge, 1 43. BARAFF, D., AND WITKIN, 
A. 1992. Dynamic simulation of non-penetrating .exible bodies. Computer Graphics (Proceedings of SIGGRAPH 
92) 26, 2, 303 308. BARAFF, D., AND WITKIN, A. 1998. Large steps in cloth simulation. In Proceedings 
of SIGGRAPH 98, 43 54. BRO-NIELSEN, M., AND COTIN, S. 1996. Real-time volumetric deformable models for 
surgery simulation using .nite elements and condensation. Computer Graphics Forum (Proceedings of Eurographics 
96) 15, 3, 57 66. CAPELL, S., GREEN, S., CURLESS, B., DUCHAMP, T., AND POPOVI C´, Z. 2002. A multiresolution 
framework for dynamic deformations. University of Washington, Department of Computer Science and Engineering, 
Technical Report 02-04-02. CIRAK, F., AND ORTIZ, M. 2001. Fully c1-conforming subdivision elements for 
.nite deformation thin-shell analysis. International Journal for Numerical Methods in Engineering 51, 
7 (July), 813 833. DEBUNNE, G., DESBRUN, M., BARR, A., AND CANI, M.-P. 1999. Interactive multiresolution 
animation of deformable models. Eurographics Workshop on Ani­mation and Simulation. DEBUNNE, G., DESBRUN, 
M., CANI, M.-P., AND BARR, A. H. 2001. Dynamic real-time deformations using space &#38; time adaptive 
sampling. In Proceedings of SIGGRAPH 2001, 31 36. DESBRUN, M., SCHR ¨ ODER, P., AND BARR, A. 1999. Interactive 
animation of struc­tured deformable objects. Graphics Interface 99 (June), 1 8. FALOUTSOS, P., VAN DE 
PANNE, M., AND TERZOPOULOS, D. 1997. Dynamic free­form deformations for animation synthesis. IEEE Transactions 
on Visualization and Computer Graphics 3, 3 (July Sept.), 201 214. GORTLER, S. J., AND COHEN, M. F. 1995. 
Hierarchical and variational geometric modeling with wavelets. Symposium on Interactive 3D Graphics, 
35 42. GOURRET, J.-P., THALMANN, N. M., AND THALMANN, D. 1989. Simulation of object and human skin deformations 
in a grasping task. Computer Graphics (Pro­ceedings of SIGGRAPH 89) 23, 3 (July), 21 30. GRINSPUN, E., 
KRYSL, P., AND SCHR ¨ ODER, P. 2002. Charms: A simple framework for adaptive simulation. To appear in 
the Proceedings of SIGGRAPH 2002. HSU, W. M., HUGHES, J. F., AND KAUFMAN, H. 1992. Direct manipulation 
of free-form deformations. Computer Graphics (Proceedings of SIGGRAPH 92) 26, 2 (July), 177 184. JAMES, 
D. L., AND PAI, D. K. 1999. Artdefo -accurate real time deformable objects. Proceedings of SIGGRAPH 99 
(August), 65 72. KOCH, R. M., GROSS, M. H., CARLS, F. R., VON B¨ UREN, D. F., FANKHAUSER, G., AND PARISH, 
Y. 1996. Simulating facial surgery using .nite element methods. Proceedings of SIGGRAPH 96 (August), 
421 428. LEWIS, J. P., CORDNER, M., AND FONG, N. 2000. Pose space deformation: A uni.ed approach to shape 
interpolation and skeleton-driven deformation. In Pro­ceedings of SIGGRAPH 2000, 165 172. LI, X., WOON, 
T. W., TAN, T. S., AND HUANG, Z. 2001. Decomposing polygon meshes for interactive applications. In ACM 
Symposium on Interactive 3D Graph­ics, 35 42. MACCRACKEN, R., AND JOY, K. I. 1996. Free-form deformations 
with lattices of arbitrary topology. Computer Graphics (Proceedings of SIGGRAPH 96) 30, 181 188. METAXAS, 
D., AND TERZOPOULOS, D. 1992. Dynamic deformation of solid prim­itives with constraints. Computer Graphics 
(Proceedings of SIGGRAPH 92) 26,2 (July), 309 312. PENTLAND, A., AND WILLIAMS, J. 1989. Good vibrations: 
Modal dynamics for graphics and animation. Computer Graphics (Proceedings of SIGGRAPH 89) 23, 3 (July), 
215 222. PICINBONO, G., DELINGETTE, H., AND AYACHE, N. 2000. Real-time large dis­placement elasticity 
for surgery simulation: Non-linear tensor-mass model. In Pro­ceedings of the Third International Conference 
on Medical Robotics, Imaging and Computer Assisted Surgery: MICCAI 2000, 643 652. PLATT, J. C., AND BARR, 
A. H. 1988. Constraint methods for .exible models. Computer Graphics (Proceedings of SIGGRAPH 88) 22, 
4 (August), 279 288. PRENTER, P. M. 1975. Splines and Variational Methods. John Wiley and Sons. ROTH, 
S. H. M., GROSS, M. H., TURELLO, S., AND CARLS, F. R. 1998. A bernstein-b´ ezier based approach to soft 
tissue simulation. Computer Graphics Fo­rum 17, 3, 285 294. SEDERBERG, T. W., AND PARRY, S. R. 1986. 
Free-form deformation of solid geo­metric models. Computer Graphics (Proceedings of SIGGRAPH 86) 20, 
4 (Aug.), 151 160. SHABANA, A. 1998. Dynamics of Multibody Systems. Cambridge University Press. SINGH, 
K., AND KOKKEVIS, E. 2000. Skinning characters using Surface-Oriented Free-Form deformations. In Proceedings 
of the Graphics Interface 2000, 35 42. SLOAN, P.-P. J., ROSE, C. F., AND COHEN, M. F. 2001. Shape by 
example. In Symposium on Interactive 3D Graphics, 135 144. STOLLNITZ, E. J., DEROSE, T. D., AND SALESIN, 
D. H. 1996. Wavelets for Com­puter Graphics: Theory and Applications. Morgan Kaufmann, San Francisco, 
CA. TEICHMANN, M., AND TELLER, S. 1998. Assisted articulation of closed polygonal models. In Computer 
Animation and Simulation 98, 87 101. TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling inelastic deformation: 
Vis­coelasticity, plasticity, fracture. Computer Graphics (Proceedings of SIGGRAPH 88) 22, 4 (August), 
269 278. TERZOPOULOS, D., AND WITKIN, A. 1988. Physically based models with rigid and deformable components. 
IEEE Computer Graphics and Applications 8, 6 (Nov.), 41 51. TERZOPOULOS, D., PLATT, J., BARR, A., AND 
FLEISCHER, K. 1987. Elastically deformable models. Computer Graphics (Proceedings of SIGGRAPH 87) 21,4 
(July), 205 214. WILHELMS, J., AND GELDER, A. V. 1997. Anatomically based modeling. In Pro­ceedings of 
SIGGRAPH 97, 173 180. WITKIN, A., AND WELCH, W. 1990. Fast animation and control of nonrigid struc­tures. 
Computer Graphics (Proceedings of SIGGRAPH 90) 24, 4 (August), 243 252.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566623</article_id>
		<sort_key>594</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Robust treatment of collisions, contact and friction for cloth animation]]></title>
		<page_from>594</page_from>
		<page_to>603</page_to>
		<doi_number>10.1145/566570.566623</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566623</url>
		<abstract>
			<par><![CDATA[We present an algorithm to efficiently and robustly process collisions, contact and friction in cloth simulation. It works with any technique for simulating the internal dynamics of the cloth, and allows true modeling of cloth thickness. We also show how our simulation data can be post-processed with a collision-aware subdivision scheme to produce smooth and interference free data for rendering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[cloth]]></kw>
			<kw><![CDATA[collision detection]]></kw>
			<kw><![CDATA[collision response]]></kw>
			<kw><![CDATA[contacts]]></kw>
			<kw><![CDATA[kinetic friction]]></kw>
			<kw><![CDATA[physically based animation]]></kw>
			<kw><![CDATA[static friction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP309772500</person_id>
				<author_profile_id><![CDATA[81100248660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bridson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14211105</person_id>
				<author_profile_id><![CDATA[81100612327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fedkiw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University and Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP45023346</person_id>
				<author_profile_id><![CDATA[81314493471]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D., AND WITKIN, A. 1992. Dynamic simulation of non-penetrating flexible bodies. In Proc. of SIGGRAPH 1992, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 303-308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D., AND WITKIN, A. 1994. Global methods for simulating contacting flexible bodies. In Computer Animation Proc., Springer-Verlag, 1-12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D., AND WITKIN, A. 1998. Large steps in cloth simulation. In Proc. of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 1-12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74356</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D. 1989. Analytical methods for dynamic simulation of non-penetrating rigid bodies. In Proc. of SIGGRAPH 1989, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D. 1990. Curved surfaces and coherence for non-penetrating rigid body simulation. In Proc. of SIGGRAPH 1990, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122722</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D. 1991. Coping with friction for non-penetrating rigid body simulation. In Proc. of SIGGRAPH 1991, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 31-40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D. 1993. Issues in computing contact forces for non-penetrating rigid bodies. Algorithmica, 10, 292-352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D. 1994. Fast contact force computation for nonpenetrating rigid bodies. In Proc. of SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D. 2001. Collision and contact. In SIGGRAPH 2001 Course Notes, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[BAREQUET, G., CHAZELLE, B., GUIBAS, L., MITCHELL, J., AND TAL, A. 1996. BOXTREE: A hierarchical representation for surfaces in 3D. Comp. Graphics Forum 15, 3, 387-396.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192259</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[BREEN, D. E., HOUSE, D. H., AND WOZNY, M. J. 1994. Predicting the drape of woven cloth using interacting particles. In Proc. of SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 365-372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>302658</ref_obj_id>
				<ref_obj_pid>302638</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[CARAMANA, E., BURTON, D., SHASHKOV, M., AND WHALEN, P. 1998. The construction of compatible hydrodynamics algorithms utilizing conservation of total energy. Journal of Computational Physics 146, 227-262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134017</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[CARIGNAN, M., YANG, Y., MAGNENAT-THALMANN, N., AND THALMANN, D. 1992. Dressing animated synthetic actors with complex deformable clothes. In Proc. SIGGRAPH 1992, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 99-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344882</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[CHENNEY, S., AND FORSYTH, D. A. 2000. Sampling plausible solutions to multi-body constraint problems. In SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, Comp. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[DEROSE, T., KASS, M., AND TRUONG, T. 1998. Subdivision surfaces in character animation. In Proc. SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 85-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., AND GASCUEL, M.-P. 1994. Highly deformable material for animation and collision processing. In 5th Eurographics worshop on animation and simulation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351638</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., SCHR&#214;DER, P., AND BARR, A. 1999. Interactive animation of structured deformable objects. In Graphics Interface, 1-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[DOGHRI, I., MULLER, A., AND TAYLOR, R. L. 1998. A general three-dimensional contact procedure for finite element codes. Engineering Computations 15, 2, 233-259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166157</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[GASCUEL, M.-P. 1993. An implicit formulation for precise contact modeling between flexible solids. In SIGGRAPH 1993, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 313-320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[GOTTSCHALK, S., LIN, M. C., AND MANOCHA, D. 1996. Obbtree: a hierarchical structure for rapid interference detection. In Proc. of SIGGRAPH 1996, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 171-179.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74335</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[GOURRET, J.-P., MAGNENAT-THALMANN, N., AND THALMANN, D. 1989. Simulation of object and human skin deformations in a grasping task. In Proc. of SIGGRAPH 1989, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 21-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601723</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[GRINSPUN, E., AND SCHR&#214;DER, P. 2001. Normal bounds for subdivision-surface interference detection. In Proc. of IEEE Scientific Visualization, IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378530</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[HAHN, J. K. 1988. Realistic animation of rigid bodies. In Proc. of SIGGRAPH 1988, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97883</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[HERZEN, B. V., BARR, A. H, AND ZATZ, H. R. 1990. Geometric collisions for time-dependent parametric surfaces. In Proc. of SIGGRAPH 1990, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 39-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>350448</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[HOUSE, D. H., AND BREEN, D. E., Eds. 2000. Cloth modeling and animation. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[HOWLETT, P., AND HEWITT, W. T. 1998. Mass-spring simulation using adaptive non-active points. In Computer Graphics Forum, vol. 17, 345-354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[HUGHES, T. J. R. 1987. The finite element method: linear static and dynamic finite element analysis. Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[JIMENEZ, S., AND LUCIANI, A. 1993. Animation of interacting objects with collisions and prolonged contacts. In Modeling in computer graphics---methods and applications, Springer-Verlag, B. Falcidieno and T. L. Kunii, Eds., Proc. of the IFIP WG 5.10 Working Conference, 129-141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[KANE, C., REPETTO, E., ORTIZ, M., AND MARSDEN, J. 1999. Finite element analysis of nonsmooth contact. Comput. Methods Appl. Mech. Eng. 180, 1-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[LAFLEUR, B., MAGNENAT-THALMANN, N., AND THALMANN, D. 1991. Cloth animation with self-collision detection. In Proc. of the Conf. on Modeling in Comp. Graphics, Springer, 179-187.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[LIN, M., AND GOTTSCHALK, S. 1998. Collision detection between geometric models: A survey. In Proc. of IMA Conf. on Mathematics of Surfaces.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[LOOP, C. 2001. Triangle mesh subdivision with bounded curvature and the convex hull property. Tech. Rep. MSR-TR-2001-24, Microsoft Research.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[MARHEFKA, D. W., AND ORIN, D. E. 1996. Simulation of contact using a nonlinear damping model. In Proc. of the 1996 IEEE Int'l Conf. on Robotics and Automation, IEEE, 1662-1668.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383263</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[MILENKOVIC, V. J., AND SCHMIDT, H. 2001. Optimization-based animation. In Proc. of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199436</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[MIRTICH, B., AND CANNY, J. 1995. Impulse-based simulation of rigid bodies. In Proc. of 1995 symposium on interactive 3d graphics, 181-188, 217.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344866</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[MIRTICH, B. 2000. Timewarp rigid body simulation. In Proc. of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 193-200.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[MOORE, M., AND WILHELMS, J. 1988. Collision detection and response for computer animation. In SIGGRAPH 1988, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 289-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618376</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[NG, H. N., AND GRIMSDALE, R. L. 1996. Computer graphics techniques for modeling cloth. IEEE Computer Graphics and Applications, 28-41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical modeling and animation of brittle fracture. In SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 137-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134019</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[OKABE, H., IMAOKA, H., TOMIHA, T., AND NIWAYA, H. 1992. Three dimensional apparel CAD system. In SIGGRAPH 1992, ACM Press/ACM SIGGRAPH, Comp. Graphics Proc., 105-110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[PANDOLFI, A., KANE, C., MARSDEN, J., AND ORTIZ, M. 2002. Time-discretized variational formulation of non-smooth frictional contact. Int. J. Num. Methods in Eng. 53, 1801-1829.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[PROVOT, X. 1995. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In Graphics Interface, 147-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[PROVOT, X. 1997. Collision and self-collision handling in cloth model dedicated to design garment. Graphics Interface, 177-89.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>829576</ref_obj_id>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[SAAD, Y. 1996. Iterative methods for sparse linear systems. PWS Publishing. New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[SIMS, K. 1994. Evolving virtual creatures. In SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, Comp. Graphics Proc., 15-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND FLEISCHER, K. 1988. Deformable models. The Visual Computer, 4, 306-331.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling inelastic deformation: viscoelasticity, plasticity, fracture. In Proc. of SIGGRAPH 1988, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 269-278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>102333</ref_obj_id>
				<ref_obj_pid>102313</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND WITKIN, A. 1988. Physically based models with rigid and deformable components. In Graphics Interface, 146-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 1987. Elastically deformable models. In SIGGRAPH 1987, ACM Press/ACM SIGGRAPH, Comp. Graphics Proc., 205-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91430</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[THINGVOLD, J. A., AND COHEN, E. 1992. Physical modeling with B-spline surfaces for interactive design and animation. In Proc. of SIGGRAPH 1992, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 129-137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., AND MAGNENAT-THALMANN, N. 1994. Efficient self-collision detection on smoothly discretized surface animations using geometrical shape regularity. In Proc. of Eurographics, vol. 13 of Computer Graphics Forum, Eurographics Association, C-155-166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., AND MAGNENAT THALMANN, N. 1995. Collision and self-collision detection: Efficient and robust solutions for highly deformable surfaces. In Comp. Anim. and Simulation, Springer-Verlag, D. Terzopoulos and D. Thalmann, Eds., 55-65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836188</ref_obj_id>
				<ref_obj_pid>832294</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., AND MAGNENAT-THALMANN, N. 1997. Developing simulation techniques for an interactive clothing system. In Proc. of the 1997 International Conf. on Virtual Systems and MultiMedia, IEEE, 109-118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218432</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., COURCHESNE, M., AND MAGNENAT-THALMANN, N. 1995. Versatile and efficient techniques for simulating cloth and other deformable objects. In Proc. of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872899</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., COURCHESNE, M., AND MAGNENAT-THALMANN, N. 2000. Accurate collision response on polygonal meshes. In Proc. of Computer Graphics, 179-188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>135556</ref_obj_id>
				<ref_obj_pid>129873</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[WEBB, R., AND GIGANTE, M. 1992. Using dynamic bounding volume hierarchies to improve efficiency of rigid body simulations. In Comm. with Virtual Worlds, CGI Proc. 1992, 825-841.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Robust Treatment of Collisions, Contact and Friction for Cloth Animation Robert Bridson Ronald Fedkiw 
John Anderson Stanford University Stanford University Industrial Light &#38; Magic rbridson@stanford.edu 
Industrial Light &#38; Magic janders@pixar.com fedkiw@cs.stanford.edu Abstract We present an algorithm 
to ef.ciently and robustly process colli­sions, contact and friction in cloth simulation. It works with 
any technique for simulating the internal dynamics of the cloth, and allows true modeling of cloth thickness. 
We also show how our simulation data can be post-processed with a collision-aware sub­division scheme 
to produce smooth and interference free data for rendering. CR Categories: I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism Animation; I.3.5 [Computer Graphics]: Computational Geometry and 
Object Modeling Physically based modeling; Keywords: cloth, collision detection, collision response, 
contacts, kinetic friction, static friction, physically based animation 1 Introduction Collisions are 
a major bottleneck in cloth simulation. Since all points are on the surface, all points may potentially 
collide with each other and the environment in any given time step. Moreover, for believable animation 
the number of particles is generally in the tens of thousands or more. Since cloth is very thin, even 
small inter­penetrations can lead to cloth protruding from the wrong side. This is visually disturbing 
and can be dif.cult to correct after the fact either in the next time step or in post-processing. While 
rigid body simulations often have relatively few collisions per object (apart from resting contact), 
deformable bodies naturally give rise to large numbers of collisions varying in strength from resting 
contact to high speed impact. Two-dimensional manifolds like cloth are the worst case. Na¨ive methods 
for detecting and stopping every colli­sion can quickly grind the simulation to a halt. This paper presents 
a collision handling algorithm that works with any method for simulating the internal dynamics (i.e. 
stretch­ing, shearing, and bending) to ef.ciently yet robustly produce vi­sually complex motion free 
from interference as in .gure 1. The key idea is to combine a fail-safe geometric collision method with 
a fast (non-stiff) repulsion force method that models cloth thickness as well as both static and kinetic 
friction. Ever since [Moore and Wilhelms 1988] proposed that repulsion forces are useful for con­tact 
whereas exact impulse-based treatment is useful for high veloc- Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Figure 1: Initially, a curtain 
is draped over a ball on the ground. The ball moves, .ipping the curtain over on top of itself pro­ducing 
stable folds and wrinkles using our static friction model. Another ball enters the scene and pushes through 
the compli­cated structure of folds before slipping underneath unraveling the curtain. ity impact, authors 
have toyed with using both. For example, [Sims 1994] switched between instantaneous impulses for high 
velocities and penalty spring forces for low velocities to treat his evolving articulated rigid body 
creatures. Although similar in spirit to our approach, we always use both techniques in a fully hybridized 
and ef.cient manner. We view repulsion forces, e.g. during resting contact, as a way to deal with this 
vast majority of collisions in a simple and ef.cient manner allowing us to use a more expensive but completely 
robust method to stop the few that remain. Since our repulsion forces han­dle most of the self-interaction, 
it is desirable to make them compu­tationally ef.cient to apply. Therefore we propose using a repulsion 
spring model that is not stiff. In contrast, many authors use compu­tationally expensive stiff repulsion 
springs, e.g. with force inversely proportional to separation distance, since they do not have a robust 
alternative for stopping any remaining collisions. See e.g. [Moore and Wilhelms 1988; La.eur et al. 1991; 
Baraff and Witkin 1998], although we note that some of the dif.culties associated with the stiffness 
of the repulsion springs was partially alleviated by using an implicit method for the time integration 
in [Baraff and Witkin 1998]. Our robust geometric collision algorithm is the .rst scheme that guarantees 
no dynamic self-interference of cloth. [Moore and Wil­helms 1988] started in this direction proposing 
a hard-to-solve .fth order polynomial to detect point-face collisions. This was aban­doned by the community 
until [Provot 1997] reduced it to a cubic and extended it to treat edge-edge collisions. However, Provot 
did not properly account for rounding errors, so self-intersection could still occur. Generally, these 
dif.culties led the community to allow self-intersection, and then attempt to detect and correct it after 
the fact. For example, [Volino and Magnenat Thalmann 1995; Volino et al. 1995; Volino and Magnenat-Thalmann 
1997; Volino et al. 2000] proposed a number of methods such as most probable ori­entations for collisions, 
i.e. nodes vote on which side they would like to be on and the majority wins. Although this gives no 
guar­antee of physical consistency, or that the method even works, they did produce convincing simulations 
of a ribbon folding, garments crumpling in a dryer, and a stack of cloth. The complexity involved with 
unraveling self-intersection has led many to use large repul­sion forces to keep the cloth well separated, 
but this leads to visual artifacts with cloth seemingly .oating over itself at large distances with little 
or no friction. Since our method avoids nonphysical self-interference altogether, we do not need large 
repulsion forces or complicated and unreliable algorithms for detecting and .xing self-intersection. 
A further advantage over methods that allow self­intersection, even when they succeed in recovering from 
it, is that our cloth is always properly constrained. This is necessary to cap­ture the bulk and small 
scale crumpling apparent in complex folds. A key ingredient of our new algorithm is that we do not work 
directly with positions but only obtain positions by integrating ve­locities. Thus, given a current non-interfering 
state for our cloth, the collision handling problem can be reduced to .nding velocities that guarantee 
a non-interfering state at the end of the time step. Moreover, given a current non-interfering state 
and a proposed set of new positions at the end of the time step, under a linear trajec­tory assumption 
we can compute a velocity to be used along with the initial state in our collision processing algorithm. 
This allows us to cleanly separate the time evolution of the internal cloth dynamics (and the environment 
around the cloth) from the collision process­ing algorithm. That is, the result of the time evolution 
is merely used as an initial guess for the .nal position of the cloth. Then this initial guess is modi.ed 
to account for any collisions. This allows us to easily integrate our collision, contact and friction 
processing algorithm with a pre-existing cloth dynamics engine. Our approach to static and kinetic friction 
is based on the re­pulsion forces and is trivial to apply even for cloth-cloth interac­tion. Correctly 
preventing self-intersection and modeling static and kinetic friction, especially for cloth-cloth contacts, 
is essential for producing the detailed time-evolving folds and wrinkles exhibited by cloth. Our treatment 
gives highly realistic folds and wrinkles as demonstrated in the .gures. 2 Other Work There is a rich 
history of research on contact and collisions in the graphics community, and we cannot possibly cover 
it all due to space limitations. However, we will mention a number of these works where appropriate throughout 
the text. For example, [Baraff and Witkin 1998; Provot 1995; Provot 1997; Volino et al. 1995; Jimenez 
and Luciani 1993; Moore and Wilhelms 1988] are cited a number of times. Baraff carried out a detailed 
study of numerical methods for rigid body motion with contact and friction in a series of papers [Baraff 
1989; Baraff 1990; Baraff 1991; Baraff 1993; Baraff 1994]. [Gour­ret et al. 1989] simulated a hand grasping 
an object using a .nite element model for both the hand and the object. They detected collision as the 
overlapping of volumetric objects and subsequently treated collision, contact and friction based on the 
size of the over­lap (including reactive repulsive forces). [Mirtich and Canny 1995] showed that one 
could produce physically plausible results model­ing colliding, rolling, sliding, and resting contact 
for rigid bodies as a series of collision impulses, or micro-collisions. For exam­ple, a block sitting 
on a table experiences many micro-collisions keeping it from sinking into the table. They used an in.nitesi­mal 
collision time, Poisson s hypothesis, and a Coulomb friction model. [O Brien and Hodgins 1999] used a 
.nite element model to simulate elastic brittle objects producing impressive animations of fracture. 
Collisions were detected via static interpenetration and resolved with penalty forces. Although the penalty 
forces could be stiff, they stated that their .nite element model was relatively just as stiff dictating 
a small time step anyhow. Although we use triangles to represent our surface, other rep­resentations 
may be used. [Herzen et al. 1990] addressed colli­sions between parametric surfaces, [Grinspun and Schr¨oder 
2001] worked with subdivision surfaces, and an implicit surface formula­tion of contact and collision 
processing was demonstrated in [Gas­cuel 1993; Desbrun and Gascuel 1994]. 3 Cloth Model Since this paper 
is concerned with collisions, particularly self­collisions, we do not address internal cloth dynamics. 
Those in­terested in cloth modeling are referred to the survey article of [Ng and Grimsdale 1996] and 
the book of [House and Breen 2000]. We also make speci.c mention of the CAD apparel system in [Okabe 
et al. 1992], the work of [Breen et al. 1994] using experimentally determined measurements for cloth 
properties, and the seminal pa­pers of [Terzopoulos et al. 1987; Terzopoulos and Fleischer 1988a; Terzopoulos 
and Fleischer 1988b; Terzopoulos and Witkin 1988] on constitutive modeling of deformable bodies for computer 
graph­ics. In addition, [Baraff and Witkin 1998] proposed an implicit time stepping method and generated 
convincing results despite dropping nonsymmetric terms from their matrix. Further approximations were 
made in [Desbrun et al. 1999], e.g. violating local preser­vation of angular momentum, in order to obtain 
interactive rates while sacri.cing a little realism. For the purposes of demonstrating our collision 
handling, we use a simple mass-spring model for the internal cloth dynamics, as op­posed to a true constitutive 
model. However, in .gure 4 we also illustrate the ef.cacy of our approach with a highly sophisticated 
model from an industrial production system. In our basic model, particles are arranged in a rectangular 
array with structural springs connecting immediate neighbors. Diagonal springs provide shear support, 
and springs connected to every other node (with a stabi­lization spring attached to the center node in 
between) resist bend­ing. We make the edges and corners of the cloth slightly heavier by giving all particles 
the same mass instead of a mass proportional to the area of the surrounding cloth.1 The heavier edges 
and corners give the cloth an attractive .are similar to that of real cloth where tailors often make 
hems a little heavier. This basic cloth model has been used by many authors, e.g. [Provot 1995]. 1The 
equal mass assumption also simpli.es many of the formulas pre­sented in this paper. Generalizing to the 
unequal mass case is straightfor­ward. 4 Limiting the Strain and Strain Rate Sometimes triangles are 
undesirably stretched or compressed by large percentages. A rule of thumb in computational mechanics 
is that a triangle edge should not change length by more than 10% in a single time step, see e.g. [Caramana 
et al. 1998]. This can be enforced by either adaptively decreasing the time step or nonphysi­cally decreasing 
the strain rate. This rule of thumb is generally used to obtain better accuracy, as opposed to stability, 
and thus it is used in conjunction with implicit time stepping algorithms as well, see e.g. [Baraff and 
Witkin 1998]. [Provot 1995] addressed this issue in a novel way processing the cloth after each time 
step with an iterative algorithm that repairs ex­cessively deformed triangles. This algorithm focused 
on the overall strain as opposed to the strain rate (although [Provot 1995] mistak­enly referred to this 
as the deformation rate). [Provot 1995] looped through the mesh changing the positions of the nodes on 
edges that had deformed by over 10%. Since adjusting the position of one node affects the length of all 
the edges containing it, an iterative process was used. Good results were obtained even though the al­gorithm 
does not converge in certain situations, e.g. when all the boundaries of the cloth mesh have constrained 
(.xed) positions that force an expansion beyond 10%. [Provot 1995] illustrated that this iterative method 
was signi.cantly more ef.cient than arbitrarily in­creasing the spring stiffness when one is dissatis.ed 
with the degree of mesh deformation in a numerical simulation. Although this method seems to work well, 
it involves moving nodes and can therefore induce self-intersection in the mesh. Thus, to .t this method 
into the framework of our collision processing algorithm, we adjust velocities instead of positions. 
At each time step, we calculate the candidate new spring lengths using the cur­rent velocities. Then 
we apply momentum-conserving corrective impulses to the velocities to ensure that all springs are deformed 
by a maximum of 10% from their rest length at the end of the time step (ignoring bending springs). These 
impulses in.uence the fu­ture strains of surrounding springs, and thus an iterative procedure is needed 
to guarantee that no spring deforms to over 10% of its rest length during the time step. This is essentially 
equivalent to using biphasic springs with a much stiffer spring constant beyond 10% deformation, and 
the iterative procedure is similar to using implicit time stepping when the stiffer spring constant is 
activated. This mimics the physical behavior of cloth (and skin!), which offers lit­tle resistance to 
small deformations but becomes stiff after a critical deformation is reached. We apply this deformation 
limiting procedure using a Jacobi iter­ative approach (parallel rather than sequential), and although 
con­vergence is not guaranteed, generally only one or two iterations per time step are required for visually 
pleasing results. Although a Gauss-Seidel iterative approach (sequential rather than parallel) generally 
converges faster, it can introduce a noticeable bias accord­ing to which parts of the cloth are updated 
.rst (although this can be mitigated to some degree by using random orderings). Moreover, Jacobi style 
iteration is easier to parallelize for high performance. In addition to the strain, we also limit the 
strain rate according to the rule of thumb mentioned above. Although, this is usually done by adaptively 
reducing the time step, these smaller time steps can lead to a loss of ef.ciency. To avoid slowing the 
simulation, we continually monitor the strain rate and use momentum-conserving impulses to reduce it 
so that springs do not change their current length by more than 10% during a time step. This trade-off 
of ac­curacy for ef.ciency does not seem to induce any unwanted visual artifacts and is similar to the 
traditional damping of an implicit time discretization of the equations. We use a Gauss-Seidel iterative 
ap­proach in order to accelerate convergence. Only a few iterations are needed as the fastest deforming 
edges are rapidly damped to reason­able deformation rates. Convergence is not required since we can still 
adaptively reduce the time step, and after only a few iterations only a moderate reduction of the time 
step is necessary. [Volino et al. 1995] proposed a philosophically similar technique that mon­itors local 
mechanical energy variations and arti.cially distributes kinetic energy through momentum transfers in 
regions where insta­bility might occur. Similarly, [Baraff and Witkin 1998] used their implicit time 
integration scheme to automatically damp the local energy generated by their treatment of collisions. 
 5 Time Discretization We cleanly separate the time evolution of the internal cloth dynam­ics (and the 
environment around the cloth) from the collision pro­cessing algorithm. This allows us to easily integrate 
our collision, contact and friction processing algorithms with a pre-existing cloth dynamics engine. 
Starting at time t = 0 with cloth positions x0 and velocities v0, the algorithm is as follows. For n 
= 0, 1,2,... Select a collision time step size .t and set tn+1 = tn + .t  Advance to candidate positions 
x¯n+1 and velocities v¯n+1 at time tn+1 with the cloth internal dynamics  Compute the average velocity 
v¯n+1/2 =(x¯n+1 - xn)/.t  Check xn for proximity (section 6), then apply repulsion im­  pulses (section 
7.2) and friction (section 7.3) to the average vn+1/2 velocity to get Check linear trajectories from 
xn with v n+1/2 for collisions (section 6), resolving them with a .nal midstep velocity vn+1/2 (sections 
7.4 and 7.5)  Compute the .nal positions xn+1 = xn + .tvn+1/2 vn+1  If there were no repulsions or 
collisions, set vn+1 = ¯  Otherwise, advance the midstep velocity vn+1/2 to vn+1 (sec­tion 7.6)  When 
repulsions or collisions appear, our method for deter­mining the .nal velocities is essentially central 
time differencing [Hughes 1987]. In fact, we use central time differencing for our in­ternal cloth dynamics 
as well, though we stress that any reasonable algorithm could be used for that purpose, e.g. one large 
implicit time step as in [Baraff and Witkin 1998] or many small explicit Runge-Kutta steps. The algorithm 
is stable for any collision time step .t, thus .t can be chosen adaptively in a straightforward manner. 
For example, we choose a minimum .t as the time step of the cloth dynamics evolu­tion and a maximum .t 
on the order of one frame, and start with the maximum. We halve the time step when an actual collision 
occurs, i.e. the repulsion forces aren t adequate, and try the time step over again only doing the full 
collision processing at the minimum .t. Whenever we get three successful time steps in a row we double 
.t again. Adaptive time stepping was also addressed in [Baraff and Witkin 1998]. 6 Proximity and Collision 
Detection To accelerate the detection of proximities for repulsions and of in­tersections for collisions, 
we use an axis-aligned bounding box hier­archy. It is built bottom-up once at the beginning of the simulation 
using the topology of the cloth mesh. In one sweep we greedily pair off adjacent triangles to get parent 
nodes, then in a sweep in the opposite direction pair off these to get the next level up, and so on alternating 
sweep directions until we are left with one root node. At each time step we calculate the extents of 
the axis-aligned boxes for the repulsion calculation (section 7.2) by taking a box around each triangle 
enlarged by the thickness of the cloth (e.g. 10-3m or 1mm), and then taking the union of the extents 
in each axis direction as we move up the hierarchy. We also recalculate the hierarchy for each iteration 
of the collision algorithm (section 7.4) by taking a box around each triangle and its candidate position 
at the end of the step (since we have to cover the path that the triangle takes during the time step) 
enlarged by the rounding error tolerance (e.g. 10-6m), again merging as we move up the hierarchy. In 
both cases, we get a set of candidates for interference by intersecting the box to be tested with the 
root of the tree; only if it overlaps do we recursively check its children, proceeding until we reach 
the leaves of the tree. The leaves we reach indicate on which triangles the ac­tual geometry tests need 
to be performed. These tests break down into checking points against triangular faces and edges against 
other edges (naturally we don t compare a point against the triangle that contains it, or an edge against 
an edge that shares an endpoint). For more details on hierarchical methods and bounding volume hier­archies, 
see [Hahn 1988; Webb and Gigante 1992; Barequet et al. 1996; Gottschalk et al. 1996; Lin and Gottschalk 
1998]. Further pruning of unnecessary tests between adjacent patches in low cur­vature regions of cloth 
is possible, at least for static proximity tests, see [Volino and Magnenat-Thalmann 1994; Volino et al. 
1995]. In what follows we use the shorthand xi j to mean xi - xj. To check if a point x4 is closer than 
some thickness h to a triangle x1 x2 x3 with normal n we .rst check if the point is close to the plane 
containing the triangle: | x43 ·n | < h. If so, we project the point onto the plane and compute the barycentric 
coordinates w1,w2,w3 with respect to the triangle: x13 · x13 x13 · x23 w1 = x13 · x43 x13 · x23 x23 
· x23 w2 x23 · x43 w1 + w2 + w3 = 1. These are the normal equations for the least-squares problem of 
.nding the point w1 x1 + w2 x2 + w3 x3 (in the plane) closest to x4. If the barycentric coordinates are 
all within the interval [-d ,1 + d ] where d is h divided by a characteristic length of the triangle, 
the point is close. To check if an edge x1 x2 is close to another edge x3 x4 we .nd the pair of points, 
one on each edge, that are closest and check their distance. The search for the two closest points begins 
by check­ing for the degenerate case of parallel edges, i.e. if | x21 × x43| is smaller than a round-off 
tolerance. If so, it reduces to a simple one­dimensional problem. Otherwise, we .nd the points on the 
in.nite lines that are closest via the normal equations: x21 · x21 - x21 · x43 a = x21 · x31 - x21 · 
x43 x43 · x43 b - x43 · x31 . If these points are on the .nite edges we are done, otherwise we clamp 
them to the endpoints. The point that moved the most during clamping is guaranteed to be one part of 
the answer, and the second point is found by projecting the .rst onto the second in.nite line and clamping 
to the .nite edge. The direction pointing from one point to the other is saved as the normal vector. 
We also save their relative positions along the edges, i.e. the weights 0 = a,b = 1 so that the points 
are x1 + a x21 and x3 + b x43. To detect a collision between a moving point and a moving trian­gle, or 
between two moving edges, we .rst .nd the times at which the four points are coplanar assuming they move 
with constant ve­locity over the collision time step as in [Moore and Wilhelms 1988; Provot 1997; Doghri 
et al. 1998]. The latter two showed this re­duces to .nding the roots of a cubic equation, ( x21 +t v21) 
× ( x31 + t v31) · ( x41 + t v41)= 0. Any roots outside of the interval [0,.t] are discarded, and then 
the remaining roots are checked in increasing order with proximity tests at time t. If the elements are 
closer than a small rounding error tol­erance (10-6m for our simulations, which is 1000 times smaller 
than the cloth thickness), we register a collision. We likewise check for proximity at the end of the 
time step, t = .t, in case rounding er­rors hide a collision at the boundary between two time steps. 
While earlier works neglected rounding error, our approach guarantees (if collisions are resolved) that 
the cloth is separated by at least the rounding error tolerance at every time step and never self-intersects 
during time steps. 7 Contact and Collision Response 7.1 Interpolating within the cloth We often need 
to deal with two points from the cloth, computing their relative velocity or applying an impulse to them. 
However, we cannot directly look up or alter the velocities of such points, and instead must deal with 
the corners of the triangle or endpoints of the edges. To compute the velocity of a point interior to 
a triangle or edge we use linear interpolation, which is exact for af.ne deformations (i.e. translation, 
rotation, and af.ne shearing and scaling). In partic­ular, if a point interior to a triangle x1 x2 x3 
has barycentric coordi­nates w1,w2,w3 (calculated during proximity or collision detection) its interpolated 
velocity is w1 v1 + w2 v2 + w3 v3, and similarly if a point interior to an edge x1 x2 is the fraction 
a along the edge then its interpolated velocity is (1 -a) v1 + a v2. Note that we are .nding the velocity 
of a speci.c piece of material involved in a contact or collision, i.e. the weights wi or a are .xed 
so their time derivatives do not appear. If an impulse of magnitude I in direction n needs to be applied 
to two points in the cloth (i.e. In to the .rst and -In to the second), we instead apply impulses to 
the triangle corners or edge endpoints, weighted by the barycentric coordinates, so that the desired 
change in relative (interpolated) velocity for the two points is achieved. For the point-triangle case, 
where an interior point of triangle x1 x2 x3 with barycentric coordinates w1,w2,w3 is interacting with 
point x4, we compute adjusted impulses I = 2I 222 1+w 1+w2+w3 new v= vi + wi(I /m)n i = 1,2,3 i new 
v= v4 - (I /m)n 4 assuming all nodes have mass m. For the edge-edge case where a point with relative 
position a along the edge x1 x2 interacts with a point with relative position b along the edge x3 x4, 
the adjusted impulses are I = 2I a2+(1-a)2+b2+(1-b)2 new new v= v1 +(1 - a)(I /m)n v= v2 + a(I /m)n 12 
new new v= v3 - (1 - b)(I /m)n v= v4 - b(I /m)n . 34 Weighting the impulses in this way introduces appropriate 
torques for off-center interactions as well as giving continuity across trian­gle boundaries, and converges 
to the expected formulas when the interior points approach mesh nodes. 7.2 Repulsions Resolving the 
tens of thousands of collisions that can readily oc­cur in folding and contact situations can be prohibitively 
expensive. This is why repulsion forces are mandatory. They dramatically re­duce the number of collisions 
usually eliminating them altogether making our collision processing algorithm not only tractable but 
ef.cient. Our cloth is given a realistic thickness, e.g. 1mm, and re­pulsion forces are only applied 
between pieces of cloth that have this close proximity. As discussed in section 6, we use an axis­aligned 
bounding box hierarchy to make this proximity detection ef.cient. Proximity is determined for both point-triangle 
pairs and edge-edge pairs. If a pair is close enough, then two kinds of repul­sion forces are applied. 
The .rst is based on an inelastic collision, and the second is a spring based force. [Baraff and Witkin 
1992; Baraff and Witkin 1994] discussed col­lision modeling for deformable bodies pointing out that when 
a dis­cretized rod collides with a wall, the endpoint of the rod should come impulsively to rest, i.e. 
the endpoint is subject to a completely inelastic collision impulse. Subsequently the rod stores energy 
in compression and then expands, separating from the wall. The load­ing and unloading of the elastic 
rod models a completely elastic collision. They pointed out that inelasticity can only be introduced 
by adding damping forces internal to the rod that dissipate energy due to the collision. Other authors 
have used completely inelas­tic collisions as well, such as [Desbrun et al. 1999] and [Carignan et al. 
1992] (who used completely inelastic collisions to remove the kicks generated by the repulsion springs 
of [La.eur et al. 1991]). We take a similar approach, modeling all cloth-cloth collisions and cloth-object 
collisions using an identically zero restitution coef.­cient in Poisson s hypothesis. The energy stored 
in deformations of our mass-spring model when one of the nodes abruptly comes to rest against an object 
in its path is released as the cloth restores itself, causing it to bounce. In fact, most real-world 
cloth-cloth and cloth-object collisions are fairly inelastic, so even with a zero resti­tution coef.cient 
one should take care to monitor the energy stored within the cloth. We do this intrinsically by limiting 
the strain rate as discussed in section 4. Since our repulsion forces are meant to dramatically reduce 
the number of subsequent collisions, we incorporate a completely in­elastic collision into our repulsion 
force. Suppose two points in the cloth, one inside a triangle and one a node of the mesh or both in­terior 
to mesh edges, are close and have relative velocity vN in the normal direction which is less than zero, 
i.e. they are approaching each other. (See section 7.1 above for details on interpolating ve­locities 
interior to triangles and edges, and section 6 for details on the normal direction used in point-triangle 
and edge-edge interac­tions.) To stop the imminent collision we apply an inelastic impulse of magnitude 
Ic = mvN /2 in the normal direction. (See section 7.1 for how we distribute the impulse to the mesh nodes 
involved.) Since our cloth model includes a realistic cloth thickness, we would like to ensure that pieces 
of the cloth are well separated at a distance on the order of this cloth thickness. This helps cloth 
to slide over itself (and objects) without the excessive snagging caused by the discretization errors 
resulting from the representa­tion of smooth surfaces with discrete triangles. When pieces of cloth are 
too close together, there is a compression of cloth .bers, and a second repulsion force is applied to 
model this compression. The repulsion force is proportional to the overlap beyond the cloth thickness 
h (e.g. 1mm). However, since our robust collision han­dling algorithm (see section 7.4) will stop every 
intersection, our spring repulsion force is limited to a maximum when the objects touch, thus avoiding 
problems with stiffness. Furthermore, we limit our repulsion force so that objects are never propelled 
outside this overlap region in a single time step. This not only helps to reduce stiffness, but allows 
cloth in contact to stay close enough together to feel repulsion forces in subsequent time steps. This 
is important since the repulsion force magnitude is used to model friction, and thus friction forces 
are also felt in future time steps producing stable folds and wrinkles that add to the visual realism. 
Many other au­thors have used spring based repulsion forces, see e.g. [Jimenez and Luciani 1993; Marhefka 
and Orin 1996], but their methods suffered from undue stiffness since an arbitrary amount of interpenetration 
was allowed to occur. Again, this is alleviated in our model by the robust geometric collision algorithm 
that stops interpenetration re­sulting in a bound on the magnitude of the spring based repulsion force. 
The spring based repulsion force is modeled with a spring of stiffness k. As a simple heuristic, we found 
that matching the stiff­ness of the stretch springs in the cloth gave good results. The over­lap is d 
= h - (x4 - w1x1 - w2x2 - w3x3) · n giving a spring force of kd in the direction n. Multiplying by .t 
gives the impulse. As discussed above, we limit this so that the rel­ative velocity change will reduce 
the overlap by at most some .xed fraction (e.g. .1h) in one .t time step. If the normal component of 
relative velocity vN = .1d/.t already we apply no repulsion, other­wise we compute the impulse magnitude 
 .1d Ir = -min.tkd,m- vN .t and distribute it to the mesh nodes as explained in section 7.1. The repulsion 
forces can either be applied sequentially or all at once in a parallel update. One drawback of the parallel 
update is that situations involving multiple interactions can lead to undesir­able behavior, e.g. as 
impulses from multiple inelastic collisions are added together. This can be alleviated to some degree 
by keeping track of the number of interactions a node is involved in and divid­ing the resulting impulses 
by that number. Another remedy consists of multiplying the inelastic collision impulses by a suitable 
relax­ation parameter less than one. We found .25 works .ne though the algorithm seems fairly insensitive 
to small changes in this value. 7.3 Friction We use Coulomb s model for friction, both static and kinetic, 
with a single friction parameter µ. The repulsion force FR from section 7.2 is the negative of the normal 
force FN pressing the cloth together. Therefore a friction force, in the direction of the pre-friction 
rel­ative tangential velocity vpre but opposite to it, has magnitude at T most µFN . This integrates 
to an impulse of magnitude µFN .t in the same direction, and thus a change in the relative tangential 
ve­locity of at most µFN .t/m where m is the mass (assumed equal pre for all particles involved). If 
this is larger than |v|, then either T the cloth was slipping and stopped due to kinetic friction, or 
was stuck and shouldn t be allowed to start slipping due to static fric­tion. Either way the new relative 
tangential velocity should be zero. If not, we can simply subtract this off the magnitude of the relative 
tangential velocity to account for kinetic friction slowing down the slipping. This calculation can be 
simpli.ed by noting that FN .t/m is just .vN , the change in relative velocity in the normal direction, 
which can be directly calculated in the repulsion algorithm. Then the decrease in the magnitude of the 
relative tangential velocity is pre min(µ.vN ,|v|), i.e. our .nal relative tangential velocity is T 
.vN vT = max1 - µ pre , 0vpre |v| T T We apply impulses to achieve this for both point-face proximities 
and edge-edge proximities. In effect, we are modeling frictional contact with micro­collisions. We note 
that if we applied our friction algorithm to rigid bodies, it would solve the inclined plane problem 
as discussed in [Mirtich and Canny 1995]. A similar algorithm for kinetic friction was proposed by [Jimenez 
and Luciani 1993] who also calculated a normal force FN from the magnitude of their spring repulsion 
force and subsequently used it to evaluate their Coulomb friction model. For static friction they attached 
a spring between the closest points of two objects when the normal force is nonzero. This attractive 
spring models static friction until the force exerted by this spring reaches the threshold µsFN . Beyond 
this, the spring is removed and only kinetic friction applies. 7.4 Geometric Collisions Repulsion forces 
alone cannot ensure that no interpenetrations will occur since positions are only checked at discrete 
moments in time. For robust collision handling, the path of the cloth between time steps must be considered 
as discussed in section 6. Some authors back up simulations in time to treat collisions in chronological 
order, e.g. [Hahn 1988]. When a single time step may have thousands of collisions and contacts, as is 
characteristic of highly deformable bodies like cloth, this is quite expensive and can grind the simulation 
to a halt. The problem was addressed for rigid bodies by [Mirtich 2000] who processed the rigid bodies 
in parallel using a timewarp algorithm to back up just the objects that are involved in collisions while 
still evolving non-colliding objects forward in time. This method works well except when there are a 
small number of contact groups which unfortunately is the case for cloth as the entire piece of cloth 
has every node in contact with every other node through the mass-spring network. Instead of rewinding 
the simulation to process one collision at a time, we resolve them all simultaneously with an iterative 
pro­cedure as did [Volino et al. 1995; Provot 1997; Milenkovic and Schmidt 2001]. This does not give 
the same result as processing the collisions in chronological order. However, there is enough un­certainty 
in the collision modeling and error in the cloth discretiza­tion that we are already guaranteed to not 
get the exact physically correct answer. Instead we will obtain a physically plausible solu­tion, i.e. 
one of many possible physically correct outcomes which may vary signi.cantly with slight perturbations 
in initial conditions or the inclusion of unmodeled phenomena such as interactions be­tween fuzzy strands 
of cloth. More details on sampling plausible solutions according to probability distributions re.ecting 
a number of factors can be found in [Chenney and Forsyth 2000] who ad­dressed the related problem of 
multiple colliding rigid bodies. Sim­ulating plausible motion in chaotic scenarios was also addressed 
by [Milenkovic and Schmidt 2001] who studied problems where large numbers of rigid bodies were in a single 
contact group and employed iterative collision processing techniques, phrased as op­timization procedures, 
in order to adjust the positions of the bodies to avoid overlap. As discussed in section 6, our geometric 
collision processing al­gorithm is activated either when a collision actually occurs or when geometry 
(points and faces or edges and edges) is in (too) close proximity at the end of a time step. Thus, we 
need to account for both approaching and separating objects when a collision is reg­istered. If the geometry 
is approaching, we apply a completely in­elastic repulsion impulse. Otherwise, if the geometry is already 
separating (as may happen at the end of the time step, i.e. a close call rather than a true collision), 
we apply a spring based repulsion force. See section 7.2 for more details on both of these. The collision 
impulses can either be applied sequentially or all at once in a parallel update. Once again, in the case 
of a parallel update, situations involving multiple interactions can lead to un­desirable behavior. This 
can once again (similar to the repulsion forces) be alleviated by dividing the resulting impulses by 
the num­ber of interactions, or by using a suitable relaxation parameter less than one (e.g. .25). While 
processing all the collisions that occurred during a time step, we may have inadvertently caused new 
secondary collisions to occur. In order to keep the cloth interference free, we must analyze the newly 
computed post-collision path of the cloth for possible collisions and process these as well. This means 
that the bounding box hierarchy needs to be adjusted to account for the new post­collision velocities. 
Then secondary collisions can be detected and corrected, etc., and the process continues until a .nal 
interference free state can be computed. Since relatively large bounding boxes that contain the moving 
triangles need to be recomputed for every iteration, and a cubic equation has to be solved for every 
possi­ble collision, this may be expensive. Luckily, our repulsion forces tend to catch and treat almost 
all collisions making the iteration scheme here practical to apply even for high velocity cloth with 
many nodes and a high degree of folding and contact. Also, there are some multiple collision situations, 
such as a node sandwiched between two approaching triangles, that are resolved immediately if we apply 
impulses in parallel (but can iterate for a long time if they are applied sequentially instead, although 
Gauss-Seidel gener­ally converges faster than Jacobi iteration). However, there are still situations 
where many iterations are required, so after a few itera­tions we switch to a failsafe method which quickly 
eliminates all collisions. We use the method proposed by [Provot 1997], but not followed through in the 
literature, possibly because the formulas proposed in [Provot 1997] do not give true rigid body motion. 
We give corrected versions below. 7.5 Rigid Impact Zones [Provot 1997] proposed collecting the nodes 
involved in multiple collisions into impact zones which are treated as rigid bodies. This is justi.ed 
by observing that when cloth bunches together fric­tion will prevent most relative motion. Thus, after 
a few iterations of applying local impulses as outlined above, we instead switch to merging lists of 
nodes representing impact zones. Initially, each node in its own list. Then, when a point-face or edge-edge 
colli­sion occurs, the lists containing the four involved nodes are merged together into one larger impact 
zone. The impact zones are grown until the cloth is collision free. The velocity of the nodes in the 
im­pact zone is derived from a rigid body motion that preserves linear and angular momentum. The formula 
for angular velocity given in [Provot 1997] is .awed, so we present a corrected version here. To .nd 
the rigid body motion we .rst compute the initial center of mass of the impact zone and its average velocity 
n+1/2.i mxin .i mv i = , = xCM vCM .im .im then the angular momentum of the nodes with respect to their 
center of mass nn+1/2 L = .m(xi - xCM) × (vi - vCM ) i and the 3 × 3 inertia tensor of the current con.guration 
of nodes (using d to represent the identity tensor) n nn I = .m |xi - xCM|2d - (xi - xCM ) . (xi - xCM) 
. i The rigid body angular velocity that would preserve angular mo­mentum is . = I-1L, so the new instantaneous 
velocity of node i is vCM + . × (xi - xCM ) However, we want the average velocity over the time step 
of .nite n+1 size .t, so that the update x= xn +.tvn+1/2 exactly corresponds to a rigid body motion, 
i.e. so that lengths and angles stay .xed. If this last condition is not enforced (it was not addressed 
in [Provot 1997]), then self-intersection can occur. Assuming that we can ac­cept a small O(.t) error 
in the axis and angle of the total rotation, we make the approximation that . stays constant over the 
time step. Then we .nd the .xed and rotating components of the position xF =(xi - xCM) ·|..||..| , xR 
= xi - xCM - xF giving the .nal position n+1 xi = xCM + .tvCM + xF + cos(.t|.|)xR + sin(.t|.|) |..|× 
xR. n+1/2 n+1 The new average velocity is then v =(x- xin)/.t. ii Applied on its own, this impact zone 
method has a tendency to freeze the cloth into nonphysical clumps. However, a combination of our repulsion 
forces and the initial collision impulses tend to keep these impact zones small, isolated and infrequent. 
Moreover, once formed, they are short-lived as the repulsion forces tend to quickly separate the offending 
nodes. 7.6 Updating the Final Velocity When there are repulsions or collisions, we need to update the 
ve­locity from vn+1/2 to vn+1. For central differencing we need to solve the implicit equation n+1 n+1/2 
+ .tn+1 n+1) v= v2 a(tn+1 ,x, v where a(t, x,v) is the acceleration. In many cloth models, such as ours, 
the damping forces (hence accelerations) are linear in the velocities giving the linear system (I - .t 
.a n+1 n+1/2 + .tn+1). )v= v2 ae(tn+1 , x 2 .v Here . a/. v is the Jacobian matrix of accelerations with 
respect to velocities, and ae(t,x) is the elastic component of acceleration, i.e. everything apart from 
damping. In any reasonable cloth model this matrix will be symmetric positive de.nite (or can be safely 
made so, see [Baraff and Witkin 1998]) after multiplying both sides by the mass matrix, i.e. converting 
velocities into momenta and ac­celerations into forces. Then the conjugate gradient algorithm (e.g. [Saad 
1996]) can be used to solve for vn+1. For nonlinear damping Newton s method can be used requiring similar 
linear solves. As an alternative for the nonlinear case, or when solving the lin­ear system proves too 
dif.cult, one can explicitly march the velocity forward in time. Starting with u0 = vn+1/2 we advance 
n+1 um+1 = um + ka(tn+1 ,x,um+1) ending with vn+1 = uM where .t/2 = Mk. Each substep is solved (0) with 
.xed point iteration, starting with initial guess um+1 = um for um+1 and continuing with (i+1) n+1 (i) 
u= um + ka(tn+1 ,x, u m+1 m+1) for a given number of iterations. The substep size k is chosen with k|.max(. 
a/. v)| < 1 in order to guarantee convergence. This is es­sentially the same as the stability limit of 
forward Euler time step­ping. Note that this can be made more ef.cient by separating the calculation 
of acceleration into the elastic component ae mentioned above (which does not change) and the damping 
component. As mentioned in section 4, we monitor the strain rate in the cloth and introduce additional 
damping if necessary. This is important for dealing with especially dif.cult collision situations. While 
our algorithm can fully robustly handle them, bad strain rates in the cloth due to the collision impulses 
can cause additional unwanted collisions in subsequent time steps slowing the simulation down and producing 
visually inaccurate results. In addition, when one node collides and impulsively changes its velocity 
so that it is dra­matically out of sync with surrounding nodes, our strain rate lim­iting procedure puts 
the nodes back in sync reducing the velocity of surrounding nodes even though they have not yet been 
involved in a collision. This keeps our cloth from experiencing unnecessary stress and also increases 
the likelihood that repulsion forces will stop the surrounding nodes before they actually collide, again 
dra­matically reducing the number of collisions that have to be dealt with.  8 Post-Processing with 
Subdivision Sharp folds and wrinkles in the cloth mesh give undesirable angu­lar features when rendered 
as plain triangles. For visually pleasing animations a smoother surface is desired. Some authors have 
di­rectly simulated smooth surfaces instead of simple triangle meshes. For example, [Thingvold and Cohen 
1992] used dynamic B-splines which allowed them to interactively re.ne the mesh in regions of in­terest 
associating the control points with their dynamic simulation mesh nodes. They derived a number of rules 
for when, where and how to re.ne, even detecting when mesh re.nement would cause intersection and then 
either stopping re.nement or backing up the simulation in time to avoid intersection. [DeRose et al. 
1998] ex­ploited the convex hull properties of their subdivision surface model of cloth to accelerate 
collision detection. [Grinspun and Schr¨oder 2001] rigorously modeled thin manifolds with subdivision 
surfaces detecting collisions with their derived bounds on surface normals and re.ning the mesh as required 
to resolve them. We propose a fast and simple yet collision-aware post-processing subdivision scheme 
to smooth our triangle mesh. Our post­processing scheme takes the existing intersection free simulation 
data and produces a .ner, more detailed and smoother approxima­tion to the manifold. We never have to 
back the mesh up in time or cease re.nement, or even have to consider re.nement at all in the simulation. 
Our algorithm ef.ciently works independently from the simulation on the positions recorded for each frame. 
In addition, each frame can be processed independently just as in a rendering pipeline. Another motivation 
for our post-processing is found in [Howlett and Hewitt 1998] who addressed cloth collisions with volumetric 
objects. They ensured that cloth nodes remained outside the objects making the collision-handling algorithm 
faster and simpler, but al­lowed edge and face collisions with the objects. These were han­dled in a 
post-processing step before rendering where they added nonactive points and adjusted their positions 
to eliminate intersec­tions. Although we treat cloth-object collisions in a more detailed manner, our 
subdivision approach naturally allows this clever lie about it strategy explained in [Baraff 2001] where 
small interpen­etrations are allowed in the simulation but are corrected before ren­dering. [Howlett 
and Hewitt 1998] further processed their cloth in an attempt to preserve area, but we do not undertake 
this endeavor since the idea of subdivision is that it recovers the true geometry approximated by the 
coarse simulation mesh. Our post-processing algorithm proceeds as follows. If we al­lowed intersection 
with objects in the simulation, we begin by ad­justing the cloth positions in the given frame to eliminate 
them it­erating back and forth with an adjustment to eliminate cloth-cloth intersections that those adjustments 
may have caused. We use the repulsions and collision impulses from sections 7.2 and 7.4. When this is 
.nished, our original mesh is intersection free even account­ing for rounding error. We then subdivide 
the mesh putting a new node at the midpoint of each edge. Since the original mesh was intersection free 
and the subdivided mesh lies exactly within the original mesh, the subdivided mesh is guaranteed to be 
intersection free as well. Next we use the modi.ed Loop subdivision scheme [Loop 2001] to .nd smoother 
positions for all the nodes of the subdivided mesh. Unfortunately, moving to these smoothed positions 
may create in­tersections. However, we can view the vector from a node s original position to its smoothed 
position as a pseudo-velocity and apply our collision detection algorithms from section 6 to determine 
when the intersection would occur. We stop the nodes at that point (or just before that point to avoid 
dif.culties with rounding error) as if they had inelastically collided. We of course need to check again 
to see if these adjustments to the smoothed positions caused new intersec­tions. Typically only a few 
iterations are required to eliminate all  Figure 2: The friction between a rotating sphere and a piece 
of cloth draped over it creates a complex structure of wrinkles and folds. intersections especially since 
the convex-hull property of the subdi­vision means intersections are unlikely in the .rst place. A solution 
is guaranteed to exist, since the new nodes can simply be left on the triangle they were created on. 
Once we have a smoothed but inter­section free subdivided mesh, we can subdivide again continuing until 
the desired resolution is reached. Since the cloth is originally separated by a .nite distance, but each 
step of subdivision smooth­ing moves the nodes exponentially less and less, we very quickly .nd no more 
adjustments need to be made. We caution the reader that this post-processing technique per­forms exceptionally 
well because we use a fairly high resolution dynamic simulation mesh. The ef.ciency of our repulsion 
and col­lision processing algorithms allows the use of such a mesh, and we have have not noticed any 
problems with visual artifacts. How­ever, on a relatively coarser mesh, one should be aware of potential 
artifacts such as popping that might result from using this subdi­vision scheme.  9 Examples We demonstrate 
several examples using our simple cloth model with highly complicated folding where most of the nodes 
(tens of thousands in the dynamics and hundreds of thousands after subdi­vision) are in close contact 
with each other as opposed to, say, the simple draping of a skirt about a mannequin. In .gure 1, a curtain 
is draped over the ground and a sphere. Our biphasic spring model en­ables complex wrinkling and eliminates 
undue deformation. When the sphere moves up and away, the curtain .ips back over on itself resulting 
in a large number of contacts and collisions. The highly Figure 3: A tablecloth draped over table legs 
with no tabletop is dragged to the ground by a descending sphere. complex structure of folds and wrinkles 
is stable due to our static friction model. When a second ball pushes through the complex structure eventually 
slipping underneath, the algorithm still ef.­ciently and correctly resolves all contacts and collisions. 
Note how realistically the cloth unravels by the .nal frame. Figure 2 illustrates our static and kinetic 
friction algorithm with a piece of cloth draped over a rotating sphere. Figure 3 shows a table­cloth 
draped over four table legs with no tabletop. The object-cloth contact is tricky due to the sharp corners 
of the legs particularly when a sphere descends through the cloth down onto the ground, but our repulsion 
forces prevent unnatural snagging. Simulation times were reasonable even for these complex examples. 
Typically, a piece of cloth with 150 × 150 nodes runs at about 2 minutes per frame on a 1.2GHz Pentium 
III. Finally, .gure 4 shows the draping and folding of a robe around a digital character from a production 
animation system utilizing a number of our techniques. 10 Conclusions and Future Work The synergy of 
ef.cient repulsion forces combined with robust geo­metric treatment of collisions has allowed us to ef.ciently 
simulate complex cloth motion. The prevention of self-intersection together with kinetic and static friction 
produces complex, yet stable folding and wrinkling unachievable by simpler approaches. In addition, our 
post-processing subdivides simulation data without introducing self-intersection resulting in even higher 
quality animations. Our algorithm makes few assumptions about the internal cloth dynam­ics, and thus 
can easily be incorporated into existing codes with advanced cloth models. We are close to a fully parallel 
implementation exploiting the parallel nature of most of our scheme. Other areas we plan to de­velop 
include modeling different values for kinetic and static fric­tion coef.cients, adaptive meshing to better 
resolve folds, and op­timization of the bounding volume hierarchy. Furthermore, we are eager to apply 
our techniques to characters with highly wrinkled loose .tting skin. Two rather important problems that 
we have not addressed are the interactions between cloth with sharp objects and the behav­ior of cloth 
when trapped in between two solid deformable or rigid bodies. We refer the reader interested in sharp 
objects to the recent developments of [Kane et al. 1999; Pandol. et al. 2002]. For the case of intersecting 
collision bodies additional technologies like the ones developed by Baraff, Witkin and Kass (Personal 
Communica­tion 2002) are required.  Figure 4: Frames from a production animation of a robe draped over 
a digital character.  11 Acknowledgement Research supported in part by an ONR YIP and PECASE award N00014-01-1-0620. 
The .rst author was supported in part by a Stanford Graduate Fellowship. The authors would like to thank 
Igor Neverov, Joseph Teran, and Neil Molino for their help producing the .nal animations, and Hen­rik 
Wann Jensen for the use of his dali rendering system. The authors would like to thank Cliff Plumer, Andy 
Hendrick­son, Sebastian Marino and Lucas.lm for their guidance and sup­port, as well as the images in 
.gure 4.  References BARAFF, D., AND WITKIN, A. 1992. Dynamic simulation of non­penetrating .exible 
bodies. In Proc. of SIGGRAPH 1992, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 303 308. BARAFF, 
D., AND WITKIN, A. 1994. Global methods for simu­lating contacting .exible bodies. In Computer Animation 
Proc., Springer-Verlag, 1 12. BARAFF, D., AND WITKIN, A. 1998. Large steps in cloth simu­lation. In Proc. 
of SIGGRAPH 1998, ACM Press / ACM SIG-GRAPH, Comput. Graphics Proc., 1 12. BARAFF, D. 1989. Analytical 
methods for dynamic simulation of non-penetrating rigid bodies. In Proc. of SIGGRAPH 1989, ACM Press 
/ ACM SIGGRAPH, Comput. Graphics Proc. BARAFF, D. 1990. Curved surfaces and coherence for non­penetrating 
rigid body simulation. In Proc. of SIGGRAPH 1990, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc. BARAFF, 
D. 1991. Coping with friction for non-penetrating rigid body simulation. In Proc. of SIGGRAPH 1991, ACM 
Press / ACM SIGGRAPH, Comput. Graphics Proc., 31 40. BARAFF, D. 1994. Fast contact force computation 
for nonpene­trating rigid bodies. In Proc. of SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, Comput. Graphics 
Proc. BARAFF, D. 2001. Collision and contact. In SIGGRAPH 2001 Course Notes, ACM. BAREQUET, G., CHAZELLE, 
B., GUIBAS, L., MITCHELL, J., AND TAL, A. 1996. BOXTREE: A hierarchical representation for surfaces in 
3D. Comp. Graphics Forum 15, 3, 387 396. BREEN, D. E., HOUSE, D. H., AND WOZNY, M. J. 1994. Predict­ing 
the drape of woven cloth using interacting particles. In Proc. of SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, 
Comput. Graphics Proc., 365 372. CARAMANA, E., BURTON, D., SHASHKOV, M., AND WHALEN, P. 1998. The construction 
of compatible hydrodynamics algo­rithms utilizing conservation of total energy. Journal of Compu­tational 
Physics 146, 227 262. CARIGNAN, M., YANG, Y., MAGNENAT-THALMANN, N., AND THALMANN, D. 1992. Dressing 
animated synthetic actors with complex deformable clothes. In Proc. SIGGRAPH 1992, ACM Press / ACM SIGGRAPH, 
Comput. Graphics Proc., 99 104. CHENNEY, S., AND FORSYTH, D. A. 2000. Sampling plausi­ble solutions to 
multi-body constraint problems. In SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, Comp. Graphics Proc. DEROSE, 
T., KASS, M., AND TRUONG, T. 1998. Subdivision sur­faces in character animation. In Proc. SIGGRAPH 1998, 
ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 85 94. DESBRUN, M., AND GASCUEL, M.-P. 1994. Highly 
deformable material for animation and collision processing. In 5th Euro­graphics worshop on animation 
and simulation. DESBRUN, M., SCHR ¨ODER, P., AND BARR, A. 1999. Interactive animation of structured deformable 
objects. In Graphics Inter­face, 1 8. DOGHRI, I., MULLER, A., AND TAYLOR, R. L. 1998. A general three-dimensional 
contact procedure for .nite element codes. Engineering Computations 15, 2, 233 259. GASCUEL, M.-P. 1993.Animplicitformulationforprecisecontact 
modeling between .exible solids. In SIGGRAPH 1993, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 
313 320. GOTTSCHALK, S., LIN, M. C., AND MANOCHA, D. 1996. Obb­tree: a hierarchical structure for rapid 
interference detection. In Proc. of SIGGRAPH 1996, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 
171 179. GOURRET, J.-P., MAGNENAT-THALMANN, N., AND THAL-MANN, D. 1989. Simulation of object and human 
skin defor­mations in a grasping task. In Proc. of SIGGRAPH 1989, ACM Press / ACM SIGGRAPH, Comput. Graphics 
Proc., 21 30. GRINSPUN, E., AND SCHR ODER¨, P. 2001. Normal bounds for subdivision-surface interference 
detection. In Proc. of IEEE Sci­enti.c Visualization, IEEE. HAHN, J. K. 1988. Realistic animation of 
rigid bodies. In Proc. of SIGGRAPH 1988, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc. HERZEN, B. 
V., BARR, A. H., AND ZATZ, H. R. 1990. Geomet­ric collisions for time-dependent parametric surfaces. 
In Proc. of SIGGRAPH 1990, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 39 48. HOUSE, D. H., AND 
BREEN, D. E., Eds. 2000. Cloth modeling and animation. A. K. Peters. HOWLETT, P., AND HEWITT, W. T. 1998. 
Mass-spring simulation using adaptive non-active points. In Computer Graphics Forum, vol. 17, 345 354. 
HUGHES, T. J. R. 1987. The .nite element method: linear static and dynamic .nite element analysis. Prentice 
Hall. JIMENEZ, S., AND LUCIANI, A. 1993. Animation of interacting objects with collisions and prolonged 
contacts. In Modeling in computer graphics methods and applications, Springer-Verlag, B. Falcidieno and 
T. L. Kunii, Eds., Proc. of the IFIP WG 5.10 Working Conference, 129 141. KANE, C., REPETTO, E., ORTIZ, 
M., AND MARSDEN, J. 1999. Finite element analysis of nonsmooth contact. Comput. Methods Appl. Mech. Eng. 
180, 1 26. LAFLEUR, B., MAGNENAT-THALMANN, N., AND THALMANN, D. 1991. Cloth animation with self-collision 
detection. In Proc. of the Conf. on Modeling in Comp. Graphics, Springer, 179 187. LIN, M., AND GOTTSCHALK, 
S. 1998. Collision detection be­tween geometric models: A survey. In Proc. of IMA Conf. on Mathematics 
of Surfaces. LOOP, C. 2001. Triangle mesh subdivision with bounded curva­ture and the convex hull property. 
Tech. Rep. MSR-TR-2001-24, Microsoft Research. MARHEFKA, D. W., AND ORIN, D. E. 1996. Simulation of con­tact 
using a nonlinear damping model. In Proc. of the 1996 IEEE Int l Conf. on Robotics and Automation, IEEE, 
1662 1668. MILENKOVIC, V. J., AND SCHMIDT, H. 2001. Optimization­based animation. In Proc. of SIGGRAPH 
2001, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc. MIRTICH, B., AND CANNY, J. 1995. Impulse-based 
simulation of rigid bodies. In Proc. of 1995 symposium on interactive 3d graphics, 181 188, 217. MIRTICH, 
B. 2000. Timewarp rigid body simulation. In Proc. of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, Comput. 
Graphics Proc., 193 200. MOORE, M., AND WILHELMS, J. 1988. Collision detection and response for computer 
animation. In SIGGRAPH 1988, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 289 298. NG, H. N., AND 
GRIMSDALE, R. L. 1996. Computer graphics techniques for modeling cloth. IEEE Computer Graphics and Applications, 
28 41. O BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical modeling and animation of brittle fracture. 
In SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 137 146. OKABE, H., IMAOKA, H., TOMIHA, 
T., AND NIWAYA, H. 1992. Three dimensional apparel CAD system. In SIGGRAPH 1992, ACM Press/ACM SIGGRAPH, 
Comp. Graphics Proc., 105 110. PANDOLFI, A., KANE, C., MARSDEN, J., AND ORTIZ, M. 2002. Time-discretized 
variational formulation of non-smooth fric­tional contact. Int. J. Num. Methods in Eng. 53, 1801 1829. 
PROVOT, X. 1995. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In 
Graphics Interface, 147 154. PROVOT, X. 1997. Collision and self-collision handling in cloth model dedicated 
to design garment. Graphics Interface, 177 89. SAAD, Y. 1996. Iterative methods for sparse linear systems. 
PWS Publishing. New York, NY. SIMS, K. 1994. Evolving virtual creatures. In SIGGRAPH 1994, ACM Press 
/ ACM SIGGRAPH, Comp. Graphics Proc., 15 22. TERZOPOULOS, D., AND FLEISCHER, K. 1988. Deformable mod­els. 
The Visual Computer, 4, 306 331. TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling in­elastic deformation: 
viscoelasticity, plasticity, fracture. In Proc. of SIGGRAPH 1988, ACM Press / ACM SIGGRAPH, Comput. Graphics 
Proc., 269 278. TERZOPOULOS, D., AND WITKIN, A. 1988. Physically based models with rigid and deformable 
components. In Graphics In­terface, 146 154. TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 
1987. Elastically deformable models. In SIGGRAPH 1987, ACM Press/ACM SIGGRAPH, Comp. Graphics Proc., 
205 214. THINGVOLD, J. A., AND COHEN, E. 1992. Physical model­ing with B-spline surfaces for interactive 
design and animation. In Proc. of SIGGRAPH 1992, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc., 129 
137. VOLINO, P., AND MAGNENAT-THALMANN, N. 1994. Ef.cient self-collision detection on smoothly discretized 
surface anima­tions using geometrical shape regularity. In Proc. of Eurograph­ics, vol. 13 of Computer 
Graphics Forum, Eurographics Associ­ation, C 155 166. VOLINO, P., AND MAGNENAT THALMANN, N. 1995. Collision 
and self-collision detection: Ef.cient and robust solutions for highly deformable surfaces. In Comp. 
Anim. and Simulation, Springer-Verlag, D. Terzopoulos and D. Thalmann, Eds., 55 65. VOLINO, P., AND MAGNENAT-THALMANN, 
N. 1997. Devel­oping simulation techniques for an interactive clothing system. In Proc. of the 1997 International 
Conf. on Virtual Systems and MultiMedia, IEEE, 109 118. VOLINO, P., COURCHESNE, M., AND MAGNENAT-THALMANN, 
N. 1995. Versatile and ef.cient techniques for simulating cloth and other deformable objects. In Proc. 
of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Comput. Graphics Proc. VOLINO, P., COURCHESNE, M., AND MAGNENAT-THALMANN, 
N. 2000. Accurate collision response on polygonal meshes. In Proc. of Computer Graphics, 179 188. WEBB, 
R., AND GIGANTE, M. 1992. Using dynamic bounding volume hierarchies to improve ef.ciency of rigid body 
simula­tions. In Comm. with Virtual Worlds, CGI Proc. 1992, 825 841. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566624</article_id>
		<sort_key>604</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Stable but responsive cloth]]></title>
		<page_from>604</page_from>
		<page_to>611</page_to>
		<doi_number>10.1145/566570.566624</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566624</url>
		<abstract>
			<par><![CDATA[We present a semi-implicit cloth simulation technique that is very stable yet also responsive. The stability of the technique allows the use of a large fixed time step when simulating all types of fabrics and character motions. The animations generated using this technique are strikingly realistic. Wrinkles form and disappear in a quite natural way, which is the feature that most distinguishes textile fabrics from other sheet materials. Significant improvements in both the stability and realism were made possible by overcoming the <i>post-buckling instability</i> as well as the numerical instability. The instability caused by buckling arises from a structural instability and therefore cannot be avoided by simply employing a semi-implicit method. Addition of a damping force may help to avoid instabilities; however, it can significantly degrade the realism of the cloth motion. The method presented here uses a particle-based physical model to handle the instability in the post-buckling response without introducing any fictitious damping.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[deformations]]></kw>
			<kw><![CDATA[numerical analysis]]></kw>
			<kw><![CDATA[physically based animation]]></kw>
			<kw><![CDATA[physically based modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.7</cat_node>
				<descriptor>Convergence and stability</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.7</cat_node>
				<descriptor>Stiff equations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003728</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Ordinary differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003728</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Ordinary differential equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P163975</person_id>
				<author_profile_id><![CDATA[81447598118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kwang-Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P113193</person_id>
				<author_profile_id><![CDATA[81423594747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hyeong-Seok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AMIRBAYAT, J., AND HEARLE, J. 1989. The anatomy of buckling of textile fabrics : Drape and conformability. Journal of Textile Institute 80, 1, 51-70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARAFF, D., AND WITKIN, A. 1998. Large steps in cloth simulation. In Proceedings of SIGGRAPH 98, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 43-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BATHE, K. J. 1996. Finte Element Procedures. Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192259</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BREEN, D. E., HOUSE, D. H., AND WOZNY, M. J. 1994. Predicting the drape of woven cloth using interacting particles. In Proceedings of SIGGRAPH 94, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 365-372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134017</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CARIGNAN, M., YANG, Y., MAGNENAT-THALMANN, N., AND THALMANN, D. 1992. Dressing animated synthetic actors with complex deformable clothes. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), ACM, 99-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218432</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COURSHESNES, M., VOLINO, P., AND MAGNENAT THALMANN, N. 1995. Versatile and efficient techniques for simulating cloth and other deformable objects. In Proceedings of SIGGRAPH 95, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 137-144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351638</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DESBRUN, M., SCHR&#214;DER, P., AND BARR, A. 1999. Interactive animation of structured deformable objects. In Graphics Interface, 1-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618378</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[EBERHARDT, B., WEBER, A., AND STRASSER, W. 1996. A fast, flexible, particle-system model for cloth draping. IEEE Computer Graphics and Applications 16, 5 (Sept.), 52-59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618380</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[EISCHEN, J. W., DENG, S., AND CLAPP, T. G. 1996. Finite-element modeling and control of flexible fabric parts. IEEE Computer Graphics and Applications 16, 5 (Sept), 71-80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GERE, J. M. 2001. Mechanics of Materials. Brooks/Cole.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[KANG, T., JOO, K., AND LEE, K. 2001. Analysis of fabric buckling based on nonlinear bending properties. (Submitted), http://textile.snu.ac.kr/upjuk/PDF/IJ_TRJ_2002_KHJOO.PDF.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[PROVOT, X. 1995. Deformation constraints in a mass-spring model to describe rigid cloth behavior. In Graphics Interface '95, 147-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling inelastic deformation: Viscoelasticity, plasticity, fracture. In Computer Graphics (Proceedings of ACM SIGGRAPH 88), ACM, 269-278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>793086</ref_obj_id>
				<ref_obj_pid>792759</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., AND MAGNENAT-THALMANN, N. 2000. Implementing fast cloth simulation with collision response. In Proceedings of the Conference on Computer Graphics International (CGI-00), 257-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>735221</ref_obj_id>
				<ref_obj_pid>647781</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[VOLINO, P., AND MAGNENAT-THALMANN, N. 2001. Comparing efficiency of integration methods for cloth animation. In Proceedings of the Conference on Computer Graphics International (CGI-01).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Yu, W., KANG, T., AND CHUNG, K. 2000. Drape simulation of woven fabrics by using explicit dynamic analysis. Journal of Textile Institute 91 Part 1, 2, 285-301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826507</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[ZHANG, D., AND YUEN, M. 2000. Collision detection for clothed human animation. In Proceedings of the 8th Pacific Graphics Conference on Computer Graphics and Application (PACIFIC GRAPHICS-00), 328-337.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stable but Responsive Cloth Kwang-Jin Choi Hyeong-Seok Ko Graphics and Media Lab Seoul National University 
 Abstract We present a semi-implicit cloth simulation technique that is very stable yet also responsive. 
The stability of the technique allows the use of a large .xed time step when simulating all types of 
fab­rics and character motions. The animations generated using this technique are strikingly realistic. 
Wrinkles form and disappear in a quite natural way, which is the feature that most distinguishes tex­tile 
fabrics from other sheet materials. Signi.cant improvements in both the stability and realism were made 
possible by overcom­ing the post-buckling instability as well as the numerical instabil­ity. The instability 
caused by buckling arises from a structural in­stability and therefore cannot be avoided by simply employing 
a semi-implicit method. Addition of a damping force may help to avoid instabilities; however, it can 
signi.cantly degrade the realism of the cloth motion. The method presented here uses a particle­based 
physical model to handle the instability in the post-buckling response without introducing any .ctitious 
damping. Keywords: Deformations, Numerical Analysis, Physically Based Animation, Physically Based Modeling 
CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; G.1.7 [Numerical 
Analysis]: Ordinary Differential Equations Convergence and stability, Stiff equations I.6.5 [Simulation 
and Modeling]: Model Development 1 Introduction A normal out.t covers more than 90 percent of the human 
body. Therefore, the realistic animation of cloth is imperative if we are to animate humans to a satisfactory 
level of detail and realism. Over the last decade a great deal of research has been dedicated to sim­ulating 
cloth motion [Terzopoulos and Fleischer 1988; Carignan et al. 1992; Breen et al. 1994; Courshesnes et 
al. 1995; Provot 1995; Eberhardt et al. 1996; Eischen et al. 1996; Baraff and Witkin 1998; Desbrun et 
al. 1999; Volino and Magnenat-Thalmann 2000]. All of the methods proposed to date boil down to numerically 
solving an ordinary differential equation, although they differ in regard to characteristics such as 
stability, allowed time step size, etc. Cloth is characterized by strong resistance to stretch and weak 
resistance to bending, which leads to a stiff set of equations and thus prohibits the use of large time 
steps. However, cloth simulation techniques must be stable and fast if they are to be of practical use. 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 Previous studies have shown that implicit methods are well suited to solving stiff equations with 
a reasonable step size, and successful results have been reported in [Baraff and Witkin 1998; Volino 
and Magnenat-Thalmann 2000]. Another property that is crucial to the appearance of fabrics in motion 
is their buckling behavior. The buckling of fabrics is the process by which wrinkles form, and leads 
to structures such as those shown in Figure 1. The buckling of textile fabrics has a quite different 
nature from solid materials [Amirbayat and Hearle 1989], thus animation of a cloth would not look natural 
without having such property. Nevertheless it has been largely ignored though its problematic nature 
(instability and non-linearity) was recently pointed out by [Eischen et al. 1996] and [Yu et al. 2000]. 
This pa­per presents a stable and practical solution to this problem. Figure 1: Buckling of Real Fabrics 
The buckling of a thin material involves a very unstable state, regardless of whether it is rigid (e.g. 
aluminum sheet) or .exible (e.g. fabrics). When a compressive force is applied at the extremes of a thin 
material, it initially resists changing shape. As this force is increased, it eventually reaches the 
neutral equilibrium, the point at which an in.nitesimal increase or decrease in the force bifurcates 
the situation in two radically different directions: increasing the force leads to an unstable post-buckling 
response whereas the sys­tem remains at stable equilibrium if the force is decreased. (Buck­ling will 
be described in detail in Section 3.2.) Given that buck­ling is a ubiquitous characteristic of fabrics, 
creating natural look­ing cloth in an animation is very dif.cult without a stable way to model this phenomenon. 
The instability of the post-buckling response arises from a structural instability [Bathe 1996], not 
from the stiff equations. Therefore, the buckling instability cannot be overcome by simply employing 
implicit methods. Some cloth simulation techniques [Baraff and Witkin 1998; Volino and Magnenat-Thalmann 
2000] avoid this instability by adding damping forces1. The damping 1Damping is an important concept 
in this work. The Implicit method has an intrinsic damping effect that comes from the formulation itself, 
which we refer to as arti.cial damping. This damping is not related to the nature of the cloth. On the 
other hand, we refer to damping that is deliberately added to the formulation to simulate the nature 
of the cloth as material intrinsic damping. A third kind of damping is sometimes added to enhance numerical 
stability. We refer to this as .ctitious damping. A damping term appearing forces help to stabilize the 
physical system, or equivalently, they make the system matrix better conditioned and help to maintain 
positive de.niteness in a semi-implicit formulation. However, the damping forces can signi.cantly degrade 
the realism of the sim­ulated cloth movement. For example, [Volino and Magnenat-Thalmann 2000] and [Volino 
and Magnenat-Thalmann 2001] found that the damping forces can lead to systems in which wrinkles will 
not form on the cloth surface, wrinkles resist disappearing, or even the fabrics resist falling under 
their own weight. The arti.cial damping in implicit methods mainly affects the in-plane deformation of 
the cloth because the in-plane rigidity is much higher than the bending rigidity. Therefore, although 
arti.cial damping is expected to be partially responsible for the degradation of the quality of the out-of-plane 
movement of the cloth, we con­jecture that the degradation in the quality mentioned above arises mainly 
from .ctitious damping. The method we propose in this pa­per includes arti.cial damping (since it is 
an implicit method) and material intrinsic damping, but does not include .ctitious damping. The need 
for .ctitious damping is avoided through the use of the predicted static post-buckling response as an 
effective way to han­dle the instabilities associated with post-buckling situations. Be­cause .ctitious 
damping is not used, our method gives signi.cantly more realistic cloth motion. This represents a signi.cant 
step for­ward for the simulation of textile fabrics. For solid materials, buckling signi.es a failure 
and thus only the mechanics prior to buckling have been studied (e.g. determining the critical load on 
a column). Even in the study of textiles, there has been no signi.cant result on the buckling process 
that can be ap­plied to the dynamic simulation of cloth movement. Therefore, in­stead of physically simulating 
the unstable post-buckling dynamic response, we solve the instability problem by calculating the de­formation 
energy of the shape at the predicted static equilibrium of the post-buckling state. We treat the numerical 
instability caused by stiff equations using implicit time stepping. Using the physical model outlined 
above and implicit time stepping, we could stably integrate the equation of motion with a large .xed 
time step and without the need for .ctitious damping forces. 2 The Physical Model Before we describe 
our particle-based physical model of fabrics, we discuss the problems in recently proposed physical models. 
2.1 Problems in Previous Physical Models Cloth is not a homogeneous continuum. Therefore modeling fab­rics 
as a continuum and employing FEM or FDM has several poten­tial drawbacks, a number of which are described 
in [Amirbayat and Hearle 1989]. One drawback of such methods is that they require a very .ne meshing 
to produce large deformations. For a simu­lator to be practical in a computer graphics application, however, 
coarse discretization (about 1 ~ 5 cm spacing between the nodal points) should be allowed to guarantee 
reasonable performance. In the analysis of almost incompressible and/or thin materials such as cloth, 
a continuum formulation with the elements at this scale might produce highly erratic results in the stress 
and strain [Bathe 1996], regardless of the interpolation order. Another problem confronting the continuum 
approach is treat­ing the divergence associated with buckling. [Eischen et al. 1996] used a non-linear 
shell model for cloth and performed .nite ele­ment analysis to obtain the drape shape. They had to take 
special care and use measures such as arc length control to prevent diver­gence due to the non-linearity 
of the load-de.ection curve or the in an equation may serve for both material intrinsic damping and .ctitious 
damping. (a) all the connections for P (i, j) (b) connections among neighboring particles in a particular 
direction Figure 2: Connectivity of Interacting Particles inde.niteness of the instantaneous stiffness 
matrix caused by buck­ling. [Yu et al. 2000] performed explicit dynamic simulation with .ctitious viscous 
damping to avoid the divergence problem in the static analysis of the drape. The fabric models in [Baraff 
and Witkin 1998] and [Courshesnes et al. 1995] are similar and can be understood as systems of con­nected 
.at triangles that are treated as a continuum. The in-plane deformation energy (or stress/strain relationship) 
of each triangle is derived from the continuum mechanics formulation. The bend­ing deformation measure, 
on the other hand, is based on the an­gle between adjacent triangles. Therefore the fabric is not treated 
as a single homogeneous continuum and the bending and in-plane properties are modeled separately. The 
independent treatment of in-plane and out-of-plane cloth properties allows large bending de­formation 
between triangles regardless of the in-plane rigidity of each triangle. However, the post-buckling instability 
can be a prob­lem in this model, because each triangle is modeled as an almost incompressible material 
and the bending rigidity between triangles is very weak. Another problem of this physical model is that 
the bending characteristics largely depend on the triangulation. Since each triangle has very high in-plane 
rigidity, deformations tend to develop along the edges of the triangles. This causes a problem in systems 
comprised of aligned triangles because bending will occur along the edges. This problem can be cured 
by irregular triangu­lation, although this remedy introduces arti.cial .exural rigidity [Courshesnes 
et al. 1995]. As the triangulation becomes coarser, the arti.cial .exural rigidity will grow accordingly. 
 2.2 Using Interacting Particles In our search for a way to overcome the drawbacks of the physical models 
outlined above, we found that systems of interacting par­ticles are better suited for generating large 
deformations and han­dling the buckling problem. The method presented here draws on the idea that inspired 
the work of [Breen et al. 1994], who .rst applied the particle model to the simulation of textile fabrics. 
How­ever, our particle model is much simpler than that used by Breen et al. and the treatment of compression 
and bending deformation is quite different from their approach. In this section, we describe the connectivity 
of the mass points representing the cloth surface. The associated energy functions and their derivatives 
are presented in the next section. We approximate a cloth with a quadrilateral mesh of particles; thus 
each particle can be indexed as P (i, j). Figure 2(a) shows all the connections associated with a given 
center particle. With the exception of particles at the boundaries, where some connections are broken, 
every particle has the connectivity shown in Figure 2(a). Two types of particle interaction model are 
employed, which are referred to as type 1 and type 2. The type 1 interaction model (red lines in Figure 
2) is respon­sible for stretch and shear resistance. For this type of interaction, the particle P (i, 
j) is connected to P (i ± 1,j), P (i, j ± 1), and P (i ± 1,j ± 1). Such connections are referred to as 
sequential connections. The type 2 interaction model (blue lines in Figure 2) is responsible for .exural 
and compression resistance. For type 2 interaction, the particles connected to P (i, j) are P (i ± 2,j), 
P (i, j ± 2), and P (i ± 2,j ± 2). Note that type 2 connections are made with every other particle, leading 
us to refer to them as interlaced connections. Figure 2(b) illustrates the sequential and interlaced 
connections in a particular direction, in which S1 ~ S4 are sequential connections and I1 ~ I3 are interlaced 
connections.  3 Energy Functions and Derivatives In this section we .rst describe the type 1 interaction 
model. We then elaborate on the concept of buckling and its profoundly differ­ent meanings in rigid materials 
and fabrics, after which we describe how this distinction is re.ected in our handling of the post-buckling 
instability. Based on fabric-speci.c buckling behavior, we formu­late the type 2 interaction model. Finally, 
we add a material intrinsic damping to those models. In the description of the energy functions and their 
deriva­tives presented below, the distinctive features of our formulation are highlighted by comparing 
the results of the proposed model with those presented in [Baraff and Witkin 1998] and [Volino and Magnenat-Thalmann 
2000]. 3.1 Type 1 Interaction The type1 interaction is represented by a simple linear spring model, for 
which the energy function for particles i and j is, 1 ks(|xij |- L)2 : |xij |= L E = 2 (1) 0: |xij | 
<L where xij = xj - xi, L is the natural length, and ks is the spring constant. Note that this energy 
function accounts for the tension only. The force acting on particle i due to the deformation between 
the two particles is, .E ks(|xij |- L) xij : |xij |= L fi = - = |xij| (2) .xi 0: |xij | <L The Jacobian 
matrix of the force vector is . TT xij xij L xijxij .fi ks + ks(1 - )(I - ): |xij |= L TT = xij xij |xij 
| xijxij .xj 0: |xij | <L (3) The .rst term tells us that the stiffness in the direction of the spring 
interaction is constant, which is an obvious consequence of model­ing the interaction using a linear 
spring. The second term tells us that the stiffness orthogonal to the interaction direction is propor­tional 
to (1 - L ). If we consider the 2-dimensional structure of |xij | the cloth, the direction orthogonal 
to all interactions corresponds to the out-of-plane direction. When the spring is stretched, in an im­plicit 
formulation, the second term plays an important role in stabi­lizing the spring because it introduces 
large positive eigenvalues of the system matrix in that direction. (If the same function were to be used 
for compression, the second term might turn the overall sys­tem matrix (I - a.f/.v - ß.f/.x) inde.nite 
since ks(1 - L ) |xij | is negative and can be arbitrarily large regardless of the time step size, as 
|xij |. 0. For this reason we do not use the same function for compression. Compression is handled by 
the type 2 interaction model, which is presented in Section 3.3.) [Volino and Magnenat-Thalmann 2000] 
used the same spring model (i.e., 1 ks(|xij |-L)2) for both stretching and compression in 2 running a 
semi-implicit method. In their formulation, however, the second term of the Jacobian matrix in Equation 
3 is omitted for both stretch and compression. They added a damping term to avoid null eigenvalues in 
the orthogonal direction. For a compressed spring, addition of a damping force in the out-of-plane direction 
can make the system matrix better conditioned. However, this damping force may generate unnecessarily 
high resistance to the movement of the cloth. [Baraff and Witkin 1998] used a linear elastic model for 
both stretching and compression. Although their formulation looks dif­ferent from ours, we can make the 
similarity apparent by converting their formulation to that of the linear spring model. If we de.ne the 
behavior function (as they refer to it) as c = |xij |- L, then the force vector will be same as Equation 
2 and consequently the Ja­cobian .f/.x will also be the same. Therefore, when the spring is compressed, 
the Jacobian matrix has negative eigenvalues in the orthogonal direction as indicated above. [Baraff 
and Witkin 1998] reduced the possibility of the system matrix becoming inde.nite by the inclusion of 
a specially designed damping force. In their formulation, the damping force for stretch/compression does 
not contribute to the out-of-plane motion, but the damping force for bending acts along the out-of-plane 
direction. This damping force for bending will help to make the system matrix better conditioned at the 
cost of adding resistance to the out-of-plane movement of the cloth. Unfortunately there is still no 
guarantee that the damping will make the matrix positive de.nite. In our formulation, a highly stiff 
linear spring is used only for tension. Therefore the system matrix is guaranteed to be positive de.nite, 
and no additional damping term is needed to cure the diver­gence problem. However, the cloth may shrink 
under compressive load if no compression resistance is included in the model. In our method, the type 
2 interaction model simultaneously accounts for compression and bending, and consequently the buckling 
problem is handled in a single interaction model. 3.2 Our Solution to the Post-Buckling Instability 
As mentioned above, buckling causes serious stability problems in physical simulations of cloth. It is 
interesting to note that buck­ling has quite different meanings in rigid materials and textile fab­rics. 
Buckling means a failure in rigid materials, whereas it means success in textile fabrics [Amirbayat and 
Hearle 1989]. The post­buckling behavior in rigid materials is quite destructive, while the same behavior 
in textile fabrics naturally evolves into the shapes that are the essence of a fabric s appearance. We 
refer to [Amir­bayat and Hearle 1989] for this fabric-speci.c property. Although there are clear distinctions 
between the buckling behavior of fab­rics and rigid materials, they are not re.ected in most existing 
cloth simulation techniques. Since our solution to the post-buckling in­stability is based on the distinguished 
feature of fabric buckling, we clarify the concept of buckling in this section. First, we analyze the 
buckling of a rigid material in terms of solid mechanics. We then contrast this phenomenon with the buckling 
of textile fabrics. Consider the idealized rigid column shown in Fig­ure 3(a), which consists of two 
rigid bars connected at point C by a rotational spring of stiffness k.. In this con.guration, the bending 
resistance is condensed at point C. Now, suppose an axially com­pressive force P is applied at point 
A. Since the rotational spring resists bending, the two bars remain straight at equilibrium (Figure 3(b)). 
Now, suppose that the structure under force P is disturbed by an external force that causes a small lateral 
movement of point C (Figure 3(c)). The compressive force P will try to increase the  Figure 3: Column 
Buckling lateral displacement, while the rotational spring will try to restore the system to the original 
straight position. Suppose that the distur­bance is removed at this moment. If P is relatively small, 
the bars will return to the straight position (i.e. the structure is stable). How­ever, if P is relatively 
large the lateral displacement will become larger and larger (i.e. the structure is unstable), and the 
structure will eventually collapse by lateral buckling. The magnitude of the axial force at which the 
structure bifurcates into either the stable or unstable condition by application of an in.nitesimal increase 
or de­crease in force is known as the critical load and is denoted by Pcr. At the critical load, the 
de.ection of the structure is mathematically arbitrary. Once the axial load exceeds Pcr the structure 
collapses and the original shape cannot be recovered, regardless of the degree to which the load exceeds 
Pcr, and no matter how quickly the load is reduced back to a value less than or equal to Pcr. Now, let 
us consider the behavior when a fabric buckles. As for rigid materials, a textile fabric will buckle 
when subjected to an axial force greater than the critical load. When it buckles, it exhibits an unstable 
post-buckling response similar to that found in rigid materials. In contrast to rigid materials, however, 
fabrics do not break or collapse. Instead, they quickly pass the unstable state and reach a stable equilibrium 
(a smoothly bent shape). Moreover, the bent shape tends to return to the original straight shape when 
the axial load is removed [Amirbayat and Hearle 1989]. As described above, textile fabrics pass through 
an unstable state when they buckle. The simulation of this unstable post-buckling response requires special 
care if divergence problems are to be avoided. Once the material goes into the unstable post-buckling 
state, the de.ection increases even when the load decreases. In other words, the stiffness of the material 
in the buckling direc­tion is instantaneously negative. Especially in semi-implicit meth­ods where the 
internal force is explicitly predicted with derivatives [Baraff and Witkin 1998; Volino and Magnenat-Thalmann 
2000], this structural instability makes the system matrix extremely ill­conditioned or inde.nite, and 
a large time step often leads to di­vergence. There have been a number of efforts to avoid this post­buckling 
instability in the analysis of cloth deformation. For ex­ample, [Eischen et al. 1996] used an adaptive 
arc-length control of the load-de.ection curve, and [Yu et al. 2000] employed an explicit method with 
.ctitious damping to avoid divergence. We solve the structural instability problem by predicting the 
static post-buckling response. The approach developed here is based on the above observations regarding 
fabric-speci.c behavior. The concept underlying our approach is that since the fabric quickly passes 
the unstable post-buckling state to reach a stable equilib­rium, it has little chance to get into the 
unstable post-buckling state at the discrete time steps of the simulation. Thus we assume that the fabric 
is not in the unstable post-buckling state at any time step. Then, in calculating the internal force 
at each time step, we can evaluate the deformation energy in the area where the cloth buckles Figure 
4: Simpli.ed Structure for Type 2 Interaction (a) P/kb=23.50 (b) P/kb=35.25 Figure 5: Numerical Solutions 
of Moment Equilibrium Equation from the locally estimated deformed shape, which corresponds to the shape 
at the static equilibrium after buckling. The details of this procedure are presented in the next section. 
Unless the time step is miniscule, the loss of accuracy resulting from the approximated buckling response 
will be much less than the accuracy loss (and instabilities) resulting from the plain implicit time stepping. 
The approximated response model relieves the burden of simulating the unstable post-buckling dynamic 
response. According to our simu­lations, the approximated response model generates very realistic cloth 
motion with signi.cantly improved stability. Wrinkle forma­tion was quite natural. The simulation could 
be performed with a large step size. 3.3 Type 2 Interaction The type 2 interaction model is responsible 
for the post-buckling response created by compressive and bending forces. We predict the shape of the 
fabric after buckling and calculate the deforma­tion energy from the deformed shape as described in the 
previous section. The beam structure shown in Figure 4(a) approximates the re­gion between two particles. 
Prior to buckling, the structure is a straight beam of length L. After the structure buckles under a 
compressive load (Fig­ure 4(b)), it will eventually reach a stable equilibrium structure. To predict 
the equilibrium shape, we use the moment equilibrium equation under the pinned ends condition[Gere 2001], 
which is given by kb. + Py =0, (4) where kb is the .exural rigidity, . is the curvature, P is the com­pressive 
load, and y is the de.ection. Because we are modeling systems with large de.ections, we cannot use the 
approximation . = yll. Using the exact expression for the curvature2, we obtained several numerical solutions 
corresponding to different values of kb 2For a more general moment equilibrium equation for the analysis 
of fabric buckling, see [Kang et al. 2001]. and P . Two of these solutions are shown in Figure 5. The 
re­sults show that the shape after buckling is close to a circular arc even when the deformation is large. 
Therefore, we approximated the equilibrium shape as a circular arc with constant arc length. As an alternative, 
we could have constructed a table of the numerical solutions of the moment equation at various values 
of kP b for more accurate analysis. This was not undertaken because the results pro­duced using the circular 
arc assumption were quite realistic. The bending deformation energy can be calculated from the estimated 
shape using the relation: L 1 E = M.dx (5) 2 0 where M is the bending moment and . is the curvature. 
Taking into account the linear relationship between the curvature and bending moment, and the constant 
curvature through the structure, the inte­gral yields the solution: 1 E = kbL.2 , (6) 2 where kb is the 
.exural rigidity. Since the arc length is assumed to be the same as the initial straight length L, the 
curvature . can be expressed solely in terms of the distance |xij | between the two extremities. 2 |xij 
| . = sinc-1(), (7) LL where sinc(x)= sin xx . The force vector is derived as, d. xij d|xij | -1 xij 
fi = kb.L = kb.L(8) d|xij ||xij | d.|xij | 2.L .L -1 xij = kb.(cos - sinc( ))(9) 22 |xij | = fb(|xij 
|) xij (10)|xij | The blue curve in Figure 6 depicts the dependence of fb on the dis­tance between particles 
(approximated with a .fth order polynomial function). The unit of each axis in this graph is made dimensionless. 
The system shows the following behavior. When the compression force P is initially applied (top right 
corner of Figure 6), the struc­ture remains straight until the load reaches the buckling load Pcr. However, 
in real systems geometric imperfections in the structure cause the fabric to start to buckle at the onset 
of loading, giving an actual curve (the green curve in the graph) that exhibits .nite de.ection even 
at small magnitudes of the compression force, and asymptotically approaches fb as compressive force increases 
[Gere 2001]. To model this characteristic, we used the function fb * in our .nal implementation: * cb(|xij 
|- L): fb <cb(|xij |- L) fb = (11) fb : otherwise where cb is a constant of our choice, usually assigned 
a value com­parable to ks. Although we could have used a higher order function to model the de.ection 
at small values of the compression force, we found no signi.cant difference in the results obtained using 
higher order functions and those obtained with the linear function cb(|xij |- L) (red curve in Figure 
6). The Jacobian matrix of the force vector is derived as T f * T .fi df b * xij xij b xij xij = T +(I 
- T ). (12) .xj d|xij | xij xij |xij | xij xij Figure 6: Nondimensionalized Curves of P vs. |xij | df 
* f * In the above equation, b is always positive but b is always d|xij||xij | negative, creating the 
possibility that the second term could turn the system matrix inde.nite. To guarantee the positive de.niteness 
of the system matrix, we dropped this term and thus the force in the orthogonal direction is not affected 
by implicit .ltering. Although this force is not .ltered, there is little possibility that the system 
will diverge even when a large time step is employed because the magnitude of this force is very small 
compared to the stretch force, and it is always .nite. Note that in a model where the repulsive and contractive 
forces are equally strong, dropping the orthogonal term can make the system unstable under a large time 
step unless the bending resistance is of comparable strength. The above discussion of the type 2 interaction 
model highlights the necessity of interlacing the type 2 connections in the manner shown in Figure 2(b). 
If only sequential connections were used, the global shape could be bent without increasing the bending 
energy provided that each local connection maintained the initial distance. 3.4 Damping The physical 
model described above is quite stable; thus, there is no need for additional energy dissipative terms 
to stabilize the nu­merical procedure. However, we do need to consider the intrinsic damping property 
of fabrics. Without an appropriate (material in­trinsic) damping term, the simulated fabric can exhibit 
large unre­alistic in-plane oscillations. To include this type of damping, we added a simple linear damper 
along the direction of interaction. The damping force exerted on particle i from the interaction with 
particle j is given by, fi = -kd(vi - vj ) (13) and the Jacobian matrix is simply expressed as, .fi .vj 
= kdI. (14) Note that the force term in Equation 13 does not add any damp­ing to the orthogonal direction. 
This is important because the most interesting fabric deformation occurs in the out-of-plane direction. 
The above force term does not create a .ltering effect under a semi-implicit formulation when the cloth 
undergoes a linear rigid motion (i.e., when the velocity vectors of all the particles are identi­cal). 
This is the case because for such a motion the 3×3 block-wise row sums of the matrix ..vf are 3×3 zero 
matrices from the above equations, and the rigid motion vector vrigid is an eigenvector of ..vf with 
zero eigenvalue. Therefore, there is no implicit .ltering ef­fect caused by .f . Even when the .f terms 
in Equations 3 and 12 .v .x are included in the system equation, there will be no implicit .lter­ing 
to the rigid motion, provided that all the interacting directions of particle i are orthogonal to vi 
for all i and that the cloth is not stretched, since the ..xf terms in Equations 3 and 12 also produce 
null vectors when multiplied by vrigid. Therefore, the motion to the orthogonal direction would not be 
.ltered under a semi-implicit formulation. In more general situations, even though the above condition 
is not met, our formulation has very small arti.cial damping in the out-of-plane direction, which potentially 
makes the cloth movement look more realistic.  4 Numerical Integration We use semi-implicit integration 
with a second-order backward dif­ference formula (BDF). The k-th order BDF is de.ned as, k d 11 -1)q 
= (., (15) dt .tq q=1 where -1 n+1 n .x = x - x . For k =2, the discretization of x.becomes, 13 1 n+1 
nn-1 x.= ( x - 2x + x ). (16) .t 22 Considering both performance and accuracy, we chose second order 
BDF for the semi-implicit formulation. The second order BDF cre­ates less arti.cial damping than the 
.rst order BDF, but is equally stable. The state equation of motion x.= v (17) M-1 v.f is discretized 
with the second order BDF as 3 n+1 - 2x n 1 n-1 n+1 1 x + xv 32 n+1 - 2v n 21 n-1 = M-1fn+1 . (18) .t 
v + v 22 The nonlinear term fn+1 in the above equation is replaced with fn+1 .f n+1 n.f n+1 n = fn +(x 
- x )+ (v - v ). (19) .x .v By combining Equations 18 and 19, we can obtain a linear sys­tem rearranged 
either for .-1 x or .-1 v. If we rearrange the lin­ear system for .-1 x, the equation becomes, 2 -1 .f 
2 4 -1 .f n+1 n (I - .t M- .t M)(x - x ) 3 .v 9 .x 1 nn-1.t nn-1 =(x - x )+ (8v - 2v ) 39 4.t2 -1.f n2.t 
-1 .f nn-1 + M(fn - v ) - M(x - x ). (20) 9 .v 9 .v The linear system of Equation 20 is sparse and generally 
unbanded. We solve this system using a preconditioned conjugate gradient method. We used a 3 × 3 block 
diagonal matrix for the precondi­tioner, which showed an improvement of approximately 20% over the diagonal 
preconditioner. In addition, we assessed other precon­ditioners such as IC and ILU but found no performance 
gain though the number of iterations decreased. 5 Collision Handling Collision detection and response 
model is not a contribution of this paper. In this section, we brie.y describe how we handled collisions 
in our implementation. To detect collisions we use a voxel-based collision detection al­gorithm similar 
to that proposed by [Zhang and Yuen 2000]. After voxelizing the space in which the cloth is enclosed, 
we register each cloth particle and solid triangle to the corresponding voxels based on their spatial 
coordinates. Then, we independently perform colli­sion detection for each voxel. This voxelization method 
locates the possible collisions very ef.ciently and shows nearly linear perfor­mance. We detect the cloth-solid 
collision by checking the particle­triangle pairs to determine if particles are beneath the solid surface. 
To avoid missing pairs near the voxel boundaries, the triangles are redundantly registered to the nearby 
voxels. When a collision is detected, the particle s next displacement along the normal direc­tion of 
the colliding surface is determined, and this constraint is en­forced using the invariant method in the 
conjugate gradient iteration proposed in [Baraff and Witkin 1998]. For the tangential direction, we add 
a frictional force that is proportional both to the constraint force and to the velocity difference between 
the solid surface and the particle in contact. To test for self-collision, we check the particle-particle 
pairs. If the particles are too close, we simply add a repulsive proximity force between the colliding 
particles. The Jacobian matrix of this force is made to have null eigenvalues in the directions orthogonal 
to the repelling direction as in the case of the type 2 interaction model in Section 3.3. 6 Results 
This section reports the results from several simulations. The animations of these simulations can be 
found at http://graphics.snu.ac.kr/~kjchoi/cloth.htm Table 1 summarizes the performance of our algorithm 
on a Pentium3-550 machine. In this table, the CPU sec/frame .eld cites the total CPU time required to 
carry out all of the steps (i.e., col­lision detection, linear system setup, conjugate gradient iteration, 
etc.) required to produce one frame of 30 Hz animation. For all the simulations, the collision detection 
time was less than 20% of the total CPU time. The mesh resolutions of the clothes in the anima­tions 
were about L =1 ~ 2cm. For the simulations involving human motions (Animations 1~4), the time step was 
.xed to .t =1/90s throughout the anima­tion; thus the simulator produced one frame of 30 Hz animation 
with three time steps. The simulations were stable despite the use of a .xed time step. All attempts 
to use a time step greater than 1/90s encountered collision handling failures before the stability limit 
was reached. In Animation 1(a), the character is wearing a one-piece made of a thin fabric. The nature 
of the fabric was controlled by assigning a small value to the bending rigidity. The character walks 
at a normal pace without any fast movements. Nevertheless the cloth motion is quite responsive; the wrinkle 
details delicately form and disappear. However, we considered the cloth motion in Animation 1(a) to be 
more responsive than would be expected for a real cloth. To pro­duce a more fabric-like motion, we increased 
the bending rigidity and intrinsic damping, and reduced the frictional force. The result was Animation 
1(b). Animations 2 and 3 contain more vigorous character motions, which created very dynamic movement 
of the cloth and wrinkles. In Animation 4 the character is wearing jeans. The jeans fabric was modeled 
by assigning it a high bending rigid­ity and a high resistance to buckling. The animation produced the 
Table 1: Performance Summary animation # particles of cloth # triangles of solid CPU sec/ frame time 
step (s) 1(a) 5608 13802 6.13 0.011 1(b) 5579 13802 5.86 0.011 2 5579 15308 5.68 0.011 top3 skirt 3396 
3456 15308 15308 2.98 3.01 0.011 0.011 top4 pants 3294 6624 14324 14324 3.25 7.01 0.011 0.011 5 2601 
0 0.18 0.2  Figure 7: Snapshots from Animation 5 : Stability Test (.t =0.2s) buckled shapes near the 
knees and ankles quite well, which are fre­quently observed in real jeans. Several snapshots taken from 
each of the animations 1~4 are shown in Figure 8. Animation 5 was designed purely to determine the maximum 
time step that could be used in our cloth animation technique; it was not intended as a realistic animation. 
The simulation modeled a square of fabric draped over a solid box, as shown in Figure 7. To exclude the 
collision detection problem we did not explicitly include the box; Instead, we simply constrained the 
movement of the sub-square region of the fabric. Additionally, we disabled both self-collision and solid-collision. 
Under the above conditions, we veri.ed that the algorithm runs stably with time step sizes up to 100s, 
although the resulting animation was very choppy and it re­quired hundreds of time steps for the fabric 
to settle down to the .nal shape. Such a large time step is not meaningful and cannot possibly generate 
realistic animation because the derivatives have no signi.cance after 100 seconds. A marginally acceptable 
anima­tion with self-collision enabled was obtained with .t =0.2s. Sev­eral snapshots taken during this 
animation are shown in Figure 7. Although there is no established method or system for validat­ing the 
dynamic motion of cloth, our technique produced anima­tions that are visually quite believable. It is 
noteworthy that these animations were obtained using a reasonable, practical amount of computation. 
7 Conclusion The groundbreaking work of [Baraff and Witkin 1998] on im­plicit time stepping greatly reduced 
the computational cost of in­tegrating the stiff equations used in simulating textile fabrics, and thereby 
provided a practical solution for animating clothed char­acters. However, the phenomenon of buckling, 
which is another source of instability and a crucial property in cloth deformation, has been largely 
ignored until now. If the buckling problem can­not be handled appropriately, natural cloth animation 
would require enormous (almost impractical) amount of computation. This paper represents the .rst report 
of a stable and practical method to handle the post-buckling instability without introducing a damping 
force into the dynamic simulation. The proposed method was shown to produce very realistic motion of 
clothes made from a range of fabric types using a uniform time step size. In particular, the power of 
the new method was shown in the animation of a light and thin cloth where sensitive response of cloth 
is required, which was very dif.cult to produce using the previous methods. The tremendously increased 
stability of our algorithm allowed the simulation of the motion of cloth with time steps of 0.2 seconds 
or longer which could not be achieved in previous methods.  Acknowledgments This work was supported 
by Korea Ministry of Information and Communication. This work was also partially supported by Au­tomation 
and Systems Research Institute at Seoul National Univer­sity, and the Brain Korea 21 Project.  References 
AMIRBAYAT, J., AND HEARLE, J. 1989. The anatomy of buckling of textile fabrics : Drape and conformability. 
Journal of Textile Institute 80, 1, 51 70. BARAFF, D., AND WITKIN, A. 1998. Large steps in cloth simulation. 
In Proceedings of SIGGRAPH 98, ACM Press / ACM SIGGRAPH, Computer Graphics Proceed­ings, Annual Conference 
Series, ACM, 43 54. BATHE, K. J. 1996. Finte Element Procedures. Prentice-Hall. BREEN, D. E., HOUSE, 
D. H., AND WOZNY, M. J. 1994. Predicting the drape of woven cloth using interacting particles. In Proceedings 
of SIGGRAPH 94, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 
365 372. CARIGNAN, M., YANG, Y., MAGNENAT-THALMANN, N., AND THALMANN, D. 1992. Dressing animated synthetic 
actors with complex deformable clothes. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), ACM, 99 
104. COURSHESNES, M., VOLINO, P., AND MAGNENAT THALMANN, N. 1995. Versatile and ef.cient techniques for 
simulating cloth and other deformable objects. In Pro­ceedings of SIGGRAPH 95, ACM Press / ACM SIGGRAPH, 
Computer Graphics Proceedings, Annual Conference Series, ACM, 137 144. DESBRUN, M., SCHR ¨ ODER, P., 
AND BARR, A. 1999. Interactive animation of struc­tured deformable objects. In Graphics Interface, 1 
8. EBERHARDT, B., WEBER, A., AND STRASSER, W. 1996. A fast, .exible, particle­system model for cloth 
draping. IEEE Computer Graphics and Applications 16,5 (Sept.), 52 59. EISCHEN, J. W., DENG, S., AND CLAPP, 
T. G. 1996. Finite-element modeling and control of .exible fabric parts. IEEE Computer Graphics and Applications 
16,5 (Sept.), 71 80. GERE, J. M. 2001. Mechanics of Materials. Brooks/Cole. KANG, T., JOO, K., AND LEE, 
K. 2001. Analysis of fab­ric buckling based on nonlinear bending properties. (Submitted), http://textile.snu.ac.kr/upjuk/PDF/IJ 
TRJ 2002 KHJOO.PDF. PROVOT, X. 1995. Deformation constraints in a mass-spring model to describe rigid 
cloth behavior. In Graphics Interface 95, 147 154. TERZOPOULOS, D., AND FLEISCHER, K. 1988. Modeling 
inelastic deformation: Viscoelasticity, plasticity, fracture. In Computer Graphics (Proceedings of ACM 
SIGGRAPH 88), ACM, 269 278. VOLINO, P., AND MAGNENAT-THALMANN, N. 2000. Implementing fast cloth sim­ulation 
with collision response. In Proceedings of the Conference on Computer Graphics International (CGI-00), 
257 268. VOLINO, P., AND MAGNENAT-THALMANN, N. 2001. Comparing ef.ciency of in­tegration methods for 
cloth animation. In Proceedings of the Conference on Com­puter Graphics International (CGI-01). YU, W., 
KANG, T., AND CHUNG, K. 2000. Drape simulation of woven fabrics by using explicit dynamic analysis. Journal 
of Textile Institute 91 Part 1, 2, 285 301. ZHANG, D., AND YUEN, M. 2000. Collision detection for clothed 
human animation. In Proceedings of the 8th Paci.c Graphics Conference on Computer Graphics and Application 
(PACIFIC GRAPHICS-00), 328 337.  Figure 8: Snapshots from Animations 1~4. Each animation corresponding 
to each row shows different materials with different parameters. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566625</section_id>
		<sort_key>612</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Humans and animals]]></section_title>
		<section_page_from>612</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14028670</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566626</article_id>
		<sort_key>612</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Articulated body deformation from range scan data]]></title>
		<page_from>612</page_from>
		<page_to>619</page_to>
		<doi_number>10.1145/566570.566626</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566626</url>
		<abstract>
			<par><![CDATA[This paper presents an example-based method for calculating skeleton-driven body deformations. Our example data consists of range scans of a human body in a variety of poses. Using markers captured during range scanning, we construct a kinematic skeleton and identify the pose of each scan. We then construct a mutually consistent parameterization of all the scans using a posable subdivision surface template. The detail deformations are represented as displacements from this surface, and holes are filled smoothly within the displacement maps. Finally, we combine the range scans using <i>k</i>-nearest neighbor interpolation in pose space. We demonstrate results for a human upper body with controllable pose, kinematics, and underlying surface shape.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[character animation]]></kw>
			<kw><![CDATA[deformation]]></kw>
			<kw><![CDATA[human body simulation]]></kw>
			<kw><![CDATA[synthetic actor]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382423</person_id>
				<author_profile_id><![CDATA[81100651995]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P310471</person_id>
				<author_profile_id><![CDATA[81100620346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AUBEL, A., AND THALMANN, D. 2001. Interactive modeling of the human musculature. In Proc. of Computer Animation 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLANZ, V., AND VETTER, T. 1999. A morphable model for the synthesis of 3D faces. In Proceedings of ACM SIGGRAPH 99, Addison Wesley, New York, A. Rockwood, Ed., Annual Conference Series, 187-194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383309</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BUEHLER, C., BOSSE, M., MCMILLAN, L., GORTLER, S. J., AND COHEN, M. F. 2001. Unstructured lumigraph rendering. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Annual Conference Series, ACM, 425-432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CATMULL, E., AND CLARK, J. 1978. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer-Aided Design 10 (Sept.), 350-355.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74358</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHADWICK, J. E., HAUMANN, D. R., AND PARENT, R. E. 1989. Layered construction for deformable animated characters. Computer Graphics (Proceedings of ACM SIGGRAPH 89) 23, 3, 243-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>840039</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., AND LEVOY, M. 1995. Better optical triangulation through spacetime analysis. In Proceedings of IEEE International Conference on Computer Vision, 987-994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., AND LEVOY, M. 1996. A volumetric method for building complex models from range images. In Proceedings of ACM SIGGRAPH 96, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., Annual Conference Series, ACM, 303-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GILL, P. E., MURRAY, W., AND WRIGHT, M. H. 1989. Practical Optimization. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248379</ref_obj_id>
				<ref_obj_pid>243877</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GOLD, S., AND RANGARAJAN, A. 1996. A graduated assignment algorithm for graph matching. IEEE Transactions on Pattern Analysis and Machine Intelligence 18, 4, 377-388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GOWITZKE, B. A., AND MILNER, M. 1988. Scientific Bases of Human Movement, third ed. Williams & Wilkins, Baltimore, MD.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GUENTER, B., GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. 1998. Making faces. In Proceedings of ACM SIGGRAPH 98, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM, 55-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HERDA, L., FUA, P., PL&#196;NKERS, R., BOULIC, R., AND THALMANN, D. 2001. Using skeleton-based tracking to increase the reliability of optical motion capture. Human Movement Science Journal 20, 313-341.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KRISHNAMURTHY, V., AND LEVOY, M. 1996. Fitting smooth surfaces to dense polygon meshes. In Proceedings of ACM SIGGRAPH 96, Addison Wesley, H. Rushmeier, Ed., Annual Conference Series, ACM, 313-324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344829</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LEE, A., MORETON, H., AND HOPPE, H. 2000. Displaced subdivision surfaces. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., ACM, 85-94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344862</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J. P., CORDNER, M., AND FONG, N. 2000. Pose space deformations: A unified approach to shape interpolation and skeleton-driven deformation. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., Annual Conference Series, ACM, 165-172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LUTTGENS, K., AND WELLS, K. F. 1982. Kinesiology: scientific basis of human motion, seventh ed. CBS College Publishing, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>102317</ref_obj_id>
				<ref_obj_pid>102313</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MAGNENAT-THALMANN, N., LAPERRIERE, R., AND THALMANN, D. 1988. Joint-dependent local deformations for hand animation and object grasping. In Proc. Graphics Interface, 26-33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312164</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[NG-THOW-HING, V. 1999. Physically based anatomic modeling for construction of musculoskeletal systems. In Proceedings of the 1999 SIGGRAPH annual conference: Conference abstracts and applications, ACM Press, New York, NY 10036, USA, Computer Graphics, ACM, 264-264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, D. H. 1998. Synthesizing realistic facial expressions from photographs. In Proceedings of ACM SIGGRAPH 98, Addison Wesley, M. Cohen, Ed., Annual Conference Series, ACM, 75-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383277</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[PRAUN, E., SWELDENS, W., AND SCHR&#214;DER, P. 2001. Consistent mesh parameterizations. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Annual Conference Series, ACM, 179-184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[ROSE, C., COHEN, M. F., AND BODENHEIMER, B. 1998. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics and Applications 18, 5 (Sept./Oct.), 32-41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258827</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SCHEEPERS, F., PARENT, R. E., CARLSON, W. E., AND MAY, S. F. 1997. Anatomy-based modeling of the human musculature. In Proceedings of ACM SIGGRAPH 97, T. Whitted, Ed., Annual Conference Series, ACM, 163-172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SHEN JIANHUA, THALMANN, N. M., AND THALMANN, D. 1994. Human skin deformation from cross-sections. In Computer Graphics Int. '94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SILAGHI, M.-C., PL&#196;NKERS, R., BOULIC, R., FUA, P., AND THALMANN, D. 1998. Local and global skeleton fitting techniques for optical motion capture. In Proceedings of the International Workshop on Modelling and Motion Capture Techniques for Virtual Environments (CAPTECH-98), Springer, Berlin, N. Magnenat-Thalmann and D. Thalmann, Eds., vol. 1537 of LNAI, 26-40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364382</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SLOAN, P.-P., ROSE, C., AND COHEN, M. F. 2001. Shape by example. In Proceedings of 2001 Symposium on Interactive 3D Graphics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[TALBOT, J. 1998. Accurate Characterization of Skin Deformations Using Range Data. Master's thesis, Department of Computer Science, University of Toronto.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[TURK, G., AND LEVOY, M. 1994. Zippered polygon meshes from range images. In Proceedings of ACM SIGGRAPH 94, ACM Press, vol. 28 of Annual Conference Series, ACM, 311-318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258833</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[WILHELMS, J., AND GELDER, A. V. 1997. Anatomically based modeling. In Proceedings of ACM SIGGRAPH 97, T. Whitted, Ed., Annual Conference Series, ACM, 173-180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>279236</ref_obj_id>
				<ref_obj_pid>279232</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[ZHU, C., BYRD, R. H., LU, P., AND NOCEDAL, J. 1997. Algorithm 778. L-BFGS-B: Fortran subroutines for Large-Scale bound constrained optimization. ACM Transactions on Mathematical Software 23, 4 (Dec.), 550-560.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Articulated Body Deformation from Range Scan Data Brett Allen Brian Curless Zoran Popovi´c University 
of Washington Figure 1 Each of these 3D meshes are made from a skeletally driven subdivision surface. 
The displacements for the subdivision surface are interpolated from range-scan examples of the arm, shoulder, 
and torso in various poses. The joint angles for each pose are drawn from optical motion capture data. 
Abstract This paper presents an example-based method for calculating skeleton-driven body deformations. 
Our example data consists of range scans of a human body in a variety of poses. Using markers captured 
during range scanning, we construct a kinematic skeleton and identify the pose of each scan. We then 
construct a mutually consistent parameterization of all the scans using a posable subdi­vision surface 
template. The detail deformations are represented as displacements from this surface, and holes are .lled 
smoothly within the displacement maps. Finally, we combine the range scans using k-nearest neighbor interpolation 
in pose space. We demon­strate results for a human upper body with controllable pose, kine­matics, and 
underlying surface shape. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object 
Modeling Curve, surface, solid and object modeling; I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism Animation; Keywords: animation, character animation, deformation, human body simulation, 
synthetic actor 1 Introduction Creating realistic, virtual actors remains one of the grand challenges 
in computer graphics. Convincingly modeling human shape, mo­tion, and appearance is dif.cult, because 
we are accustomed to seeing other humans and are quick to detect .aws. One possible avenue to realism 
is through direct observation and measurement of people. Motion capture, for instance, has become a standard 
method for obtaining detailed samples of skeletal motion which can themselves be edited plausibly, and 
image-based techniques show promise for accurately modeling the appearance of skin. In this pa­per, we 
explore a data-driven approach to modeling the shape of the email: {allen,curless,zoran}@cs.washington.edu 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 human body in arbitrary poses. Recent years have witnessed the evolution of numerous range scanning 
technologies, including whole-body scanners that can capture the static shape of a person quite accurately. 
Given such a static scan, an animator can warp the body into a different pose, but this approach ignores 
an important aspect of human movement: muscles, bones, and other anatomical structures continuously shift 
and change the shape of the body. Clearly, to create compelling animations by observation we need more 
than just a single scan. Scanning the subject in every pose needed for every frame of an animation is 
impractical; instead, we propose a system in which body parts are scanned in a set of key poses, and 
then animations are generated by smoothly interpolating among these poses using scattered data interpolation 
techniques. The concept of interpolating sampled poses is not a new idea. What makes our approach unique 
is the use of real-world data to create a fully posable 3D model. In the process, we face several challenges. 
First, in order to establish a domain for interpolation, we must discover the pose of each scan. Second, 
interpolation techniques require a one-to-one correspondence between points on the scanned surfaces, 
but the scanned data consists of unstructured meshes with no such correspondence. This problem is particularly 
challenging because the scans are in different poses, so standard rigid-body registration techniques 
will not work. Third, range scans are frequently incomplete because of occlusions and grazing angle views. 
Thus, we are faced with the challenge of .lling holes in the range data. Finally, due to the combinatorics 
of the problem, we cannot capture a human body in every possible pose. Thus, we must blend between independently 
posed scans. In this paper, we present a general framework that addresses each of these problems. Using 
markers placed on the subject during range scanning, we reconstruct the pose of each scan. We then create 
a hole-.lled, parameterized reconstruction at each pose us­ing displacement-mapped subdivision surfaces. 
Lastly, we create shapes in new poses using scattered data interpolation and spatially varying surface 
blending. On the way to achieving our goal we make contributions to the problems of .tting a skeleton 
to marker data, surface correlation for articulated objects, fair hole .lling of surfaces, example-based 
in­terpolation with quaternion parameters, and blending range scans. Our primary contribution, however, 
is the process itself and the demonstration that we can derive realistic, posable human body de­formations 
from range scan data. 1.1 Related work The two main approaches to modeling body deformations are anatomical 
modeling and example-based approaches. The idea be­hind anatomical modeling is to use an accurate representation 
of the major bones, muscles, and other interior structures of the body. These structures are deformed 
as necessary when the body moves, and a skin simulation is wrapped around the underlying anatomy to obtain 
the .nal geometry. There is a large body of work on anatom­ically based approaches, including Wilhelms 
and Gelder [1997], Scheepers et al. [1997], Victor Ng-Thow-Hing [1999], and Aubel and Thalmann [2001]. 
The primary strength of anatomical approaches is the ability to simulate dynamics and complex collisions. 
The main drawback is their computational expense, since one must perform a physical simulation to generate 
every frame, while taking care to conserve muscle volumes, and stretch the skin correctly. An alternative 
paradigm is the example-based approach, where an artist generates a model of some body part in several 
different poses with the same underlying mesh structure. These poses are correlated to various degrees 
of freedom, such as joint angles. An animator can then supply new values for the degrees of freedom and 
the examples are interpolated appropriately. Example-based approaches are much faster computationally, 
and creating exam­ples is often easier than creating a detailed and accurate anatomical model. Lewis 
et al. [2000] and Sloan et al. [2001] describe similar tech­niques for applying example-based approaches 
to meshes. Both techniques use radial basis functions to supply the interpolation weights for each example, 
and, for shape interpolation, both re­quire hand-sculpted meshes that ensure a one-to-one vertex corre­spondence 
exists between each pair of examples. This paper will also use an example-based approach, but the key 
difference is that we will start with uncorrelated range-scan data. In fact, with our method, even the 
poses of the examples will be derived from the data. Other example-based approaches use scanned or photographed 
data. In the domain of facial animation, example-based techniques have been developed by Pighin et al. 
[1998], Guenter et al. [1998], and Blanz and Vetter [1999]. One of the few attempts to create articulated 
deformations from scanned examples is the work of Tal­bot [1998], who created a partial arm model with 
one degree of freedom. Our work takes a broader scope and can be applied to complex articulated .gures. 
 1.2 Problem formulation We formulate the problem of creating a posable human body as a scattered data 
interpolation problem in which shape examples are blended linearly to create new shapes. Thus, we must 
de.ne a do­main over which samples are taken and represent the samples in a form suitable for blending. 
The domain consists of all of the knobs that an animator will be able to tweak, such as controls for 
joint angles, muscle loads, body types, and so on. In our example upper­body model, the domain will consist 
entirely of joint angles, but in principle any kind of parameters could be used. Throughout this paper 
we will refer to a vector in the joint space as q. Having established the interpolation domain, we next 
need to select and enforce a representation suitable for blending between the example shapes. For unstructured 
range scans, this amounts to constructing a correspondence between surface points on dif­ferent scans, 
i.e. , a mutually consistent parameterization. To this end, we will employ displaced subdivision surfaces, 
as introduced by Lee et al. [2000]. Displaced subdivision surfaces consist of a template subdivision 
surface, T, and a displacement map d that describes the .nal surface S by displacing the template along 
the normal, n , to the template surface. This representation is a kind of layered model [Chadwick et 
al. 1989], where the the local de­tail deformations are separated from the large scale (mostly af.ne) 
 Figure 2 (a) Photograph of the subject in the scanner. The left arm is about to be scanned. The ropes 
help the subject remain motionless during the scan. (b) The scanned surface with color data, rendered 
emissively. Note that sur­faces parallel to the scanner s rays, such as the side of the torso, are not 
cap­tured. The four meshes that were captured simultaneously have been regis­tered. (c) Scanned surfaces 
after applying dot-enhancing .lter to the color data. (d) Combined and clipped arm scan, rendered with 
Gouraud shading. transformations applied to each body part. We will drive the under­lying template surface 
using the pose, q, resulting in a pose-varying surface: S(u, q)= T(u, q)+ d(u, q)n (u, q) (1) Notice 
that d is also a function of the pose, q. Unlike stan­dard displaced subdivision surfaces, our displacements 
are based on multiple example shapes, allowing scattered data interpolation techniques to be applied. 
In particular, the interpolated displace­ments are a weighted sum of the example displacements: n L d(u, 
q)= wi(u, q)di(u) (2) i=1 where n is the number of examples, di(u) is the displacement map for the ith 
example, wi(u, q) is the scattered data interpolation weighting function for the ith example, and d(u, 
q) is the interpo­lated displacement map for pose q. In the remainder of the paper, we describe the steps 
taken to construct an example-based posable human body: 1. Capture a set of example scans with markers 
(Section 2). 2. Using the markers, solve for the global kinematics of the body, k, and the local pose 
of each scan, qi (Section 3). 3. Create a template surface, T(u, q), based on the kinematics of the 
body, parameterize and resample the examples into dis­placement maps di(u), and .ll in any missing values 
(Sec­tion 4). 4. Compute the interpolation weights, wi(u, q) (Section 5).  We demonstrate results using 
an upper body model in Section 6 and discuss conclusions and future work in Section 7. 2 Data acquisition 
This section explains how we acquired our example data set. The overall idea is to sample the body s 
shape in a variety of poses cov­ering the full range of motion for each joint. At the same time, we capture 
the location of markers on the body that we will use to determine the pose of each scan. Left arm data 
set (36 scans) Elbow bend 0. , 60. , 90. , 130. Elbow twist 0. , 60. , 130. Wrist .exion -45. , 0. , 
30. Left shoulder data set (33 scans) Shoulder and clavicle .exion, neutral, extension abduction, neutral, 
adduction medial rotation, neutral, lateral rotation shoulder girdle elevation (shrug), depression, protraction 
(forwards), retraction (backwards) Torso data set (27 scans) Waist and abdomen pronation, neutral, supination 
(twist) left and right lateral .exion, neutral left and right rotation, neutral Table 1 We captured 
three data sets, each of which covered the range of mo­tion of a group of joints, shown in the left column. 
The joint angles that we sampled are described in the right column. For an explanation of the termi­nology, 
the reader may refer to any reference on biomechanics or kinesiology, such as Gowitzke and Milner [1988]. 
2.1 Range scanning We acquired our surface data using a Cyberware WB4 whole-body range scanner. This 
scanner captures range scans and color data from four directions simultaneously and has a sampling pitch 
of 5 mm horizontally and 2 mm vertically. Figure 2(a) shows the subject in the scanner. Overhead ropes 
helped the subject remain motionless during the seventeen seconds of scanning time. The scanned surface 
with color data is shown in Figure 2(b). The same mesh after merging the four scans [Curless and Levoy 
1996] and clipping out the arm is shown in Figure 2(d). 2.2 Pose coverage To create an upper body model, 
we needed to sample all poses of the wrist, elbow, shoulder, and torso. Due to the combinatorial nature 
of the problem, we split the upper body into three data sets captured separately: the arm, the shoulder, 
and the torso. We can split the body up because the joints on each part have little in.uence on the shape 
of distant body parts. At the interface between adjacent body parts, we must overlap the capture regions 
and blend them at a later stage. We also save work by capturing only the left arm and left shoulder and 
later mirroring the data to the right side. Table 1 gives a summary of all captured poses. In the interest 
of taking as few scans as possible we made our sampling space fairly sparse. We sampled at least three 
angles for each degree of freedom, giving us a neutral middle value and the two extremes that generally 
have the most dramatic shape changes. 2.3 Markers To enable precise determination of each scan, we placed 
colored markers on the subject using costume make-up. The markers are picked up by the scanner s color 
video cameras and mapped onto each range image. We used eight different marker colors to aid the identi.cation 
process. Note that re.ectance discontinuities when picked up by a range sensor can lead to geometric 
errors [Curless and Levoy 1995]. Our range data does not suffer from these arti­facts, because the whole-body 
scanner uses an infrared laser and does not distinguish between skin and make-up colors. For the arm 
and shoulder data sets, we used forty-eight markers, and for the torso data set we used ninety markers. 
Our goal was to have at least three markers visible per body part (the minimum number of markers capable 
of establishing a coordinate frame), and since the markers were often occluded or hard to identify we 
placed roughly four times that many. Estimating poses from the marker imagery requires two pre­processing 
steps: locating the 3D coordinates of each marker and Figure 3 (a) Our upper body skeleton after optimization. 
The large spheres are quaternion joints, and the cones are single-axis joints. (b) The con­trol points 
for this skeleton, and the corresponding subdivision surface. The checkerboard pattern delineates the 
subdivision patches. (c) The control points and subdivision surface after re.tting. labeling and identifying 
the markers across all scans. To automate the process of locating the markers, we applied a broad Laplacian 
convolution .lter to the color data of each range scan. This .lter makes the dots stand out from the 
skin, as shown in Figure 2(c), so they can easily be identi.ed by searching for extreme color values. 
Our marker-.nding algorithm groups neighboring pixels of similar color and rejects clusters that are 
too large, too small, or too near the edge. By referring back to the range values, we .nd the 3D location 
of each pixel in the cluster and take the centroid. The second step of marker-.nding is to label the 
markers. Each marker that was placed on the subject is assigned a numerical index. We then determine 
the index of each marker located in the marker­.nding step. We applied the graph-matching technique of 
Gold and Rangarajan [1996] based on matching geometric relationships and marker color; unfortunately 
we found this approach unsuitable due to the large number of missing markers. Consequently we label the 
markers manually after running our automatic location-.nding technique. We hope to automate this step 
in future work.  3 Determining kinematics and pose We can think of each scan as an example of the body 
s shape in one particular pose. Therefore, we need to know the exact pose, qi, of each scan. We also 
need to know the kinematics, k, of the body s skeleton, that is, the .xed transformations between each 
joint. This section describes our method for automatically deter­mining the poses and kinematics of the 
scanned bodies. 3.1 Skeleton We construct a skeleton containing the joints that the end user of our system 
will be able to animate. The goal is to have a skeleton that is a good approximation of true human kinematics, 
but not too com­plicated to solve for or animate. This tradeoff exposes some impor­tant design issues. 
For example, the human shoulder joint consists of four joints: one between the sternum and clavicle, 
one between the clavicle and scapula, one between the scapula and humerus, and one between the scapula 
and the rib cage [Luttgens and Wells 1982]. However, the second and third joints are very close together, 
and the fourth joint has very little independent movement. Thus, we simplify the shoulder complex to 
two joints: a clavicle joint and a shoulder joint. The human spine is much more complicated, consisting 
of seventeen joints each with its own range of motion. We reduce the spine to just two joints, one at 
the waist and one at the abdomen. Another example is the elbow, which consists of two single-axis joints. 
Animators typically make the assumption that the axes of these joints are perpendicular and colocated. 
This is not in keeping with the actual bone structure of the human arm, and so our choice of skeleton 
allows the axes to have any relative orientation and an arbitrary translation between them. (We prefer 
a small translation, to prevent the bones from moving along the axes of rotation during the optimization.) 
The skeleton hierarchy is rooted with a base transformation which moves from the origin of world coordinates 
to the coordinate frame of the hips. After the base transformation, each rotation joint in our skeleton 
is followed by a translation to the next joint. We will call these translation components the bone translations. 
Our upper-body skeleton (after optimization) is show in Figure 3(a). 3.2 Local marker positions The 
local marker positions, m, are a collection of 3D points de­scribing the position of each marker within 
its joint s coordinate frame. We initially assign each marker to a joint coordinate frame based on its 
location on the body. For example, markers on the lower back are placed in the waist joint s coordinate 
frame, and markers on the upper back are placed in the abdomen joint s coor­dinate frame. The markers 
will be treated as if they moved rigidly with the skeleton. This assumption is not entirely accurate 
be­cause of the body deformations that move the marker in non-rigid ways. However, we have obtained satisfactory 
results by using many markers and taking a least squares approach. Even though the local marker positions 
will not be used at all in our deformation-building process, it is necessary to calculate them when solving 
for the poses and kinematics.  3.3 Optimization A summary of all of the skeleton parameters is shown 
in Table 2. The goal of the optimization step is to determine the values of all of these parameters that 
best match the marker data. Note that we can generate arbitrarily many versions of the same skeleton 
by applying a constant rotation to one joint in all frames, and then adjusting the bone translations 
and local marker posi­tions to compensate. As a result, our skeleton parameterization is under-determined. 
For example, we could call the elbow angle of a straight arm 0. or 180. or any other angle, and all other 
arm poses will be measured relative to this. To eliminate this extra degree of freedom, we must lock 
all of the rotation joints in one of the scans to .xed values (such as zero) in order to provide a frame 
of refer­ence to which the rotations will be compared. We call this special scan the reference scan. 
De.ning a reference scan offers an additional advantage: it pro­vides an initial guess for the local 
marker positions, m. Since the joint angles are pre-determined for the reference scan, we need only supply 
a rough approximation for the base transformation. The lo­cal marker positions for all markers visible 
in that scan can be easily computed and later re.ned. We also lock any degrees of freedom that cannot 
be determined from the given marker data. For example, since we scanned only the left arm, we cannot 
solve for the joint angles in the right arm. In addition, we lock the torso joints for all of the arm 
and shoulder scans, and the arm angles in all of the torso scans. We can now optimize over all remaining 
degrees of freedom. The objective function minimizes the sum of the squares of the dis- Name # global 
DOFs # per-scan DOFs Base translation 0 3 Base rotation 0 4 Waist rotation 0 4 Waist translation 3 0 
Abdomen rotation 0 4 Left/right abdomen translation 3 0 Left/right clavicle rotation 0 8 Left/right clavicle 
translation 3 0 Left/right shoulder rotation 0 8 Left/right upper arm translation 3 0 Left/right elbow 
bend 2 2 Left/right elbow translation 3 0 Left/right elbow twist 2 2 Left/right lower arm translation 
3 0 Left/right wrist bend 2 2 Left/right hand translation 0 0 Local marker positions 411 0 Table 2 Degrees 
of freedom (DOFs) of the skeleton. Global DOFs are con­stant across all scans; per-scan DOFs take on 
a different value for each scan. The left/right translations are mirror images of each other and thus 
share DOFs. The single-axis rotations in the arm have two global DOFs indicat­ing the direction of the 
axis and two per-scan DOFs for the angles about that axis on the left and right arm. The hand translation 
has no DOFs because there are no joints below the hand in our model. We will call the per-scan DOFs qi, 
and the local marker positions m; the remaining global DOFs comprise the kinematics, k. tances between 
the calculated marker positions and the observed marker positions: pm LL arg min .oij - c(mj; qi, k).2 
(3) m,q,k i=1 j=1 where p is the number of poses, m is the number of markers, oij is the observed location 
of marker j in scan i, and c(mj; qi, k) is the calculated position of the same marker. In cases where 
a marker cannot be located in a pose due to scanning limitations, we omit the corresponding term from 
the summation. This skeleton-.nding problem is identical to the problem of .t­ting a skeleton to optical 
motion capture data. Silaghi et al. [1998] and Herda et al. [2001] have investigate this problem and 
describe a local (joint-by-joint) optimization technique for initializing the global optimization stated 
above. An initialization is necessary be­cause the search space contains many local minima. However, 
we can avoid this extra step of running a local optimization using two improvements. First, because we 
calculated initial values for the local marker positions using the reference scan, we can start our global 
solver with these positions locked. The solver usually reaches a bad local minimum because it moved the 
local marker positions to unrea­sonable locations and compensated with erroneous poses and bone lengths. 
By locking the local marker positions, we guide the solver toward .nding reasonable poses .rst. After 
this optimization con­verges, we run it again with the local marker positions unlocked to get the best 
.t. The second technique we use to aid convergence is scaling the degrees of freedom (DOFs). By scaling 
the DOFs, we ensure that all of their gradients have the same magnitude, improving solver performance 
[Gill et al. 1989]. First of all, we must account for the fact that our set of DOFs contains three kinds 
of values: radians, meters, and quaternions. We scale each of these so that their values range from -1 
to 1. We then further scale each DOF according to how many joints are in.uenced by it. Thus, per-scan 
DOFs have a scaling factor equal to the number of transforms below that DOF,  Figure 4 (a) To construct 
a displaced subdivision surface, we cast rays (red arrows) perpendicular to the template subdivision 
surface (dashed blue line) to the nearest scanned surface (thick gray line). Because the direction of 
each ray is determined by the subdivision surface, we need only record the distance. (b) If the template 
surface is too curved and the scanned surface is too far away, then the rays can cross, causing the parameterization 
to fold over on itself. This can be avoided by ensuring that the template surface is close to the scanned 
surface. and global DOFs are weighted by the number of transforms they in.uence multiplied by the number 
of scans. We use L-BFGS-B, a quasi-Newtonian solver to optimize the goal function [Zhu et al. 1997]. 
We analytically compute the deriva­tives of Equation 3 relative to each degree of freedom. The running 
time for convergence is approximately forty minutes on a 1.5 GHz Pentium 4. This optimization only has 
to be run once because it incorporates all of the scanned poses for all body parts.  4 Determining 
deformations At this point, we have determined the joint angles and the bone lo­cations for each scan. 
The next step is to represent the deformations that each body part undergoes in each pose. The key issue 
here is one of correspondence: if we choose a vertex in one scan, where is the corresponding vertex in 
the other scans? 4.1 Parameterization To overcome this dif.culty, we devise a parameterization that is 
based on the skeleton, since each scan has the same skeleton in a known pose. To do this, we need to 
choose a parameterization that can move with the skeleton. One possibility is a cylindrical cross­section 
based parameterization as used by Shen et al. [1994]. This parameterization works well for cylinder-like 
body parts such as the arm, but it is inconvenient to use for branching body parts, such as the torso. 
A more general parameterization can be derived from displaced subdivision surfaces, as described by Lee 
et al. [2000]. Essentially, one creates a subdivision surface that approximates the real sur­face, and 
records the distance to the real surface along the normal by raycasting, as shown in Figure 4(a). We 
employ a Catmull-Clark subdivision surface [Catmull and Clark 1978] starting from a quadrilateral control 
mesh. We could have used other displacement­based approaches, such as displaced B-spline surfaces [Krishna­murthy 
and Levoy 1996]; we chose a subdivision surface template because of its ease in handling irregular vertices, 
i.e., control ver­tices with valence other four, which appear near the red patches in Figure 3(c). The 
work of Praun et al. [2001] could also provide con­sistent parameterizations across poses, though this 
approach oper­ates on hole-free meshes and would require substantial modi.cation to interpolate articulated 
body structures. Lee et al. [2000] use a simpli.ed version of the target mesh to de.ne the control points. 
In our case, we want the control points to depend on the skeleton. We de.ne coordinate frames on the 
skele­ton based on joint coordinate frames. We then place rings of control points into these frames and 
a form a quadrilateral control mesh that follows the skeleton. To ensure that we have smooth transi­tions 
near the joints, the control point coordinate frames may be combinations of adjacent joint coordinate 
frames. For example, the coordinate frame centered at the abdomen joint has a rotation half­way between 
the lower spine s orientation and the upper spine s orientation. The resulting surface appears in Figure 
3(b). 4.2 Fair hole .lling One of the critical problems with range scan data is that the meshes are 
generally incomplete. To interpolate the examples, however, we need complete information. One might think 
that the problem could be avoided by basing the pose space interpolation at each vertex on just the examples 
that do not contain a hole at that point. This approach has two problems. First, surface discontinuities 
will arise at the hole boundaries, because the adjacent points will be based on data drawn from different 
meshes. Secondly, the presence of holes is strongly correlated with the pose of the body, and so entire 
groups of poses will not have any data for a particular area. Thus, the missing data could only be drawn 
from poses that are quite different from the ones with holes. Instead, we .ll holes directly in each 
scan. Hole-.lling in 3D can be quite tricky; we simplify the problem by operating directly on the displacement 
maps. We can easily detect the presence of holes within our parameterization when a displacement ray 
does not hit the surface. Our idea is to .ll the holes by smoothly inter­polating displacement values 
from neighboring vertices. Using the displacement parameterization we have made our 3D hole-.lling problem 
analogous to the 2D problem of image inpainting, by con­sidering the displacement values to be a grayscale 
image (on an unusual manifold). Observing that our displacement images are typically very smooth and 
continuous, we .ll in the missing area by minimizing curvature using a discrete thin-plate objective 
function. Since the points near the missing data are typically unreliable, we also apply the objective 
function near the edges of the holes, but with an addi­tional term to keep those points close to their 
original value. Stated mathematically, we compute: n L 2 2 arg min jd(uj) - d (uj)+(1 - j) 2d(uj)(4) 
d(uj) j=1 where n = the number of points to be .lled or faired uj = jth point in the parameterization 
u d(uj) = the new displacement at uj d (uj) = the original displacement at uj j = 0 inside the hole, 
ramping toward 1 within three pixels of the hole The results of this algorithm as applied to one of the 
scans are shown in Figure 5. The results are generally satisfactory; the most noticeable artifact is 
the absence of range sensor noise in the .lled region. 4.3 Re.tting A signi.cant problem with displaced 
subdivision surfaces occurs when the template surface is too far from the scanned surface and the curvature 
is too high. In this situation adjacent rays will cross and part of the surface will be covered several 
times. This prob­lem is illustrated in Figure 4(b). The solution is to ensure that the subdivision surface 
is close enough to the scanned surface. Our initial mesh, shown in Figure 3(b), is reasonably close to 
the scanned mesh, but it still has some problem areas. To avoid requir­ing excessive hand-tweaking from 
the user, we seek an automatic re.tting step. Ideally, we would like to optimize the template s con­trol 
points so that the surface is as close as possible to the data surface at all points in all poses. We 
take a simpler approach that works reasonably well in practice. After calculating the hole-.lled displaced 
subdivision surface, we move the control points so that the subdivision surface goes exactly through 
the scanned surface at the control points in a selected average pose. This step is done by Figure 5 
Hole-.lling an arm scan. On the top we show the displacement val­ues on the template surface; blue indicates 
zero displacement, magenta a neg­ative displacement, and cyan a positive displacement. The subdivision 
surface with the displacements applied is shown on the bottom. (a) Original surface after parameterization. 
There is a large hole along then forearm, and smaller holes in the underarm, shoulder, and hand. (b) 
We initialize the missing areas with a zero displacement. (c) After running one-quarter of the smoothing 
opti­mization. (d) After full optimization. The discontinuity between the shoulder and the torso is intentional. 
solving the linear system MA = V, where V is the desired template surface locations at each control point, 
M is the limit mask matrix, and A is the new control point positions. Since there are only 72 control 
points, this is an easy calculation. The re.tted surface is shown in Figure 3(c). An additional motivation 
for having a template surface that is close to the scanned mesh is that it allows us to reject rays that 
intersect too far away. This problem occurs particularly in regions such as the elbow crease and underarm 
where cast rays pass through holes in the mesh and hit a surface much further away. Having a well-.t 
template surface allows us to easily reject these rays by treating large displacements as holes.  5 
Interpolation and Reconstruction Referring back to our formulation in Equations 1 and 2, we have now 
established a template surface, T(u, q), and a complete dis­placement map, di(u, q), for each example. 
All that remains is to specify the weighting function for each example, wi(u, q). We split this into 
two functions: wip(q), which performs scattered data inter­polation based on the pose, and wib(u), which 
blends the arm, shoul­der, and torso data sets. These two functions will be multiplied to give: wi(u, 
q)= wip(q)wib(u). 5.1 Pose-based weight calculation Given a new point in the pose space we need to calculate 
a weight, wip(q), for each example. The interpolated displacements will be a linear combination of the 
examples, using these weights. These weights have three constraints: 1. At an example point, the weight 
for that example must be one, and all other weights must be zero. 2. The weights must always sum to 
one. 3. The weights must be continuous so that animation will be smooth.  We initially tried using 
cardinal radial basis functions, as de­scribed by Sloan et al. [2001]. This worked well for the our arm 
model because it consists only of single-axis rotations. However, when working with full quaternion rotations, 
naive application of Figure 6 Blending the three data sets. (a) A sample arm pose. (b) A sample shoulder 
pose. (c) A sample torso pose. (d) Blend of arm, shoulder, and torso, with a mirrored right shoulder 
and right arm. Color indicates the blending weight. radial basis function interpolation does not work 
well, because it treats the quaternion components (or Euler angles) as if they were independent linear 
dimensions. Another problem with cardinal ra­dial basis functions is that they give negative weights. 
Although there is no problem with small negative weights, in some regions of joint space the magnitude 
of the weights becomes quite large, exaggerating the deformations to an unreasonable degree. An alternative 
technique is k-nearest-neighbors interpolation. The idea is to choose the k closest example points and 
assign each of them a weight based on their distance. All other points are assigned a weight of zero. 
The goal is to create a function of the distances that meets the three criteria listed above. Buhler 
et al. [2001] devel­oped an interpolation function of this sort. Before normalization, it takes the form: 
p 11 wi (q)= - (5) D(q, qi) D(q, qt) where D(q, qi) is the distance between the new points and example 
i, and t is the index of the kth closest example. For our upper body model, we found that k = 8 gave 
satisfactory results. We use a different distance function for each data set. For scans from the arm 
data set we use a distance function of (.elbow angle)2 + (.forearm twist)2 + (.wrist angle)2. In the 
shoulder and torso data sets, the pose space includes quaternions, so we de.ne a more appropriate distance 
function: the great-arc length between the two quaternions on a four-dimensional sphere. The weights 
must be normalized since they will not necessarily sum to one. If the desired pose is equal to an example 
pose, then that example has in.nite weight and, after normalization, is the sole contributor to the reconstructed 
shape in that pose.  5.2 Combining Parts Using the technique above for calculating wip, we can interpolate 
the shape of each body part separately. The .nal step is to blend the body parts together using the spatially 
varying blending weight, b wi . Subdivision patches which are only covered by one data set have  Figure 
7 An interpolation, in gray, between two poses with different elbow angles (above) and different shoulder 
and clavicle angles (below). The red models on the right were generated by applying the displacements 
from the left-most poses onto the subdivision surface from the right-most poses. The top red model shows 
that the protrusion of the elbow and the slight contraction of the biceps are determined by the displacements. 
In the bottom red model notice that the dimples at the top of the shoulder and at the scapulae, and the 
correction of the underarm are not visible if the displacements are not updated. a blending weight wib(u) 
of 1 if i is a member of the data set and 0 otherwise. For areas that are covered by more than one data 
set, we need to smoothly blend across the overlapping region. Therefore our blending function must take 
the value 0 at one boundary and 1 at the other boundary of the overlap region. A linear blending function 
based on Euclidean distance has this property. However, we also want our function to be C1 continuous 
at the edges. Therefore we need to use a higher order blending function; we chose to use one wave of 
the cosine function as follows: [()] b 1 b(ui) wi (u)=1+ cos - 1. 07(6) 2 x where b(ui) is the distance 
between ui and the patch boundary, and x is the width of the overlap region. The blending weights for 
our upper body model and a sample blend are shown in Figure 6. We can construct a right arm and shoulder 
by mirroring all joint angles and deformation data through the sagittal plane, thereby avoiding the work 
of scanning both arms.  6 Results We have tested our system for creating posable human shapes start­ing 
from the data set described in Section 2. Figure 7 shows two simple interpolations between novel poses. 
In each of these .g­ures we also show the effect of moving the template surface but not adjusting the 
displacements in order to illustrate the difference between the deformation caused by the template surface 
and the de­formation cause by the interpolated displacements. The most egre­gious error in the non-interpolated 
meshes is at the elbow, where the bones of the arm do not protrude. Other prominent artifacts in­clude 
a lack of swelling of the biceps, and for the shoulder example, missing creases in the shoulder, and 
a protrusion in the right under­arm. The interpolated meshes have none of these problems and are a more 
faithful portrayal of the subject s anatomy. We can also control our model with motion capture data. 
Fig­ure 1 demonstrates a variety of poses drawn from motion capture of a different individual, with the 
joint angles mapped onto the range scanned subject s skeleton. The accompanying video shows full animations 
generated from motion capture data. Although some of the poses in Figure 1 go beyond the range of pose 
space that we captured, the template surface enables extreme poses to look rea­sonable. The biggest problems 
arise in the crease areas, such as the inside of the elbow and the underarm. Creases cause problems for 
three reasons. First, they cannot be accurately scanned because parts of the surface are completely occluded. 
Secondly, creases are by their very nature areas of high curvature, which, as shown in Figure 4(b), can 
be a problem for the displaced subdivision surface parameter­ization. Our re.tting algorithm helps, but 
occasionally poorly pa­rameterized areas remain. Finally, our approach does not perform actual collision 
detection. As a result, it cannot be expected to ob­tain correct results when the deformations are caused 
by collisions. Figure 7 shows evidence of these issues; in the top right gray pose, a small ridge near 
the elbow crease is caused by interpolating a creased and non-creased surface. One strength of our approach 
is speed. Our upper body model has a control mesh with 65 faces, and each face is subdivided .ve times, 
giving a mesh with roughly sixty-six thousand vertices. Even with this dense mesh, we can generate and 
render novel poses at nearly interactive rates (3-5 frames per second); this rate can be increased by 
by sampling at a smaller subdivision level. Although the model we have developed yields a fairly faithful 
reproduction of the posable shape of only a single individual, the framework does enable some editing 
operations to change the body appearance of that individual. For instance, changing bone lengths or scaling 
the template control points relative to the skeleton are straightforward to implement; examples of these 
operations appear in Figure 8. 7 Conclusion We have developed an end-to-end system for capturing human 
body scans, estimating poses and kinematics, reconstructing a complete displaced subdivision surface 
in each pose, and combining the sur­faces using k-nearest-neighbors scattered data interpolation in pose 
space. The result is an example-based, posable model that captures high de.nition shape changes over 
large ranges of motion. The in­terpolations are nearly interactive, with the capability of trading off 
speed for resolution, and the representation permits editing opera­tions such as changing the underlying 
surface shape and kinematics. Our work leaves ample room for future research. In the short term, we would 
like to explore more automatic techniques for pose estimation, such as fully automatic marker detection 
and identi.­cation or even non-rigid, markerless registration. As noted in the previous section, creases 
cause problems for constructing displaced subdivision surfaces. Possible solutions include .nding a better 
template surface optimized across all poses (rather than an arbi­trary average pose) or computing a template 
that itself changes from pose to pose after .tting to each one. Extending our technique to handle other 
degrees of freedom such as muscle load (e.g, when  Figure 8 By simply scaling the location of the template 
surface s control points relative to the bones, we can alter the appearance of the animated char­acter. 
(a) Original appearance; (b) forearm 6 cm shorter; (c) 20% thinner across all body parts; (d) 44% fatter. 
lifting heavy objects) would also be useful. In the longer term, generalizing beyond a single example 
sug­gests a number of future directions. The ability to edit the surface template and the skeleton hints 
at the possibility of more sophis­ticated editing, e.g., exaggerating deformations, or even mapping deformations 
onto other bodies scanned in fewer poses. In addi­tion, scanning large numbers of people would allow 
more degrees of freedom for modeling the human body by example, e.g., expos­ing controls for male-ness 
and female-ness [Blanz and Vetter 1999]. Finally, the posable model we have developed does not encompass 
dynamical behaviors or deformations due to collisions. Combining example-based techniques with anatomical 
and physically based modeling promises to be a fruitful area for future research. Acknowledgments We 
would like to thank David Addleman and Christian Juhring of Cyberware for their assistance and the use 
of their whole-body range scanner. Thanks also go to Steve Capell for use of his subdi­vision code, Keith 
Grochow and Eugene Hsu for their motion cap­ture work, and to Daniel Wood and others who provided assistance 
and feedback on this paper and the video. We also thank Michael Cohen for his helpful discussions on 
interpolation techniques. This work was supported by the University of Washington Animation Research 
Lab, industrial gifts from Intel and Microsoft Research, and NSF grant CCR-0098005. References AUBEL, 
A., AND THALMANN, D. 2001. Interactive modeling of the human muscu­lature. In Proc. of Computer Animation 
2001. BLANZ, V., AND VETTER, T. 1999. A morphable model for the synthesis of 3D faces. In Proceedings 
of ACM SIGGRAPH 99, Addison Wesley, New York, A. Rockwood, Ed., Annual Conference Series, 187 194. BUEHLER, 
C., BOSSE, M., MCMILLAN, L., GORTLER, S. J., AND COHEN, M. F. 2001. Unstructured lumigraph rendering. 
In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Annual Conference Series, 
ACM, 425 432. CATMULL, E., AND CLARK, J. 1978. Recursively generated B-spline surfaces on arbitrary topological 
meshes. Computer-Aided Design 10 (Sept.), 350 355. CHADWICK, J. E., HAUMANN, D. R., AND PARENT, R. E. 
1989. Layered con­struction for deformable animated characters. Computer Graphics (Proceedings of ACM 
SIGGRAPH 89) 23, 3, 243 252. CURLESS, B., AND LEVOY, M. 1995. Better optical triangulation through spacetime 
analysis. In Proceedings of IEEE International Conference on Computer Vision, 987 994. CURLESS, B., AND 
LEVOY, M. 1996. A volumetric method for building complex models from range images. In Proceedings of 
ACM SIGGRAPH 96, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., Annual Confer­ence 
Series, ACM, 303 312. GILL, P. E., MURRAY, W., AND WRIGHT, M. H. 1989. Practical Optimization. Academic 
Press. GOLD, S., AND RANGARAJAN,A.1996.Agraduatedassignmentalgorithmforgraph matching. IEEE Transactions 
on Pattern Analysis and Machine Intelligence 18, 4, 377 388. GOWITZKE, B. A., AND MILNER, M. 1988. Scienti.c 
Bases of Human Movement, third ed. Williams &#38; Wilkins, Baltimore, MD. GUENTER, B., GRIMM, C., WOOD, 
D., MALVAR, H., AND PIGHIN, F. 1998. Mak­ing faces. In Proceedings of ACM SIGGRAPH 98, Addison Wesley, 
M. Cohen, Ed., Annual Conference Series, ACM, 55 66. HERDA, L., FUA, P., PL¨ 2001.ANKERS, R., BOULIC, 
R., AND THALMANN, D. Using skeleton-based tracking to increase the reliability of optical motion capture. 
Human Movement Science Journal 20, 313 341. KRISHNAMURTHY, V., AND LEVOY, M. 1996. Fitting smooth surfaces 
to dense poly­gon meshes. In Proceedings of ACM SIGGRAPH 96, Addison Wesley, H. Rush­meier, Ed., Annual 
Conference Series, ACM, 313 324. LEE, A., MORETON, H., AND HOPPE, H. 2000. Displaced subdivision surfaces. 
In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., 
ACM, 85 94. LEWIS, J. P., CORDNER, M., AND FONG, N. 2000. Pose space deformations: A uni.ed approach 
to shape interpolation and skeleton-driven deformation. In Pro­ceedings of ACM SIGGRAPH 2000, ACM Press 
/ ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., Annual Conference Series, ACM, 165 172. LUTTGENS, 
K., AND WELLS, K. F. 1982. Kinesiology: scienti.c basis of human motion, seventh ed. CBS College Publishing, 
New York, NY. MAGNENAT-THALMANN, N., LAPERRIERE, R., AND THALMANN, D. 1988. Joint­dependent local deformations 
for hand animation and object grasping. In Proc. Graphics Interface, 26 33. NG-THOW-HING, V. 1999. Physically 
based anatomic modeling for construction of musculoskeletal systems. In Proceedings of the 1999 SIGGRAPH 
annual confer­ence: Conference abstracts and applications, ACM Press, New York, NY 10036, USA, Computer 
Graphics, ACM, 264 264. PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, D. H. 1998. 
Synthesizing realistic facial expressions from photographs. In Proceedings of ACM SIGGRAPH 98, Addison 
Wesley, M. Cohen, Ed., Annual Conference Series, ACM, 75 84. PRAUN, E., SWELDENS, W., AND SCHR ODER¨ 
, P. 2001. Consistent mesh param­eterizations. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIG-GRAPH, 
E. Fiume, Ed., Annual Conference Series, ACM, 179 184. ROSE, C., COHEN, M. F., AND BODENHEIMER, B. 1998. 
Verbs and adverbs: Multi­dimensional motion interpolation. IEEE Computer Graphics and Applications 18, 
5 (Sept./Oct.), 32 41. SCHEEPERS, F., PARENT, R. E., CARLSON, W. E., AND MAY, S. F. 1997. Anatomy­based 
modeling of the human musculature. In Proceedings of ACM SIGGRAPH 97, T. Whitted, Ed., Annual Conference 
Series, ACM, 163 172. SHEN JIANHUA, THALMANN, N. M., AND THALMANN, D. 1994. Human skin deformation from 
cross-sections. In Computer Graphics Int. 94. SILAGHI, M.-C., PL¨ ANKERS, R., BOULIC, R., FUA, P., AND 
THALMANN, D. 1998. Local and global skeleton .tting techniques for optical motion capture. In Pro­ceedings 
of the International Workshop on Modelling and Motion Capture Tech­niques for Virtual Environments (CAPTECH-98), 
Springer, Berlin, N. Magnenat-Thalmann and D. Thalmann, Eds., vol. 1537 of LNAI, 26 40. SLOAN, P.-P., 
ROSE, C., AND COHEN, M. F. 2001. Shape by example. In Proceed­ings of 2001 Symposium on Interactive 3D 
Graphics. TALBOT, J. 1998. Accurate Characterization of Skin Deformations Using Range Data. Master s 
thesis, Department of Computer Science, University of Toronto. TURK, G., AND LEVOY, M. 1994. Zippered 
polygon meshes from range images. In Proceedings of ACM SIGGRAPH 94, ACM Press, vol. 28 of Annual Conference 
Series, ACM, 311 318. WILHELMS, J., AND GELDER, A. V. 1997. Anatomically based modeling. In Proceedings 
of ACM SIGGRAPH 97, T. Whitted, Ed., Annual Conference Series, ACM, 173 180. ZHU, C., BYRD, R. H., LU, 
P., AND NOCEDAL, J. 1997. Algorithm 778. L-BFGS-B: Fortran subroutines for Large-Scale bound constrained 
optimization. ACM Trans­actions on Mathematical Software 23, 4 (Dec.), 550 560.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566627</article_id>
		<sort_key>620</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Interactive multiresolution hair modeling and editing]]></title>
		<page_from>620</page_from>
		<page_to>629</page_to>
		<doi_number>10.1145/566570.566627</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566627</url>
		<abstract>
			<par><![CDATA[Human hair modeling is a difficult task. This paper presents a constructive hair modeling system with which users can sculpt a wide variety of hairstyles. Our Multiresolution Hair Modeling (MHM) system is based on the observed tendency of adjacent hair strands to form clusters at multiple scales due to static attraction. In our system, initial hair designs are quickly created with a small set of hair clusters. Refinements at finer levels are achieved by subdividing these initial hair clusters. Users can edit an evolving model at any level of detail, down to a single hair strand. High level editing tools support curling, scaling, and copy/paste, enabling users to rapidly create widely varying hairstyles. Editing ease and model realism are enhanced by efficient hair rendering, shading, antialiasing, and shadowing algorithms.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[generalized cylinders]]></kw>
			<kw><![CDATA[hair modeling]]></kw>
			<kw><![CDATA[hair rendering]]></kw>
			<kw><![CDATA[level of detail]]></kw>
			<kw><![CDATA[multiresolution modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Modeling packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011070</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Application specific development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14224401</person_id>
				<author_profile_id><![CDATA[81365596229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39053030</person_id>
				<author_profile_id><![CDATA[81100662479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ulrich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>337683</ref_obj_id>
				<ref_obj_pid>337680</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGUADO, A. S., MONTIEL, E., ZALUSKA, E. 1999. Modeling Generalized Cylinders via Fourier Morphing. ACM Transactions on Graphics. 18(4), 293-315.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134021</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ANJYO, K., USAMI, Y., AND KURIHARA, T. 1992. A Simple Method for Extracting the Natural Beauty of Hair. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), 26(4), ACM, 111-120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90927</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J. 1990. Calculation of Reference Frames Along A Space Curve. In A. Glassner, editor, Graphics Gems, Academic Press, 567-571.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHEN, L., SAEYOR, S., DOHI, H., AND ISHIZUKA, M. 1999. A System of 3D Hairstyle Synthesis Based on the Wisp Model. The Visual Computer, 15(4), 159-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807458</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CSURI, C., HAKATHORN, R., PARENT, R., CARLSON, W., AND HOWARD, M. 1979. Towards an Interactive High Visual Complexity Animation System. In Computer Graphics (Proceedings of ACM SIGGRAPH 79), 13(4), ACM, 288-299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DALDEGAN, A., THALMANN, N. M., KURIHARA, T., AND THALMANN, D. 1993. An Integrated System for Modeling, Animating and Rendering Hair. In Computer Graphics Forum (Proceedings of Eurographics 93), 211-221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FALK, R. AND SAND, L. R. (ORGANIZERS) 2001. "Shrek": The Story Behind The Screen. ACM SIGGRAPH Course Note 19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258807</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GOLDMAN, D. 1997. Fake Fur Rendering. In Proceedings of ACM SIGGRAPH 97, ACM Press / ACM SIGGRAPH, New York, 127 - 134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GRABLI, S., SILLION, F. X., MARSCHNER, S. R., AND LENGYEL, J. E. 2002. Image-Based Hair Capture by Inverse Lighting. In Proceedings of Graphics Interface 2002, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HADAP, S. AND THALMANN, N. M. 2000. Interactive Hair Styler Based on Fluid Flow. Eurographics Workshop on Computer Animation and Simulation, 87-100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HADAP, S. AND THALMANN, N. M. 2001. Modeling Dynamic Hair as a Continuum. In Computer Graphics Forum (Proceedings of Eurographics 2001), 329 - 338.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383327</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HAUSNER, A. 2001. Simulating Decorative Mosaics. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, New York, 573-580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. AND KAY, T. 1989. Rendering Fur with Three Dimensional Textures. In Computer Graphics (Proceedings of ACM SIGGRAPH 89), 23(4), ACM, 271-280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KIM, M., PARK, E., AND LEE, H. 1994. Modeling and Animation of Generalized Cylinders with Variable Radius Offset Space Curves. The Journal of Visualization and Computer Animation, 5(4), 189-207.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872905</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KIM, T. AND NEUMANN, U. 2000. A Thin Shell Volume for Modeling Human Hair. In Computer Animation 2000, Philadelphia, IEEE Computer Society, 121-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KIM, T. AND NEUMANN, U. 2001. Opacity Shadow Maps. Rendering Techniques 2001, Springer, 177-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LAFRANCE, M. 2001. First Impressions and Hair Impressions. Unpublished manuscript. Yale University, Department of Psychology, New Haven, Connecticut. http://www.physique.com/sn/sn_yale-study2.asp]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508127</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LEE, D.-W. AND KO, H.-S. 2001. Natural Hairstyle Modeling and Animation. Graphical Models, 63(2), 67-85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LENGYEL, J. E. 2000. Realtime Fur. Rendering Techniques 2000, Springer, 243-256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364407</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LENGYEL, J. E., PRAUN, E., FINKELSTEIN, A., AND HOPPE, H. 2001. Real-Time Fur over Arbitrary Surfaces. ACM Symposium on Interactive 3D Techniques 2001, 227-232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M. AND WHITTED, T. 1985. The Use of Points as a Display Primitive. Technical Report 85-022, Computer Science Department, University of North Carolina at Chapel Hill, January.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LOKOVIC, T. AND VEACH, E. 2000. Deep Shadow Maps, In Proceedings of SIGGRAPH 2000, ACM, New York, 385-392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MAUCH, S. 2000. A Fast Algorithm for Computing the Closest Point and Distance Transform. Submitted for publication in the Jounral of SIAM SISC. http://www.acm.caltech.edu/~seanm/software/cpt/cpt.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MCREYNOLDS, T. (Organizer) 1997. Programming with OpenGL: Advanced Techniques, ACM SIGGRAPH Course Note 11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614392</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[NEYRET, F. 1997. Modeling, Animating, and Rendering Complex Scenes Using Volumetric Textures. IEEE Transaction on Visualization and Computer Graphics, 4(1), 55-70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, D., FISHER, S., AND LIN M. 2001. Automatic Simplification of Particle System. In Computer Animation 2001, Seoul, Korea, IEEE Computer Society, 210-219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>249651</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. AND WATERS, K. 1996. Computer Facial Animation. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776363</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PLANTE, E., CANI, M.-P., AND POULIN, P. 2001. A Layered Wisp Model for Simulating Interactions Inside Long Hair. Eurographics Workshop on Computer Animation and Simulation 2001, 139-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[ROSENBLUM, R., CARLSON, W., AND TRIPP E. 1991. Simulating the Structure and Dynamics of Human Hair: Modeling, Rendering and Animation. The Journal of Visualization and Computer Animation, 2(4), 141-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280946</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SINGH, K. AND FIUME, E. 1998. Wires: A Geometric Deformation Technique. In Proceedings of ACM SIGGRAPH 98, ACM, New York, 405 - 415.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286071</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[STOLLNITZ, E. AND DEROSE, T. D. AND SALESIN, D. H. 1996. Wavelets for Computer Graphics. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[THALMANN, N. M. AND HADAP, S. 2000. State of the Art in Hair Simulation. International Workshop on Human Modeling and Animation, Korea Computer Graphics Society, 3-9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[TURK, G. 1991. Generating Textures on Arbitrary Surface Using Reaction Diffusion. In Computer Graphics (Proceedings of ACM SIGGRAPH 91), 21(4), 289-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617720</ref_obj_id>
				<ref_obj_pid>616021</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WATANABE, Y. AND SUENAGA, Y. 1992. A Trigonal Prism-Based Method for Hair Image Generation. IEEE Computer Graphics and Application, 12(1), 47-53.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618822</ref_obj_id>
				<ref_obj_pid>616070</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[XU, Z. AND YANG, X. D. 2001. V-HairStudio: An Interactive Tool for Hair Design. IEEE Computer Graphics and Applications, 21(3), 36-43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345147</ref_obj_id>
				<ref_obj_pid>345140</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[YANG, X. D., XU, Z., YANG J., AND WANG, T. 2000. The Cluster Hair Model. Graphical Models, 62(2), 85-103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Multiresolution Hair Modeling and Editing Tae-Yong Kim and Ulrich Neumann {taeyong | uneumann}@graphics.usc.edu 
Computer Graphics and Immersive Technologies Laboratory Integrated Media Systems Center University of 
Southern California  Figure 1. An example multiresolution hair design procedure. Each hair model results 
from interactive multiresolution editing operations. 1. The user roughly designs a hairstyle with about 
30 high-level clusters. 2. One hair cluster (inside the ellipse) is subdivided and made curly. 3. The 
curly cluster is copied onto other clusters. 4. The bang hair cluster (inside the ellipse) is subdivided 
and refined. 5. Final hair model after further refinements. Abstract Human hair modeling is a difficult 
task. This paper presents a constructive hair modeling system with which users can sculpt a wide variety 
of hairstyles. Our Multiresolution Hair Modeling (MHM) system is based on the observed tendency of adjacent 
hair strands to form clusters at multiple scales due to static attraction. In our system, initial hair 
designs are quickly created with a small set of hair clusters. Refinements at finer levels are achieved 
by subdividing these initial hair clusters. Users can edit an evolving model at any level of detail, 
down to a single hair strand. High level editing tools support curling, scaling, and copy/paste, enabling 
users to rapidly create widely varying hairstyles. Editing ease and model realism are enhanced by efficient 
hair rendering, shading, antialiasing, and shadowing algorithms. CR Categories and Subject Descriptions: 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; I.3.5 [Computer Graphics]: Computational 
Geometry and Object Modeling Modeling Package Additional Keywords: hair modeling, multiresolution modeling, 
level of detail, hair rendering, generalized cylinders 1 INTRODUCTION Hairstyle is a determining factor 
of a person s first impression when meeting someone [LaFrance 2001]. Thus, hair is an important aspect 
of personal identity, but hair modeling remains a major obstacle in realistic human face synthesis. The 
volumetric nature of hair is simply not captured by surface models, so it is often simplified or hidden 
by objects like hats. Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 Surface modeling methods can use high-density data acquired from range scanners. 
Such model acquisitions are not yet practical for hair since we lack suitable volumetric scanner technologies. 
The recent work by Grabli et al. [2002] attempts to automatically reconstruct hair models from photographs. 
Although promising, the approach is still in its inception stage and does not recover complete hair models. 
Our goal is an interactive hair modeling system that allows a user to easily and quickly design a wide 
range of hairstyles (Figure 1 illustrates an example hairstyling process). Cluster (or wisp) hair models 
[Chen et al. 1999; Falk and Sand 2001; Kim and Neumann 2000; Plante et al. 2001; Watanabe and Suenaga 
1992; Xu and Yang 2001; Yang et al. 2000] exploit the observed tendency of adjacent hair strands to form 
clusters due to static attraction or artificial styling. These models employ two-step manipulation. 1) 
The rough geometry of a hair cluster is modeled. 2) Details are added by rendering each hair strand or 
volume density. These models, however, lack stylistic variations in each hair strand due to a limited 
set of parameters, especially, for complex variations such as curly hair, as noted by Xu and Yang [2001]. 
Strand hair models [Anjyo et al. 1992; Daldegan et al. 1993; Hadap and Thalmann 2001; Lee and Ko 2001; 
Rosenblum et al. 1991] allow every hair strand to be explicitly designed. However, manual modeling of 
individual hair strands is extremely tedious. Designing just the key hair strands can consume five to 
ten hours [Thalmann and Hadap 2000]. Thus, strand hair models are often coupled with dynamics simulations. 
External parameters such as gravity, wind forces, and stiffness affect the global shape of the final 
hair model. However, the dynamics methods are often limited to relatively simple hairstyles and complex 
hairstyles are not easily modeled, even after repeated trial and error iterations of minutes or hours 
of simulation. Problems arise from the complexity of real human hairstyles, often created by hairstylists 
with extensive efforts such as curling and combing (Figure 2). Structured and discontinuous clusters 
(e.g., braids, combing effects) are difficult to model with strand hair models, whereas stylistic strand 
variations are difficult to achieve with cluster/wisp hair models. Our multiresolution hair Figure 2. 
Photographs of human hair. model aims at bridging this gap by allowing detailed local control as well 
as global control. To model complex objects, researchers successfully employ multiresolution concepts 
for continuous curves, surface editing, and volume sculpting (see [Stollnitz et al. 1996] for examples). 
Their major benefit is the user s freedom to choose the appropriate level of detail for a desired model 
manipulation. In this spirit, we develop a Multiresolution Hair Modeling (MHM) system. However, hair 
models are inherently volumetric and contain a large number of disjoint hair strands. Thus, our treatment 
differs from other multiresolution techniques in applying the concept of multiresolution to hair. In 
our context, multiresolution manipulations are achieved with a hierarchy of generalized cylinders. MHM 
allows users to interactively move, scale, curl, and copy a portion of an evolving hair model at any 
level of detail. Rendering is considered a separate offline process in existing hair modeling systems. 
However, such non-interactive processes can significantly slow down the modeling process, especially 
for complex hairstyles for which rendering effects such as self-shadowing are crucial. We aim at providing 
an interactive rendering capability, enabling users to get immediate visual feedback during modeling. 
To our knowledge, no reported hair modeling systems interactively render explicit hair models, complete 
with shading, antialiasing, and self-shadowing. The contributions of our work lie in a novel multiresolution 
hair representation, interactive tools for editing hair, and efficient hair rendering methods, all aimed 
at decreasing a user s time and tedium for modeling complex hairstyles. The remainder of this paper is 
organized as follows. Section 2 reviews related work and section 3 provides a brief system overview. 
The multiresolution hair representation is detailed in section 4, and section 5 presents editing tools 
suited to the representation. Interactive rendering algorithms are presented in section 6, and results 
are presented and discussed in section 7. Sections 8 and 9 discuss implementation issues and future directions. 
2 Related Work Since the pioneering work by Csuri et al. [1979], researchers have developed a number 
of hair modeling systems. These systems are categorized based on hairstyles to which they are best suited. 
Refer to [Parke and Waters 1996; Thalmann and Hadap 2000] for more comprehensive survey on human hair 
modeling and rendering research. Smooth Hairstyle: Strand hair models are often limited to smooth hairstyles, 
due to the lack of mechanisms to simulate discontinuous clustering effects in their simplified physics 
simulation or interpolation methods. Anjyo et al. use cantilever beam dynamics and one-dimensional angular 
dynamics for hair modeling and animation [1992], later extended by Lee and Ko [2001] for interactive 
hairstyling. Rosenblum et al. produce hair motion by mass-spring simulation of individual hair strands 
[1991]. Exploiting the similarity between fluid and smooth hairstyles, Hadap and Thalmann develop an 
interactive hairstyling system [2000] and a dynamics system simulating hair/hair interaction [2001]. 
 Watanabe and Suenaga [1992] propose a wisp model, later extended in an interactive hairstyling system 
by Chen et al. [1999]. In an integrated hair system by Daldegan et al. [1993], characteristic hair strands 
define the boundary of wisps. The common assumption that hair strands are parallel inside a wisp makes 
it hard to model rigorous strand variations such as curliness with random parameters to perturb wisps. 
Also, discontinuous clusters (e.g., due to combing) are not efficiently handled due to the underlying 
interpolation between key hair strands. Clusters and Discontinuity: The cluster hair model by Yang et 
al. [2000] defines the boundary of a wisp with generalized cylinders, later used in an interactive hairstyling 
system in [Xu and Yang 2001]. Strand variations are implicitly modeled with ray tracing volume density. 
A similar method is used in the production of the movie Shrek in which wisps are defined by control polygons 
[Falk and Sand 2001]. These methods handle bounded, man-made hairstyles such as braids, but stylistic 
strand variations are difficult to model. A method to model discontinuities in smooth hairstyles is reported 
in [Kim and Neumann 2000]. The dynamics system by Plante et al. [2001] simulates the interactions between 
discontinuous wisps in motion. However, the clustering is pre-determined before simulation and the distribution 
of strands inside a wisp is fixed. Animal Fur or Short Hair: Explicit geometry is often avoided for short 
animal fur. Kajiya and Kay [1989] use volumetric texture (or texels) to mimic a group of hair strands, 
later extended by Neyret [1997] for multiresolution. An appearance-based model of distant animal fur 
is developed [Goldman 1997]. For real-time rendering of animal fur, concentric textures are used to sample 
the volumetric texture functions, augmented with large-scale geometry [Lengyel 2000; Lengyel et al. 2001]. 
A common assumption with animal fur is that the distribution of hair has repeated patterns. However, 
the assumption does not hold with longer human hair, as adjacent hair strands at the scalp split, curl, 
and move away from each other. Such styling effects require a significantly different modeling method 
from existing animal fur techniques [Lengyel et al. 2001]. 3 Overview Figure 3 illustrates the structure 
of a multiresolution hair model. A parametric patch on the scalp surface (section 4.2), defines the region 
where hair can originate. A hair model is constructed hierarchically, starting from a small set of generalized 
cylinders (GCs). A GC defines the boundary of each hair cluster, controlling a group of clusters or hair 
strands (see section 4.1 for our definition of a GC). The user interactively subdivides each hair cluster, 
adding more detail until the desired appearance is achieved. Subdivision steps (section 4.4) add new 
nodes to the hierarchy we call a hair tree. Editing operations such as curling, moving, copying, and 
selecting are applied to nodes of the hair tree (section 5). While the user edits a hairstyle, the system 
interactively visualizes the edited model using various rendering options (section 6).  Hair model A 
head mesh with a scalp surface A hair tree = + + +  Generalized Skeleton Contour Scale Twist (curliness) 
Cylinder curve functions SN(t), SB(t) C(t) W(t) V(r, ., t) R(.,t) = Hair strand Polyline Figure 3. 
Graphical view of a multiresolution hair model. 4 Hair Cluster Representation A GC defines the boundary 
of a hair cluster. In the following sections, we do not differentiate GCs from hair clusters and both 
terms are used interchangeably. 4.1 Generalized Cylinder A GC is defined with a skeleton curve C(t) 
and a set of contours placed along the curve1. GC (., t )=C() t +R(., t ) (1) where C(t) is a space curve 
parameterized by t and R(.,t) is a contour function centered at C(t). . is an angle around the principal 
axis of a reference frame. With an additional parameter r, any point around the GC can be defined as 
V (r, ., t )=C() t +r R (., t ) (2) Constructing a GC requires each contour to be properly aligned with 
reference frames. We use the frame generation method by Bloomenthal [1990]. The frame is represented 
with the tangent vector of the skeleton curve T , the principal normal N and the binormal vector B (Figure 
4). Both N and B lie on the plane of the contour perpendicular to the skeleton curve. The pre-computed 
frames are interpolated to form a smoothly shaped GC. Let T(t) , N(t) , and B(t) denote the interpolated 
frame vectors at t (0 = t = 1). The world coordinate of a point parameterized by (r,.,t) is  1 Similar 
definitions can be found in [Aguado et al. 1999; Xu and Yang 2000]. Alternatively, a GC can be viewed 
as rotational sweep of a profile curve [Kim et al. 1994]. B . B P T r P . N C(t) N Figure 4. Definition 
of a reference frame along the space curve C(t). The parametric coordinate for a point P is given as 
(r,.,t), where . is defined as the angle around the tangent vector T , starting from the principal normal 
N . Representation of Contour: R(.) is a contour shape function dependent on .. ()is defined as an 
offset distance function R . from a circle with a global radius s ( R(.) =s + ()). N pairs of R. angles 
and offset radius values control the shape of a contour along with the global radius s. The current implementation 
uses .= 2ip / N and () is computed by linear interpolation of the angle / offset pairs. By definition, 
the contour is star-shaped, which we find flexible enough to represent the boundary of hair clusters. 
The offset function can efficiently represent star-shaped contours with a small set of parameters (currently, 
N = 32 is used). The continous contour function R(., t) is linearly interpolated from a discrete number 
of contours.2 i R . Auxiliary Functions: Adding auxiliary scale and twist terms completes our description 
of GC. The scale terms SN(t) and SB(t) let the user easily control the stretching and shrinking of GCs 
along the two axis vectors N and B , while direct modification of the contour functions allows fine detailed 
control over the shape. The twist term W(t) controls the curliness of a hair cluster. Adding these terms 
yields  where . + (). =. W t  4.2 Scalp Surface A scalp surface is a parametric patch defining the 
region of a head where the user can place hair. We use a tensor-product Catmull-Rom spline patch that 
the user wraps over the head model. The current system allows a user to interactively specify each control 
point of the spline patch. The scalp surface is constructed once for each head model and it is useful 
in many ways. 1) The user can easily place and move the GC contour on the scalp mesh (Figure 5). 2) Sampling 
and assigning hair strands become simpler (section 4.3). 3) The subdivision of hair clusters is reduced 
to a 2D problem (section 4.4). 4) The scalp parameterization provides the user with convenient means 
to select clusters in 2D space (section 5.3). Scalp space (middle figure in Figure 5) is spanned by u 
and v, the parametric coordinates of the surface. World space is specified by the 3D world coordinate 
frame. We denote the contour function in scalp space as Rs(.) and contours in world space as Rw(.). Scalp 
space contours are used for hair strand generation and for the initialization of multiple world space 
contours that define the shape of a GC. Appendix A describes how to generate an initial GC using the 
scalp space contour. 2There exist more rigorous methods to define and interpolate the contours of GCs 
[Aguado et al. 1999]. Our definition of contours aims at simplicity and computational efficiency. Figure 
5. Placing a contour on the scalp. The user selects a contour (right) and interactively places it in 
2D scalp space (yellow circle in the middle), which defines its 3D position (red circle on the left). 
The grid is color coded to help the user match corresponding positions.  4.3 Generation of Hair Strands 
Hair strands are represented as polylines (a set of connected line segments). Creating hair strands independently 
within each GC (Figure 6a) can cause visually distracting hair density variation since we allow arbitrarily 
shaped scalp space contours that can overlap each other (Figure 7). Rather than trying to prevent the 
overlaps, we sample the root positions of hair strands first, and assign the strands to their owner GCs 
(Figure 6b).  (b) (a) Figure 6. (a) Independent hair generation for each cluster causes uneven hair 
density in overlapping regions. (b) By assigning pre-sampled hair strands to clusters, the overlap problem 
is solved. Initially, (or whenever the total number of hair strands changes), the root position (uh,vh) 
of each hair strand is uniformly sampled on the scalp grid. For each hair strand, we determine if there 
exists a GC that owns the strand (the details of the ownership decision are deferred until section 4.4). 
If there is such a GC, the parametric Figure 7. Scalp space coordinate (rh, .h) of the strand is contours 
can overlap each computed (see appendix B for details). other. Colored dots Given (rh, .h), the hair 
strand is illustrate the root positions created by connecting points of strands. Each color computed 
with equation 4, indicates the owner cluster incrementing t from 0 to 1. of hair strands. Note that equation 
4 assumes that each GC contour lies in the plane perpendicular to the skeleton curve. Since the scalp 
shape is curved, planar contours can misalign the root positions of strands. To handle the problem, the 
root position (t = 0) of each hair strand is directly evaluated on the scalp, using V (r ,.,0 )= S(u 
, vh ) (5) where S(u,v) is the parametric scalp patch. However, this modification can still cause unnatural 
bending along hair strands if other points of polyline segments are directly evaluated from equation 
4 (Figure 8a). Similar problems can arise from self-intersections in GCs of high curvature. We interpolate 
a Catmull-Rom spline curve for each hair strand to guarantee smoothness (Figure 8b). The number of spline 
control points is hh h  (a) (b) Figure 8. (a) Straight line connections can cause unnatural bending 
around the root position and in highly curved region. (b) Using spline interpolation at strand-level, 
smoothness is guaranteed. the same as that of the skeleton curve of the strand s owner GC. The polyline 
segments of each hair strand are evaluated from the spline interpolation. Varying the length of each 
hair strand increases the model s natural appearance. A random sample . is taken from a uniform distribution 
from 1-k to 1 (0 = 1-k =.=1), where k denotes the degree of tip length variation. Adding the length variation, 
we evaluate the spline controls points of each hair strand using .t instead of t for equation 4. 4.4 
Subdivision When a parent cluster is subdivided, contours and skeleton curves of its child clusters are 
created. The contours of child clusters are inherited from those of the parent cluster unless the user 
specifies alternative shapes. Inherited contours are scaled by . / N , where N is the user-specified 
number of child clusters and . controls the degree of overlap between child clusters. For world space 
contours, . is set to 1.0. For scalp space contours . is set to a value larger than 1.0 (typically 1.3<.<1.5), 
to ensure that the subdivided region is fully covered by the contours of child clusters. To place the 
child clusters on the scalp, we first sample random positions inside the contour of their parent. This 
often produces uneven distributions of contour positions (Figure 9a). We use the position relaxation 
algorithm by Turk [1991] to improve the distribution (Figure 9b). Given these positions, skeleton curves 
are created using the method described in section 4.3. (a) (b) (c) Figure 9. Subdivision of contours 
and tiling of the scalp space. (a) Random positioning (.=1.4). (b) Position relaxation. (c) An example 
tiling of the scalp surface after a few subdivision steps. Deciding the Owners of Hair Strands After 
child clusters are positioned on the scalp, hair strands are re-assigned to the new clusters. As shown 
in Figure 9c, contours of the child clusters can overlap with each other or contours of existing clusters. 
In overlapping regions, we assign hair strands to a cluster with the closest center position in scalp 
space, similar to computing the centroidal voronoi diagram [Hausner 2001] of the center positions of 
all the clusters on the scalp (Figure 10a). However, simply computing the voronoi diagram may be problematic 
when precise controls for hair grouping are desired (e.g., for parting). Our hair ownership decision 
algorithm shown below as a pseudo code allows the user to design and tune the tiling and subdivision 
patterns if necessary (Figure 10b). For each hair strand, the algorithm traverses the hair tree starting 
from root (b) User controlled tiling and subdivision. Note the clear borders between contours. clusters. 
The closest cluster containing the strand is chosen at each depth. The iteration is repeated until it 
reaches a leaf cluster. If there is only one such leaf cluster, the strand is assigned to the cluster 
regardless of the distance to other clusters.3 When two or more leaf clusters contain the same strand 
due to overlaps, the cluster with a center position closest to the strand is selected. When a hair strand 
is contained in a non-leaf cluster, but not contained in any of its descendents4, the strand is assigned 
to the closest descendent leaf cluster. Note that we allow only leaf clusters to own hair strands, and 
for each hair strand, there is at most one owner. Thus, we can maintain consistent hair density and limit 
the total number of hair strands; both features prove useful while level-of-detail manipulations control 
the appearance of the overall hair model (section 6). function FINDOWNEROFAHAIRSTRAND (HairStrand H) 
O . NULL , C . first root cluster, done . FALSE while (!done and C != NULL) do done . TRUE forall {S 
| S is a sibling of C or C itself } do CHECKFORINCLUSION(H,S) if S contains H and its center is closer 
to H than O O . S done . FALSE if (!done) C . O.FIRSTCHILD if (O is not a leaf cluster) forall { L | 
L is a descendent of O and a leaf node } do if L s center is closer to H than O O . L return O   5 
INTERACTIVE MANIPULATION The core of MHM lies in the user s ability to edit any node in the hair tree. 
The user controls the position, contours, scale, and twist of any GC, affecting the hair model at multiple 
levels of detail (section 5.1). A sub-tree in the hair tree can be used as a user-defined style template. 
The copy/paste tool (section 5.2) transfers a user-designed style from one cluster to other clusters. 
Section 5.3 describes a set of selection methods that exploit the scalp surface and the hair tree. During 
interactive manipulation, hair/head penetration is detected and avoided (section 5.4). 3 In contrast, 
a voronoi diagram based approach may assign the strand to another cluster even if the hair strand does 
not originate from the region bounded by the cluster.4 This case occurs when the contours of child clusters 
do not perfectly cover the contour of the parent cluster even after the position relaxation step. Note 
that we set . to a value larger than 1.0 to minimize this artifact. 5.1 Editing Properties of Hair Clusters 
The user interactively manipulates GC parameters such as scale, twist, and skeleton shape (equation 4). 
When editing leaf clusters, the system simply reassigns the parameters and update hair strands. When 
editing a non-leaf cluster, all of its descendent clusters must follow the shape changes to preserve 
their relative position and orientation (Figure 11). (a) (b) (c) (d) Figure 11. a) The user picks the 
control point (inside the blue circle) of a high-level cluster. The user can b) move c) scale d) twist 
the region around the control point. When a non-leaf cluster is edited, all the descendant clusters 
are bound to the cluster. Let V be the cluster before editing. Assume that the user changes V to V . 
Then each descendant cluster is updated as follows. 1) For each control point P of the skeleton curve, 
we find the parametric coordinate (r,.,t) of P with regard to the cluster V such that P = V(r,.,t) (bind). 
2) The new position P is recalculated using P = V (r,.,t) (update). The bind procedure is the inverse 
transform of equation 4, i.e. (r,.,t) = V-1(P). Details are given in appendix C. The bind process is 
performed whenever the user initiates an action (for example, selects a hair cluster and starts editing 
it), whereas the update process occurs at every step of the user interaction. Hair strands are considered 
statically bound to a cluster when their owner is decided. The bind/update method is similar to Wires 
[Singh and Fiume 1998], but our method compensates for the axial scaling and twist terms, and no falloff 
term is used. Temporary/Local Binding To manipulate a group of clusters with no common ancestor (e.g., 
attaching a root cluster to another root cluster), the user selects a group of clusters and binds the 
clusters to an arbitrary binder cluster. Then, these selected clusters are temporarily bound to the binder 
cluster instead of their parent clusters until the user nullifies the setting. This option is especially 
useful to control root clusters (e.g., to make a pony tail style), or to locally control a set of clusters 
that stem from two disjoint root positions (Figure 12). (a) (b) Figure 12. In this figure, only skeleton 
curves are shown. (a) Temporary binding. The cluster A is temporarily attached to cluster B. (b) Two 
root clusters are locally bound to another (binder) cluster and selected control points (red) are twisted 
around the binder cluster.   5.2 Copy and Paste   A B Figure 13. Copying a braid from one cluster 
to another. a) The user designs a braid style. b) The style is copied from cluster A to cluster B. c) 
Result. We can treat a hair cluster and its descendent clusters as a user-defined style template. The 
copy process illustrated in Figure 13 transfers a style from one cluster to another, by copying the sub-tree 
parameters. Copying is similar to the editing process in section 5.1. When the style of cluster A is 
copied to cluster B, the descendent clusters of cluster A are first bound to A, but these clusters are 
updated with cluster B as their parent. These updated clusters replace existing descendents of cluster 
B. Contours are scaled by the ratio between the size of contours of cluster A and B. Other style parameters 
such as scale and twist are simply duplicated. Note that any node in the hair tree can be copied to 
another resulting from multiple node regardless of depth; making copy/paste operations at every edit 
the user makes a potential multiple scales. style template (Figure 14). 5.3 Selection Methods When a 
hair model is subdivided into many small clusters, it becomes difficult to select some portions of the 
model due to its volumetric nature (Figure 15). The following selection methods are available. 1) The 
user picks a control point of a cluster or specifies a region with a sphere. 2) The user selects clusters 
in scalp space with standard 2D selection methods (for example, dragging a rectangle). 3) The user picks 
a cluster and traverses the hair tree. Operations such as pick first child node or pick next sibling 
are mapped to the keyboard, which proves useful in manipulating the multiresolution model. 4) Selection 
based on depth is provided (e.g., display only the clusters of depth < 2 ).  5.4 Avoiding Hair-Head 
Penetration It is visually distracting if hair strands penetrate the head. During editing, hair strands 
and skeleton curves are tested for intersection. We use the closest-point-transform (CPT) algorithm [Mausch 
2000]. The CPT provides a fast look-up table solution with the preprocessing of the distances and gradients 
to the closest points on the head mesh. Let D(p) be the distance from a point p to the closest point 
on the mesh and .(p) be the gradient of the distance. Penetration is avoided by altering each point p 
inside the head mesh (D(p) < 0) with equation 6.  (6)  6 INTERACTIVE RENDERING Hair models in our 
framework are explicitly rendered; every hair strand is drawn as polylines in OpenGL. This section describes 
methods tailored to render such explicit models for interactive modeling purpose. Shading Model: The 
lighting calculation provided in OpenGL is disabled and shading is calculated in software, using the 
anisotropic shading model by Kajiya and Kay [1989]. The shaded color is computed at each point of the 
line segments and colors are interpolated with OpenGL. Other shading models such as that in [Goldman 
1997] could be equally applicable. Self-shadowing: Self-shadowing is an essential cue to depict volumetric 
hair (Figure 17a and Figure 17b). We use our opacity shadow maps algorithm [Kim and Neumann 2001], a 
fast approximation of deep shadow maps [Lokovic and Veach 2000]. Since shadows are view-independent, 
they can be computed once and cached for reuse while the user interactively changes views. Antialiasing: 
Since hair strands are very thin, it is important to draw them smoothly with correct filtering. The antialiased 
line drawing option in OpenGL alone is not sufficient since the correct result depends on the drawing 
order [McReynolds 1997]. Our visibility ordering algorithm, inspired by [Levoy and Whitted 1985], determines 
the drawing orders for polyline segments of hair strands based on the distance from the camera (Figure 
16). First, the bounding box of all the segments is sliced with planes perpendicular to the camera. Each 
bin, a volume bounded by a pair of adjacent planes, drawn as a color bar in Figure 16, stores indices 
of segments whose farthest end point is contained by the bin. After other objects (e.g., a head mesh) 
are drawn, the depth buffer update is disabled. Then, the segments are drawn as antialiased lines such 
that the ones indexed by the farthest bin are drawn first. Although simple, the method is fast and converges 
to exact ordering as hair strands are drawn with more segments. Since we keep relatively dense line segments 
for each hair strand, the algorithm produces visually satisfactory results.  (a) (b) (c) Figure 17. 
Effects of shadows and level of detail. Images were captured during interactive user sessions. (a) A: 
front lighting, B: back lighting. (b) A: without shadows, B: with shadows. (c) A: 1,200,000 segments 
(20000 strands, 60 segments per strand, and a = 0.3), B: 100,000 segments (5000 strands, 20 segments 
per strand, and a = 1.0). The model A is 12 times more complex than model B, hence the rendering time 
is 12 times slower for A than B. In the interactive modeling framework, the viewpoint does not change 
much from frame to frame. This coherence enables us to perform the visibility ordering periodically. 
In contrast, depth buffer based super-sampling methods (e.g., accumulation buffer [McReynolds 1997]) 
must compute visibility at every frame. In addition, the alpha values of segments can control the perceived 
thickness of hair strands. As strands become thinner, super-sampling methods would require more samples 
while alpha value changes suffice in the visibility-ordered hair model. Level of Detail and Interactive 
Rendering: With explicit hair models, the results of rendering (e.g., shadows, colors, etc.) can be cached, 
allowing users to interactively view the model during editing. Users can control three parameters - alpha 
values, the number of hair strands, and the number of segments per strand -, to adjust the speed of rendering 
(Figure 17c). These parameters give users choices in the tradeoff between speed of rendering and image 
quality. Increasing the number of hair strands and the number of segments per strand contributes to improved 
rendering quality, whereas decreasing these parameters allows the user to interactively examine the hair 
model at higher frame rates.  7 RESULTS Our system assists users in creating a variety of complex models 
(Figure 18 ~ 22). Depending on the complexity, it takes from 10~20 minutes to a few hours to model each 
hairstyle. The most time-consuming step in modeling is the positioning of initial root clusters. Typically 
users design 10 to 30 root clusters, which consumes about 70 ~ 80 percent of the total modeling time. 
Hairstylists often use curling and combing as their means to promote clustering effects, adding visual 
richness due to shadows. Likewise, we observe that adding more hair strands does not necessarily enhance 
the visual complexity of a hair model. As the hair volume is filled with more strands, room for shadows 
diminishes. Thus, we find that, to model a natural hairstyle, the structural aspects are as important 
as individual strand details. Note that the spaces between clusters after subdivision amplify shadows 
between clusters, enhancing the perceived complexity of the model. Shadowing is less salient for smooth 
hairstyles. However, subdividing smooth hair models can also add visually interesting combing effects 
(Figure 19).  8 IMPLEMENTATION AND DISCUSSION Our implementation runs on a system with an Intel PIII 
700 Mhz CPU, 512 MB memory, and nVidia Quadro 2 Pro graphics card. Table 1 shows time measurements for 
a hair model of 10,000 hair strands and 40 segments per each strand. For shadow calculation, 40 opacity 
maps of 200 x 200 pixels were used. During the interactive rendering sessions, a typical setting is 
to use about 150,000 segments (e.g., 5000 hair strands with 30 segments per strand) with alpha value 
of 0.5 in a window size of 512 by 512. The system interactively (> 5 frames per second) renders the edited 
model with antialiasing. The frame rate will only get better with progress in CPU and graphics hardware 
performance. Shadow computation remains a bottleneck in rendering and we hope to further accelerate the 
shadow calculation with 3D texture hardware [Kim and Neumann 2001]. As reported in [Singh and Fiume 1998], 
the closest point on a curve becomes ambiguous as a point moves away from the curve or if the curve is 
of a high curvature. This can cause the control points of skeleton curves to drift during the bind/update 
procedure. However, the bind/update procedure is only used for rough editing of the hair model, while 
the user eventually corrects the drift by refining child clusters. This problem could be obviated by 
fitting the entire skeleton curves of the bound clusters, not just the control points, to the skeleton 
curve of the binder cluster. In the bind/update procedure, every descendent cluster is bound to the edited 
cluster, not just immediate child clusters. It is tempting to store the positions of child clusters as 
offsets to their parent cluster to speed up the bind operation. However, there are more time-consuming 
operations such as hair strands updates, temporary binding, and penetration avoidance. The offset approach 
could be inefficient for these operations that require the world coordinates of each cluster. For example, 
if all the root clusters are subdivided three times, the offset approach will incur three times more 
GC computations than the current approach. To speed up the model updates, we use caching schemes such 
as tabulation of sine functions and curve approximation to find the closest point on a curve. Thus, memory 
requirement is currently high (200 MB at maximum). Considering that the data structure of the hair tree 
itself is compact (4KB per each GC), the memory usage could be reduced by further optimizations and faster 
CPUs. The current bottleneck in the model update is the curve evaluation for each strand (table 1). For 
efficiency, we tag hair clusters that the user is currently editing and update hair strands only for 
these clusters. The spline curve drawing in OpenGL may speed up the process if combined with programmable 
vertex shaders for local shading calculation. However, that would require an alternative antialiasing 
algorithm since our visibility algorithm requires every strand to be represented as polyline segments. 
When the root positions of hair strands are sampled, the hair density can vary over the curved scalp 
surface. Although this is not a serious problem, we could provide an additional density map on the scalp 
that the user can paint. The scalp surface may also be used as an atlas for texture maps of other parameters 
such as colors, thickness, etc. As with any multiresolution approach, the benefit of MHM is maximized 
with complex models, such as natural curly hairs. The major benefit of MHM lies in a user s capability 
to rapidly add structural details and variations, given initial templates. Providing a gallery of rough 
style templates would greatly speed up the production of various hairstyles. We use GCs as our cluster 
representation mainly due to its modeling efficiency (e.g., axial scaling/twist) and also because a hair 
strand can be treated as a very thin GC. However, GCs may not be best suited to modeling global shapes 
for smooth hair or short hair. Existing methods may suffice for smoothly varying hairstyles (e.g., [Anjyo 
et al. 1992; Hadap and Thalmann 2000]) and short hair/animal fur (e.g, [Goldman 1997; Lengyel 2000; Lengyel 
et al. 2001]). Thus, it may be worth investigating methods to fit root clusters into other control primitives 
(e.g., polygonal models) or using MHM as a backend system to refine the results from other modeling methods. 
  9 CONCLUSION Human hair modeling is often an arduous task. The interactive multiresolution hair modeling 
(MHM) system presented in this paper strives to ease the process. The system evolved from a quest to 
find a fundamental hair representation flexible enough to encompass the wide range of human hairstyles. 
MHM extends the scope of hair models ranging from smooth shapes and short hair, to complex cases such 
as curly hair, braids and spiky clusters. However, avenues remain for future extensions. Currently, the 
hair model is defined for a specific head model. It would be useful to transfer hairstyles from one head 
model to another. The scalp surface abstraction and the copy/paste tool may provide good starting points. 
Scattered data interpolation techniques such as Radial Basis Functions may also provide reasonable adaptation 
of the hair model to different head meshes. Completely automating the process may require sophisticated 
dynamics to handle both hair/hair and hair/head interactions. In our framework, the user implicitly designs 
the hair/hair interactions in the form of multiresolution clusters. Extending MHM to support animation 
seems feasible in many ways. At the highest level, we could provide the user with kinematics or dynamics 
controls over root clusters. Alternatively, strand level animation techniques could be employed to animate 
the lowest level strands. However, in reality when hair moves, hair/hair interactions cause hair clusters 
to change dynamically, from large coherently moving clusters to independently moving strands, and vice 
versa. Simulation of such dynamic clustering effects is a challenging problem. The preservation of user-defined 
style raises another issue. Currently, we do not know of any animation technique that can handle this 
complexity. The fluid flow model by Hadap and Thalmann [2001] and the layered wisp model by Plante et 
al. [2001] approach the hair/hair interaction problem at discrete levels, the former at strand level 
and the latter at cluster level. Simulating dynamic clustering effects may involve combining those methods 
in a continuous manner. Ultimately, hair clusters should split and merge dynamically, automatically, 
and often partially. We anticipate that the dynamic clustering problem could be formulated as that of 
dynamically updating the hair tree, consistently changing the hair strand ownership. The rich volumetric 
models generated by MHM hint that realistic hair motion may be efficiently simulated in a multiresolution 
framework, in the sprit of the simulation level of detail systems [O Brien et al. 2001]. ACKNOWLEDGEMENTS 
This work received funding from the Annenberg Center and the National Science Foundation through its 
ERC funding of the Integrated Media Systems Center at USC. We recognize the holistic support from our 
colleagues in the USC CGIT laboratory. Special thanks go to Jun-Yong Noh, Douglas Fidaleo, and Clint 
Chua for proofreading the initial manuscript and video editing. We deeply thank Hiroki Itokazu for the 
head model and Bret StClair for his wonderful textures. We also thank J. P. Lewis for his numerous contributions 
and Dr. LaFrance for kindly showing us her unpublished manuscript. Many thanks go to Sean Mausch for 
his CPT software and Laehyun Kim for directing us to this algorithm. We also appreciate the anonymous 
reviewers and referee for their many valuable comments and suggestions.  References AGUADO, A. S., 
MONTIEL, E., ZALUSKA, E. 1999. Modeling Generalized Cylinders via Fourier Morphing. ACM Transactions 
on Graphics. 18(4), 293-315. ANJYO, K., USAMI, Y., AND KURIHARA, T. 1992. A Simple Method for Extracting 
the Natural Beauty of Hair. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), 26(4), ACM, 111-120. 
BLOOMENTHAL, J. 1990. Calculation of Reference Frames Along A Space Curve. In A. Glassner, editor, Graphics 
Gems, Academic Press, 567-571. CHEN, L., SAEYOR, S., DOHI, H., AND ISHIZUKA, M. 1999. A System of 3D 
Hairstyle Synthesis Based on the Wisp Model. The Visual Computer, 15(4), 159-170. CSURI, C., HAKATHORN, 
R., PARENT, R., CARLSON, W., AND HOWARD, M. 1979. Towards an Interactive High Visual Complexity Animation 
System. In Computer Graphics (Proceedings of ACM SIGGRAPH 79), 13(4), ACM, 288-299. DALDEGAN, A., THALMANN, 
N. M., KURIHARA, T., AND THALMANN, D. 1993. An Integrated System for Modeling, Animating and Rendering 
Hair. In Computer Graphics Forum (Proceedings of Eurographics 93), 211-221. FALK, R. AND SAND, L. R. 
(ORGANIZERS) 2001. Shrek : The Story Behind The Screen. ACM SIGGRAPH Course Note 19. GOLDMAN, D. 1997. 
Fake Fur Rendering. In Proceedings of ACM SIGGRAPH 97, ACM Press / ACM SIGGRAPH, New York, 127 134. 
GRABLI, S., SILLION, F. X., MARSCHNER, S. R., AND LENGYEL, J. E. 2002. Image-Based Hair Capture by Inverse 
Lighting. In Proceedings of Graphics Interface 2002, to appear. HADAP, S. AND THALMANN, N. M. 2000. Interactive 
Hair Styler Based on Fluid Flow. Eurographics Workshop on Computer Animation and Simulation, 87-100. 
HADAP, S. AND THALMANN, N. M. 2001. Modeling Dynamic Hair as a Continuum. In Computer Graphics Forum 
(Proceedings of Eurographics 2001), 329 338. HAUSNER, A. 2001. Simulating Decorative Mosaics. In Proceedings 
of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, New York, 573-580. KAJIYA, J. AND KAY, T. 1989. Rendering 
Fur with Three Dimensional Textures. In Computer Graphics (Proceedings of ACM SIGGRAPH 89), 23(4), ACM, 
271-280. KIM, M., PARK, E., AND LEE, H. 1994. Modeling and Animation of Generalized Cylinders with Variable 
Radius Offset Space Curves. The Journal of Visualization and Computer Animation, 5(4), 189-207. KIM, 
T. AND NEUMANN, U. 2000. A Thin Shell Volume for Modeling Human Hair. In Computer Animation 2000, Philadelphia, 
IEEE Computer Society, 121-128. KIM, T. AND NEUMANN, U. 2001. Opacity Shadow Maps. Rendering Techniques 
2001, Springer, 177-182. LAFRANCE, M. 2001. First Impressions and Hair Impressions. Unpublished manuscript. 
Yale University, Department of Psychology, New Haven, Connecticut. http://www.physique.com/sn/sn_yale-study2.asp 
LEE, D.-W. AND KO, H.-S. 2001. Natural Hairstyle Modeling and Animation. Graphical Models, 63(2), 67-85. 
LENGYEL, J. E. 2000. Realtime Fur. Rendering Techniques 2000, Springer, 243-256. LENGYEL, J. E., PRAUN, 
E., FINKELSTEIN, A., AND HOPPE, H. 2001. Real-Time Fur over Arbitrary Surfaces. ACM Symposium on Interactive 
3D Techniques 2001, 227-232. LEVOY, M. AND WHITTED, T. 1985. The Use of Points as a Display Primitive. 
Technical Report 85-022, Computer Science Department, University of North Carolina at Chapel Hill, January. 
LOKOVIC, T. AND VEACH, E. 2000. Deep Shadow Maps, In Proceedings of SIGGRAPH 2000, ACM, New York, 385-392. 
MAUCH, S. 2000. A Fast Algorithm for Computing the Closest Point and Computer Graphics. Morgan Kaufmann. 
 THALMANN, N. M. AND HADAP, S. 2000. State of the Art in Hair Simulation. International Workshop on Human 
Modeling and Animation, Korea Computer Graphics Society, 3-9. TURK, G. 1991. Generating Textures on 
Arbitrary Surface Using Reaction Diffusion. In Computer Graphics (Proceedings of ACM SIGGRAPH 91), 21(4), 
289-298. WATANABE, Y. AND SUENAGA, Y. 1992. A Trigonal Prism-Based Method for Hair Image Generation. 
IEEE Computer Graphics and Application, 12(1), 47-53. XU, Z. AND YANG, X. D. 2001. V-HairStudio: An 
Interactive Tool for Hair Design. IEEE Computer Graphics and Applications, 21(3), 36-43. YANG, X. D., 
XU, Z., YANG, J., AND WANG, T. 2000. The Cluster Hair Model. Graphical Models, 62(2), 85-103. Appendix 
A. Generation of Generalized Cylinder from Scalp Surface When the user places a 2D contour on the scalp, 
the corresponding GC is created. First, the root position P0 of the skeleton curve is calculated as P0 
= C(0) = S(uc, vc), where uc and vc denote the center of the scalp space contour. Let N0 be the surface 
normal at P0. Initially, the skeleton curve is formed as a straight line (Each control point Pi of the 
skeleton curve is given as Pi = P0 + iL / (N 1) N0, where N is the number of control points) and L is 
the length of the skeleton curve. As a next step, we convert Rs(.), the scalp space contour to Rw(.) 
in world space. Let Ri be the 3D position of each sample point of the contour (recall from section 4.1 
that each contour is represented with the sample values R(.i)). ss i c ii  R = S(u + R(.i )cos(. ), 
vc + R(. )sin(.i )) To make the contour planar, we project Ri to the plane formed by P0 and N0. Let 
R be such a projected point. Then, Rw(.i) is the distance i from the center point P0 to R in 3D space. 
i R w(.i ) = Ri - P0 The contour function is then decomposed into the global scale s and offset function 
as defined in section 4.1. The number of contours is the same as the number of control points of the 
skeleton curves. These contours are simply copied at each control point of the skeleton curves. B. Inclusion 
Test for a Hair Strand A hair strand is given its location on the scalp by (uh,vh). Let the center of 
the contour Rs(.) of each cluster be represented by (uc,vc). Let .u = uh -uc and .v = vh -vc. Then, the 
angle .h is given as Distance Transform. Submitted for publication in the Jounral of SIAM SISC. http://www.acm.caltech.edu/~seanm/software/cpt/cpt.pdf 
... . u . .= cos - 1 h . MCREYNOLDS, T. (Organizer) 1997. Programming with OpenGL: Advanced Techniques, 
ACM SIGGRAPH Course Note 11. NEYRET, F. 1997. Modeling, Animating, and Rendering Complex Scenes Using 
Volumetric Textures. IEEE Transaction on Visualization and Computer Graphics, 4(1), 55-70. O BRIEN, D., 
FISHER, S., AND LIN M. 2001. Automatic Simplification of Particle System. In Computer Animation 2001, 
Seoul, Korea, IEEE Computer Society, 210-219. PARKE, F. AND WATERS, K. 1996. Computer Facial Animation. 
A K Peters. PLANTE, E., CANI, M.-P., AND POULIN, P. 2001. A Layered Wisp Model for Simulating Interactions 
Inside Long Hair. Eurographics Workshop on Computer Animation and Simulation 2001,139 148. ROSENBLUM, 
R., CARLSON, W., AND TRIPP E. 1991. Simulating the Structure and Dynamics of Human Hair: Modeling, Rendering 
and Animation. The Journal of Visualization and Computer Animation, 2(4), 141-148. SINGH, K. AND FIUME, 
E. 1998. Wires: A Geometric Deformation Technique. In Proceedings of ACM SIGGRAPH 98, ACM, New York, 
405 -415. STOLLNITZ, E. AND DEROSE, T. D. AND SALESIN, D. H. 1996. Wavelets for and . h = 2p -. h if 
.u < 0 (B.1) S 22 A hair strand is contained by a cluster if R(. ) =.u +.v h A bounding box of each 
contour is used to speed up the test. Also, the inverse cosine function is tabulated. If a hair strand 
is assigned to a cluster, the parametric coordinate for the root position of the hair strand is given 
as (rh ,.h) ,where .u 2 +.v 2. r = h R s(. h ) C. Binding a Point P to a Cluster V Given a point P and 
a cluster V, the parametric coordinate (r, ., t) of the point with respect to the cluster is found as 
follows. First, t is chosen as the value that minimizes the Euclidean distance between point P and the 
skeleton curve C(t). Let PC be such a point that PC = C(t). Then . is given as the angle between a vector 
connecting P and PC and the principle normal N(t). The angle should be corrected for the scaling terms 
SN(t), SB(t). Let the projection of the vector on N(t), B(t) be PN, PB. PC P Then, the angle . is given 
using equation B.1, by letting .u = PN / SN(t), and .v = PB / SB(t). After correcting for the twist term 
W(t), .=.- W(t). The parameter r is the ratio between the Euclidean distance between P, PC and R(., t). 
r = P - PC /R (. , t ).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566628</article_id>
		<sort_key>630</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Modeling and rendering of realistic feathers]]></title>
		<page_from>630</page_from>
		<page_to>636</page_to>
		<doi_number>10.1145/566570.566628</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566628</url>
		<abstract>
			<par><![CDATA[We present techniques for realistic modeling and rendering of feathers and birds. Our approach is motivated by the observation that a feather is a branching structure that can be described by an L-system. The parametric L-system we derived allows the user to easily create feathers of different types and shapes by changing a few parameters. The randomness in feather geometry is also incorporated into this L-system. To render a feather realistically, we have derived an efficient form of the bidirectional texture function (BTF), which describes the small but visible geometry details on the feather blade. A rendering algorithm combining the L-system and the BTF displays feathers photorealistically while capitalizing on graphics hardware for efficiency. Based on this framework of feather modeling and rendering, we developed a system that can automatically generate appropriate feathers to cover different parts of a bird's body from a few "key feathers" supplied by the user, and produce realistic renderings of the bird.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[L-system]]></kw>
			<kw><![CDATA[bidirectional texture function]]></kw>
			<kw><![CDATA[bird]]></kw>
			<kw><![CDATA[feather]]></kw>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003766.10003767.10003769</concept_id>
				<concept_desc>CCS->Theory of computation->Formal languages and automata theory->Formalisms->Rewrite systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003766.10003771</concept_id>
				<concept_desc>CCS->Theory of computation->Formal languages and automata theory->Grammars and context-free languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P304750</person_id>
				<author_profile_id><![CDATA[81408597193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yanyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15025395</person_id>
				<author_profile_id><![CDATA[81365590697]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yingqing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14040907</person_id>
				<author_profile_id><![CDATA[81100085615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Baining]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14159670</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. Synthesizing feather textures in galliformes. Computer Graphics Forum, 14(3):407-420, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kristin J. Dana and Shree Nayar. 3d textured surface modeling. In Proceedings of IEEE Workshop on the Integration of Appearance and Geometric Methods in Object Recognition, pages 46-56, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. Reflectance and texture of real-world surfaces. ACM Transactions on Graphics, 18(1):1-34, January 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>677816</ref_obj_id>
				<ref_obj_pid>646015</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cristiano G. Franco and Marcelo Walter. Modeling the structure of feathers. In Proceedings of SIBGRAPI 2001 - XIV Brazilian Symposium on Computer Graphics and Image Processing, page 381, October 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>920650</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[James Hanan. Parametric L-Systems and Their Application to the Modeling and Visualization of Plants. PhD Thesis, University of Regina, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Jan J. Koenderink and Andrea J. Van Doorn. Illuminance texture due to surface mesostructure. Journal of the Optical Society of America, 13(3):452-463, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Aristid Lindenmayer. Mathematical models for cellular interaction in development, parts i and ii. Journal of Theoretical Biology, 18:280-315, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383269</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Xinguo Liu, Yizhou Yu, and Heung-Yeung Shum. Synthesizing bidirectional texture functions for real-world surfaces. Computer Graphics Proceedings, Annual Conference Series, pages 97-106, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Tom Malzbender, Dan Gelb, and Hans Wolters. Polynomial texture maps. Proceedings of SIGGRAPH 2001, pages 519-528, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Yoav I. H. Parish and Pascal M&#252;ller. Procedural modeling of cities. In Proceedings of SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, pages 301-308, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Christopher M. Perrins and Alex L. A. Middleton. The Encyclopedia of Birds. Checkmark Books, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344987</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Emil Praun, Adam Finkelstein, and Hugues Hoppe. Lapped textures. Proceedings of SIGGRAPH 2000, pages 465-470, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz, Mark Hammel, Jim Hanan, and Radom&#237;r Mech. Visual models of plant development. Handbook of Formal Languages, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz, Mark Hammel, Radom&#237;r Mech, and Jim Hanan. The artificial life of plants. SIGGRAPH 95 Course Notes, 7:1-38, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty of Plants. Springer-Verlag, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Balajee Ramakrishnananda and Kok Cheong Wong. Animating bird flight using aerodynamics. The Visual Computer, 15(10):494-508, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Craig W. Reynolds. Flocks, herds, and schools: A distributed behavioral model. In Computer Graphics (Proceedings of SIGGRAPH 87), volume 21, pages 25-34, Anaheim, California, July 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Bart Rulon. Painting Birds Step by Step. North Light Books, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>266784</ref_obj_id>
				<ref_obj_pid>266774</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Morgan Schramm, Jay Gondek, and Gary Meyer. Light scattering simulations using complex subsurface models. In Graphics Interface '97, pages 56-67, May 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566634</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Xin Tong, Jingdan Zhang, Ligang Liu, Xi Wang, Baining Guo, and Heung-Yeung Shum. Synthesis of bidirectional texture functions on arbitrary surfaces. Computer Graphics Proceedings, Annual Conference Series, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-tiling polygonal surfaces. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):55-64, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling and Rendering of Realistic Feathers Yanyun Chen Yingqing Xu Baining Guo Heung-Yeung Shum  
Microsoft Research Asia Abstract We present techniques for realistic modeling and rendering of feath­ers 
and birds. Our approach is motivated by the observation that a feather is a branching structure that 
can be described by an L­system. The parametric L-system we derived allows the user to easily create 
feathers of different types and shapes by changing a few parameters. The randomness in feather geometry 
is also in­corporated into this L-system. To render a feather realistically, we have derived an ef.cient 
form of the bidirectional texture function (BTF), which describes the small but visible geometry details 
on the feather blade. A rendering algorithm combining the L-system and the BTF displays feathers photorealistically 
while capitalizing on graphics hardware for ef.ciency. Based on this framework of feather modeling and 
rendering, we developed a system that can automatically generate appropriate feathers to cover different 
parts of a bird s body from a few key feathers supplied by the user, and produce realistic renderings 
of the bird. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; F.4.2 
[Mathematical Logic and Formal Lan­guages]: Grammars and Other Rewriting Systems; I.3.3 [Computer Graphics]: 
Picture/Image Generation. J.3 [Life and Medical Sci­ences]: Biology; Keywords: L-system, bidirectional 
texture function, natural phe­nomena, rendering, feather, bird 1 Introduction Since the earliest times, 
the natural beauty of feathers has fascinated humans from Hawaiian chiefs adorned in brightly colored 
robes of feathers to Native Americans wearing exquisitely crafted head­dresses. Feathers are everyday 
wear for birds, who use feathers to .y, to keep warm, and to attract mates. Despite their ubiquitous 
na­ture, we do not have a systematic way to model and render feathers in computer graphics. This work 
is an attempt to .ll that gap. There are two main tasks in feather modeling and rendering. The .rst is 
the modeling and rendering of individual feathers. At the macroscopic level, feathers come in different 
types and shapes: bristles, contour feathers, down, .ight feathers, semiplumes, and .loplumes [11]. At 
the microscopic level, feathers have a special appearance which is attributable to their barbs and barbules 
[11] 3F Beijing Sigma Center, No 49 Zhichun Road, Haidian District, Bei­jing 100080, P R China, email: 
bainguo@microsoft.com Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 Figure 1: An eagle modeled and rendered using our system. shown in Fig. 
3. These .ne-level visible geometry details consti­tute the mesostructure of the feather blade [6, 3]. 
The main chal­lenges in modeling and rendering feathers are to allow the user to easily generate many 
feathers of different shapes and to capture the special appearance of feathers. The second task is growing 
feathers on birds. Looking at a bird you can see that various types of feathers are arranged over the 
bird s body in an extremely ordered fashion. Feathers overlap each other, usually covering most of a 
bird s skin and thus creat­ing a streamlined shape that is perfect for .ight. Achieving this feather 
arrangement by manually placing thousands of feathers onto a bird s body is clearly very tedious and 
time-consuming. In this paper, we present two techniques, one for each task de­scribed above. For modeling 
and rendering of individual feath­ers, our technique uses a bidirectional texture function (BTF) [3] 
controlled by a parametric L-system [14]. Consistent with feather anatomy [18], this L-system allows 
the user to generate feathers of different shapes by adjusting a few parameters. Another important feature 
of our L-system is that it simulates the random gaps in the vanes of feathers, which are important for 
visual realism. The special feather appearance arises from both spatially-variant surface re.ectance 
and local surface height variations due to the barb-barbule mesostructure. The mesostructure, for example, 
cre­ates .ne-scale shadows, occlusions, and specularities that are inte­gral parts of the feather appearance 
[6, 3]. To capture this level of complexity, we developed a feather rendering algorithm based on the 
L-system and a realistic BTF [3] that models the barb-barbule mesostructure and the directional radiance 
distribution at each sur­face point. This BTF is pre-computed so that graphics hardware can be used to 
render feathers ef.ciently. Our technique for feather growing allows the user to provide a 3D model of 
a bird and a number of key feathers at different Figure 2: The user interface of our feather modeling 
system. The sketch on the left illustrates the rachis curve, the left and right barb curves, and the 
left and right outline curves. The user can interactively change these curves to control the overall 
shape of the feather. Window (1) is for controlling the rachis curve, window (2) for the left and right 
barb curves, and window (4) for the left and right outline curves. Window (3) is a user-supplied texture. 
locations on the model. The feather growing algorithm then auto­matically generates all feathers on the 
model, guided by the shape and orientation hints from the user. The feather growing process includes 
creating a feather for the current location based on nearby key feathers and placing all feathers so 
created in an orderly fash­ion. The main challenge in growing feathers is determining the orientations 
of feathers such that neighboring feathers do not inter­penetrate. We address this problem with a recursive 
collision detec­tion technique. Fig. 1 provides an example of bird rendering generated by our system. 
Applications of realistic feather modeling and rendering in­clude display of birds and feather objects 
(Native Americans head­dresses, feather ornaments, etc.) in motion pictures and games, es­pecially those 
including birds or feather objects as main characters or objects of importance. Realistic rendering of 
birds and feather objects are also desirable for web-based education (e.g., virtual bird walk) or product 
display. The rest of the paper is organized as follows. In Section 2, we review some related work. Section 
3 discusses the modeling and rendering of individual feathers. Section 4 describes our system for placing 
feathers on a bird s body. In section 5, we show examples of feather modeling and rendering. We conclude 
with some discussion about future work in Section 6. 2 Related Work A number of researchers have mentioned 
feathers in their papers. Dai et al. developed a method for synthesizing feather textures of a special 
class of feathers, i.e., Galliformes feathers [1]. Using chaotic structures, they modeled the eye-like 
spot and river-like strip for feathers in the Galliformes family. They also crafted an ad-hoc branching 
structure to model feather skeleton. Schramm et al. applied subsurface modeling technique to the study 
of the surface re.ectance properties of an iridescent hummingbird feather [19]. Franco and Walter presented 
a parametric model for feather modeling based on Bezier curves [4]. Two nice examples are shown in [4], 
but no algorithm details are given. L-systems have been used by Prusinkiewicz et al. extensively to capture 
the natural beauty of plants [13, 14, 15]. A plant is mod­eled as a linear or branching structure composed 
of repeated units called modules. An L-system represents the development of this structure by productions. 
In the original L-system [7], the modeled structure is a .nite collection of modules and each module 
is in one of the .nite number of states. Parametric L-systems increase the expressiveness of L-systems 
by adding a state vector of numerical parameters [5, 15]. The meanings of the parameters depend on the 
semantics of the module de.nition. For example, the parameters may describe the shape of a plant being 
modeled. Parametric L­systems are important for feather modeling because it allows the user to easily 
create a large number of feathers by varying the pa­rameters. L-systems have also been used by Parish 
and M¨uller for modeling urban environments [10]. We are not aware of L-systems used for feathers or 
birds. The BTF was introduced by Dana et al. for describing real-world textures [3]. A 2D texture is 
a poor description for real-world tex­tures because it completely ignores surface mesostructures. The 
BTF, on the other hand, captures surface mesostructure well. Sum­maries of recent work on the BTF can 
be found in [2, 8, 20]. A concept related to the BTF is the polynomial texture map proposed by Malzbender 
et al. [9]. The polynomial texture map can model the appearance changes of real-world textures due to 
change of il­lumination, but the viewing direction must be .xed. 3 Individual Feathers 3.1 Feather Geometry 
Fig. 2 illustrates the user interface for modeling feather geometry. We know from Fig. 3 that the geometry 
of a feather is de.ned by its rachis and the barbs distributed along both sides of the rachis. In our 
system, the shapes of the rachis and the barbs are mainly controlled by the rachis curve tm(ebl , left 
barb curve ba1m(ebl , right barb curve ba m(ebl , left outline curve O 1m(ebl , and right outline curve 
O m(ebl as shown in Fig. 2. With these curves, the user has .ne control over feather geometry, which 
is necessary for modeling various types of feathers. For example, a .ight feather has a narrow leading 
edge that the wind hits .rst and a wider trailing edge. Flightless birds, on the other hand, have almost 
symmetrical sides on corresponding feathers. This difference in feather geometry can be captured by adjusting 
the outline curves O1m(ebl and O m(eel . For simplicity, we assume that the shapes of all barb curves 
on the same side of the rachis are identical except for their lengths.  Figure 3: Feather anatomy. The 
rachis or shaft of the .ight feather supports the vanes (i.e., the blades) of the feather. Within the 
vane of the feather, there are two lateral sets of barbs, interlocking the feather together. On both 
sides of a barb are many barbules. m(eB jl Under this assumption the barb curves at the positionalong 
the rachis are ba1mmm l and ba mm tl with lengths determined by O 1m(e jl and O m(e )l respectively. 
We can regard a feather as a branching structure composed of repeated units called modules. An L-system 
represents the devel­opment of a branching structure by productions [14]. A production replaces a predecessor 
module by several successor modules. A production can either be context-free and depends only on the 
mod­ule replaced, or be context-sensitive, in which case the production depends on the replaced modules 
as well as its immediate neighbor modules. We use context-free productions of the form F R P eF R Bbm 
F P eF bm where is the production label, whereas ,, and are the predecessor, condition, and successor 
respectively. The produc­tion is carried out only if the condition is met. Given the rachis, barb, and 
outline curves, we can model feather geometry using a parametric L-system [14, 13] as follows: R b Pww:R 
bm(F+lmFlRFF <(BR B imF RFFl)&#38;R B <mFBRF+l)&#38;bmF F1l P lR BimF R)jlRej<AiBBimFBR jRF1l (1)P 2 
R B<mFBRjjlRij<A<BB<mFBRjj+F1l ( where de.nes the length of the feather as well as the density AiA< of 
the barbs at each side of the rachis while and de.ne the lengths of the left and right barbs respectively. 
We follow the bm(FFl notation of [14, 13]. The axiom generates a feather based on Pw the rachis and barb 
curves. Production produces a small segment om(ebl of the rachis according to the rachis curveand grows 
a barb P on each side of the rachis using recursion. Productioncreates a small segment of the left barb 
according to the left barb curve ba1mmm lP 2 while production proceeds similarly on the right barb. The 
leftmost image in Fig. 4 shows a feather created using equation (1). A problem with equation (1) is that 
it ignores the interaction be­tween neighboring barbs. A feather geometry generated by equa­tion (1) 
looks plausible but too regular. For a real feather, the two lateral sets of barbs within the vane of 
the feather interlock the feather together. The interlocking is important for .ight, keeping the air 
from rushing right through the feather. When the interlock­ing system of a feather is disturbed, as when 
a twig brushes through a feather, random gaps form between the barbs on the same side of the rachis. 
This is a phenomenon that we want to capture qualita­tively. On the one hand, neighboring barbs cling 
to each other by the little hooks called cilium on the ends of the barbules shown in Figure 4: The leftmost 
feather is generated without random gaps in the feather blade. The other four feathers demonstrate different 
kinds of feathers that can be generated by changing a few parame­ters in the feather modeling system. 
 Fig. 3 [11]. On the other hand, external forces can break the inter­locking if the total external force 
exceeds that exerted by the little hooks. To simulate this effect, we introduce external forces into 
our parametric L-system as follows R bm(FRFRF+l PwR+bF Re+iRe+<RFF <(+i++ F+<++ F &#38;&#38; &#38;&#38; 
 BR Bmiml<F RFFl)&#38;bmF F1FRe+iF+ a Rb+<F+ al F RFFl)&#38;R Bm P +R+bF Re+iRe+<RFF <(+iF+ F+<++ F &#38;&#38; 
&#38;&#38; BR o i B iF RFFl)&#38;R B <F F1FRRe+&#38;<F+a P 2 R+bm<RFF <(+i++ F+<F+ F mmlmF RFFl)&#38;bmFl 
F Re+iRe+l &#38;&#38; &#38;&#38; BR BiF RFFl)&#38;R o<B<FBRFFl)&#38;bF F1FRt+iF+ aiRFFl P R+bF Re+iRe+<lRFF 
<(+iF+ F+<F+ F mmm &#38;&#38; &#38;&#38; BR omiBiR o<B<F F1FR FR mF RFFl)&#38;mF RFFl)&#38;bmF+l P 
+R+B imF R)jlRij<AiBB imF R)j+F1l P 5 R+B<mF R)jlRej<A<BB<mF R)j+F1l +i+< where and are the total external 
forces on the left and right oio< barbs respectively. andare directional rotations of the left +i+<Pw 
 and right barbs in response to and . The productions P throughbasically say that for each step we move 
along the rachis +ai+a<+a curve, we increment and by a random external force . If +i+<+ F at some point 
() exceeds the force exerted by the cilium, the left (right) barb is rotated by a random angle e in a 
direction +i+<B determined by (). The rotation of a barbis assumed to be within the tangent plane de.ned 
by the tangent vectors of the rachis BB and barbat the point where the rachis and barb intersect. The 
random rotation angle is computed as e=Ae mbl , where be mbl is the k -th random number generated with 
the random seed and A is +i+< a user-de.ned constant. After the rotation, () starts to accu­mulate again 
from zero. The random seed of each feather is saved so that its shape remains the same every time it 
is rendered. Fig. 4 contains a number of feathers created with random gaps between the barbs. Fig. 4 
also exhibits feathers of different types and shapes by changing parameters of the L-system. 3.2 Feather 
Appearance We use a BTF to capture the mesostructure and the directional ra­diance distribution at each 
point on the feather surface. A BTF is a 6D function Tmm mR a Ree lReh Rei1Rth1l , where mee Reh l is 
the viewing direction v and ei1Rbh1 is the lighting direction P at surface point mm Real [3]. To calculate 
the BTF, we built a geometry model for the Figure 5: Sampling the BTF on the barb-barbules mesostructure. 
The sampling is done along the horizontal line highlighted in red. barbs and barbules as shown in Fig. 
5 and render this structure for all viewing and lighting settings. Since the rendering is done off­line, 
we can afford complicated geometry and sophisticated light­ing models. The geometry shown in Fig. 5 is 
built according to the anatomy of the barb-barbule structure. This model is opaque with both diffuse 
and specular re.ections. As Fig. 5 indicates, we only sample the BTF along the -axis to obtain a 5D BTF 
Rei RehRRehlmm Rea+FFRei RehRRehl TTT)Tmm e11=Te11 aF for some constant . As we shall see, this 5D BTF 
TTT)T suf.ces for rendering the actual 6D BTF of a feather because of the spatial arrangement of barbs 
and barbules. We render the mesostructure of barbs and barbules such that .ne-scale shadows, occlusions, 
and specularities are well-captured in TTT)T . The rendering is done off­line by a ray-tracer. The above 
model of feather mesostructure has a number of ad­vantages. First, the off-line BTF calculations allow 
us to capture a very complicated mesostructure and directional radiance distribu­tion at each surface 
point. Second, the BTF can model additional effects such as oil-.lm interference and iridescence, which 
is im­portant for a class of familiar birds such as hummingbirds and ducks [19]. Finally, we can easily 
support level-of-detail rendering with a BTF. In close-up views, a BTF shows the mesostructure, as Fig. 
7 demonstrates. As the viewing distant increases, we can mipmap the BTF, which eventually becomes a BRDF 
at a distance.  3.3 Feather Rendering When rendering a feather, we call our parametric L-system to gen­erate 
the feather at run-time. The storage requirement is modest for each feather because only its L-system 
parameters and the ran­dom seeds are stored; details such as barb curves (polylines) and the random gaps 
on the vane (the feather blade). As illustrated in Fig. 3, a feather is composed of a series of barbs 
on both side of the rachis and each barb has its barbules. We use the pre-computed 5D BTF TTT)T to ef.ciently 
draw the barb-barbule mesostructure and thus achieve realistic rendering for a wide range of viewing 
distances. Fig. 6 illustrates the rendering of a feather. The feather L-system describes a barb B by 
a polyline LnB with vertices { XFFR L LsL RXw . B When we render a barb, we want to render the barb as 
well as B the barbules attached to . For this reason, we build a quadrilateral strip along the polyline 
LB as shown in Fig. 6. The local light­ing direction PmX )l and viewing direction vmX )l are calculated 
at every vertex X of LaB using the local coordinate frame at X . On each short edge E across the barb 
polyline LnB , a 1D texture is Figure 7: Left: A feather rendered without the BTF. Right: A feather rendered 
with the BTF.   created by looking up color values from TTb)b using the directions cnoXoi ) and loXoik) 
. Thus we obtain oo ) 1D textures of resolu­tion b where b is the spatial resolution of the BTF Tb)b 
. These textures are combined with the RGBA texture of the feather (see window (3) of Fig. 2) to render 
the barb B by multi-texturing and alpha-blending using graphics hardware. Fig. 7 compares feather rendering 
with and without the BTF. Fig. 8 illustrates different effects that can be achieved with the BTF. When 
sampling the BTF with a ray tracer, we can adjust pa­rameters so that the BTF gives a hard or soft appearance 
to the feather. Fig. 8 (c) and (d) demonstrate occlusions and specularities caused by barb mesostructure. 
 4 Feathering a Bird 4.1 Wings and Tail We .rst construct feathers on the wings and the tail using skele­tons. 
The feathers on a wing include the primaries, secondaries, humerals, primary coverts, and secondary coverts 
(these feathers Figure 8: (a) A feather with a hard appearance. (b) A feather with a soft appearance. 
(c) Local occlusion on the feather blade. (d) Local specularities on the feather blade. are rooted on 
the scapula, ulna/radius, metacarpus and phalanx re­spectively) [18]. As shown in Fig. 9, we use a polyline 
consisting of four line segments to represent the wing skeleton. Similarly a quadrilateral is used as 
the skeleton for the tail. Generally speak­ing, a bird has about 9 to 11 primaries, 6 to 24 secondaries, 
and 8 to 24 tail feathers [11, 18]. In our system, the user speci.es the num­bers of feathers of each 
type and edits 8 key feathers on the wing and 4 key feathers on the tail. The system generates other 
feathers by interpolation. Fig. 9 illustrates the feather placement on a wing.  4.2 Contour Feathers 
Contour feathers are the feathers that cover the body of a bird. Given a polygonal model describing a 
bird s body (without feath­ers), we want to place feathers of different sizes and shapes on the model. 
The huge number of feathers on a bird makes it impossible to manually place and edit individual feathers. 
In our system, we let the user specify a number of key feathers and their growing di­rections; the system 
automatically generates a full coverage of the bird based on the key feathers. This full coverage is 
created in three steps: a) re-tile the polygonal model to generate feather grow­ ing positions, b) interpolate 
the key feather growing directions to all feather growing points to get an initial growing di­ rection 
at each point, and c) recursively determine the .nal feather growing direc­ tion at every feather growing 
point, with collisions between feathers detected and recti.ed. The output is a feather placement map 
indicating feather growing positions and directions. The feather shape parameters are inter­polated from 
that of nearby key feathers. These shape parameters are used to generate a simpli.ed geometry for each 
feather. This simpli.ed geometry is used for collision detection in step (c). For an interpolated feather, 
the random gaps on the feather blade is generated by giving the feather a new random seed.  Figure 9: 
Feather placement on a wing. Left: The skeleton is drawn in red over the geometry model of the wing. 
The black lines show the position and orientation of .ight feathers. Middle: Rendering of .ight feathers. 
Right: Rendering of .ight feathers along with other small wing feathers. Feather Growing Positions: The 
vertices of the given polygonal model usually cannot be used directly as feather growing positions. The 
feathers at different parts of a bird have different sizes, and small feathers need to grow densely in 
order to cover the bird s skin. In addition, feathers tend to distribute evenly in a region of constant 
feather density. Vertices of a polygonal model often do not have these properties. To address this problem, 
we retile the polygonal model using Turk s algorithm [21]. This retiling creates a polygonal model whose 
vertices are evenly distributed over a region of constant den­sity. Turk also introduced a simple technique 
for adjusting vertex density based on curvature. We adapt this technique to controlling vertex density 
based on the sizes of feathers, which are interpolated from that of the key feathers using Gaussian radial 
basis functions, where radius is de.ned as distance over the mesh, as computed us­ing Dijkstra s shortest 
path algorithm. The user has control over the spatial extent and weight of each basis function. This 
interpolation scheme is similar to that used by Praun et al. for creating vector .elds on a polygonal 
surface [12]. After retiling, the vertices of the new polygonal model are the feather growing positions. 
Feather Growing Directions: From the growing directions of key feathers, we calculate initial growing 
directions at all vertices us­ing Gaussian radial basis functions as described before. The initial growing 
directions tend to cause inter-penetration of feathers be­cause these directions are derived without 
any consideration to the shapes of the feathers. To determine the .nal growing directions, we need to 
perform collision detection on the feathers based on their simpli.ed geometry and to adjust the feather 
growing direc­tions accordingly. Because of the large number of feathers and the complex shape of a bird 
s body, a collision detection between ev­ery pair of feathers is likely to be very expensive. To address 
this problem we adopt two strategies. First, we grow feathers in an or­derly fashion according to the 
initial growing directions. Second, we only consider local collisions between neighboring feathers be­cause 
collisions rarely happen between feathers far away from each other (two feathers are neighboring feathers 
if their growing posi­tions are connected by an edge). We implement these two strategies using a recursive 
collision detection algorithm. As Fig. 10 illustrates, we .rst classify the vertices around each c vertex 
into two groups according to the initial growing direc­tion O at c . The .rst group consists of vertices 
{cwrow cwr crc , where cwrt c means omcwrauc)o O >O . The second group is .... .... .F..F. . .. . . 
. .. ...... ... ...a. Figure 10: Recursive collision for eliminating the inter-penetration of neighboring 
feathers. {cwrw cwr 'mcrc , with cwr 'mc meaning omcwr uc)o O :O . Af­ ter the vertices around every 
vertex are so classi.ed, we invoke the following recursive collision detection algorithm at every vertex. 
c FindGrowingDirection() { c If the growing direction at has already been found return; corw c For each 
vertex FindGrowingDirection(cwr ); ccrcrc While feather() collides with feather() for some c adjust 
the growing direction at ; cr mc For each vertex c FindGrowingDirection(cwr ); cc and cwr Here feather() 
and feather(cwr ) are the feathers at vertices respectively. c To see how this algorithm works, consider 
a vertex on the retiled model with initial growing direction O , as is shown in ccc Fig 10. The vertices 
around are vertices Anthrough at. Among cc agcc mcc mccc .Tc cnAnArm these, and , whereas , , atc mc 
and . The algorithm will .rst determine the .nal growing c .Tc ag directions at and by recursion. Based 
on the .nal growing di­ c Tc gcc T rections at and as well as the shapes of feathers at , , and c ag, 
we can detect the collision between these feathers and adjust c the feather growing direction at by rotating 
it toward the surface c normal at . We rotate in small increments, stopping as soon as c no more collisions 
are detected. The growing direction at is now considered .nal and we can process c n , c n , c r, and 
c tthrough recursions. For fast collision detection, a simpli.ed geometry is used for each feather. Fig. 
11 shows the results of the .nal growing directions.   5 Results We have implemented our feather modeling 
and rendering system on a PC with a 864 MHz Pentium III processor, 256M RAM and an NVIDIA GeForce3 display 
adaptor. Fig. 12 shows two images. On the top is a real image of a feather, while a rendering from a 
similar viewing distance is shown on the bottom. Comparison of the two images demonstrates the similarity 
of our modeling and rendering result to an actual feather. Notice the mesostructure and random gaps on 
the feather blade are real­istically portrayed in the synthetic feather. These effects would be very 
dif.cult to capture using textured polygons. It usually takes the user a few minutes to model a feather. 
Fig. 13 shows an American Indian Headdress. The feathers in the image are modeled and rendered using 
our system. The feathers were manually placed by a graphics artist. The white plume-like effect is not 
rendered by our system but added by postprocessing. Fig. 1 shows an eagle modeled and rendered using 
our system. About 3500 feathers were placed on the bird. It took the user about 30 minutes to specify 
the 50 key feathers. Determining the feather growing positions and the initial growing directions took 
a few min­utes. The most time-consuming step is the calculation of the .nal growing directions, which 
took about 30 minutes. Rendering speed is about one minute per frame. 6 Conclusions We have presented 
techniques for modeling and rendering feathers. Our main idea is to represent a feather as a branching 
structure de­scribed by a parametric L-system. This approach allows the user to generate feathers of 
different types and shapes by adjusting a few parameters. The randomness in feather geometry is also 
simulated in the parametric L-system we derived. We described a rendering algorithm based on this L-system 
and an ef.cient form of the BTF representing the mesostructure of a feather blade. This algorithm can 
ef.ciently generate photo-realistic renderings of feathers. We also developed a system that allows the 
user to easily create a large number of feathers on a bird s body and render the bird realistically. 
 It is our hope that this work will stimulate other researchers to explore the beautiful world of birds. 
Our work is only a .rst step towards realistic and ef.cient rendering of feathers and birds. Many aspects 
of this topic need further research. A limitation of our cur­rent system is that it does not support 
down feather rendering. An­other limitation is that we do not handle oil-.lm interference and iridescence 
[19], which is important for a class of familiar birds such as ducks and hummingbirds. We plan to address 
these issues in the near future. Another interesting area of future work is bird an­imation [16]. The 
.ocking of birds has been explored by Reynolds [17]. More work is needed to understand how individual 
birds .y. Finally, we are interested in applying L-systems to other natural phenomena. Acknowledgments 
We would like to thank Xinguo Liu for useful discussions. Many thanks to Dongyu Cao for her illustrations, 
to Yin Li, Gang Chen, and Steve Lin for their help in video production, to Steve Lin for proofreading 
this paper, and to anonymous reviewers for their con­structive critique, which has signi.cantly improved 
the presentation of this paper.  References [1] Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. 
Synthesizing feather textures in galliformes. Computer Graphics Forum, 14(3):407 420, August 1995. [2] 
Kristin J. Dana and Shree Nayar. 3d textured surface modeling. In Proceedings of IEEE Workshop on the 
Integration of Appearance and Geometric Methods in Object Recognition, pages 46 56, June 1999. [3] Kristin 
J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. Re.ectance and texture of real-world 
surfaces. ACM Transactions on Graphics, 18(1):1 34, January 1999. [4] Cristiano G. Franco and Marcelo 
Walter. Modeling the structure of feathers. In Proceedings of SIBGRAPI 2001 -XIV Brazilian Symposium 
on Computer Graphics and Image Processing, page 381, October 2001.  [5] James Hanan. Parametric L-Systems 
and Their Application to the Modeling and Visualization of Plants. PhD Thesis, University of Regina, 
1992. [6] Jan J. Koenderink and Andrea J. Van Doorn. Illuminance texture due to surface mesostructure. 
Journal of the Optical Society of America, 13(3):452 463, 1996. [7] Aristid Lindenmayer. Mathematical 
models for cellular interaction in develop­ment, parts i and ii. Journal of Theoretical Biology, 18:280 
315, 1968. [8] Xinguo Liu, Yizhou Yu, and Heung-Yeung Shum. Synthesizing bidirectional texture functions 
for real-world surfaces. Computer Graphics Proceedings, An­nual Conference Series, pages 97 106, August 
2001. [9] Tom Malzbender, Dan Gelb, and Hans Wolters. Polynomial texture maps. Pro­ceedings of SIGGRAPH 
2001, pages 519 528, August 2001. [10] Yoav I. H. Parish and Pascal M¨uller. Procedural modeling of cities. 
In Proceed­ings of SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, pages 301 
308, August 2001. [11] Christopher M. Perrins and Alex L. A. Middleton. The Encyclopedia of Birds. Checkmark 
Books, 1985. [12] Emil Praun, Adam Finkelstein, and Hugues Hoppe. Lapped textures. Proceed­ings of SIGGRAPH 
2000, pages 465 470, July 2000. [13] Przemyslaw Prusinkiewicz, Mark Hammel, Jim Hanan, and Radom ´ir 
Mech. Vi­sual models of plant development. Handbook of Formal Languages, 1996. [14] Przemyslaw Prusinkiewicz, 
Mark Hammel, Radom ´ir Mech, and Jim Hanan. The arti.cial life of plants. SIGGRAPH 95 Course Notes, 7:1 
38, 1995. [15] Przemyslaw Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty of Plants. Springer-Verlag, 
1990. [16] Balajee Ramakrishnananda and Kok Cheong Wong. Animating bird .ight using aerodynamics. The 
Visual Computer, 15(10):494 508, 1999. [17] Craig W. Reynolds. Flocks, herds, and schools: A distributed 
behavioral model. In Computer Graphics (Proceedings of SIGGRAPH 87), volume 21, pages 25 34, Anaheim, 
California, July 1987. [18] Bart Rulon. Painting Birds Step by Step. North Light Books, 1996. [19] Morgan 
Schramm, Jay Gondek, and Gary Meyer. Light scattering simulations using complex subsurface models. In 
Graphics Interface 97, pages 56 67, May 1997. [20] Xin Tong, Jingdan Zhang, Ligang Liu, Xi Wang, Baining 
Guo, and Heung-Yeung Shum. Synthesis of bidirectional texture functions on arbitrary surfaces. Com­puter 
Graphics Proceedings, Annual Conference Series, July 2002. [21] Greg Turk. Re-tiling polygonal surfaces. 
Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):55 64, July 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566629</article_id>
		<sort_key>637</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Eyes alive]]></title>
		<page_from>637</page_from>
		<page_to>644</page_to>
		<doi_number>10.1145/566570.566629</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566629</url>
		<abstract>
			<par><![CDATA[For an animated human face model to appear natural it should produce eye movements consistent with human ocular behavior. During face-to-face conversational interactions, eyes exhibit conversational turn-taking and agent thought processes through gaze direction, saccades, and scan patterns. We have implemented an eye movement model based on empirical models of saccades and statistical models of eye-tracking data. Face animations using stationary eyes, eyes with random saccades only, and eyes with statistically derived saccades are compared, to evaluate whether they appear natural and effective while communicating.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[HCI (human-computer interface)]]></kw>
			<kw><![CDATA[eye movement synthesis]]></kw>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[saccades]]></kw>
			<kw><![CDATA[statistical modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39041820</person_id>
				<author_profile_id><![CDATA[81452602366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sooha]]></first_name>
				<middle_name><![CDATA[Park]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382440</person_id>
				<author_profile_id><![CDATA[81100637289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Smith-Kettlewell Eye Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15038212</person_id>
				<author_profile_id><![CDATA[81452608047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Norman]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Badler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARGYLE, M., AND COOK, M. 1976. Gaze and Mutual Gaze. Cambridge University Press, London.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ARGYLE, M., AND DEAN, J. 1965. Eye-contact, distance and affiliation. Sociometry, 28, 289-304.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAHILL, A., ANDLER, D., AND STARK, L. 1975. Most naturally occuring human saccades have magnitudes of 15 deg or less. In Investigative Ophthalmol., 468-469.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BECKER, W. 1989. Metrics, chapter 2. In The Neurobiology of Saccadic Eye Movements, R H Wurtz and M E Goldberg (eds), 13-67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BEELER, G. W. 1965. Stochastic processes in the human eye movement control system. PhD thesis, California Institute of Technology, Pasadena, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BIZZI, E. 1972. Central programming and peripheral feedback during eye-head coordination in monkeys. In Bibl. Ophthal. 82, 220-232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BLANZ, V., AND VETTER, T. 1999. A morphable model for the synthesis of 3D faces. In Computer Graphics (SIGGRAPH '99 Proceedings), 75-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311537</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BRAND, M. 1999. Voice puppetry. In Computer Graphics (SIGGRAPH '99 Proceedings, 21-28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>11275</ref_obj_id>
				<ref_obj_pid>11274</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CANNY, J. 1986. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8, 679-698.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192272</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J., PELACHAUD, C., BADLER, N., STEEDMAN, M., ACHORN, B., BECHET, T., DOUVILLE, B., PREVOST, S., AND STONE, M. 1994. Animated conversation: Rule-based generation of facial expression gesture and spoken intonation for multiple converstaional agents. In Computer Graphics (SIGGRAPH '94 Proceedings), 413-420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J., TORRES, O., AND PREVOST, S. 1999. Turn taking vs. discourse structure: How best to model multimodal conversation. In In Machine Conversations, Y. Wilks (eds), 143-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383315</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J., VILHJALMSSON, H., AND BICKMORE, T. 2001. BEAT:The Behavior Expression Animation Toolkit. In Computer Graphics (SIGGRAPH '01 Proceedings), 477-486.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>301152</ref_obj_id>
				<ref_obj_pid>301136</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[CHOPRA-KHULLAR, S., AND BADLER, N. 1999. Where to look? automating visual attending behaviors of virtual human characters. In Autonomous Agents Conf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[COLBURN, R., COHEN, M., AND DRUCKER, S. 2000. Avatar mediated conversational interfaces. In Microsoft Technical Report.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280823</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[DECARLO, D., METAXAS, D., AND STONE, M. 1998. An anthropometric face model using variational techniques. In Computer Graphics (SIGGRAPH '98 Proceedings), 67-74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[DUNCAN, S. 1974. Some signals and rules for taking speaking turns in conversations. Oxford University Press, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>840052</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[ESSA, I., AND PENTLAND, A. 1995. Facial expression recognition using a dynamic model and motion energy. In ICCV95, 360-367.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[FAIGIN, G. 1990. The artist's complete guide to facial expression. Watson-Guptill Publications, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[GUENTER, B., GRIMM, C., AND WOOD, D. 1998. Making faces. In Computer Graphics (SIGGRAPH '98 Proceedings), 55-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[ISO/IEC JTC 1/SC 29/WG11 N3055. Text for CD 14496-1 Systems MPEG-4 Manual. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[ISO/IEC JTC 1/SC 29/WG11 N3056. Text for CD 14496-2 Systems MPEG-4 Manual. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[KALRA, P., MANGILl, A., MAGNENAT-THALMANN, N., AND THALMANN, D. 1992. Simulation of muscle actions using rational free form deformations. In Proceedings Eurographics '92 Computer Graphics Forum, Vol. 2, No. 3, 59-69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[KENDON, A. 1967. Some functions of gaze direction in social interaction. Acta Psychologica 32, 1-25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LEE, Y., WATERS, K., AND TERZOPOULOS, D. 1995. Realistic modeling for facial animation. In Computer Graphics (SIGGRAPH '95 Proceedings), 55-62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[LEIGH, R., AND ZEE, D. 1991. The Neurology of Eye Movements, 2 ed. FA Davis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>907312</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. 1974. Parametrized Models for Human Faces. PhD thesis, University of Utah.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PELACHAUD, C., BADLER, N., AND STEEDMAN, M. 1996. Generating facial expressions for speech. Cognitive Science 20, 1, 1-46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PETAJAN, E. 1999. Very low bitrate face animation coding in MPEG-4. In Encyclopedia of Telecommunications, Volume 17, 209-231.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND SALESIN, D. 1998. Synthesizing realistic facial expressions from photographs. In Computer Graphics (SIGGRAPH '98 Proceedings), 75-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>806812</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[PLATT, S., AND BADLER, N. 1981. Animating facial expressions. In Computer Graphics (S1GGRAPH '81 Proceedings), 279-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[VERTEGAAL, R., DER VEER, G. V., AND VONS, H. 2000. Effects of gaze on multiparty mediated communication. In Proceedings of Graphics Interface 2000, Morgan Kaufmann Publishers, Montreal,Canada: Canadian Human-Computer Communications Society, 95-102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>633442</ref_obj_id>
				<ref_obj_pid>633292</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[VERTEGAAL, R., SLAGTER, R., DER VEER, G. V., AND NIJHOLT, A. 2000. Why conversational agents should catch the eye. In Summary of ACM CHI 2000 Conference on Human Factors in Computing Systems.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>365119</ref_obj_id>
				<ref_obj_pid>365024</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[VERTEGAAL, R., SLAGTER, R., DER VEER, G. V., AND NIJHOLT, A. 2001. Eye gaze patterns in conversations: There is more to conversational agents than meets the eyes. In ACM CHI 2001 Conference on Human Factors in Computing Systems, 301-308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WARABI, T. 1977. The reaction time of eye-head coordination in man. In Neurosci. Lett. 6, 47-51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[WATERS, K. 1987. A muscle model for animating three-dimensional facial expression. In Computer Graphics (SIGGRAPH '87 Proceedings), 17-24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Eyes Alive Sooha Park Lee* Jeremy B. Badler Norman I. Badler* University of Pennsylvania * The Smith-Kettlewell 
Eye Research Institute  Figure 1: Sample images of an animated face with eye movements Abstract For 
an animated human face model to appear natural it should pro­duce eye movements consistent with human 
ocular behavior. Dur­ing face-to-face conversational interactions, eyes exhibit conversa­tional turn-taking 
and agent thought processes through gaze direc­tion, saccades, and scan patterns. We have implemented 
an eye movement model based on empirical models of saccades and statis­tical models of eye-tracking data. 
Face animations using stationary eyes, eyes with random saccades only, and eyes with statistically derived 
saccades are compared, to evaluate whether they appear natural and effective while communicating. Keywords: 
Eye movement synthesis, facial animation, statistical modeling, saccades, HCI (Human-Computer Interface) 
 1 Introduction In describing for artists the role of eyes, [Faigin 1990] illustrated that downcast eyes, 
upraised eyes, eyes looking sideways, and even out-of-focus eyes are all suggestive of states of mind. 
Given that eyes are a window into the mind, we propose a new approach for synthesizing the kinematic 
characteristics of the eye: the spatio­temporal trajectories of saccadic eye movement. . . . Saccadic 
eye movements take their name from the French saccade , meaning jerk , and connoting a dis­ continuous, 
stepwise manner of movement as opposed to *{sooha,badler}@graphics.cis.upenn.edu jbadler@ski.org Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
a .uent, continuous one. The name very appropriately describes the phenomenological aspect of eye movement 
[Becker 1989]. We present a statistical eye movement model, which is based on both empirical studies 
of saccades and acquired eye movement data. There are three strong motivations for our work. First, for 
ani­mations containing close-up views of the face natural-looking eye movements are desirable. Second, 
traditionally it is hard for an an­imator to obtain accurate human eye movement data. Third, the animation 
community appears to have had no proposals for sac­cadic eye movement models that are easily adopted 
for speaking or listening faces. In recent years, there has been considerable interest in the con­struction 
and animation of human facial models. Applications in­clude such diverse areas as advertising, .lm production, 
game de­sign, teleconferencing, social agents and avatars, and virtual real­ity. To build a realistic 
face model, many factors including mod­eling of face geometry, simulation of facial muscle behavior, 
lip synchronization, and texture synthesis have been considered. Sev­eral early researchers [Parke 1974; 
Platt and Badler 1981; Waters 1987; Kalra et al. 1992] were among those who proposed various methods 
to simulate facial shape and muscle behavior. A number of investigators have recently emphasized building 
more realistic face models [Lee et al. 1995; Guenter et al. 1998; Pighin et al. 1998; Blanz and Vetter 
1999; Brand 1999]. [DeCarlo et al. 1998] sug­gested automatic methods of building varied geometric models 
of human faces. [Petajan 1999] and [Essa and Pentland 1995] used motion capture methods to replay prerecorded 
facial skin motion or behaviors. Research on faces has not focused on eye movement, although the eyes 
play an essential role as a major channel of non-verbal communicative behavior. Eyes help to regulate 
the .ow of conver­sation, signal the search for feedback during an interaction (gazing at the other person 
to see how she follows), look for information, express emotion (looking downward in case of sadness, 
embarrass­ment, or shame), or in.uence another person s behavior (staring at a person to show power) 
[Duncan 1974; Pelachaud et al. 1996]. Recently, proper consideration of eye movement is getting more 
attention among researchers. Cassell and colleagues [Cassell et al. 1994; Cassell et al. 1999; Cassell 
et al. 2001] in particular have ex­plored eye engagement during social interactions or discourse be­tween 
virtual agents. They discuss limited rules of eye engagement between animated participants in conversation. 
[Chopra-Khullar and Badler 1999] generated the appropriate attentional (eye gaze or looking) behavior 
for virtual characters existing or performing tasks in a changing environment (such as walk to the lamp 
post , mon­itor the traf.c light , reach for the box , etc. [Colburn et al. 2000] proposed behavioral 
models of eye gaze patterns for an avatar and investigated gaze behavior to see how observers reacted 
to whether an avatar was looking at or looking away from them. Vertegaal and colleagues [Vertegaal et 
al. 2000a; Vertegaal et al. 2000b; Vertegaal et al. 2001] presented experimental results which show that 
gaze directional cues of users could be used as a means of establishing who is talking to whom, and implemented 
probablistic eye gaze models for a multi-agent conversational system that uses eye input to determine 
whom each agent is listening or speaking to. Note that the above research focused on eye gaze patterns 
rather than how to generate detailed saccadic eye movements. In this paper, we propose a new approach 
for synthesizing the trajectory kinematics and statistical distribution of saccadic eye movements. We 
present an eye movement model which is based on both empirical studies of saccades and statistical models 
of eye­tracking data. The overview of our approach is as follows. First, we analyze a sequence of eye-tracking 
images in order to extract the spatio­temporal trajectory of the eye. Although the eye-tracking data 
can be directly replayed on a face model, its primary purpose is for deriving a statistical model of 
the saccades which occur. The eye­tracking video is further segmented and classi.ed into two modes, a 
talking mode and a listening mode, so that we can construct a sac­cade model for each. The models re.ect 
the dynamic1 character­istics of natural eye movement, which include saccade magnitude, direction, duration, 
velocity, and inter-saccadic interval. Based on the model, we synthesize a face character with more natural 
looking and believable eye movements. The remainder of this paper describes our approach in detail. In 
Section 2, we review pertinent research about saccadic eye move­ments and the role of gaze in communication. 
Section 3 presents an overview of our system architecture. Then, in Section 4, we in­troduce our statistical 
model based on the analysis of eye-tracking images. An eye saccade model is constructed for both talking 
and listening modes. Section 5 describes the architecture of our eye movement synthesis system. Subjective 
test results on the realism of our characters are presented in Section 6. Finally we give our conclusions 
and closing remarks.  2 Background 2.1 Saccades Saccades are rapid movements of both eyes from one 
gaze position to another [Leigh and Zee 1991]. They are the only eye movement that can be readily, consciously, 
and voluntarily executed by human subjects. Saccades must balance the con.icting demands of speed and 
accuracy, in order to minimize both time spent in transit and time spent making corrective movements. 
There are a few conventions used in the eye movement literature when describing saccades. The magnitude 
(also called the ampli­tude) of a saccade is the angle through which the eyeball rotates as it changes 
.xation from one position in the visual environment to an­other. Saccade direction de.nes the 2D axis 
of rotation, with zero degrees being to the right. This essentially describes the eye posi­tion in polar 
coordinates. For example, a saccade with magnitude 10 and direction 45 degrees is equivalent to the eyeball 
rotating 10 degrees in a rightward-upward direction. Saccade duration is the 1In this paper, dynamic 
refers to spatio-temporal. amount of time that the movement takes to execute, typically de­termined 
using a velocity threshold. The inter-saccadic interval is the amount of time which elapses between the 
termination of one saccade and the beginning of the next one. The metrics (spatio-temporal characteristics) 
of saccades have been well studied (for a review, see [Becker 1989]). A normal sac­cadic movement begins 
with an extremely high initial acceleration (as much as 30,000 deg/sec2) and terminates with almost as 
rapid a deceleration. Peak velocities for large saccades can be between 400 and 600 deg/sec. Saccades 
are accurate to within a few degrees. Saccadic reaction time is between 180 and 220 msec on average. 
Minimum intersaccadic interval ranges from 50 to 100 msec. The duration and velocity of a saccade are 
functions of its magni­tude. For saccades between 5 and 50 deg, the duration has a nearly constant rate 
of increase with magnitude and can be approximated by the linear function: D = D0 + d * A (1) where 
D and A are duration and amplitude of the eye movement, respectively. The slope, d, represents the increment 
in duration per degree. It ranges from 2 - 2.7 msec/deg. The intercept or catch-up time D0 typically 
ranges from 20 - 30 ms [Becker 1989]. Saccadic eye movements are often accompanied by a head rota­tion 
in the same direction (gaze saccades). Large gaze shifts always include a head rotation under natural 
conditions; in fact, naturally occurring saccades rarely have a magnitude greater than 15 degrees [Bahill 
et al. 1975]. Head and eye movements are synchronous [Bizzi 1972; Warabi 1977]. 2.2 Gaze in social interaction 
According to psychological studies [Kendon 1967; Duncan 1974; Argyle and Cook 1976], there are three 
functions of gaze: (1) sending social signals speakers use glances to emphasize words, phrases, or entire 
utterances while listeners use glances to signal continued attention or interest in a particular point 
of the speaker, or in the case of an averted gaze, lack of interest or disapproval; (2) open a channel 
to receive information a speaker will look up at the listener during pauses in speech to judge how their 
words are being received, and whether the listener wishes them to con­tinue while listeners continually 
monitor the facial expressions and direction of gaze of the speaker; and (3) regulate the .ow of conversation 
the speaker stops talking and looks at the listener, indicating that the speaker is .nished and conversational 
partici­pants can look at a listener to suggest that the listener be the next to speak. Aversion of gaze 
can signal that a person is thinking. For ex­ample, someone might look away when asked a question as 
they compose their response. Gaze is lowered during discussion of cog­nitively dif.cult topics. Gaze 
aversion is also more common while speaking as opposed to listening, especially at the beginning of ut­terances 
and when speech is hesitant. [Kendon 1967] found additional changes in gaze direction, such as the speaker 
looking away from the listener at the beginning of an utterance and towards the listener at the end. 
He also compared gaze during two kinds of pauses during speech: phrase boundaries, the pause between 
two grammatical phrases of speech, and hesita­tion pauses, delays that occur when the speaker is unsure 
of what to say next. The level of gaze rises at the beginning of a phrase boundary pause, similar to 
what occurs at the end of an utterance in order to collect feedback from the listener. Gaze level falls 
at a hesitation pause, which requires more thinking. Figure 2: Overall system architecture  3 Overview 
of system architecture Figure 2 depicts the overall system architecture and anima­tion procedure. First, 
the eye-tracking images are analyzed and a statistically based eye movement model is generated using 
MATLABTM(The MathWorks, Inc.) (Block 1). Meanwhile, for lip movements, eye blinks, and head rotations, 
we use the alterEGO face motion analysis system (Block 2), which was developed at face2faceTM, Inc2. 
The alterEGO system analyzes a series of im­ages from a consumer digital video camera and generates a 
MPEG­4 Face Animation Parameter (FAP) .le [Petajan 1999; N3055 1999; N3056 1999]. The FAP .le contains 
the values of lip move­ments, eye blinking, and head rotation [Petajan 1999]. Our prin­cipal contribution 
, the Eye Movement Synthesis System (EMSS) (Block 3) takes the FAP .le from the alterEGO system and adds 
values for eye movement parameters based on the statistical model. As an output, the EMSS produces a 
new FAP .le that contains eye­ball movement as well as the lip and head movement information. We constructed 
the Facial Animation System (Block 4) by adding eyeball movement capability to face2face s Animator plug-in 
for 3D Studio MaxTM(Autodesk, Inc.) In the next section, we will explain the analysis of the eye­tracking 
images and the building of the statistical eye model (Block 1). More detail about the EMSS (Block 3) 
will be presented in Section 5.  4 Analysis of eye tracking data 4.1 Images from the eye tracker We 
analyzed sequences of eye-tracking images in order to extract the dynamic characteristics of the eye 
movements. Eye move­ments were recorded using a light-weight eye-tracking visor (IS-CAN Inc.). The visor 
is worn like a baseball cap, and consists of a monocle and two miniature cameras. One camera views the 
vi­sual environment from the perspective of the participant s left eye and the other views a close-up 
image of the left eye. Only the eye image was recorded to a digital video tape using a DSR-30 dig­ital 
VCR (Sony Inc.). The ISCAN eye-tracking device measures the eye movement by comparing the corneal re.ection 
of the light source (typically infra-red) relative to the location of the pupil cen­ter. The position 
of the pupil center changes during rotation of the eye, while the corneal re.ection acts as a static 
reference point. 2http://www.f2f-inc.com  (a) (b) Figure 3: (a) Original eye image from the eyetracker 
(left), (b) Out­put of Canny Enhancer (right) distribution The sample video we used is 9 minutes long 
and contains infor­mal conversation between two people. The speaker was allowed to move her head freely 
while the video was taken. It was recorded at the rate of 30 frames per second. From the video clip, 
each image was extracted using Adobe PremiereTM(Adobe Inc.). Figure 3 (a) is an example frame showing 
two crosses, one for the pupil center and one for the corneal re.ection. We obtained the image (x,y) 
coordinates of the pupil center by using a pattern matching method. First, the features of each image 
are extracted by using the Canny operator [Canny 1986] with the default threshold grey level. Figure 
3(b) is a strength image output by the Canny enhancer. Second, to determine a pupil center the position 
histograms along the x and y axes are calculated. Then, the coordinates of the two center points with 
maximum correlation values are chosen. Finally, the sequences of (x,y) coordinates are smoothed by a 
median .lter. 4.2 Saccade statistics Figure 4(a) shows the distributions of the eye position in image 
coordinates. The red circle is the primary position (PP), where the speaker s eye is .xated upon the 
listener. Figure 4(b) is the same distribution plotted in 3 dimensions, with the z-axis representing 
the frequency of occurrence at that position. The peak in the 3-D plot corresponds to the primary position. 
The saccade magnitude is the rotation angle between its starting position S(xs, ys) and ending position 
E(xe,ye), which can be com­  Direction 0 deg 45 deg 90 deg 135 deg Percent(%) 15.54 6.46 17.69 7.44 
Direction 180 deg 225 deg 270 deg 315 deg Percent(%) 16.80 7.89 20.38 7.79 (a) (b) Figure 4: (a) Distribution 
of pupil centers, (b) 3-D view of same distribution (a) (b) Figure 5: (a) Frequency of occurrence of 
saccade magnitudes, (b) Cumulative percentage of magnitudes puted by (xe - xs)2 +(ye - ys)2 . arctan(d/r)= 
arctan( ), (2) r where d is the Euclidean distance traversed by the pupil center and r is the radius 
of the eyeball. The radius r is assumed to be one half of xmax, the width of the eye-tracker image (640 
pixels). The frequency of occurrence of a given saccade magnitude dur­ing the entire recording session 
is shown in Figure 5(a). Using a least mean squares criterion the distribution was .tted to the expo­nential 
function - A P = 15.7e6.9 , (3) where P is the percent chance to occur and A is the saccade magni­tude 
in degrees. The .tted function is used for choosing a saccade magnitude during synthesis. Figure 5 (b) 
shows the cumulative percentage of saccade magni­tudes, i.e. the probability that a given saccade will 
be smaller than magnitude x. Note that 90% of the time the saccade angles are less than 15 degrees, which 
is consistent with a previous study [Bahill et al. 1975]. Saccade directions are also obtained from the 
video. For sim­plicity, the directions are quantized into 8 evenly spaced bins with centers 45 degrees 
apart. The distribution of saccade directions is shown in Table 1. One interesting observation is that 
up-down and left-right movements happened more than twice as often as diago­nal movements. Also, Up-down 
movements happened equally as often as left-right movements. Saccade duration was measured using a velocity 
threshold of 40 deg/sec (1.33 deg/ f rame). The durations were then used to derive an instantaneous velocity 
curve for every saccade in the eye­track record. Sample curves are shown in Figure 6 (black dot­ted lines). 
The duration of each eye movement is normalized to Table 1: Distribution of saccade directions 6 frames. 
The normalized curves are used to .t a 6-dimensional polynomial (red solid line), Y = 0.13X6 - 3.16X5 
+ 31.5X4 - 155.87X3 + 394X2 - 465.95X + 200.36, (4) where X is frame 1 to 6 and Y is instantaneous velocity 
(degrees/ f rame). Figure 6: Instantaneous velocity functions of saccades The inter-saccadic interval 
is incorporated by de.ning two classes of gaze, mutual and away. Mutual gaze indicates that the subject 
s eye is in the primary position, while gaze away indicates that it is not. The duration that the subject 
remains in one of these two gaze states is analogous to the inter-saccadic interval. Fig­ures 7(a) and 
7(b) plot duration distributions for the two types of gaze while the subject was talking. They show the 
percent chance of remaining in a particular gaze mode (i.e., not making a saccade) as a function of elapsed 
time. The polynomial .tting function for mutual gaze duration is Y = 0.0003X2 - 0.18X + 32, (5) and 
for gaze away duration is Y = - 0.0034X3 + 0.23X2 - 6.7X + 79 (6) Note that the inter-saccadic interval 
tends to be much shorter when the eyes are not in the primary position. 4.3 Talking mode vs. Listening 
mode It can be observed that the characteristics of gaze differ depend­ing on whether a subject is talking 
or listening [Argyle and Cook 1976]. In order to model the statistical properties of saccades in talking 
and listening modes, the modes are used as a basis to fur­ther segment and classify the eye movement 
data. The segmentation and classi.cation were performed by a human operator inspecting the original eye-tracking 
video. Figures 8 (a) and (b) show the eye position distributions for talk­ing mode and listening mode, 
respectively. In talking mode, 92%  (a) (b) Figure 7: (a) Frequency of mutual gaze duration while talking, 
(b) Frequency of gaze away duration while talking (a) (b) Figure 8: Distribution of saccadic eye movements 
(a) in talking mode, (b) in listening mode of the time saccade magnitude is 25 degrees or less. In listening 
mode, over 98% of the time the magnitude is less than 25 degrees. The average magnitude is 15.64 ± 11.86 
degrees (mean ± stdev) for talking mode and 13.83 ± 8.88 degrees for listening mode. In general the magnitude 
distribution of listening mode is much nar­rower than that of talking mode, indicating that when the 
subject is speaking eye movements are more dynamic and active. This is also apparent while watching the 
eye-tracking video. Inter-saccadic intervals also differ between talking and listening modes. In talking 
mode, the average mutual gaze and gaze away du­rations are 93.9 ±94.9 frames and 27.8 ±24.0 frames, respectively. 
The complete distributions are shown in .gures 7(a) and 7(b). In listening mode, the average durations 
are 237.5 ± 47.1 frames for mutual gaze and 13.0 ± 7.1 frames for gaze away. These distri­butions were 
far more symmetric and could be suitably described with Gaussian functions. The longer mutual gaze times 
for listen­ing mode are consistent with earlier empirical results [Argyle and Cook 1976] in which the 
speaker was looking at the listener 41% of the time, while the listener was looking at the speaker 75% 
of the time.  5 Synthesis of natural eye movement A detailed block diagram of the statistical eye movement 
synthesis model is illustrated in Figure 9. The key components of the model are (1) Attention Monitor 
(AttMon), (2) Parameter Generator (ParGen), and (3) Saccade Synthesizer (SacSyn). AttMon monitors the 
system state and other necessary informa­tion, such as whether it is in talking or listening mode, whether 
the direction of the head rotation has changed, or whether the current frame has reached the mutual gaze 
duration or gaze away duration. By default, the synthesis state starts from the mutual gaze state.  
Figure 9: Block diagram of the statistical eye movement model The agent mode (talking or listening mode) 
can be provided by a human operator using linguistic information. The head rotation is monitored by the 
following procedure: 1: Initialize start and duration index for head rotation 2: for each frame 3: Determine 
direction and amplitude of head rotation for current frame by comparing with head rotation FAP values 
of current frame and previous frame 4: if direction has been changed in this frame 5: Calculate head 
rotation duration by searching backwards until reaching starting index value 6: Set starting index to 
the current frame number 7: Set duration index to 0 else 8 Increment duration index 9: end If the direction 
of head rotation has changed and its amplitude is bigger than an empirically chosen threshold then it 
invokes ParGen to initiate eye movement. Also, if the timer for either mutual gaze or gaze away duration 
is expired, it invokes ParGen. ParGen determines saccade magnitude, direction, duration and instantaneous 
velocity. It also decides the gaze away duration or mutual gaze duration depending on the current state. 
Then, it in­vokes the SacSyn, where appropriate saccade movement is synthe­sized and coded into the FAP 
values. Saccade magnitude is determined using the inverse of the ex­ponential .tting function shown in 
Figure 5(a). First, a random number between 0 and 15 is generated. The random number corre­sponds to 
the y-axis (percentage of frequency) in Figure 5(a). Then, the magnitude can be obtained from the inverse 
function of Equa­tion 3, A = - 6.9 * log(P/15.7) (7) where A is saccade magnitude in degrees and P is 
the random num­ber generated, i.e., the percentage of occurrence. This inverse map­ping using a random 
number guarantees the saccade magnitude has the same probability distribution as shown in Figure 5(a). 
Based on the analysis result in section 4.3, the maximum saccade mag­nitude is limited to 27.5 degrees 
for talking mode and 22.7 degrees for listening mode.3 Saccade direction is determined by two criteria. 
If the head rota­tion is larger than a threshold, the saccade direction follows the head rotation. Otherwise, 
the direction is determined based on the dis­tribution shown in Table 1. A uniformly distributed random 
num­ber between 0 to 100 is generated and 8 non-uniform intervals are assigned to the respective directions. 
That is, a random number be­tween 0 to 15.54 is assigned to the direction 0 deg (right), a number between 
15.54 to 22.00 to the direction 45 deg (up-right), and so on. Thus 15.54% of the time a pure rightward 
saccade will occur, and 6.46% of the time a up-rightward saccade will be generated. Given a saccade magnitude 
A, the duration is calculated using Equation 1 with values d = 2.4 msec/deg and D0 = 25 msec. The velocity 
of the saccade is then determined using the .tted instanta­neous velocity curve (Equation 4.) Given the 
saccade duration D in frames, the instantaneous velocity model is resampled at D times the original sample 
rate (1/6). The resulting velocity follows the shape of the .tted curve with the desired duration D. 
In talking mode, the mutual gaze duration and gaze away dura­tion are determined similarly to the other 
parameters, using inverses of the polynomial .tting functions (equations 5 and 6). Using the random numbers 
generated for the percentage range, correspond­ing durations are calculated by root solving the .tting 
functions. The resulting durations have the same probability distributions. In listening mode, inter-saccadic 
intervals are obtained using Gaus­sian random numbers with the duration values given in section 4.3: 
237.5 ±47.1 frames for mutual gaze and 13.0 ±7.1 frames for gaze away. The SacSyn collects all synthesis 
parameters obtained above and calculates the sequence of the coordinates of the eye centers. The coordinate 
values for eye movements are then translated into the FAP values for the MPEG4 standard [N3055 1999; 
N3056 1999]. For facial animation, we merge the eye movement FAP values with the parameters for lip movement, 
head movement, and eye blinking provided by the alterEGO system. Each frame is rendered in the 3D StudioMax 
environment. After synthesizing a saccade movement, the SacSyn sets the synthesis state to either gaze 
away state or mu­tual gaze state. Again, the AttMon checks the head movement, internal mode of the agent, 
and the timer for gaze away duration. When a new eye movement has to be synthesized, the ParGen is invoked 
in order to determine the next target position. Depending on the next target position, the state either 
stays at the gaze away state or goes back to the mutual gaze state. We generate facial animation using 
the face2face Animation Plug-In by applying FAP values to the face model in 3D StudioMax. We added the 
eye animation capability to the Plug-In. In addition to applying the saccade data from the FAP .le, our 
modi.ed plug-in incorporates the vestibulo-ocular re.ex (VOR). The VOR stabilizes gaze during head movements 
(as long as they are not gaze saccades) by causing the eyes to counter-roll in the opposite direction 
[Leigh and Zee 1991].  6 Results In order to compare the proposed saccade model to simpler tech­niques, 
we synthesized eye movements on our face model using three different methods. In the .rst (Type I), the 
subject does not have any saccadic movements. The eyeballs remain .xated on the camera. In the second 
(Type II), the eye movement is random. The saccade magnitude, direction and inter-saccadic interval are 
chosen by random number generators. In the third (Type III), the eye move­ments are sampled from our 
estimated distributions. The statisti­cal model re.ects the dynamic characteristics of natural eye move­ 
3The maximum magnitude thresholds are determined by the average magnitude plus one standard deviation 
for each mode. Questions p-values Type I vs. Type III Type II vs. Type III Overall 0.0000 0.0000 Q1 0.1321 
0.0588 Q2 0.1127 0.0006 Q3 0.0037 0.0029 Q4 0.0000 0.1310 Table 2: Results of Newman-Keuls test ments. 
Also, the model eye movements are synchronized with head movements and speech acts. Figure 1 shows several 
samples of the output images. We conducted a subjective test to evaluate the three types of eye movement. 
The three characters (Type I, II, III) were presented in random order to 12 subjects. The subjects were 
asked the following questions: Q1: Did the character on the screen appear interested in (5) or indifferent 
(1) to you?  Q2: Did the character appear engaged (5) or distracted (1) during the conversation?  Q3: 
Did the personality of the character look friendly (5) or not (1)?  Q4: Did the face of the character 
look lively (5) or deadpan (1)?  Q5: In general, how would you describe the character?  Note that 
higher scores correspond to more positive attributes in a conversational partner. Most of the subjects 
were naive in the sense that they were not familiar with computer graphics or neurobiology, and none 
of the subjects were authors of the study. For questions 1 to 4, the score was graded on a scale of 5 
to 1. Figure 10 summarizes the average score and standard devia­tion for the .rst four questions. The 
scores were analyzed using the STATISTICATM software package (StatSoft, Inc.). A Kruskal-Wallis ANOVA 
indicated that the three character types had signif­icantly different scores (p = 0.0000). To further 
quantify the dif­ferences between the characters, a standard 2-way ANOVA and Newman-Keuls post-hoc test 
were performed (Table 2). The in­teractions between the three models and four questions were tested while 
the subjects were pooled. Overall, the scores for type III char­acters were signi.cantly higher than 
either type I or type II charac­ters, while type I and II characters scored the same (not shown in table; 
p = 0.7178). The results for individual questions agree well with intuition. Type I characters (staring 
eyes) were not rated as sig­ni.cantly less interested in (Q1) or engaged with (Q2) the subjects than 
type III characters (normal eyes). Type II characters (erratic eyes) were not signi.cantly less lively 
(Q4) than type III charac­ters. They were also not signi.canly less interested than type III characters, 
though only marginally. In all other cases type III char­acters scored signi.cantly higher than the others. 
According to the general remarks in Q5, the subjects tended to believe the following: 1. Type I looked 
interested in the viewers, but it seemed to have a cautious, demanding, sleepy-looking (not lively) and 
cold personality. 2. Type II s eye movement was unnatural, jittery and distracted, but more lively and 
friendly. No head-eye synchronization was jarring. Resulting in a character who looked unstable and schizophrenic. 
 3. Type III had better eye movement, which was purposeful, nat­ural and realistic. The character looked 
more friendly and out­going.  [Argyle and Dean 1965] found that very high amounts of eye contact (i.e. 
direct gaze)may be perceived as too intimate for that particular encounter and hence may be less favorably 
rated. Our .ndings using characters with no saccadic eye movement are con­sistent with those conclusions. 
In summary, 10 out of 12 subjects chose Type III as the most natural, while two subjects had no pref­erence. 
 7 Conclusions In this paper, we presented eye saccade models based on the sta­tistical analysis of 
eye-tracking video. The eye-tracking video is segmented and classi.ed into two modes: talking mode and 
lis­tening mode. A saccade model is constructed for each of the two modes. The models re.ect the dynamic 
characteristics of natural eye movement, which include saccade magnitude, duration, veloc­ity, and inter-saccadic 
interval. We synthesized a face character using 3 different types of eye movements: stationary, random, 
and model-based. We conducted a subjective test using these movements. The test results show that the 
model generated eyeball movement that made the face character look more natural, friendly and outgoing. 
No eye movement gave the character a lifeless quality, while random eye movement gave the character an 
unstable quality. Another way to generate eye movements on a face model is to replay the eye tracking 
data previously recorded from a subject. Preliminary tests using this method indicated that the replayed 
eye movements looked natural by themselves, but were often not syn­chronized with speech or head movement. 
An additional drawback to this method is that it requires new data to be collected every time a novel 
eye-track record is desired. Once the distributions for the statistical model are derived, any number 
of unique eye movement sequences can be animated. The eye movement video used to construct the saccade 
statistics was limited to a frame rate of 30 Hz, which can lead to aliasing. In practice this is not 
a signi.cant problem, best illustrated by an example. Consider a small saccade of 2 degrees, which will 
have a duration of around 30 msec (Equation 1). To completely lose all information on the dynamics of 
this saccade, it must begin within three msec of the .rst frame capture, so that it is completely .nished 
by the second frame capture 33 msec later. This can be expected to happen around 10 % of the time (3 
/ 33). From Figure 5 (b), it can be seen that saccades this small comprise about 20 % of all saccades 
in the record, so only around 2 % of all saccades should be severely aliased. This small percentage has 
little effect on the instantaneous velocity function of Figure 6. Since saccade starting and ending positions 
are still recoverable from the video, magnitude and direction are much less susceptible to aliasing problems. 
 A more important consideration is the handling of the VOR dur­ing the eye movement recording. A change 
in eye position that is due to a saccade (e.g., up and to the left) must be distinguishable from a change 
that is due to head rotation (e.g., down and to the right). One solution is to include a sensor which 
monitors head position. When head position is added to eye position, the resul­tant gaze position is 
without the effects of the VOR. However, this introduces the new problem that eye and head movements 
are no longer independent. An alternate approach is to differentiate the eye position data, and threshold 
the resultant eye velocity (e.g, at 80 deg/sec) to screen out non-saccadic movements. Although this can 
be performed post-hoc, it is not robust at low sampling rates. For example, revisiting the above example, 
a 2 degree position change that occurred between two frames may have taken 33 msec (velocity = 60 deg/sec) 
or 3 msec (velocity = 670 deg/sec). In this study, head movements in subjects occurred infrequently enough 
that they were unlikely to severely contaminate the saccade data. However, in future work they can be 
better accounted for, using improved equipment, more elaborate analysis routines, or a combi­nation of 
the two. There are a number of enhancements to our system which could be implemented in the future. During 
the analysis of eye-tracking images, we noticed a high correlation between the eyes and the eyelid movement 
which could be incorporated. Only the cognitive states of talking and listening were considered. The 
number of states could be expanded to model gaze patterns during other phases of speech, such as the 
tendency to look away at the beginning of an utterance, look toward the listener at the end, or to look 
up when thinking of what to say next. A scan-path model could be added, using not only the tracking of 
close-up eye images but also the visual environment images taken from the perspective of the participant 
s eye. Additional subjects could be added to the pool of saccade data, reducing the likelihood of idiosyncracies 
in the statistical model. Other modeling procedures themselves could be investigated, such as neural 
networks or Markov models. Improvements such as these will further increase the realism of a conversational 
agent. 8 Acknowledgment We would like to thank Eric Petajan, Doug DeCarlo, and Ed Roney for their valuable 
comments. We thank face2face,inc for providing the face tracking software. A special thanks goes to Minkyu 
Lee for his endless discussion and support in making this work possi­ble. We greatly acknowledge John 
Trueswell for the eye tracking data and Andrew Weidenhammer for the face model and the subject data. 
Finally, gratitude is given to everybody in the University of Pennsylvania Graphics Lab, especially Jan 
Allbeck, Karen Carter, and Koji Ashida. This research is partially supported by Of.ce of Naval Research 
K-5-55043/3916-1552793, NSF IIS99-00297, and NSF-STC Cooperative Agreement number SBR-89-20230.  References 
ARGYLE, M., AND COOK, M. 1976. Gaze and Mutual Gaze. Cambridge University Press, London. ARGYLE, M., 
AND DEAN, J. 1965. Eye-contact, distance and af.liation. Sociometry, 28, 289 304. BAHILL, A., ANDLER, 
D., AND STARK, L. 1975. Most naturally occuring human saccades have magnitudes of 15 deg or less. In 
Investigative Ophthalmol., 468 469. BECKER, W. 1989. Metrics, chapter 2. In The Neurobiology of Saccadic 
Eye Movements, R H Wurtz and M E Goldberg (eds), 13 67. BEELER, G. W. 1965. Stochastic processes in the 
human eye move­ment control system. PhD thesis, California Institute of Technol­ogy, Pasadena, CA. BIZZI, 
E. 1972. Central programming and peripheral feedback during eye-head coordination in monkeys. In Bibl. 
Ophthal. 82, 220 232. BLANZ, V., AND VETTER, T. 1999. A morphable model for the synthesis of 3D faces. 
In Computer Graphics (SIGGRAPH 99 Proceedings), 75 84. BRAND, M. 1999. Voice puppetry. In Computer Graphics 
(SIG-GRAPH 99 Proceedings, 21 28. CANNY, J. 1986. A computational approach to edge detection. IEEE Transactions 
on Pattern Analysis and Machine Intelligence PAMI-8, 679 698. CASSELL, J., PELACHAUD, C., BADLER, N., 
STEEDMAN, M., ACHORN, B., BECHET, T., DOUVILLE, B., PREVOST, S., AND STONE, M. 1994. Animated conversation: 
Rule-based genera­tion of facial expression gesture and spoken intonation for mul­tiple converstaional 
agents. In Computer Graphics (SIGGRAPH 94 Proceedings), 413 420. CASSELL, J., TORRES, O., AND PREVOST, 
S. 1999. Turn taking vs. discourse structure: How best to model multimodal conver­sation. In In Machine 
Conversations, Y. Wilks (eds), 143 154. CASSELL, J., VILHJALMSSON, H., AND BICKMORE, T. 2001. BEAT:The 
Behavior Expression Animation Toolkit. In Com­puter Graphics (SIGGRAPH 01 Proceedings), 477 486. CHOPRA-KHULLAR, 
S., AND BADLER, N. 1999. Where to look? automating visual attending behaviors of virtual human charac­ters. 
In Autonomous Agents Conf. COLBURN, R., COHEN, M., AND DRUCKER, S. 2000. Avatar me­diated conversational 
interfaces. In Microsoft Technical Report. DECARLO, D., METAXAS, D., AND STONE, M. 1998. An anthro­pometric 
face model using variational techniques. In Computer Graphics (SIGGRAPH 98 Proceedings), 67 74. DUNCAN, 
S. 1974. Some signals and rules for taking speaking turns in conversations. Oxford University Press, 
New York. ESSA, I., AND PENTLAND, A. 1995. Facial expression recognition using a dynamic model and motion 
energy. In ICCV95, 360 367. FAIGIN, G. 1990. The artist s complete guide to facial expression. Watson-Guptill 
Publications, New York. GUENTER, B., GRIMM, C., AND WOOD, D. 1998. Making faces. In Computer Graphics 
(SIGGRAPH 98 Proceedings), 55 66. ISO/IEC JTC 1/SC 29/WG11 N3055. Text for CD 14496-1 Sys­tems MPEG-4 
Manual. 1999. ISO/IEC JTC 1/SC 29/WG11 N3056. Text for CD 14496-2 Sys­tems MPEG-4 Manual. 1999. KALRA, 
P., MANGILI, A., MAGNENAT-THALMANN, N., AND THALMANN, D. 1992. Simulation of muscle actions using ra­tional 
free form deformations. In Proceedings Eurographics 92 Computer Graphics Forum, Vol. 2, No. 3, 59 69. 
KENDON, A. 1967. Some functions of gaze direction in social interaction. Acta Psychologica 32, 1 25. 
LEE, Y., WATERS, K., AND TERZOPOULOS, D. 1995. Realis­tic modeling for facial animation. In Computer 
Graphics (SIG-GRAPH 95 Proceedings), 55 62. LEIGH, R., AND ZEE, D. 1991. The Neurology of Eye Movements, 
2 ed. FA Davis. PARKE, F. 1974. Parametrized Models for Human Faces. PhD thesis, University of Utah. 
PELACHAUD, C., BADLER, N., AND STEEDMAN, M. 1996. Gen­erating facial expressions for speech. Cognitive 
Science 20, 1, 1 46. PETAJAN, E. 1999. Very low bitrate face animation coding in MPEG-4. In Encyclopedia 
of Telecommunications, Volume 17, 209 231. PIGHIN, F., HECKER, J., LISCHINSKI, D., SZELISKI, R., AND 
SALESIN, D. 1998. Synthesizing realistic facial expressions from photographs. In Computer Graphics (SIGGRAPH 
98 Pro­ceedings), 75 84. PLATT, S., AND BADLER, N. 1981. Animating facial expressions. In Computer Graphics 
(SIGGRAPH 81 Proceedings), 279 288. VERTEGAAL, R., DER VEER, G. V., AND VONS, H. 2000. Ef­fects of gaze 
on multiparty mediated communication. In Pro­ceedings of Graphics Interface 2000, Morgan Kaufmann Pub­lishers, 
Montreal,Canada: Canadian Human-Computer Commu­nications Society, 95 102. VERTEGAAL, R., SLAGTER, R., 
DER VEER, G. V., AND NI-JHOLT, A. 2000. Why conversational agents should catch the eye. In Summary of 
ACM CHI 2000 Conference on Human Fac­tors in Computing Systems. VERTEGAAL, R., SLAGTER, R., DER VEER, 
G. V., AND NI-JHOLT, A. 2001. Eye gaze patterns in conversations: There is more to conversational agents 
than meets the eyes. In ACM CHI 2001 Conference on Human Factors in Computing Systems, 301 308. WARABI, 
T. 1977. The reaction time of eye-head coordination in man. In Neurosci. Lett. 6, 47 51. WATERS, K. 1987. 
A muscle model for animating three­dimensional facial expression. In Computer Graphics (SIG-GRAPH 87 
Proceedings), 17 24. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566630</article_id>
		<sort_key>645</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Physiological measures of presence in stressful virtual environments]]></title>
		<page_from>645</page_from>
		<page_to>652</page_to>
		<doi_number>10.1145/566570.566630</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566630</url>
		<abstract>
			<par><![CDATA[A common measure of the quality or effectiveness of a virtual environment (VE) is the mount of <i>presence</i> it evokes in users. Presence is often defined as the sense of <i>being there</i> in a VE. There has been much debate about the best way to measure presence, and presence researchers need, and have sought, a measure that is <b>reliable, valid, sensitive, and objective.</b>We hypothesized that to the degree that a VE seems real, it would evoke physiological responses similar to those evoked by the corresponding real environment, and that greater presence would evoke a greater response. To examine this, we conducted three experiments, the results of which support the use of physiological reaction as a reliable, valid, sensitive, and objective presence measure. The experiments compared participants' physiological reactions to a non-threatening virtual room and their reactions to a stressful virtual height situation. We found that change in heart rate satisfied our requirements for a measure of presence, change in skin conductance did to a lesser extent, and that change in skin temperature did not. Moreover, the results showed that inclusion of a passive haptic element in the VE significantly increased presence and that for presence evoked: 30FPS &gt; 20FPS &gt; 15FPS.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[frame rate]]></kw>
			<kw><![CDATA[haptics]]></kw>
			<kw><![CDATA[physiology]]></kw>
			<kw><![CDATA[presence]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Evaluation/methodology</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003122</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->HCI design and evaluation methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39037855</person_id>
				<author_profile_id><![CDATA[81100327214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Meehan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382422</person_id>
				<author_profile_id><![CDATA[81100140336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Insko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P193375</person_id>
				<author_profile_id><![CDATA[81100122627]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14037393</person_id>
				<author_profile_id><![CDATA[81100077256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Frederick]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of North Carolina, Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Abelson, J. L. and G. C. Curtis (1989). Cardiac and neuroendocrine responses to exposure therapy in height phobics. Behavior Research and Therapy, 27(5): 561-567.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Andreassi, J. L. (1995). Psychophysiology: Human behavior and physiological response. Hillsdale, N. J., Lawrence Erlbaum Associates.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>216189</ref_obj_id>
				<ref_obj_pid>216164</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Barfield, W., T. Sheridan, D. Zeltzer and M. Slater (1995). Presence and performance within virtual environments. In W. Barfield and T. Furness, Eds., Virtual environments and advanced interface design. London, Oxford University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cowings, P., S. Jensen, D. Bergner and W. Toscano (2001). A lightweight ambulatory physiological monitoring system. NASA Ames, California.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dillon, C., E. Keogh, J. Freeman and J. Davidoff (2001). Presence: Is your heart in it? 4th Int. Wkshp. on Presence, Philadelphia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ellis, S. R. (1996). Presence of mind: A reaction to Thomas Sheridan's "Further musings on the psychophysics of presence". Presence: Teleoperators and Virtual Environments, 5(2): 247-259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Emmelkamp, P. and M. Felten (1985). The process of exposure in vivo: cognitive and physiological changes during treatment of acrophobia. Behavior Research and Therapy, 23(2): 219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Freeman, J., S. E. Avons, D. Pearson, D. Harrison and N. Lodge (1998). Behavioral realism as a metric of presence. 1st Int. Wkshp. on Presence.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Guyton, A. C. (1986). Basic characteristics of the sympathetic and parasympathetic function. In Textbook of Medical Physiology, 688-697. Philadelphia, W. B. Saunders Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>196589</ref_obj_id>
				<ref_obj_pid>196564</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Heeter, C. (1992). Being there: The subjective experience of presence. Presence: Teleoperators and Virtual Environments, 1: 262-271.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[IJsselsteijn, W. A. and H. d. Ridder (1998). Measuring temporal variations in presence. 1st Int. Wkshp. on Presence.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>933178</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[B. Insko (2001). Passive haptics significantly enhance virtual environments, Doctoral Dissertation. Computer Science. University of North Carolina, Chapel Hill, NC, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>59564</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kleinbaum, D., L. Kupper, K. Muller and A. Nizam (1998). Applied regression analysis and other multivariate methods.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lipsey, M. W. (1998). Design sensitivity: Statistical power for applied experimental research. In L. Brickman and D. J. Rog, Eds., Handbook of applied social research methods, 39-68. Thousand Oaks, California, Sage Publications, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lombard, M. and T. Ditton (1997). At the heart of it all: The concept of presence. Journal of Computer Mediated Communication, 3(2).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[McMurray, D. R. (1999). Director of Applied Physiology lab, University of North Carolina. Personal Communication.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>933179</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M. Meehan (2001). Physiological reaction as an objective measure of presence in virtual environments. Doctoral Dissertation. Computer Science. University of North Carolina, Chapel Hill, NC, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Regenbrecht, H. T. and T. W. Schubert (1997). Measuring presence in virtual environments. In Proc. of Human Computer Interface International, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>521260</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SAS (1990). SAS/STAT User's Guide, Version 6, Fourth Edition. Cary, NC, USA, SAS Institute Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Schubert, T., F. Friedmann and H. Regenbrecht (1999). Embodied presence in virtual environments. In R. Paton and I. Neilson, Eds., Visual Representations and Interpretations. London, Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sheridan, T. B. (1996). Further musings on the psychophysics of presence. Presence: Teleoperators and Virtual Environments, 5(2): 241-246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Singleton, R. A., B. C. Straits and M. M. Straits (1993). Approaches to Social Research. New York, Oxford University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Slater, M., M. Usoh and A. Steed (1994). Depth of presence in virtual environments. Presence: Teleoperators and Virtual Environments, 3(2): 130-144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>210084</ref_obj_id>
				<ref_obj_pid>210079</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Slater, M., M. Usoh and A. Steed (1995). Taking steps: The influence of a walking technique on presence in virtual reality. ACM Transactions on Computer Human Interaction (TOCHI), 2(3): 201-219.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246846</ref_obj_id>
				<ref_obj_pid>1246838</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Slater, M. (1999). Measuring Presence: A Response to the Witmer and Singer Presence Questionnaire. Presence: Teleoperators and Virtual Environments, 8(5): 560-565.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Slonim, N. B., Ed. (1974). Environmental Physiology. Saint Louis. The C. V. Mosby Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Sutherland, S. (1996). The international dictionary of psychology. New York, The Crossroads Publishing Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311589</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Usoh, M., K. Arthur, M. Whitton, R. Bastos, A. Steed, M. Slater and F. Brooks (1999). Walking &gt; walking-in-place &gt; flying in virtual environments. In Proc. of ACM SIGGRAPH 99. ACM Press/ ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Weiderhold, B. K., R. Gervirtz and M. D. Wiederhold (1998). Fear of flying: A case report using virtual reality therapy with physiological monitoring. CyberPsychology and Behavior, 1(2): 97-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246762</ref_obj_id>
				<ref_obj_pid>1246761</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Witmer, B. G. and M. J. Singer (1998). Measuring presence in virtual environments: A presence questionnaire. Presence: Teleoperators and Virtual Environments, 7(3): 225-240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physiological Measures of Presence in Stressful Virtual Environments Michael Meehan Brent Insko Mary 
Whitton Frederick P. Brooks, Jr. Computer Science Department, University of North Carolina, Chapel Hill 
{meehan, insko, whitton, brooks}@cs.unc.edu Abstract A common measure of the quality or effectiveness 
of a virtual environment (VE) is the amount of presence it evokes in users. Presence is often defined 
as the sense of being there in a VE. There has been much debate about the best way to measure presence, 
and presence researchers need, and have sought, a measure that is reliable, valid, sensitive, and objective. 
We hypothesized that to the degree that a VE seems real, it would evoke physiological responses similar 
to those evoked by the corresponding real environment, and that greater presence would evoke a greater 
response. To examine this, we conducted three experiments, the results of which support the use of physiological 
reaction as a reliable, valid, sensitive, and objective presence measure. The experiments compared participants 
physiological reactions to a non-threatening virtual room and their reactions to a stressful virtual 
height situation. We found that change in heart rate satisfied our requirements for a measure of presence, 
change in skin conductance did to a lesser extent, and that change in skin temperature did not. Moreover, 
the results showed that inclusion of a passive haptic element in the VE significantly increased presence 
and that for presence evoked: 30FPS > 20FPS > 15FPS. Categories: I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism -Virtual Reality. H.5.2 [Information Interfaces and Presentation]: User Interfaces 
 Evaluation/ Methodology. Keywords: Presence, Physiology, Haptics, Frame Rate. 1. Introduction 1.1. 
Presence and virtual environments Virtual environments (VEs) are the most sophisticated human­computer 
interfaces yet developed. The effectiveness of a VE might be defined in terms of enhancement of task 
performance, effectiveness for training, improvement of data comprehension, etc. A common metric of VE 
quality is the degree to which the VE creates in the user the subjective illusion of presence a sense 
of being in the virtual, as opposed to the real, environment. Since presence is a subjective condition, 
it has most commonly been measured by self-reporting, either during the VE experience or immediately 
afterwards by questionnaires. There has been vigorous debate as to how to best measure presence [Barfield 
et al. 1995; Ellis 1996; Freeman et al. 1998; IJsselsteijn and de Ridder 1998; Lombard and Ditton 1997; 
Regenbrecht and Schubert 1997; Schubert et al. 1999; Sheridan 1996; Slater 1999; Witmer and Singer 1998]. 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 In order to study a VE s effectiveness in evoking presence, researchers need a well-designed and 
verified measure of the phenomena. This paper reports our evaluation of three physiological measures 
 heart rate, skin conductance, and skin temperature as alternate operational measures of presence in 
stressful VEs. Since the concept and idea of measuring presence are heavily debated, finding a measure 
that could find wide acceptance would be ideal. In that hope, we investigated the reliability, validity, 
sensitivity, and objectivity of each physiological measure. Figure 1. Side view of the virtual environment. 
Subjects start in the Training Room and later enter the Pit Room. 1.2. Physiological Reaction as a Surrogate 
Measure of Presence As VE system and technology designers, we have sought for a presence measure that 
is Reliable produces repeatable results, both from trial to trial on the same subject and across subjects, 
Valid measures subjective presence, or at least correlates with well-established subjective presence 
measures, Sensitive discriminates among multiple levels of presence, and Objective is well shielded 
from both subject and experimenter bias. We hypothesize that to the degree that a VE seems real, it will 
evoke physiological responses similar to those evoked by the corresponding real environment, and that 
greater presence will evoke a greater response. If so, these responses can serve as objective surrogate 
measures of subjective presence. Of the three physiological measures in our studies, Change in Heart 
Rate performs best. It consistently differentiates among conditions with more sensitivity and more statistical 
power than the other physiological measures, and more than most of the self­reported measures. It also 
best correlates with the reported measures. Figure 2. View of the 20 pit from the wooden ledge. Change 
in Skin Temperature is less sensitive, less powerful, and slower responding than Change in Heart Rate, 
although its response curves are similar. It also correlates with reported measures. Our results and 
the literature on skin temperature reactions suggest that Change in Skin Temperature would differentiate 
among conditions better if the exposures to the stimulus were at least 2 minutes [McMurray 1999; Slonim 
1974]. Ours averaged 1.5 minutes in each experiment. Change in Skin Conductance Level yielded significant 
differentiation in some experiments but was not so consistent as Change in Heart Rate. More investigation 
is needed to establish whether it can reliably differentiate among multiple levels of presence. Since 
Change in Heart Rate best followed the hypotheses, the remainder of this paper will treat chiefly the 
results for it. For a full account of all measures, please see [Meehan 2001].  1.3. Our Environment 
and Measures We use a derivative of the compelling VE reported by Usoh et al. [1999]. Figure 1 shows 
the environment: a Training Room, quite ordinary, and an adjacent Pit Room, with an unguarded hole in 
the floor leading to a room 20 ft. below. On the upper level the Pit Room is bordered with a 2-foot wide 
walkway. The 18x32 foot, 2­room virtual space fits entirely within the working space of our lab s wide-area 
ceiling tracker. Users, equipped with a head­tracked stereoscopic head-mounted display, practice walking 
about and picking up and placing objects in the Training Room. Then they are told to carry an object 
into the next room and place it at a designated spot. The door opens, and they walk through it to an 
unexpected hazard, a virtual drop of 20 ft. if they move off the walkway. Below is a furnished Living 
Room (Figure 2). Users report feeling frightened. Some report vertigo. Some will not walk out on the 
ledge and ask to stop the experiment or demo at the doorway. A few boldly walk out over the hole, as 
if there were a solid glass floor. For most of us, doing that, if we can, requires conscious mustering 
of will. This environment, with its ability to elicit a fear reaction in users, enables investigation 
of physiological reaction as a measure of presence. If so strong a stress-inducing VE does not produce 
significant physiological reactions, a less stressful VE won t. This investigation is a first step. Follow-on 
research should investigate whether less stressful environments also elicit statistically significant 
physiological reactions. This remainder section will discuss the physiological measures we tested and 
the reported measures we used to evaluate validity. 1.3.1. The Physiological Measures As stated above, 
we investigated three physiological metrics that measure stress in real environments [Andreassi 1995; 
Guyton 1986; Weiderhold et al. 1998]: Change in heart rate (. Heart Rate). The heart beats faster in 
stress. Change in skin conductance (. Skin Conductance Level). The skin of the palm sweats more in stress, 
independently of temperature, so its conductance rises. Change in skin temperature (. Skin Temperature). 
Circulation slows in the extremities in stress, causing skin temperature to drop. Each of these measures 
was constructed to increase when the physiological reaction to the Pit Room was greater. . Heart Rate 
= mean HR Pit Room mean HR Training Room. . Skin Conductance = mean SC Pit Room mean SC Training Room 
. Skin Temperature = mean ST Training Room mean ST Pit Room We first measured heart rate with a convenient 
finger-mounted blood-pulse plethysmograph, but the noise generated by the sensor moving on the finger 
made the signal unstable and unusable. We then went to more cumbersome chest-attached three-electrode 
electrocardiography (ECG). This gave a good signal. Skin conductivity and skin temperature were successfully 
measured on the fingers. Once connected, users reported forgetting about the physiological sensors they 
did not cause breaks in presence during the experiments. Figure 3 shows a subject wearing the physiological 
monitoring equipment. 1.3.2. The Reported Measures Reported Presence. We used the University College 
London (UCL) questionnaire [Slater et al. 1995; Usoh et al. 1999]. The UCL questionnaire contains seven 
questions that measure presence (Reported Presence), three questions that measure behavioral presence 
(Reported Behavioral Presence) does the user act as if in a similar real environment and three that 
measure ease of locomotion (Ease of Locomotion). Responses for each question are on a scale of 1 to 7. 
Reported Ease of Locomotion was administered for consistency with earlier experiments, but we do not 
report on it in this paper. Even though each question is rated on a scale of 1-7, Slater et al. use it 
only to yield a High-Presence/ Low-Presence result. A judgment must be made as to the high-low threshold. 
Slater et al. have investigated the use of 6 and 7 as high responses [=6] and the use of 5, 6, and 7 
as high responses [=5] as well as other constructions: addition of raw scores, and a combination based 
on principal-components analysis. They have found that [=6] better followed conditions [Slater et al. 
1994], and, therefore they chose that construction. We found that the [=5] construction better follows 
presence conditions but has lower correlations with our physiological measures. Therefore, in order to 
best follow the original intention of the measures, irrespective of the lower correlations with our measures, 
we choose the [=5] construction. On the study for which data is published, Slater s subjects rarely (<10%) 
reported 5 values; over 25% of our subjects did. One explanation for this difference in subjects reporting 
may be that university students today expect more technically of a VE than they did several years ago 
and, therefore, are more likely to report lower values (5s) even for the most presence-inducing VEs. 
Reported Behavioral Presence. Three questions asked subjects if they behaved as if present when in the 
VE. The count of high scores [=5] on these questions made up the Reported Behavioral Presence measure. 
 Frame Rate Presence in VEs Does presence decrease with exposures? Passive Haptics increase presence? 
Higher Frame Rate increases presence? Reliability of Measures Are repeated measures highly correlated? 
Regardless of condition, will the Pit Room evoke similar physiological reactions on every exposure? Validity 
Do results correl ate with reported measures? Sensitivity of Measures Do measures detect any effect? 
Do measures distinguish between 2 conditions? Do measures distinguish among 4 conditions? Table 1. Questions 
investigated in each study.  1.4. Methods and Procedures 1.4.1. Experimental procedures. We conducted 
three experiments: Effects of Multiple Exposures on Presence (Multiple Exposures), Effects of Passive 
Haptics on Presence (Passive Haptics), and Effects of Frame Rate on Presence (Frame Rate). Each of the 
three studies investigated some interesting aspect of VEs and the properties of the physiological measures 
themselves. Table 1 summarizes all the questions studied. For all studies we excluded subjects who had 
previously experienced VEs more than three times. The experiments were also limited to subjects who were 
ambulatory, could use stereopsis for depth perception, had no history of epilepsy or seizure, were not 
overly prone to motion sickness, were in their usual state of good physical fitness at the time of the 
experiment, and were comfortable with the equipment. Multiple Exposures: 10 subjects (average age 24.4; 
s = 8.2; 7 female, 3 male) were trained to pick up books and move about in the Training Room at which 
time a physiological baseline was taken. Subjects then carried a virtual book from the Training Room 
and placed it on a virtual chair on the far side of the Pit Room. After that, they returned to the Training 
Room. The subjects performed this task three times per day on four separate days. We investigated whether 
the presence-evoking power of a VE declines with multiple exposures. Heart Rate was not successfully 
measured in this study due to problems with the sensor.  Figure 4. Subject in slippers with toes over 
1.5-inch ledge. Passive Haptics: 52 subjects (average age 21.4; s = 4.3; 16 female, 36 male) reported 
on two days. Subjects experienced the VE with the 1.5-inch wooden ledge on one of their two days. The 
1.5-inch height was selected so that the edge-probing foot did not normally contact the real laboratory 
floor where the virtual pit was seen. On their other day, subjects experienced the VE without the ledge. 
Subjects were counterbalanced as to the order of presentation of the physical ledge. Subjects performed 
all exposures to the VE wearing only thin sock-like slippers (Figure 4). The task was the same as in 
the Multiple Exposures study except subjects were instructed to walk to the edge of the wooden platform, 
place their toes over the edge, and count to ten before they proceeded to the chair on the far side of 
the room to drop the book. We investigated whether the 1.5-inch wooden ledge increased the presence-evoking 
power of the VE. Frame Rate: 33 participants (average age 22.3; s = 3.6; 8 female, 25 male) entered the 
VE four times on one day and were presented the same VE with a different frame rate each time. The four 
frame rates were 10, 15, 20, and 30 frames-per-second (FPS). Subjects were counterbalanced as to the 
order of presentation of the four First Exposure Only (Between Subjects) N < .001 99% 112 .002 100% 
92.3 . mSiemens 2.9 . mSiemens < .001 77% 94 .015 100% 7  0.6 . oF 1.2 . oF < .001 89% 92 < .001 85% 
466.3 . BPM  6.2 . BPM < .001 100% 100 < .001 100% 504.7 . mSiemens  4.8 . mSiemens < .001 90% 98 
< .001 94% 49  1.1 . oF 1.1 . oF < .001 91% 132 < .001 91% 336. 3 . BPM 8.1 . BPM < .001 87% 132 < 
.001 97% 332.0 . mSiemens  2.6 . mSiemens < .001 100% 132 < .001 100% 33  0.8 . oF 1.0 . oF Table 
2. Summary of means and significance of differences (.) between Training Room and Pit Room. The mean, 
P-value for the one-sample t-test, percentage of times the measure was > 0, and number of samples are 
shown. The left side shows the means and significances of all exposures. The right side shows these for 
only subjects first exposures. The greater mean is shown in bold. frame rates. Subjects were trained 
to pick up and drop blocks in the Training Room and then carried a red block to the Pit Room and dropped 
it on a red X-target on the floor of the Living Room, a procedural improvement that forced subjects to 
look down into the pit. They then plucked from the air two other colored blocks floating in the Pit Room 
and dropped each on the correspondingly­colored Xs on the floor of the Living Room. The X-targets and 
the green and blue blocks are visible in Figures 1 and 2. In this study, we investigated the effect of 
several different frame rates on presence and hypothesized that the higher the frame rate, the greater 
the presence evoked. In all three studies, the amount of physical activity (walking, manipulating objects) 
was approximately balanced between the Pit and Training Rooms. This lessened any difference between the 
two rooms in physiological reaction due to physical activity. 1.4.2. Statistics In this paper, we define 
statistical significance at the 5% level, i.e. P < 0.050. Findings significant at the 5% level are discussed 
as demonstrated or shown . To find the best statistical model for each measure, we used Stepwise Selection 
and Elimination as described by Kleinbaum et al. [1998]. As they suggest, to account better (statistically) 
for variation in the dependent variable (e.g., .Heart Rate), we included all variables in the statistical 
models that were significant at the P < 0.100 level. The analysis of differences in physiological reaction 
between the Pit Room and the Training Room for all studies (Table 2) was performed with a One-Sample 
T-Test. The correlations among measures were performed using the Bivariate Pearson Correlation. We analyzed 
order effects and the effects on presence of passive haptics and frame rate with the Univariate General 
Linear Model, using the repeated measure technique described in the SAS 6.0 Manual [SAS 1990]. This technique 
allows one to investigate the effect of the condition while taking into account inter-subject variation, 
order effects, and the effects of factors that change from exposure to exposure such as loss of balance 
on the 1.5-inch ledge. Section 2 details our evaluation of physiological measures as a surrogate for 
presence. In Section 3, we analyze physiological reactions as between-subject measures. In Section 4, 
we summarize the results as they pertain to interesting aspects of VEs.  2. Physiological measures 
of presence In this section, we discuss the reliability, validity, sensitivity, and objectivity of the 
physiological measures. 2.1. Reliability Reliability is the extent to which the same test applied on 
different occasions yields the same result [Sutherland 1996]. Specifically, we wanted to know whether 
the virtual environment would consistently evoke similar physiological reactions as the subject entered 
and remained in the Pit Room on several occasions. Inconsistency could manifest itself as either a systematic 
increase or decrease in reactions or in uncorrelated measures for repeated exposure to the same VE. In 
the Multiple Exposures study the condition was the same each time, so this was our purest measure of 
reliability. We also hypothesized that in the Passive Haptics and Frame Rate studies, regardless of condition, 
that the Pit Room would also evoke similar physiological reactions on every exposure. We hypothesized 
that simply being exposed to the Pit Room would cause a greater physiological reaction than the difference 
between high and low presence conditions. Therefore, all three studies provide information on reliability. 
As we hypothesized, the environment consistently evoked physiological reactions over multiple exposures 
to the Pit Room. When analyzing the data from all exposures, we found there were significant physiological 
reactions to the Pit Room: heart rate and skin conductance were significantly higher and skin temperature 
was significantly lower in the Pit Room in all three studies. Heart rate was higher in the Pit Room for 
90% of the exposures to the VE, skin conductance was higher for nearly 95%, and skin temperature was 
lower for 90%. Table 2 shows the mean difference, t-test, percentage of occurrences where the measure 
was above zero, and the total count for each physiological measure for each study. It shows results both 
for all exposures taken together, which is the approach discussed for most of the paper, and for analysis 
of the first exposure only, which we discuss in Section 4. We also wanted to know whether the physiological 
reactions to the environment would diminish over multiple exposures. Since our hypotheses relied on presence 
in the VE evoking a stress reaction over multiple exposures (2-12 exposures), we wanted to know whether 
physiological reactions to the VE would drop to zero or become unusably small due to habituation. In 
fact, .Skin Temperature, Reported Presence, Reported Behavioral Presence, and .Heart Rate each decreased 
with multiple exposures in every study (although this effect was not always statistically significant), 
and . Skin Conductance decreased in all but one study. None decreased to zero, though, even after twelve 
exposures to the VE. Table 3 shows the significant order effects. A decrease in physiological reaction 
over multiple exposures would not necessarily weaken validity, since the literature shows that habituation 
diminishes the stress reactions to real heights and other stressors [Abelson and Curtis, 1989; Andreassi 
1995]. Since, however, the reported presence measures, not just the physiological stress measures, decrease 
over multiple exposures, the decreases may not be due to habituation to the stressor; there may also 
be, as Heeter hypothesized, a decrease in a VE s ability to evoke presence as novelty wears off [Heeter 
1992]. Orienting Effect. In general, each measure decreased after the first exposure. Moreover, for each 
measure except .Heart Rate, there was a significant decrease after the first exposure in at least one 
of the studies (see Table 3). For physiological responses, this is called an orienting effect a higher 
physiological reaction when one sees something novel [Andreassi 1995]. Though this term traditionally 
refers to physiological reactions, we will also use the term for the initial spike in the reported measures. 
We attempted, with only partial success, to overcome the orienting effect by exposing subjects to the 
environment once as part of their orientation to the experimental setup and prior to the data­gathering 
portion of the experiment. In the Passive Haptics and Frame Rate studies, subjects entered the VE for 
approximately two Reported Behavioral Presence (Count high ) Multiple Exposures NA -0.7 (1st) -0.9 
(1st) - -0.7 (1st) Passive Haptics - - - -0.8 (1st) -0.4 (1st) Frame Rate -1.0 (Task) -0.8 (1st) -0.3 
(1st) - -0.2 (Task) Table 3. Significant order effects for each measure in each study. (1st) indicates 
a decrease after the first exposure only. (Task) indicates a decrease over tasks on the same day. There 
was an order effect for each measure in at least one study. NA is Not available . Significant results 
are listed at the P < 0.050 level (bold) and P < 0.100 (normal text). Full details given in [Meehan 2001]. 
minutes and were shown both virtual rooms before the experiment started. These pre-exposures reduced 
but did not eliminate the orienting effects.  2.2. Validity Validity is the extent to which a test or 
experiment genuinely measures what it purports to measure [Sutherland 1996]. The concept of presence 
has been operationalized in questionnaires so the validity of the physiological measures can be established 
by investigating how well the physiological reactions correlate with one or more of the questionnaire-based 
measures of presence. We investigated their correlations with two such measures: Reported Presence and 
Reported Behavioral Presence. Reported Presence. Of the physiological measures, .Heart Rate correlated 
best with the Reported Presence. There was a significant correlation in the Frame Rate study (corr. = 
0.265, P < 0.005) and no correlation (corr. = 0.034, P = 0.743) in the Passive Haptics study. In the 
Multiple Exposures study, where .Heart Rate was not available, .Skin Conductance had the highest correlation 
with Reported Presence (corr. = 0.245, P < 0.010). Reported Behavioral Presence. .Heart Rate had the 
highest correlation, and a significant one, with Reported Behavioral Presence in the Frame Rate study 
(corr. = 0.192, P < 0.050), and there was no correlation between the two (corr. = 0.004, P = 0.972) in 
the Passive Haptics study. In the Multiple Exposures study, where .Heart Rate was not measured, .Skin 
Conductance had the highest correlation with reported behavioral presence (corr. = 0.290, P < 0.005). 
The correlations of the physiological measures with the reported measures give some support to their 
validities. The validity of .Heart Rate appears to be better established by its correlation with the 
well-established reported measures. There was also some support for the validity of .Skin Conductance 
from its correlation with reported measures. Following hypothesized relationships. According to Singleton, 
the validation process includes examining the theory underlying the concept being measured, and the more 
evidence that supports the hypothesized relationships [between the measure and the underlying concept], 
the greater one s confidence that a particular operational definition is a valid measure of the concept 
[Singleton et al. 1993]. We hypothesized that presence should increase with frame rate and with the inclusion 
of the 1.5-inch wooden ledge, since each of these conditions provides increased sensory stimulation fidelity. 
As presented in the next section, our physiological measures did increase with frame rate and with inclusion 
of the 1.5-inch wooden ledge. This helps validate the physiological reactions as measures of presence. 
 2.3. Sensitivity and multi-level sensitivity Sensitivity is the likelihood that an effect, if present, 
will be detected [Lipsey 1998]. The fact that the physiological measures reliably distinguished between 
subjects reaction in the Pit Room versus the Training Room in every study assured us of at least a minimal 
sensitivity. For example, heart rate increased an average across all conditions of 6.3 beats / minute 
(BPM) in the Pit Room (P < 0.001) compared to the Training Room in both the Passive Haptics and Frame 
Rate studies. See Table 2 for a full account of sensitivity of physiological measures to the difference 
between the two rooms. Acrophobic patients , when climbing to the second story of a fire escape (with 
a handrail), waiting one minute, and looking down, averaged an increase in heart rate of 13.4 BPM [Emmelkamp 
and Felten 1985]. Our subjects were non-phobic, and our height was virtual; so, we would expect, and 
did find, our subjects heart rate reactions to be lower but in the same direction. Multi-level sensitivity. 
For guiding VE technological development and for better understanding of the psychological phenomena 
of VEs, we need a measure that reliably yields a higher value as a VE is improved along some goodness 
dimension, i.e., is sensitive to multiple condition values. We distinguish this from sensitivity as described 
above and call this multi-level sensitivity. The Passive Haptics study provided us some evidence of the 
measures ability to discriminate between two high presence situations. We have informally observed that 
walking into the Pit Room causes a strong reaction in users, and this reaction seems greater in magnitude 
than the differences in reaction to the Pit Room between any two experimental conditions (e.g., with 
and without the 1.5-inch wooden ledge). Therefore, we expected the differences in reaction among the 
conditions to be less than the differences between the two rooms. For example, in Passive Haptics, we 
expected there to be a significant difference in the physiological measures between the two conditions 
(with and without the 1.5-inch wooden ledge), but expected it to be less than the difference between 
the Training Room and Pit Room in the lower presence condition (without the 1.5-inch wooden ledge). For 
.Heart Rate, we did find a significant difference between the two conditions of 2.7 BPM (P < 0.050), 
and it was less than the inter-room difference for the without-ledge condition: 4.9 BPM. See Figure 5. 
Figure 6 shows that the differences among the conditions in the FR study are smaller in magnitude as 
compared to the differences between the two rooms. Figure 5. . Heart Rate in Passive Haptics study. 
In the Passive Haptics study, we investigated the multi-level sensitivity of the measures by testing 
whether presence was significantly higher with the 1.5-inch wooden ledge. Presence as measured by each 
of .Heart Rate (2.7 BPM; P < 0.050), .Skin Conductance (0.8 mSiemens; P < 0.050), and Reported Behavioral 
Presence (0.5 more high responses; P < 0.005) was significantly higher with the wooden ledge. Reported 
Presence had a strong trend in the same direction (0.5 more high responses; P = 0.060). In the Frame 
Rate study, we investigated the multi-level sensitivity of the measures by testing whether presence increased 
significantly as graphic frame update rates increased. We hypothesized that physiological reactions would 
increase monotonically with frame rates of 10, 15, 20, and 30 FPS. They did not do exactly that (see 
Figure 6). During the 10 FPS condition, there was an anomalous reaction for all of the physiological 
measures and for Reported Behavioral Presence. That is, at 10 FPS, subjects had higher physiological 
reaction and reported more behavioral presence. We believe that this reaction at 10 FPS was due to discomfort, 
added lag, and reduced temporal fidelity while in the ostensibly dangerous situation of walking next 
to a 20-foot pit [Meehan 2001]. We also observed that subjects often lost their balance while trying 
to inch to the edge of the wooden platform at this low frame rate; their heart rate jumped an average 
of 3.5 BPM each time they lost their balance (P < 0.050). Statistically controlling for these Loss of 
Balance incidents improved the significance of the statistical model for .Heart Rate and brought the 
patterns of responses closer to the hypothesized monotonic increase in presence with frame rate but 
did not completely account for the increased physiological reaction at 10 FPS. Loss of Balance was not 
significant in any other model. Change in beats/minute 10 9 8 7 6 5 4 3 2 1 0 10 15 20 30 Figure 6. 
. heart rate, after correcting for Loss of Balance, at 10, 15, 20, and 30 frames per second. Beyond 10 
FPS, .Heart Rate followed the hypothesis. After we statistically controlled for Loss of Balance, .Heart 
Rate significantly increased between 15 FPS and 30 FPS (3.2 BPM; P < 0.005) and between 15 FPS and 20 
FPS (2.4 BPM; P < 0.050). There was also a non-significant increase between 20 FPS and 30 FPS (0.7 BPM; 
P = 0.483) and a non-significant decrease between 10 FPS and 15 FPS (1.6 BPM; P = 0.134). Reported Presence, 
and Reported Behavioral Presence also increased with frame rate from 15-20-30 FPS, but with less distinguishing 
power. These findings support the multi-level sensitivity of .Heart Rate. 2.4. Objectivity The measure 
properties of reliability, validity, and multi-level sensitivity are established quantitatively. Objectivity 
can only be argued logically. We argue that physiological measures are inherently better shielded from 
both subject bias and experimenter bias than are either reported measures or measures based on behavior 
observations. Reported measures are liable to subject bias the subject reporting what he believes the 
experimenter wants. Post-experiment questionnaires are also vulnerable to inaccurate recollection and 
to modification of impressions garnered early in a run by impressions from later. Having subjects report 
during the session, whether by voice report or by hand-held instrument, intrudes on the very presence 
illusion one is trying to measure. Behavioral measures, while not intrusive, are subject to bias on the 
part of the experimenters who score the behaviors. Physiological measures, on the other hand, are much 
harder for subjects to affect, especially with no biofeedback. These measures are not liable to experimenter 
bias, if instructions given to the participants are properly limited and uniform. We read instructions 
from a script in the Multiple Exposures study. We improved our procedure in the later Passive Haptics 
and Frame Rate studies by playing instructions from a compact disk player located in the real laboratory 
and represented by a virtual radio in the VE. 2.5. Summary and discussion The data presented here show 
that physiological reactions can be used as reliable, valid, multi-level sensitive, and objective measures 
of presence in stressful VEs. Of the physiological measures, .Heart Rate performed the best. There was 
also some support for .Skin Conductance. .Heart Rate significantly differentiated between the Training 
Room and the Pit Room, and although this reaction faded over multiple exposures, it never decreased to 
zero. It correlated with the well-established reported measure, the UCL questionnaire. It distinguished 
between the presence and absence of passive haptics and among frame rates at and above 15 FPS. As we 
argued above, it is objective. In total, it satisfies all of the requirements for a reliable, valid, 
multi-level sensitive, and objective measure of presence in a stressful VE. .Skin Conductance has some, 
but not all, of the properties we desire in a measure of presence. In particular, it did not differentiate 
among frame rates. We do not have a theory as to why. Although, .Heart Rate satisfied the requirements 
for a presence measure for our VE, which evokes a strong reaction, it may not for less stressful VEs. 
To determine whether physiological reaction can more generally measure presence, a wider range of VEs 
must be tested, including less stressful, non-stressful, and relaxing environments. Investigation is 
currently under way to look at physiological reaction in relaxing 3D Television environments [Dillon 
et al. 2001]. The height reaction elicited by our VE could be due to vertigo, fear, or other innate or 
learned response. The reactions are well known in the literature and manifest as increased heart rate 
and skin conductance and decreased skin temperature [Andreassi 1995; Guyton 1986]. We hypothesized that 
the more present a user feels in our stressful environment, the more physiological reaction the user 
will exhibit. What causes this higher presence and higher physiological reaction? Is it due to a more 
realistic flow of visual information? Is it due to more coherence between the visual and haptic information? 
Is it due to the improved visual realism? All of these are likely to improve presence. We cannot, however, 
answer these questions definitively. We can say, though, that we have empirically shown that physiological 
reaction and reported presence are both higher when we present a higher presence VE. Whatever it is that 
causes the higher reported presence and physiological reaction, it causes more as we improve the VE. 
An additional desirable aspect of a measure is ease of use in the experimental setting. We did not record 
the time needed for each measure, but after running many subjects we can say with some confidence that 
use of the physiological monitoring and of the presence questionnaire each added approximately the same 
amount of time to the experiment. It took about five minutes per exposure to put on and take off the 
physiological sensors. It took about an extra minute at the beginning and end of each set of exposures 
to put on and take off the ECG sensor it was left on between exposures on the same day. It took subjects 
about five minutes to fill out the UCL Presence Questionnaire. It took some training for experimenters 
to learn the proper placement of the physiological equipment on the hands and chest of the subject thirty 
minutes would probably be sufficient. Another aspect of ease of use is the amount of difficulty participants 
have with the measure and to what extent the measure, if concurrent with an experimental task, interferes 
with the task. No subjects reported difficulties with the questionnaires. Only about one in ten subjects 
reported noticing the physiological monitoring equipment on the hands during the VE exposures. Our experiment, 
though, was designed to use only the right hand, keeping the sensor-laden left hand free from necessary 
activity. No subjects reported noticing the ECG sensor once it was attached to the chest. In fact, many 
subjects reported forgetting about the ECG electrodes when prompted to take them off at the end of the 
day. There are groups investigating less cumbersome equipment, which would probably improve ease of use, 
including a physiological monitoring system that subjects wear like a shirt [Cowings et al. 2001]. Overall, 
questionnaires and physiological monitoring were both easy to use and non-intrusive.  3. Physiological 
reactions as between-subjects measures We conducted all of the studies as within-subjects to avoid the 
variance due to natural human differences. That is, each subject experienced all of the conditions for 
the study in which she participated. This allowed us to look at relative differences in subject reaction 
among conditions and to overcome the differences among subjects in reporting and physiological reaction. 
The UCL questionnaire has been used successfully between­subjects [Usoh et al. 1999]. We suspected, however, 
that physiological reaction would not perform as well if taken between­subjects. We expected the variance 
among subjects would mask, at least in part, the differences in physiological reaction evoked by the 
different conditions. We investigated this hypotheses by analyzing the data using only the first task 
for each subject eliminating order effects and treating the reduced data sets as between-subjects experiments. 
That is, we treat each experiment as if only the first task for each subject was run. This means that 
the analysis uses only 10 data points (10 subjects first exposure only) for the Multiple Exposures study, 
52 data points for the Passive Haptics study, and 33 data points for the Frame Rate study. Reliability 
between-subjects: Physiological reaction in the Pit Room. Even between subjects, we expected that there 
would be a consistent physiological reaction to the Pit Room, since we expected such a reaction for every 
exposure to the VE. We expected the significance to be lower, however, because of the reduced size of 
the data set. We found exactly that. The right half of Table 2 shows the values of the physiological 
measures averaged across conditions for the between-subjects analysis. As compared to the full data set, 
the between-subjects data have lower significance values, but subjects still have strong physiological 
reactions to the Pit Room. Table 2 demonstrates that the physiological orienting effects caused the averages 
for the first exposures to be higher than for the full data set. Validity between-subjects: Correlation 
with established measures. We expected correlations with the reported measures to be lower when taken 
between subjects since there were fewer data points and individual differences in physiological reaction 
and reporting would confound the correlations. This was the case. No physiological measure correlated 
significantly with any reported measure when analyzing between-subjects. Multi-level sensitivity between-subjects: 
Differentiating among presence conditions. We expected inter-subject variation in physiological reaction 
to mask the differences in physiological reactions evoked by the presence conditions (e.g., various frame 
rates). Contrary to this expectation, however, we found strong trends in the physiological measures among 
conditions in both the Passive Haptics and Frame Rate studies. (The condition was not varied in the Multiple 
Exposures study.) In the Passive Haptics study, both .Heart Rate and .Skin Conductance both varied in 
the expected direction non­significantly (3.3 BPM, P = 0.097; 1.0 mSiemens, P = 0.137, respectively). 
In the Frame Rate study, .Heart Rate followed hypothesized patterns, but .Skin Conductance did not. After 
the anomalous reaction at 10 FPS (as in full data set compare Figures 6 and 7), .Heart Rate differentiated 
among presence conditions: at 30 FPS it was higher than at 15 FPS, and this difference was nearly significant 
(7.2 BPM; P = 0.054). Overall, .Heart Rate shows promise as a between-subjects measure of presence. Though 
it did not correlate well with the reported measures (between-subjects), it did differentiate among the 
conditions with some statistical power in Passive Haptics and Frame Rate. .Skin Conductance did not show 
as much promise as a between-subjects measure. For more discussion of physiological reactions as between-subjects 
measures of presence, see [Meehan 2001]. 10 1520 30 Figure 7. Between-subjects analysis: .Heart Rate. 
 4. VE Effectiveness results Above we described the experiments as they related to the testing of the 
physiological presence measures, below we discuss each experiment with respect to the aspect of VEs it 
investigated. Effect of Multiple Exposures on Presence. As described in Section 1.4.1, ten users go through 
the same VE twelve times (over four days) in order to study whether the presence inducing power of a 
VE declines, or becomes unusably small, over multiple exposures. We did find significant decreases in 
each presence measure (reported and physiological) in either this experiment or one of the subsequent 
two experiments (see Table 3). However, none of the measures decreased to zero nor did any become unusably 
small. The findings support our hypothesis that all presence measures decrease over multiple exposures 
to the same VE, but not to zero. Effect of Passive Haptics on Presence. Our hypothesis was that supplementing 
a visual-aural VE with even rudimentary, low­fidelity passive haptics cues significantly increases presence. 
This experiment was only one of a set of studies investigating the passive haptics hypothesis. The detailed 
design, results, and discussion for the set are reported elsewhere [Insko 2001]. We found significant 
support for the hypothesis in that, with the inclusion of the 1.5-inch ledge, presence as measured by 
.Heart Rate, Reported Behavioral Presence, and .Skin Conductance was significantly higher at the P < 
0.05 level. Reported Presence also had a strong trend (P < 0.10) in the same direction. Effect of Frame 
Rate on Presence. Our hypothesis was that as frame rate increases from 10, 15, 20, 30 frames/second, 
presence increases. For frame rates of 15 frames/second and above, the hypothesis was largely confirmed. 
It was confirmed with statistical significance for 15 to 20 FPS and 15 to 30 FPS. 20 to 30 FPS though 
not statistically significant was in the same direction. 10 FPS gave anomalous results on all measures 
except Reported Presence, which increased monotonically with frame rate with no statistical significance. 
 5. Future Work Given a compelling VE and a sensitive, quantitative presence measure, the obvious strategy 
is to degrade quantitative VE quality parameters in order to answer the questions: What makes a VE compelling? 
What are the combinations of minimum system characteristics to achieve this? For example, we would like 
to study the effect of Latency  Self-avatar fidelity  Aural localization  Visual Detail  Lighting 
Realism  Realistic physics in interactions with objects  Interactions with other people or agents 
 Then we hope to begin to establish trade-offs for presence evoked: Is it more important to have latency 
below 50 ms or frame rate above 20 FPS? Additionally, we must eliminate the cables that tether subjects 
to the monitoring, tracking, and rendering equipment. Our subjects reported this encumbrance as the greatest 
cause of breaks in presence. 6. Acknowledgements We would like to thank the University of North Carolina 
(UNC) Graduate School, the Link Foundation, and the National Institutes of Health National Center for 
Research Resources (Grant Number P41 RR 02170) for funding this project. We would like to thank the members 
of the Effective Virtual Environments group, the UNC Computer Science Department, and Dr. McMurray of 
the UNC Applied Physiology Department. Without their hard work, none of this research would have been 
possible. We would like to thank Drs. Slater, Usoh, and Steed of the University College of London who 
built much of the foundation for this work. We would also like to thank the reviewers for their thoughtful 
comments and suggestions. 7. References Abelson, J. L. and G. C. Curtis (1989). Cardiac and neuroendocrine 
responses to exposure therapy in height phobics. Behavior Research and Therapy, 27(5): 561-567. Andreassi, 
J. L. (1995). Psychophysiology: Human behavior and physiological response. Hillsdale, N.J., Lawrence 
Erlbaum Associates. Barfield, W., T. Sheridan, D. Zeltzer and M. Slater (1995). Presence and performance 
within virtual environments. In W. Barfield and T. Furness, Eds., Virtual environments and advanced interface 
design. London, Oxford University Press. Cowings, P., S. Jensen, D. Bergner and W. Toscano (2001). A 
lightweight ambulatory physiological monitoring system. NASA Ames, California. Dillon, C., E. Keogh, 
J. Freeman and J. Davidoff (2001). Presence: Is your heart in it? 4th Int. Wkshp. on Presence, Philadelphia. 
Ellis, S. R. (1996). Presence of mind: A reaction to Thomas Sheridan's "Further musings on the psychophysics 
of presence". Presence: Teleoperators and Virtual Environments, 5(2): 247-259. Emmelkamp, P. and M. Felten 
(1985). The process of exposure in vivo: cognitive and physiological changes during treatment of acrophobia. 
Behavior Research and Therapy, 23(2): 219. Freeman, J., S. E. Avons, D. Pearson, D. Harrison and N. Lodge 
(1998). Behavioral realism as a metric of presence. 1st Int. Wkshp. on Presence. Guyton, A. C. (1986). 
Basic characteristics of the sympathetic and parasympathetic function. In Textbook of Medical Physiology, 
688-697. Philadelphia, W.B. Saunders Company. Heeter, C. (1992). Being there: The subjective experience 
of presence. Presence: Teleoperators and Virtual Environments, 1: 262-271. IJsselsteijn, W. A. and H. 
d. Ridder (1998). Measuring temporal variations in presence. 1st Int. Wkshp. on Presence. B. Insko (2001). 
Passive haptics significantly enhance virtual environments, Doctoral Dissertation. Computer Science. 
University of North Carolina, Chapel Hill, NC, USA. Kleinbaum, D., L. Kupper, K. Muller and A. Nizam 
(1998). Applied regression analysis and other multivariate methods. Lipsey, M. W. (1998). Design sensitivity: 
Statistical power for applied experimental research. In L. Brickman and D. J. Rog, Eds., Handbook of 
applied social research methods, 39-68. Thousand Oaks, California, Sage Publications, Inc. Lombard, M. 
and T. Ditton (1997). At the heart of it all: The concept of presence. Journal of Computer Mediated Communication, 
3(2). McMurray, D. R. (1999). Director of Applied Physiology lab, University of North Carolina. Personal 
Communication. M. Meehan (2001). Physiological reaction as an objective measure of presence in virtual 
environments. Doctoral Dissertation. Computer Science. University of North Carolina, Chapel Hill, NC, 
USA. Regenbrecht, H. T. and T. W. Schubert (1997). Measuring presence in virtual environments. In Proc. 
of Human Computer Interface International, San Francisco. SAS (1990). SAS/ STAT User's Guide, Version 
6, Fourth Edition. Cary, NC, USA, SAS Institute Inc. Schubert, T., F. Friedmann and H. Regenbrecht (1999). 
Embodied presence in virtual environments. In R. Paton and I. Neilson, Eds., Visual Representations and 
Interpretations. London, Springer-Verlag. Sheridan, T. B. (1996). Further musings on the psychophysics 
of presence. Presence: Teleoperators and Virtual Environments, 5(2): 241-246. Singleton, R. A., B. C. 
Straits and M. M. Straits (1993). Approaches to Social Research. New York, Oxford University Press. Slater, 
M., M. Usoh and A. Steed (1994). Depth of presence in virtual environments. Presence: Teleoperators and 
Virtual Environments, 3(2): 130-144. Slater, M., M. Usoh and A. Steed (1995). Taking steps: The influence 
of a walking technique on presence in virtual reality. ACM Transactions on Computer Human Interaction 
(TOCHI), 2(3): 201-219. Slater, M. (1999). Measuring Presence: A Response to the Witmer and Singer Presence 
Questionnaire. Presence: Teleoperators and Virtual Environments, 8(5): 560-565. Slonim, N. B., Ed. (1974). 
Environmental Physiology. Saint Louis. The C. V. Mosby Company. Sutherland, S. (1996). The international 
dictionary of psychology. New York, The Crossroads Publishing Company. Usoh, M., K. Arthur, M. Whitton, 
R. Bastos, A. Steed, M. Slater and F. Brooks (1999). Walking > walking-in-place > flying in virtual environments. 
In Proc. of ACM SIGGRAPH 99. ACM Press/ ACM SIGGRAPH. Weiderhold, B. K., R. Gervirtz and M. D. Wiederhold 
(1998). Fear of flying: A case report using virtual reality therapy with physiological monitoring. CyberPsychology 
and Behavior, 1(2): 97-104. Witmer, B. G. and M. J. Singer (1998). Measuring presence in virtual environments: 
A presence questionnaire. Presence: Teleoperators and Virtual Environments, 7(3): 225-240.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566631</section_id>
		<sort_key>653</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Texture synthesis]]></section_title>
		<section_page_from>653</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P382421</person_id>
				<author_profile_id><![CDATA[81100306908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Freeman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Artificial Intelligence Laboratory]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566632</article_id>
		<sort_key>653</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Self-similarity based texture editing]]></title>
		<page_from>653</page_from>
		<page_to>656</page_to>
		<doi_number>10.1145/566570.566632</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566632</url>
		<abstract>
			<par><![CDATA[We present a simple method of interactive texture editing that utilizes self-similarity to replicate intended operations globally over an image. Inspired by the recent successes of hierarchical approaches to texture synthesis, this method also uses multi-scale neighborhoods to assess the similarity of pixels within a texture. However, neighborhood matching is not employed to generate new instances of a texture. We instead locate similar neighborhoods for the purpose of replicating editing operations on the original texture itself, thereby creating a fundamentally new texture. This general approach is applied to texture painting, cloning and warping. These global operations are performed interactively, most often directed with just a single mouse movement.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[image cloning]]></kw>
			<kw><![CDATA[interactive]]></kw>
			<kw><![CDATA[texture editing]]></kw>
			<kw><![CDATA[texture warping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14037106</person_id>
				<author_profile_id><![CDATA[81100076680]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024618</person_id>
				<author_profile_id><![CDATA[81100184450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Neil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dodgson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>364405</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ASHIKHMIN, M. 2001. Synthesizing Natural Textures. ACM Symposium on Interactive 3D Graphics, 217-226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614486</ref_obj_id>
				<ref_obj_pid>614282</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BAR-JOSEPH, Z., EL-YANIV, R., LISCHINSKI, D., AND WERMAN, M. 2001. Texture Mixing and Texture Movie Synthesis Using Statistical Learning. IEEE Transactions on Visualization and Computer Graphics, 7, 2, 120-135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BEIER, T., AND NEELY, S. 1992. Feature-Based Image Metamorphosis. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), 26(2), ACM, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BERMAN, D., BARTELL, J., AND SALESIN, D. 1994. Multiresolution Painting and Compositing. ACM SIGGRAPH 94, 85-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BURT, P., AND ADELSON, E. 1983. A Multiresolution Spline with Application to Image Mosaics. ACM Transactions on Graphics, 2, 4, 217-236.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DE BONET, J. S. 1997. Multiresolution Sampling Procedure for Analysis and Synthesis of Texture Images. ACM SIGGRAPH 97, 361-368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618591</ref_obj_id>
				<ref_obj_pid>616056</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DISCHLER, J., AND GHAZANFARPOUR, D. 1999. Interactive Image-Based Modeling of Macrostructured Textures. IEEE Computer Graphics and Applications, 19, 1, 66-74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[EBERT, D., MUSGRAVE, F., PEACHEY, D., PERLIN, K., AND WORLEY, S. 1994. Texturing and Modeling: A Procedural Approach. AP Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[EFROS, A., AND FREEMAN, W. 2001. Image Quilting For Texture Synthesis and Transfer. ACM SIGGRAPH 2001, 341-346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794800</ref_obj_id>
				<ref_obj_pid>794191</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[ELDER, J., AND GOLDBERG, R. 1998. Image Editing In the Contour Domain. IEEE Computer Vision and Pattern Recognition, 374-381.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GLASBEY, C., AND MARDIA, K. 1998. A Review of Image-Warping Methods. Journal of Applied Statistics, 25, 2, 155-172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218441</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GLEICHER, M. 1995. Image Snapping. ACM SIGGRAPH 95, 183-190.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HEEGER, D. J., AND BERGEN, J. R. 1995. Pyramid-Based Texture Analysis/Synthesis. ACM SIGGRAPH 95, 229-238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., JACOBS, C., OLIVER, N., CURLESS, B., AND SALESIN, D. H. 2001. Image Analogies. ACM SIGGRAPH 2001, 327-340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>857646</ref_obj_id>
				<ref_obj_pid>857188</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KEAHEY, A., AND ROBERTSON, E. 1997. Nonlinear Magnification Fields. IEEE Symposium on Information Visualization, 51-58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378495</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KURLANDER, D., AND BIER, E. 1988. Graphical Search and Replace. In Computer Graphics (Proceedings of ACM SIGGRAPH 88), 22(4), ACM, 113-120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808605</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J. P. 1984. Texture Synthesis for Digital Painting. Computer Graphics, 18, 3, 245-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>501787</ref_obj_id>
				<ref_obj_pid>501786</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LIANG, L., LIU, C., XU, Y., GUO, B., AND SHUM, H. 2001. Real-Time Texture Synthesis by Patch-Based Sampling. ACM Transactions on Graphics. 20, 3, 127-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218442</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E., AND BARRETT, W. 1995. Intelligent Scissors for Image Composition. ACM SIGGRAPH 95, 191-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383310</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[OH, B., CHEN, M., DORSEY, J., AND DURAND, F. 2001. Image-Based Modeling and Photo Editing. ACM SIGGRAPH 2001, 433-442.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218437</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND VELHO, L. 1995. Live Paint: Painting With Procedural Multiscale Textures. ACM SIGGRAPH 95, 153-160.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PORTER, T., AND DUFF, T. 1984. Compositing Digital Images. Computer Graphics, 18, 3, 253-259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SIMS, K. 1993. Interactive Evolution of Equations for Procedural Models. The Visual Computer, 9, 8, 466-476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[WEI, L., AND LEVOY, M. 2000. Fast Texture Synthesis Using Tree-Structured Vector Quantization. ACM SIGGRAPH 2000, 479-488.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566633</article_id>
		<sort_key>657</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Jigsaw image mosaics]]></title>
		<page_from>657</page_from>
		<page_to>664</page_to>
		<doi_number>10.1145/566570.566633</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566633</url>
		<abstract>
			<par><![CDATA[This paper introduces a new kind of mosaic, called Jigsaw Image Mosaic (JIM), where image tiles of arbitrary shape are used to compose the final picture. The generation of a Jigsaw Image Mosaic is a solution to the following problem: given an arbitrarily-shaped container image and a set of arbitrarily-shaped image tiles, fill the container as compactly as possible with tiles of similar color to the container taken from the input set while optionally deforming them slightly to achieve a more visually-pleasing effect. We approach the problem by defining a mosaic as the tile configuration that minimizes a mosaicing energy function. We introduce a general energy-based framework for mosaicing problems that extends some of the existing algorithms such as Photomosaics and Simulated Decorative Mosaics. We also present a fast algorithm to solve the mosaicing problem at an acceptable computational cost. We demonstrate the use of our method by applying it to a wide range of container images and tiles.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[mosaics]]></kw>
			<kw><![CDATA[optimization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14228315</person_id>
				<author_profile_id><![CDATA[81100664221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junhwan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43116383</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>81144</ref_obj_id>
				<ref_obj_pid>81139</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AMINI, A. A. 1990. Using Dynamic Programming for Solving Variational Problems in Vision. IEEE Trans. on PAMI, Vol. 12, no 9, pp. 855-867, Sept. 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>177483</ref_obj_id>
				<ref_obj_pid>177478</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ARAD, N., DYN, N., REISFELD, D., AND YESHURUN, Y. 1994. Image warping by Radial Basis Functions: Application to Facial Expressions. Computer Vision, Graphics, and Image Processing. GMIP, 56 (2), 161-172, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>105491</ref_obj_id>
				<ref_obj_pid>105488</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ARKIN, M., CHEW, P., HUTTENLOCHER, D. P., KADEM, K., AND MITCHELL, J. S. B. 1991. An Efficiently Computable Metric for Comparing Polygonal Shapes. IEEE Trans. on PAMI, Vol. 13, No. 3, 209-216, Mar. 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DOWSLAND, K. A. AND DOWSLAND, W. B. 1992. Packing Problems. European Journal of Operational Research, 56:2 - 14, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DOWSLAND, K. A. AND DOWSLAND, W. B. 1995. Solution Approaches to Irregular Nesting Problems. European Journal of Operational Research, 84:506-521, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>725891</ref_obj_id>
				<ref_obj_pid>647506</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FINKELSTEIN, A. AND RANGE, M. 1998. Image Mosaics. In Roger D. Hersch, Jacques Andr&#233;, and Heather Brown, Ed., Artistic Imaging and Digital Typography, LNCS, No. 1375, Heidelberg: Springer-Verlag 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P. 1990. Paint by Numbers. In Computer Graphics (Proceedings of ACM SIGGRAPH 90), 24(4), ACM, 207-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383327</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HAUSNER, A. 2001. Simulating Decorative Mossaics. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, New York, E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 573-580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345022</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[KAPLAN, C.S. AND SALESIN, D. H. 2000. Escherization. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, New York, K. Akeley, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 499-510.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[KASS, M., WITKIN, A., AND TERZOPOULOS, D. 1987. Snakes: Active Contour Models, International Journal of Computer Vision, 1:321-331, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[LLOYD, S. 1982. Least Square Quantization in PCM. IEEE Transactions on Information Theory, 28(1982): 129-137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>316661</ref_obj_id>
				<ref_obj_pid>316660</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[MILENKOVIC, V. J. 1999. Rotational Polygon Containment and Minimum Enclosure using only Robust 2D Constructions, Computational Geometry, 13(1):3-19, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MILENKOVIC, V. J. AND DANIELS, K. 1999. Translational Polygon Containment and Minimal Enclosure using Mathematical Programming. Transactions in Operational Research, 6:525-554, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MOORE, M. P. AND WILHELMS, J. 1988. Collision Detection and Response for Computer Animation, In Computer Graphics (Proceedings of ACM SIGGRAPH 88), 22(4), ACM, 289-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>193191</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[RUSSELL, S AND NORVIG, P. 1994. Artificial Intelligence: A Modern Approach, Prentice Hall, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>550396</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SILVERS, R AND HAWLEY, M. 1997. Photomosaics, New York: Henry Holt, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[STRAND, C. 1999. Hello, Fruit Face! : The Paintings of Guiseppe Arcimboldo, Prestel, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>615522</ref_obj_id>
				<ref_obj_pid>615254</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WOLFSON, H. J. AND RIGOUTSOS, I. 1997. Geometric Hashing: An Overview. IEEE Computational Science and Engineering, Vol. 4, No. 4, pp. 10-21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Jigsaw Image Mosaics Junhwan Kim Fabio Pellacini Dept. of Computer Science, Cornell University Program 
of Computer Graphics, Cornell University  Figure 1: The Jigsaw Image Mosaic (JIM) algorithm takes as 
input an arbitrarily-shaped container image and a set of image tiles of arbitrary shape (left) and generates 
a mosaic (right); it then packs the container as compactly as possible with tiles of similar color to 
the container taken from the input set while optionally deforming them slightly to achieve a more visually-pleasing 
effect. ABSTRACT This paper introduces a new kind of mosaic, called Jigsaw Image Mosaic (JIM), where 
image tiles of arbitrary shape are used to compose the final picture. The generation of a Jigsaw Image 
Mosaic is a solution to the following problem: given an arbitrarily-shaped container image and a set 
of arbitrarily-shaped image tiles, fill the container as compactly as possible with tiles of similar 
color to the container taken from the input set while optionally deforming them slightly to achieve a 
more visually­pleasing effect. We approach the problem by defining a mosaic as the tile configuration 
that minimizes a mosaicing energy function. We introduce a general energy-based framework for mosaicing 
problems that extends some of the existing algorithms such as Photomosaics and Simulated Decorative Mosaics. 
We also present a fast algorithm to solve the mosaicing problem at an acceptable computational cost. 
We demonstrate the use of our method by applying it to a wide range of container images and tiles. CR 
Categories: I.3.8 [Computer Graphics]: Application; I.3.5 [Computational Geometry and Object Modeling]: 
Geometric algorithms, languages, and systems; J.5 [Arts and Humanities]: Fine arts Keywords: Mosaics, 
Morphing, Optimization 1. INTRODUCTION Mosaics are a form of art in which a large image is formed by 
a collection of small images called tiles. Various mosaics can be created for an image depending on the 
choice of tiles and the Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 restriction in their placement. Tile mosaics, for example, are images made 
by cementing together uniformly colored polygonal tiles carefully positioned to emphasize edges in the 
composite picture; Simulated Decorative Mosaics [Hausner 2001] is an algorithm that can generate tile 
mosaics. Photomosaics [Silvers and Hawley 1997] are a different kind of mosaic where a collection of 
small images is arranged in a rectangular grid in such a way that when they are seen together from a 
distance they suggest a larger image. Finally, Arcimboldo, a Renaissance Italian painter, was the self-proclaimed 
inventor of a form of painting called the composite head where faces are painted, not in flesh, but with 
rendered clumps of vegetables and other materials slightly deformed to better match the human features 
[Strand 1999]. Inspired by Arcimboldo, we propose a new kind of mosaic where image tiles of arbitrary 
shape are used to compose the final arbitrarily-shaped picture. We called this new kind of mosaic Jigsaw 
Image Mosaic (JIM). Figure 1 illustrates the process of creating a JIM. Our algorithm takes as input 
a container image of arbitrary shape and a set of image tiles of arbitrary shape; it then packs the container 
as compactly as possible with tiles of similar color to the container taken from the input set while 
optionally deforming them slightly to achieve a more visually-pleasing effect. We can formally define 
the problem as follows: Problem (Jigsaw Image Mosaic): Given an arbitrarily­ shaped container image and 
a set of arbitrarily-shaped tiles {Ti}, find a set of shapes {Sj} such that the union over the Sj resembles 
the container image as closely as possible; and  each Sj is a translated and rotated copy of one of 
the Ti, possibly incorporating a small deformation.  In order to compute a JIM, we introduce a general 
energy-based framework for mosaicing problems, where a mosaic is defined as the tile configuration that 
minimizes a weighted sum of energy terms. By changing the weights in the energy formulation, various 
kinds of mosaics can be generated. Our framework generalizes a) Input image c) Jigsaw Image Mosaic (JIM) 
d) Photomosaic e) Simulated Decorative Mosaic    b) JIM tile contours some of the existing mosaicing 
techniques previously presented in the computer graphics literature such as Photomosaics [Silvers and 
Hawley 1997] and Simulated Decorative Mosaics [Hausner 2001]. A comparison of the images obtained by 
the three algorithms is presented in Figure 2. As with Photomosaics, our algorithm uses tiles containing 
smaller images. As in Simulated Decorative Mosaics, the Jigsaw Image Mosaics maintain important edges 
found in the container image; while the first algorithm does so by reorienting the tiles, our approach 
uses oriented tiles of the best-fitting shape as shown, for example, by the wedge-shaped tiles used in 
the sharp corners of the drops in Figure 2b and Figure 2c. The two algorithms use a segmentation of the 
original image in order to specify important edges. Our framework has three major advantages. First, 
a user can easily control the result image by changing the weights in the energy formulation. Second, 
we can introduce new mosaicing generation rules by introducing additional energy terms in the energy 
formulation. Finally, the mosaic generation and tile preparation is completely automatic requiring no 
user intervention. Since the Jigsaw Image Mosaic problem can be cast as an instance of an energy minimization 
problem, various algorithms such as simulated annealing could be employed to find a solution. Unfortunately, 
due to its high dimensional search space, most of the standard minimization techniques would demand too 
many resources to be run. This paper also presents a fast minimization algorithm tailored to solve the 
generalized mosaicing problem. We believe that the two major contributions of this paper are an energy-based 
framework for the mosaicing problem which generalizes on known algorithms  an energy-minimization algorithm 
that solves the mosaicing problem at an acceptable computational cost  Also, since our framework presents 
a general solution to soft packing problems, where small deformations are acceptable, our framework can 
be applied to feature-based texture synthesis and to various instances of product manufacturing. Mosaics 
are just one application. The rest of this paper is organized as follows: Section 2 summarizes related 
work. In Section 3, we describe how to automatically prepare the required inputs. Sections 4 and 5 address 
the energy minimization framework of the mosaicing problem, and the basic algorithm for the framework 
respectively. Section 6 presents optimization techniques on top of the basic algorithm. We present our 
results in Section 7, and close with discussion and future work in Section 8. 2. RELATED WORK In the 
computer graphics literature, the works most closely related to our approach are the various mosaicing 
algorithms that can be categorized by the choice of tiles and the restriction on their placement. Photomosaics 
[Finkelstein and Range 1998; Silvers and Hawley 1997] are a collection of small images arranged in a 
rectangular grid in such a way that when they are viewed together from a distance they suggest a larger 
image (e.g. Figure 2d). For each rectangular block of pixels in the input image, the photomosaic algorithm 
searches a large database of tiles to find the one that most closely resembles the original block. The 
algorithm gives impressive results using only small resources, but unfortunately it is limited to square 
tiles on a rectangular grid. Simulated Decorative Mosaic [Hausner 2001] approaches the problem of aligning 
square tiles with varying orientations to preserve input image edges while maximizing the area covered 
by the colored tiles (e.g. Figure 2e). Our algorithm resembles that approach since it tries to maintain 
edges in the input image while maximizing coverage. Unfortunately, since we use tiles of different shapes, 
we cannot directly apply the Simulated Decorative Mosaic method for finding low-energy configurations. 
Our technique can compute the same results as these two algorithms, although its generality exacts a 
penalty in speed. Kaplan and Salesin [2000] presented a solution to the Escherization" problem that finds 
a regular tiling using a closed figure that is as similar as possible to the original figure. Their work 
resembles our approach in that they slightly distort the original tile if necessary, but is different 
in that they seek regular tilings whereas we allow small gaps and overlaps. Haeberli [1990] randomly 
chose the tile positions, found the voronoi diagram of these positions, and filled each voronoi region 
with a color sampled from the underlying image. While his approach tessellates the main image using tiles 
of different shapes and completely arbitrary placements, the shapes are arbitrary and may not fit any 
of the given input tiles as required in our formulation. Another body of work related to our approach 
is the packing problem. The packing problem has been extensively studied in operations research and computational 
geometry with application to a broad spectrum of layout problems, such as for cloth, leather and glass. 
Since the packing problem is NP-hard [Milenkovic 1999], numerous heuristics have been developed: boundary 
matching, database driven layout, or leftmost placement policy (See [Dowsland and Dowsland 1995] for 
an extensive survey). Recent work of Milenkovic and his colleagues [1999] combined Trying 2nd tile Trying 
3rd tile Trying 4th tile Trying 5th tile Container Trying 1st tile    high color high gap high overlap 
high deformation lowest energy Legend energy discard energy discard energy discard energy discard 
 Accept Container to be filled Container filled Tiles     Accepted tile Gap region Overlap region 
Shape mismatchColor mismatch Figure 3: Illustration of mosaicing energy terms. computational geometry 
and mathematical programming for dense packing of polygons. Their approach applied to marker layout problems 
achieves packing efficiencies comparable to those of human experts. Our problem differs from the standard 
packing problem in that our aim is not to achieve maximum­density packing, but to reach aesthetically 
pleasing packing. Inspired by Arcimboldo, we allow a small user-specified deformation of the tiles when 
necessary, which is not allowed in the standard packing problem formulation. 3. PREPARING INPUTS The 
JIM algorithm takes as input a container image of arbitrary shape and a set of tiles of arbitrary shape. 
The shape of tiles and container is represented by a polygon. Since we require a fairly high number of 
tiles, we need to be able to extract the shape of the tiles directly from the images themselves. We do 
so using active contours [Kass et al. 1987]. All of the 900 tiles used to generate the images in this 
paper are segmented completely automatically from clip art harvested from the Web. Active contours are 
also used to extract the container shape. Hausner [2001] showed the importance of preserving important 
edges in the input image when generating a mosaic. Following his approach, we segment the input image 
to generate a set of disjoint arbitrarily-shaped containers. Since we preserve the edge of each container, 
the final composite will preserve the important edges on the input image. Figure 2c shows this behavior. 
Within each part of the segmentation, the algorithm runs independently. By allowing the user to input 
arbitrary segmentations, we can also introduce edges that are not present in the input image, but are 
important to maintain for the user. 4. MOSAICING FRAMEWORK 4.1 Problem formalization In order to achieve 
user controllable and extensible framework, we cast the problem of generating a mosaic in an energy minimization 
framework. We define a tile configuration as a subset of the input tiles with repetition, along with 
their associated transformations (translation, rotation, deformation). We say that a tile configuration 
is a Jigsaw Image Mosaic when it minimizes the energy E defined as E = wC · EC + wG · EG + wO · EO + 
wD · ED . (1) The energy is a weighted sum of various terms. Figure 3 illustrates the behaviors of each 
of these energy terms in a simple example. The color energy term EC penalizes configurations that do 
not maintain the color of the input image. The gap energy term EG penalizes configurations that have 
too much empty space in the final image, called gap, while a big overlap between tiles gives large overlap 
energy EO. Finally, the deformation energy ED penalizes configurations where tiles are highly deformed. 
Inspired by Arcimboldo, we allow small deformations for each tile since we may not find a configuration 
where gaps or overlaps are small enough to achieve a pleasing visual effect. This is more likely to happen 
for smaller tile databases. Since collecting a large number of tiles may be a long process, we believe 
that allowing the user to specify the amount of deformation necessary makes the algorithm more usable. 
In order to compute a Photomosaic in our formulation, we can simply restrict the tile database to rectangular 
tiles and set the weights for gap, overlap and deformation to infinity. To compute a Simulated Decorative 
Mosaic, we restrict the database to square tiles with uniform color, where the colors are chosen from 
the palette of the input image, and segment the container to preserve edges. We then set the deformation 
weight to infinity and a very high overlap weight (note that Simulated Decorative Mosaics results have 
sometimes very small overlaps [Hausner 2001]) and moderately high gap weight. We believe that our formulation 
is fairly intuitive to use, since the user can easily adjust the weights in the energy function to obtain 
different results. It is also easily extensible, since we can add new energy terms in order to introduce 
additional rules for image generation. 4.2 Energy terms evaluation The color energy EC is estimated 
by taking the average of the L2 differences of the colors of the final image and the input container 
at random locations on the surface of the container. We evaluate this term for each tile separately to 
ensure a good sampling of the tile area. We evaluate gap EG and overlap EO energies using the spring 
energy formulation as originally employed to prevent bodies in resting contact from penetrating in rigid 
body simulations [Moore and Wilhelms 1988]. More specifically, each vertex of a tile is attached with 
a spring to the nearest edge of the other tiles or the container. If the signed distance d between the 
vertex and the anchor is positive, i.e. there is a gap between them, we add d2/2 to EG. On the other 
hand, if d is negative, i.e. the vertex penetrates the nearest edge, we add d2/2 to EO. The deformation 
energy ED is the sum of the deformation energies for each tile, which measures the difference in shape 
between the deformed tile and the original one. We evaluate ED in a similar way to the active contour 
model, given by 2 ds , (2) D''(s) - T ''(s) 2 +ß D ' '(s) - T ' ('s) ED = 1 .ik =1 .01a ii ii 2 where 
Ti(s) and Di(s) are the original shape and the deformed shape of the i-th tile in the current solution, 
parameterized by s. [0,1]. The first term and the second term inside the integral measure the difference 
between the original tile and the deformed tile with respect to the stretching and flexing respectively, 
where a and ß are sensitivity parameters. Among numerous shape metrics such as [Arkin et al. 1991], we 
choose the above one, since it provides good results in our case and it is easily integrated in our algorithm. 
 Figure 4: Jigsaw Image Mosaic algorithm phases.  5. BASIC MOSAICING ALGORITHM 5.1 Overview To efficiently 
compute a Jigsaw Image Mosaic, we propose an effective algorithm organized in three phases shown in Figure 
4. In the first phase, we choose and roughly place the tiles, ignoring deformation. In the second phase, 
we refine the placement of each tile and deform them if necessary. Finally in the third phase, we assemble 
the final mosaic by placing each tile in its position and warping each image to its final deformed shape 
using the image warping technique presented in [Arad et al. 1994]. Intuitively, this three-phase approach 
works in our case because the deformations we allow are always much smaller than the smallest tile in 
the database. 5.2 Packing The first phase of the algorithm finds an approximately good configuration 
ignoring the deformation term, i.e. the configuration that minimizes the gap and overlaps in the image 
while maintaining the color, as measured by E = w · E + w · E + w · E . (3) CC GG OO To do so, we use 
a best first search [Russell and Norvig 1994]. Our algorithm places one tile at a time. For each new 
tile to place, we find a roughly suitable position in the container. We then search the database to determine 
which tile we should use, and determine the exact position and orientation of the tile in such a way 
that the tile is maximally aligned to the boundary of container, i.e., E in Equation 3 is minimized. 
This is a typical registration problem except that we register the tile to a part of the container, rather 
than the container itself. We will explain in more detail in Section 6 how to efficiently find a suitable 
spot using a centroidal voronoi diagram (CVD) [Lloyd 1982] and how to search the database using geometric 
hashing. After we place a tile, a new container is computed by subtracting the tile shape from the original 
container, as shown in Figure 5. The new container is used to place the next tile. We keep placing tiles 
until either the tiles completely fill out the container or we cannot find a suitable tile to fill a 
container. If this happens, we backtrack to the configuration that has minimal energy so far. Figure 
6 illustrates the algorithm sequence.  Initial Tile Container with Container for container placed tile 
next iteration   5.3 Refinement Even after finding the best possible tile arrangement, too much gap 
or overlap may remain and be aesthetically displeasing, especially when using a small number of tiles. 
Sometimes we can obtain a better looking result by slightly deforming the tiles to reduce gaps and overlaps 
significantly, as long as the deformation does not alter the original tile too much. While this generally 
produces better looking images, the user has the option to define the amount or skip deformation. The 
refinement phase of our approach solves this issue by deforming the tiles obtained from the packing stage, 
while balancing between maintaining the original tile shape as closely as possible and minimizing the 
gap, overlap and color differences (i.e. minimizing the full energy equation). We compute the final configuration 
using a set of active contours [Kass et al. 1987] interacting with each other. Intuitively, each vertex 
of a contour is subject to forces that tend to maintain the contour s original shape and to repulse two 
contours if they penetrate, or attract them if there is gap between them. The tile configuration that 
minimizes E in Equation 1 must satisfy the following euler equation: w ·.E + w ·.E + w ·.E + w ·.E = 
0 . (4) C CGGOOD D .EC is close to zero for our case since the deformations are much smaller than the 
smallest tile. For each vertex, .EG=2d·n if the vertex is in a gap (0 otherwise), where n is the unit 
vector perpendicular to the nearest edge, d is the signed distance to the nearest edge. .EO=2d·n if the 
vertex penetrates another tile (0 otherwise). .EO makes the tile shrink if it is too big while .EG expands 
it if too small. The deformation term can be computed by differentiating Equation 2 for each vertex: 
.ED = a(Di ''(s) - Ti ' (s))+ ß(Di ' '(s) - Ti ' ('s)) . (5) Containera Available Tilesb Place 1st tile 
Try again 1st tile c Cannot place next Backtrack Place next Done  Figure 5: Container update. Figure 
6: Tile placement. Notice that Equation 5 is exactly the same energy formulation used for standard snakes 
but the relaxed state is defined as the original tile shape rather than a simple straight line. Also, 
external forces are determined not by an image (as in the standard snake), but by gap and overlap between 
tiles and with the container. The solution to the above equation can still be found by solving the discrete 
system iteratively [Amini 1990]. Figure 7 shows the evolution from the original tiles to the deformed 
ones.  6. ALGORITHM OPTIMIZATIONS In the previous section, we presented the basic algorithm for tile 
placement. A naïve implementation would be too resource demanding so we present several optimization 
techniques in this section. The time complexity of the algorithm is roughly given by O(V · N ·V · N ·(1 
+ b)) . (6) tile tile container tileInContainer where Vtile is the number of vertices per tile, Ntile 
is the number of tiles in the database, Vcontainer is the number of vertices in container, NtileInContainer 
is the number of tiles in the container, and b is the overhead due to branching in the search tree (backtracking). 
In the following subsections we will introduce optimization for each of the factors in Equation 6. 6.1 
Placing a tile When placing a tile in a container of arbitrary shape, it would be prohibitive to try 
every possible location. As we mentioned before, we update the container after placing every tile. In 
order to reduce the branching overhead b, we try those locations that are most likely to make the container 
shape easier to fill after updating. Unfortunately this depends on which tile we place. Nevertheless, 
we can guess how the container would look after we put an average tile. A container will be easier to 
fill if it does not have a protrusion and is as convex as possible. Before placing a new tile, we construct 
a CVD, where each site has an area roughly equal to the average size of tiles (a similar technique has 
been previously used in [Hausner 2001]). We then select a random site among the ones that have the least 
number of neighbors, thus making the container as easy as possible to fill. Figure 8 shows the selection 
process. Notice that placing one tile at a time allows us to handle tiles with different sizes. Figure 
2c, for instance, contains tiles that differ in size by 7 times. 6.2 Branch-and-bound with look-ahead 
Every time we cannot find a suitable tile to fill a container, we need to backtrack to the configuration 
that has minimal energy so far. To reduce this branching overhead b, we use a look-ahead technique [Russell 
and Norvig 1994]. When placing a new tile, we penalize tiles that will make it harder to fill the container 
in the next iteration. To do this we add a term to the energy formulation that takes into account how 
the container will look after tile placement. Thus, the energy in Equation 3 becomes: E = w · E + w · 
E + w · E + w · E . (7) GG OO CC LALA The container shape term advocates for containers with a small 
area and short circumference, or 2 E = w · area +(1- w )·length , (8) where area is the container s 
area, length is its boundary length, LAA A a) Initial c) Converged contours b) Intermediate contours 
contours Figure 7: Evolution of active contours. Legend CVD connected graph CVD cells Selected position: 
only two neighbors in the CVD graph Figure 8: Selecting a tile position by CVD. and wA controls the 
weight of area in relation to the weight of length. Adding the container shape term in the energy evaluation 
prevents the algorithm from placing a tile that fits well but that leads to a harder-to-fill updated 
container.  6.3 Container cleanup After we place a tile in a container, we update the container by 
subtracting the tile from the container. However, the new container can be very jagged, or even have 
disjoint regions. If these fragments are shallower than the shallowest tile, we know it can never be 
filled with any existing tile. In that case, it is safe to separate those fragments and consider them 
as a gap. This cleanup process reduces the running time by cutting the number of vertices in the container 
Vcontainer. It also reduces the branching overhead b, since it prevents the algorithm from wasting time 
attempting to fill unpromising fragments of the container. 6.4 Geometric hashing Given a container 
and a location in the container, we need to try each tile in the database and their positions and orientations. 
Since the number of tiles is fairly high, a linear search would be prohibitive. To this end, we employ 
geometric hashing, a technique originally developed in computer vision for matching geometric features 
against a database of such features [Wolfson and Rigoutsos 97]. We use geometric hashing to select a 
few tiles that will suit to a particular position in the container. We then evaluate the energy term 
for them and pick the best fitting one. Intuitively we use geometric hashing as a pruning technique to 
reject bad tiles. In order to use geometric hashing, we will create a grid of squares in the plane in 
a preprocessing phase. Each square corresponds to a hash table entry. If a shape boundary crosses a square, 
we will record the tile ID and its orientation as an entry in the list attached to that hash table entry. 
In the preprocessing phase we place all tiles with all possible discrete orientations in the grid to 
build the hash table. Every time we need to place a new tile in a specific position in the container 
during the packing stage we register the container boundary segment to the hash table and access the 
hash table entries of the squares that the container passes through; for every entry found there, we 
cast a vote for the (tile ID, tile orientation) pair. We proceed to determine those entries that received 
more than a certain number of votes. Each such entry corresponds to a potential candidate. See Figure 
9 for an illustration. This hashing technique reduces the time complexity of the algorithm from O(Ntile) 
to O(hgrid), where hgrid is the grid granularity. Legend a) Bad tile: 15 votes b) Good tile: 22 votes 
Container contour Tile contour Container and tile contour overlap: cast a vote Figure 9: Geometric 
hashing for the 3rd and 5th tile in Figure 3.  7. RESULTS We have used our algorithm to produce a number 
of Jigsaw Image Mosaics using various container images. The images contained in this paper were generated 
from a database of 900 tiles, some of which are from the Columbia Coil-100 dataset and Coolarchive.com. 
Size of the tiles varies by up to 8 times. It took about 10 minutes to 2 hours to generate the results. 
Figures 1 and 2 show that our algorithm faithfully reproduces colors and boundaries of letters and logos. 
Figure 10 shows three variations of the J mosaic in Figure 10a obtained by changing the parameters in 
the energy formulation. Figure 10b shows the result for a very low color weight. Figure 10c was computed 
allowing a large overlap between tiles. Figure 10d is a picture generated with tiles in different scales. 
These variations show how simply changing the weights in the energy function can generate different looking 
images that an artist can easily tweak. Figure 11 shows the result for a photograph of a panda. Given 
the container image and its segmentation, our algorithm reproduces the container image in a visually 
pleasing way. As in [Hausner 2001], we used the different scales of tiles to faithfully reproduce fine 
details of the containers, such as the mouth of the panda. Figure 12 shows a different example where 
the user draws additional edges to emphasize features of the picture, in this case the feathers of the 
parrot. As a result, our algorithm clearly reproduces the user-supplied features. Figure 13 shows an 
artistic picture of a kabuki, generated by preserving the edges of the original picture, but assigning 
different colors associated to each segment and a texture to the background. 8. CONCLUSIONS AND FUTURE 
WORK In this paper, we have presented a general energy-based framework for mosaicing problems that generalizes 
some of the existing algorithms. We also introduce a new kind of mosaic, the Jigsaw Image Mosaic (JIM), 
where tiles and container are arbitrarily-shaped images. Finally we presented an effective algorithm 
to quickly compute a JIM. Our method produces good results, and is general enough to be applied to other 
soft packing problems such as texture synthesis and product manufacturing. This research suggests a number 
of directions for further study. Our current approach uses a search algorithm for packing. Even though 
it is effective because of the elaborate use of look-ahead technique and other optimizations, it is difficult 
to formally prove bounds on the energy of the final configuration. Approaches based on mathematical programming 
or computational geometry as in [Milenkovic and Daniels 1999] could be fruitful. Our framework could 
also be extended to 3D mosaic, where the container is a 3D object and the tiles can be 2D to fill out 
the surface of the container, or 3D to fill out the container itself. 9. ACKNOWLEDGEMENTS We would like 
to thank Eva Tardos, Klara Kedem, Paul Chew, Shimon Edelman, James E. Cutting, Vladimir Kolmogorov, and 
Amy Gale for their insights and comments and to Ramin Zabih and Donald P. Greenberg for their encouragement. 
Peggy Anderson, Parag Tole, and Steven Westin carefully read the manuscripts. We would also like to thank 
Alejo Hausner for providing us his software and to the anonymous reviewers for their constructive critiques. 
Some of the tiles in Figure 2d were obtained from the MIT VisTex web page (Copyright &#38;#169; 1995 
MIT. All rights reserved). Junhwan Kim was supported by NSF grants IIS-9900115 and CCR­0113371 and a 
grant from Microsoft Research, while Fabio Pellacini was supported by NSF Science and Technology Center 
for Computer Graphics and Scientific Visualization (ASC-8920219). 10. BIBLIOGRAPHY AMINI, A. A. 1990. 
Using Dynamic Programming for Solving Variational Problems in Vision. IEEE Trans. on PAMI, Vol. 12, no 
9, pp. 855-867, Sept. 1990. ARAD, N., DYN, N., REISFELD, D., AND YESHURUN, Y. 1994. Image warping by 
Radial Basis Functions: Application to Facial Expressions. Computer Vision, Graphics, and Image Processing. 
GMIP, 56 (2), 161--172, 1994. ARKIN, M., CHEW, P., HUTTENLOCHER, D. P., KADEM, K., AND MITCHELL, J.S.B. 
1991. An Efficiently Computable Metric for Comparing Polygonal Shapes. IEEE Trans. on PAMI, Vol. 13, 
No. 3, 209-216, Mar. 1991. DOWSLAND, K. A. AND DOWSLAND, W. B. 1992. Packing Problems. European Journal 
of Operational Research, 56:2 - 14, 1992. DOWSLAND, K. A. AND DOWSLAND, W. B. 1995. Solution Approaches 
to Irregular Nesting Problems. European Journal of Operational Research, 84:506--521, 1995. FINKELSTEIN, 
A. AND RANGE, M. 1998. Image Mosaics. In Roger D. Hersch, Jacques André, and Heather Brown, Ed., Artistic 
Imaging and Digital Typography, LNCS, No. 1375, Heidelberg: Springer-Verlag 1998. HAEBERLI, P. 1990. 
Paint by Numbers. In Computer Graphics (Proceedings of ACM SIGGRAPH 90), 24(4), ACM, 207-214. HAUSNER, 
A. 2001. Simulating Decorative Mosaics. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, 
New York, E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 573-580. KAPLAN, 
C.S. AND SALESIN, D.H. 2000. Escherization. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, 
New York, K. Akeley, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 499-510. KASS, 
M., WITKIN, A., AND TERZOPOULOS, D. 1987. Snakes: Active Contour Models, International Journal of Computer 
Vision, 1:321--331, 1987. LLOYD, S. 1982. Least Square Quantization in PCM. IEEE Transactions on Information 
Theory, 28(1982): 129-137. MILENKOVIC, V.J. 1999. Rotational Polygon Containment and Minimum Enclosure 
using only Robust 2D Constructions, Computational Geometry, 13(1):3-19, 1999. MILENKOVIC, V. J. AND DANIELS, 
K. 1999. Translational Polygon Containment and Minimal Enclosure using Mathematical Programming. Transactions 
in Operational Research, 6:525-554, 1999. MOORE, M. P. AND WILHELMS, J. 1988. Collision Detection and 
Response for Computer Animation, In Computer Graphics (Proceedings of ACM SIGGRAPH 88), 22(4), ACM, 289--298. 
RUSSELL, S AND NORVIG, P. 1994. Artificial Intelligence: A Modern Approach, Prentice Hall, 1994. SILVERS, 
R AND HAWLEY, M. 1997. Photomosaics, New York: Henry Holt, 1997. STRAND, C. 1999. Hello, Fruit Face! 
: The Paintings of Guiseppe Arcimboldo, Prestel, 1999. WOLFSON, H. J. AND RIGOUTSOS, I. 1997. Geometric 
Hashing: An Overview. IEEE Computational Science and Engineering, Vol. 4, No. 4, pp. 10-21.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566634</article_id>
		<sort_key>665</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Synthesis of bidirectional texture functions on arbitrary surfaces]]></title>
		<page_from>665</page_from>
		<page_to>672</page_to>
		<doi_number>10.1145/566570.566634</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566634</url>
		<abstract>
			<par><![CDATA[The bidirectional texture function (BTF) is a 6D function that can describe textures arising from both spatially-variant surface reflectance and surface mesostructures. In this paper, we present an algorithm for synthesizing the BTF on an arbitrary surface from a sample BTF. A main challenge in surface BTF synthesis is the requirement of a consistent mesostructure on the surface, and to achieve that we must handle the large amount of data in a BTF sample. Our algorithm performs BTF synthesis based on <i>surface textons,</i> which extract essential information from the sample BTF to facilitate the synthesis. We also describe a general search strategy, called the <i>k-coherent search,</i> for fast BTF synthesis using surface textons. A BTF synthesized using our algorithm not only looks similar to the BTF sample in all viewing/lighthing conditions but also exhibits a consistent mesostructure when viewing and lighting directions change. Moreover, the synthesized BTF fits the target surface naturally and seamlessly. We demonstrate the effectiveness of our algorithm with sample BTFs from various sources, including those measured from real-world textures.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D textons]]></kw>
			<kw><![CDATA[bidirectional texture function]]></kw>
			<kw><![CDATA[reflectance and shading models]]></kw>
			<kw><![CDATA[surfaces]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39040796</person_id>
				<author_profile_id><![CDATA[81100393166]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31028321</person_id>
				<author_profile_id><![CDATA[81542373556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jingdan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tsinghua University and Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14183082</person_id>
				<author_profile_id><![CDATA[81335493715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ligang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14085927</person_id>
				<author_profile_id><![CDATA[81546818956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tsinghua University and Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14040907</person_id>
				<author_profile_id><![CDATA[81100085615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Baining]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14159670</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia, 3F Beijing Sigma Center, Haidian District, Beijing 100080, P R China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>293348</ref_obj_id>
				<ref_obj_pid>293347</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sunil Arya, David Mount, Nathan Netanyahu, Ruth Silverman, and Angela Wu. An optimal algorithm for approximate nearest neighbor searching. Journal of the ACM, 45:891-923, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364405</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Michael Ashikhmin. Synthesizing natural textures. 2001 ACM Symposium on Interactive 3D Graphics, pages 217-226, March 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Oana G. Cula and Kristin J. Dana. Compact representation of bidirectional texture functions. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, December 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In Proceedings of SIGGRAPH 96, Computer Graphics Proceedings, Annual Conference Series, pages 303-312, New Orleans, Louisiana, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kristin J. Dana and Shree Nayar. 3d textured surface modeling. In Proceedings of IEEE Workshop on the Integration of Appearance and Geometric Methods in Object Recognition, pages 46-56, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. Reflectance and texture of real-world surfaces. ACM Transactions on Graphics, 18(1):1-34, January 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>893689</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Paul E. Debevec, Yizhou Yu, and George D. Borshukov. Efficient view-dependent image-based rendering with projective texture-mapping. Eurographics Rendering Workshop 1998, pages 105-116, June 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Alexei A. Efros and Thomas K. Leung. Texture synthesis by non-parametric sampling. In Proceedings of International Conference on Computer Vision, September 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gabriele Gorla, Victoria Interrante, and Guillermo Sapiro. Growing fitted textures. SIGGRAPH 2001 Sketches and Applications, page 191, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>347521</ref_obj_id>
				<ref_obj_pid>347511</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pei hsiu Suen and Glenn Healey. The analysis and recognition of real-world textures in 3d. IEEE Transactions on Patten Analysis and Machine Intelligence, 22(5):491-503, May 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Jan J. Koenderink and Andrea J. Van Doorn. Illuminance texture due to surface mesostructure. Journal of the Optical Society of America, 13(3):452-463, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>543017</ref_obj_id>
				<ref_obj_pid>543015</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Thomas Leung and Jitendra Malik. Representing and recognizing the visual appearance of materials using 3d textons. International Journal of Computer Vision, 43(1):29-44, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383269</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Xinguo Liu, Yizhou Yu, and Heung-Yeung Shum. Synthesizing bidirectional texture functions for real-world surfaces. Proceedings of SIGGRAPH 2001, pages 97-106, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J&#233;r&#244;me Maillot, Hussein Yahia, and Anne Verroust. Interactive texture mapping. Proceedings of SIGGRAPH 93, pages 27-34, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Tom Malzbender, Dan Gelb, and Hans Wolters. Polynomial texture maps. Proceedings of SIGGRAPH 2001, pages 519-528, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344987</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Emil Praun, Adam Finkelstein, and Hugues Hoppe. Lapped textures. Proceedings of SIGGRAPH 2000, pages 465-470, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-tiling polygonal surfaces. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):55-64, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383297</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Texture synthesis on surfaces. Proceedings of SIGGRAPH 2001, pages 347-354, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Li-Yi Wei and Marc Levoy. Fast texture synthesis using tree-structured vector quantization. Proceedings of SIGGRAPH 2000, pages 479-488, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383298</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Li-Yi Wei and Marc Levoy. Texture synthesis over arbitrary manifold surfaces. Proceedings of SIGGRAPH 2001, pages 355-360, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344925</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Daniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Curless, Tom Duchamp, David H. Salesin, and Werner Stuetzle. Surface light fields for 3d photography. In Proceedings of SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, pages 287-296, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732304</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Lexing Ying, Aaron Hertzmann, Henning Biermann, and Denis Zorin. Texture and shape synthesis on surfaces. Proceedings of 12th Eurographics Workshop on Rendering, pages 301-312, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Synthesis of Bidirectional Texture Functions on Arbitrary Surfaces Xin Tong Jingdan Zhang. Ligang Liu 
Microsoft Research Asia. Abstract The bidirectional texture function (BTF) is a 6D function that can 
describe textures arising from both spatially-variant surface re­.ectance and surface mesostructures. 
In this paper, we present an algorithm for synthesizing the BTF on an arbitrary surface from a sample 
BTF. A main challenge in surface BTF synthesis is the requirement of a consistent mesostructure on the 
surface, and to achieve that we must handle the large amount of data in a BTF sample. Our algorithm performs 
BTF synthesis based on surface textons, which extract essential information from the sample BTF to facilitate 
the synthesis. We also describe a general search strategy, called the .-coherent search, for fast BTF 
synthesis using surface textons. A BTF synthesized using our algorithm not only looks similar to the 
BTF sample in all viewing/lighthing conditions but also exhibits a consistent mesostructure when viewing 
and lighting directions change. Moreover, the synthesized BTF .ts the target surface naturally and seamlessly. 
We demonstrate the effectiveness of our algorithm with sample BTFs from various sources, including those 
measured from real-world textures. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism color, shading, shadowing, and texture; I.2.10 [Arti.cial Intelligence]: Vision and Scene 
Understanding texture; I.3.3 [Computer Graphics]: Picture/Image Generation. Keywords: Bidirectional texture 
function, 3D textons, re.ectance and shading models, texture synthesis, texture mapping, surfaces 1 
Introduction Two main ingredients for visual realism are surface geometry and surface details. With recent 
advances in surface texture synthesis [20, 18, 22, 9], we can now decorate a real-world surface (e.g., 
reconstructed from laser range scans [4]) with a texture that .ts the surface naturally and seamlessly. 
However, we are still a step away from reality because textures in traditional graphics represent only 
color or albedo variations on smooth surfaces. Real-world tex­tures, on the other hand, arise from both 
spatially-variant surface re­.ectance and surface mesostructures, i.e., the small but visible local geometric 
details [11]. Mesostructures, which are responsible for .3F Beijing Sigma Center, No 49 Zhichun Road, 
Haidian District, Bei­jing 100080, P R China, email: bainguo@microsoft.com .This research was done when 
Jingdan Zhang and Xi Wang were work­ing as part-time interns at Microsoft Research Asia. Copyright &#38;#169; 
2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use is granted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 Xi Wang. Baining Guo Heung-Yeung Shum .Tsinghua University. Figure 1: Left column exhibits images of 
a surface texture. Right column exhibits images of a surface BTF. .ne-scale shadows, occlusions, and 
specularities [6, 12, 10, 13], are ignored by conventional textures. Fig. 1 compares a surface texture 
with a surface BTF. The BTF introduced by Dana et al. [6] is a representation of real-world textures 
that can model surface mesostructures and re­.ectance variations. The BTF is a 6D function whose variables 
are the 2D position and the viewing and lighting directions. In this pa­per, we present an algorithm 
for synthesizing the BTF on arbitrary polygonal surfaces. Given a BTF sample and a mesh, we synthesize 
a BTF on the mesh such that: (a) the surface BTF is perceptually similar to the given BTF sample in all 
viewing/lighting conditions, and (b) the surface BTF exhibits a consistent mesostructure when viewing 
and lighting directions change. The requirement of a con­sistent mesostructure is where surface BTF synthesis 
differs funda­mentally from surface texture synthesis since conventional textures ignore mesostructures 
completely. A BTF can be mapped onto surfaces using texture mapping tech­niques. However, BTF mapping 
on arbitrary surfaces can introduce inconsistent mesostructures. The usual technique for texture map­ping 
arbitrary surfaces is to use a collection of overlapping patches [14, 16], and textures in the overlapping 
regions are blended to hide seams (e.g., see [16]). This technique works well for many textures [16], 
but for the BTF, blending can introduce inconsis­tent mesostructures, as Fig. 2 illustrates. Of course, 
BTF mapping on arbitrary surfaces also suffers from the usual problems of tex­ture mapping, which include 
distortion, seams, and considerable user intervention needed for creating good-quality texture maps [20, 
18, 22]. Liu et al. recognized the importance of mesostructures in de­veloping their algorithm for synthesizing 
a continuous BTF from  V X (V, L0) (V, L1) (a) (b) (c) Figure 2: Inconsistent mesostructures caused 
by BTF blending. (a) The viewing/lighting setting for the images in (b) and (c). ..is the lighting direction 
for the image shown in (b) and ..is the lighting direction for the image shown in (c). Both images are 
for the square outlined by the yellow line in (a). The viewing direction for both images is the same. 
sparse measurements [13]. Indeed, they explicitly recovered geom­etry details using a shape-from-shading 
method. The appearance of the recovered geometry is rendered and used for synthesis images under different 
viewing/lighting settings. Unfortunately, adapting [13] to surface BTF synthesis is not possible because 
it is time con­suming to reconstruct/render the appearance from the recovered ge­ometry for all lighting 
and viewing settings, especially for BTFs with complex geometry or radiance distributions. Moreover, 
cur­rent shape-from-shading techniques will have problems handling real-world textures with complex bump 
structures or without dom­inant diffuse components [13]. A possible way to achieve a consistent mesostructure 
on a sur­face is to directly apply surface texture synthesis techniques to sur­face BTF synthesis. The 
sample BTF may be regarded as a 2D texture map, in which the BTF value at a pixel is a 4D function of 
the viewing and lighting directions, and this 4D function can be discretized into a vector for texture 
synthesis. Unfortunately, this approach incurs a huge computational cost because of the large amount 
of data in a BTF sample. At the resolution of ........., the BTF value at a pixel is a 10800-dimensional 
vector, as opposed to the usual 3D RGB vectors [20, 18, 22]. Since texture synthesis time grows linearly 
with the vector dimension, a surface BTF can take days [20] or even months [18] to compute. Principal 
compo­nents analysis (PCA) can reduce the vector dimension somewhat but cannot alter the nature of the 
problem. Leung and Malik introduced 3D textons based on the observa­tion that, at the local scale, there 
are only a small number of per­ceptually distinguishable mesostructures and re.ectance variations on 
the surface [12]. Their observation raises the hope of a compact data structure for extracting the essential 
information for surface BTF synthesis. However, 3D textons themselves are not compact because of their 
huge appearance vectors [12]. Indeed, the recon­structive BTF synthesis proposed in [12] is not feasible 
on surfaces because the basic operations [18] for surface BTF synthesis tex­ton resampling, distance 
computation, and BTF reconstruction are prohibitively expensive with 3D texton [12]. To get a feel of 
the problem, consider the following: surface BTF reconstruction requires a 4D function to be assigned 
to each mesh vertex, which implies a 2.5 Gb storage for a mesh of 250k vertices [18]. In this paper, 
we show that surface BTF can be ef.ciently syn­thesized using the surface textons, which are derived 
from 3D tex­tons [12]. Having no appearance vectors, surface textons consti­tute a compact data structure 
for extracting the essential informa­tion from the BTF sample to facilitate surface BTF synthesis. We 
present an algorithm based on surface textons for synthesizing a surface texton map, which is a texton-based 
compact representa­tion of the synthesized surface BTF. This compact representation can be directly used 
for rendering; no BTF reconstruction like the one in [12] is necessary. We also describe a search strategy, 
called the .-coherent search, for fast surface BTF synthesis using surface textons. Existing general-purpose 
search strategies, such as those used in [20, 18, 9, 22], can also be adapted for surface textons, but 
the search speed is orders of magnitude slower. We will demonstrate the effectiveness of our algorithm 
using surface BTFs synthesized from both real and synthetic BTF sam­ples. With the increasing availability 
of BTF samples measured from real-world textures [6], surface BTFs provide a way to dec­orate real-world 
geometry with real-world textures. We also show that surface BTFs provide an ef.cient way for rendering 
surfaces with complex synthetic appearance models and geometry details. The rest of the paper is organized 
as follows. After a brief review of related work in Section 2, we give an overview of our approach in 
Section 3. Section 4 discusses how to extract surface textons from the sample BTF. Section 5 provides 
details about surface BTF synthesis. Section 6 describes how to render surface BTFs synthe­sized by our 
algorithm. Results are reported in Section 7, followed by a conclusion and discussion about future work 
in Section 8.  2 Related Work Several models exist for BTF analysis and material recognition, in­cluding 
the histogram model and correlation model for simple BTFs captured from random height .elds with Lambertian 
re.ectance [5], a color correlation model [10], a 3D-texton-based histogram model [12], and a model combining 
PCA and 2D textons [3]. Existing BTF synthesis methods [5, 12, 13] are designed for 2D rectangles [12] 
or rectangular surface patches [5, 13], not for arbi­trary surfaces. Texture morphing [5] is a technique 
for BTF syn­thesis under the assumption that the surface is a simple height .eld with Lambertian re.ectance. 
In [12], a BTF synthesis algorithm was suggested based on 3D textons. This algorithm .rst synthe­sizes 
a 2D map of texton labels using non-parametric sampling [8] and then reconstructs a BTF from the synthesized 
texton labels. A big problem with this reconstructive synthesis of a BTF is the high computational costs 
for synthesizing texton labels and reconstruct­ing the BTF using the huge appearance vectors of textons 
(e.g., an appearance vector is 57600-dimensional if ...sample images of the BTF are used as in [12]). 
To capture and ef.ciently render the complex appearance of the real world surface, Malzbender et al. 
proposed polynomial texture maps for capturing the surface appearance under a .xed viewpoint but different 
lighting directions [15]. For surface light .elds (e.g., [21]), the appearance of surfaces under .xed 
light sources but dif­ferent view directions are captured and stored in a compact way for rendering. 
Generating textures on arbitrary surfaces has been an active area of research (e.g., [14, 16, 20, 18, 
22, 9]). One approach is to map texture patches onto the target surface [14, 16]. A good represen­tative 
work following that approach is the lapped texture technique by Praun et al. [16]. They randomly paste 
texture patches onto the surface following orientation hints provided by the user. To hide the mismatched 
features across patch boundaries, textures in the over­lapping regions are blended. This technique works 
well for many textures, but for highly structured textures and textures with strong low-frequency components, 
the seams along patch boundaries are still evident [16]. A number of algorithms have been proposed for 
directly synthe­sizing textures on arbitrary surfaces. Turk s algorithm [18], Wei and Levoy s algorithm 
[20], and the multi-resolution synthesis al­gorithm by Ying et al. [22] are general-purpose algorithms 
based on the search strategy proposed by Wei and Levoy [19]. The algorithm by Gorla et al. [9] is also 
a general-purpose algorithm, based on the search strategy proposed by Efros and Leung [8]. These algorithms 
tend to be slow, but they can be accelerated by using either tree­structured vector quantization [19, 
20] or a kd-tree [22]. Generally, these algorithms produce high-quality results. A special type of tex­tures 
that cannot be handled well by general-purpose algorithms is the so-called natural textures [2]. Ashikhmin 
proposed a special­purpose algorithm for natural textures [2], and his algorithm has been adapted for 
natural surface textures [22]. However, [2, 22] do not generalize well to other types of textures. 3 
Overview Fig. 3 provides an overview of our system. The given sample BTF ..................is regarded 
as a texture map in which every pixel .....has the value of a 4D function .................... Given 
.and a mesh ., we synthesize the surface BTF ..in two steps: texton analysis and surface BTF synthesis. 
In the .rst step, we generate a 2D texton map ...and build the surface texton space ., which is represented 
by the dot-product ma­trix .. From the sample BTF ., we construct a 3D texton vocab­ulary ..............as 
in [12]. Based on .we can assign a texton label to each pixel of the sample BTF .and thus generate the 
2D texton map .... The surface texton space .is the inner-product space spanned by using the 3D textons 
............as basis vectors. Each ele­ment of .is called a surface texton. The surface texton space 
. is represented by the dot-product matrix .,an .....matrix that stores the dot-product of every pair 
of 3D textons in .. The fact that there are only a small number of 3D textons [12] implies that the dot-product 
matrix .is compact, and so is .. For example, a .....BTF sample consisting of ....color images is about 
59 Mb. Its representation with 400 3D textons extracted from ... sample images is about 92Mb; the corresponding 
dot-product ma­trix is only 640 Kb. The construction of .is simple: all we need to do is calculate the 
dot-product matrix .and discard the appearance vectors. . In the surface BTF synthesis step, we use the 
surface texton map to compactly represent the surface BTF ... The surface texton map ...is a list of 
entries, one for each mesh vertex. The entry for vertex ., ......., consists of a texton label and a 
texture coordinate .........., implicitly de.ning . .............................. ....... We treat the 
2D texton map ...as a texture sample and perform sur­face texture synthesis to generate the surface texton 
map .....We generate the surface texton map entries for mesh vertices incremen­tally, one vertex at a 
time. At each mesh vertex ., we simultane­ously synthesize the texton label and generate a texture coordinate 
de.ning the BTF value at .. The basic operations in surface texton map synthesis are texton resampling 
and the distance computation between surface textons. All these calculations can be carried out as operations 
in the surface texton space .and thus are fully determined by the pre-computed dot-product matrix .. 
 4 Texton Analysis The texton analysis takes the following steps: (a) build a vocabulary of 3D textons 
from the sample BTF ., (b) assign texton labels to the pixels of .to get the 2D texton map ..., and (c) 
construct the surface texton space .by calculating the dot-product matrix .and discarding the appearance 
vectors. 3D Texton Vocabulary: The construction of 3D textons is mostly based the original 3D texton 
paper by Leung and Malik [12]. As in Surface Texton Map tout Figure 3: Data .ow in our system. [12], 
we construct 3D textons from a BTF using K-means cluster­ing. To capture the appearance of mesostructures 
at different view­ing/lighting conditions, we treat the BTF sample ...as a stack of . images and .lter 
each image with a .lter bank of .....Gaussian derivative .lters. For each pixel of ..., the .lter responses 
of .. selected images are concatenated into a ....-dimensional data vec­tor. These data vectors are clustered 
using the K-means algorithm. The resulting K-means centers ............are the 3D textons, and the associated 
....-dimensional concatenated .lter response vec­tors ............are the appearance vectors. We also 
generate an extra texton by averaging the appearance vectors of all textons. This extra texton is used 
as the default texton in surface BTF synthesis. We use ......for all examples in this paper. Our 3D texton 
construction differs from [12] in choosing the .. selected images. The reason for only selecting ..images 
from ... for clustering, where ...., is to reduce computation by explor­ing the coherence of the same 
material in different viewing/lighting settings. In [12], the ..images are randomly chosen. However, 
this is suboptimal because the radiance distribution is non-uniform in the viewing-lighting space. Cula 
and Dana [3] proposed a method for automatically selecting a subset representative images from a BTF 
sample. Their method works with a statistical representa­tion of a BTF sample (the histogram of 2D textons). 
Because a histogram-based representation discards the spatial distribution of the BTF data, adopting 
[3] in BTF synthesis framework is dif.cult. We choose the ..representative images by K-means clustering 
in the viewing-lighting dimensions of the BTF sample .... Specif­ically, we .lter image ..of ...using 
our .lter bank of ..Gaus­sian derivative .lters, producing a 48-dimensional .lter-response vector for 
each pixel of ... The .lter-response vectors of pixels on a regularly-spaced subsampling grid in ..is 
concatenated into an image appearance vector representing ... The image appear­ance vectors of all images 
in ...are then K-means clustered. For each cluster, the image whose image appearance vector is nearest 
to the cluster center is selected as the representative image. Note that forming an image appearance 
vector by concatenating only .lter-response vectors on a subsampling grid is the key to saving computation, 
and we are allowed to subsample because, as far as clustering is concerned, the .lter-response vector 
of a pixel cap­tures enough local structure around the pixel. We used ...... for all examples in this 
paper. 2D Texton Map: Once we have the texton vocabulary ...,...,...., we can easily assign a texton 
label to each pixel of .... The texton .. . label at pixel .is ..........................where .... is 
the ....-dimensional concatenated .lter response vector of pixel ., and ..is the appearance vector of 
3D texton ... The resulting . ..is called a 2D texton map, or texton map for short. Surface Textons: 
The 3D textons ............can be regarded as abstract vectors and they span a vector space .. Any vector 
.in . .is of the form .= .. ...., where .......and ... are real ... numbers. We call .the surface texton 
space. The surface texton space is actually an inner-product space. The dot product of two basis vectors 
..and ..is de.ned as ..... = ....., where ..and  t ( pi ,ti ) 22 ( pi ,ti ) p 11 ( pi ,ti ) s 00 (a) 
(b) (c) Figure 4: The surface patch around the red vertex .in (a) is .at­tened into the blue patch ....in 
(b). The neighborhood template for resampling is shown in red in (b). The resampling of textons from 
vertices of the yellow patch triangle to a neighborhood pixel .is shown in (b) and (c). ..are the appearance 
vectors of ..and ..respectively. We pre­compute the dot product of every pair of basis vectors and store 
the results in an .....matrix .......such that .......... Once .is computed, we discard all appearance 
vectors.  An element of the surface texton space .is a surface texton. Note that ............, with 
their appearance vectors discarded, are also surface textons because they are the basis of .. The resam­pling 
and distance computation for surface textons as required by surface BTF synthesis can be formulated as 
linear transformations and dot-products in the surface texton space .. All these operations are abstract 
in that they do not refer to the appearance vectors. In particular, the dot product of any two vectors 
.and ..in .can be obtained easily from ., without referring to any appearance vector. .. .. ..Let .. 
....and ... .... Then it is easy to verify .. ... ... . that ....= .. .... ..... .....  5 Surface BTF 
Synthesis 5.1 BTF Synthesis with Surface Textons Before the BTF synthesis starts, we have the 2D texton 
map ... and the surface texton space .with the pre-computed dot-product matrix ..We de.ne a local texture 
coordinate frame (.. . ... ) at each vertex of the target mesh .. The vector . is the surface normal 
at ., whereas . and .are the right and up directions determined . .by a vector .eld, which is either 
interpolated from a number of user-speci.ed directions [18] or generated by relaxation [20]. Surface 
Texton Map Synthesis: A single-resolution version of the surface texton map synthesis proceeds as follows. 
We walk through the vertices of the target mesh .and compute an surface texton for every vertex. At each 
vertex ., the surface texton map entry ......is obtained through the following steps. First, we construct 
a neighborhood ....in the .....-plane of . s local texture coordi­nate frame (.. . . ... ). Then, we 
build a candidate set ....consisting of the candidate pixels for .in the 2D texton map .... Next, we 
search the candidate set ....to .nd a pixel ..........such that the distance between ....and the neighborhood 
of .., .....,is the smallest. Finally, we set the texton label of the surface tex­ton map entry .......to 
be .......and the texture coordinate of ......to be ........ The pseudo-code of the surface texton map 
synthesis is as follows. For each vertex .on surface construct neighborhood textons .... smallest match 
= BIG; form the candidates set .... For each pixel .......in .... construct neighborhood textons .... 
new match = ..................; If (new match .smallest match) smallest match = new match ........., 
............. . ............. ........ . .............. ................. The surface texton map synthesis 
essentially generates the BTF value at vertex .by copying the BTF value at location ..in the sample BTF. 
Location ..is chosen according to the neighborhood similarity of ....and .....as measured by their surface 
tex­tons. This is a valid similarity measure because the texton-based similarity of ....and .....implies 
their similarity as measured by their BTF values [12]. Of course a big advantage of the texton­based 
neighborhood similarity measure is that texton distances can be ef.ciently evaluated for surface textons. 
Texton Resampling: Texton resampling is necessary for construct­ing neighborhood ..... We construct ....in 
the .....-plane of . s local texture coordinate frame (.. . ... ) as follows. First, a patch ....is generated 
in the .....-plane by .attening a set of triangles near .[16, 20]. Then, the pixels in the neighborhood 
....are resampled from the patch triangles using a neighborhood template [20], as is shown in Fig. 4. 
Finally, a surface texton ....is ob­tained at each neighborhood pixel .in ....through the following interpolation: 
....................... (1) where .........is the barycentric coordinates of .in the patch . triangle 
that contains ., and ..., ..., and ...are textons at the ver­tices of that patch triangle. For implementation, 
....can be ef­.ciently represented by a .-tuple ....................... The default texton is assigned 
to neighborhood pixels that are not con­tained by any patch triangle. Distance Computation: We need to 
.nd a pixel .......... from the candidate set ....such that the distance between the two neighborhoods, 
....and ....., is the smallest. For this purpose we need to compute, for each pixel .in ...., the distance 
between the neighborhoods ....and ..... This distance can be written as .. . . ................... ............. 
... where ..is the number of pixels in ....and each .....is a sur­face texton. Each term ............. 
.of the above distance can be written as the dot product of two surface textons, ........................ 
which can be easily evaluated using the pre-computed dot-product matrix .. Multi-Resolution Synthesis: 
To improve synthesis quality, we have designed and implemented a two-pass multi-resolution version of 
our BTF synthesis algorithm in a fashion similar to [20, 18, 22]. In the pre-processing stage of the 
two-pass version, we build a tex­ton pyramid and a mesh pyramid. For the texton pyramid, we .rst construct 
an image pyramid for each image of the BTF sample. Then a 2D texton map and a dot-product matrix is generated 
at each level. The number of textons at the next lower resolution .... is about a quarter of that at 
the current resolution ... For the mesh pyramid, we use Turk s algorithm [17]. Starting from the highest 
resolution mesh, we generate the mesh in the next lower resolution level ....by retiling the current 
level ..mesh with about a quarter of the vertices. The vertices at each level of the mesh are randomly 
mapped to the pixels of the texton map at the same level, with the texton label and texture coordinate 
of a vertex coming from its cor­responding pixel. (a) (b) (c) Figure 5: The .-coherence candidates of 
a pixel ..for .... (a) The neighborhood .....for pixel ..(colored black) in ..... (b) The pixels of ......in 
...are colored black. Each black pixel is the coherence candidate corresponding to a colored pixel in 
.....: ..is the coherence candidate corresponding to the green pixel, .. is the coherence candidate corresponding 
to the red pixel, and ..is the coherence candidate corresponding to both the yellow and blue pixels. 
(c) For each pixel in ......, its 3 nearest neighbors are added to ....... Here we only show the 3 nearest 
neighbors of ... In the .rst pass, the surface texton map at the level ..mesh is synthesized from the 
level ....texton map. For a mesh vertex .. at level ..,we .nd a point ....at the level ....mesh by following 
the surface normal at ..on the level ..mesh. We compute the surface texton map entry at ....using the 
level ....texton map. The texture coordinate of ..is derived from that of ..... The texton label at ..is 
fetched from the level ..texton map using .. s texture coordinate. In the second pass, when synthesizing 
the surface texton map entry at vertex ..in the level ..mesh, we use the neighborhood of ..as well as 
that of ....at level ...., where ....is found as in the .rst pass. For vertex .., we form the candidate 
set .....using .. s neighborhood at level ..only. The two-level neighborhoods and the corresponding dot-product 
matrices are used for neighborhood distance computation when searching for the best candidate from ..... 
 5.2 Fast Search for Surface Textons So far we have not described the search strategy, i.e., the strategy 
to form the candidate set ....for a mesh vertex .. Most existing search strategies [20, 18, 22, 9] can 
be adapted for constructing ..... We have experimented with the full search, in which .... consists of 
all pixels of the 2D texton map .... This is a general search strategy used by [20, 18, 9] and the multiresolution 
synthesis algorithm in [22]. Unfortunately, the full search is painfully slow with surface textons for 
two reasons. First, the full search is itself slow because the candidate set is as big as it gets. Second, 
existing acceleration techniques including vector quantization [19] and the kd-tree [1] do not work well 
with surface textons because surface textons are not the usual intensity values. The kd-tree, for example, 
requires sorting data vectors by one of their components [1]. Such sorting is not possible when the data 
vectors are surface textons. For fast searching with surface textons, we have developed a general search 
strategy called the .-coherence search. Candidate Set for 2D Textures: For simplicity we explain the 
.­coherence search in the context of synthesizing a 2D texture ..... Suppose we are to synthesize a pixel 
..of ....based on the already synthesized pixels in a neighborhood .....of .., as is illustrated in Fig. 
5. Every synthesized pixel ..in .....corresponds to a pixel ..in the input sample texture .... We call 
..a coherence candidate for ..because it is a good candidate according to the coherence of ....: a pixel 
that is appropriately forward-shifted with respected to a pixel already used for synthesis is well-suited 
to .ll in ..[2]. The coherence candidates are collected in ......, the coherence candidate set. The .-coherence 
search constructs the candidate set .....as the .-coherence candidate set ......, which is formed by 
adding, for each pixel ..of ......, a set of pixels ...........of ...such that the newly-added pixels 
are closer to ..than any other pixels in ... by the neighborhood distance. The idea of .-coherence search 
is to speed up the search by guiding it to pixels of ...that are close to the coherence candidates according 
to the neighborhood distance. This guidance is valid because by the Markov property [8, 19], whether 
a pixel is an eligible candidate is completely determined by pixels in its surrounding neighborhood. 
If the coherence candidates are suitable to .ll .., then pixels close to the coherence candidates by 
the neighborhood distance are also good candidates for ... The .-coherence search is fast because the 
.-coherence candi­dates set is much smaller (usually ....) than that of the full search and it can be 
constructed very quickly with the pre-computed list of .nearest neighbors for each pixel of .... For 
a small ...( ......), the .nearest neighbors of every pixel of ...can be pre-computed fairly quickly 
by an exhaustive search in .... For a large ...(......), a two-level pyramid is built to speed up the 
pre-processing of lists of .nearest neighbors for all pixels in .... Speci.cally, to compute the .nearest 
neighbors of a pixel ......, we .rst compute .initial candidates for ..........in the low­resolution 
version of ..., where .....in our implementation. For each initial candidate in the low-resolution version 
of ..., its four corresponding pixels in ...are added to the set of initial can­didates in .... After 
all ...initial candidates are so generated, the .nearest neighbors of pixel .are found from these initial 
can­didates. An important advantage of the .-coherent search is that its pyramid-based acceleration also 
works for surface textons. For the .-coherent search, the low-pass .ltering needed for pyramid-based 
acceleration only takes place on the 2D texton map .... The tex­ton pyramid constructed for multi-resolution 
synthesis can also be used for building the list of the .nearest neighbors. As a result, we do not need 
to low-pass .lter the surface textons during the surface texton map synthesis. Low-pass .ltering surface 
textons is a hard operation to de.ne because surface textons have no appearance vec­tors. Fig. 6 shows 
the basic behaviors of .-coherence search. When ....the results of the .-coherence search are practically 
the same as that by the full search [19]. As .decreases, the results look less and less like that of 
the full search. When ..., the results become the same as those generated by Ashikhmin s algorithm [2]. 
Candidate Set on Surfaces: We now consider the construction of the .-coherence candidate set .....for 
a mesh vertex .. Let ...........be the set of all vertices in the .attened patch .... whose surface textons 
have been synthesized. Vertex ..hasatex­ture coordinate .......and an offset .......from .in the patch 
..... As shown in Fig. 4 (b), we forward-shift .......by the off­ . set .......in the 2D texton map ..., 
getting to location .......= . .............in .... Then we fetch the list ..of .nearest . neighbors 
at the pixel closest to ........ The candidate set ..... consists of all .nearest neighbors in all the 
lists ..through ... In multiresolution synthesis, a list of .nearest neighbors is built for each pixel 
of the texton map at every level. In the second pass of a two-pass synthesis, we also use a two-level 
neighborhood when building the list of .nearest neighbors for every pixel so that the neighborhoods on 
the side of the texton pyramid are consistent with the two-level neighborhoods on the side of the mesh 
pyramid. Discussion: The .-coherence search was inspired by Ashikhmin s work [2]. However, our goal is 
different from Ashikhmin s. We want to derive a general-purpose search strategy for fast search with 
surface textons; his goal was to develop a special-purpose algorithm for handling natural textures, i.e., 
textures consisting of arrange­ments of small objects of familiar but irregular shapes [2]. . Model Sample 
Size Vertex Number Time (minutes) Dinosaur ..... 250k 21 (k=1) Horse ....... 250k 22 (k=1) Cat ..... 
300k 141 Bunny ....... 300k 186  Table 1: Timings for synthesizing the surface BTFs shown in Fig. 7. 
Sample (a) (b) Sample Size Full Search .-Coherence Search ..... 747 min. 70 min. ..... 3000 min. 123 
min. ....... 8066 min. 157 min. (c) (d) Figure 6: 2D texture synthesis results using the .-coherence 
search and the full search. (a) .-coherence search with .... (b) .­coherence search with .... (c) .-coherence 
search with ..... (d) The full search [19].  6 Surface BTF Rendering From the surface texton map ....and 
the sample BTF ., we can ef­.ciently render the BTF on the target mesh .as follows. First, we compute 
the viewing and lighting directions for each mesh vertex .in its local texture coordinate frame from 
the given light source location and the viewpoint. Vertices occluded from either the light sources or 
the viewpoint are ignored. Then, a set of nearby images are found from the BTF sample .. Using . s texture 
coordinate, we can look up colors from this set of images and blend them to get the color of .. With 
all vertex colors obtained, the mesh can be sent to the graphics pipeline for display. This procedure 
repeats for every novel lighting/viewing con.guration. Finding the nearest images from the sample BTF 
.is simple because the images in .are evenly distributed in the viewing and lighting space. We .rst .nd 
.nearest sample viewing directions and .nearest sample lighting directions separately. The angle be­tween 
two lighting/viewing directions is used as the distance mea­sure. Then, the ...nearest images are simply 
those corresponding to all combinations of the viewing/lighting directions found in the previous step. 
Debevec et al. [7] proposed a general technique for .nding the nearest images for a given viewing/lighting 
setting.  7 Results We have implemented our surface BTF synthesis algorithm on a PC. The system is easy 
to use. The texton analysis stage involves no user intervention. The BTF synthesis stage is as automatic 
as sur­face texture synthesis (e.g., [18, 20]). The system only requires the user to determine whether 
the BTF sample resemble natural tex­tures and if so, set ...for the .-coherence search. Anisotropic BTFs 
are handled the same way as anisotropic textures in surface texture synthesis [18, 20]. Like [18, 20], 
an optional but often help­ful user intervention for an anisotropic BTF is to specify a vector .eld to 
guide the orientation of the BTF on the target surface. In the following we report synthesis results 
for both sample BTFs of real-world textures and synthetic BTF samples. The real-world samples were taken 
from the CUReT database [6]. All surface BTFs shown in this paper were synthesized using three-or four­level 
pyramids of the meshes and the sample BTFs. All examples use .-coherence search with ....unless otherwise 
mentioned. Table 2: Speed comparison for BTF synthesis using the full search (e.g. [18]) and the .-coherence 
search. Fig. 7 exhibits several surface BTFs. Timings for surface BTF synthesis are summarized in Table 
1. Timings are in minutes mea­sured on a 700MHz Pentium III. The time complexity of our al­gorithm only 
depends on size of the neighborhood and .for the .-coherence search. During the BTF synthesis, our algorithm 
uses about the same amount of memory as surface texture synthesis al­gorithms such as [20, 18]. The only 
extra memory we need is that for the dot-product matrix, which is less than 1 Mb in our system. Timings 
are for BTF synthesis only. The texton analysis time for a BTF sample of size .....takes about 45 minutes 
on the same machine. For the BTF sample in Fig. 7 (a) and (b), we set ... because the samples resemble 
natural textures [2]. Like surface textures, the synthesized BTFs .t the surface geom­etry naturally 
and seamlessly. More importantly, the surface BTFs capture the .ne-scale shadows, occlusions, and specularities 
caused by surface mesostructures. Comparison of the synthesized surface BTFs with the sample BTFs demonstrates 
their similarity. In the companion video, we show that this similarity remains in all view­ing and lighting 
conditions. Moreover, the synthesized mesostruc­tures are consistent as viewing and lighting directions 
change. Table 2 compares the BTF synthesis speed for the full search (e.g. [18]) and the .-coherence 
search using the dinosaur model with 250k vertices. For a small BTF sample (.....), BTF syn­thesis based 
on the .-coherence search is about 10 times faster than that based on the full search. The speed gain 
increases quickly as the BTF sample gets larger. In Fig. 9, we compare the qualities of the surface BTFs 
synthesized with these two search strategies by rendering the BTFs with identical viewing and lighting. 
Our experiments demonstrate that the .-coherence search allows us to synthesize surface BTFs orders of 
magnitude faster while getting comparable quality. Surface BTFs provide an ef.cient way for rendering 
surfaces with complex appearance models that are created synthetically. Fig. 8 shows an example, in which 
a small BTF sample is rendered by ray tracing and then synthesized onto a surface. The render­ing of 
the surface BTF captures the shadow and occlusion caused by the height .eld of the synthetic appearance 
model. Conventional textures and bump maps cannot capture these effects. Although dis­placement maps 
can capture these effects, rendering displacement maps is very expensive. In addition, some appearance 
models have complex local geometry details and BRDF variations that cannot be rendered by displacement 
maps. Surface BTFs will not have prob­lems handling these appearance models. With our unoptimized im­plementation, 
the surface BTF shown in Fig. 8 (300k vertices) can be rendered at a speed of more than one frame per 
second.  Figure 7: Top row: Surface BTFs synthesized using real BTF samples from the CUReT database 
[6]. Bottom row: Surface BTFs synthesized from synthetic BTF samples. Figure 8: (a) Surface BTF. (b) 
Surface texture. (c) Surface BTF. (d) Surface texture. Figure 9: Quality comparison for BTF synthesis 
using the .-coherence search (left) and the full search (right).  8 Conclusion We have presented an 
algorithm for synthesizing BTFs on arbitrary manifold surfaces using surface textons. A BTF synthesized 
using our algorithm not only looks similar to the sample BTF in all view­ing and lighting conditions 
but also exhibits a consistent mesostruc­ture when the viewing and lighting directions change. Because 
the BTF can describe real-world textures, our algorithm enables the user to decorate real-world geometry 
with real-world textures. A limitation of surface BTF synthesis using surface textons is that the synthesis 
algorithm does not work well for materials that cannot be described by 3D textons. So far one such material 
has been reported [12], i.e., the Aluminum Foil dataset in the CUReT database [6]. It will be desirable 
to develop more sophisticated .l­tering/clustering techniques for this sort of materials. We are also 
interested in ef.cient rendering methods for a surface BTF repre­sented by a sample BTF and a surface 
texton map. The basic ren­dering operations of a surface BTF so represented are simple and should be 
amenable to hardware acceleration. Finally, an intriguing possibility is to use synthesis methods to 
produce appearance not captured by the BTF, e.g., subsurface scattering. Acknowledgments: We would like 
to thank Yanyun Chen and Xin­guo Liu for useful discussions. Many thanks to Yanyun Chen for his help 
in generating synthetic BTF data, to Yin Li, Gang Chen, and Steve Lin for their help in video production, 
to Steve Lin for proofreading this paper, and to anonymous reviewers for their con­structive critique. 
Xiang Cao implemented the .rst version of 3D textons.  References [1] Sunil Arya, David Mount, Nathan 
Netanyahu, Ruth Silverman, and Angela Wu. An optimal algorithm for approximate nearest neighbor searching. 
Journal of the ACM, 45:891 923, 1998. [2] Michael Ashikhmin. Synthesizing natural textures. 2001 ACM 
Symposium on Interactive 3D Graphics, pages 217 226, March 2001. [3] Oana G. Cula and Kristin J. Dana. 
Compact representation of bidirectional tex­ture functions. In Proceedings of IEEE Conference on Computer 
Vision and Pattern Recognition, December 2001. [4] Brian Curless and Marc Levoy. A volumetric method 
for building complex models from range images. In Proceedings of SIGGRAPH 96, Computer Graphics Proceedings, 
Annual Conference Series, pages 303 312, New Orleans, Louisiana, August 1996. [5] Kristin J. Dana and 
Shree Nayar. 3d textured surface modeling. In Proceedings of IEEE Workshop on the Integration of Appearance 
and Geometric Methods in Object Recognition, pages 46 56, June 1999. [6] Kristin J. Dana, Bram van Ginneken, 
Shree K. Nayar, and Jan J. Koenderink. Re.ectance and texture of real-world surfaces. ACM Transactions 
on Graphics, 18(1):1 34, January 1999. [7] Paul E. Debevec, Yizhou Yu, and George D. Borshukov. Ef.cient 
view­dependent image-based rendering with projective texture-mapping. Eurograph­ics Rendering Workshop 
1998, pages 105 116, June 1998. [8] Alexei A. Efros and Thomas K. Leung. Texture synthesis by non-parametric 
sampling. In Proceedings of International Conference on Computer Vision, September 1999. [9] Gabriele 
Gorla, Victoria Interrante, and Guillermo Sapiro. Growing .tted tex­tures. SIGGRAPH 2001 Sketches and 
Applications, page 191, August 2001. [10] Pei hsiu Suen and Glenn Healey. The analysis and recognition 
of real-world textures in 3d. IEEE Transactions on Patten Analysis and Machine Intelligence, 22(5):491 
503, May 2000. [11] Jan J. Koenderink and Andrea J. Van Doorn. Illuminance texture due to surface mesostructure. 
Journal of the Optical Society of America, 13(3):452 463, 1996. [12] Thomas Leung and Jitendra Malik. 
Representing and recognizing the visual appearance of materials using 3d textons. International Journal 
of Computer Vision, 43(1):29 44, June 2001. [13] Xinguo Liu, Yizhou Yu, and Heung-Yeung Shum. Synthesizing 
bidirectional tex­ture functions for real-world surfaces. Proceedings of SIGGRAPH 2001, pages 97 106, 
August 2001. [14] J´ome Maillot, Hussein Yahia, and Anne Verroust. Interactive texture mapping. er Proceedings 
of SIGGRAPH 93, pages 27 34, August 1993. [15] Tom Malzbender, Dan Gelb, and Hans Wolters. Polynomial 
texture maps. Pro­ceedings of SIGGRAPH 2001, pages 519 528, August 2001. [16] Emil Praun, Adam Finkelstein, 
and Hugues Hoppe. Lapped textures. Proceed­ings of SIGGRAPH 2000, pages 465 470, July 2000. [17] Greg 
Turk. Re-tiling polygonal surfaces. Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):55 64, July 
1992. [18] Greg Turk. Texture synthesis on surfaces. Proceedings of SIGGRAPH 2001, pages 347 354, August 
2001. [19] Li-Yi Wei and Marc Levoy. Fast texture synthesis using tree-structured vector quantization. 
Proceedings of SIGGRAPH 2000, pages 479 488, July 2000. [20] Li-Yi Wei and Marc Levoy. Texture synthesis 
over arbitrary manifold surfaces. Proceedings of SIGGRAPH 2001, pages 355 360, August 2001. [21] Daniel 
N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Curless, Tom Duchamp, David H. Salesin, and Werner Stuetzle. 
Surface light .elds for 3d photography. In Proceedings of SIGGRAPH 2000, Computer Graphics Proceedings, 
Annual Conference Series, pages 287 296, July 2000. [22] Lexing Ying, Aaron Hertzmann, Henning Biermann, 
and Denis Zorin. Texture and shape synthesis on surfaces. Proceedings of 12th Eurographics Workshop on 
Rendering, pages 301 312, June 2001. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566635</article_id>
		<sort_key>673</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Hierarchical pattern mapping]]></title>
		<page_from>673</page_from>
		<page_to>680</page_to>
		<doi_number>10.1145/566570.566635</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566635</url>
		<abstract>
			<par><![CDATA[We present a multi-scale algorithm for mapping a texture defined by an input image onto an arbitrary surface. It avoids the generation and storage of a new, specific texture. The idea is to progressively cover the surface by texture patches of various sizes and shapes, selected from a single input image. The process starts with large patches. A mapping that minimizes the texture fitting error with already textured neighbouring patches is selected. When this error is above a threshold, the patch is split into smaller ones, and the algorithm recursively looks for good fits at a smaller scale. The process ends when the surface is entirely covered. Our results show that the method correctly handles a wide set of texture patterns, which can be used at different mapping scales. Hierarchical texture mapping only outputs texture coordinates in the original texture for each triangle of the initial mesh. Rendering is therefore easy and memory cost minimal. Moreover the initial geometry is preserved.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[level of detail algorithms]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31031821</person_id>
				<author_profile_id><![CDATA[81100200748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS-GRAVIR, INRIA Rhne-Alpes, 655 avenue de l'Euorope, Montbonnot, 38334 Saint Ismier Cedex, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P189239</person_id>
				<author_profile_id><![CDATA[81407594555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS-GRAVIR, INRIA Rhne-Alpes, 655 avenue de l'Euorope, Montbonnot, 38334 Saint Ismier Cedex, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382218</person_id>
				<author_profile_id><![CDATA[81100420080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angelidis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS-GRAVIR, INRIA Rhne-Alpes, 655 avenue de l'Euorope, Montbonnot, 38334 Saint Ismier Cedex, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>364405</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ASHIKHMIN, M. 2001. Synthesizing natural textures. 2001 ACM Symposium on Interactive 3D Graphics (March), 217-226. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BENNIS, C., V&#201;ZIEN, J.-M., IGL&#201;SIAS, G., AND GAGALOWICZ, A. 1991. Piece-wise surface flattening for non-distorted texture mapping. In Computer Graphics (SIGGRAPH '91 Proceedings), T. W. Sederberg, Ed., vol. 25, 237-246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>572337</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[EBERT, D., MUSGRAVE, F., PEACHEY, D., PERLIN, K., AND WORLEY, S., Eds. 1998. Texturing and Modelling: A procedural approach. Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. 1995. Multiresolution analysis of arbitrary meshes. In SIGGRAPH 95 Conference Proceedings, Addison Wesley, R. Cook, Ed., ACM SIGGRAPH, 173-182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A., AND FREEMAN, W. T. 2001. Image quilting for texture synthesis and transfer. Proceedings of SIGGRAPH 2001 (August), 341-346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[EFROS, A., AND LEUNG, T. 1999. Texture synthesis by non-parametric sampling. In International Conference of Computer Vision, vol. 2, 1033-1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364345</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., WILLMOTT, ., AND HECKBERT, P. 2001. Hierarchical face clustering on polygonal surfaces. In ACM Symposium on Interactive 3D Graphics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., AND SALESIN, D. H. 2001. Image analogies. Proceedings of SIGGRAPH 2001 (August), 327-340. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280930</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[L&#201;VY, B., AND MALLET, J.-L. 1998. Non-distorted texture mapping for sheared triangulated meshes. Proceedings of SIGGRAPH 98 (July), 343-352. ISBN 0-89791-999-8. Held in Orlando, Florida.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[L&#201;VY, B. 2001. Constrained texture mapping for polygonal meshes. Proceedings of SIGGRAPH 2001 (August), 417-424. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MAILLOT, J., YAHIA, H., AND VERROUST, A. 1993. Interactive texture mapping. In Computer Graphics (SIGGRAPH '93 Proceedings), J. T. Kajiya, Ed., vol. 27, 27-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311561</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[NEYRET, F., AND CANI, M.-P. 1999. Pattern-based texturing revisited. Proceedings of SIGGRAPH 99 (August), 235-242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344987</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[PRAUN, E., FINKELSTEIN, A., AND HOPPE, H. 2000. Lapped textures. Proceedings of SIGGRAPH 2000 (July), 465-470. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PRESS, TEUKOLSKI, VETTERLING, AND FLANNERY. 1992. Numerical Recipes in C. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[TURK, G. 1991. Generating textures for arbitrary surfaces using reaction-diffusion. In Computer Graphics (SIGGRAPH '91 Proceedings), T. W. Sederberg, Ed., vol. 25, 289-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[TURK, G. 1992. Re-tiling polygonal surfaces. In Computer Graphics (SIGGRAPH '92 Proceedings), E. E. Catmull, Ed., vol. 26, 55-64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383297</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[TURK, G. 2001. Texture synthesis on surfaces. Proceedings of SIGGRAPH 2001 (August), 347-354. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383294</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WALTER, M., FOURNIER, A., AND MENEVAUX, D. 2001. Integrating shape and pattern in mammalian models. Proceedings of SIGGRAPH 2001 (August), 317-326. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WEI, L.-Y., AND LEVOY, M. 2000. Fast texture synthesis using tree-structured vector quantization. Proceedings of SIGGRAPH 2000 (July), 479-488. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383298</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[WEI, L.-Y., AND LEVOY, M. 2001. Texture synthesis over arbitrary manifold surfaces. Proceedings of SIGGRAPH 2001 (August), 355-360. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732304</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[YING, L., HERTZMANN, A., BIERMANN, H., AND ZORIN, D. 2001. Texture and shape synthesis on surfaces. In Eurographics Rendering Workshop 2001, Springer Wein, S. Gortler and K. Myszkowski, Eds., Eurographics, 301-312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical Pattern Mapping Cyril Soler Marie-Paule Cani Alexis Angelidis iMAGIS./ GRAVIR-IMAG / INRIA 
Abstract We present a multi-scale algorithm for mapping a texture de.ned by an input image onto an arbitrary 
surface. It avoids the generation and storage of a new, speci.c texture. The idea is to progressively 
cover the surface by texture patches of various sizes and shapes, selected from a single input image. 
The process starts with large patches. A mapping that minimizes the texture .tting error with already 
textured neighbouring patches is selected. When this error is above a threshold, the patch is split into 
smaller ones, and the algorithm recursively looks for good .ts at a smaller scale. The process ends when 
the surface is entirely covered. Our results show that the method correctly handles a wide set of texture 
patterns, which can be used at different mapping scales. Hierarchical texture mapping only outputs texture 
coordinates in the original texture for each triangle of the initial mesh. Rendering is therefore easy 
and memory cost minimal. Moreover the initial geometry is preserved. Keywords: Level of Detail Algorithms, 
Texture Mapping, Texture Synthesis 1 Introduction Being able to add small non-geometric details to CG 
objects is es­sential for enhancing the visual complexity of virtual worlds. Tex­turing arbitrary surfaces 
has thus attracted much interest within the past few years. Among these methods, those which map or gener­ate 
repetitive patterns onto surfaces (i.e, pattern mapping) are par­ticularly promising, since they allow 
easy modeling of natural-style materials such as stone, wood, marble or scales. Pattern mapping is more 
effective, in terms of memory cost, than painting or generating a new global texture map on each object. 
However, regularly mapping rectangular texture samples is not ap­plicable in the general case, since 
most surfaces have no global pa­rameterization and cannot be unfolded onto a plane. Our new hier­archical 
texture mapping method addresses this problem. It shows that seamless texturing can be obtained by directly 
selecting and mapping, from coarse to .ne scales, some possibly not contiguous texture patches from the 
input image. This method, which outputs texture coordinates at the original mesh vertices, works for 
arbitrary manifold meshes and texture patterns. .iMAGIS is a joint project of CNRS, INRIA, Institut National 
Poly­technique de Grenoble and Universit´e Joseph Fourier. Address: iMAGIS-GRAVIR, INRIA Rhne-Alpes, 
655 avenue de l Europe, Montbonnot, 38334 Saint Ismier Cedex, France. Email: [Cyril.SolerIMarie-Paule.CaniIAlexis.Angelidis]@imag.fr. 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 1.1 Previous work A complete review of all previous texture mapping and texture syn­thesis methods 
is beyond the scope of this paper. Instead, we focus on the recent progress made towards texturing an 
arbitrary surface with patterns from an input image. Pattern mapping methods Traditional approaches for 
textur­ing a surface with repetitive patterns consist of mapping a 2D, toroidal texture pattern on it. 
However, most surfaces do not have a toroidal parameterization and cannot be unfolded onto a plane. Op­timization 
algorithms can be used to limit texture distortion [Bennis et al. 1991; Maillot et al. 1993; L´evy and 
Mallet 1998; L´evy 2001], but cracks and singularities can hardly be avoided. Moreover, tex­ture periodicity 
resulting from the mapping of a rectangular pattern is obvious and often spoils the visual quality of 
results. Recently, two new approaches were introduced to overcome these problems. Neyret and Cani [Neyret 
and Cani 1999] precompute a set of tri­angular texture samples that match together along borders. The 
sur­face is tiled at the desired scale into curved triangular areas in which these texture samples are 
mapped. Arbitrary surfaces are thus tex­tured with low distortion, no cracks, and no singularities. Moreover, 
the method uses little memory, since no global texture map needs to be stored. It is, however, restricted 
to isotropic texture patterns. In addition, creating the samples either demands delicate manual edi­tion, 
or the implementation of speci.c procedural texture synthesis methods. Lastly, the resulting textured 
mesh is more complex than the initial one, since all the triangles crossing the limit of a texture tile 
are split during the process. A year later, Praun, Finkelstein and Hoppe [Praun et al. 2000] proposed 
a suitable method for anisotropic textures. The idea is to iteratively paste irregular texture patches 
onto a surface. The patches locally overlap and align their main features with a pre­de.ned vector .eld. 
The method generates very nice results, but is again only applicable to a speci.c class of textures: 
the latter should not be sensitive to discontinuities (in practice, the textures used have internal discontinuities), 
and should have no low frequency variations. As with the previous method, this one demands some speci.c 
user input (the user must de.ne texture tiles that .t with the intrinsic texture discontinuities). Lastly, 
rendering requires ei­ther composition operations or the storage of a global texture map. Our method 
can be seen as an extension of these two techniques. However, we believe that performing texture mapping 
at a given, preselected scale is not suf.cient: as stressed by Ying [Ying et al. 2001], the above methods 
cannot use texture tiles that are large with respect to the underlying geometry, thus considerably restricting 
the range of scales at which the pattern can be mapped. Moreover, they cannot capture low-frequency patterns 
while preserving high frequency randomness in the texture. Our method does not suffer from these drawbacks, 
since it uses a hierarchical approach inspired from the texture synthesis techniques described next. 
Texture synthesis on surfaces The most recent contribu­tions [Ying et al. 2001; Wei and Levoy 2001; Turk 
2001] to tex­turing an arbitrary surface from example follow a different strategy. Rather than mapping 
texture samples, they generate a new texture directly on the surface, an idea that had only been tried 
in the past for speci.c classes of procedural textures [Turk 1991; Ebert et al. 1998; Walter et al. 2001]. 
These new methods were inspired by the recent progress of 2D texture synthesis from example [Efros and 
Leung 1999; Wei and Levoy 2000; Ashikhmin 2001; Hertzmann et al. 2001]. Image generation is performed 
pixel by pixel by .tting a region adjacent to the current pixel to similar regions of the input sample. 
The pixel value is then chosen randomly among the possible, resulting values. Wei and Levoy [Wei and 
Levoy 2000] make the method more robust for smooth textures by synthesizing an image pyra­mid from coarse 
to .ne scales, and accelerate neighborhood search using tree-structured vector quantization. Ashikhmin 
[Ashikhmin 2001] instead restricts the search to locations predicted by already processed neighbouring 
pixels, thus reducing computations and in­creasing image coherency. This makes the method usable for 
highly structured patterns (e.g., small objects of familiar shapes). Hertz­mann [Hertzmann et al. 2001] 
combines Wei s and Ashikhmin s approaches, thus getting the bene.ts of both. Three slightly different 
generalizations of Wei and Levoy s multi-scale synthesis algorithm [Wei and Levoy 2000] to surface texturing 
were proposed in 2001, respectively by themselves [Wei and Levoy 2001], by Turk [Turk 2001], and by Ying 
et al. [Ying et al. 2001]. The .rst step for making the method usable in 3D is to compute or de.ne a 
vector .eld on the surface (as noted by Wei, a random .eld can be used if the texture pattern is isotropic). 
A mesh hierarchy is then built to serve as the image pyramid in the 2D synthesis approach. Since a level 
of the hierarchy will corre­spond to a given texture scale, uniform surface sampling is required for 
all meshes. Both Wei and Turk rely on Turk s mesh re-tiling method [Turk 1992] for building such a hierarchy 
from an arbitrary mesh. Ying s implementation only handles multi-resolution sub­division surfaces. Another 
step is to de.ne a correspondence be­tween the texture grid and the region of the mesh that surrounds 
a vertex. The multi-scale synthesis process can then be run, by treat­ing vertices either in sweeping 
or in random order. The quality of the results is almost similar to those obtained for 2D images. Ying 
also extends Ashikhmin s coherent synthesis algorithm [Ashikhmin 2001], thus making texture synthesis 
on surfaces usable for highly structured patterns. Although they rely on a hierarchy of regular meshes 
for texture generation, the methods above may output tex­ture coordinates on the initial mesh, using 
either a global texture map [Turk 2001] or a set of local maps that cover the surface [Ying et al. 2001]. 
In all cases, they require the storage of a new texture that fully covers each object. In a way, our 
method synthesizes the texture on the surface, and also uses a multi-scale process for ensuring global 
pattern co­herency while minimizing texture distortion. However, we do so by directly selecting texture 
patches of various size from the input image. This accelerates the process since surface regions that 
could be textured at a large scale require no further processing. More­over, it suppresses texture storage 
memory requirements, since no new texture is created. Stitching image patches The idea of creating a 
new texture by stitching together texture patches from a sample image was al­ready used in 2D. Efros 
and Freeman [Efros and Freeman 2001] create a larger image by selecting rectangular texture tiles of 
a given size from the input image. Discontinuities across tiles boundaries are reduced by connecting 
them along a non-straight path, which minimizes an error. This method gives impressive results for a 
wide range of textures, although small scale artifacts are generally no­ticeable. Extending Efros s approach 
to 3D is not straightforward: in 2D, Efros makes the method work by choosing the size of the texture 
tiles according to the size of the patterns in the input image. This cannot be done in 3D since the sharpness 
of the local surface ge­ometry must also be taken into account. Moreover, texture patches cannot be rectangles 
anymore if we want to make the method ef­fective on arbitrary surfaces. Lastly, the orientation of the 
patches in the texture may vary, which makes the search for good .ts much more dif.cult. Our hierarchical 
texture mapping algorithm solves these problems. It improves the quality of connectivity across tex­ture 
patches edges thanks to a local relaxation process. 1.2 Overview The aim of this paper is to provide 
a general texturing method, where the user provides a sample image and an arbitrary 3D mesh, and gets 
texture coordinates on the mesh vertices as an output. The new method belongs to pattern mapping approaches, 
our basic claim being that everything we need is already there in the sample image. As shown by the work 
cited above, such pattern mapping cannot be performed at a single scale: Mapping large parts of the image 
sample will work in almost .at regions, but may cause un­bearable texture distortions over the surface 
s sharp features. More­over, due to the surface s speci.c topology and geometry, texture .tting with 
neighbouring patches cannot necessarily be done at a large scale. We solve these problems by introducing 
a hierarchi­cal texturing algorithm. The idea is to recursively pick up texture patches from the sample 
image and map them onto the surface so that they .t with their neighbors. We do so in a coarse-to-.ne 
manner, a surface region that could not be textured correctly be­ing subdivided into smaller regions 
to improve the mapping. In the worst case, the process ends up at the scale of individual mesh faces, 
where remaining gaps in the texture are .lled as well as possible. Since the texture we get is made of 
patches of different shapes, sizes, and orientations from the input image, no periodicity can be observed. 
Moreover, although there will always be remaining ar­tifacts if we zoom in close enough, visible texture 
discontinuities along texture patch edges can be avoided. This is done by keeping the .tting error under 
a given threshold.  2 Hierarchical mapping 2.1 Preprocessing Before running the algorithm, the mesh, 
which is supposed to be a single manifold surface, is .rst converted into a hierarchy of re­gions. regions 
at each level of the hierarchy form a partition of the surface. Sibling hierarchies of regions have already 
been used in computer graphics [Garland et al. 2001]; We will use the already existing term of face-clusters. 
Each cluster is de.ned by the list of the mesh faces it includes and must have the topology of a disc. 
For our algorithm, there are no further constraints: a face-cluster may possess any number of sub-clusters 
at the next hierarchy level; the cluster s shapes are arbitrary. Their sizes need not be homo­geneous, 
even at a given hierarchy level. Although many methods could be used for building such a hierarchy, we 
give a very simple algorithm for constructing it in Section 3. In addition to the mesh, the user inputs 
an image of the de­sired texture pattern. This image needs not be toroidal, although this property will 
increase the number of possible .ts for texture patches. The user also speci.es at which scale he wants 
the pattern to be used. 2.2 Algorithm The cluster hierarchy is processed in a coarse-to-.ne manner until 
all the mesh has been textured. At each level of the hierarchy, tex­ture mapping is grown from one face-cluster 
to neighbouring ones. Only clusters where no good texture mapping could be found at a given hierarchy 
level are marked for further processing at the next level. The traversal of a level is implemented using 
a priority queue: clusters that have the largest number of already textured neighbour­ing regions are 
processed .rst, so as to increase the chances for .nding compatible solutions for all patches. The process 
starts at the coarsest level of the hierarchy which has a region verifying the .atness criteria. This 
face-cluster is .at­tened in texture space, and a random choice is taken for its texture map, according 
to the user-speci.ed pattern scale. The neighbour­ing clusters of the same hierarchy level are added 
to the priority queue. The process then drains all queues from coarse to .ne hier­archy levels: 1. Take 
a face-cluster from the current queue; 2. Flatten the current cluster; If it is not suf.ciently .at, 
com­puting the mapping at this scale would produce texture distor­tions. In this case, put its children 
into the queue correspond­ing to the next hierarchy level, and go back to step 1; 3. Look in the texture 
for a possible position and orientation of the .attened face-cluster that matches already textured neigh­bors. 
Estimate the error due to texture discontinuities; 4. If this error is below a threshold, map texture 
coordinates onto all polygons in the face-cluster; Else subdivide the region (i.e., put its children 
into the queue corresponding to the next level); 5. Add the non-textured neighbors of the face-cluster 
to the cur­rent queue;  Although a vector .eld could be precomputed or speci.ed to guide pattern orientation 
as was done in [Praun et al. 2000; Wei and Levoy 2001; Turk 2001], we instead let the orientation propagate 
from the .rst texture patch to the others. As shown in Section 5, this simple approach gives good results 
for the set of anisotropic textures we used. 2.3 Problems to be solved In addition to computing a hierarchy 
of regions from an arbitrary mesh, we need algorithms for .attening a given face-cluster and for ef.ciently 
searching for texture patches that .t with already tex­tured neighboring regions. This needs to be done 
ef.ciently, since evaluating the .t, pixel by pixel, with each possible position and orientation could 
easily become a bottleneck in the algorithm.  3 Hierarchy of face-clusters 3.1 Setting up the hierarchy 
Our algorithm needs a hierarchy of regions subdivided into levels, such that (a) the regions of a given 
level constitute a partition of the input mesh; (b) region borders are de.ned using the edges of the 
input mesh; and (c) the regions of the deepest level are the faces of the input mesh. We de.ne the control 
vertices of a face-cluster at level l as the points of its contour shared by at least three regions at 
this level. For each region, we store the list of control vertices and border vertices, as well as pointers 
to its sub-regions and to the neighboring regions of the same hierarchy level (see Figure 1). We present 
two ways of obtaining such hierarchies: subdivision surfaces and general mesh hierarchies. Figure 1: 
Face cluster hierarchies. top row : level1and2ofa hierarchy built from subdivision surfaces (loop scheme). 
bottom row: levels 1 and 2 of a general hierarchy constructed using our method. Subdivision surfaces 
Getting the required hierarchy of re­gions is obvious if the input mesh is a subdivision surface. We 
sim­ply use the parent-child relationship between subdivision patches for setting up the hierarchy. Control 
vertices are de.ned as the ver­tices of the subdivision patches (see Figure 1, top row). Some of the 
results we show in Section 5 were computed with such surfaces. The drawback of this approach is that 
its restricts the class of usable surfaces. General mesh hierarchies For a given input mesh, we pro­ceed 
as follows: The .rst, coarsest face-cluster is made of the entire surface. Then, we recursively subdivide 
a face-cluster into a vari­able number n of sub-regions in the following way: we .rst select n faces 
of the mesh in this region to be used as seeds for grow­ing the sub-regions; for each untreated face 
of the mesh sharing an edge with at least one of the growing sub-regions we merge it to the sub-region 
for which the distance to the seed face is mini­mal. Sub-regions grow inside the current face-cluster 
until paving is completed. The number n of sub-regions is adapted to each face-cluster so as to keep 
a nearly uniform region size at each hierarchy level and to avoid sub-regions with fewer than three control 
vertices. The seeds are randomly chosen while checking that the control vertices of the sub-regions satisfy 
the non alignment property enounced in Section 3.2, for the stable computation of texture coordinates. 
At each level we compute the borders of the sub-regions and possi­bly exchange triangles between neighboring 
sub-regions in order to improve the border s regularity. We also deduce from these borders the control 
vertices of the sub-regions. This method results in a hierarchy of regions of arbitrary shapes and sizes. 
This is not a problem since the .tting method described in Section 4 works for any projected patch shape 
in texture space. We may even say that for some textures, complex region borders tend to give better 
visual results, since tinny .tting errors are less visible when they occur along a non-straight path 
[Efros and Free­man 2001]. 3.2 Flattening a surface region Mapping a portion of the input texture onto 
the mesh requires the de.nition of a local mapping from surface space to texture space. Each time a face-cluster 
is processed (see Section 2), it .rst needs to be .attened. In practice, regions at the coarsest levels 
of the hierarchy are hardly ever used, since there is little chance to unfold them properly, at least 
for closed meshes. They may even have a non-zero topological genus. Similarly, face-clusters whose border 
is made of several disconnected components (such as a cylinder for instance) are discarded. Most local 
.attening methods used in previous work [Bennis et al. 1991; Praun et al. 2000; Wei and Levoy 2001; Turk 
2001] were aimed at .attening the neighborhood of a given mesh vertex. A spiral traversal has been, for 
instance, used for progressively .at­tening the region around a vertex. Our problem is different, since 
we have to .atten regions of totally arbitrary shapes in such a way that their projected border .ts, 
in 2D, with the projected border computed for neighboring regions. This can actually be achieved using 
harmonic maps [Eck et al. 1995] through the resolution of a linear system. We have chosen to develop 
our own approach to avoid the cost of inverting a linear system for each mapped face­cluster, based on 
barycentric coordinates. This approach is de­scribed next. Barycentric coordinates for mesh vertices 
We do not compute any analytical expression for the mapping function of a region of the mesh to texture 
space. It is rather de.ned by a recur­sive procedure which relies on the face-cluster hierarchy: (1) 
Suppose that we know the coordinates for control points P0...Pn of a face-cluster in texture space. We 
de.ne the relative positions of the mapped control points P/0...P/N of its sub-faces us­ ing barycentric 
coordinates (see Fig. 2). Let us call T(l,j)the vector of coordinates of the control points of a patch 
j at level l in tex­ ture space, and C(l,)j bary the matrix of its barycentric coordinates with respect 
to the parents control points. We set: T (l,j)bary XT (l-1,Parent(l,=Cl,jj)) (1) (2) When projecting 
a face-cluster in texture-space however, we need to .x the texture coordinates of its control vertices. 
These tex­ture coordinates are precomputed by constructing a control polygon in texture space. As any 
face-cluster may be a starting point for re­cursive texture mapping, such a control polygon is computed 
for each face-cluster (see right parts of Figures 2 and 3). The control polygon of a face-cluster is 
obtained by measuring the relative distance and angles of the polygon formed by its con­trol vertices 
in 3D space. Then we build, in texture space, a closed polygon with the same edge lengths, and angles 
as close as possible to the original ones. Flat polygons in 3D space are shape-invariant through this 
transformation. Others are all the more deformed that the face-cluster contour is large and non .at (see 
examples in Fig­ures 2 and 3). 0.0 P2 P2 0.0 1.0 P 0 0.058 0P 1P P 2 P1 0P 1.0 0.0 0.0 0.546 0.288 0.166 
1P 0P 0.0 1.0 0.0 0.505 0.437 0.632 -0.246 0.612 2P 1P Figure 2: Top left: a face-cluster de­.ned 
by 3 control points P0,P1 and P2 P2, one of its sub-faces out-lined in blue. Top right: the corresponding 
con­ trol polygon in texture space, and the barycentric coordinates of the control vertices of sub-faces, 
in blue. Right: the resulting .attened patch in texture P0 1P 1P 0P space. 2P The barycentric coordinates 
of the sub-vertices in texture space are computed in three different ways depending on their nature: 
(1) Vertices that are already vertices of the parent face-cluster get a zero coordinate everywhere except 
on that vertex. (2) For vertices that belong to an edge of the parent face-cluster, we compute the average 
value of the mesh normal along this edge and project the 3D polygon formed by the parent face-cluster 
s control points onto the plane de.ned by this normal and the current vertex. Barycen­tric coordinates 
of the vertex are computed with respect to this pro­jected polygon. (3) Finally, for vertices lying inside 
the parent face­cluster, we use the orientation of the mean square plane de.ned by the parent face-cluster 
border: we project the polygon formed by the parent face-cluster s control points and the current vertex 
onto this plane. We then compute the barycentric coordinates using this projected polygon. P1P2 P 2 P1 
P 3 P3 P0 P 0 Figure 3: A face-cluster in surface-space (left) with its 4 control points, and its projection 
in texture space using our procedural map­ping algorithm (right). Only the control points of this cluster 
and the barycentric coordinates of clusters below are needed to produce the mapping. This procedural 
de.nition has some advantages against explicit mapping functions : First it lets us continuously deform 
the patch in texture space by moving one of its control vertices. Secondly, the mapping of a face-cluster 
is very fast and straightforward what­ever the geometry of its control polygon in texture space (this 
is an advantage over harmonic maps), which lets us modify the mapping at an optimal cost. Finally the 
computation of texture coordinates is very stable, provided that the control points the barycentric co­ordinates 
are refering to do not form a very .at polygon (this pro­duces very high or even in.nite values that 
accumulate errors down the recursive computation of texture coordinates). In any case, we may choose 
among the possible vertices of the parent control poly­gon the base that gives the smallest barycentric 
coordinates, which is all the more easy that the face-clusters are not too much elon­gated. This condition 
is intrinsically veri.ed by subdivision sur­faces, whereas it is enforced for general surface hierarchies 
during their construction. Flattening error criterion For a given face-cluster, let r be the ratio between 
its area in surface space and its area in texture space. We then de.ne the .attening error of a face-cluster 
as Eflat by: E flat =lr -1l By construction Eflat is zero for .at face-clusters and grows with surface 
deformation. It is used as an heuristic measure of .attening error in step 2 of the algorithm of Section 
2.  4 Texture patches .tting This section explains how to texture a face-cluster which has some already 
textured neighbors while ensuring the match of the texture along common edges. The next paragraphs describe 
the four steps of the algorithm: we .rst extract a mask that represents the (already .xed) texture surrounding 
the current patch. We then look for the location in the texture image that best .ts this mask. In a third 
step, we obtain texture coordinates through a recursive computation. Fi­nally, we slightly tune texture 
coordinates along the edges using a local relaxation. This gives us the .nal mapping and an output .tting 
error for the patch. 4.1 Extraction of the mask For most texture patterns, matching color values of pixels 
along common edges would not be suf.cient for obtaining a good .t. We rather match the texture over the 
union of narrow bands extracted from textured neighbouring regions, along common edges (see Fig­ure 5, 
left). This is a simple way for taking into account derivatives and local characteristics of the texture 
across regions borders. The extraction of the mask from neighboring texture patches re­quires being able 
to cut from these patches a band of approximately constant width. Without this ability, the algorithm 
would assign a nonuniform importance to the quality of texture .tting at different locations along the 
edge. In order to achieve this, we equip the con­trol vertices of all face-clusters in the hierarchy 
with so called topo­logical barycentric coordinates computed in surface space (see Fig­ure 4, left). 
Note that these coordinates are different from the texture space barycentric coordinates de.ned in Section 
3.2 (which will serve for recursive texture mapping) and are related to the topology of the surface mesh, 
hence their name. P2P 0.0 2  0.0 0.5 0.5 0.2 0.5 0.5 0.0 Figure 4: Left: topological coordinates assigned 
to control points of a sub-cluster (see text). Right: points in the patch for which the position value 
(in magenta) toward the edge is less than 0.2 are kept to obtain the mask (in cyan). j) Let us call C(l, 
 topo the matrix of topological coordinates for the control vertices of a face-cluster de.ned with respect 
to the parent s control vertices. To extract a band along the edge of a given face­cluster, we need such 
coordinates for all mesh faces (i.e., for control points at the bottom of the hierarchy), but in the 
base of the current face-cluster j0,of level l0. Let us call these coordinates M(l,j)for the face-cluster 
j at level l. We have: M(l,j) Ctopo (l,j)XM(l-1,Parent(l,M(l0,j0) =j)) and =Identity This gives us a 
recursive way of computing Mmaxlevel,j. To extract the band, we then de.ne the position value of a ver­tex 
with respect to an edge of the face-cluster j0 as being the sum of its topological barycentric coordinates 
with respect to j0, except for the coordinates associated with the two vertices that de.ne the edge. 
For instance, in Figure 4, right, each face has three topologi­cal barycentric coordinates respectively 
associated with P0, P1, and P2; the position value of a vertex with respect to the edge (P1,P2) is given 
by its .rst coordinate. The band is obtained by selecting the portion of the base mesh triangles for 
which the position value is less than a given threshold (typically 0.2). For subdivision surfaces, obtaining 
topological barycentric coor­ j)dinates C(l, topo with respect to the parent face-cluster is straightfor­ward, 
since control vertices of the sub-faces of a given face-cluster are always located at the middle of an 
edge. Values of the topo­logical coordinates Cl,j are thus set to 1 with respect to the two 2 extremities 
of the edge and 0 with respect to the other control ver­tices of the parent face-cluster. For a general 
face-cluster hierarchy, we keep the same policy for control points located on edges. For other ones, 
we use a set of coordinates proportional to the respective distances to the control points of the parent 
face-cluster. This ap­parently complex solution actually allows us to extract a band from an edge of 
any face-cluster, regardless of its shape and deformation. The mask I of the current region is de.ned 
as the union of the bands from neighbouring already textured face-clusters. In practice we obtain it 
using off-screen rendering (see Figure 5,right).  4.2 Search for the best match The second step of texture 
patch .tting is to look for the portion of the texture image that best matches the mask. We operate entirely 
in texture space. To do this, we try to .nd the position and orientation of the mask such that it .ts 
best in the underlying texture. Once this position found, the adequate part of the texture adjacent to 
the mask is used to texture the current face-cluster. Best mask position for a given orientation Let 
T (x), x E [0,1F2, be the texture sample, .rst supposed to be toroidal. Let J be the support function 
of the current mask I, that is, J(x)=1or0 depending whether x lies in the union of bands from neighbouring 
textured patches or not. Note that J(x)T (x)=I(x). For a given translation x0 of I over T , we de.ne 
the mean square error between the mask I and the sample T by: E(x0)=.J(x)(I(x)-T (x +x0))2 x We are thus 
looking for points x0 for which E(x0)is minimal. We have: E(x0)=. I(x)2 -2 . I(x)T (x +x0)+. T (x +x0)2 
J(x) J(x)t0 t0 J(x)t0 But J cancels in the .rst two terms since I is supposed to be zero out of the mask 
support. Moreover, if we denote by f og the correlation operation2 between two images f and g, we can 
write: 2 E(x0)=.I(x)-2(I oT)(x0)+(J o(T2 ))(x0) x () =.I(x)2 +-2(I oT )+J o(T 2)(x0)(2) x 2The correlation 
between two images f and g is de.ned by: (f og)(x0) .xf (x)g(x +x0). Figure 5: To texture a region that 
has already tex­tured neighbors (upper-left, in red), we build a mask image (at right) from the texture 
of these neighbors (upper-middle, light red). We then look over the texture sample for candidate locations 
that match this mask (upper-middle, dark red and  white). The best location (in white) is used to de.ne 
the texture of the face-cluster (upper-right). Computing this term directly would lead to a huge amount 
of computations, since the complexity would be the square of the number N of pixels in the texture. Fortunately, 
the correlation of two functions f and g can be computed O(N log N)by moving to Fourier space. We use 
the fast Fourier transform (FFT) (see [Press et al. 1992]) for computing the Fourier transform F(f )and 
the con­ jugate Fourier transform F(g). We then have: f og =F-1 (F(f ).F(g)) Looking at Equation (2), 
we thus need to compute the Fourier transforms of T , T 2, I and J, build the image of E and look into 
it for values near 0. Fortunately, T and T2 being constants, their Fourier transforms need to be computed 
only once for texturing an entire object. If the texture sample is not toroidal, the technique above 
still works. However, the position found must not make the new texture patch cross the border of the 
image. This reduces the number of possible matches, especially when the mapping scale is large. Best 
mask position and orientation The computation above .nds the best mask position for a given image orientation. 
In prac­ tice, we are also looking for the best .t through various orientations. For this, we precompute 
and store the Fourier transforms of rotated versions of T and T2 for a .xed number of angles (19 in our 
im­ plementation). During the texturing process, we compute the best position for the mask for each orientation, 
and select the best result. (A random orientation is used when rendering the mask, in order to avoid 
getting trivial solutions such as the direct neighbor of a sin­ gle neighbouring patch). The overall 
cost of each .tting is still the cost of a single Fourier transform for I and J, so the computation remains 
ef.cient. In practice, all orientations of T and T2 are precomputed using the graphics hardware. Then 
we precompute and store their Fourier transforms. During texturing, I and J corresponding to the current 
region to texture are computed using off-screen rendering of the mask. I is obtained by reading the color 
values, whereas J comes from the depth values. From these two images we compute .I(x) 2 and the Fourier 
transforms F(I)and F(J), and thus get the error function E using equation 2 for all orientations. The 
computations have been presented up to now without paying attention to the value domain for functions 
T and I. We work in hsv color space. We .nd that this gives better results than working in rgb space 
or gray levels, especially for textures like the one in Figure 5. Note that only T,T2 and I (but not 
J) need to be turned into hsv in equation 2.  4.3 Recursively mapping texture coordinates From the position 
of the mask in the texture sample produced by the previous calculation, we deduce the position in texture 
space of the control polygon of the current face-cluster. We still need to assign texture coordinates 
to the vertices of the faces of the base mesh that belong to it. This is done by recursively applying 
equation (1) down to the mesh triangles. Careful attention must however be paid to the following issues: 
(1) some vertices of the base-mesh are reached more than once dur­ing the recursive traversal of the 
hierarchy (typically vertices on the boundary of two sub-clusters). Although the coordinates of these 
vertices are computed using the same original set of coordinates for the cluster at the top of the hierarchy, 
their .nal values may differ slightly: due to the nonlinearity of the procedural mapping function de.ned 
in section 3.2, their position may depend on the sequence of parents through which they have been computed. 
This may cause what we call in-cluster cracks. (2) this difference of relative posi­tion also appears 
also on vertices of the edges shared with already computed neighbors of the face-cluster. We call these 
errors border­cracks. Both problems are ef.ciently solved as follows: an array of texture coordinates 
for the vertex of the base mesh is initialized with zero values; then, the contribution of all vertices 
is added dur­ing the recursive traversal of the hierarchy and averaged to obtain a unique value of texture 
coordinate for each vertex. This solves the in-cluster cracks problem. The second problem is solved by 
com­puting, for each shared vertex, the texture coordinates it would take if computed from the neighboring 
patch positioned exactly next to the current one. These values are used during the recursive traver­sal 
of the current face-cluster in replacement of the locally com­puted values, which exactly adapts the 
texture deformation to .t the mapped edge of already textured neighbors. The resulting small de­formation 
is automatically distributed inside the face-cluster thanks to the use of the barycentric coordinates. 
 4.4 Local relaxation along edges and output error After mapping the texture onto a face-cluster, some 
discontinuities with the texture on the neighboring face-clusters usually persist. This happens because 
a perfect solution for matching the mask of the neighbors does not always exist, especially when more 
than one neighbor is textured. Even a small discontinuity may become no­ticeable, due to the particular 
sensitivity of the human eye. Consequently, we deform, in texture space, the edges that are shared with 
neighboring patches in order to minimize discontinu­ities along them. This is done by recursively visiting 
the vertices of sub-clusters located along the edge, and moving them in order to minimize a local error 
criterion. This local error is de.ned as the sum of differences per pixel, along the common edge, between 
the textures of the current and neighbouring patches, weighted by a function representing the in.uence 
of the current vertex, depicted in Figure 6, right. The search for a new position for each vertex is 
done using a recursive greedy algorithm which randomly moves the vertex within a smaller and smaller 
area depending on the hierarchy level. An similar algorithm is also applied to the vertices of the con­trol 
polygon in order to minimize error at these particular points. In any case, we only move vertices of 
the current face-cluster, not those of its already textured neighbors. Note that the side effect of moving 
the control vertices of a face­cluster in texture space is to deform the texture mapped on the sur­face. 
This deformation is seamlessly distributed inside the face­cluster thanks to our recursive mapping method. 
  5 Results Multi-scale pattern mapping works surprisingly well for a wide va­riety of textures. Most 
of the texture samples used in this section 00000000P 0 00000000P 0 111111111 111111111 000000000 00000000Texture 
0 1.0 22 Texture 111111111 111111111 000000001111111101 000000000 111111111 000000001111111101 000000000 
 P 111111111 000000001111111101 000000000 1 111111111 000000001111111101 000000000 c 111111111 00000000P 
0 00000000P 0 111111111 111111111 000000001111111101 000000000 0 111111111 000000001111111101 000000000 
 a f 111111111 P d P0 e g 000000001111111101 000000000 111111111 b 000000001111111101 000000000 111111111 
000000001111111101 000000000 P 111111111 000000001111111101 000000000 111111111 2P1 P1PP P 0 12 000000000111111111 
000000000 111111111 P Figure 6: Local relaxation along the edge (P0,P2)of a face-cluster: Two successive 
steps of discontinuity minimization are shown. Left : point P1 / is .rst allowed to move in the red circle. 
Once its new position is found, we look for vertices of the next level (Middle ); // is concerned and 
its new position is searched within a smaller region, and so on. In pink, we show the region of the patch 
onto which the deformation of the texture will occur due to the vertex displacement. Right : weighting 
function used when measuring the discontinuity at a point with respect to an edge. were taken from previous 
work [Efros and Leung 1999; Neyret and Cani 1999; Ashikhmin 2001; Ying et al. 2001; Wei and Levoy 2001; 
Turk 2001; Efros and Freeman 2001], which simpli.es the comparison of results. The table below shows 
typical computation times. Object (type) Polygons Related .gure, Time (min) and texture size Sphere (S) 
3072 Fig. 10, 256 X256 1 Bunny (M) 2962 Fig. 9,128 X128 2 Pumpkin (M) 10 000 Fig. 8,128 X128 21 Octopus 
(S) 34 176 Fig. 9, 256 X256 29 Triceratops (M) 5 660 Fig. 10, 128 X128 30 Figure 7: Computation times: 
Types (S) and (M) respectively indi­cate a subdivision surface or a general mesh. Using our method is 
easy: the user just provides a mesh, a tex­ture pattern, and speci.es the scale at which the pattern 
should be mapped, as illustrated in Figure 8. He may also choose the .rst texture patch for controlling 
the global orientation of the patterns over the surface. The hierarchical texture mapping computation 
then takes from a few minutes to a few tens of minutes on standard graphics workstations. Since the texturing 
method will be able to save time by mapping .at regions at a very coarse scale (if the tex­ture is not 
too constrained yet), better results are obtained by start­ing texture mapping in such regions. Figure 
8: The user controls the scale at which the texture pattern is mapped. Right : face-clusters used by 
the algo­rithm during the mapping of the top­right pumpkin. Note the appearent continuity of the result, 
and the various sizes (and hierarchy levels) of the clusters used. 5.1 Isotropic versus anisotropic patterns 
The method works with no problem for isotropic patterns such as those used in [Neyret and Cani 1999; 
Ying et al. 2001]. Results are depicted in .gure 9. Figure 9: Mapping isotropic texture patterns. In 
the case of anisotropic patterns, increasing the width of the band used for texture .tting along patch 
boundaries may be neces­sary to better capture and propagate pattern orientation. Once this parameter 
was tuned, we had no problem generating textures such as bark, wood, or text with a coherent orientation. 
See Figure 10. Figure 10: Our method succeeds in capturing and propagating the main orientation of anisotropic 
texture patterns. 5.2 Highly structured patterns Our method is particularly adapted to highly structured 
patterns. Some results are depicted in Figure 11. As with previous methods [Neyret and Cani 1999; Efros 
and Freeman 2001], the minimal size of the texture patches should not be smaller than the pattern size, 
or we may loose the pattern struc­ture. A solution to this problem, which would occur if we mapped the 
pattern at a very large scale relatively to the surface geometry, is suggested in the next section. 
  6 Conclusion The hierarchical texture mapping algorithm we have introduced demonstrates that an arbitrary 
surface can be textured by directly mapping patches of various sizes, shapes and orientations from an 
input image. This new method combines many advantages: The texture patches mapped at a large scale propagate 
the global struc­ture of the pattern, while texturing at a smaller scale adequately .lls the gaps and 
reduces texture distortion over sharp geometric fea­tures. Combining these different scales increases 
ef.ciency, since texture mapping at the .nest scale will only be done in speci.c sparse regions. Moreover, 
the algorithm directly outputs texture coordinates for the triangles of the initial mesh, which minimizes 
memory requirement (no new texture needs to be stored) and eases rendering (which makes it preferable 
to methods like lapped tex­tures at equivalent image quality). Lastly, the approach works for a wide 
range of textures, regardless of their isotropy, frequency of variation and inner structure. The method 
will generally leave small texture .tting errors across patch boundaries. For many applications, this 
is not a prob­lem: just as with surface approximations using triangles, the tex­tures we generate should 
be viewed from at least a given minimal distance. Texture discontinuities will indeed appear in very 
close views, together with other artifacts such as tangent discontinuities due to the discretization 
into triangles. If we need to avoid these discontinuities, an idea would be to re.ne the mesh using a 
subdi­vision scheme, in order to map texture on sub-triangles. However, using smaller texture patches 
is not suf.cient for improving the vi­sual quality of the mapping: while doing so, the low frequency 
structure of the texture patterns may be lost. A solution would then be to combine our multi-scale mapping 
algorithm with the texture transfer methods of Efros [Efros and Freeman 2001] and Hertz­mann [Hertzmann 
et al. 2001]. The texture found to .t badly at a large scale would serve as a reference image to guide 
texture map­ping at a .ner scale, thus maintaining the global coherency of the patterns. Implementing 
this extension would only demand a change to the error measure, in order to take the reference image 
into ac­count. Another idea would be to rely on texture transfer tech­niques for offering more user control, 
as .rst suggested by Ashikhmin [Ashikhmin 2001]. The user would paint the desired av­erage colors directly 
on the initial mesh, and texture mapping would follow these choices by picking the appropriate texture 
patches in the sample image. Another extension of our method would consist of locally con­trolling the 
scale of the texture along the surface, as many natural objects tend to produce a similar patterns at 
multiple scales. Also, mapping more than one texture while maintaining smooth transi­tions would further 
enrich the method. 7 Acknowledgements Special thanks to John F. Hugues for re-reading the last version 
of the paper and for his suggestions about possible improvements. Thanks also to Rob Jagnow for the pumpkin 
model and to the re­viewers for their comments.  References ASHIKHMIN, M. 2001. Synthesizing natural 
textures. 2001 ACM Symposium on Interactive 3D Graphics (March), 217 226. ISBN 1-58113-292-1. BENNIS, 
C., V ´ ESIAS, G., AND GAGALOWICZ, A. 1991. Piece- EZIEN, J.-M., IGL ´ wise surface .attening for non-distorted 
texture mapping. In Computer Graphics (SIGGRAPH 91 Proceedings), T. W. Sederberg, Ed., vol. 25, 237 246. 
EBERT, D., MUSGRAVE,F., PEACHEY, D., PERLIN, K., AND WORLEY, S., Eds. 1998. Texturing and Modelling: 
A procedural approach. Morgan Kaufmann Pub­lishers. ECK, M., DEROSE,T., DUCHAMP,T., HOPPE, H., LOUNSBERY, 
M., AND STUET-ZLE, W. 1995. Multiresolution analysis of arbitrary meshes. In SIGGRAPH 95 Conference Proceedings, 
Addison Wesley, R. Cook, Ed., ACM SIGGRAPH, 173 182. EFROS, A. A., AND FREEMAN, W. T. 2001. Image quilting 
for texture synthesis and transfer. Proceedings of SIGGRAPH 2001 (August), 341 346. EFROS, A., AND LEUNG, 
T. 1999. Texture synthesis by non-parametric sampling. In International Conference of Computer Vision, 
vol. 2, 1033 1038. GARLAND, M., WILLMOTT,., AND HECKBERT, P.2001.Hierarchicalfaceclustering on polygonal 
surfaces. In ACM Symposium on Interactive 3D Graphics. HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, 
B., AND SALESIN,D. H. 2001. Image analogies. Proceedings of SIGGRAPH 2001 (August), 327 340. ISBN 1-58113-292-1. 
´ LEVY, B., AND MALLET, J.-L. 1998. Non-distorted texture mapping for sheared triangulated meshes. Proceedings 
of SIGGRAPH 98 (July), 343 352. ISBN 0­89791-999-8. Held in Orlando, Florida. L´ EVY, B. 2001. Constrained 
texture mapping for polygonal meshes. Proceedings of SIGGRAPH 2001 (August), 417 424. ISBN 1-58113-292-1. 
MAILLOT, J., YAHIA, H., AND VERROUST, A. 1993. Interactive texture mapping. In Computer Graphics (SIGGRAPH 
93 Proceedings), J. T. Kajiya, Ed., vol. 27, 27 34. NEYRET,F., AND CANI, M.-P. 1999. Pattern-based texturing 
revisited. Proceedings of SIGGRAPH 99 (August), 235 242. PRAUN, E., FINKELSTEIN, A., AND HOPPE, H. 2000. 
Lapped textures. Proceedings of SIGGRAPH 2000 (July), 465 470. ISBN 1-58113-208-5. PRESS,TEUKOLSKI,VETTERLING, 
AND FLANNERY. 1992. Numerical Recipes in C. Cambridge University Press. TURK, G. 1991. Generating textures 
for arbitrary surfaces using reaction-diffusion. In Computer Graphics (SIGGRAPH 91 Proceedings), T. W. 
Sederberg, Ed., vol. 25, 289 298. TURK, G. 1992. Re-tiling polygonal surfaces. In Computer Graphics (SIGGRAPH 
92 Proceedings), E. E. Catmull, Ed., vol. 26, 55 64. TURK, G. 2001. Texture synthesis on surfaces. Proceedings 
of SIGGRAPH 2001 (August), 347 354. ISBN 1-58113-292-1. WALTER, M., FOURNIER, A., AND MENEVAUX, D. 2001. 
Integrating shape and pattern in mammalian models. Proceedings of SIGGRAPH 2001 (August), 317 326. ISBN 
1-58113-292-1. WEI, L.-Y., AND LEVOY, M. 2000. Fast texture synthesis using tree-structured vector quantization. 
Proceedings of SIGGRAPH 2000 (July), 479 488. ISBN 1-58113­208-5. WEI, L.-Y., AND LEVOY, M. 2001. Texture 
synthesis over arbitrary manifold sur­faces. Proceedings of SIGGRAPH 2001 (August), 355 360. ISBN 1-58113-292-1. 
YING, L., HERTZMANN, A., BIERMANN, H., AND ZORIN, D. 2001. Texture and shape synthesis on surfaces. In 
Eurographics Rendering Workshop 2001, Springer Wein, S. Gortler and K. Myszkowski, Eds., Eurographics, 
301 312. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566636</article_id>
		<sort_key>681</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Improving noise]]></title>
		<page_from>681</page_from>
		<page_to>682</page_to>
		<doi_number>10.1145/566570.566636</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566636</url>
		<abstract>
			<par><![CDATA[Two deficiencies in the original Noise algorithm are corrected: second order interpolation discontinuity and unoptimal gradient computation. With these defects corrected, Noise both looks better and runs faster. The latter change also makes it easier to define a uniform mathematical reference standard.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[procedural texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39034493</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>551861</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EBERT, D. ET AL. 1998. Texturing and Modeling; A Procedural Approach, Second Edition. AP Professional, Cambridge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>208249</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FOLEY, J. ET AL. 1996. Computer Graphics: Principles and Practice. Addison-Wesley, Reading.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., ACM SIGGRAPH 84 conference, course in "Advanced Image Synthesis."]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1985. An Image Synthesizer. In Computer Graphics (Proceedings of ACM SIGGRAPH 85), 24. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. AND HOFFERT, E. 1989. Hypertexture. In Computer Graphics (Proceedings of ACM SIGGRAPH 89), 23, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>534134</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. 1990. The RenderMan Companion: A Programmer's Guide to Realistic Computer Graphics. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improving Noise Ken Perlin Media Research Laboratory, Dept. of Computer Science, New York University 
perlin@cat.nyu.edu ABSTRACT Two deficiencies in the original Noise algorithm are corrected: second order 
interpolation discontinuity and unoptimal gradient computation. With these defects corrected, Noise both 
looks better and runs faster. The latter change also makes it easier to define a uniform mathematical 
reference standard. Keywords procedural texture 1 INTRODUCTION Since its introduction 17 years ago [Perlin 
1984; Perlin 1985; Perlin and Hoffert 1989], Noise has found wide use in graphics [Foley et al. 1996; 
Upstill 1990]. The original algorithm, although efficient, suffered from two defects: second order discontinuity 
across coordinate-aligned integer boundaries, and a needlessly expensive and somewhat problematic method 
of computing the gradient. We (belatedly) remove these defects. 2 DEFICIENCIES IN ORIGINAL ALGORITHM 
As detailed in [Ebert et al 1998], Noise is determined at point (x,y,z) by computing a pseudo-random 
gradient at each of the eight nearest vertices on the integer cubic lattice and then doing splined interpolation. 
Let (i,j,k) denote the eight points on this cube, where i is the set of lower and upper bounding integers 
on x: {| x |,| x |+1}, and similarly j = { | y |,| y |+1} and k = { | z |,| z |+1}. The eight gradients 
are given by gi,j,k = G[P[P[P[i]+j]+k]] where precomputed arrays P and G contain, respectively, a pseudo-random 
permutation, and pseudo-random unit-length gradient vectors. The successive application of P hashes each 
lattice point to de-correlate the indices into G. The eight linear functions gi,j,k . (x-i,y-j,z-k) are 
then trilinearly interpolated by s(x-| x |), s(y-| y |) and s(z-| z |), where s(t) = 3t2-2t3 . The above 
algorithm is very efficient but contains some deficiencies. One is in the cubic interpolant function's 
second derivative 6-12t, which is not zero at either t=0 or t=1. This non­zero value creates second order 
discontinuities across the coordinate-aligned faces of adjoining cubic cells. These discontinuities become 
noticeable when a Noise-displaced surface Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 is shaded; then the surface normal (which is itself a 
derivative operator) has a visibly discontinuous derivative (Figure 1a).  Figure 1b: Noise-displaced 
superquadric with new interpolants The second deficiency is that whereas the gradients in G are distributed 
uniformly over a sphere, the cubic grid itself has directional biases, being shortened along the axes 
and elongated on the diagonals between opposite cube vertices. This directional asymmetry tends to cause 
a sporadic clumping effect, where nearby gradients that are almost axis-aligned, and therefore close 
together, happen to align with each other, causing anomalously high values in those regions (Figure 2a). 
  Figure 2b: High-frequency Noise, with new gradient distributions 3 MODIFICATIONS The above deficiencies 
are addressed as follows. 3t2-2t3 is replaced by 6t5-15t4+10t3 , which has zero first and second derivatives 
at both t=0 and t=1. The absence of artifacts can be seen in Figure 1b. The key to removing directional 
bias in the gradients is to skew the set of gradient directions away from the coordinate axes and long 
diagonals. In fact, it is not necessary for G to be random at all, since P provides plenty of randomness. 
The corrected version replaces G with the 12 vectors defined by the directions from the center of a cube 
to its edges: (1,1,0),(-1,1,0),(1,-1,0),(-1,-1,0), (1,0,1),(-1,0,1),(1,0,-1),(-1,0,-1), (0,1,1),(0,-1,1),(0,1,-1),(0,-1,-1) 
 Gradients from this set are chosen by using the result of P, modulo 12. This set of gradient directions 
was chosen for two reasons: (i) it avoids the main axis and long diagonal directions, thereby avoiding 
the possibility of axis-aligned clumping, and (ii) it allows the eight inner products to be effected 
without requiring any multiplies, thereby removing 24 multiplies from the computation. To avoid the cost 
of dividing by 12, we pad to 16 gradient directions, adding an extra (1,1,0),(-1,1,0),(0,-1,1) and (0,-1,-1). 
These form a regular tetrahedron, so adding them redundantly introduces no visual bias in the texture. 
The final result has the same non-directional appearance as the original distribution but less clumping, 
as can be seen in Figure 2b. 4 PERFORMANCE In a timing comparison (C implementations on the Intel optimizing 
compiler running on a Pentium 3), the new algorithm runs approximately ten percent faster than the original. 
The cost of the extra multiplies required to compute the three corrected interpolants is apparently outweighed 
by the savings from the multiplies no longer required to compute the eight inner products. Examination 
of the assembly code indicates that the Intel processor optimizes by pipelining the successive multiplies 
of the three interpolant calculations since no memory fetches are required within this block of computations. 
Rather than use a 12-entry table to avoid inner product multiples, the G table can also be expanded and 
used to replace the last lookup into P. Whether this method is more efficient is processor dependent. 
For example, 3D inner products are single operations on both nVidia and ATI pixel processors. 5 CONCLUSIONS 
The described changes result in an implementation of Noise which is both visually improved and computationally 
more efficient. Also, with the pseudo-random gradient table removed, the only pseudo-random component 
left is the ordering of the permutation table P. Once a standard permutation order is determined, it 
will at last be possible to give a uniform mathematical definition for the Noise function, identical 
across all software and hardware environments.  ACKNOWLEDGEMENTS Thanks to the reviewers for constructive 
criticisms that improved the paper, to Denis Zorin for his invaluable suggestions, and to Nathan Wardrip-Fruin 
and Chris Poultney, who helped greatly in the rush of production. References EBERT, D. ET AL. 1998. 
Texturing and Modeling; A Procedural Approach, Second Edition. AP Professional, Cambridge. FOLEY, J. 
ET AL. 1996. Computer Graphics: Principles and Practice. Addison-Wesley, Reading. PERLIN, K., ACM SIGGRAPH 
84 conference, course in "Advanced Image Synthesis." PERLIN, K. 1985. An Image Synthesizer. In Computer 
Graphics (Proceedings of ACM SIGGRAPH 85), 24. 3. PERLIN, K. AND HOFFERT, E. 1989. Hypertexture. In Computer 
Graphics (Proceedings of ACM SIGGRAPH 89), 23, 3. UPSTILL, S. 1990. The RenderMan Companion: A Programmer's 
Guide to Realistic Computer Graphics. Addison-Wesley.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566637</section_id>
		<sort_key>683</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Graphics hardware]]></section_title>
		<section_page_from>683</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP31031149</person_id>
				<author_profile_id><![CDATA[81100186713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kilgard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566638</article_id>
		<sort_key>683</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The SAGE graphics architecture]]></title>
		<page_from>683</page_from>
		<page_to>692</page_to>
		<doi_number>10.1145/566570.566638</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566638</url>
		<abstract>
			<par><![CDATA[The Scalable, Advanced Graphics Environment (SAGE) is a new high-end, multi-chip rendering architecture. Each single SAGE board can render in excess of 80 million fully lit, textured, anti-aliased triangles per second. SAGE brings high quality antialiasing filters to video rate hardware for the first time. To achieve this, the concept of a frame buffer is replaced by a fully double-buffered sample buffer of between 1 and 16 non-uniformly placed samples per final output pixel. The video output raster of samples is subject to convolution by a 5x5 programmable reconstruction and bandpass filter that replaces the traditional <sc>RAMDAC</sc>. The reconstruction filter processes up to 400 samples per output pixel, and supports any radially symmetric filter, including those with negative lobes (full Mitchell-Netravali filter). Each SAGE board comprises four parallel rendering sub-units, and supports up to two video output channels. Multiple SAGE systems can be tiled together to support even higher fill rates, resolutions, and performance.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anti-aliasing]]></kw>
			<kw><![CDATA[frame buffer algorithms]]></kw>
			<kw><![CDATA[graphics hardware]]></kw>
			<kw><![CDATA[graphics systems]]></kw>
			<kw><![CDATA[hardware systems]]></kw>
			<kw><![CDATA[rendering hardware]]></kw>
			<kw><![CDATA[video]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP94030161</person_id>
				<author_profile_id><![CDATA[81100240083]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382426</person_id>
				<author_profile_id><![CDATA[81408593744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naegle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sun Microsystems, UMPK27-101, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AKELEY, K. 1993. RealityEngine Graphics. In Proceedings of SIGGRAPH 1993, ACM Press / ACM SIGGRAPH, New York. Kajiya, J., Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 109-116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AKELEY, K. 2001. Course notes of CS448A, taught fall semester at Stanford University. URL: http://graphics.stanford.edu/courses/cs448a-01-fall/lectures/lecture5/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[COOK, R, CARPENTER, L, and CATMULL, E. 1987. The Reyes Image Rendering Architecture. In Computer Graphics (Proceedings of SIGGRAPH 87), 21 (4) ACM, 95-102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DEERING, M., WINNER, S., SCHEDIWY, B., DUFFY, C and HUNT, N. 1988. The Triangle Processor and Normal Vector Shader: A VLSI system for High Performance Graphics. In Computer Graphics (Proceedings of SIGGRAPH 88), 22 (4) ACM, 21-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192194</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DEERING, M., SCHLAPP, S., and LAVELLE, M. 1994. FBRAM: A new Form of Memory Optimized for 3D Graphics. In Proceedings of SIGGRAPH 1994, ACM Press / ACM SIGGRAPH, New York. Glassner, A., Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 167-174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DOMIN&#201;, S. 2001. OpenGL Pixel Formats and Multisample Anti-aliasing. URL: http://developer.nvidia.com/docs/IO/1594/ATT/PixelformatsAndMultisample.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258714</ref_obj_id>
				<ref_obj_pid>258694</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[EYLES, J., MOLNAR, S., POULTON, J., GREER, T., LASTRA, A., ENGLAND, N., and WESTOVER, L. 1997. PixelFlow: The Realization. '97 Eurographics/SIGGRAPH Workshop on Graphics Hardware (Los Angeles, CA, Aug 3-4, 1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GLASSNER, A. 1995. Principles of Digital Image Synthesis. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P., and AKELEY, K. 1990. The Accumulation Buffer: Hardware Support for High-Quality Rendering. In Computer Graphics (Proceedings of SIGGRAPH 90), 24 (4) ACM, 309-318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378514</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MITCHELL, D., and NETRAVALI, A. 1988. Reconstruction Filters in Computer Graphics. In Computer Graphics (Proceedings of SIGGRAPH 88), 22 (4) ACM, 221, 228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S., COX, M., ELLSWORTH, D., and FUCHS, H. 1994. A Sorting Classification of Parallel Rendering, IEEE Computer Graphics and Applications, July 1994, 23-32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258871</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[MONTRYM, J., BAUM, D., DIGNAM, D., and MIGDAL, C. 1997. InfiniteReality: A Real-Time Graphics System. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, New York. Whitted, T., Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 293-302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383273</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[STOLL, G., ELDRIDGE, M., PATTERSON, D., WEBB, A., BERMAN, S., LEVY, R., CAYWOOD, C., TAVEIRA, M., HUNT., S., and HANRAHAN, P. 2001. Lightning-2: A High Performance Display Subsystem for PC Clusters. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, New York. E. Fume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 141-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[TAROLLI, G.1999. Real-Time Cinematic Effects on the PC: The 3Dfx T-Buffer. Hot 3D presentation in Eurographics/SIGGRAPH Workshop on Graphics Hardware 1999, IEEE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>624400</ref_obj_id>
				<ref_obj_pid>623296</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[TREMBLAY, M., CHAN, J., CHAUDHRY, S., CONIGLIARO, A., TSE, S. S., 2000. The MAJC Architecture; A Synthesis of Parallelism and Scalability. IEEE Micro Mag. Nov/Dec 2000, Vol 20, 12-25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. 1990. The RenderMan Companion. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258872</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[WINNER, S., KELLY, M., PEASE, B., RIVARD, B., and YEN, Y. 1997. Hardware Accelerated Rendering Of Antialiasing Using A Modified A-buffer Algorithm. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, New York. Whitted, T., Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 307-316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The SAGE Graphics Architecture Michael Deering David Naegle Sun Microsystems ABSTRACT The Scalable, 
Advanced Graphics Environment (SAGE) is a new high-end, multi-chip rendering architecture. Each single 
SAGE board can render in excess of 80 million fully lit, textured, anti­aliased triangles per second. 
SAGE brings high quality antialiasing .lters to video rate hardware for the .rst time. To achieve this, 
the concept of a frame buffer is replaced by a fully double-buffered sample buffer of between 1 and 16 
non-uniformly placed samples per .nal output pixel. The video output raster of samples is subject to 
convolution by a 5×5 programmable reconstruction and bandpass .lter that replaces the traditional RAMDAC. 
The reconstruction .lter processes up to 400 samples per output pixel, and supports any ra­dially symmetric 
.lter, including those with negative lobes (full Mitchell-Netravali .lter). Each SAGE board comprises 
four paral­lel rendering sub-units, and supports up to two video output chan­nels. Multiple SAGE systems 
can be tiled together to support even higher .ll rates, resolutions, and performance. CR Categories and 
Subject Descriptors: I.3.1 [Computer Graph­ics]: Hardware Architecture; I.3.3 [Computer Graphics]: Picture/ 
Image Generation Display Algorithms; I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism. 
Additional Keywords and Phrases: rendering hardware, anti­aliasing, graphics hardware, frame buffer algorithms, 
graphics sys­tems, hardware systems, video. 1 INTRODUCTION The history of computer graphics hardware 
has been blazed by high end architectures, ever advancing in features and performance. But such systems 
have also been ever increasing in cost to develop, and in more recent times many new graphics features 
have instead made their debut in lower cost implementations aimed at home entertain­ment markets. But 
the formidable cost constraints on products for the consumer market precludes many features, interfaces, 
and lev­els of performance essential to the higher end needs of the scientif­ic, medical, manufacturing, 
visual simulation, and other industrial markets. For the forseeable future, the only way to meet these 
ever growing needs is to architect graphics systems where multiple ren­ 901 San Antonio Road, UMPK27-101 
Palo Alto, CA 94303-4900 david.naegle@Eng.Sun.Com (650) 786-3939 michael.deering@acm.org Copyright &#38;#169; 
2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use is granted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
dering chips can be applied in parallel, but appear to the application as a single, high performance 
rendering pipe. While there have been some attempts to build such systems out of arrays of inexpensive 
game chips [Stoll et al. 2001], such chips were never designed to be clustered, and so far at least, 
the resulting systems have not yet been proven effective for traditional high end applications. In this 
paper we describe the architecture of SAGE (Scalable Ad­vanced Graphics Environment), a major new multi-chip 
high end rendering system designed to meet the needs of industrial strength 3D graphics applications. 
A single SAGE board can render over 80 million fully lit, textured, antialiased triangles per second. 
SAGE brings high quality antialiasing filters to video rate hardware for the first time. To achieve this, 
the concept of a frame buffer is replaced by a fully double-buffered sample buffer of between 1 and 16 
non­uniformly placed samples per final output pixel. The raster of sam­ples is subject to convolution 
by a 5×5 programmable reconstruction and bandpass filter that replaces the traditional RAMDAC. This con­volution 
is performed on-the-fly during video output, adding less than one additional scan line time of latency. 
The reconstruction fil­ter processes up to 400 samples per output pixel, and supports any radially symmetric 
filter, including those with negative lobes, e.g., full Mitchell-Netravali filters. Each SAGE board contains 
four par­allel rendering sub-units, and supports up to two video output chan­nels. Multiple SAGE systems 
can be tiled together to support even higher fill rates, resolutions, and performance. The flow of the 
paper is as follows: after some discussion of market requirements, an overview of the SAGE architecture 
will be pre­sented, followed by more detailed discussion of the various pipeline stages. The focus here 
is on the mechanisms that allow us to seam­lessly aggregate single chip rasterizers into a more powerful 
overall system. In the second half of the paper, our approach to video rate antialiasing is described 
in depth, as it is the most novel feature of the new architecture. 2 HIGH END MARKET NEEDS Customers 
have ever increasing needs for higher display resolution, but unfortunately display technology doesn 
t come close to follow­ing Moore s Law: there has been less than a factor of two improve­ment in available 
display resolution in more than a decade; less than 10% a year [Akeley 2001]. But IC process capability 
has followed Moore s Law; it has advanced sufficiently to allow us to put photo­realistic software renderers 
antialiasing algorithms into real-time hardware. This allows us to increase the effective resolution 
of cur­rent display devices. The preceding observation was one of the prime motivators of SAGE s very 
high quality real-time antialiasing pipeline: massive supersampling with large area resampling and bandpass 
filters in effect provides a back end architecture very similar to software ren­dering. The pin-count, 
storage, and computational requirements of these proven algorithms will be beyond the reach of single 
chip ren­dering pipelines for several more years to come, even with the an­nual boost from Moore. This 
is because pin-count × pin-data-rate = bandwidth-per-chip also lags well below Moore s rate. Another 
way to increase display resolutions would be to tile the dis­plays, but we found that most customers 
who use tiling also want each display antialiased. Part of the reason for this is that, so far, projectors 
adequate for tiling cost more each than a SAGE system. The historical trend of ever increasing demand 
for higher triangle rates is coupled with a nearly corresponding decrease in the median size of triangles, 
so the implied demand for the product of the two, fill rate, is more slowly increasing. This makes sense; 
applications are displaying scenes of slowly increasing depth complexity but with finer and finer surface 
tesselations to produce more and more detailed and realistic imagery. But as the median triangle size 
ap­proaches a pixel, they effectively become micro-triangles, and fur­ther reductions in triangle size 
will have diminishing returns in vis­ible detail or realism. For hardware architectures, this means that 
more attention should be paid to other measures of increasing the detail and realism of images, such 
as antialiasing and more complex shader support. This trend was already becoming apparent during the 
design of SAGE. This is why we put more emphasis on increas­ing the sampling density than on the triangle 
rate. Window system and other legacy API and application support are important market requirements, and 
in the design of SAGE we made sure that these were not forgotten. Even non-antialiased ap­plications 
work with SAGE s video resizing.  3 SAGE ARCHITECTURE 3.1 Overview SAGE s block diagram is seen in figure 
1. In this diagram we have expanded out the external buses, internal FIFOs, and internal multi­plexers 
/ load-balancing switches so that the overall data flow and required sorting may be more easily seen 
at the system level. SAGE s inter-chip connections are typically unidirectional, point­to-point, source-synchronous 
digital interconnects. The top half of SAGE s diagram is fairly similar to other sort-last architectures: 
command load balancing is performed across parallel transform and rasterize blocks, followed by the sort-last 
tree (the Sched chips) in­terfacing to the frame buffer. In SAGE, however, the frame buffer is replaced 
with a sample buffer containing 20 million samples. On the output side of the sample buffer, SAGE introduces 
an entirely new graphics hardware pipeline stage that replaces the RAMDAC: a sam­ple sort tree followed 
by parallel Convolve chips that apply a 5×5 programmable reconstruction and bandpass filter to rasters 
of sam­ples. Each Convolve chip is responsible for antialiasing a separate vertical column of the screen; 
the finished pixels are emitted from the Convolves in video raster order. 3.2 Command Distribution At 
the top of the pipeline, the Master chip performs DMA from the host to fetch OpenGL command and graphics 
data streams. The DMA engine is bidirectional, and contains an MMU so that applica­tion data can reside 
anywhere in virtual memory: no locking of ap­plication data regions is necessary. For geometric primitives, 
streams of vertex data are distributed in a load-balanced way to the four parallel render pipelines below. 
 3.3 Rendering: Transform, Lighting, Setup, Rasterization Each render pipeline consists of two custom 
chips plus several memory chips. The first custom chip is a MAJC multi-processor [Tremblay et al. 2000], 
the second is the Rasterize chip, which per­forms set-up, rasterization, and textured drawing. Many previous 
architectures are sort-middle (following the taxono­my of [Molnar et al. 1994]): they have parallel transform, 
lighting, and set-up pipelines, but recombine the streams into a one-primitive­at-a-time distributed 
drawing stage. In architecting SAGE, certain bandwidth advantages motivated our choice of a sort-last 
architec­ture: as the average pixel size of application triangles shrinks, the size of the set-up data 
becomes larger than the actual sample data of the triangle. This also improves efficiency by allowing 
each rasterization chip to generate all the samples in a triangle, rather than just a fraction of them, 
as occurs with interleaved rasterization. The MAJC chip contains specialized vertex data handling circuits 
that support two fully programmable VLIW CPUs. These CPUs are pro­grammed to implement the classic graphics 
pipeline stages of trans­form, clip check, clipping, face determination, lighting, and some geo­metric 
primitive set-up operations. The special vertex data circuitry handles vertex strip and mesh connectivity 
data, so that the CPUs only see streams of non-redundant vertex data most of the time. Thus redun­dant 
lighting computations are avoided, and vertex re-use can asymp­totically reduce the required vertex processing 
operations to 1/2 of a vertex per triangle processed when vertex mesh formats are used. To support a 
sample buffer with programmable non-uniform sam­ple positions, the hardware fill algorithms must be extended 
beyond simple scan-line interpolation. Generalizations of plane equation evaluation are needed to ensure 
correctly sampled renderings of geometric primitives. Furthermore, the sample fill rate has to run 8 
to 16 times faster than the rasterize fill of a non-supersampled ma­chine just to keep up. The aggregate 
equivalent commodity DRAM bandwidth of SAGE s eight 3DRAM memory interleaves is in ex­cess of 80 gigabytes 
per second. The Rasterize chip rasterizes textured triangles, lines, and dots into the sample buffer 
at the current sample density. It also performs some imaging functions and more traditional raster-op 
and window operations. Each MAJC + Rasterization pipeline can render more than 20 million lit textured 
supersampled triangles per second. Each Rasterizer chip has it own dedicated 256 megabytes of texture 
memory. This supports 256 megabytes of user texture memory, at four times the bandwidth of a single pipe, 
or up to one gigabyte of texture memory, when applications use the OpenGL targeted tex­ture extension 
(this is a common case for volume visualization ap­plications). For textured triangles, the Rasterize 
chip first determines which pixels are touched (even fractionally) by the triangle, applies layers of 
(MIP-mapped, optionally anisotropic filtered) texturing to each pixel, determines which (irregular) sample 
positions are within the triangle, and interpolates the color, Z, and alpha channel data to each sample 
point. The output is a stream of sample rgbaZ packets with a screen pixel xy address and sample index 
implying the sam­ple s sub-pixel location within that pixel. Each Rasterize chip has two external output 
buses so that the first stage routing of sample data to sample memory is performed before the samples 
leave the Rasterize chip.  3.4 Sort-Last Below the Rasterize chips lies a network comprising two Sched 
chips to route samples produced by any of the Rasterize outputs to any pixel interleave of the sample 
buffer below. As samples arrive in the Sched chip input FIFO from Rasterize chips, they are routed into 
the appropriate second stage FIFO based on their destination memory interleave. The output of each second 
stage FIFO is con­trolled by the load balancing switch for its memory interleave. Each switch acts like 
a traffic light at a busy intersection; traffic from one source is allowed to flow unimpeded for a time 
while the other sources are blocked. This (programmable) hysteresis in the flow of  Video out 1 2 Figure 
1. SAGE block diagram. Thick boxes are CUSTOM CHIPS. Red boxes are FIFOS. Green circles are load balancing 
SWITCHES. sample data from different Rasterize chips ensures good cache lo­cality within the 3DRAM memories 
below. The third layer of FIFOs in the Sched chip is a final sample pre-write queue in front of a single 
memory interleave. (An interleave is a group of 4 3DRAM chips connected to the same pre-write queue.). 
The Sched chips snoop this queue to perform 3DRAM cache prefetches, before scheduling the sample writes 
into the sample buffer. Because the parallel rasterized sample streams merge together here, the Sched 
chip is also the place where special control tokens enforce various render-order constraints. For example, 
most algorithms that make use of the stencil buffer require at least two passes one to prepare the stencil 
buffer with a special pattern, and then another pass with sample writes conditionally enabled by the 
presence of that special stencil pattern. Clearly all stencil writes of the first pass must complete 
before any of the second pass sample writes can be allowed to go forward. When a given interleave on 
a given Sched chip encounters a special synchronization token marking a hard or­dering constraint (e.g, 
the boundary between the two passes), then no more samples from that rasterizer will be processed until 
the oth­er three rasterizer inputs have also encountered and stopped at the synchronization token. When 
this occurs, all samples generated by primitives that entered the SAGE system before the synchronization 
token have been processed (the first pass in our example), and now it is okay to allow the pending samples 
that entered the system after the synchronizing token to proceed. The OpenGL driver knows to generate 
this ordering token when it is in unordered rendering mode, and then sees a command to transition to 
ordered rendering mode immediately followed by a command to change back to unordered rendering mode. 
Other more complex situations are supported by more complex special token generation by the driver. As 
controllers of the 3DRAM chips, the Sched chips also respond to requests from the Convolve chips for 
streams of samples to be sent out over the 3DRAM video output pins to the parallel Convolve chips to 
generate the video output.  3.5 The Sample Buffer The sample buffer consists of 32 3DRAM chips, organized 
into eight independent interleaves of four chips each. On the input side, four 3DRAMs share a single 
set of control, address, and data lines to one of four sets of memory interleave pins on a Sched chip. 
On the out­put side, each 3DRAM outputs 40-bit samples by double pumping 20 video output pins. Each of 
these pins has an individual wire to a Route chip, for a total of 640 wires entering the lower route 
network. Logically, the sample buffer is organized as a two dimensional ras­ter of lists of samples. 
All lists are the same length, because all pix­els on the screen have the same number of samples. The 
list-order of a sample implies its sub-pixel location; the Rasterize and Con­volve chips contain identical 
sample-location tables accessed by the sample index, so no space is allocated within the sample buffer 
for the sub-pixel location of the sample. The memories are interleaved per-sample: adjacent samples in 
a list are in different 3DRAM pack­ages. Unlike first-generation 3DRAM components, the new 3DRAMs used 
on SAGE contain an internal 2:1 multiplexer driven by each sam­ple s window ID, so sample-by-sample double-buffering 
occurs in­side the 3DRAM chip. Thus only the final rgb alpha/window control bits emerge in the 40-bit-per-sample 
output packet. The size of the sample buffer is enough to support 1280×1024 double­buffered samples with 
Z at a sample density of 8, or 1920×1200 at a sample density of 4. The high sample densities require 
correspondingly high render bandwidth into the sample buffer. This was achieved by a new generation of 
3DRAM [Deering et al. 1994]. Because 3DRAM per­forms z-buffer compare and alpha buffer blending internally, 
the tradi­tional z-buffer read-modify-write operation is simplified into just a write operation. The 
important operation of clearing the sample buffer for a new frame of rendering is also potentially adversely 
affected by the high sample density, but this too is greatly accelerated by the 3DRAM chips; initializing 
all samples in a 1280x1024x8 sample raster takes less than 200 usec. (less than 2% of a 76-hz frame time). 
 3.6 Sample Raster Delivery The 640 outputs of the sample buffer feed into an array of 10 Route chips. 
Each Route chip is a 2-bit slice of a router function. Each Route chip connects to 2 output data pins 
from each of the 32 3DRAMs, and can redirect this data to any of the four Convolve chips attached to 
it below. Because of the need for the Convolve chips to be fed a contig­uous vertical swath of the pixel 
interleaved sample buffer, samples are read from the sample buffers in quarter scan line wide, one pixel 
high bursts directed at one of the four Convolve chips. (More details will be discussed in the Convolve 
section.) It is the job of the Route chip to absorb these bursts into internal FIFOs, and then dribble 
them back out to their destination Convolve chip. 3.7 Convolution, CLUT, Video Timing Finally, the four 
Convolve chips perform the reconstruction and band limiting filtering of the raster stream of samples, 
producing pixels that are fed into the next Convolve chip before final video output. The Convolve chips 
replace the digital portion of the RAM-DAC; they contain color look-up and gamma tables, as well as the 
video timing generator, cursor logic, and genlock interface. The Convolve chips do not contain D/A converters. 
Instead, the Con­volve video outputs are digital, to support various existing and emerging digital video 
interfaces. Two high quality external D/A converters and an S-video interface are on the SAGE video daugh­ter 
board to support analog video devices. The traditional graphics hardware taxonomy refers to this section 
as display, however the RenderMan term imaging pipeline may be a more accurate description of this new 
functionality. The next several sections describe the convolution processing in more detail, starting 
off with a discussion of previous attempts to implement video rate antialiasing.  4 CONVOLUTION INTRODUCTION 
For over a decade now, users of most (batch) photorealistic render­ing software have been able to obtain 
high quality antialiased imag­ery, usually by means of various supersampling algorithms. How­ever, for 
real-time hardware systems, cost constraints have preclud­ed the deployment of all but the most simplistic 
approximations to these algorithms. Fill rate limitations make real-time generation of enough samples 
challenging. Restrictions in hardware polygon fill algorithms can preclude sub-pixel spatially variant 
sampling. Mem­ory costs and bandwidth limits have prohibited use of double-buff­ered supersampled frame 
buffers. Finally, the computational cost of real-time antialiasing reconstruction filters has limited 
hardware implementations to box or tent filters, which are inferior to most software reconstruction filters. 
Various alternatives to stochastic supersampling have been tried over the years in attempts to avoid 
high hardware costs, but to date all such attempts have limited the generality of the rendering and have 
not seen much use in real-time general-purpose graphics hard­ware systems. Their use has been confined 
to applications whose structure could be adequately constrained: flight simulation and some video games. 
Once the non-uniform supersampling approach is taken, a number of other rendering effects can be performed 
by applications through the use of multi-pass algorithms and user programmable sample mask patterns. 
These include motion blur, depth of field, anisotro­pic texture filtering, subject to supported sample 
densities. In this paper we do not directly address these features, rather, we focuses on the basic back-end 
architecture required to support filtered su­persampled buffers. 5 PREVIOUS ANTIALIASING WORK 5.1 PREVIOUS 
WORK, SOFTWARE Antialiasing has a rich and detailed history. The mainstream ap­proach in recent years 
has been to evaluate the image function at multiple irregularly spaced sample points per pixel, followed 
by ap­plying a reconstruction filter and then resampling with a low-pass filter. Originally referred 
to as stochastic supersampling, the basic idea is to trade off visually annoying aliasing artifacts (jaggies) 
for less visually perceptible noise. [Glassner 1995] contains an excel­lent survey and discussion of 
the many variants of this approach that have been implemented over the years. The pioneering com­mercial 
software implementation of this approach is Pixar s Photo-Realistic RenderMan [Cook et al. 1987][Upstill 
1990]. PREVIOUS WORK, HARDWARE 5.2 Flight simulators, back-to-front sorting-based algorithms Real-time 
antialiasing has been a requirement of flight simulation hardware for several decades. However, most 
of the early work in the field took advantage of known scene structure, usually the abil­ity to constrain 
the rendering of primitives to back-to-front. But these algorithms do not scale well as the average scene 
complexity grows from a few hundred to millions of polygons per frame. 5.3 Percentage Coverage Algorithms 
Some systems, for example [Akeley 1993][Winner et al. 1997], have employed polygon antialiasing algorithms 
based on storing extra information per pixel about what polygon fragments cover what fractions of the 
pixel. In principle, algorithms of this class can produce higher quality results than even supersampling 
techniques, because the exact area contribution of each polygon fragment to the final visible pixel can 
be known. In practice, hardware systems can only afford to maintain a limited amount of shape information 
about a limited number of polygon fragments within each pixel. For scenes consisting of small numbers 
of large polygons, most poly­gons are very much greater in area than a pixel, and the vast major­ity 
of pixels are either completely covered by just one or two poly­gons. Occlusion edges and silhouettes 
would then have their jaggies removed. With care, even corner cases when more than two poly­gons of one 
continuous surface land within one pixel can often be merged back into the single polygon case. However, 
with today s typical polygon shrinking towards a micro­polygon, such algorithms rapidly become confused, 
causing unac­ceptable visible artifacts. 5.4 Multi-pass Stochastic Accumulation Buffers The first attempted 
support for general full scene antialiasing inde­pendent of render order in near-real-time hardware was 
the multi­pass stochastic accumulation buffer [Deering et al. 1988][Haeberli and Akeley 1990]. The approach 
here was to render the scene mul­tiple times with different sub-pixel initial screen offsets, and then 
combine these samples with an incremental filter into an accumula­tion buffer before final display. However, 
the multiple passes and the overhead of filtering and image copying reduced the perfor­mance of the systems 
by an order of magnitude or more, while still adding substantial cost for the (deeper pixel) accumulation 
buffer. As a result, while the technique has been supported by multiple vendors, it has never found much 
use in interactive applications. Also, because the sub-pixel sample positions correlate between pix­els, 
the final quality does not match that of software systems. 5.5 Supersampling Some architectures have 
implemented subsets of the general super­sampling antialiasing algorithm. [Akeley 1993] and [Montrym 
et al. 1997] implement a one through eight sample-per-pixel rendering into a single-buffered sample buffer. 
When sample rendering is complete, the samples within each pixel are all averaged together and transferred 
to an output pixel buffer for video display. The com­bined reconstruction and low-pass filter is thus 
a 1×1 box filter, and does not require any multiplies. The 1x1 region of support also eliminates the 
need for neighboring pixels to communicate during filtering. While the quality does not match that of 
batch software renderers, the results are appreciably better than no supersampling, and have proven good 
enough to be used in flight simulation and virtual set applications, among others. [Eyles at. al. 1997] 
imple­mented supersampled rendering with a 1×1 weighted filter. At the lower end, some simple processing 
for antialiasing is begin­ning to show up in game chips. [Tarolli et al. 1999] appears to be an implementation 
of a 2×2 single buffered supersampled buffer, but it is not clear if other than box filtering is supported. 
The nVidia Geforce3 supports sample densities of either 2 or 4, with either a 1×1 box filter, or a 3×3 
tent filter [Dominé 2001]. The resultant quality is better than no antialiasing at all, but still far 
from the qual­ity of batch photorealism software. The frame rates, however, do suffer almost linearly 
in proportion to sample density. The OpenGL 1.3 specification does contain support for supersam­pling, 
but only in the context of applying the filtering before the render buffer to display buffer swap.  
 6 SAGE SUPERSAMPLING ISSUES 6.1 Programmable Nonuniform Sample Pattern An important component of high-quality 
supersampling based anti­aliasing algorithms is the use of carefully controlled sample patterns that 
are not locally periodic. Today s best patterns are constrained random perturbations of uniform grids. 
Software algorithms can af­ford the luxury of caching tens of thousands of pixel area worth of pre-computed 
sample patterns. On-chip hardware is much more se­verely space constrained; SAGE only supports a pattern 
RAM of 64 (2x2 pixels x 16 samples) of 6-bit x and 6-bit y sub-pixel offset en­tries. However, the effective 
non-repeating size of this pattern is ex­tended to 128x128 pixels by the use of a 2D hardware hash function 
that permutes access to the pattern entries. The effectiveness of this hash function can be seen in Figure 
5, where each large colored dot corresponds to a sample. Because the table is so small, it is easily 
changeable in real-time on a frame-by-frame basis, supporting tem­poral perturbation of the sample pattern. 
Note that in the SAGE system the sample tables for the frame cur­rently being displayed are stored in 
the Convolve chips, while the sample tables for the frame currently being rendered are stored in the 
Rasterizer chips. If the tables are not static, system software must ensure that they are updated at 
the appropriate time bound­aries.  7 CONVOLVE CHIP ARCHITECTURE DETAILS One of the primary ways in which 
our architecture differs from pre­vious systems is that there is no attempt to compute the antialiased 
pixels on the render side of the frame buffer. As far as the sample buffer is concerned, the output display 
device is capable of display­ing supersamples; it is up to the back end reconstruction filter pipe­line 
to convert streams of supersamples into antialiased pixels on the fly at full high-resolution video rates. 
The peak data rates required to support this are impressive: the frame buffer has to output 1.6 billion 
samples per second, or ap­proximately 8 gigabytes per second of data. Real-time high-quality filtering 
of this much data is beyond the capabilities of today s sil­icon in a single chip. Thus, we had to find 
a way to spread the con­volution processing of this fire-hose of data across multiple chips. As seen 
in Figure 1, our convolution pipeline is split up into four chips. Each chip is assigned a different 
vertical swath of the screen s samples. Because reconstruction filters of up to 5×5 are supported, each 
of these vertical swaths must overlap their horizon­tal neighbors by up to 2 pixels (half the filter 
width). The final video stream is assembled as video is passed from chip to chip; each chip inserts its 
portion of each scan line into the aggregate stream. The last chip delivers the complete video stream. 
(An optional second video stream also emerges from this last chip.) The 5×5 filter size also implies 
that each sample will potentially be used in up to 25 different pixel computations. To avoid re-fetching 
samples from off-chip, 6 swath-lines worth of sample data is cached on each Convolve chip. (This RAM 
consumes half the ac­tive area of the chip.) The internal architecture of the chip is shown in Figure 
2. The video generation process for each chip starts with the generation of a raster of convolution center 
(output pixel center) locations across and down each swath. As the convolution center location moves, 
sample data is transferred from the swath-line buffers into a 5×5 filter processor array. A schematic 
of a filter processor is shown in Figure 3. Each filter processor is responsible for all the samples 
from one pixel from the sample buffer; the 5×5 array has access to all the samples that may contribute 
to a single output pixel. The filter processor computes the contribution of its samples to the total 
convolution; the partial results from all 25 filter processors are then summed to form the un­normalized 
convolution result. Because of the nonuniform, non-lo­cally repeating properties of good sample patterns, 
it is not feasible to cache pre-computed convolution filter coefficients. Instead, each filter processor 
contains circuitry for dynamically computing cus­  Inverse SampleX Filter Radius2 KernelX SampleY 
1.0  RGBA, KernelY sum-coeff tom filter coefficients for arbitrary sample locations. It also con­tains 
the multiplier-accumulator that actually weights its samples. This filter coefficient computation proceeds 
as follows. First, the sample location relative to the convolution center is computed by subtracting 
the sample xy location (generated by the sample pattern RAM) from the convolution center xy location. 
Squaring and sum­ming the these delta xy components results in the squared radial dis­tance of the sample 
location from the convolution center. This squared distance is scaled by the square of the inverse filter 
radius; results greater than unity force a zero filter coefficient. Next the squared distance is encoded 
into a 3-bit exponent, 5-bit mantissa (+1 hidden bit) floating-point representation. This 8-bit floating­point 
number is then used as an index into a (RAM) table of squared distance vs. filter coefficients. From 
a numeric linearity point of view, the squaring and floating-point encoding nearly cancel out, resulting 
in accurate, relatively equally-spaced filter coefficients. This can be seen in a plot of the synthesized 
filter values vs. dis­tance in Figure 4. The filter coefficient output of this table is a signed 14-bit 
floating­point number, which is used to weight the rgba sample values. The multiplied result is converted 
back into a 27-bit fixed-point number, and directed into a set of summing trees. A separate running sum 
of applied filter coefficients is similarly calculated. Thus our hardware places only two restrictions 
on the reconstruc­tion filter: it must be radially symmetric in the convolution space, and the filter 
radial cross section has to be quantized to 256 values. Note that through non-uniform video and/or screen 
space scalings, elliptical filters in physical display space can be supported. Separa­ble filters have 
theoretical advantages over radial filters, but radial filtering was less complex to implement in hardware. 
Technically our filter is a weighted average filter, because of how we handle filter normalization. We 
perform a floating-point recip­rocal operation on the sum of the filter coefficients, and a normal­izing 
multiply on r, g, b, and a. There was an unexpected advantage in using dynamic normalization: in simulations, 
the error compared to the exact solution came out well below expectations. This is be­cause slight errors 
in coefficient generation produce a similar bias in the normalization value and are mostly canceled out. 
Remaining numeric errors in coefficient generation have less perceptual effect because they are equivalent 
to a correct coefficient at an incorrect estimation of the sample distance from the center of the filter 
(error in sample position). But samples should be quite representative of the true underlying image in 
their vicinity. Most visible errors in an­tialiased output are due to a sample just missing a significant 
change in the underlying image (e.g. from black to white). The con­tribution to image output errors due 
to errors in computing filter co­efficient values is quite small by comparison. Hyper-accurate filtering 
can preserve the quantization present in the original samples (sometimes called contouring). To mitigate 
con­touring, we dither 12-bit rgba samples computed during rendering to the 10-bit rgba values actually 
stored in the sample buffer. Con­ Figure 3: Filter Processor Detail. Figure 4: Numerical accuracy of 
filter representation. volution reconstruction of dithered sample values effectively revers­es this dithering, 
achieving 12 bits of accuracy per rgba component. 7.1 Video Outputs Up to two simultaneous, potentially 
asynchronous, video rasters can be generated in parallel by partitioning the four Convolve chips into 
two subsets; both video streams will emerge from the digital video out ports of the last Convolve chip. 
The swap circuit shown in Figure 2 allows each Convolve chip to add its results to either of the incoming 
streams, and pass the other through unmodified. (There is also a post-processing swap not shown.) One 
use for this second channel is to be able to read the antialiased image back into the computer, through 
the outer ring bus shown in Figure 1. Without this option, the host computer would have no way to get 
a copy of the antialiased image! This is useful when performing antialiased rendering intended for later 
reuse as reflection maps, etc. The Convolve video timing circuit can run as either a sync master or as 
a sync slave genlocked to an external sync source. The two video streams can be sub-regions of a what 
the window system and rendering system think of as a single display (useful for tiling two lower-resolution 
projectors/displays). Alternatively, the two video sources can be two separate, potentially asynchronous, 
image re­gions with potentially different sample densities. 7.2 Video Resizing A key benefit of the 
SAGE architecture is that video resolution is de­termined by the convolution hardware, not by rendering 
hardware. Thus the same hardware used for antialiasing also provides an ex­tremely high quality video 
rescaler, with better filtering quality than is possible with an external scaler because it operates 
on the original samples, not pixels, and SAGE correctly performs the filtering in linear light space. 
One use of this is to generate NTSC video of arbi­trary zoomed and panned sub-regions of a higher resolution 
display, as might be used to document a software program. A more important use is in systems with real-time 
guarantees: to conserve fill rate, the actual size of the image rendered can be dynamically reduced, 
and then interpolated back up to the fixed video output size. So a flight simulator using a 1280×1024 
video format might actually be render­ing at 960×768 when the load gets heavy, saving nearly half the 
fill time. The system described in [Montrym et al. 1997] also supports dynamic video resizing, but uses 
a simple tent filter, and performs the filtering in a non-linear (post-gamma) light space. 7.3 Fully 
Antialiased Alpha Channel SAGE s sample filtering algorithm operates not only on the rgb channels, but 
also on stored double-buffered alpha if enabled. For example, for virtual set applications this means 
that SAGE auto­matically generates a very high quality soft key signal for blend­ing antialiased edges 
of virtual objects in front of physical objects, as well as blending variably transparent rendered objects 
in front of physical objects.  8 CHOICE OF RECONSTRUCTION FILTER Because it is programmable, the choice 
of reconstruction filters can be left to the end user, but in general we have found that the same Mitchell-Netravali 
family of cubic filters [Mitchell and Netravali 1988] used in high quality software renderers work well 
for hard­ware. The choice of reconstruction filter has a subjective compo­nent: some users prefer smoother 
filters that banish all jaggies at the expense of a slight blurring of the image; other users desire 
a filter that preserves sharpness at the risk of a few artifacts. There is also a display-device-specific 
aspect to the choice of reconstruction fil­ter: to get close to the same end-user look on a CRT vs. a 
flat panel LCD display, slightly different filters are needed. While not of gen­eral use, more exotic 
filters can be used to help simulate the appear­ance of special imaging devices. 8.1 Effects of Negative 
Lobes One of the prices that must be paid for the use of high quality recon­struction filters is occasional 
artifacts ( ringing or fringing ) due to the presence of negative lobes. Our filter hardware clamps nega­tive 
color components to zero, and it also keeps a histogram of the frequency and extent of such occurrences. 
This histogram data can be used to dynamically reduce the negative lobes of the reconstruc­tion filter 
if artifacts are too severe. 9 Legacy and Compatibility Issues There are several legacy and compatibility 
issues that SAGE must address. Many of these are handled by properties associated with window ID tags 
that are part of each sample. One example is support for applications that were programmed as­suming 
a non-linear light space and/or a pseudo color space. The non-linear light space is typically a particular 
gamma space. SAGE supports these applications by providing pseudo color, direct color, and non-linear 
true color LUTs as specified by window ID proper­ties of samples. In SAGE, these LUTs are applied to 
samples before the convolution. Of course, most 3D rendering is performed in lin­ear light space, and 
so can by-pass these pre-convolve LUTs. This pre-processing ensures that all sample inputs to the antialiasing 
convolution process are in the same linear light space. After the convolution process generates (linear 
light space) pixels, the pixels are converted to the proper non-linear light space (e.g. gamma cor­rection) 
for the particular display device attached to the system. Not all pixels should be antialiased. Proper 
emulation of 2D win­dow system rendering and legacy applications require accurate em­ulation of all those 
jaggies. Our solution is to disable any filtering of such pixels via a special property of the window 
ID tag of the pixels of such windows. Instead, when so tagged, a sample (typical­ly the one closest to 
the convolution center) is chosen to be output in place of the convolution result. Thus it is possible 
for the screen to simultaneously support antialiased and non-antialiased windows. Because our filter 
has a 5×5 extent, care must be taken to ensure that such unfiltered pixels do not contribute any samples 
to nearby filtered pixels. This is the case, for example, when a non-antialiased window occludes an antialiased 
window. Once again the dynamic filter normalization circuit comes to the rescue; we simply don t ap­ply 
any filter coefficients from aliased pixels within the 5×5 win­dow of an antialiased pixel, and still 
get unit volume under the ker­nel. The same approach is also used to eliminate artifacts at the vis­ible 
video border, in place of the traditional approach of adding an extra non-visible strip of 2 pixels all 
around the full screen. Other legacy issues include proper support of traditional antialiased lines when 
also subject to supersampling and filtering. Our goal is to allow as much as possible for existing applications 
to move to full scene antialiased operation with minimal source code changes. 10 RESULTS 10.1 Images 
Figure 5 is an image from the SAGE debugging simulator, and shows the details of our sampling pattern 
for sample density 16 ren­dering. The intensity of each dot corresponds to the computed sam­ple value 
for rgb; the green lines are triangle tesselation boundaries; the faint red grid lines are the pixel 
boundaries. 11 triangles are shown: a 12-segment radius-3 pie wedge with one slice missing. Because SAGE 
s native output environment is a display, the next set of images are digital photos of functional SAGE 
hardware driv­ing a CRT screen. Figures 6 through 10 are shots of a portion of a 1280×1024 CRT display. 
Each shows the same portion of the same object, a honeybee. The differences are in the sample count and 
re­construction filter. Figure 6 shows one (uniformly spaced) sample per pixel, with no reconstruction 
filtering. Figures 7 through 10 are rendered using a sample density of 8. Figures 7 and 10 use the 4×4 
Mitchell-Netravali 1/3 1/3 filter of Figure 4. Figure 8 uses a diam­eter 4 cylinder filter, and shows 
considerable blur. Figure 9 uses a Laplacian filter, and shows enhanced edges. Figure 10 is a wider (approximately 
800×800 pixel) shot of the bee. 10.2 Comparison to RenderMan During SAGE s development, Pixar s Photorealistic 
RenderMan (PRMAN) was used to verify the quality of the antialiasing algo­rithms. Custom RenderMan shaders 
were written to mimic the dif­ferent lighting algorithms employed. The same scene descriptions, camera 
parameters, sampling rates, and reconstruction filters were used to generate images from both renderers. 
The resulting images cannot be expected to be numerically identical at every pixel, pri­marily because 
of the different sample patterns used, as well as the different numeric accuracies employed. (PRMAN uses 
full 32-bit IEEE floating-point arithmetic internally.) So as a control, we also ran PRMAN at a sample 
density of 256. Numerically, comparing our hardware 16 sample rendering with that of PRMAN, fewer than 
1% of the pixels differed in value by more than 6% (the contribu­tion of a single sample). However, about 
the same variance was seen between the 16-sample and 256 sample PRMAN images. This explains the visual 
results: in general, expert observers could not determine which image was rendered by which system. 10.3 
Data Rates and Computational Requirements A double-buffered sample buffer supporting 8× supersampled 
1280×1024 imagery requires storage of over 20 million samples (approximately an eighth of a gigabyte, 
including single-buffered Z). For 76 Hz video display, because of overheads and fragmenta­tion effects, 
we designed in a peak video output bandwidth of 1.6 billion samples per second, or 8 gigabytes per second. 
(Note that the render fill data rate has to be several times larger than this to sup­port interesting 
depth complexity scenes at full frame rates.) A5×5 filter at a sample density of 4 requires 25*4*4 floating-point 
multiply-adds per output pixel, or 800 operations per pixel. A sim­ilar number of operations are needed 
to generate all the filter coef­ficients per pixel. At peak video output rates of 250 MHz, the total 
operation count per second exceeds 0.4 teraflops. While these are numbers generated by specialized hardware, 
it is important to note that a (much) greater number of flops are consumed by general pur­pose computers 
running equivalent software antialiasing algo­rithms for equivalent work. 10.4 Scalable The SAGE chip 
set was designed to scale the performance of a two chip rendering sub-system into a parallel pipeline 
rendering system. Not all the combinatorial variations of chip configurations allowed by the SAGE architecture 
have been described in this paper on the first implementation of a SAGE chip-set based system. In addition, 
each current SAGE board has all the necessary hooks to be scaled at the computer system level, to support 
even higher fill rates, reso­lutions, and performance. These include the ability to function as a sync 
slave, and synchronization signals for both stereo frame parity and render buffer flip, as well as some 
special features enabled by the architecture of the SAGE Convolve chip. SAGE is not unique in this respect; 
one can also tile multiple commodity PC solutions together. But with SAGE, one starts with a much more 
powerful building block with high geometry and fill rates, large texture stores, and that already performs 
high quality supersampled anti­aliasing. 11 OTHER FEATURES SAGE is a complex multi-chip machine. Details 
of textured render­ing, lighting, picking, texture read-back, context switching, etc. were re-implemented 
for SAGE, often somewhat differently than has been done before. In this short paper, we chose to focus 
on the major effects that the antialiasing algorithm had on the architecture; this is not to detract 
from other areas of 3D graphics hardware where the implementers pushed the envelope as well. 12 IMPLEMENTATION 
STATUS Complete SAGE prototype hardware is up and running, with OpenGL rendering and full scene antialiasing 
with arbitrary filters as described in this paper. The board is shown in Figure 11. 13 CONCLUSIONS A 
new high end architecture and implementation for 3D graphics rendering, SAGE, has been described. The 
performance goal of over 80 million lit, textured, antialiased triangles per second has been met. We 
have also achieved our goal of producing a hardware antialiasing system whose images are numerically 
and perceptually indistinguishable from images generated by the antialiasing portion of leading software 
renderers. This is achieved through the use of a hardware double-buffered sample buffer with on-the-.y 
video-out spatial .ltering, capable of implementing non-uniform supersam­pling with cubic reconstruction 
.lters.   ACKNOWLEDGEMENTS Thanks to Dean Stanton and Dan Rice for programming, proofread­ing, and 
photo composition. Thanks to Clayton Castle for help with video recording. Thanks to the entire SAGE 
development team, without whom SAGE would not be possible.  REFERENCES AKELEY, K. 1993. RealityEngine 
Graphics. In Proceedings of SIG-GRAPH 1993, ACM Press / ACM SIGGRAPH, New York. Ka­jiya, J., Ed., Computer 
Graphics Proceedings, Annual Confer­ence Series, ACM, 109-116. AKELEY, K. 2001. Course notes of CS448A, 
taught fall semester at Stanford University. URL: http://graphics.stanford.edu/cours­es/cs448a-01-fall/lectures/lecture5/ 
COOK,R,CARPENTER, L, and CATMULL, E. 1987. The Reyes Image Rendering Architecture. In Computer Graphics 
(Proceedings of SIGGRAPH 87), 21 (4) ACM, 95-102. DEERING, M., WINNER, S., SCHEDIWY, B., DUFFY, C and 
HUNT,N. 1988. The Triangle Processor and Normal Vector Shader: A VLSI system for High Performance Graphics. 
In Computer Graphics (Proceedings of SIGGRAPH 88), 22 (4) ACM, 21-30. DEERING, M., SCHLAPP, S., and LAVELLE, 
M. 1994. FBRAM: A new Form of Memory Optimized for 3D Graphics. In Proceed­ings of SIGGRAPH 1994, ACM 
Press / ACM SIGGRAPH, New York. Glassner, A., Ed., Computer Graphics Proceedings, An­nual Conference 
Series, ACM, 167-174. DOMINÉ, S. 2001. OpenGL Pixel Formats and Multisample Anti­aliasing. URL: http://developer.nvidia.com/docs/IO/1594/ATT/ 
PixelformatsAndMultisample.pdf EYLES, J., MOLNAR, S., POULTON, J., GREER, T., LASTRA, A., EN-GLAND, N., 
and WESTOVER, L. 1997. PixelFlow: The Realiza­tion. 97 Eurographics/SIGGRAPH Workshop on Graphics Hardware 
(Los Angeles, CA, Aug 3-4, 1997). GLASSNER, A. 1995. Principles of Digital Image Synthesis. Morgan Kaufmann. 
HAEBERLI, P., and AKELEY, K. 1990. The Accumulation Buffer: Hard­ware Support for High-Quality Rendering. 
In Computer Graphics (Proceedings of SIGGRAPH 90), 24 (4) ACM, 309-318. MITCHELL, D., and NETRAVALI, 
A. 1988. Reconstruction Filters in Computer Graphics. In Computer Graphics (Proceedings of SIGGRAPH 88), 
22 (4) ACM, 221, 228. MOLNAR, S., COX, M., ELLSWORTH, D., and FUCHS, H. 1994. A Sorting Classification 
of Parallel Rendering, IEEE Computer Graphics and Applications, July 1994, 23-32. MONTRYM, J., BAUM, 
D., DIGNAM, D., and MIGDAL, C. 1997. Infi­niteReality: A Real-Time Graphics System. In Proceedings of 
SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, New York. Whitted, T., Ed., Computer Graphics Proceedings, Annual 
Con­ference Series, ACM, 293-302. STOLL,G., ELDRIDGE,M., PATTERSON,D., WEBB,A., BERMAN,S., LEVY,R., CAYWOOD,C., 
TAVEIRA,M., HUNT., S., and HANRAH-AN, P. 2001. Lightning-2: A High Performance Display Subsystem for 
PC Clusters. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, New York. E. Fume, Ed., Computer 
Graph­ics Proceedings, Annual Conference Series, ACM, 141-148. TAROLLI, G.1999. Real-Time Cinematic Effects 
on the PC: The 3Dfx T-Buffer. Hot 3D presentation in Eurographics/SIG-GRAPH Workshop on Graphics Hardware 
1999, IEEE Press. TREMBLAY, M., CHAN, J., CHAUDHRY, S., CONIGLIARO, A., TSE, S.S., 2000. The MAJC Architecture; 
A Synthesis of Parallelism and Scalability. IEEE Micro Mag. Nov/Dec 2000, Vol 20, 12-25. UPSTILL, S. 
1990. The RenderMan Companion. Addison-Wesley. WINNER, S., KELLY, M., PEASE, B., RIVARD, B., and YEN, 
Y. 1997. Hardware Accelerated Rendering Of Antialiasing Using A Modified A-buffer Algorithm. In Proceedings 
of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, New York. Whitted, T., Ed., Computer Graphics Proceedings, 
Annual Conference Se­ries, ACM, 307-316.    Figure 9: Bee, sample density 8, Laplacian filter. Figure 
5: Sample pattern at density 16   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566639</article_id>
		<sort_key>693</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Chromium]]></title>
		<subtitle><![CDATA[a stream-processing framework for interactive rendering on clusters]]></subtitle>
		<page_from>693</page_from>
		<page_to>702</page_to>
		<doi_number>10.1145/566570.566639</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566639</url>
		<abstract>
			<par><![CDATA[We describe Chromium, a system for manipulating streams of graphics API commands on clusters of workstations. Chromium's stream filters can be arranged to create sort-first and sort-last parallel graphics architectures that, in many cases, support the same applications while using only commodity graphics accelerators. In addition, these stream filters can be extended programmatically, allowing the user to customize the stream transformations performed by nodes in a cluster. Because our stream processing mechanism is completely general, any cluster-parallel rendering algorithm can be either implemented on top of or embedded in Chromium. In this paper, we give examples of real-world applications that use Chromium to achieve good scalability on clusters of workstations, and describe other potential uses of this stream processing technology. By completely abstracting the underlying graphics architecture, network topology, and API command processing semantics, we allow a variety of applications to run in different environments.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[cluster rendering]]></kw>
			<kw><![CDATA[parallel rendering]]></kw>
			<kw><![CDATA[remote graphics]]></kw>
			<kw><![CDATA[scalable rendering]]></kw>
			<kw><![CDATA[stream processing]]></kw>
			<kw><![CDATA[tiled displays]]></kw>
			<kw><![CDATA[virtual graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.2.2</cat_node>
				<descriptor>Applications (SMTP, FTP, etc.)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.2.4</cat_node>
				<descriptor>Client/server</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.2.4</cat_node>
				<descriptor>Distributed applications</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010971.10011120.10011680</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Software system structures->Distributed systems organizing principles->Organizing principles for web applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003033.10003039.10003051</concept_id>
				<concept_desc>CCS->Networks->Network protocols->Application layer protocols</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP79028562</person_id>
				<author_profile_id><![CDATA[81365593097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Humphreys]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP79028768</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14164482</person_id>
				<author_profile_id><![CDATA[81100471423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77046086</person_id>
				<author_profile_id><![CDATA[81416598247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Randall]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frank]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382464</person_id>
				<author_profile_id><![CDATA[81100201556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ahern]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382456</person_id>
				<author_profile_id><![CDATA[81335493060]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Kirchner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T.J. Watson Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P132712</person_id>
				<author_profile_id><![CDATA[81100083824]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Klosowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T.J. Watson Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Advanced Graphics Progamming Techniques Using OpenGL. SIGGRAPH 1998 Course Notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>603884</ref_obj_id>
				<ref_obj_pid>603867</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shivnath Babu and Jennifer Widom. Continuous queries over data streams. SIGMOD Record, pages 109-120, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>348233</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ian Buck, Greg Humphreys, and Pat Hanrahan. Tracking graphics state for networked rendering. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 87-95, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>347094</ref_obj_id>
				<ref_obj_pid>347090</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Corrina Cortes, Kathleen Fisher, Daryl Pregibon, Anne Rodgers, and Frederick Smith. Hancock: A language for extracting signatures from data streams. Proceedings of 2000 ACM SIGKDD International Conference on Knowledge and Data Mining, pages 9-17, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237273</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Thomas Funkhouser. Coarse-grained parallelism for hierarchical radiosity using group iterative methods. Proceedings of SIGGRAPH 96, pages 343-352, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617870</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Christopher Giertsen and Johnny Peterson. Parallel volume rendering on a network of workstations. IEEE Computer Graphics and Applications, pages 16-23, November 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>370400</ref_obj_id>
				<ref_obj_pid>370049</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Greg Humphreys, Ian Buck, Matthew Eldridge, and Pat Hanrahan. Distributed rendering for scalable displays. IEEE Supercomputing 2000, October 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383272</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Greg Humphreys, Matthew Eldridge, Ian Buck, Gordon Stoll, Matthew Everett, and Pat Hanrahan. WireGL: A scalable graphics system for clusters. Proceedings of SIGGRAPH 2001, pages 129-140, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319377</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Greg Humphreys and Pat Hanrahan. A distributed graphics system for large tiled displays. IEEE Visualization '99, pages 215-224, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280837</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Homan Igehy, Gordon Stoll, and Pat Hanrahan. The design of a parallel graphics interface. Proceedings of SIGGRAPH 98, pages 141-150, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617354</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kwan-Liu Ma, James Painter, Charles Hansen, and Michael Krogh. Parallel volume rendering using binary-swap image compositing. IEEE Computer Graphics and Applications, pages 59-68, July 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383527</ref_obj_id>
				<ref_obj_pid>383507</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[William Mark and Kekoa Proudfoot. The F-buffer: A rasterization order FIFO buffer for multi-pass rendering. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 57-64, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lee Markosian, Michael Kowalski, Samuel Trychin, Lubomir Bourdev, Daniel Goldstein, and John Hughes. Real-time non-photorealistic rendering. Proceedings of SIGGRAPH 1997, pages 415-420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364392</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Alex Mohr and Michael Gleicher. Non-invasive, interactive, stylized rendering. ACM Symposium on Interactive 3D Graphics, pages 175-178, March 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Steve Molnar, Michael Cox, David Ellsworth, and Henry Fuchs. A sorting classification of parallel rendering. IEEE Computer Graphics and Algorithms, pages 23-32, July 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>878995</ref_obj_id>
				<ref_obj_pid>876875</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Liadan O'Callaghan, Nina Mishra, Adam Meyerson, Sudipto Guha, and Rajeev Motwani. Streaming-data algorithms for high-quality clustering. To appear in Proceedings of IEEE International Conference on Data Engineering, March 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>346883</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[John Owens, William Dally, Ujval Kapasi, Scott Rixner, Peter Mattson, and Ben Mowery. Polygon rendering on a stream architecture. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 23-32, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344976</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Mark Peercy, Marc Olano, John Airey, and Jeffrey Ungar. Interactive multi-pass programmable shading. Proceedings of SIGGRAPH 2000, pages 425-432, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>582039</ref_obj_id>
				<ref_obj_pid>582034</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Kenneth Perrine and Donald Jones. Parallel graphics and interactivity with the scaleable graphics engine. IEEE Supercomputing 2001, November 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Pixar animation studios. PhotoRealistic RenderMan Toolkit. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Thomas Porter and Tom Duff. Compositing digital images. Proceedings of SIGGRAPH 84, pages 253-259, July 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383275</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Kekoa Proudfoot, William Mark, Svetoslav Tzvetkov, and Pat Hanrahan. A real time procedural shading system for programmable graphics hardware. Proceedings of SIGGRAPH 2001, pages 159-170, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383525</ref_obj_id>
				<ref_obj_pid>383507</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ramesh Raskar. Hardware support for non-photorealistic rendering. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 41-46, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300539</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ramesh Raskar and Michael Cohen. Image precision silhouette edges. ACM Symposium on Interactive 3D Graphics, pages 135-140, April 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91419</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Rodney Recker, David George, and Donald Greenberg. Acceleration techniques for progressive refinement radiosity. ACM Symposium on Interactive 3D Graphics, pages 59-66, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Jareck Rossignac and Maarten van Emmerik. Hidden contours on a framebuffer. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, September 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364350</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Szymon Rusinkiewicz and Marc Levoy. Streaming QSplat: A viewer for networked visualization of large, dense models. ACM Symposium on Interactive 3D Graphics, pages 63-68, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>502140</ref_obj_id>
				<ref_obj_pid>502125</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Rudrajit Samanta, Thomas Funkhouser, and Kai Li. Parallel rendering with k-way replication. IEEE Symposium on Parallel and Large-Data Visualization and Graphics, October 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>348237</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Rudrajit Samanta, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Sort-first parallel rendering with a cluster of PCs. SIGGRAPH 2000 Technical Sketch, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>348237</ref_obj_id>
				<ref_obj_pid>346876</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Rudrajit Samanta, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Hybrid sort-first and sort-last parallel rendering with a cluster of PCs. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 97-108, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311584</ref_obj_id>
				<ref_obj_pid>311534</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Rudrajit Samanta, Jiannan Zheng, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Load balancing for multi-projector rendering systems. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 107-116, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383273</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Gordon Stoll, Matthew Eldridge, Dan Patterson, Art Webb, Steven Berman, Richard Levy, Chris Caywood, Milton Taveira, Stephen Hunt, and Pat Hanrahan. Lightning-2: A high-performance display subsystem for PC clusters. Proceedings of SIGGRAPH 2001, pages 141-148, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378517</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Douglas Voorhies, David Kirk, and Olin Lathrop. Virtual graphics. Proceedings of SIGGRAPH 88, pages 247-253, August 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Chromium: A Stream-Processing Framework for Interactive Rendering on Clusters Greg Humphreys. Mike Houston. 
Ren Ng. Randall Frank Sean Ahern Peter D. Kirchner James T. Klosowski .Stanford University Lawrence 
Livermore National Laboratory IBM T.J. Watson Research Center Abstract We describe Chromium, a system 
for manipulating streams of graphics API commands on clusters of workstations. Chromium s stream .lters 
can be arranged to create sort-.rst and sort-last par­allel graphics architectures that, in many cases, 
support the same applications while using only commodity graphics accelerators. In addition, these stream 
.lters can be extended programmatically, al­lowing the user to customize the stream transformations performed 
by nodes in a cluster. Because our stream processing mechanism is completely general, any cluster-parallel 
rendering algorithm can be either implemented on top of or embedded in Chromium. In this paper, we give 
examples of real-world applications that use Chromium to achieve good scalability on clusters of workstations, 
and describe other potential uses of this stream processing technol­ogy. By completely abstracting the 
underlying graphics architec­ture, network topology, and API command processing semantics, we allow a 
variety of applications to run in different environments. CR Categories: I.3.2 [Computer Graphics]: Graphics 
Systems Distributed/network graphics; I.3.4 [Computer Graphics]: Graph­ics Utilities Software support, 
Virtual device interfaces; C.2.2 [Computer-Communication Networks]: Network Protocols Applications; C.2.4 
[Computer-Communication Networks]: Dis­tributed Systems Client/Server, Distributed Applications Keywords: 
Scalable Rendering, Cluster Rendering, Parallel Ren­dering, Tiled Displays, Remote Graphics, Virtual 
Graphics, Stream Processing 1 Introduction The performance of consumer graphics hardware is increasing 
at such a fast pace that a large class of applications can no longer utilize the full computational potential 
of the graphics processor. This is largely due to the slow serial interface between the host and the 
graphics subsystem. Recently, clusters of workstations have emerged as a viable option to alleviate this 
bottleneck. However, cluster rendering systems have largely been focused on providing speci.c algorithms, 
rather than a general mechanism for enabling interactive graphics on clusters. The goal of our work is 
to allow applications to utilize more easily the aggregate rendering power of a collection of commodity 
graphics accelerators housed in a cluster Copyright &#38;#169; 2002 by the Association for Computing 
Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 of workstations, without imposing a speci.c scalability 
algorithm that may not meet an application s needs. To achieve this goal, we have designed and built 
a system that provides a generic mechanism for manipulating streams of graphics API commands. This system, 
called Chromium, can be used as the underlying mechanism for any cluster-graphics algorithm by having 
the algorithm use OpenGL to move geometry and imagery across a network as required. In addition, existing 
OpenGL applications can use a cluster with very few modi.cations, because Chromium provides an industry-standard 
graphics API that virtualizes the dis­joint rendering resources present in a cluster. In some cases, 
the application does not even need to be recompiled. Compatibility with existing applications may accelerate 
the adoption of rendering clusters and high resolution displays, encouraging the development of new applications 
that exploit resolution and parallelism. Chromium s stream processors are implemented as modules that 
can be interchanged and combined in an almost completely arbi­trary way. By modifying the con.guration 
of these stream pro­cessors, we have built sort-.rst and sort-last parallel graphics ar­chitectures that 
can, in many cases, support the same applications without recompilation. Unlike previous work, our approach 
does not necessarily require that any geometry be moved across a net­work (although this may be desirable 
for load-balancing reasons). Instead, applications can issue commands directly to locally housed graphics 
hardware, thereby achieving the node s full advertised ren­dering performance. Because our focus is on 
clusters of commod­ity components, we consider only architectures that do not require communication between 
stages in the pipeline that are not normally exposed to an application. For example, a sort-middle architecture, 
which requires communication between the geometry and rasteri­zation stages, is not a good match for 
our system. Chromium s stream processors can be extended programmati­cally. This added .exibility allows 
Chromium users to solve more general problems than just scalability, such as integration with an existing 
user interface, stylized drawing, or application debugging. This extensibility is one of Chromium s key 
strengths. Because we simply provide a programmable .lter mechanism for graphics API calls, Chromium 
can implement many different underlying algo­rithms. This model can be thought of as an extension of 
Voorhies s virtual graphics pipeline [33], which insulates applications from the details of the underlying 
implementations of a common API. 2 Background and Related Work 2.1 Cluster Graphics Clusters have long 
been used for parallelizing traditionally non­interactive graphics tasks such as ray-tracing, radiosity 
[5, 25], and volume rendering [6]. Other cluster-parallel rendering efforts have largely concentrated 
on exploiting inter-frame parallelism rather than trying to make each individual frame run faster [20]. 
We are in­terested in enabling fast, interactive rendering on clusters, so these techniques tend to be 
at most loosely applicable to our domain. In the last few years, there has been growing interest in using 
clusters for interactive rendering tasks. Initially, the goal of these systems was to drive large tiled 
displays. Humphreys and Hanrahan described an early system designed for 3D graphics [9]. Although the 
system described in that paper ran on an SGI In.niteReality, it was later ported to a cluster of workstations. 
At .rst, their cluster­based system, called WireGL, only allowed a single serial applica­tion to drive 
a tiled display over a network [7]. WireGL used tra­ditional sort-.rst parallel rendering techniques 
to achieve scalable display size with minimal impact on the application s performance. The main drawback 
of this system was its poor utilization of the graphics resources available in a cluster. Because it 
only focused on display resolution, applications would rarely run faster on a cluster than they would 
locally. Other approaches focused on scalable rendering rates. Samanta et al. described a cost-based 
model for load-balancing rendering tasks among nodes in a cluster, eventually redistributing the re­sulting 
non-overlapping pixel-tiles to drive a tiled display [29, 31]. They then extended this technique to allow 
for tile overlap, creat­ing a hybrid sort-.rst and sort-last algorithm that could effectively drive a 
single display [30]. All of these algorithms required the full replication of the scene database on each 
node in the cluster, so further work was done to only require partial replication, trad­ing off memory 
usage for ef.ciency [28]. Although these papers present an excellent study of differing data-management 
strategies in a clustered environment, they all provide algorithms rather than mechanisms. Applying these 
techniques to a big-data visualization problem would require signi.cant reworking of existing software. 
A different approach to dataset scalability was taken by Humphreys et al. when they integrated a parallel 
interface into WireGL [8]. By posing as the system s OpenGL driver, WireGL intercepts OpenGL commands 
made by an application (or multi­ple applications), and generates multiple new command sequences, each 
represented in a compact wire protocol. Each sequence is then transmitted over a network to a different 
server. Those servers man­age image tiles, and execute the commands encoded in the streams on behalf 
of the client. Finally, the resulting framebuffer tiles are extracted and transmitted to a compositing 
server for display. Or­dering between streams resulting from a parallel application is con­trolled using 
the parallel immediate mode graphics extensions pro­posed by Igehy et al [10]. WireGL can use either 
software-based image reassembly or custom hardware such as Lightning-2 [32] to reassemble the resulting 
image tiles and form the .nal output. This approach to cluster rendering allows existing applications 
to be par­allelized easily, since it is built upon a popular, industry-standard API. However, by imposing 
a sort-.rst architecture on the resulting application, it can be dif.cult to load-balance the graphics 
work. Load-balancing is usually attempted by using smaller tiles, but this will tend to cause primitives 
to overlap more tiles, resulting in ad­ditional load on the network and reduced scalability. More funda­mentally, 
WireGL requires that all geometry be moved over a net­work every frame, but today s networks are not 
fast enough to keep remote graphics cards busy.  2.2 Stream Processing Continual growth in typical dataset 
size and network bandwidth has made stream-based analysis a hot topic for many different disci­plines, 
such as telephone record analysis [4], multimedia, render­ing of remotely stored 3D models [27], database 
queries [2], and theoretical computer science [16]. In these domains, streams are an appropriate computational 
primitive because large amounts of data arrive continuously, and it is impractical or unnecessary to 
retain the entire data set. In the broadest sense, a stream is a potentially in.nite ordered sequence 
of records. Applications designed to op­erate on streams only access the elements of the sequence in 
order, although it is possible to buffer a portion of a stream for more global analysis. Any stream processing 
algorithm must operate on this po­tentially in.nite input set using only .nite resources. Many of the 
traditional techniques used to solve problems in computer graphics can be thought of as stream processing 
algo­rithms. Immediate-mode rendering is a classic example. In this graphics model, an unbounded sequence 
of primitives is sent one at a time through a narrow API. The graphics system processes each primitive 
in turn, using only a .nite framebuffer (and possibly tex­ture memory) to store any necessary intermediate 
results. Because such a graphics system does not have memory of past primitives, its computational expressiveness 
is limited1. Owens et al. imple­mented an OpenGL-based polygon renderer on Imagine, a pro­grammable stream 
processor [17]. Using Imagine, they achieved performance that is competitive with custom hardware while 
en­abling greater programmability at each stage in the pipeline. Mohr and Gleicher demonstrated that 
a variety of stylized draw­ing techniques could be applied to an unmodi.ed OpenGL applica­tion by only 
analyzing and modifying the stream of commands [14]. They intercept the application s API commands by 
posing as the system s OpenGL driver, in exactly the same way Chromium ob­tains its command source. Although 
some of their techniques re­quire potentially unbounded memory, some similar effects can be achieved 
using Chromium and multiple nodes in a cluster.  3 System Architecture The overall design of Chromium 
was in.uenced by Stanford s WireGL system [8]. Although the sort-.rst architecture imple­mented by WireGL 
is fairly restrictive, one critical aspect of the design led directly to Chromium: The wire protocol 
used to move image tiles from the servers to the compositor is the same as the networked-OpenGL protocol 
used to move geometry from the clients to the servers. In effect, WireGL s servers themselves be­come 
clients of a second parallel rendering application, which uses imagery as its fundamental drawing primitive. 
This means that the compositing node is not special; in fact, it is just another instance of the same 
network server executing OpenGL commands and re­solving ordering constraints on behalf of some parallel 
client. If we consider a sequence of OpenGL commands to be a stream, WireGL provides three main stream 
.lters . First, it can sort a serial stream into tiles. Next, it can dispatch a stream to a local im­plementation 
of OpenGL. Finally, WireGL can read back a frame­buffer and generate a new stream of image-drawing commands. 
In WireGL, these stream transformations can only be realized at spe­ci.c nodes in the cluster (e.g., 
an application s stream can only be sorted). To arrive at Chromium s design, we realized that it would 
be useful to perform other transformations on API streams, and it would also be necessary to arrange 
cluster nodes in a more generic topology than WireGL s many-to-many-to-few arrangement. 3.1 Cluster Nodes 
Chromium users begin by deciding which nodes in their cluster will be involved in a given parallel rendering 
run, and what communi­cation will be necessary. This is speci.ed to a centralized con.gu­ration system 
as a directed acyclic graph. Nodes in this graph rep­resent computers in a cluster, while edges represent 
network traf.c. Each node is actually divided into two parts: a transformation por­tion and a serialization 
portion. 1Because most graphics API s have some mechanism to force data to .ow back towards the host 
(i.e., glReadPixels), graphics hardware is actu­ally not a purely feed-forward stream processor. This 
fact has been exploited to perform more general computation using graphics hardware [18, 22], and extensions 
to the graphics pipeline have been proposed to further generalize its computational expressiveness [12]. 
The transformation portion of a node takes a single stream of OpenGL commands as input, and produces 
zero or more streams of OpenGL commands as output. The mapping from input to out­put is completely arbitrary. 
The output streams (if any) are sent over a network to another node in the cluster to be serialized and 
transformed again. Stream transformations are described in greater detail in section 3.2. The serialization 
portion of a node consumes one or more in­dependent OpenGL streams, each with its own associated graph­ics 
context, and produces a single OpenGL stream as output. This task is analogous to the scheduler in a 
multitasking operating sys­tem; the serializer chooses a stream to execute , and copies that stream to 
its output until the stream becomes blocked . It then se­lects another input stream, performs a context 
switch, and continues copying. Streams block and unblock via extensions to the OpenGL API that provide 
barriers and semaphores, as proposed by Igehy et al [10]. These synchronization primitives do not block 
the issuing process, but rather encode ordering constraints that will be enforced by the serializer. 
Because the serializer may have to switch be­tween contexts very frequently, we use a hierarchical OpenGL 
state tracker similar to the one described by Buck et al [3]. This state representation allows for the 
ef.cient computation of the difference between two graphics contexts, allowing for .ne-grained sharing 
of rendering resources. A node s serializer can be implemented in one of two ways. Graph nodes that have 
one or more incoming edges are realized by Chromium s network server, and are referred to as server nodes. 
Servers manage multiple incoming network connections, interpret­ing messages on those connections as 
packed representations of OpenGL streams. On the other hand, nodes that have no incoming edges must generate 
their (already serial) OpenGL streams programmati­cally. These nodes are called client nodes. Clients 
obtain their streams from standalone applications that use the OpenGL API. Chromium s application launcher 
causes these programs to load our OpenGL shared library on startup. Chromium s OpenGL library injects 
the application s commands into the node s stream trans­former, so the application does not have to be 
modi.ed to initialize or load Chromium. If there is only one client in the graph, it will typically be 
an unmodi.ed off-the-shelf OpenGL application. For graphs with multiple clients, the applications will 
have to specify the ordering constraints on their respective streams.  3.2 OpenGL Stream Processing 
Stream transformations are performed by OpenGL Stream Pro­cessing Units , or SPUs. SPUs are implemented 
as dynamically loadable libraries that provide the OpenGL interface, so each node s serializer will load 
the required libraries at run time and build an OpenGL dispatch table. SPUs are normally designed as 
generically as possible so they can be used anywhere in a graph. A simple example con.guration is shown 
in .gure 1. The client loads the tilesort SPU, which incorporates all of the sort-.rst stream processing 
logic from WireGL. The servers use the render SPU, which dispatches the incoming streams directly to 
their local graphics accelerators. This con.guration has the effect of running the unmodi.ed client application 
on a tiled display using sort-.rst stream processing, giving identical semantics and similar perfor­mance 
to the tiled display system described by Humphreys et al [7]. Notice that in .gure 1, the graph edges 
originate from the tilesort SPU, not the application itself. This convention is used because the SPU 
in fact manages its own network resources, originates connec­tions to servers, and generates traf.c. 
Chromium Server Render Chromium Server Application Tilesort  . . Figure 1: A simple Chromium con.guration. 
In this example, a serial application is made to run on a tiled display using a sort-.rst stream processor 
called tilesort.  3.3 SPU Chains A node s stream transformation need not be performed by only a single 
SPU; serializers can load a linear chain of SPUs at run time. During initialization, each SPU receives 
an OpenGL dispatch table for the next SPU in its local chain, meaning simple SPUs can be chained together 
to achieve more complex results. Using this fea­ture, a SPU might intercept and modify (or discard) calls 
to one par­ticular OpenGL function and pass the rest untouched to its down­stream SPU. This allows a 
SPU, for example, to adjust the graphics state slightly to achieve a different rendering style. One example 
of such a SPU is a wireframe style .lter. This SPU issues a glPolygonMode call to its downstream SPU 
at startup to set the drawing mode to wireframe. It then passes all OpenGL calls directly through except 
glPolygonMode, which it discards, preventing the application from resetting the drawing mode. Note that 
Chromium does not require a stream to be rendered on a dif­ferent node from where it originated; it is 
straightforward for the client to load the render SPU as part of its chain. In this way, an application 
s drawing style can be modi.ed while it runs directly on the node s graphics hardware, without any network 
traf.c. SPU chains are always initialized in back-to-front order, starting with the .nal SPU in the chain. 
At initialization, a SPU must return a list of all the functions that it implements. A SPU that wants 
to pass a function call through to the SPU immediately downstream can return the downstream SPU s function 
pointer as its own. Be­cause there is no indirection in this model, passing OpenGL calls through multiple 
SPUs does not incur any performance overhead. Such function pointer copying is common in Chromium; as 
long as SPUs copy and change OpenGL function tables using only our pro­vided API s, they can change their 
own exported interface on the .y and automatically propagate those changes throughout the node. 3.4 
SPU Inheritance A SPU need not export a complete OpenGL interface. Instead, SPUs bene.t from a single-inheritance 
model in which any func­tions not implemented by a SPU can be obtained from a parent , or super SPU. 
The SPU most commonly inherited from is the passthrough SPU, which passes all of its calls to the next 
SPU in its node s chain. The wireframe drawing SPU mentioned in the pre­vious section would likely be 
implemented this way it would im­plement only glPolygonMode, and rely on the passthrough SPU to handle 
all other OpenGL functions. At initialization, each SPU is given a dispatch table for its parent. When 
the wireframe SPU wishes to set the drawing mode to wireframe during initialization, it calls the passthrough 
SPU s implementation of glPolygonMode.  . .  . . . . Figure 2: Chromium con.gured as a complete WireGL 
re­placement. A parallel application drives a tiled display using the sort-.rst logic in the tilesort 
SPU. Imagery is then read back from the servers managing those tiles and sent to a .nal compositing server 
for display.  3.5 Provided Tools and SPUs Chromium provides four libraries that encapsulate frequently 
per­formed stream operations. The .rst is a stream packing library. This library takes a sequence of 
commands and produces a serialized en­coding of the commands and their arguments. Although this library 
is normally used to prepare commands for network transmission, it can also be used to buffer a group 
of commands for later analysis, as described in section 4.3. We use a very similar encoding method to 
the one described by Buck et al [3]. It incurs almost no wasted space, retains natural argument alignment, 
and allows a group of command opcodes and their arguments to be sent with a single call to the networking 
library. Second, we provide a stream unpacking library. This library de­codes an already serialized representation 
of a sequence of com­mands and dispatches those commands to a given SPU. This library is primarily used 
by Chromium s network server to handle incom­ing network traf.c, but it can also be used by SPUs that 
need to locally buffer a portion of a stream in order to perform more global analysis or make multiple 
passes over that portion. The third is a point-to-point connection-based networking ab­straction. This 
library abstracts the details of the underlying trans­port mechanism; we have implemented this API on 
top of TCP/IP and Myrinet. In addition, the library can be used by SPUs and applications to communicate 
with each other along channels other than those implied by the con.guration graph described in sec­tion 
3.1. This out-of-band communication allows complex com­positing SPUs to be built, such as the one described 
in section 4.1. Finally, Chromium includes a complete OpenGL state tracker. In addition to maintaining 
the entire OpenGL state, this library can ef.ciently compute the difference between two graphics contexts, 
generating a call to a given SPU for every discrepancy found. This ef.cient context differencing operation 
is due to a hierarchical rep­resentation described by Buck et al [3]. In addition to these support libraries, 
Chromium provides a num­ber of SPUs that can be used as is or extended to realize the de­sired stream 
transformation. There are too many SPUs to list here; a complete list can be found in the Chromium documentation 
at http://chromium.sourceforge.net.  3.6 Realizing Parallel Rendering Architectures We now present two 
examples of parallel rendering architectures that can be realized using Chromium. As described by Molnar 
et al., parallel rendering architectures can be classi.ed according to the point in the graphics pipeline 
at which data is sorted from an Figure 3: Another possible Chromium con.guration. In this example, nodes 
in a parallel application render their portion of the scene directly to their local hardware. The color 
and depth buffers are then read back and transmitted to a .nal composit­ing server, where they are combined 
to produce the .nal im­age. object-parallel distribution to an image-parallel distribution [15]. The 
.rst con.guration, shown in .gure 2, shows a sort-.rst graphics architecture that functions identically 
to WireGL. As in .gure 1, we use the tilesort SPU to sort the streams into tiles. Each intermediate server 
serializes its incoming streams and passes the result to the readback SPU. The readback SPU inherits 
from the render SPU using the mechanism described in section 3.4, so the streams are rendered on the 
locally housed graphics hardware. However, the readback SPU provides its own implementation of SwapBuffers, 
so at the end of the frame it extracts the framebuffer and uses glDrawPixels to pass the pixel data to 
another SPU. In the .gure, each pixel array is passed to a send SPU, which trans­mits the data to a .nal 
server for tile reassembly. Each readback SPU is con.gured at startup to know where its tiles should 
end up in the .nal display; these coordinates are passed to the send SPU using glRasterPos. The readback 
SPU also uses Igehy s parallel graphics synchronization extensions [10] to ensure that the tiles all 
arrive at their destination before the .nal rendering server displays its results. This .nal tile reassembly 
step could also be performed using custom hardware such as Lightning-2 [32]. A dramatically different 
architecture is shown in .gure 3. In this .gure, the readback SPU is loaded directly by the applica­tions. 
Recall that the readback SPU dispatches all of the OpenGL API directly to the underlying graphics hardware, 
so the application running in this con.guration bene.ts from the full performance of local 3D acceleration. 
In this case, the readback SPU is con.gured to extract both the color and depth buffers, sending them 
both to a .nal compositing server along with the appropriate OpenGL com­mands to perform a depth composite. 
In contrast to WireGL, this is a sort-last architecture. In practice, having many full framebuffers arriving 
at a single display server would be a severe bottleneck, so this architecture is rarely used. In addition, 
when doing depth com­positing in Chromium, it can be bene.cial to write a special SPU to perform the 
composite in software, because compositing depth images in OpenGL requires using the stencil buffer in 
a way that is quite slow on many architectures. A more advanced (and practical) Chromium-based sort-last 
architecture is presented in section 4.1. Because Chromium provides a virtual graphics pipeline with 
a parallel interface, the application in .gure 3 could be run unmodi­.ed on the architecture in .gure 
2 simply by specifying a different con.guration DAG. The architectures may provide different seman­tics 
(e.g., the sort-last architecture cannot guarantee ordering con­straints), but the application need not 
be aware of the change. Volume Renderer Volume Renderer Binary Swap Send Volume Renderer Binary Swap 
Send Volume Renderer Binary Swap Send Binary Swap Send Figure 4: Con.guration used for a four-node 
version of our cluster-parallel volume rendering system. Each client renders its local portion of the 
volume using local graphics hardware. Next, the volumes are composited using the binaryswap SPU. The 
SPUs use out-of-band communication to exchange partial framebuffers until each SPU contains one quarter 
of the .nal image. These partial images are then sent to a single server for display.  4 Results In 
this section, we present three different Chromium usage scenar­ios: a parallel volume renderer used to 
interactively explore a large volumetric dataset, the reintegration of an application s graphics stream 
into its original user interface on a high-resolution display device, and a stream transformation to 
achieve a non-photorealistic drawing style. 4.1 Parallel Volume Rendering Our volume rendering application 
uses 3D textures to store vol­umes and renders them with view-aligned slicing polygons, com­posited from 
back to front. Using Stanford s Real-Time Shading Language [22], we can implement different classi.cation 
and shad­ing schemes using the latest programmable graphics hardware, such as NVIDIA s GeForce3. Small 
shaders can easily exhaust these cards resources; for example, a shader that implements a simple 2D transfer 
function and a specular shading model requires two 3D texture lookups, one 2D texture lookup (dependent 
on one of the 3D lookups), and all eight register combiners. Because we store our volumes as textures, 
the maximum size of the volume that can be rendered is limited by the amount of avail­able texture memory. 
In practice, on a single GeForce3 with 64 MB of texture memory, the largest volume that can be rendered 
with the shader described above is 256.256.128. In addition, the speed of volume rendering with 3D textures 
is limited by the .ll rate of our graphics accelerator. While the theoretical .ll rate of the GeForce3 
is 800 Mpix/sec, complex fragment processing greatly decreases the attainable performance. Depending 
on the complexity of the shader being used, we achieve between 42 and 190 Mpix/sec, or roughly 5% to 
24% of the GeForce3 s theoretical peak .ll rate. Both of these limitations can be mitigated by parallelizing 
the rendering across a cluster. We .rst divide the volume among the nodes in our cluster. Each node renders 
its subvolume on locally housed graphics hardware using the binaryswap SPU, which com­posites the resulting 
framebuffers using the binary swap tech­nique described by Ma et al [11]. In this technique, rendering 
nodes are .rst grouped into pairs. Each node sends one half of its image to its counterpart, and receives 
the other half of its counterpart s im­age. This communication uses Chromium s connection-based net­working 
abstraction, described in section 3.5. The SPUs then com­posite the image they received with their local 
framebuffer. This newly composited sub-region of the image is then split in half, a dif­ferent pairing 
is chosen, and the process repeats. If there are n nodes Frames per second 20 15 10 5 0 Isosurface 2D 
Transfer Function Lit Isosurface Lit 2D Transfer Function   0 50 100 Millions of Voxels Figure 5: 
Performance of our volume renderer as larger volumes are used. In this graph, each node renders a 256.256.128 
subvolume to a 1024.256 window. The data points correspond to a cluster of 1, 2, 4, 8, and 16 nodes. 
At 16 nodes, we are rendering two copies of the full 256.256.1024 dataset. in our cluster, after log(n)steps 
each node will have completely composited 1 of the total image. Because we are compositing trans­ n 
parent images using Porter and Duff s over operator [21], the se­quence of pairings is chosen carefully 
so that blending is performed in the correct order with respect to the viewpoint. Our scalability experiments 
were conducted on a cluster of six­teen nodes, each running RedHat Linux 7.2. The nodes contain an 800 
MHz Pentium III Xeon, a GeForce3 with 64 MB of video memory, 256 MB of main memory, and a Myrinet network 
with a maximum bandwidth of approximately 100 MB/sec. The dataset is a 256.256.1024 magnetic resonance 
scan of a mouse. All of our renderings are performed in a window of size 1024.256, ensuring that each 
voxel is sampled exactly once. Table 1 shows the four shaders we used to vary the achievable per-node 
performance. Figure 4 shows the Chromium communication graph for a cluster of four nodes. Note that a 
minimum of eight nodes is required to render the full mouse volume, because each node in our cluster 
has only 64 MB of texture memory. Figure 5 shows the performance of our volume renderer as the size of 
the volume is scaled. In this experiment, we rendered a portion of the mouse dataset on each node in 
our cluster. The initial drop in performance is due to the additional framebuffer reads required, but 
because the binary swap algorithm keeps all the nodes busy while compositing, the graph .attens out, 
and we sustain nearly constant performance as the size of the volume is repeatedly doubled. At 16 nodes, 
we render two copies of the full 256.256.1024 volume at a rate between 643 MVox/sec and 1.59 GVox/sec, 
depending on the shader used. If we instead .x the size of the volume and parallelize the render­ing, 
we quickly become limited by our pixel readback and network performance. When rendering a single 256.256.128 
volume split across multiple nodes, the rendering rate rapidly becomes negli­gible. When creating a 1024.256 
image, our volume renderer s performance converges to approximately 22 frames per second. Be­cause the 
parallel image compositing and .nal transmission for dis­play happen sequentially, we can analyze this 
performance as fol­lows: With 16 nodes, each node eventually extracts and sends 15   Isosurface 2D 
Transfer Function Lit Isosurface Lit 2D Transfer Function Shader 3D textures  2D (dependent) textures 
 Register Combiners Single-Node Fill Rate (Mpix/sec)  Isosurface 1 0 6 190 2D Transfer Function 1 1 
4 98 Lit Isosurface 2 0 8 78 Lit 2D Transfer Function 2 1 8 42 Table 1: Shaders used in our volume rendering 
experiments. The lit 2D transfer function shader exhausts the resources of a GeForce3. Mouse dataset 
courtesy of the Duke Center for In Vivo Microscopy. of its framebuffer, requiring four bytes per pixel. 
The .nal trans­ 1 mission sends only 16 of a framebuffer at three bytes per pixel, but because all of 
these framebuffer portions arrive at the same node, we must consider the aggregate incoming bandwidth 
at that node, which is a full framebuffer at three bytes per pixel. This adds up to 1.69 MB/frame, or 
37.1 MB/sec. This measurement is close to our measured RGBA readback performance of the GeForce3, which 
is clearly the limiting factor for the binaryswap SPU, since our net­work can sustain 100 MB/sec. Future 
improvements in pixel read­back rate and network bandwidth would result in higher framerates, as would 
an alpha-compositing mode for a post-scanout composit­ing system such as Lightning-2.  4.2 Integration 
With an Existing User Interface Normally, when Chromium intercepts an application s graphics commands, 
that application s graphics window will be blank, with the rendering appearing in one or more separate 
windows, poten­tially distributed across multiple remote computers. Because the interface is now separated 
from the visualization, this can interfere with the productive use of some applications. To address this 
prob­lem, we have implemented the integration SPU to reincorporate remotely rendered tiles into the application 
s user interface. This way, users can apply a standard user interface to a parallel client. This manipulation 
can also be useful for serial applications. Even though the net effect is a null transformation on the 
appli­cation s stream, it can aid in driving high resolution displays. For our experiments, we use the 
IBM T221, a 3840.2400 LCD. Few graphics cards can drive this display directly, and those that can do 
not have suf.cient scanout bandwidth to do so at a high refresh rate. The T221 can be driven by up to 
four separate synchronized digital video inputs, so we can achieve higher bandwidth to the dis­play using 
a cluster and special hardware such as Lightning-2 [32], or a network-attached parallel framebuffer such 
as IBM s Scalable Graphics Engine (SGE) [19]. The SGE supports up to 16 one­gigabit ethernet inputs, 
can double buffer up to 16 million pixels, and can drive up to eight displays. In our tests, we used 
the SGE to supply four synchronized DVI outputs that collectively drive the T221 at its highest resolution. 
An X-Windows server for the SGE provides a standard user interface for this con.guration. The integration 
SPU is conceptually similar to the readback SPU in that it inherits almost all of its functionality from 
the render SPU. To extract the color information from the framebuffer, the integration SPU implements 
its own SwapBuffers handler, which uses the SGE to display those pixels on the T221. The con­.guration 
graph used to conduct this experiment is shown in .g­ure 8. The application s graphics stream is sorted 
into tiles man­aged by multiple Chromium servers, each of which dispatches its tile s stream to the integration 
SPU. The integration SPU places the resulting pixels into X regions by tunneling, meaning that the pixels 
are transferred to the SGE s framebuffer without the involve­ment of the X server that manages the display. 
Because the SGE supports multiple simultaneous writes to the framebuffer, this tech­nique does not unnecessarily 
serialize tile placement. Note that the number of tiles sent to the SGE is independent of the number 
of the SGE s outputs, so we use an 8-node cluster to drive the four outputs at interactive rates. The 
integration SPU must also properly handle changes to the size of the application s rendering area. When 
an applica­tion window is resized, it will typically call glViewport to reset its drawing area. Accordingly, 
the integration SPU overrides the render SPU s implementation of glViewport to detect these changes, 
and adjusts the size of the render tiles if necessary. Be­cause the tilesort SPU sorts based on a logical 
decomposition of the screen, it does not need to be noti.ed of this change2. Although the integration 
SPU enables functionality that is not otherwise possible, it is still important that it not impede in­teractivity. 
For our performance experiments, we used a cluster of eight nodes running RedHat Linux 7.1, each with 
two 866MHz Pentium III Xeon CPUs, 1GB of RDRAM, NVIDIA Quadro graph­ics, and both gigabit ethernet and 
Myrinet 2000 networking. One of our cluster nodes runs the SGE s X-windows server in addi­tion to the 
Chromium server. We successfully tested applications ranging from trivial (simple demos from the GLUT 
library) to a medium-complexity scienti.c visualization application (OpenDX) to a closed-source, high-complexity 
CAD package (CATIA). The graph shown in .gure 6 shows the average frame rate as we scale the display 
resolution of the T221 from 800.600 to 3840.2400. Four curves are shown, corresponding to a cluster of 
1, 2, 4, and 8 nodes. Because we want to measure only the perfor­mance impact of the integration SPU, 
we rendered only small amounts of geometry (approximately 5000 vertices per frame) us­ing the GLUT atlantis 
demo. This demo runs at a much greater rate than the refresh rate of the display, so its effect on performance 
is minimal compared to the expense of extracting and transmitting tiles. The maximum frame rate achieved 
using 4 or 8 nodes is 41 Hz, which is exactly the vertical refresh rate of the T221. Because 2Our example 
application uses only geometric primitives. In order for pixel-based primitives to be rendered correctly, 
the tilesort SPU would need to be noti.ed when the window size changes. Alternately, the tilesort SPU 
could be con.gured to broadcast all glDrawPixels calls. Frames per second 30 20 10 0 Figure 6: Performance 
of the GLUT atlantis demo using the integration SPU to drive the T221 display at differ­ent resolutions. 
Each curve shows the relationship between performance and resolution for a given number of rendering 
servers. For smaller windows, the SPU becomes limited by the vertical refresh rate of the display (41 
Hz). As the res­olution approaches 3840.2400 (9.2 million pixels), a small 8-server con.guration still 
achieves interactive refresh rates. the SGE requires hardware synchronization to the refresh rate, no 
higher frame rate can be achieved. For a given .xed resolution, the integration SPU achieves the expected 
performance increase as more rendering nodes are used, because this application is com­pletely limited 
by the speed at which we can redistribute pixels. Figure 7 shows this phenomenon more clearly. In this 
graph, the same data are plotted showing seconds per frame rather than frames per second. In addition, 
the data have been normalized by the number of nodes used, so the quantity being measured is the pixel 
throughput per node. The coincidence of the four curves shows that there is no penalty associated with 
adding rendering nodes, so lin­ear speedup is achieved until the display s refresh rate becomes the limiting 
factor. The rate at which each node can read back pixels and send them to the SGE is given by the slope 
of the line, which is approximately 12 MPix/second/node, or 48 MB/second/node. Ex­trapolating to a very 
small image size, the system overhead is ap­proximately 15 milliseconds, which indicates that the maximum 
system response rate of the integration SPU is approximately 70 Hz (in the absence of monitor refresh 
rate limitations). The measurements presented here give a worst-case scenario for the integration SPU, 
in which it is responsible for almost 100% of the overhead in the system. We are able to demonstrate 
frame rates exceeding 40Hz using only 8 nodes, and achieve an interac­tive 10 Hz even with each node 
supplying over one million pixels per frame. In addition, if measured independently, pixel readback rate 
and the SGE transfer rate can both provide bandwidths exceed­ing 23 Mpix/sec, nearly twice what they 
achieve when measured together. This leads us to believe that the system I/O bus or mem­ory subsystem 
is under-performing when these two tasks are being performed simultaneously, an effect that will likely 
be eliminated with the introduction of new I/O subsystems designed speci.cally for high-end servers. 
This is a similar contention effect to that ob­served by Humphreys et al. when evaluating WireGL on a 
cluster of SMP nodes [8].  0.2 Seconds per frame 0.1 0 Figure 7: We have replotted the data from .gure 
6 to show seconds per frame versus pixels per node, to show per-node throughput. The coincidence of the 
four curves shows that there is insigni.cant overhead to doubling the number of ren­dering nodes, so 
linear speedup is achieved until the monitor refresh rate becomes the limiting performance factor.  
4.3 Stylized Drawing For a long time, research on non-photorealistic, or stylized , ren­dering focused 
on non-interactive, batch-mode techniques. In re­cent years, however, there has been considerable interest 
in real­time stylized rendering. Early interactive NPR systems required a priori knowledge of the model 
and its connectivity [13, 26]. More recently, Raskar has shown that non-trivial NPR styles can be achieved 
with no model analysis using either standard graphics pipeline tricks [24] or slight extensions to modern 
programmable graphics hardware [23]. We have developed a simple stylized rendering .lter that cre­ates 
a .at-shaded hidden-line drawing style. Our approach is sim­ilar to that taken by Mohr and Gleicher [14], 
although we show a technique that requires only .nite storage. Hidden line drawing in OpenGL is a straightforward 
multi-pass technique, accomplished by .rst rasterizing all polygons to the depth buffer, and then re­rasterizing 
the polygon edges. The polygon depth values are offset using glPolygonOffset to reduce aliasing artifacts 
[1]. Achieving this effect in Chromium can be accomplished with a single SPU. The hiddenline SPU packs 
each graphics command into a buffer as if they were being prepared for network transport. This has the 
effect of recording the entire frame into local mem­ory. Instead of actually sending them to a server, 
we instead decode the commands twice at the end of each frame, once as polygons and once as lines, achieving 
our desired style. The code required to achieve this transformation is shown in .gure 9, and the visual 
results are shown in .gure 10. The performance impact of this SPU is shown in .gure 11. There are three 
interesting notes regarding the actual implemen­tation of a hiddenline SPU. First, the application may 
generate state queries that need to be satis.ed immediately and not recorded. In order to do this, the 
entire graphics state is maintained using our state tracking library, and any function that might affect 
the state is passed to the state tracker before being packed. This behavior is frequently overly cautious; 
most state queries are attempts to deter­  CATIA Tilesort  Integration  Figure 8: Con.guration used 
to drive IBM s 3840.2400 T221 display using Chromium. The commercial CAD package CATIA is used to create 
a tiled rendering of a jet engine nacelle (model courtesy of Goodrich Aerostructures). The tiles are 
then re-integrated into the application s original user interface, allowing CATIA to be used as designed, 
despite the distribution of its graphics workload on a cluster. Due to the capacity and range of gigabit 
ethernet, all of the computational and 3D graphics hardware can be remote from the eventual display. 
mine some fundamental limit of the graphics system (such as the maximum size of a texture), rather than 
querying state that was set by the application itself. Robust implementations of style .lters like the 
hiddenline SPU would likely bene.t from the ability to disable full state tracking. Second, the SPU does 
not play back the exact calls made in the frame. Because we want to draw all polygons in the same color 
(and similarly for lines), the application must be prevented from enabling texturing, changing the current 
color, turning on light­ing, changing the polygon draw style, enabling blending, chang­ing the line width, 
disabling the depth test, or disabling writes to the depth buffer. To accomplish this, a new OpenGL dispatch 
ta­ble is built, containing mostly functions from the SPU immediately following the hiddenline SPU in 
its chain, but with our own ver­sions of glEnable, glDisable, glDepthMask, glPolygonMode, glLineWidth, 
and all the glColor variants, which enforce these rules. Applications which rely on complex uses of these 
functions may not function properly using this SPU. Finally, some care must be taken to properly handle 
vertex ar­rays. Because the semantics of vertex arrays allow for the data buffer to be changed (or discarded) 
after it is referenced, we cannot store vertex array calls verbatim and expect them to decode prop­erly 
later in the frame. Instead, we transform uses of vertex arrays back into sequences of separate OpenGL 
calls. Although this could be done by the hiddenline SPU itself, we have found this trans­formation to 
be useful in other situations, so we have implemented the vertex array .ltering in a separate vertexarray 
SPU. This SPU appears immediately before the hiddenline SPU in .gure 10. It should be noted that the 
hiddenline SPU as presented re­quires potentially in.nite storage, since it buffers the entire frame, 
and therefore cannot be considered a true stream processor. There are two possible solutions to this 
problem. One is to perform primitive assembly in the hiddenline SPU, drawing each styl­ized primitive 
separately. This technique does satisfy our resource constraints (extremely large polygonal primitives 
can be split into smaller ones), but would result in a signi.cant performance penalty for applications 
with a high frame rate, due to the overhead of soft­ware primitive assembly as well as the frequent state 
changes. A better solution to this problem is to use multiple cluster nodes, as shown in .gure 12. Rather 
than buffering the entire frame, we void hiddenline_SwapBuffers( void ) { /* Draw filled polygons */ 
super.Clear( color and depth ); super.PolygonOffset( 1.5f, 0.000001f ); super.PolygonMode( GL_FRONT_AND_BACK, 
GL_FILL ); super.Color3f( poly_r, poly_g, poly_b ); PlaybackFrame( modified_child_dispatch ); /* Draw 
outlined polygons */ super.PolygonMode( GL_FRONT_AND_BACK, GL_LINE ); super.Color3f( line_r, line_g, 
line_b ); PlaybackFrame( modified_child_dispatch ); super.SwapBuffers(); } Figure 9: End-of-frame logic 
for a simple hidden-line style SPU. The entire frame is played back twice, once as depth­offset .lled 
polygons, and once as lines. We modify the down­stream SPU s dispatch table to discard calls that would 
affect our drawing style, such as texture enabling and color changes. send the entire stream verbatim 
to two servers, one rendering the incoming stream as depth-offset polygons, the other as lines. In­stead 
of writing two new SPUs for each of these rendering styles, we would inject the appropriate OpenGL calls 
into the streams be­fore transmission. We then use the readback and send SPUs to combine the two renderings 
using a depth-compositing network, as described in section 3.6. Note that we could more economically 
use our resources by rendering depth-offset polygons locally and for­warding the stream to a single line-rendering 
node (or vice versa), thereby requiring only three nodes instead of four, although this would require 
a more complex implementation.  5 Discussion and Future Work In their seminal paper on virtual graphics, 
Voorhies, Kirk and Lath­rop note that providing a level of abstraction between an applica­Figure 10: 
Drawing style enabled by the hiddenline SPU. After uses of vertex arrays are .ltered out, the SPU records 
the entire frame, and plays it back twice to achieve a hidden-line effect. No high-level knowledge of 
the model is required. Frames per second 200 150 100 50 0 Figure 11: Performance of Quake III running 
a prerecorded demo. The .rst 90 frames are devoted to an introductory splash screen and are not shown 
here. The red curve shows the performance achieved by the application alone. The blue curve shows the 
same demo using just the vertexarray SPU, and the green curve gives the performance of the demo ren­dering 
with a hidden-line style. Despite more than a 2:1 re­duction in speed, the demo still runs at approximately 
40-50 frames per second. tion and the graphics hardware allows for cleaner software de­sign, higher performance, 
and effective concurrent use of the dis­play [33]. We believe that the power and implications of these 
observations have not yet been fully explored. Chromium provides a compelling mechanism with which to 
further investigate the po­tential of virtual graphics. Because Chromium provides a complete graphics 
API (many of the key SPUs such as tilesort, send, and render pass almost all of the OpenGL conformance 
tests), it is no longer necessary to write custom applications to test new ideas in graphics API processing. 
Also, the barrier to entry is quite low; for example, the hiddenline SPU described in section 4.3 adds 
only approximately 250 lines of code to Chromium s SPU template. In the future, we would like to see 
Chromium applied to new application domains, especially new ideas in scalable interactive graphics on 
clusters. Of particular interest is the problem of man­aging enormous time-varying datasets, both volumetric 
and polyg-Figure 12: A different usage model for achieving a hidden­line drawing style. In this example, 
the .lled polygon stream and the wireframe stream are sent to two different rendering servers and the 
resulting images are depth composited. This way, no single SPU needs to buffer the entire frame, and 
the system requires only .nite resources.  onal. Today s time-varying volumetric datasets can easily 
exceed 30 terabytes in size. We intend to build a new parallel rendering application designed speci.cally 
for interactively visualizing these datasets on a cluster, using Chromium as the underlying transport, 
rendering, and compositing mechanism. We are particularly interested in building infrastructure to sup­port 
.exible remote graphics. We believe that a clean separation between a scalable graphics resource and 
the eventual display has the potential to change the way we use graphics every day. We are actively pursuing 
a new direction to make scalable cluster­based graphics appear as a remote, shared service akin to a 
network mounted .lesystem. Most of all, we hope that Chromium will be adopted as a com­mon low-level 
mechanism for enabling new graphics algorithms, particularly for clusters. If this happens, research 
results in cluster graphics can more easily be applied to existing problems outside the original researcher 
s lab. 6 Conclusions We have described Chromium, a .exible framework for manipulat­ing streams of graphics 
API commands on clusters of workstations. Chromium s stream processors can be con.gured to provide a 
sort­.rst parallel rendering architecture with a parallel interface, or a sort-last architecture capable 
of handling most of the same appli­cations. Chromium s .exibility makes it an ideal launching point for 
new research in parallel rendering systems, particularly those that target clusters of commodity hardware. 
In addition, it is likely that Chromium s stream-processing model can be applied to other problems in 
visualization and computer illustration. Application Hiddenline2 Chromium Server Readback Send Chromium 
Server Readback Send Chromium Server Render   Acknowledgments The authors would like to thank Brian 
Paul and Alan Hourihaine for their tireless efforts to make Chromium more robust. Allan Johnson, Gary 
Cofer, Sally Gewalt, and Laurence Hedlund from the Duke Center for In Vivo Microscopy (an NIH/NCRR National 
Re­source) provided the dataset for our volume renderer. Kekoa Proud­foot and Bill Mark provided assistance 
with the implementation of a volume renderer on top of the Stanford Real Time Shading Lan­guage. Finally, 
we would like to especially thank all the WireGL and Chromium users for their continued support. This 
work was funded by DOE contract B504665, and was also performed under the auspices of the U.S. Department 
of Energy by the University of California, Lawrence Livermore National Laboratory under con­tract No. 
W-7405-Eng-48 (UCRL-JC-146802).  References [1] Advanced Graphics Progamming Techniques Using OpenGL. 
SIGGRAPH 1998 Course Notes. [2] Shivnath Babu and Jennifer Widom. Continuous queries over data streams. 
SIGMOD Record, pages 109 120, September 2001. [3] Ian Buck, Greg Humphreys, and Pat Hanrahan. Track­ing 
graphics state for networked rendering. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, 
pages 87 95, August 2000. [4] Corrina Cortes, Kathleen Fisher, Daryl Pregibon, Anne Rodgers, and Frederick 
Smith. Hancock: A language for ex­tracting signatures from data streams. Proceedings of 2000 ACM SIGKDD 
International Conference on Knowledge and Data Mining, pages 9 17, August 2000. [5] Thomas Funkhouser. 
Coarse-grained parallelism for hierar­chical radiosity using group iterative methods. Proceedings of 
SIGGRAPH 96, pages 343 352, August 1996. [6] Christopher Giertsen and Johnny Peterson. Parallel volume 
rendering on a network of workstations. IEEE Computer Graphics and Applications, pages 16 23, November 
1993. [7] Greg Humphreys, Ian Buck, Matthew Eldridge, and Pat Han­rahan. Distributed rendering for scalable 
displays. IEEE Su­percomputing 2000, October 2000. [8] Greg Humphreys, Matthew Eldridge, Ian Buck, Gordon 
Stoll, Matthew Everett, and Pat Hanrahan. WireGL: A scalable graphics system for clusters. Proceedings 
of SIGGRAPH 2001, pages 129 140, August 2001. [9] Greg Humphreys and Pat Hanrahan. A distributed graphics 
system for large tiled displays. IEEE Visualization 99, pages 215 224, October 1999. [10] Homan Igehy, 
Gordon Stoll, and Pat Hanrahan. The design of a parallel graphics interface. Proceedings of SIGGRAPH 
98, pages 141 150, July 1998. [11] Kwan-Liu Ma, James Painter, Charles Hansen, and Michael Krogh. Parallel 
volume rendering using binary-swap image compositing. IEEE Computer Graphics and Applications, pages 
59 68, July 1994. [12] William Mark and Kekoa Proudfoot. The F-buffer: A ras­terization order FIFO buffer 
for multi-pass rendering. Pro­ceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 
57 64, August 2001. [13] Lee Markosian, Michael Kowalski, Samuel Trychin, Lubomir Bourdev, Daniel Goldstein, 
and John Hughes. Real-time non­photorealistic rendering. Proceedings of SIGGRAPH 1997, pages 415 420. 
[14] Alex Mohr and Michael Gleicher. Non-invasive, interac­tive, stylized rendering. ACM Symposium on 
Interactive 3D Graphics, pages 175 178, March 2001. [15] Steve Molnar, Michael Cox, David Ellsworth, 
and Henry Fuchs. A sorting classi.cation of parallel rendering. IEEE Computer Graphics and Algorithms, 
pages 23 32, July 1994. [16] Liadan O Callaghan, Nina Mishra, Adam Meyerson, Sudipto Guha, and Rajeev 
Motwani. Streaming-data algorithms for high-quality clustering. To appear in Proceedings of IEEE International 
Conference on Data Engineering, March 2002. [17] John Owens, William Dally, Ujval Kapasi, Scott Rixner, 
Pe­ter Mattson, and Ben Mowery. Polygon rendering on a stream architecture. Proceedings of SIGGRAPH/Eurographics 
Work­shop on Graphics Hardware, pages 23 32, August 2000. [18] Mark Peercy, Marc Olano, John Airey, and 
Jeffrey Ungar. In­teractive multi-pass programmable shading. Proceedings of SIGGRAPH 2000, pages 425 
432, August 2000. [19] Kenneth Perrine and Donald Jones. Parallel graphics and in­teractivity with the 
scaleable graphics engine. IEEE Super­computing 2001, November 2001. [20] Pixar animation studios. PhotoRealistic 
RenderMan Toolkit. 1998. [21] Thomas Porter and Tom Duff. Compositing digital images. Proceedings of 
SIGGRAPH 84, pages 253 259, July 1984. [22] Kekoa Proudfoot, William Mark, Svetoslav Tzvetkov, and Pat 
Hanrahan. A real time procedural shading system for pro­grammable graphics hardware. Proceedings of SIGGRAPH 
2001, pages 159 170, August 2001. [23] Ramesh Raskar. Hardware support for non-photorealistic ren­dering. 
Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 41 46, August 2001. [24] Ramesh 
Raskar and Michael Cohen. Image precision silhou­ette edges. ACM Symposium on Interactive 3D Graphics, 
pages 135 140, April 1999. [25] Rodney Recker, David George, and Donald Greenberg. Ac­celeration techniques 
for progressive re.nement radiosity. ACM Symposium on Interactive 3D Graphics, pages 59 66, 1990. [26] 
Jareck Rossignac and Maarten van Emmerik. Hid­den contours on a framebuffer. Proceedings of SIGGRAPH/Eurographics 
Workshop on Graphics Hard­ware, September 1992. [27] Szymon Rusinkiewicz and Marc Levoy. Streaming QSplat: 
A viewer for networked visualization of large, dense models. ACM Symposium on Interactive 3D Graphics, 
pages 63 68, 2001. [28] Rudrajit Samanta, Thomas Funkhouser, and Kai Li. Paral­lel rendering with k-way 
replication. IEEE Symposium on Parallel and Large-Data Visualization and Graphics, October 2001. [29] 
Rudrajit Samanta, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Sort-.rst parallel rendering with 
a clus­ter of PCs. SIGGRAPH 2000 Technical Sketch, August 2000. [30] Rudrajit Samanta, Thomas Funkhouser, 
Kai Li, and Jaswinder Pal Singh. Hybrid sort-.rst and sort-last par­allel rendering with a cluster of 
PCs. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, pages 97 108, August 2000. [31] 
Rudrajit Samanta, Jiannan Zheng, Thomas Funkhouser, Kai Li, and Jaswinder Pal Singh. Load balancing for 
multi-projector rendering systems. Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware, 
pages 107 116, August 1999. [32] Gordon Stoll, Matthew Eldridge, Dan Patterson, Art Webb, Steven Berman, 
Richard Levy, Chris Caywood, Milton Taveira, Stephen Hunt, and Pat Hanrahan. Lightning-2: A high-performance 
display subsystem for PC clusters. Pro­ceedings of SIGGRAPH 2001, pages 141 148, August 2001. [33] Douglas 
Voorhies, David Kirk, and Olin Lathrop. Virtual graphics. Proceedings of SIGGRAPH 88, pages 247 253, 
Au­gust 1988. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566640</article_id>
		<sort_key>703</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Ray tracing on programmable graphics hardware]]></title>
		<page_from>703</page_from>
		<page_to>712</page_to>
		<doi_number>10.1145/566570.566640</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566640</url>
		<abstract>
			<par><![CDATA[Recently a breakthrough has occurred in graphics hardware: fixed function pipelines have been replaced with programmable vertex and fragment processors. In the near future, the graphics pipeline is likely to evolve into a general programmable stream processor capable of more than simply feed-forward triangle rendering.In this paper, we evaluate these trends in programmability of the graphics pipeline and explain how ray tracing can be mapped to graphics hardware. Using our simulator, we analyze the performance of a ray casting implementation on next generation programmable graphics hardware. In addition, we compare the performance difference between non-branching programmable hardware using a multipass implementation and an architecture that supports branching. We also show how this approach is applicable to other ray tracing algorithms such as Whitted ray tracing, path tracing, and hybrid rendering algorithms. Finally, we demonstrate that ray tracing on graphics hardware could prove to be faster than CPU based implementations as well as competitive with traditional hardware accelerated feed-forward triangle rendering.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[programmable graphics hardware]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382471</person_id>
				<author_profile_id><![CDATA[81100647803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Purcell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35026534</person_id>
				<author_profile_id><![CDATA[81100248942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39035744</person_id>
				<author_profile_id><![CDATA[81100279370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Mark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University and NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[3DLABS, 2001. OpenGL 2.0 whitepapers web site. http://www.3dlabs.com/support/developer/ogl2/index.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>255132</ref_obj_id>
				<ref_obj_pid>77726</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALVERSON, R., CALLAHAN, D., CUMMINGS, D., KOBLENZ, B., PORTERFIELD, A., AND SMITH, B. 1990. The Tera computer system. In Proceedings of the 1990 International Conference on Supercomputing, 1-6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[AMANATIDES, J., AND WOO, A. 1987. A fast voxel traversal algorithm for ray tracing. In Eurographics '87, 3-10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258725</ref_obj_id>
				<ref_obj_pid>258694</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ANDERSON, B., STEWART, A., MACAULAY, R., AND WHITTED, T. 1997. Accommodating memory latency in a low-cost rasterizer. In 1997 SIGGRAPH/Eurographics Workshop on Graphics hardware, 97-102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[ATI, 2001. RADEON 8500 product web site. http://www.ati.com/products/pc/radeon8500128/index.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569052</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CARR, N. A., HALL, J. D., AND HART, J. C. 2002. The ray engine. Tech. Rep. UIUCDCS-R-2002-2269, Department of Computer Science, University of Illinois.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>55429</ref_obj_id>
				<ref_obj_pid>55364</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DELANY, H. C. 1988. Ray tracing on a connection machine. In Proceedings of the 1988 International Conference on Supercomputing, 659-667.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FAJARDO, M. 2001. Monte carlo ray tracing in action. In State of the Art in Monte Carlo Ray Tracing for Realistic Image Synthesis - SIGGRAPH 2001 Course 29. 151-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13044</ref_obj_id>
				<ref_obj_pid>13043</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FUJIMOTO, A., TANAKA, T., AND IWATA, K. 1986. ARTS: Accelerated ray tracing system. IEEE Computer Graphics and Applications 6, 4, 16-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HALL, D., 2001. The AR350: Today's ray trace rendering processor. 2001 SIGGRAPH / Eurographics Workshop On Graphics Hardware - Hot 3D Session 1. http://graphicshardware.org/previous/www_2001/presentations/Hot3D_Daniel_Hall.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HAVRAN, V., PRIKRYL, J., AND PURGATHOFER, W. 2000. Statistical comparison of ray-shooting efficiency schemes. Tech. Rep. TR-186-2-00-14, Institute of Computer Graphics, Vienna University of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285321</ref_obj_id>
				<ref_obj_pid>285305</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[IGEHY, H., ELDRIDGE, M., AND PROUDFOOT, K. 1998. Prefetching in a texture cache architecture. In 1998 SIGGRAPH / Eurographics Workshop on Graphics hardware, 133-ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. T. 1986. The rendering equation. In Computer Graphics (Proceedings of ACM SIGGRAPH 86), 143-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KHAILANY, B., DALLY, W. J., RIXNER, S., KAPASI, U. J., MATTSON, P., NAMKOONG, J., OWENS, J. D., AND TOWLES, B. 2000. IMAGINE: Signal and image processing using streams. In Hot Chips 12. IEEE Computer Society Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KIRK, D., 2001. GeForce3 architecture overview. http://developer.nvidia.com/docs/IO/1271/ATT/GF3ArchitectureOverview.ppt.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LINDHOLM, E., KILGARD, M. J., AND MORETON, H. 2001. A user-programmable vertex engine. In Proceedings of ACM SIGGRAPH 2001, 149-158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383527</ref_obj_id>
				<ref_obj_pid>383507</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MARK, W. R., AND PROUDFOOT, K. 2001. The F-buffer: A rasterization-order FIFO buffer for multi-pass rendering. In 2001 SIGGRAPH / Eurographics Workshop on Graphics Hardware.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MARSHALL, B., 2001. DirectX graphics future. Meltdown 2001 Conference. http://www.microsoft.com/mscorp/corpevents/meltdown2001/ppt/DXG9.ppt.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MICROSOFT, 2001. DirectX product web site. http://www.microsoft.com/directx/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S., EYLES, J., AND POULTON, J. 1992. PixelFlow: High-speed rendering using image composition. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), 231-240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[NVIDIA, 2001. GeForce3 Ti Family: Product overview. 10.01v1. http://www.nvidia.com/docs/lo/1050/SUPP/gf3ti_overview.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288266</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PARKER, S., SHIRLEY, P., LIVNAT, Y., HANSEN, C., AND SLOAN, P.-P. 1998. Interactive ray tracing for isosurface rendering. In IEEE Visualization '98, 233-238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300537</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PARKER, S., MARTIN, W., SLOAN, P.-P. J., SHIRLEY, P., SMITS, B., AND HANSEN, C. 1999. Interactive ray tracing. In 1999 ACM Symposium on Interactive 3D Graphics, 119-126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344976</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[PEERCY, M. S., OLANO, M., AIREY, J., AND UNGAR, P. J. 2000. Interactive multi-pass programmable shading. In Proceedings of ACM SIGGRAPH 2000, 425-432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732126</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[REINHARD, E., SMITS, B., AND HANSEN, C. 2000. Dynamic acceleration structures for interactive ray tracing. In Rendering Techniques 2000: 11th Eurographics Workshop on Rendering, 299-306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SPITZER, J., 2001. Texture compositing with register combiners. http://developer.nvidia.com/docs/IO/1382/ATT/RegisterCombiners.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[TORBORG, J., AND KAJIYA, J. T. 1996. Talisman: Commodity realtime 3D graphics for the PC. In Proceedings of ACM SIGGRAPH 96, 353-363.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732298</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[WALD, I., SLUSALLEK, F., AND BENTHIN, C. 2001. Interactive distributed ray tracing of highly complex models. In Rendering Techniques 2001: 12th Eurographics Workshop on Rendering, 277-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WALD, I., SLUSALLEK, P., BENTHIN, C., AND WAGNER, M. 2001. Interactive rendering with coherent ray tracing. Computer Graphics Forum 20, 3, 153-164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WHITTED, T. 1980. An improved illumination model for shaded display. Communications of the ACM 23, 6, 343-349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ray Tracing on Programmable Graphics Hardware Timothy J. Purcell Ian Buck William R. Mark * Pat Hanrahan 
Stanford University Abstract Recently a breakthrough has occurred in graphics hardware: .xed function 
pipelines have been replaced with programmable vertex and fragment processors. In the near future, the 
graphics pipeline is likely to evolve into a general programmable stream processor capable of more than 
simply feed-forward triangle rendering. In this paper, we evaluate these trends in programmability of 
the graphics pipeline and explain how ray tracing can be mapped to graphics hardware. Using our simulator, 
we analyze the per­formance of a ray casting implementation on next generation pro­grammable graphics 
hardware. In addition, we compare the perfor­mance difference between non-branching programmable hardware 
using a multipass implementation and an architecture that supports branching. We also show how this approach 
is applicable to other ray tracing algorithms such as Whitted ray tracing, path tracing, and hybrid rendering 
algorithms. Finally, we demonstrate that ray trac­ing on graphics hardware could prove to be faster than 
CPU based implementations as well as competitive with traditional hardware accelerated feed-forward triangle 
rendering. CR Categories: I.3.1 [Computer Graphics]: Hardware Architecture Graphics processors I.3.7 
[Computer Graphics]: Three-Dimensional Graphics and Realism Raytracing Keywords: Programmable Graphics 
Hardware, Ray Tracing 1 Introduction Real-time ray tracing has been a goal of the computer-graphics community 
for many years. Recently VLSI technology has reached the point where the raw computational capability 
of a single chip is suf.cient for real-time ray tracing. Real-time ray tracing has been demonstrated 
on small scenes on a single general-purpose CPU with SIMD .oating point extensions [Wald et al. 2001b], 
and for larger scenes on a shared memory multiprocessor [Parker et al. 1998; Parker et al. 1999] and 
a cluster [Wald et al. 2001b; Wald et al. 2001a]. Various efforts are under way to develop chips spe­cialized 
for ray tracing, and ray tracing chips that accelerate off-line rendering are commercially available 
[Hall 2001]. Given that real­time ray tracing is possible in the near future, it is worthwhile to study 
implementations on different architectures with the goal of providing maximum performance at the lowest 
cost. *Currently at NVIDIA Corporation {tpurcell, ianbuck, billmark, hanrahan}@graphics.stanford.edu 
Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from 
Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 
$5.00 In this paper, we describe an alternative approach to real-time ray tracing that has the potential 
to out perform CPU-based algorithms without requiring fundamentally new hardware: using commodity programmable 
graphics hardware to implement ray tracing. Graph­ics hardware has recently evolved from a .xed-function 
graph­ics pipeline optimized for rendering texture-mapped triangles to a graphics pipeline with programmable 
vertex and fragment stages. In the near-term (next year or two) the graphics processor (GPU) fragment 
program stage will likely be generalized to include .oat­ing point computation and a complete, orthogonal 
instruction set. These capabilities are being demanded by programmers using the current hardware. As 
we will show, these capabilities are also suf­.cient for us to write a complete ray tracer for this hardware. 
As the programmable stages become more general, the hardware can be considered to be a general-purpose 
stream processor. The stream processing model supports a variety of highly-parallelizable algo­rithms, 
including ray tracing. In recent years, the performance of graphics hardware has in­creased more rapidly 
than that of CPUs. CPU designs are opti­mized for high performance on sequential code, and it is becoming 
increasingly dif.cult to use additional transistors to improve per­formance on this code. In contrast, 
programmable graphics hard­ware is optimized for highly-parallel vertex and fragment shading code [Lindholm 
et al. 2001]. As a result, GPUs can use additional transistors much more effectively than CPUs, and thus 
sustain a greater rate of performance improvement as semiconductor fabri­cation technology advances. 
The convergence of these three separate trends suf.cient raw performance for single-chip real-time ray 
tracing; increasing GPU programmability; and faster performance improvements on GPUs than CPUs make 
GPUs an attractive platform for real-time ray tracing. GPU-based ray tracing also allows for hybrid rendering 
algorithms; e.g. an algorithm that starts with a Z-buffered rendering pass for visibility, and then uses 
ray tracing for secondary shadow rays. Blurring the line between traditional triangle rendering and ray 
tracing allows for a natural evolution toward increased realism. In this paper, we show how to ef.ciently 
implement ray tracing on GPUs. The paper contains three main contributions: We show how ray tracing 
can be mapped to a stream pro­cessing model of parallel computation. As part of this map­ping, we describe 
an ef.cient algorithm for mapping the in­nermost ray-triangle intersection loop to multiple rendering 
passes. We then show how the basic ray caster can be ex­tended to include shadows, re.ections, and path 
tracing.  We analyze the streaming GPU-based ray caster s perfor­mance and show that it is competitive 
with current CPU-based ray casting. We also show initial results for a system including secondary rays. 
We believe that in the near future, GPU-based ray tracing will be much faster than CPU-based ray tracing. 
 To guide future GPU implementations, we analyze the com­pute and memory bandwidth requirements of ray 
casting on GPUs. We study two basic architectures: one architecture without branching that requires multiple 
passes, and another with branching that requires only a single pass. We show that  the single pass version 
requires signi.cantly less bandwidth, and is compute-limited. We also analyze the performance of the 
texture cache when used for ray casting and show that it is very effective at reducing bandwidth. 2 
Programmable Graphics Hardware 2.1 The Current Programmable Graphics Pipeline Figure 1: The programmable 
graphics pipeline. A diagram of a modern graphics pipeline is shown in .gure 1. Today s graphics chips, 
such as the NVIDIA GeForce3 [NVIDIA 2001] and the ATI Radeon 8500 [ATI 2001] replace the .xed­function 
vertex and fragment (including texture) stages with pro­grammable stages. These programmable vertex and 
fragment en­gines execute user-de.ned programs and allow .ne control over shading and texturing calculations. 
An NVIDIA vertex program consists of up to 128 4-way SIMD .oating point instructions [Lind­holm et al. 
2001]. This vertex program is run on each incoming ver­tex and the computed results are passed on to 
the rasterization stage. The fragment stage is also programmable, either through NVIDIA register combiners 
[Spitzer 2001] or DirectX 8 pixel shaders [Mi­crosoft 2001]. Pixel shaders, like vertex programs, provide 
a 4-way SIMD instruction set augmented with instructions for texturing, but unlike vertex programs operate 
on .xed-point values. In this pa­per, we will be primarily interested in the programmable fragment pipeline; 
it is designed to operate at the system .ll rate (approxi­mately 1 billion fragments per second). Programmable 
shading is a recent innovation and the current hardware has many limitations: Vertex and fragment programs 
have simple, incomplete in­struction sets. The fragment program instruction set is much simpler than 
the vertex instruction set.  Fragment program data types are mostly .xed-point. The in­put textures 
and output framebuffer colors are typically 8-bits per color component. Intermediate values in registers 
have only slightly more precision.  There are many resource limitations. Programs have a limited number 
of instructions and a small number of registers. Each stage has a limited number of inputs and outputs 
(e.g. the number of outputs from the vertex stage is constrained by the number of vertex interpolants). 
 The number of active textures and the number of dependent textures is limited. Current hardware permits 
certain instruc­tions for computing texture addresses only at certain points within the program. For 
example, a DirectX 8 PS 1.4 pixel  shader has two stages: a .rst texture addressing stage consist­ing 
of four texture fetch instructions followed by eight color blending instructions, and then a color computation 
stage con­sisting of additional texture fetches followed by color com­bining arithmetic. This programming 
model permits a single level of dependent texturing. Only a single color value may be written to the 
framebuffer in each pass.  Programs cannot loop and there are no conditional branching instructions. 
 2.2 Proposed Near-term Programmable Graphics Pipeline The limitations of current hardware make it dif.cult 
to implement ray tracing in a fragment program. Fortunately, due to the inter­est in programmable shading 
for mainstream game applications, programmable pipelines are rapidly evolving and many hardware and software 
vendors are circulating proposals for future hardware. In fact, many of the current limitations are merely 
a result of the fact that they represent the very .rst generation of programmable hardware. In this paper, 
we show how to implement a ray tracer on an extended hardware model that we think approximates hard­ware 
available in the next year or two. Our model is based loosely on proposals by Microsoft for DirectX 9.0 
[Marshall 2001] and by 3DLabs for OpenGL 2.0 [3DLabs 2001]. Our target baseline architecture has the 
following features: A programmable fragment stage with .oating point instruc­tions and registers. We 
also assume .oating point texture and framebuffer formats.  Enhanced fragment program assembly instructions. 
We in­clude instructions which are now only available at the vertex level. Furthermore, we allow longer 
programs; long enough so that our basic ray tracing components may be downloaded as a single program 
(our longest program is on the order of 50 instructions).  Texture lookups are allowed anywhere within 
a fragment pro­gram. There are no limits on the number of texture fetches or levels of texture dependencies 
within a program.  Multiple outputs. We allow 1 or 2 .oating point RGBA (4­vectors) to be written to 
the framebuffer by a fragment pro­gram. We also assume the fragment program can render di­rectly to a 
texture or the stencil buffer.  We consider these enhancements a natural evolution of current graphics 
hardware. As already mentioned, all these features are actively under consideration by various vendors. 
At the heart of any ef.cient ray tracing implementation is the ability to traverse an acceleration structure 
and test for an intersec­tion of a ray against a list of triangles. Both these abilities require a looping 
construct. Note that the above architecture does not in­clude data-dependent conditional branching in 
its instruction set. Despite this limitation, programs with loops and conditionals can be mapped to this 
baseline architecture using the multipass render­ing technique presented by Peercy et al. [2000]. To 
implement a conditional using their technique, the conditional predicate is .rst evaluated using a sequence 
of rendering passes, and then a sten­cil bit is set to true or false depending on the result. The body 
of the conditional is then evaluated using additional rendering passes, but values are only written to 
the framebuffer if the corresponding fragment s stencil bit is true. Although their algorithm was developed 
for a .xed-function graphics pipeline, it can be extended and used with a programmable pipeline. We assume 
the addition of two hardware features to make the Peercy et al. algorithm more ef.cient: direct setting 
of stencil bits and an early fragment kill similar to Z occlusion culling [Kirk 2001]. In the standard 
OpenGL pipeline, stencil bits may be set by testing the alpha value. The alpha value is computed by the 
frag­ment program and then written to the framebuffer. Setting the sten­cil bit from the computed alpha 
value requires an additional pass. Since fragment programs in our baseline architecture can modify the 
stencil values directly, we can eliminate this extra pass. Another important rendering optimization is 
an early fragment kill. With an early fragment kill, the depth or stencil test is executed before the 
fragment program stage and the fragment program is executed only if the fragment passes the stencil test. 
If the stencil bit is false, no in­structions are executed and no texture or framebuffer bandwidth is 
used (except to read the 8-bit stencil value). Using the combination of these two techniques, multipass 
rendering using large fragment programs under the control of the stencil buffer is quite ef.cient. As 
we will see, ray tracing involves signi.cant looping. Al­though each rendering pass is ef.cient, extra 
passes still have a cost; each pass consumes extra bandwidth by reading and writing inter­mediate values 
to texture (each pass also requires bandwidth to read stencil values). Thus, fewer resources would be 
used if these inner loops over voxels and triangles were coalesced into a single pass. The obvious way 
to do this would be to add branching to the frag­ment processing hardware. However, adding support for 
branch­ing increases the complexity of the GPU hardware. Non-branching GPUs may use a single instruction 
stream to feed several fragment pipelines simultaneously (SIMD computation). GPUs that support branching 
require a separate instruction stream for each processing unit (MIMD computation). Therefore, graphics 
architects would like to avoid branching if possible. As a concrete example of this trade off, we evaluate 
the ef.ciency of ray casting on two architec­tures, one with and one without branching: Multipass Architecture. 
Supports arbitrary texture reads, .oating-point texture and framebuffer formats, general .oat­ing point 
instructions, and two .oating point 4-vector outputs. Branching is implemented via multipass rendering. 
 Branching Architecture. Multipass architecture enhanced to include support for conditional branching 
instructions for loops and control .ow.  2.3 The Streaming Graphics Processor Abstraction As the graphics 
processor evolves to include a complete instruc­tion set and larger data types, it appears more and more 
like a general-purpose processor. However, the challenge is to intro­duce programmability without compromising 
performance, for oth­erwise the GPU would become more like the CPU and lose its cost­performance advantages. 
In order to guide the mapping of new ap­plications to graphics architectures, we propose that we view 
next­generation graphics hardware as a streaming processor. Stream processing is not a new idea. Media 
processors transform streams of digital information as in MPEG video decode. The IMAGINE processor is 
an example of a general-purpose streaming processor [Khailany et al. 2000]. Streaming computing differs 
from traditional computing in that the system reads the data required for a computation as a sequential 
stream of elements. Each element of a stream is a record of data requiring a similar computation. The 
system executes a program or kernel on each element of the input stream placing the result on an output 
stream. In this sense, a programmable graphics processor executes a vertex program on a stream of vertices, 
and a fragment program on a stream of fragments. Since, for the most part we are ignoring vertex programs 
and rasterization, we are treating the graphics chip as basically a streaming fragment processor. The 
streaming model of computation leads to ef.cient imple­mentations for three reasons. First, since each 
stream element s computation is independent from any other, designers can add ad­ditional pipelines that 
process elements of the stream in parallel. Second, kernels achieve high arithmetic intensity. That is, 
they per­form a lot of computation per small .xed-size record. As a result the computation to memory 
bandwidth ratio is high. Third, stream­ing hardware can hide the memory latency of texture fetches by 
using prefetching [Torborg and Kajiya 1996; Anderson et al. 1997; Igehy et al. 1998]. When the hardware 
fetches a texture for a frag­ment, the fragment registers are placed in a FIFO and the fragment processor 
starts processing another fragment. Only after the texture is fetched does the processor return to that 
fragment. This method of hiding latency is similar to multithreading [Alverson et al. 1990] and works 
because of the abundant parallelism in streams. In sum­mary, the streaming model allows graphics hardware 
to exploit par­allelism, to utilize bandwidth ef.ciently, and to hide memory la­tency. As a result, graphics 
hardware makes ef.cient use of VLSI resources. The challenge is then to map ray tracing onto a streaming 
model of computation. This is done by breaking the ray tracer into kernels. These kernels are chained 
together by streams of data, originating from data stored in textures and the framebuffer.  3 Streaming 
Ray Tracing In this section, we show how to reformulate ray tracing as a stream­ing computation. A .ow 
diagram for a streaming ray tracer is found in .gure 2. Camera Grid of Triangle List Offsets Triangle 
List Triangles Normals Materials Figure 2: A streaming ray tracer. In this paper, we assume that all 
scene geometry is represented as triangles stored in an acceleration data structure before rendering 
begins. In a typical scenario, an application would specify the scene geometry using a display list, 
and the graphics library would place the display list geometry into the acceleration data structure. 
We will not consider the cost of building this data structure. Since this may be an expensive operation, 
this assumption implies that the algorithm described in this paper may not be ef.cient for dynamic scenes. 
The second design decision was to use a uniform grid to accel­erate ray tracing. There are many possible 
acceleration data struc­tures to choose from: bounding volume hierarchies, bsp trees, k­d trees, octrees, 
uniform grids, adaptive grids, hierarchical grids, etc. We chose uniform grids for two reasons. First, 
many experi­ments have been performed using different acceleration data struc­tures on different scenes 
(for an excellent recent study see Havran et al. [2000]). From these studies no single acceleration data 
struc­ture appears to be most ef.cient; all appear to be within a factor of two of each other. Second, 
uniform grids are particularly sim­ple for hardware implementations. Accesses to grid data structures 
require constant time; hierarchical data structures, in contrast, re­quire variable time per access and 
involve pointer chasing. Code for grid traversal is also very simple and can be highly optimized in hardware. 
In our system, a grid is represented as a 3D texture map, a memory organization currently supported by 
graphics hardware. We will discuss further the pros and cons of the grid in section 5. We have split 
the streaming ray tracer into four kernels: eye ray generation, grid traversal, ray-triangle intersection, 
and shad­ing. The eye ray generator kernel produces a stream of viewing rays. Each viewing ray is a single 
ray corresponding to a pixel in the image. The traversal kernel reads the stream of rays produced by 
the eye ray generator. The traversal kernel steps rays through the grid until the ray encounters a voxel 
containing triangles. The ray and voxel address are output and passed to the intersection kernel. The 
intersection kernel is responsible for testing ray intersections with all the triangles contained in 
the voxel. The intersector has two types of output. If ray-triangle intersection (hit) occurs in that 
voxel, the ray and the triangle that is hit is output for shading. If no hit occurs, the ray is passed 
back to the traversal kernel and the search for voxels containing triangles continues. The shading ker­nel 
computes a color. If a ray terminates at this hit, then the color is written to the accumulated image. 
Additionally, the shading ker­nel may generate shadow or secondary rays; in this case, these new rays 
are passed back to the traversal stage. We implement ray tracing kernels as fragment programs. We ex­ecute 
these programs by rendering a screen-sized rectangle. Con­stant inputs are placed within the kernel code. 
Stream inputs are fetched from screen-aligned textures. The results of a kernel are then written back 
into textures. The stencil buffer controls which fragments in the screen-sized rectangle and screen-aligned 
textures are active. The 8-bit stencil value associated with each ray contains the ray s state. A ray 
s state can be traversing, intersecting, shad­ing,or done. Specifying the correct stencil test with a 
rendering pass, we can allow the kernel to be run on only those rays which are in a particular state. 
The following sections detail the implementation of each ray tracing kernel and the memory layout for 
the scene. We then de­scribe several variations including ray casting, Whitted ray tracing [Whitted 1980], 
path tracing, and shadow casting. 3.1 Ray Tracing Kernels 3.1.1 Eye Ray Generator The eye ray generator 
is the simplest kernel of the ray tracer. Given camera parameters, including viewpoint and a view direction, 
it computes an eye ray for each screen pixel. The fragment program is invoked for each pixel on the screen, 
generating an eye ray for each. The eye ray generator also tests the ray against the scene bounding box. 
Rays that intersect the scene bounding box are processed fur­ther, while those that miss are terminated. 
 3.1.2 Traverser The traversal stage searches for voxels containing triangles. The .rst part of the traversal 
stage sets up the traversal calculation. The second part steps along the ray enumerating those voxels 
pierced by the ray. Traversal is equivalent to 3D line drawing and has a per-ray setup cost and a per-voxel 
rasterization cost. We use a 3D-DDA algorithm [Fujimoto et al. 1986] for this traversal. After each step, 
the kernel queries the grid data struc­ture which is stored as a 3D texture. If the grid contains a null 
pointer, then that voxel is empty. If the pointer is not null, the voxel contains triangles. In this 
case, a ray-voxel pair is output and the ray is marked so that it is tested for intersection with the 
triangles in that voxel. Implementing the traverser loop on the multipass architecture re­quires multiple 
passes. The once per ray setup is done as two passes and each step through a voxel requires an additional 
pass. At the end of each pass, the fragment program must store all the stepping parameters used within 
the loop to textures, which then must be read for the next pass. We will discuss the multipass implementa­tion 
further after we discuss the intersection stage. vox0 vox1 vox2 vox3 vox4 vox5 voxm Grid Texture  vox0 
vox1 Triangle List Texture tri0 tri1 tri2 tri3 trin Triangle v0 Vertex v1 Textures v2 x yz x yz x yz 
x yz . . . x y z x yz x yz x yz x yz . . . x yz x yz x yz x yzx yz . . . x y z Figure 4: The grid 
and triangle data structures stored in texture memory. Each grid cell contains a pointer to a list of 
triangles. If this pointer is null, then no triangles are stored in that voxel. Grids are stored as 3D 
textures. Triangle lists are stored in another tex­ture. Voxels containing triangles point to the beginning 
of a triangle list in the triangle list texture. The triangle list consists of a set of pointers to vertex 
data. The end of the triangle list is indicated by a null pointer. Finally, vertex positions are stored 
in textures.  3.1.3 Intersector The triangle intersection stage takes a stream of ray-voxel pairs and 
outputs ray-triangle hits. It does this by performing ray-triangle in­tersection tests with all the triangles 
within a voxel. If a hit occurs, a ray-triangle pair is passed to the shading stage. The code for com­puting 
a single ray-triangle intersection is shown in .gure 5. The code is similar to that used by Carr et al. 
[2002] for their DirectX 8 PS 1.4 ray-triangle intersector. We discuss their system further in section 
5. Because triangles can overlap multiple grid cells, it is possible for an intersection point to lie 
outside the current voxel. The in­tersection kernel checks for this case and treats it as a miss. Note 
that rejecting intersections in this way may cause a ray to be tested against the same triangle multiple 
times (in different voxels). It is possible to use a mailbox algorithm to prevent these extra intersec­tion 
calculations [Amanatides and Woo 1987], but mailboxing is dif.cult to implement when multiple rays are 
traced in parallel. The layout of the grid and triangles in texture memory is shown in .gure 4. As mentioned 
above, each voxel contains an offset into a triangle-list texture. The triangle-list texture contains 
a delimited list of offsets into triangle-vertex textures. Note that the triangle­list texture and the 
triangle-vertex textures are 1D textures. In fact, these textures are being used as a random-access read-only 
memory. We represent integer offsets as 1-component .oating point textures and vertex positions in three 
.oating point RGB textures. Thus, theoretically, four billion triangles could be addressed in texture 
memory with 32-bit integer addressing. However, much less texture memory is actually available on current 
graphics cards. Limitations on the size of 1D textures can be overcome by using 2D textures     Shadow 
Caster Ray Caster Whitted Ray Tracer Path Tracer (a) (b) (c) (d) Figure 3: Data .ow diagrams for the 
ray tracing algorithms we implement. The algorithms depicted are (a) shadow casting, (b) ray casting, 
(c) Whitted ray tracing, and (d) path tracing. For ray tracing, each ray-surface intersection generates 
L + 2 rays, where L is the number of lights in a scene, corresponding to the number of shadow rays to 
be tested, and the other two are re.ection and refraction rays. Path tracing randomly chooses one ray 
bounce to follow and the feedback path is only one ray wide. .oat4 IntersectTriangle( .oat3 ro, .oat3 
rd, int list pos, .oat4 h ){ .oat tri id = texture( list pos, trilist ); .oat3 v0 = texture( tri id, 
v0 ); .oat3 v1 = texture( tri id, v1 ); .oat3 v2 = texture( tri id, v2 ); .oat3 edge1 = v1 -v0; .oat3 
edge2 = v2 -v0; .oat3 pvec = Cross( rd, edge2 ); .oat det = Dot( edge1, pvec ); .oat inv det = 1/det; 
.oat3 tvec = ro -v0; .oat u = Dot( tvec, pvec ) * inv det; .oat3 qvec = Cross( tvec, edge1 ); .oat v 
= Dot( rd, qvec ) * inv det; .oat t = Dot( edge2, qvec ) * inv det; bool validhit = select( u >= 0.0f, 
true, false ); validhit = select( v >= 0, validhit, false ); validhit = select( u+v <= 1, validhit, false 
); validhit = select( t < h[0], validhit, false ); validhit = select( t >= 0, validhit, false ); t = 
select( validhit, t, h[0] ); u = select( validhit, u, h[1] ); v = select( validhit, v, h[2] ); .oat id 
= select( validhit, tri id, h[3] ); return .oat4( {t, u, v, id} ); } Figure 5: Code for ray-triangle 
intersection. with the proper address translation, as well as segmenting the data across multiple textures. 
As with the traversal stage, the inner loop over all the triangles in a voxel involves multiple passes. 
Each ray requires a single pass per ray-triangle intersection.  3.1.4 Shader The shading kernel evaluates 
the color contribution of a given ray at the hit point. The shading calculations are exactly like those 
in the standard graphics pipeline. Shading data is stored in memory much like triangle data. A set of 
three RGB textures, with 32-bits per channel, contains the vertex normals and vertex colors for each 
triangle. The hit information that is passed to the shader includes the triangle number. We access the 
shading information by a simple dependent texture lookup for the particular triangle speci.ed. By choosing 
different shading rays, we can implement several .avors of ray tracing using our streaming algorithm. 
We will look at ray casting, Whitted-style ray tracing, path tracing, and shadow casting. Figure 3 shows 
a simpli.ed .ow diagram for each of the methods discussed, along with an example image produced by our 
system. The shading kernel optionally generates shadow, re.ection, re­fraction, or randomly generated 
rays. These secondary rays are placed in texture locations for future rendering passes. Each ray is also 
assigned a weight, so that when it is .nally terminated, its contribution to the .nal image may be simply 
added into the im­age [Kajiya 1986]. This technique of assigning a weight to a ray eliminates recursion 
and simpli.es the control .ow. Ray Caster. A ray caster generates images that are identical to those 
generated by the standard graphics pipeline. For each pixel on the screen, an eye ray is generated. This 
ray is .red into the scene and returns the color of the nearest triangle it hits. No secondary rays are 
generated, including no shadow rays. Most previous efforts to implement interactive ray tracing have 
focused on this type of computation, and it will serve as our basic implementation. Whitted Ray Tracer. 
The classic Whitted-style ray tracer [Whitted 1980] generates eye rays and sends them out into the scene. 
Upon .nding a hit, the re.ection model for that surface is evaluated, and then a pair of re.ection and 
refraction rays, and a set of shadow rays one per light source are generated and sent out into the 
scene. Path Tracer. In path tracing, rays are randomly scattered from surfaces until they hit a light 
source. Our path tracer emulates the Arnold renderer [Fajardo 2001]. One path is generated per sample 
and each path contains 2 bounces. Shadow Caster. We simulate a hybrid system that uses the stan­dard 
graphics pipeline to perform hidden surface calculation in the .rst pass, and then uses ray tracing algorithm 
to evaluate shadows. Shadow casting is useful as a replacement for shadow maps and shadow volumes. Shadow 
volumes can be extremely expensive to compute, while for shadow maps, it tends to be dif.cult to set 
the proper resolution. A shadow caster can be viewed as a deferred shading pass [Molnar et al. 1992]. 
The shadow caster pass gener­ates shadow rays for each light source and adds that light s contri­bution 
to the .nal image only if no blockers are found. Kernel Multipass Branching Instr. Memory Words Stencil 
Instr. Memory Words Count R W M RS WS Count R W M Generate Eye Ray 28 0 5 0 0 1 26 0 4 0 Traverse Setup 
38 11 12 0 1 0 22 7 0 0 Step 20 14 9 1 1 1 12 0 0 1 Intersect 41 14 5 10 1 1 36 0 0 10 Shade Color 36 
10 3 21 1 0 25 0 3 21 Shadow 16 11 8 0 1 1 10 0 0 0 Re.ected 26 11 9 9 1 1 12 0 0 0 Path 17 14 9 9 1 
1 11 3 0 0 Table 1: Ray tracing kernel summary. We show the number of instructions required to implement 
each of our kernels, along with the number of 32-bit words of memory that must be read and written between 
rendering passes (R, W) and the number of memory words read from random-access textures (M). Two sets 
of statistics are shown, one for the multipass architecture and another for the branching architecture. 
For the multipass architecture, we also show the number of 8-bit stencil reads (RS) and writes (WS) for 
each kernel. Stencil read overhead is charged for all rays, whether the kernel is executed or not.  
3.2 Implementation To evaluate the computation and bandwidth requirements of our streaming ray tracer, 
we implemented each kernel as an assembly language fragment program. The NVIDIA vertex program instruc­tion 
set is used for fragment programs, with the addition of a few instructions as described previously. The 
assembly language im­plementation provides estimates for the number of instructions re­quired for each 
kernel invocation. We also calculate the bandwidth required by each kernel; we break down the bandwidth 
as stream input bandwidth, stream output bandwidth, and memory (random­access read) bandwidth. Table 
1 summarizes the computation and bandwidth required for each kernel in the ray tracer, for both the multipass 
and the branch­ing architectures. For the traversal and intersection kernels that in­volve looping, the 
counts for the setup and the loop body are shown separately. The branching architecture allows us to 
combine indi­vidual kernels together; as a result the branching kernels are slightly smaller since some 
initialization and termination instructions are removed. The branching architecture permits all kernels 
to be run together within a single rendering pass. Using table 1, we can compute the total compute and 
bandwidth costs for the scene. C =R * (Cr +vCv +tCt +sCs)+R * P *Cstencil Here R is the total number 
of rays traced. Cr is the cost to generate a ray; Cv is the cost to walk a ray through a voxel; Ct is 
the cost of performing a ray-triangle intersection; and Cs is the cost of shading. P is the total number 
of rendering passes, and Cstencil is the cost of reading the stencil buffer. The total cost associated 
with each stage is determined by the number of times that kernel is invoked. This number depends on scene 
statistics: v is the average number of vox­els pierced by a ray; t is the average number of triangles 
intersected by a ray; and s is the average number of shading calculations per ray. The branching architecture 
has no stencil buffer checks, so Cstencil is zero. The multipass architecture must pay the stencil read 
cost for all rays over all rendering passes. The cost of our ray tracer on various scenes will be presented 
in the results section. Finally, we present an optimization to minimize the total num­ber of passes motivated 
in part by Delany s implementation of a ray tracer for the Connection Machine [Delany 1988]. The traver­sal 
and intersection kernels both involve loops. There are various strategies for nesting these loops. The 
simplest algorithm would be to step through voxels until any ray encounters a voxel containing triangles, 
and then intersect that ray with those triangles. How­ever, this strategy would be very inef.cient, since 
during intersec­tion only a few rays will have encountered voxels with triangles. On a SIMD machine like 
the Connection Machine, this results in very low processor utilization. For graphics hardware, this yields 
an excessive number of passes resulting in large number of stencil read operations dominating the performance. 
The following is a more ef.cient algorithm: generate eye ray while (any(active(ray))) { if (oracle(ray)) 
traverse(ray) else intersect(ray) } shade(ray) After eye ray generation, the ray tracer enters a while 
loop which tests whether any rays are active. Active rays require either further traversals or intersections; 
inactive rays have either hit triangles or traversed the entire grid. Before each pass, an oracle is 
called. The oracle chooses whether to run a traverse or an intersect pass. Vari­ous oracles are possible. 
The simple algorithm above runs an inter­sect pass if any rays require intersection tests. A better oracle, 
.rst proposed by Delany, is to choose the pass which will perform the most work. This can be done by 
calculating the percentage of rays requiring intersection vs. traversal. In our experiments, we found 
that performing intersections once 20% of the rays require intersec­tion tests produced the minimal number 
of passes, and is within a factor of two to three of optimal for a SIMD algorithm performing a single 
computation per rendering pass. To implement this oracle, we assume graphics hardware main­tains a small 
set of counters over the stencil buffer, which contains the state of each ray. A total of eight counters 
(one per stencil bit) would be more than suf.cient for our needs since we only have four states. Alternatively, 
we could use the OpenGL histogram op­eration for the oracle if this operation were to be implemented 
with high performance for the stencil buffer.  4 Results 4.1 Methodology We have implemented functional 
simulators of our streaming ray tracer for both the multipass and branching architectures. These simulators 
are high level simulations of the architectures, written in the C++ programming language. These simulators 
compute images and gather scene statistics. Example statistics include the average number of traversal 
steps taken per ray, or the average number of Figure 6: Fundamental scene statistics for our test scenes. 
The statistics shown match the cost model formula presented in section 3.2. Recall that v is the average 
number of voxels pierced by a ray; t is the average number of triangles intersected by a ray; and s is 
the average number of shading calculations per ray. Soda hall has 1.5M triangles, the forest has 1.0M 
triangles, and the Stanford bunny has 70K triangles. All scenes are rendered at 1024x1024 pixels.  
  Soda Hall Outside Soda Hall Inside Forest Top Down Forest Inside Bunny Ray Cast v t s v t s v t s 
v t s v t s 14.41 2.52 0.44 26.11 40.46 1.00 81.29 34.07 0.96 130.7 47.90 0.97 93.93 13.88 0.82 6 ray-triangle 
intersection tests performed per ray. The multipass ar­chitecture simulator also tracks the number and 
type of rendering passes performed, as well as stencil buffer activity. These statistics allow us to 
compute the cost for rendering a scene by using the cost GInstructions GInstructions 4 2 model described 
in section 3. Both the multipass and the branching architecture simulators generate a trace .le of the 
memory reference stream for process­ ing by our texture cache simulator. In our cache simulations we 
 used a 64KB direct-mapped texture cache with a 48-byte line size. This line size holds four .oating 
point RGB texels, or three .oating point RGBA texels with no wasted space. The execution order of fragment 
programs effects the caching behavior. We execute ker-0 Outside Inside Top Down Inside Bunny nels as 
though there were a single pixel wide graphics pipeline. It Soda Hall Forest Ray Cast is likely that 
a GPU implementation will include multiple parallel Multipassfragment pipelines executing concurrently, 
and thus their accesses 4  will be interleaved. Our architectures are not speci.ed at that level of 
detail, and we are therefore not able to take such effects into account in our cache simulator. 10 GBytes 
We analyze the performance of our ray tracer on .ve viewpoints from three different scenes, shown in 
.gure 6. Soda Hall is a relatively complex model that has been used to evaluate other real-time ray tracing 
systems [Wald et al. 2001b]. It has walls made of large polygons and furnishings 2 0 made from very 
small polygons. This scene has high depth Soda Hall Forest Ray Cast complexity. The forest scene includes 
trees with millions of tiny triangles. This scene has moderate depth complexity, and it is dif.cult to 
perform occlusion culling. We analyze the cache behavior of shadow and re.ection rays using this scene. 
 The bunny was chosen to demonstrate the extension of our ray tracer to support shadows, re.ections, 
and path tracing.  Figure 7 shows the computation and bandwidth requirements of our test scenes. The 
computation and bandwidth utilized is broken down by kernel. These graphs clearly show that the computation 
and bandwidth for both architectures is dominated by grid traversal and triangle intersection. Choosing 
an optimal grid resolution for scenes is dif.cult. A .ner grid yields fewer ray-triangle intersection 
tests, but leads to more traversal steps. A coarser grid reduces the number of traver­sal steps, but 
increases the number of ray-triangle intersection tests. We attempt to keep voxels near cubical shape, 
and specify grid res­olution by the minimal grid dimension acceptable along any dimen­sion of the scene 
bounding box. For the bunny, our minimal grid dimension is 64, yielding a .nal resolution of 98 × 64 
× 163. For the larger Soda Hall and forest models, this minimal dimension is 128, yielding grid resolutions 
of 250 × 198 × 128 and 581 × 128 × 581 respectively. These resolutions allow our algorithms to spend 
equal amounts of time in the traversal and intersection kernels. Branching Figure 7: Compute and bandwidth 
usage for our scenes. Each col­umn shows the contribution from each kernel. Left bar on each plot is 
compute, right is bandwidth. The horizontal line represents the per-second bandwidth and compute performance 
of our hypotheti­cal architecture. All scenes were rendered at 1024 × 1024 pixels.  4.2 Architectural 
Comparisons We now compare the multipass and branching architectures. First, we investigate the implementation 
of the ray caster on the multipass architecture. Table 2 shows the total number of rendering passes and 
the distribution of passes amongst the various kernels. The total number of passes varies between 1000-3000. 
Although the number of passes seems high, this is the total number needed to render the scene. In the 
conventional graphics pipeline, many fewer passes per object are used, but many more objects are drawn. 
In our system, each pass only draws a single rectangle, so the speed of the geometry processing part 
of the pipeline is not a factor. We also evaluate the ef.ciency of the multipass algorithm. Re­call that 
rays may be traversing, intersecting, shading, or done. The ef.ciency of a pass depends on the percentage 
of rays processed in that pass. In these scenes, the ef.ciency is between 6-10% for all of the test scenes 
except for the outside view of Soda Hall. This Table 2: Breakdown of passes in the multipass system. 
Intersection and traversal make up the bulk of passes in the systems, with the rest of the passes coming 
from ray generation, traversal setup, and shading. We also show the maximum number of traversal steps 
and intersection tests for per ray. Finally, SIMD ef.ciency measures the average fraction of rays doing 
useful work for any given pass. Pass Breakdown Per Ray Maximum SIMD Ef.ciency Total Traversal Intersection 
Other Traversals Intersections Soda Hall Outside 2443 692 1747 4 384 1123 0.009 Soda Hall Inside 1198 
70 1124 4 60 1039 0.061 Forest Top Down 1999 311 1684 4 137 1435 0.062 Forest Inside 2835 1363 1468 4 
898 990 0.068 Bunny Ray Cast 1085 610 471 4 221 328 0.105 2.0  Stencil State Variables Data Structures 
Normalized Bandwidth 1.5 1.0 0.5 GBytes Outside Inside Top Down Inside Bunny Soda Hall Forest Ray Cast 
Figure 8: Bandwidth consumption by data type. Left bars are for multipass, right bars for branching. 
Overhead for reading the 8-bit stencil value is shown on top. State variables are data written to and 
read from texture between passes. Data structure bandwidth comes from read-only data: triangles, triangle 
lists, grid cells, and shading data. All scenes were rendered at 1024 × 1024 pixels. viewpoint contains 
several rays that miss the scene bounding box entirely. As expected, the resulting ef.ciency is much 
lower since these rays never do any useful work during the rest of the compu­tation. Although 10% ef.ciency 
may seem low, the fragment pro­cessor utilization is much higher because we are using early frag­ment 
kill to avoid consuming compute resources and non-stencil bandwidth for the fragment. Finally, table 
2 shows the maximum number of traversal steps and intersection tests that are performed per ray. Since 
the total number of passes depends on the worst case ray, these numbers provide lower bounds on the number 
of passes needed. Our multipass algorithm interleaves traversal and intersec­tion passes and comes within 
a factor of two to three of the optimal number of rendering passes. The naive algorithm, which performs 
an intersection as soon as any ray hits a full voxel, requires at least a factor of .ve times more passes 
than optimal on these scenes. We are now ready to compare the computation and bandwidth requirements 
of our test scenes on the two architectures. Figure 8 shows the same bandwidth measurements shown in 
.gure 7 broken down by data type instead of by kernel. The graph shows that, as ex­pected, all of the 
bandwidth required by the branching architecture is for reading voxel and triangle data structures from 
memory. The multipass architecture, conversely, uses most of its bandwidth for writing and reading intermediate 
values to and from texture mem­ory between passes. Similarly, saving and restoring these interme­diates 
requires extra instructions. In addition, signi.cant bandwidth is devoted to reading the stencil buffer. 
This extra computation and bandwidth consumption is the fundamental limitation of the multi­pass algorithm. 
One way to reduce both the number of rendering passes and the bandwidth consumed by intermediate values 
in the multipass archi­tecture is to unroll the inner loops. We have presented data for a 0.0 Outside 
Inside Top Down Inside Bunny Shadow Reflect Soda Hall Forest Ray Cast Forest Figure 9: Ratio of bandwidth 
with a texture cache to bandwidth without a texture cache. Left bars are for multipass, right bars for 
branching. Within each bar, the bandwidth consumed with a texture cache is broken down by data type. 
All scenes were rendered at 1024 × 1024 pixels. single traversal step or a single intersection test performed 
per ray in a rendering pass. If we instead unroll our kernels to perform four traversal steps or two 
intersection tests, all of our test scenes reduce their total bandwidth usage by 50%. If we assume we 
can suppress triangle and voxel memory references if a ray .nishes in the mid­dle of the pass, the total 
bandwidth reduction reaches 60%. At the same time, the total instruction count required to render each 
scene increases by less than 10%. With more aggressive loop unrolling the bandwidth savings continue, 
but the total instruction count in­crease varies by a factor of two or more between our scenes. These 
results indicate that loop unrolling can make up for some of the overhead inherent in the multipass architecture, 
but unrolling does not achieve the compute to bandwidth ratio obtained by the branch­ing architecture. 
Finally, we compare the caching behavior of the two implemen­tations. Figure 9 shows the bandwidth requirements 
when a texture cache is used. The bandwidth consumption is normalized by di­viding by the non-caching 
bandwidth reported earlier. Inspecting this graph we see that the multipass system does not bene.t very 
much from texture caching. Most of the bandwidth is being used for streaming data, in particular, for 
either the stencil buffer or for intermediate results. Since this data is unique to each kernel in­vocation, 
there is no reuse. In contrast, the branching architecture utilizes the texture cache effectively. Since 
most of its bandwidth is devoted to reading shared data structures, there is reuse. Studying the caching 
behavior of triangle data only, we see that a 96-99% hit rate is achieved by both the multipass and the 
branching system. This high hit rate suggests that triangle data caches well, and that we have a fairly 
small working set size. In summary, the implementation of the ray caster on the multi­pass architecture 
has achieved a very good balance between com­putation and bandwidth. The ratio of instruction count to 
band­width matches the capabilities of a modern GPU. For example, the Table 3: Number of instructions 
and amount of bandwidth con­sumed by the extended algorithms to render the bunny scene using the branching 
architecture, normalized by the ray casting cost. Extension Relative Instructions Bandwidth Shadow Caster 
0.85 1.15 Whitted Ray Tracer 2.62 3.00 Path Tracer 3.24 4.06 NVIDIA GeForce3 is able to execute approximately 
2G instruc­tions/s in its fragment processor, and has roughly 8GB/s of memory bandwidth. Expanding the 
traversal and intersection kernels to per­form multiple traversal steps or intersection tests per pass 
reduces the bandwidth required for the scene at the cost of increasing the computational requirements. 
The amount of loop unrolling can be changed to match the computation and bandwidth capabilities of the 
underlying hardware. In comparison, the branching architec­ture consumes fewer instructions and signi.cantly 
less bandwidth. As a result, the branching architecture is severely compute-limited based on today s 
GPU bandwidth and compute rates. However, the branching architecture will become more attractive in the 
future as the compute to bandwidth ratio on graphics chips increases with the introduction of more parallel 
fragment pipelines.  4.3 Extended Algorithms With an ef.cient ray caster in place, implementing extensions 
such as shadow casting, full Whitted ray tracing, or path tracing is quite simple. Each method utilizes 
the same ray-triangle intersection loop we have analyzed with the ray caster, but implements a differ­ent 
shading kernel which generates new rays to be fed back through our system. Figure 3 shows images of the 
bunny produced by our system for each of the ray casting extensions we simulate. The total cost of rendering 
a scene depends on both the number of rays traced and the cache performance. Table 3 shows the number 
of instructions and bandwidth required to produce each image of the bunny relative to the ray casting 
cost, all using the branching architecture. The path traced bunny was rendered at 256 × 256 pixels with 
64 samples and 2 bounces per pixel while the others were rendered at 1024 × 1024 pixels. The ray cast 
bunny .nds a valid hit for 82% of its pixels and hence 82% of the primary rays generate secondary rays. 
If all rays were equal, one would expect the shadow caster to consume 82% of the instruc­tions and bandwidth 
of the ray caster; likewise the path tracer would consume 3.2 times that of the ray caster. Note that 
the instruction usage is very close to the expected value, but that the bandwidth consumed is more. Additionally, 
secondary rays do not cache as well as eye rays, due to their generally incoherent nature. The last two 
columns of .gure 9 illustrate the cache effectiveness on secondary rays, mea­sured separately from primary 
rays. For these tests, we render the inside forest scene in two different styles. Shadow is rendered 
with three light sources with each hit producing three shadow rays. Re.ect applies a two bounce re.ection 
and single light source shading model to each primitive in the scene. For the multipass rendering system, 
the texture cache is unable to reduce the total bandwidth consumed by the system. Once again the streaming 
data destroys any locality present in the triangle and voxel data. The branching architecture results 
demonstrate that scenes with secondary rays can bene.t from caching. The system achieves a 35% bandwidth 
reduction for the shadow computation. However caching for the re.ective forest does not reduce the required 
band­width. We are currently investigating ways to improve the perfor­mance of our system for secondary 
rays.  5 Discussion In this section, we discuss limitations of the current system and future work. 5.1 
Acceleration Data Structures A major limitation of our system is that we rely on a preprocess­ing step 
to build the grid. Many applications contain dynamic ge­ometry, and to support these applications we 
need fast incremental updates to the grid. Building acceleration data structures for dy­namic scenes 
is an active area of research [Reinhard et al. 2000]. An interesting possibility would be to use graphics 
hardware to build the acceleration data structure. The graphics hardware could scan convert the geometry 
into a grid. However, the architectures we have studied in this paper cannot do this ef.ciently; to do 
opera­tions like rasterization within the fragment processor they would need the ability to write to 
arbitrary memory locations. This is a classic scatter operation and would move the hardware even closer 
to a general stream processor. In this research we assumed a uniform grid. Uniform grids, how­ever, may 
fail for scenes containing geometry and empty space at many levels of detail. Since we view texture memory 
as random­access memory, hierarchical grids could be added to our system. Currently graphics boards contain 
relatively small amounts of memory (in 2001 a typical board contains 64MB). Some of the scenes we have 
looked at require 200MB -300MB of texture mem­ory to store the scene. An interesting direction for future 
work would be to study hierarchical caching of the geometry as is com­monly done for textures. The trend 
towards uni.ed system and graphics memory may ultimately eliminate this problem. 5.2 CPU vs. GPU Wald 
et al. have developed an optimized ray tracer for a PC with SIMD .oating point extensions [Wald et al. 
2001b]. On an 800 MHz Pentium III, they report a ray-triangle intersection rate of 20M intersections/s. 
Carr et al. [2002] achieve 114M ray-triangle inter­sections/s on an ATI Radeon 8500 using limited .xed 
point preci­sion. Assuming our proposed hardware ran at the same speed as a GeForce3 (2G instructions/s), 
we could compute 56M ray-triangle intersections/s. Our branching architecture is compute limited; if 
we increase the instruction issue rate by a factor of four (8G in­structions/s) then we would still not 
use all the bandwidth available on a GeForce3 (8GB/s). This would allow us to compute 222M ray­triangle 
intersections per second. We believe because of the inher­ently parallel nature of fragment programs, 
the number of GPU in­structions that can be executed per second will increase much faster than the number 
of CPU SIMD instructions. Once the basic feasibility of ray tracing on a GPU has been demonstrated, it 
is interesting to consider modi.cations to the GPU that support ray tracing more ef.ciently. Many possibilities 
imme­diately suggest themselves. Since rays are streamed through the system, it would be more ef.cient 
to store them in a stream buffer than a texture map. This would eliminate the need for a stencil buffer 
to control conditional execution. Stream buffers are quite similar to F-buffers which have other uses 
in multipass rendering [Mark and Proudfoot 2001]. Our current implementation of the grid traversal code 
does not map well to the vertex program instruction set, and is thus quite inef.cient. Since grid traversal 
is so similar to rasterization, it might be possible to modify the rasterizer to walk through the grid. 
Finally, the vertex program instruction set could be optimized so that ray-triangle intersection could 
be performed in fewer instructions. Carr et al. [2002] have independently developed a method of using 
the GPU to accelerate ray tracing. In their system the GPU is only used to accelerate ray-triangle intersection 
tests. As in our system, GPU memory is used to hold the state for many active rays. In their system each 
triangle in turn is fed into the GPU and tested for intersection with all the active rays. Our system 
differs from theirs in that we store all the scene triangles in a 3D grid on the GPU; theirs stores the 
acceleration structure on the CPU. We also run the entire ray tracer on the GPU. Our system is much more 
ef.­cient than theirs since we eliminate the GPU-CPU communication bottleneck. 5.3 Tiled Rendering In 
the multipass architecture, the majority of the memory band­width was consumed by saving and restoring 
temporary variables. Since these streaming temporaries are only used once, there is no bandwidth savings 
due to the cache. Unfortunately, when these streaming variables are accessed as texture, they displace 
cacheable data structures. The size of the cache we used is not large enough to store the working set 
if it includes both temporary variables and data structures. The best way to deal with this problem is 
to sepa­rate streaming variables from cacheable variables. Another solution to this problem is to break 
the image into small tiles. Each tile is rendered to completion before proceeding to the next tile. Tiling 
reduces the working set size, and if the tile size is chosen so that the working set .ts into the cache, 
then the streaming variables will not displace the cacheable data structures. We have performed some 
preliminary experiments along these lines and the results are encouraging.  6 Conclusions We have shown 
how viewing a programmable graphics processor as a general parallel computation device can help us leverage 
the graphics processor performance curve and apply it to more general parallel computations, speci.cally 
ray tracing. We have shown that ray casting can be done ef.ciently in graphics hardware. We hope to encourage 
graphics hardware to evolve toward a more general programmable stream architecture. While many believe 
a fundamentally different architecture would be required for real-time ray tracing in hardware, this 
work demonstrates that a gradual convergence between ray tracing and the feed-forward hardware pipeline 
is possible. 7 Acknowledgments We would like to thank everyone in the Stanford Graphics Lab for contributing 
ideas to this work. We thank Matt Papakipos from NVIDIA for his thoughts on next generation graphics 
hardware, and Kurt Akeley and our reviewers for their comments. Katie Till­man stayed late and helped 
with editing. This work was spon­sored by DARPA (contracts DABT63-95-C-0085 and MDA904-98­C-A933), ATI, 
NVIDIA, Sony, and Sun.  References 3DLABS, 2001. OpenGL 2.0 whitepapers web site. http://www.3dlabs.com/support/developer/ogl2/index.htm. 
ALVERSON, R., CALLAHAN,D., CUMMINGS,D., KOBLENZ, B., PORTERFIELD, A., AND SMITH, B. 1990. The Tera computer 
system. In Proceedings of the 1990 International Conference on Supercomputing, 1 6. AMANATIDES,J., AND 
WOO, A. 1987. A fast voxel traversal algorithm for ray tracing. In Eurographics 87, 3 10. ANDERSON, B., 
STEWART, A., MACAULAY, R., AND WHITTED, T. 1997. Accommodating memory latency in a low-cost rasterizer. 
In 1997 SIGGRAPH / Eurographics Workshop on Graphics hardware, 97 102. ATI, 2001. RADEON 8500 product 
web site. http://www.ati.com/products/pc/radeon8500128/index.html. CARR,N.A., HALL,J.D., AND HART, J. 
C. 2002. The ray engine. Tech. Rep. UIUCDCS-R-2002-2269, Department of Computer Science, University of 
Illinois. DELANY, H. C. 1988. Ray tracing on a connection machine. In Proceedings of the 1988 International 
Conference on Supercomputing, 659 667. FAJARDO, M. 2001. Monte carlo ray tracing in action. In State 
of the Art in Monte Carlo Ray Tracing for Realistic Image Synthesis -SIGGRAPH 2001 Course 29. 151 162. 
FUJIMOTO,A., TANAKA,T., AND IWATA, K. 1986. ARTS: Accelerated ray tracing system. IEEE Computer Graphics 
and Applications 6, 4, 16 26. HALL, D., 2001. The AR350: Today s ray trace rendering processor. 2001 
SIGGRAPH / Eurographics Workshop On Graphics Hardware -Hot 3D Session 1. http://graphicshardware.org/previous/www 
2001/presentations/ Hot3D Daniel Hall.pdf. HAVRAN,V., PRIKRYL,J., AND PURGATHOFER, W. 2000. Statistical 
comparison of ray-shooting ef.ciency schemes. Tech. Rep. TR-186-2-00-14, Institute of Computer Graphics, 
Vienna University of Technology. IGEHY, H., ELDRIDGE, M., AND PROUDFOOT, K. 1998. Prefetching in a texture 
cache architecture. In 1998 SIGGRAPH / Eurographics Workshop on Graphics hardware, 133 ff. KAJIYA, J. 
T. 1986. The rendering equation. In Computer Graphics (Proceedings of ACM SIGGRAPH 86), 143 150. KHAILANY, 
B., DALLY,W.J., RIXNER, S., KAPASI, U. J., MATTSON,P., NAMKOONG, J., OWENS, J. D., AND TOWLES, B. 2000. 
IMAGINE: Signal and image processing using streams. In Hot Chips 12. IEEE Computer Society Press. KIRK, 
D., 2001. GeForce3 architecture overview. http://developer.nvidia.com/docs/IO/1271/ATT/GF3ArchitectureOverview.ppt. 
LINDHOLM,E., KILGARD,M.J., AND MORETON, H. 2001. A user-programmable vertex engine. In Proceedings of 
ACM SIGGRAPH 2001, 149 158. MARK, W. R., AND PROUDFOOT, K. 2001. The F-buffer: A rasterization-order 
FIFO buffer for multi-pass rendering. In 2001 SIGGRAPH / Eurographics Workshop on Graphics Hardware. 
MARSHALL, B., 2001. DirectX graphics future. Meltdown 2001 Conference. http://www.microsoft.com/mscorp/corpevents/meltdown2001/ppt/DXG9.ppt. 
MICROSOFT, 2001. DirectX product web site. http://www.microsoft.com/directx/. MOLNAR, S., EYLES,J., AND 
POULTON, J. 1992. PixelFlow: High-speed rendering using image composition. In Computer Graphics (Proceedings 
of ACM SIGGRAPH 92), 231 240. NVIDIA, 2001. GeForce3 Ti Family: Product overview. 10.01v1. http://www.nvidia.com/docs/lo/1050/SUPP/gf3ti 
overview.pdf. PARKER, S., SHIRLEY,P., LIVNAT,Y., HANSEN, C., AND SLOAN, P.-P. 1998. Interactive ray tracing 
for isosurface rendering. In IEEE Visualization 98, 233 238. PARKER, S., MARTIN,W., SLOAN, P.-P. J., 
SHIRLEY,P., SMITS, B., AND HANSEN, C. 1999. Interactive ray tracing. In 1999 ACM Symposium on Interactive 
3D Graphics, 119 126. PEERCY, M. S., OLANO, M., AIREY,J., AND UNGAR, P. J. 2000. Interactive multi-pass 
programmable shading. In Proceedings of ACM SIGGRAPH 2000, 425 432. REINHARD,E., SMITS, B., AND HANSEN, 
C. 2000. Dynamic acceleration structures for interactive ray tracing. In Rendering Techniques 2000: 11th 
Eurographics Workshop on Rendering, 299 306. SPITZER, J., 2001. Texture compositing with register combiners. 
http://developer.nvidia.com/docs/IO/1382/ATT/RegisterCombiners.pdf. TORBORG,J., AND KAJIYA, J. T. 1996. 
Talisman: Commodity realtime 3D graphics for the PC. In Proceedings of ACM SIGGRAPH 96, 353 363. WALD, 
I., SLUSALLEK,P., AND BENTHIN, C. 2001. Interactive distributed ray tracing of highly complex models. 
In Rendering Techniques 2001: 12th Eurographics Workshop on Rendering, 277 288. WALD, I., SLUSALLEK,P., 
BENTHIN, C., AND WAGNER, M. 2001. Interactive rendering with coherent ray tracing. Computer Graphics 
Forum 20, 3, 153 164. WHITTED, T. 1980. An improved illumination model for shaded display. Communications 
of the ACM 23, 6, 343 349.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566641</article_id>
		<sort_key>713</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Shader-driven compilation of rendering assets]]></title>
		<page_from>713</page_from>
		<page_to>720</page_to>
		<doi_number>10.1145/566570.566641</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566641</url>
		<abstract>
			<par><![CDATA[Rendering performance of consumer graphics hardware benefits from pre-processing geometric data into a form targeted to the underlying API and hardware. The various elements of geometric data are then coupled with a shading program at runtime to draw the asset.In this paper we describe a system in which pre-processing is done in a compilation process in which the geometric data are processed with knowledge of their shading programs. The data are converted into structures targeted directly to the hardware, and a code stream is assembled that describes the manipulations required to render these data structures. Our compiler is structured like a traditional code compiler, with a front end that reads the geometric data and attributes (hereafter referred to as an <i>art asset</i>) output from a 3D modeling package and shaders in a platform independent form and performs platform-independent optimizations, and a back end that performs platform-specific optimizations and generates platform-targeted data structures and code streams.Our compiler back-end has been targeted to four platforms, three of which are radically different from one another. On all platforms the rendering performance of our compiled assets, used in real situations, is well above that of hand-coded assets.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer games]]></kw>
			<kw><![CDATA[graphics systems]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[rendering systems]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Specialized application languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.4</cat_node>
				<descriptor>Compilers</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011041</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Compilers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14033407</person_id>
				<author_profile_id><![CDATA[81100065581]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lalonde]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts (Canada) Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14071226</person_id>
				<author_profile_id><![CDATA[81100174695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schenk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts (Canada) Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>555371</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[APODACA, A. A., AND GRITZ, L. 1999. Advanced Renderman: Creating CGI for Motion Pictures. Morgan Kauffman Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARRELL, K. F. 1983. The graphical kernel system - a replacement for core. First Australasian Conference on Computer Graphics, 22-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>780997</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BOGOMJAKOV, A., AND GOTSMAN, C. 2001. Universal rendering sequences for transparent vertex caching of progressive meshes. In Proceedings of Graphics Interface 2001, 81-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L. 1984. Shade trees. In Computer Graphics (Proceedings of SIGGRAPH 84), vol. 18, 223-231.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND LAWSON, J. 1990. A language for shading and lighting calculations. In Computer Graphics (Proceedings of SIGGRAPH 90), vol. 24, 289-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311565</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. 1999. Optimization of mesh locality for transparent vertex caching. Proceedings of SIGGRAPH 99 (August), 269-276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[JAQUAYS, P., AND HOOK, B. 1999. Q3radiant shader manual.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[LANDER, J. 1998. Skin them bones: Game programming for the web generation. Game Developer Magazine, 11-16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[LINDHOLM, E., KILGARD, M. J., AND MORETON, H. 2001. A user-programmable vertex engine. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 149-158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MICROSOFT. 2000. DirectX 8 Programmer's Reference. Microsoft Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S., EYLES, J., AND POULTON, J. 1992. Pixelflow: High-speed rendering using image composition. In Computer Graphics (Proceedings of SIGGRAPH 92), vol. 26, 231-240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280857</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[OLANO, M., AND LASTRA, A. 1998. A shading language on graphics hardware: The pixelflow shading system. In Proceedings of SIGGRAPH 98, ACM SIGGRAPH / Addison Wesley, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 159-168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>554539</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[OPENGL ARCHITECTURE REVIEW BOARD, WOO, M., NEIDER, J., DAVIS, T., AND SHREINER, D. 1999. OpenGL Programming Guide: The Official Guide to Learning OpenGL, Version 1.2. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344976</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PEERCY, M. S., OLANO, M., AIREY, J., AND UNGAR, P. J. 2000. Interactive multipass programmable shading. Proceedings of SIGGRAPH 2000 (July), 425-432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1985. An image synthesizer. In Computer Graphics (Proceedings of SIGGRAPH 85), vol. 19, 287-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383275</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PROUDFOOT, K., MARK, W. R., TZVETKOV, S., AND HANRAHAN, P. 2001. A real-time procedural shading system for programmable graphics hardware. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 159-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[ROHLF, J., AND HELMAN, J. 1994. Iris performer: A high performance multiprocessing toolkit for real-time 3d graphics. In Proceedings of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 381-395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[STERN, G. 1983. Bbop --- a system for 3d keyframe figure animation. In Introduction to Computer Animation, Course Notes 7 for SIGGRAPH 83, 240-243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SUZUOKI, KUTARAGI, K., HIROI, T., MAGOSHI, H., OKAMOTO, S., OKA, M., OHBA, A., YAMAMOTO, Y., FURUHASHI, M., TANAKA, M., YUTAKA, T., OKADA, T., NAGAMATSU, M., URAKAWA, Y., FUNYU, M., KUNIMATSU, A., GOTO, H., HASHIMOTO, K., IDE, N., MURAKAMI, H., OHTAGURO, Y., , AND AONO, A. 1999. A microprocessor with a 128-bit cpu, ten floating-point mac's, four floating-point dividers, and an mpeg-2decoder. In IEEE Journal of Solid-State Circuts: Special issue on the 1999 ISSCC: Digital, Memory, and Signal Processing, IEEE Solid-State Circuits Society, 1608.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., PLATT, J., BARR, A., AND FLEISCHER, K. 1987. Elastically deformable models. In Computer Graphics (Proceedings of SIGGRAPH 87), vol. 21, 205-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[TOOL INTERFACE STANDARDS. 1998. Elf: Executable and linkable format. ftp://ftp.intel.com/pub/tis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shader-Driven Compilation of Rendering Assets Paul Lalonde Eric Schenk Electronic Arts (Canada) Inc. 
 Abstract Rendering performance of consumer graphics hardware bene.ts from pre-processing geometric data 
into a form targeted to the un­derlying API and hardware. The various elements of geometric data are 
then coupled with a shading program at runtime to draw the as­set. In this paper we describe a system 
in which pre-processing is done in a compilation process in which the geometric data are pro­cessed with 
knowledge of their shading programs. The data are converted into structures targeted directly to the 
hardware, and a code stream is assembled that describes the manipulations required to render these data 
structures. Our compiler is structured like a traditional code compiler, with a front end that reads 
the geomet­ric data and attributes (hereafter referred to as an art asset) output from a 3D modeling 
package and shaders in a platform indepen­dent form and performs platform-independent optimizations, 
and a back end that performs platform-speci.c optimizations and gener­ates platform-targeted data structures 
and code streams. Our compiler back-end has been targeted to four platforms, three of which are radically 
different from one another. On all platforms the rendering performance of our compiled assets, used in 
real sit­uations, is well above that of hand-coded assets. CR Categories: I.3.3 [Computer Graphics]: 
Picture/Image Generation; I.3.6 [Computer Graphics]: Methodology and Techniques Languages; I.3.8 [Computer 
Graphics]: Applications Computer Games; D.3.2 [Programming Lan­guages]: Language Classi.cations Specialized 
application languages Keywords: Computer Games, Graphics Systems, Rendering, Ren­dering Systems 1 Introduction 
The most recent generations of consumer level graphics hardware, found in such consumer devices as the 
Sony PlayStation2TM, Mi­crosoft XBoxTM, and Nintendo GameCubeTM, as well as in per­sonal computers, have 
brought high end real-time graphics to the consumer at large. This hardware exists principally for use 
in video games, and this is re.ected in the hardware architectures. In consumer applications such as 
video games the topology of most graphical elements is .xed, unlike the case of modeling appli­cations, 
such as Alias|Wavefront MayaTM, SoftImage XSITM, and Copyright &#38;#169; 2002 by the Association for 
Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 3D Studio MaxTM . Hardware designers, both of game consoles 
and of graphics accelerator chipsets, have exploited this and have designed their hardware to be most 
ef.cient at rendering large con­stant sets of geometry than at rendering individual polygons. This is 
re.ected in the APIs used: both Microsoft s DirectX 8 [Microsoft 2000] and OpenGL 1.1 and later [OpenGL 
Architecture Review Board et al. 1999] support calls for setting up arrays of input data (vertices, colours, 
and other per-vertex attributes, as well as index lists) that are much more ef.cient than single-polygon 
submissions. Further, groups of polygons and other rendering attributes can be collected into display 
lists for later atomic submission, also at much higher performance than single polygon submissions. In 
a consumer application, art asset authoring is part of the de­velopment cycle. The assets are pre-processed 
using some set of tools into a form suitable for both the hardware and the software architecture of the 
application. The data pre-processes typically manipulate only the geometric elements. Setting other elements 
of rendering state, such as lighting, vertex and pixel shader selections, rasterization control, transformation 
matrices, and so forth, as well as the selection of vertex buffers and vertex layouts are handled in 
the runtime engine. This requires much of the knowledge about the use of the art asset to reside in code, 
tying the art asset closely to the programmer. Programmers often attempt to generalize this code to deal 
with multiple assets, at the expense of ef.ciency. Al­though shader compilers have been explored as a 
partial solution to this problem, no one has yet exploited knowledge of the shader to systematically 
optimize rendering. Our system was driven by the following requirements: to render .xed topology objects 
ef.ciently with .xed shading effects;  to support runtime modi.cations to the objects we draw, but not 
modi.cation of their topologies;  to exploit hardware capabilities such as vertex programs, and pixel 
shaders [Microsoft 2000]; and  to have user code and art assets that are portable across hard­ware platforms. 
 We have designed and implemented a real-time rendering system comprised of a small, ef.cient runtime 
engine with a portable API, and a modular retargetable art asset compiler, called EAGL, the Electronic 
Arts Graphics Library. The runtime has been developed both on top of existing graphics APIs and at the 
driver level. An art asset is authored in some geometric modeling package, such as Alias|Wavefront Maya, 
3D Studio Max, or SoftImage XSI. The as­set consists of its geometry, its per-vertex attributes, and 
its collec­tion of materials (surface properties, colours, texture map usages, etc.) and textures. The 
asset description is suf.ciently rich that the object can be rendered as it should appear on .nal use 
without programmer intervention. The EAGL architecture is based on separating the rendering primitive 
description from the runtime using a shader language that describes not only the shader program, but 
the semantics of the in­put data to the shader. These augmented shaders are called render methods and 
a single art asset may reference many. Conversely a render method may be used by more than one art asset. 
The resulting system is fast and .exible. It has shown itself to be as fast or faster than the custom 
engines the various product teams were using before adopting our system. At the time of submis­ Figure 
1: Art asset compilation process in context sion the system has been used in 10 successful video game 
titles, in differing genres, on 4 platforms, with close to 30 more titles in de­velopment. Porting our 
system to a new platform involves writing new implementations of the runtime and compiler back end, and 
a new suite of core render methods. This amounts to approximately 3 man-months of work. In our experience 
this is signi.cantly faster than porting a custom game engine and tool suite to new hardware. 1.1 Contributions 
Our system, illustrated in Figure 1, makes three contributions. First, we describe an extension of shader 
speci.cations, called the ren­der method, that includes declaration of shader input variables and off-line 
computations to generate these variables from an art asset. Second, we describe a compiler front end 
that takes as input polyg­onal geometric data and model attributes and produces segmented, geometrically 
optimized asset fragments, called packets, represent­ing individual calls to the render methods. Alternative 
front ends are easily constructed to address non-polygonal data, such as spline surfaces or particle 
systems. Third, we describe the back end of our asset compiler that takes packets generated by the front 
end, and generates optimized code and data streams used to render the asset at runtime.  2 Background 
Two bodies of work are relevant to the discussion of our art asset compiler. The .rst is the recent work 
done on compiling shading languages. The second relates to display lists. 2.1 Shading languages Shading 
languages are an outgrowth of Cook s shade trees [Cook 1984] and Perlin s pixel stream language [Perlin 
1985]. They are now most commonly used in the form of the RenderMan Shading Language [Hanrahan and Lawson 
1990; Apodaca and Gritz 1999]. Shading languages have recently been adapted to real-time render­ing graphics 
hardware applications. Olano and Lastra [Olano and Lastra 1998] were .rst to describe a RenderMan-like 
language whose compilation is targeted to spe­ci.c graphics hardware, in their case the PixelFlow system 
[Molnar et al. 1992]. PixelFlow is by design well suited to programmable shading but is very different 
from today s consumer level hardware. id Software s Quake III product incorporates the Quake Shader Language 
[Jaquays and Hook 1999]. Here, shader speci.cations are used to control the OpenGL state machine. The 
shader language speci.es multi-pass rendering effects involving the texture units, allowing the coupling 
of application variables to the parameters of the various passes. Peercy et al. observed that treating 
the OpenGL state ma­chine as a SIMD processor yields a framework for compiling the RenderMan Shading 
Language. They decompose RenderMan shaders into a series of passes of rendering, combined in the frame 
buffer [Peercy et al. 2000]. Recently, Proudfoot et al. [Proudfoot et al. 2001], have devel­oped a shader 
language compiler that uses the programmable ver­tex shaders available in DirectX 8 [Microsoft 2000] 
and NVIDIA s NV vertex program OpenGL extension [Lindholm et al. 2001], and the the per-fragment operations 
provided by modern texture com­biner hardware. By taking into account the multiple levels at which speci.cations 
occur (object level, vertex level, or pixel level), they successfully exploit the hardware features at 
those levels. In all the above shader compilers geometric data is communi­cated to the shader through 
the underlying graphics API, as per the RenderMan model. In RenderMan both the geometry and its bind­ings 
to shaders is speci.ed procedurally using the RenderMan Inter­face Speci.cation. Likewise, Olano and 
Lastra s, and Proudfoot et al. s systems bind shaders to geometry through the OpenGL API. This requires 
either an external system to manage the binding of shaders to geometry or else explicit application code 
per art asset to manage the bindings. These programs are more complex than they might appear at .rst 
glance, since they require both runtime code to manage the bindings, as well as synchronized tool code 
to generate the appropriate data for the runtime. We have chosen to manage the binding of geometry to 
the shader explicitly in our art asset compiler. This provides us with three principal bene.ts. First, 
user code speci.c to a particular asset is only required if there are explicit exposed runtime shader 
parame­ters (§4.3). Second, as a corollary to the .rst, an artist can quickly iterate an art asset on 
the target platform without programmer in­tervention. Third, the runtime API is dramatically simpli.ed, 
since all geometry speci.cation and binding is performed off line. We do provide tools for generating 
bindings at runtime, but this use is discouraged as such dynamic models are considerably less ef.cient 
than our compiled models (§3.2). 2.2 Display lists Art assets are produced in 3D modeling and animation 
packages. These packages are usually oriented toward interactive manipula­tion of geometry data and off-line 
rendering of the resulting objects. They have rich feature sets for manipulation of geometry, topology, 
shading, and animation. However, the raw output models are rarely suited to consumer level hardware. 
Assets must be authored with sensitivity to their eventual use in real-time consumer-level applica­tions. 
The assets must be not only converted from the rich descrip­tion stored by the packages, but also optimized 
and targeted to the hardware and software architectures of the application. These pre­processing operations 
range from simple data conversion through to complex re-ordering and optimization tasks. Hoppe showed 
how re-ordering the vertices in triangle strips could yield more ef.cient rendering by exploiting hardware 
vertex caches [Hoppe 1999]. Bo­gomjakov and Gotsman showed how to exploit the vertex cache using vertex 
meshes instead of triangle strips, without knowing a priori the size of the cache [Bogomjakov and Gotsman 
2001]. Both these approaches can yield two-fold improvements in render­ing performance over using the 
original input data. No matter the level of geometric optimization, however, some level of optimization 
of graphics hardware setup and rendering sub­mission is required to obtain best performance. Early graphics 
APIs were oriented to drawing individual polygons [Barrell 1983]. The .rst version of OpenGL was similarly 
limited, leading to high func­tion call overhead on polygon submissions. The OpenGL vertex ar­rays mechanism, 
presented in OpenGL 1.1, removed much of this overhead by allowing bulk speci.cation of polygons [OpenGL 
Ar­chitecture Review Board et al. 1999]. DirectX 8 s vertex streams operate on the same principle [Microsoft 
2000]. Although vertex arrays speed submission of geometry data, the various state setting functions 
in OpenGL and DirectX 8 still re­quire considerable overhead. Both support display lists, used to collect 
both geometry and state setting calls for later atomic re­submission. Although these display lists have 
the potential for con­siderable optimization at the driver level, their construction at run­time, with 
the ensuing performance limitations, limits the degree to which display list optimization can be taken. 
In particular, parame­terized display lists are problematic. Although a single display list cannot be 
parameterized a display list may call one or more dis­play lists which may have been re-built since the 
original display list, allowing simple parameterization. This architecture does not, however, allow the 
driver to optimize state changes across such a nested display list call, as the newly de.ned list may 
affect any of the state that had been set in the parent display list. IRIS Performer [Rohlf and Helman 
1994] showed how to use a hierarchical data or­ganization to optimize away redundant state manipulations 
during rendering, but forces a direct-mode geometry submission model on the application, reducing the 
optimizations possible in display list construction. We have chosen to construct our rendering objects 
off-line, and to address parameterization of these objects through runtime link­age of external variables 
to our rendering objects and by exporting various part of our objects to the runtime for modi.cation 
(§4.3). Both methods allow aggressive optimization around parameterized elements of our models.  3 Runtime 
Environment To make our discussion of our art asset compiler concrete we brie.y describe the runtime 
environment that our compiler targets. 3.1 The in.uence of the hardware environment The target environment 
for our compiler consists of a particu­lar hardware rendering platform, together with a runtime library 
on that platform. We implemented our runtime and compiler on four platforms: Sony PlayStation2TM(PS2), 
Microsoft XBOXTM , Nintendo GameCubeTM(NGC), and DirectX 8 PC platforms. Al­though these architectures 
differ substantially[Suzuoki et al. 1999; Lindholm et al. 2001; Microsoft 2000], they share some fundamen­tal 
characteristics. These are: The CPU and GPU are separate processors that are connected by a relatively 
narrow bus.1  The GPU is user programmable.2  1Even with its Uni.ed Memory Architecture, running at 
AGP 4x speed, the XBOX still has limited bus bandwidth to the GPU. 2This is not strictly true in the 
case of the Nintendo GameCube, but a rich set of prede.ned computational elements is available. The platforms 
have varying levels of texture combining sup­port, but all texture combining occurs as a post GPU stage 
with no feedback to the GPU or CPU.3 Keeping these characteristics in mind, we want to avoid CPU operations 
of the form read/compute/write/submit to GPU which require at least 3 times the bus traf.c of a submission 
of static ver­tices to the GPU. Therefore, we privilege geometries with static vertex data and topologies. 
This is not to say that we do not sup­port animation many deformations can be applied in the GPU and 
pixel combiners without requiring the CPU to modify the in­put data. In particular, we fully support 
hierarchical coordinate frame animation [Stern 1983] with an arbitrary number of coordi­nate frames, 
including weighted skinning [Terzopoulos et al. 1987; Lander 1998]. 3.2 The runtime API Our runtime 
environment presents a small C++ API to the user, ori­ented toward drawing models as primitive objects. 
A model is a collection of geometric primitives (e.g., triangles, b-splines, points, etc.) and state 
objects (rendering states, textures, etc.) bound to the shaders required to draw them with predetermined 
effects. A model is segmented into geometries, each of which can be indepen­dently turned on or off. 
Geometries and models map directly to art assets input to the compiler. Each run of the art asset compiler 
gen­erates a single model, containing one or more geometries. Models may expose a set of control variables 
to the user (§4.3). Drawing a model is done by setting the required control variables and calling the 
model s Draw method. It is only necessary to set the control variables when their value changes as the 
system retains their val­ues. This is the mechanism used to control animations as well as other dynamic 
rendering effects. The API does not expose polygon rendering calls. The user can construct models procedurally 
using an API similar to the OpenGL vertex array construction [OpenGL Architecture Review Board et al. 
1999]. These are referred to as dynamic models, and have worse rendering performance than our compiled 
models. Apart from the model rendering, we also manage the usual house-keeping operations such as setting 
up the hardware for ren­dering. As well, we provide utility functions to manipulate hard­ware state objects 
and textures. Note that these objects reside in the compiled models, but may be exposed to the user. 
We also provide a viewport and camera abstraction as a convenience to the user, but these may be replaced 
if desired. Finally, we provide an animation engine that interfaces with models via their exposed control 
vari­ables. The details of this system are not of interest in this paper.  4 Render Methods A render 
method consists of a speci.cation of a set of variables and a shader program that uses these variables. 
Render methods are de­signed to be suf.ciently abstract that the same render method can be used over 
a wide range of art assets. Conversely, some render meth­ods are written to address a problem present 
in a speci.c art asset. Our compiler architecture allows us to apply the render method to any art asset 
that has the data necessary to satisfy the shader input variables. Figure 2 shows a simple example that 
implements gouraud shad­ing on the PS2.4 The inputs section describes the variables 3Such feedback is 
possible, and can be exploited by our shaders on some platforms, but are managed externally from our 
art asset compilation pro­cess. 4This is not the optimal gouraud shading code, there being many data 
optimizations required to achieve peak performance. #include "types.rmh" rendermethod gouraud { inputs 
{ Coordinate4 coordinates; Char geometry_name; } variables { int nverts; extern volatile Matrix 
xform_project = Viewport::XFormProject; PackCoord3 coords[nElem] = PackCoordinates(coordinates); 
ColourARGB colours[nElem]; export modifiable State geometry_name::state; noupload RenderBuffer output[nElem]; 
 } computations { TransformAndColour(nverts, coords, colours, xform_project, output); XGKick(geometry_name::state); 
 XGKick(output); } } Figure 2: A simple render method made available to the render method from the 
art asset. The variables section describes the variables used by the shader. The computationssection 
describes the shader program. 4.1 Types Because we use render methods to bind arbitrary geometric data 
di­rectly to shaders, variables in both the inputs and variables sections are typed. This assures that 
data presented to the shader are in the form required. Types are user extensible and given in the render 
method speci.cation. Type de.nitions are platform speci.c, and include such properties as the size of 
object in CPU memory, the size in GPU memory, hardware states required to transfer the type from CPU 
to GPU, and so on. The information about a type allows the compiler to manipulate the elements of the 
art asset with­out assumptions about them. We provide an identical set of base types on all platforms. 
A full description of the type construction system is beyond the scope of this paper. 4.2 Inputs The 
inputs section declares the types of the data elements re­quired by the render method. We call these 
input variables. In this example, the coordinates input variable is declared with type Coordinate4. This 
variable can then be referenced in a converter, a data manipulation program executed to construct data 
images for the variables section, further described in the next sec­tion. The input declaration can be 
accompanied by an explicit value to assign to the variable, which is used if the art asset does not pro­vide 
such an input. Input data from the art asset are bound by name to each such input variable. The variables 
provided by our compiler include vertex and material data from the art asset, as well as arbi­trary data 
the user may have tagged on vertices and materials. This allows easy extension of our compiler s functionality 
through sim­ple naming conventions binding the user data to the render method inputs. 4.3 Variables 
The variablessection declares the data that are available to the computations. Variables are tagged with 
a particular type and a number of elements as an array speci.cation. In the absence of an array speci.cation 
a length of one is assumed. The special array length nElem is a key for a simple constraint system to 
maximize the number of elements within the constraints of the hardware. All variables (save those tagged 
as temporaries by the nouploadkeyword) have values derived from the inputs declared in the inputsection. 
When an explicit converter is speci.ed using the = Converter(params,...) syntax, the converter func­tion 
(dynamically loaded from a user-extensible library) is executed with the given parameters. The parameters 
are themselves either the result of a conversion function, or an input variable. In the ab­sence of an 
explicit converter speci.cation, the identity converter is assumed. The converter mechanism allows simple 
compile-time manipulations of the data elements. Typical uses include, as in our example, data packing 
operations, as well as various re-indexing tasks, and various pre-processing operations such as binormal 
vec­tor generation or colour balancing. Since shaders are tightly cou­pled to art assets through our 
compiler, it makes sense to move as much shader-speci.c pre-processing into these converters as possi­ble, 
making the art conversion process as uniform as possible for differing assets and shaders. Not all variables 
used by a shader can be made available through a compiler pre-process, however. For example, data such 
as trans­formation matrices are not available at compile time. They could be communicated as implicit 
variables. This would restrict the user from extending the set of such variables. Instead, we chose to 
ex­tend our render method speci.cation through external linkage. By adding the externkeyword to a variable 
declaration along with an assignment of the form = variable_name, a reference is made to an external 
variable named variable_name. In our example in Figure 2 the runtime will be responsible for replacing 
unresolved references to the variable named Viewport::XFormProject with a pointer to the actual runtime 
address of this variable. This external reference is resolved when the asset is loaded. We store our 
assets in ELF format [Tool Interface Standards 1998] and provide the external linkage through our ELF 
dynamic loader. Our library registers a number of variables with the dynamic loader, making transformation 
matrices, for example, available to render methods. The user may also, at runtime, register his own variables. 
Because the GPU and CPU may operate in parallel, changes to an externally linked variable on the CPU 
may lead to race con­ditions. To eliminate the race condition without introducing un­due locking restrictions, 
we would like copy the data elements that might produce such a race. We do not want, however, to copy 
all such variable data because of the overhead required. Instead we explicitly .ag the elements that 
might induce such a race with the volatilekeyword. The user may omit the keyword, causing use by reference 
rather than copy, and more ef.cient execution. Omit­ting the volatilekeyword on externally linked variables 
is gen­erally risky, there being no control over allowed modi.cation of such variables. Another important 
aspect of our variables is that they are, in gen­eral, opaque to the runtime. It is not possible to examine 
or set values inside our compiled art assets, since the compiler may have re-ordered, simpli.ed, or otherwise 
hidden the data elements. Al­though this restriction leads to greater rendering ef.ciency, it is not 
suf.ciently .exible. There is frequently need to examine, on the CPU rather than in the render method, 
data stored in a model. In many cases it is useful to export control variables, that unlike exter­nal 
variables, reside with and in the art asset. Then the user does not need to construct, register, and 
manage these variables at runtime. Variables are declared exported in the render method by pre.x­ing 
the declaration with the keyword export. As the compiler emits data that is marked as exported, it adds 
an ELF symbol refer­ence to the variable, along with its name, to a dictionary associated with each model. 
At runtime the user can query the models dic­tionary to .nd the address of a particular variable. In 
general this is an iteration over a number of variables with the same name. If a variable shares the 
same name and binary image, only one ver­sion of that variable is added to the dictionary. If the binary 
images differ but the names are the same, both are added, with the same name. Since these names occur 
in each packet compiled with the same render method, some system is required to differentiate them. To 
allow the user to distinguish between these variables, a name extension mechanism is provided. String 
variables can be referred to in the variable name to extend the variable name at compile time. For example, 
in Figure 2 the state variable is extended with the contents of the string variable geometry_name. These 
string variables are communicated using the same mechanism as any other inputs, and so can be generated 
by the compiler front end, or be strings of user data attached to the material or the input model. This 
name extension mechanism can be used to implement a scoping system for variables. Our compiler front 
end provides scoping at the level of the Model, Geometry and packet. The user can easily manage additional 
levels of scoping by tagging their art asset in the original authoring package. Although the exported 
variables dictionary returns a pointer to the exported variable, costs are associated with allowing modi.­cation 
of compiled data. For example, on PC class architectures, vertex data is duplicated into memory that 
cannot be ef.ciently accessed from the CPU, but is very fast for the GPU. Therefore modi.cations to this 
data by the CPU require the data to be copied again. Such performance costs led us to require a formal 
declara­tion of which elements are to be modi.able at runtime, using the modifiable keyword, and we prohibit 
modi.cation of data not so .agged. The dictionary maintains a .ag indicating the modi.­able status of 
a variable, and enforces the restriction on modi.ca­tion using the C++ const mechanism. The use of the 
modifiable .ag is orthogonal to the volatile .ag. This allows use by ref­erence of exported modi.able 
data when this is appropriate (infre­quent changes to large data structures), while still allowing use 
by copy when necessary (re-use of a model in the same frame with differing parameter settings, frequent 
changes). This modi.cation mechanism gives us functionality equivalent to parameterized dis­play lists, 
constructed off-line in our compiler, and suitably opti­mized. One simple extension made possible by 
the exported variables dictionary, is a runtime GUI tool to remotely view and modify the exported variables 
associated with a model. This allows an artist or programmer to tweak the appearance and behavior of 
the model without recompiling the art asset. The compiler can in turn take the saved data from this tool 
as input to a later compilation of the same art asset, or indeed other art assets with similar layout. 
 4.4 Computations Rather than introduce a new shader language, our system uses the native shading languages 
on each target platform, plus a macro ex­pansion system to connect the variables to the shader program. 
To make it easier to write render methods we break up complex shaders into reusable parameterized macros. 
The macros are de.ned in ex­ternal .les and are catenated and assembled into machine speci.c shader programs. 
A parameter passing convention binds the vari­ables and relevant type information to the parameters of 
the macros. This lets the user quickly prototype new effects using existing pro­gram fragments. However, 
highly optimized shaders require hand crafted shader programs. Shader compilers, such as those of Proud­foot, 
et al. [Proudfoot et al. 2001] or Peercy, et al. [Peercy et al. 2000] could be adopted into this architecture 
at this level. Our example in Figure 2 shows a simple computations section in which a transform with 
colour-per-vertex function is called, fol­lowed by two XGKick blocks. These latter PlayStation2-speci.c 
computations initiate a transfer of values from the PlayStation2 Vector Unit to Graphics Synthesiser 
hardware registers, kicking off the rasterization stage of the graphics pipeline.  5 Compiler Front 
End The front end of the compiler takes an art asset and constructs the series of packets required to 
render it. Our front end deals exclu­sively with art assets that are composed of polygons, although other 
front ends have been written to deal with other kinds of data such as spline surfaces, particle systems, 
custom terrain meshes, and cus­tom city rendering systems. Our front end breaks the art asset into geometries, 
which have been de.ned by the artist. Within each geometry the polygons are collected and classi.ed by 
material and vertex properties. Materials are intrinsic to the asset editing packages we use and can 
contain arbitrary user de.ned data, as well as the standard pre­de.ned material properties. The front 
end is also responsible for identifying materials that are in fact identical and merging them. A vertex 
consists of the usual properties, such as a position, tex­ture coordinate set, normal, and so on, but 
can also include arbitrary user de.ned data. Having grouped polygons into classes the compiler must select 
a render method to associate with each class. Each class consists of: a set of material properties each 
with a name; a collection of vertices consisting of a set of named data elements, and a collection of 
poly­gons composed from the vertices. The basic mechanism to select a render method is to iterate over 
a list of available render methods until one is found whose unde.ned inputs can be satis.ed by the available 
data in the class. To provide .ner control the material can also be tagged with a requested render method. 
If the requested ren­der method s unde.ned inputs cannot be satis.ed by the available data, then the 
default mechanism is applied and a warning is issued. Once a class has been associated with a render 
method, the com­piler must construct one or more packets from the data in the class. This cutting of 
the data into smaller packets is a re.ection of hard­ware restrictions that limit the number data elements 
that can be transfered to the GPU. On some platforms this is a hard limit based on the size of GPU memory, 
on others it is an empirically deter­mined point of best ef.ciency. Some platforms differentiate be­tween 
streams of data and constant elements. For example, PlaySta­tion2 has a .xed size limit for one packet, 
including matrices, state information, as well as stream oriented data such as positions, tex­ture coordinate 
sets, and normals. All these elements must .t in 512 quadwords, including the output buffers. On the 
XBox and PC how­ever, there is a differentiation between stream data and other values streams may be 
of near-arbitrary length, but the other data, as they are stored in shader constant registers, are limited 
to 192 quad words per packet on XBox and 96 on PC. These restrictions are re.ected in the segmentation 
computation. Once constructed the packets are passed to the packet compiler layer (§6). The process of 
partitioning the data in the class into packets in­cludes the process of performing triangle stripping 
or mesh reorder­ing [Hoppe 1999] for ef.cient rendering of the underlying poly­gons, and may require 
the cloning of vertex data that must appear in more than one packet. A signi.cant step in our packetization 
process is the association of multiple coordinate frames with the vertices. Our character an­imation 
system allows for each vertex to be attached to one of a large set of coordinate frames. These coordinate 
frames are in turn constructed out of a linear combination of a smaller set of coordi­nate frames that 
represent an animation skeleton. Because of mem­ory limitations on the GPU we must limit the number of 
unique coordinate frames that appear in each packet. We refer to this set of coordinate frames as the 
matrix set. This transforms our mesh optimization step into a multi dimensional optimization problem: 
simultaneously minimize the number of vertex transforms required on the hardware, and the number of matrix 
sets induced. Often a strip cannot be extended with a particular triangle because this would cause the 
matrix set required for the packet to exceed the maximum matrix set size. Effective algorithms to solve 
this multi­dimensional optimization problem are an area of active research. 6 Packet Compiler Emitting 
a list of packets that are interpreted at runtime leads to poor runtime performance. There are two approaches 
to optimizing such a scheme. The usual approach is to optimize the runtime envi­ronment, implementing 
such strategies as minimizing modi.cation of hardware state, re-ordering rendering by texture usage, 
caching computation results for reuse and so on. However, many of these optimizations can be performed 
off-line because the data to be ren­dered is known ahead of time. The packet compiler is responsible 
for transforming the packets generated by the front end into data and associated code that can be executed 
to render the art asset without any external intervention by a programmer. The code generated by the 
packet compiler is an optimized program tailored to render exactly the input art asset. Note that the 
packet compiler uses no information about the topol­ogy. Hence, it can be used to compile arbitrary data 
sets, not just polygonal data sets. The form of the output is radically different across platforms. Despite 
these differences, there is a common structure in the back end of the compiler. The back end always generates 
a model ob­ject which contains the following: a pointer to a byte code stream that must be executed to 
render the model; a dictionary pointing to the data exported from the render methods used in the packets 
rep­resenting the model; and external references to imported data that will be resolved to pointers at 
load time. These might occur in the byte code, or in other hardware speci.c data structures. Addition­ally, 
the byte code contains references to the hardware speci.c data structures that contain the information 
required for rendering. For each platform hardware speci.c optimizations for rendering speed are performed 
on the byte code and data structures generated. These optimizations largely rely on the knowledge that 
the render­ing of a model can be treated as an atomic operation and the state of the hardware is therefore 
fully controlled between each packet submitted to the hardware in the rendering of the model. The back 
end is structured in several passes as follows. Pass 1: Packet ordering. Pass 2: Variable data construction. 
Pass 3: Export data accumulation. Pass 6: Data structure generation. Pass 5: Code generation. Pass 6: 
Global optimizations of data structures and code. Pass 7: Code and data emission. In the remaining subsections 
we examine these passes in more detail. As the .nal pass is largely self evident we omit further de­scription. 
6.1 Packet ordering To use the underlying hardware ef.ciently, we reorder packets to minimize expensive 
changes in the hardware state. We restrict re­ordering to allow the user to retain control of rendering 
order. In particular, we guarantee that the geometries of a model will be ren­dered in the order they 
appear in the art asset. Currently we imple­ment a simple heuristic to minimize expensive state changes. 
We group packets .rst by geometry, then by render method, then by textures, and .nally by matrix set. 
A more sophisticated compiler might examine the generated code stream and model the cost of its operations 
to determine the best possible data ordering. 6.2 Variable data construction The front end provides 
the back end with a list of packets, each of which has an associated render method and set of input data. 
The input data is not what is ultimately fed to the shader, and therefore it must be converted into the 
data de.ned in the variables section of the render method associated with the packet. This is accom­plished 
by executing the converter functions speci.ed by the render method. The result is an instantiated packet. 
In an instantiated packet the data for every variable is either known, or an external symbolic reference 
is known that will resolve to the memory loca­tion of that data at run time. We refer to variables that 
have fully known data contents as hard data. Variables that are only de.ned by extern declarations (imported 
data) are called soft data. At this stage, the compiler also assigns symbolic names to every vari­ables 
data. These symbolic names are used to refer to the memory location containing the data, and are used 
in the remaining passes whenever a direct reference to the data must be generated. In the case of soft 
data the symbolic name is the name de.ned by the externdeclaration in the render method. Although symbolic 
names are assigned to each block of data at this point, the data itself is neither emitted nor placed 
into speci.c data structures. This is done in the data structure generation pass, described in §6.4. 
 6.3 Export data accumulation This pass accumulates the dictionary data structure associated with the 
model that can be used at runtime to .nd exported variables. The symbolic names assigned to data in the 
prior pass are used here to .ll in pointers in the resulting dictionary. 6.4 Data structure generation 
In this pass we create the rendering data structure that holds the hard and soft data refered to by the 
instantiated packets. On many of our target platforms, we wish to feed the underlying rendering hardware 
as directly as possible. This allows us to avoid device driver overhead and unnecessary manipulation 
of the data. We ac­complish this by building the data structures in as near native form as possible. 
For example, on the PS2, a chained direct memory access (DMA) unit feeds data to the GPU in parallel 
with CPU operations. The DMA unit supports a nested CALL structure, much like a pro­cedure call. This 
allows us to pre-build large fragments of the DMA chain with the rendering data embedded in the DMA chain. 
One ad­vantage of this is that the CPU need not ever touch the data in these pre-assembled DMA fragments, 
only chain together CALL opera­tions to the DMA fragments at render time. Another advantage is that memory 
overhead required for model submission is lowered, because extra copies of the data are not required. 
On the Gamecube we construct a similar data structure that feeds the hardware directly. On the XBOX and 
PC we pre-assemble ver­tex buffers and hardware command streams. 6.5 Code generation In the code generation 
pass we generate a byte code program for each instantiated packet that performs the set of CPU operations 
required to render the data contained in the packet. In the next pass the byte code programs are catenated 
and global optimizations are performed over the resulting program. We choose to use a byte code to express 
these programs, rather than native assembly instructions. The overhead in interpreting the byte code 
is minimal and is offset by the fact that the byte code interpreter .ts into the instruction cache on 
the CPU. In fact, on some hardware the byte code interpretation is faster than executing a stream of 
in-line machine instructions. This is due to a reduction in instruction cache misses and procedure calls. 
Furthermore, byte code has two major advantages. First, programs in the byte code are very compact. Second, 
because the instruction set of our byte code is very small (10 20 instructions, depending on the platform), 
it is easy to write an optimizer for the byte code. The instruction set for a platform depends on the 
underlying hardware. For example, on the PS2 we submit a single DMA chain to the hardware encoding the 
rendering for an entire scene. The byte code instructions on this hardware perform simple operations 
geared toward assembling this DMA chain, as well as some more CPU intensive operations that generate 
data that is needed in the DMA chain. Examples of the former operations include placing a call to a .xed 
chunk of DMA data into the chain, and copying volatile data directly into the chain. Examples of the 
later include uploading a new vertex shader program to the hardware, and com­puting matrix set sets for 
an animated object from animation data. On platforms that have more of an API between us and the hard­ware, 
the byte code closely corresponds to the calls to the under­lying rendering API, for example, setting 
of vertex streams, state setting calls, and rendering submission instructions. 6.6 Global optimizations 
The speci.c optimizations performed in this pass are dependent upon the nature of the target platform. 
We classify these optimiza­tions into two classes: data transfer optimizations and redundant code removal. 
In performing data transfer optimizations we seek to remove re­dundant transfers of data to the GPU. 
This is really a special case of redundant code removal. We do this by simulating the contents of the 
GPU memory over the execution of the rendering of a model, and noting uploads that do not change the 
memory image. For ex­ample, this optimization step removes the redundant setting of the transformation 
matrix from one packet to the next. Because we do not simulate the GPU execution in detail, only the 
uploading of data to the GPU, we require hints to tell us when the GPU will modify the contents of a 
memory location, forcing us to upload into that location. We have two keywords to tag a render method 
variable as hints to the optimizer: noupload and transient. The noupload keyword indicates that a variable 
is a temporary variable to be used by the GPU as needed. The transient keyword indicates a variable that 
must be set before the shader program is run, but that will be modi.ed by the execution of the shader 
program. Along with data upload optimization, we consider similar op­timizations of machine register 
setting instructions and byte code instructions. For example, on the PS2 we note successive pack­ets 
that occur with only a CALL instruction in their byte code and merge the DMA chains for the packets together. 
This can result in very large models that are submitted with only a few byte code instructions. As another 
example, on the PS2 the data transfer mechanism is itself a state machine that must be appropriately 
set to decode the data as it is fed to the GPU. In this case we simulate the data transfer hardware to 
.nd the minimal set of register changes required to set the data transfer hardware into the desired state. 
This can account for as much as a 20% reduction of the rendering time on the PS2. Many other speci.c 
optimizations are used on the various plat­forms we support. We omit further details due to space constraints. 
 7 Results and Conclusions Table 1 summarizes some typical performance numbers achieved by our system. 
These .gures are sustainable throughput rates for production art assets. For the sake of comparison, 
we also include .gures for the Stanford Bunny model. The CPU is largely idle in these examples, as would 
be required to allow for an interactive ap­plications use of the CPU. The bottle necks on these systems 
are generally in the bus to the GPU, and the transformation and raster­ization engines. In some cases, 
better performance numbers can be achieved by intensive use of the CPU, but this would not represent 
the usage we expect. In our experience the performance achieved by our system is as good or better for 
throughput, and much better on CPU usage than the custom rendering engines it has replaced. The system 
has shown itself to be easy to port to new and dif­fering architectures. Originally designed for the 
PS2, ports to the XBox and GameCube took approximately 3 man months each. For each platform this involved 
writing a new runtime, a new compiler back end, and a suite of render methods. Porting a product that 
uses our system from one platform to an­other has been easy. Turn around times of as little as a week 
have been achieved with full products. More commonly a product port takes about a month, including required 
revisions to art assets to take into account platform performance differences. The render method paradigm 
has also proved successful, with users greatly extending the functionality of the system by writing their 
own render methods. This has included such features as a crowd rendering system (seen in .gure 3), multiple 
particle systems (an example is seen in .gure 4), as well as specialized multi-texture and multi-pass 
effects. Additionally, when our compiler front end semantics have been insuf.cient to support the needs 
of the user, new front ends have been developed and successfully deployed. For example, the cityscape 
seen in .gure 4 contains many objects that share sub­stantial portions of their geometric data. A custom 
front end was written that allowed this data to be shared across models using the hooks provided to externally 
linked variables. In another case, a front end for a custom landscape patch rendering system was writ­ten 
and deployed in less than a week. There are several potential future extensions to the system that we 
have considered. The existing mesh generation algorithm is not optimal in the presence of matrix set 
constraints. As previously mentioned, better algorithms for this problem are an active area of research. 
Our heuristic for ordering packets is simplistic at best, and should be replaced by an optimization scheme 
that attempt to .nd the ordering with minimal execution cost. One area of weakness in the current system 
is that dynamic models (those constructed at run­time) do not perform as ef.ciently as compiled models. 
To address this a light weight version of the compiler should be integrated into the runtime. Finally, 
render methods are currently platform spe­ci.c. The work on shading language compilers should be adopted 
into our render method scheme to allow them to become cross plat­form speci.cations.  References APODACA, 
A. A., AND GRITZ, L. 1999. Advanced Renderman: Creating CGI for Motion Pictures. Morgan Kauffman Publishers. 
BARRELL, K. F. 1983. The graphical kernel system -a replacement for core. First Australasian Conference 
on Computer Graphics, 22 26. BOGOMJAKOV, A., AND GOTSMAN, C. 2001. Universal rendering sequences for 
transparent vertex caching of progressive meshes. In Proceedings of Graphics In­ terface 2001, 81 90. 
 Platform Gouraud Lit Skinned Gouraud PS2 XBox NGC PC 17.0/22.6 47.2/91.4 18.7/NA 24.1/46.1 10.9/14.7 
22.4/43.4 10.3/NA 15.9/20.9 8.5/11.5 14.2/30.3 7.2/NA 5.1/10.9 25.2/31.8 63.9/93.8 NA/NA 26.3/36.2 Table 
1: Performance .gures of our system, in millions of polygons per second and vertex indices submitted 
per second. The player is a 3998 polygon model, drawn without texture or lighting, with textures and 
lighting, and skinned with textures and lighting. The bunny model is 69451 polygons. Bunny model courtesy 
of the Stan­ford Computer Graphics Laboratory. The PC is a 1.4Ghz AMD Athlon with an ATI Radeon 8500 
graphics accelerator. Figure 3: A screen capture of a PS2 application using our system, demonstrating 
skinning for the characters, a lit stadium, and a cus­tom crowd renderer, all implemented as render methods. 
 COOK, R. L. 1984. Shade trees. In Computer Graphics (Proceedings of SIGGRAPH 84), vol. 18, 223 231. 
HANRAHAN, P., AND LAWSON, J. 1990. A language for shading and lighting calcu­lations. In Computer Graphics 
(Proceedings of SIGGRAPH 90), vol. 24, 289 298. HOPPE, H. 1999. Optimization of mesh locality for transparent 
vertex caching. Pro­ceedings of SIGGRAPH 99 (August), 269 276. JAQUAYS, P., AND HOOK, B. 1999. Q3radiant 
shader manual. LANDER, J. 1998. Skin them bones: Game programming for the web generation. Game Developer 
Magazine, 11 16. LINDHOLM, E., KILGARD, M. J., AND MORETON, H. 2001. A user-programmable vertex engine. 
In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIG-GRAPH, Computer Graphics Proceedings, Annual Conference 
Series, 149 158. MICROSOFT. 2000. DirectX 8 Programmer s Reference. Microsoft Press. MOLNAR, S., EYLES, 
J., AND POULTON, J. 1992. Pixel.ow: High-speed rendering using image composition. In Computer Graphics 
(Proceedings of SIGGRAPH 92), vol. 26, 231 240. OLANO, M., AND LASTRA, A. 1998. A shading language on 
graphics hardware: The pixel.ow shading system. In Proceedings of SIGGRAPH 98, ACM SIGGRAPH / Addison 
Wesley, Orlando, Florida, Computer Graphics Proceedings, Annual Con­ference Series, 159 168. OPENGL ARCHITECTURE 
REVIEW BOARD, WOO, M., NEIDER, J., DAVIS, T., AND SHREINER, D. 1999. OpenGL Programming Guide: The Of.cial 
Guide to Learning OpenGL, Version 1.2. Addison-Wesley. PEERCY, M. S., OLANO, M., AIREY, J., AND UNGAR, 
P. J. 2000. Interactive multi­pass programmable shading. Proceedings of SIGGRAPH 2000 (July), 425 432. 
PERLIN, K. 1985. An image synthesizer. In Computer Graphics (Proceedings of SIGGRAPH 85), vol. 19, 287 
296. PROUDFOOT, K., MARK, W. R., TZVETKOV, S., AND HANRAHAN, P. 2001. A real­time procedural shading 
system for programmable graphics hardware. In Proceed­ings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, 
Computer Graphics Proceedings, Annual Conference Series, 159 170. ROHLF, J., AND HELMAN, J. 1994. Iris 
performer: A high performance multipro­cessing toolkit for real-time 3d graphics. In Proceedings of SIGGRAPH 
94, ACM SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, An­nual Conference Series, 
381 395. STERN, G. 1983. Bbop a system for 3d keyframe .gure animation. In Introduction to Computer 
Animation, Course Notes 7 for SIGGRAPH 83, 240 243. SUZUOKI, M., KUTARAGI, K., HIROI, T., MAGOSHI, H., 
OKAMOTO, S., OKA, M., OHBA, A., YAMAMOTO, Y., FURUHASHI, M., TANAKA, M., YUTAKA, T., OKADA, T., NAGAMATSU, 
M., URAKAWA, Y., FUNYU, M., KUNIMATSU, A., GOTO, H., HASHIMOTO, K., IDE, N., MURAKAMI, H., OHTAGURO, 
Y., , AND AONO, A. 1999. A microprocessor with a 128-bit cpu, ten .oating-point mac s, four .oating-point 
dividers, and an mpeg-2decoder. In IEEE Journal of Solid-State Circuts: Special issue on the 1999 ISSCC: 
Digital, Memory, and Signal Processing, IEEE Solid-State Circuits Society, 1608. TERZOPOULOS, D., PLATT, 
J., BARR, A., AND FLEISCHER, K. 1987. Elastically de­formable models. In Computer Graphics (Proceedings 
of SIGGRAPH 87), vol. 21, 205 214. TOOL INTERFACE STANDARDS. 1998. Elf: Executable and linkable format. 
ftp://ftp.intel.com/pub/tis. Figure 4: Another scene using our system, showing cityscape gen­erated by 
a user-developed alternative compiler front end, and a particle system.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566642</section_id>
		<sort_key>721</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fluids and fire]]></section_title>
		<section_page_from>721</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14220391</person_id>
				<author_profile_id><![CDATA[81100642559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566643</article_id>
		<sort_key>721</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Physically based modeling and animation of fire]]></title>
		<page_from>721</page_from>
		<page_to>728</page_to>
		<doi_number>10.1145/566570.566643</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566643</url>
		<abstract>
			<par><![CDATA[We present a physically based method for modeling and animating fire. Our method is suitable for both smooth (laminar) and turbulent flames, and it can be used to animate the burning of either solid or gas fuels. We use the incompressible Navier-Stokes equations to independently model both vaporized fuel and hot gaseous products. We develop a physically based model for the expansion that takes place when a vaporized fuel reacts to form hot gaseous products, and a related model for the similar expansion that takes place when a solid fuel is vaporized into a gaseous state. The hot gaseous products, smoke and soot rise under the influence of buoyancy and are rendered using a blackbody radiation model. We also model and render the blue core that results from radicals in the chemical reaction zone where fuel is converted into products. Our method allows the fire and smoke to interact with objects, and flammable objects can catch on fire.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[blackbody radiation]]></kw>
			<kw><![CDATA[chemical reaction]]></kw>
			<kw><![CDATA[fire]]></kw>
			<kw><![CDATA[flames]]></kw>
			<kw><![CDATA[implicit surface]]></kw>
			<kw><![CDATA[incompressible flow]]></kw>
			<kw><![CDATA[smoke]]></kw>
			<kw><![CDATA[stable fluids]]></kw>
			<kw><![CDATA[vorticity confinement]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382430</person_id>
				<author_profile_id><![CDATA[81100511689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Duc]]></first_name>
				<middle_name><![CDATA[Quang]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University and Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14211105</person_id>
				<author_profile_id><![CDATA[81100612327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fedkiw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University and Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14219695</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258757</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BUKOWSKI, R., AND SEQUIN, C. 1997. Interactive Simulation of Fire in Virtual Building Environments. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 35-44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHIBA, N., MURAOKA, K., TAKAHASHI, H., AND MIURA, M. 1994. Two dimensional Visual Simulation of Flames, Smoke and the Spread of Fire. The Journal of Visualization and Computer Animation 5, 37-53.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>513878</ref_obj_id>
				<ref_obj_pid>513867</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DEVLIN, K., AND CHALMERS, A. 2001. Realistic visualisation of the pompeii frescoes. In AFRIGRAPH 2001, ACM SIGGRAPH, A. Chalmers and V. Lalioti, Eds., 43-47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732137</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DURAND, F., AND DORSEY, J. 2000. Interactive Tone Mapping. In Proceedings of Eleventh Eurographics Workshop on rendering, 219-230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FAIRCHILD, M. 1998. Color Appearance Models. Addison Wesley Longman, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>329653</ref_obj_id>
				<ref_obj_pid>329646</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FEDKIW, R., ASLAM, T., MERRIMAN, B., AND OSHER, S. 1999. A Non-oscillatory Eulerian Approach to Interfaces in Multimaterial Flows (The Ghost Fluid Method). J. Comput. Phys. 152, 457.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FEDKIW, R., STAM, J., AND JENSEN, H. W. 2001. Visual Simulation of Smoke. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 15-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND FEDKIW, R. 2001. Practical Animation of Liquids. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258838</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND METAXAS, D. 1997. Modeling the Motion of a Hot, Turbulent Gas. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 181-188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GROSSHANDLER, W. 1995. RADCAL: A Narrow-band Model for Radiation Calculations in Combustion Environment. NIST Technical Note 1402 (April).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HENYEY, L., AND GREENSTEIN, J. 1941. Diffuse radiation in the galaxy. Astrophysics Journal 93, 70-83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90972</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[INAKAGE, M. 1989. A Simple Model of Flames. In Proceedings of Computer Graphics International 89, Springer-Verlag, 71-81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MARKSTEIN, G. H. 1964. Nonsteady Flame Propagation. Pergamon, Oxford.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351688</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MAZARAK, O., MARTINS, C., AND AMANATIDES, J. 1999. Animating Exploding Objects. In Proceedings of Graphics Interface 1999, 211-218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MUSGRAVE, F. K. 1997. Great Balls of Fire. In SIGGRAPH 97 Animation Sketches, Visual Procedings, ACM SIGGRAPH, 259-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351686</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[NEFF, M., AND FIUME, E. 1999. A Visual Model for Blast Waves and Fracture. In Proceedings of Graphics Interface 1999, 193-202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>515211</ref_obj_id>
				<ref_obj_pid>515207</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[NGUYEN, D., FEDKIW, R., AND KANG, M. 2001. A Boundary Condition Capturing Method for Incompressible Flame Discontinuities. J. Comput. Phys. 172, 71-98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311550</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical Modeling and Animation of Brittle Fracture. In Proceedings of SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 137-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[OSHER, S., AND SETHIAN, J. A. 1988. Fronts Propagating with Curvature Dependent Speed: Algorithms Based on Hamilton-Jacobi Formualtions. J. Comput. Phys. 79, 12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[PATTANAIK, S., FERWERDA, J. A., FAICHILD, M. D., AND GREENBERG, D. P. 1998. A Multiscale Model of Adaptation and Spatial Vision for Realistic Image Display. In Proceedings of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 287-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PERRY, C., AND PICARD, R. 1994. Synthesizing Flames and their Spread. SIGGRAPH 94 Technical Sketches Notes (July).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618289</ref_obj_id>
				<ref_obj_pid>616037</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[RUSHMEIER, H. E., HAMINS, A., AND CHOI, M. 1995. Volume Rendering of Pool Fire Data. IEEE Computer Graphics and Applications 15, 4, 62-67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[RUSHMEIER, H. 1994. Rendering Participating Media: Problems and Solutions from Application Areas. In Proceedings of the 5th Eurographics Workshop on Rendering, 35-56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SETHIAN, J. 1996. A Fast Marching Level Set Method for Monotonically Advancing Fronts. Proc. Nat. Acad. Sci. 93, 1591-1595.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SIEGEL, R., AND HOWELL, J. 1981. Thermal Radiation Heat Transfer. Hemisphere Publishing Corp., Washington, DC.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218430</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[STAM, J., AND FIUME, E. 1995. Depicting Fire and Other Gaseous Phenomena Using Diffusion Process. In Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 129-136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[STAM, J. 1999. Stable Fluids. In SIGGRAPH 99 Conference Proceedings, Annual Conference Series, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 121-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[STANIFORTH, A., AND COTE, J. 1991. Semi-Lagrangian Integration Schemes for Atmospheric Models: A Review. Monthly Weather Review 119, 2206-2223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[STEINHOFF, J., AND UNDERHILL, D. 1994. Modification of the Euler Equations for "Vorticity Confinement": Application to the Computation of Interacting Vortex Rings. Physics of Fluids 6, 8, 2738-2744.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[TSITSIKLIS, J. 1995. Efficient Algorithms for Globally Optimal Trajectories. IEEE Transactions on Automatic Control 40, 1528-1538.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[TURNS, S. R. 1996. An Introduction to Combustion. McGraw-Hill, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344801</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[YNGVE, G. D., O'BRIEN, J. F., AND HODGINS, J. K. 2000. Animating Explosions. In Proceedings of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 29-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physically Based Modeling and Animation of Fire Duc Quang Nguyen Ronald Fedkiw Henrik Wann Jensen Stanford 
University Stanford University Stanford University Industrial Light + Magic Industrial Light + Magic 
henrik@graphics.stanford.edu dqnguyen@stanford.edu fedkiw@cs.stanford.edu Abstract We present a physically 
based method for modeling and animating .re. Our method is suitable for both smooth (laminar) and turbulent 
.ames, and it can be used to animate the burning of either solid or gas fuels. We use the incompressible 
Navier-Stokes equations to independently model both vaporized fuel and hot gaseous products. We develop 
a physically based model for the expansion that takes place when a vaporized fuel reacts to form hot 
gaseous products, and a related model for the similar expansion that takes place when a solid fuel is 
vaporized into a gaseous state. The hot gaseous prod­ucts, smoke and soot rise under the in.uence of 
buoyancy and are rendered using a blackbody radiation model. We also model and render the blue core that 
results from radicals in the chemical reac­tion zone where fuel is converted into products. Our method 
allows the .re and smoke to interact with objects, and .ammable objects can catch on .re. CR Categories: 
I.3.5 [Computer Graphics]: Computational Ge­ometry and Object Modeling Physically based modeling; I.3.7 
[Computer Graphics]: Three-Dimensional Graphics and Realism Ray Tracing; Keywords: .ames, .re, smoke, 
chemical reaction, blackbody ra­diation, implicit surface, incompressible .ow, stable .uids, vortic­ity 
con.nement 1 Introduction The modeling of natural phenomena such as .re and .ames remains a challenging 
problem in computer graphics. Simulations of .uid behavior are in demand for special effects depicting 
smoke, wa­ter, .re and other natural phenomena. Fire effects are especially in demand due to the dangerous 
nature of this phenomenon. Fire simulations are also of interest for virtual reality effects, for exam­ple 
to help train .re .ghters or to determine proper placement of exit signs in smoke .lled rooms (i.e. so 
they can be seen). The interested reader is referred to [Rushmeier 1994]. Combustion processes can be 
loosely classi.ed into two rather distinct types of phenomena: detonations and de.agrations. In both 
of these processes, chemical reactions convert fuel into hot gaseous products. De.agrations are low speed 
events such as the .re and .ames we address in this paper, while detonations are high speed Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
 Figure 1: A turbulent gas .ame model of a .amethrower. events such as explosions where shock waves and 
other compress­ible effects are important, see e.g. [Yngve et al. 2000] and [Neff and Fiume 1999]. As 
low speed events, de.agrations can be modeled using the equations for incompressible .ow (as opposed 
to those for compressible .ow). Furthermore, since viscous effects are small, we use the incompressible 
inviscid Euler equations similar to [Fed­kiw et al. 2001]. As noted therein, these equations can be solved 
ef.ciently using a semi-Lagrangian stable .uid approach, see e.g. [Staniforth and Cote 1991] and [Stam 
1999]. An important, often neglected aspect of .re and .ame modeling concerns the expansion of the fuel 
as it reacts to form hot gaseous products. This expansion is the reason for the visual fullness ob­served 
in many .ames and is partly responsible for the visual tur­bulence as well. Since the incompressible 
equations do not account for expansion, we propose a simple thin .ame model for capturing these effects. 
This is accomplished by using an implicit surface to represent the reaction zone where the gaseous fuel 
is converted into hot gaseous products. Although real reaction zones have a nonzero (but small) thickness, 
the thin .ame approximation works well for visual modeling and has been used by scientists as well, see 
for example [Markstein 1964] who .rst proposed this methodology. Our implementation of the thin .ame 
model is as follows. First, a dynamic implicit surface is used to track the reaction zone where the gaseous 
fuel is converted into hot gaseous products. Then both the gaseous fuel and the hot gaseous products 
are separately mod­eled using independent sets of incompressible .ow equations. Fi­nally, these incompressible 
.ow equations are updated together in a coupled fashion using the fact that both mass and momentum must 
be conserved as the gas reacts at the interface. While this gives rather pleasant looking laminar (smooth) 
.ames, we include a vorticity con.nement term, see [Steinhoff and Underhill 1994] and [Fedkiw et al. 
2001], to model the larger scale turbulent .ame struc­tures that are dif.cult to capture on the relatively 
coarse grids used for ef.ciency reasons in computer graphics simulations. We also include other features 
important for visual simulation, such as the buoyancy effects generated by hot gases and the interaction 
of .re with .ammable and non.ammable objects. We render the .re as a participating medium with blackbody 
radiation using a stochastic ray marching algorithm. In our rendering we pay careful attention to the 
chromatic adaptation of the observer in order to get the cor­rect colors of the .re. 2 Previous Work 
A simple laminar .ame was texture mapped onto a .ame-like im­plicit primitive and then volume-traced 
by [Inakage 1989]. [Perry and Picard 1994] applied a velocity spread model from combus­tion science to 
propagate .ames. [Chiba et al. 1994] computed the exchange of heat between objects by projecting the 
environ­ment onto a plane. The spread of .ame was a function of both the temperature and the concentration 
of fuel. [Stam and Fiume 1995] present a similar model in three spatial dimensions for the creation, 
extinguishing and spread of .re. The spread of the .re is controlled by the amount of fuel available, 
the geometry of the environment and the initial conditions. Their velocity .eld is pre­de.ned, and then 
the temperature and density .elds are advected using an advection-diffusion type equation. They render 
the .re using a diffusion approximation which takes into account multiple scattering. [Bukowski and Sequin 
1997] integrated the Berkeley Architectural Walkthrough Program with the National Institute of Standards 
and Technology s CFAST .re simulator. The integrated system creates a simulation based design environment 
for building .re safety systems. An application of physically accurate .relight, and the impact of different 
fuel types on the color of .ames and the scene they illuminate is given in [Devlin and Chalmers 2001]. 
Ac­curate ray casting through .re using spatially sparse measured data rather than simulated data was 
discussed in [Rushmeier et al. 1995] using radiation path integration software documented in [Grosshan­dler 
1995]. Although we do not consider high-speed combustion phenom­ena such as detonations in this paper, 
there has been some notable work on this subject. [Musgrave 1997] concentrated on the explo­sive cloud 
portion of the explosion event using a fractal noise ap­proach. [Neff and Fiume 1999] model and visualize 
the blast wave portion of an explosion based on a blast curve approach. [Mazarak et al. 1999] discuss 
the elementary blast wave equations, which were used to model exploding objects. They also show how to 
incorporate the blast wave model with a rigid body motion simu­lator to produce realistic animation of 
.ying debris. Most recently, [Yngve et al. 2000] model the propagation of an explosion through the surrounding 
air using a computational .uid dynamics based ap­proach to solve the equations for compressible, viscous 
.ow. Their system includes two way coupling between solid objects and sur­rounding .uid, and uses the 
spectacular brittle fracture technology of [O Brien and Hodgins 1999]. While the compressible .ow equa­tions 
are useful for modeling shock waves and other compressible phenomena, they introduce a very strict time 
step restriction asso­ciated with the acoustic waves. We use the incompressible .ow equations instead 
to avoid this restriction making our method more computationally ef.cient.  3 Physically Based Model 
We consider three distinct visual phenomena associated with .ames. The .rst of these is the blue or bluish-green 
core seen in many .ames. These colors are emission lines from intermedi­ate chemical species, such as 
carbon radicals, produced during the chemical reaction. In the thin .ame model, this thin blue core is 
located adjacent to the implicit surface. Therefore, in order to track this blue core, we need to track 
the movement of the implicit sur­face. The second visual phenomenon is the blackbody radiation emitted 
by the hot gaseous products, in particular the carbon soot. This is characterized by the yellowish-orange 
color familiarly as­sociated with .re. In order to model this with visual accuracy we need to track the 
temperatures associated with a .ame as depicted in Figure 2 (read from left to right). If the fuel is 
solid or liquid, the .rst step is the heating of the solid until it undergoes a phase change to the gaseous 
state. (Obviously, for gas fuels, we start in Figure 2: Flame temperature pro.le for a solid (or gaseous) 
fuel. this gaseous state region in the .gure.) Then the gas heats up until it reaches its ignition temperature 
corresponding to our implicit sur­face and the beginning of the thin blue core region. The temperature 
continues to increase as the reaction proceeds reaching a maximum before radiative cooling and mixing 
effects cause the temperature to decrease. As the temperature decreases, the blackbody radiation falls 
off until the yellowish-orange color is no longer visible. The third and .nal visual effect we address 
is the smoke or soot that is apparent in some .ames after the temperature cools to the point where the 
blackbody radiation is no longer visible. We model this effect by carrying along a density variable in 
a fashion similar to the temperature. One could easily add particles to represent small pieces of soot, 
but our focus in this paper is the .re, not the smoke. For more details on smoke, see [Foster and Metaxas 
1997], [Stam 1999] and [Fedkiw et al. 2001]. Figure 3 shows smoke coupled to our gas .ame. 3.1 Blue 
Core Our implicit surface separates this gaseous fuel from the hot gaseous products and surrounding air. 
Consider for example the injection of gaseous fuel from a cylindrically shaped tube. If the fuel were 
not burning, then the implicit surface would simply move at the same velocity as the gaseous fuel being 
injected. However, when the fuel is reacting, the implicit surface moves at the veloc­ity of the unreacted 
fuel plus a .ame speed S that indicates how fast fuel is being converted into gaseous products. S indicates 
how fast the unreacted gaseous fuel is crossing over the implicit surface turning into hot gaseous products. 
The approximate surface area of the blue core, AS, can be estimated with the following equation vfAf 
= SAS, (1) Figure 3: The hot gaseous products and soot emit blackbody radiation that illuminates the 
smoke. Figure 4: Blue reaction zone cores for large (left) and small (right) values of the .ame reaction 
speed S. Note the increased turbulence on the right. where vf is the speed the fuel is injected across 
the injection sur­face with area Af , e.g. Af is the cross section of the cylindrical tube. This equation 
results from canceling out the density in the equation for conservation of mass. The left hand side is 
the fuel being in­jected into the region bounded by the implicit surface, and the right hand side is 
the fuel leaving this region crossing over the implicit surface as it turns into gaseous products. From 
this equation, we see that injecting more (less) gas is equivalent to increasing (decreas­ing) vf resulting 
in a larger (smaller) blue core. Similarly, increas­ing (decreasing) the reaction speed S results in 
a smaller (larger) blue core. While we can turn the velocity up or down on our cylin­drical jet, the 
reaction speed S is a property of the fuel. For example, S is approximately .44m/s for a propane fuel 
that has been suitably premixed with oxidizer [Turns 1996]. (We use S = .5m/s for most of our examples.) 
Figure 4 shows the effect of varying the param­eter S. The smaller value of S gives a blue core with 
more surface area as shown in the .gure. This thin .ame approximation is fairly accurate for premixed 
.ames where the fuel and oxidizer are premixed so that the injected gas is ready for combustion. Non-premixed 
.ames, commonly re­ferred to as diffusion .ames, behave somewhat differently. In a diffusion .ame, the 
injected fuel has to mix with a surrounding ox­idizer before it can combust. Figure 5 shows the injection 
of fuel out of a cylindrically shaped pipe. The cone shaped curve is the predicted location of the blue 
core for a premixed .ame while the larger rounded curve is the predicted location of the blue core for 
a diffusion .ame. As can be seen in the .gure, diffusion .ames tend to have larger cores since it takes 
a while for the injected fuel and surrounding oxidizer to mix. This small-scale molecular diffu­sion 
process is governed by a second order partial differential equa­tion that is computationally costly model. 
Thus for visual purposes, we model diffusion .ames with larger blue cores simply by using a smaller value 
of S than that used for a corresponding premixed .ame.  Figure 6: Streamlines illustrating the path 
of individual .uid elements as they across the blue reaction zone core. The curved path is caused by 
the expansion of the gas as it reacts.  3.2 Hot Gaseous Products In order to get the proper visual look 
for our .ames, it is important to track individual elements of the .ow and follow them through their 
temperature histories given by Figure 2. This is particularly dif.cult because the gas expands as it 
undergoes reaction from fuel to hot gaseous products. This expansion is important to model since it changes 
the trajectories of the gas and the subsequent look and feel of the .ame as individual elements go through 
their tempera­ture pro.le. Figure 6 shows some sample trajectories of individual elements as they cross 
over the reaction front. Note that individ­ual elements do not go straight up as they pass through the 
reaction front, but instead turn outward due to the effects of expansion. It is dif.cult to obtain visually 
full turbulent .ames without modeling this expansion effect. In fact, many practitioners resort to a 
num­ber of low level hacks (and lots of random numbers) in an attempt to sculpt this behavior, while 
we obtain the behavior intrinsically by using the appropriate model. The expansion parameter is usu­ally 
given as a ratio of densities, .f /.h where . f is the density of the gaseous fuel and .h is the density 
of the hot gaseous products. Figure 7 shows three .ames side by side with increasing amounts of expansion 
from left to right. Note how increasing the expansion makes the .ames appear fuller. We used .f = 1kg/m3 
(about the 3 density of air) for all three .ames with .h = .2kg/m, .1kg/m3 and .05kg/m3 from left to 
right. We use one set of incompressible .ow equations to model the fuel and a separate set of incompressible 
.ow equations to model the hot gaseous products and surrounding air.ow. We require a model for coupling 
these two sets of incompressible .ow equations together across the interface in a manner that models 
the expansion that takes place across the reaction front. Given that mass and mo­mentum are conserved 
we can derive the following equations for the coupling across the thin .ame front: .h(Vh - D)= . f (Vf 
- D), (2) .h(Vh - D)2 + ph = . f (Vf - D)2 + pf , (3)  Figure 5: Location of the blue reaction zone 
core for a premixed Figure 7: Comparison of .ame shapes for differing degrees of .ame versus a diffusion 
(non-premixed) .ame gaseous expansion. The amount of expansion increases from left to right making the 
.ame appear fuller and more turbulent. where Vf and Vh are the normal velocities of the fuel and the 
hot gaseous products, and pf and ph are their pressures. Here, D = Vf - S is the speed of the implicit 
surface in the normal direction. These equations indicate that both the velocity and the pressure are 
discontinuous across the .ame front. Thus, we will need to exercise caution when taking derivatives of 
these quantities as is required when solving the incompressible .ow equations. (Note that the tangential 
velocities are continuous across the .ame front.)  3.3 Solid Fuels When considering solid fuels, there 
are two expansions that need to be accounted for. Besides the expansion across the .ame front, a similar 
expansion takes place when the solid is converted to a gas. However, S is usually relatively small for 
this reaction (most solids burn slowly in a visual sense), so we can use the boundary of the solid fuel 
as the reaction front. Since we do not currently model the pressure in solids, only equation 2 applies. 
We rewrite this equation as . f (Vf - D)= .s(Vs - D), (4) where .s and Vs are the density and the normal 
velocity of the solid fuel. Substituting D = Vs - S and solving for Vf gives Vf = Vs + .s/.f - 1 S (5) 
indicating that the gasi.ed solid fuel moves at the velocity of the solid fuel plus a correction that 
accounts for the expansion. We model this phase change by injecting gas out of the solid fuel at the 
appropriate velocity. This can be used to set arbitrary shaped solid objects on .re as long as they can 
be voxelized with a suitable surface normal assigned to each voxel indicating the direction of gaseous 
injection. In .gure 8, we simulate a camp.re using two cylindrically shaped logs as solid fuel injecting 
gas out of the logs in a direc­tion consistent with the local unit surface normal. Note the realistic 
rolling of the .re up from the base of the log. The ability to inject (or not inject) gaseous fuel out 
of individual voxels on the surface of a complex solid object allows us to animate objects catching on 
.re, burn different parts of an object at different rates or not at all (by us­ing spatially varying 
injection velocities), and extinguish solid fuels simply by turning off the injection velocity. While 
building an an­imation system that allows the user to hand paint temporally and spatially varying injection 
velocities on the surface of solid objects is beyond the scope of this paper, it is a promising subject 
for future work.  4 Implementation We use a uniform discretization of space into N3 voxels with uni­form 
spacing h. The implicit surface, temperature, density and pres­sure are de.ned at the voxel centers and 
are denoted fi, j,k, Ti, j,k, .i, j,k and pi, j,k where i, j,k = 1, ···, N. The velocities are de.ned 
at the cell faces and we use half-way index notation: ui+1/2, j,k where i = 0,···,N and j, k = 1, ···, 
N; vi, j+1/2,k where j = 0,···,N and i,k = 1,···,N; wi, j,k+1/2 where k = 0,···,N and i, j = 1,···,N. 
4.1 Level Set Equation We track our reaction zone (blue core) using the level set method of [Osher and 
Sethian 1988] to track the moving implicit surface. We de.ne f to be positive in the region of space 
.lled with fuel, negative elsewhere and zero at the reaction zone. The implicit surface moves with velocity 
w = u f + Sn where uf is the velocity of the gaseous fuel and the Sn term governs the Figure 8: Two 
burning logs are placed on the ground and used to emit fuel. The crossways log on top is not lit so the 
.ame is forced to .ow around it. conversion of fuel into gaseous products. The local unit normal, n = 
.f /|.f| is de.ned at the center of each voxel using central differencing to approximate the necessary 
derivatives, e.g. fx (fi+1, j,k - fi-1, j,k)/2h. Standard averaging of voxel face values is used to 
de.ne uf at the voxel centers, e.g. ui, j,k =(ui-1/2, j,k + ui+1/2, j,k)/2. The motion of the implicit 
surface is de.ned through ft = -w · .f (6) and solved at each grid point using f new = fold - .t w1fx 
+ w2fy + w3fz (7) and an upwind differencing approach to estimate the spatial deriva­tives. For example, 
if w1 > 0, fx (fi, j,k - fi-1, j,k)/h. Otherwise if w1 < 0, fx (fi+1, j,k - fi, j,k)/h. This simple 
approach is ef.­cient and produces visually appealing blue cores. To keep the implicit surface well conditioned, 
we occasionally adjust the values of f in order to keep f a signed distance function with |.f| = 1. First, 
interpolation is used to reset the values of f at voxels adjacent to the f = 0 isocontour (which we don 
t want to move since it is the visual location of the blue core). Then we march out from the zero isocontour 
adjusting the values of f at the other grid points as we cross them. [Tsitsiklis 1995] showed that this 
could be accomplished in an accurate, optimal and ef.cient manner solving quadratic equations and sorting 
points with a binary heap data structure. Later, [Sethian 1996] proposed the .nite difference formulation 
of this algorithm that we currently use. 4.2 Incompressible Flow We model the .ow of the gaseous fuel 
and the hot gaseous products using a separate set of incompressible Euler equations for each. In­compressibility 
is enforced through conservation of mass (or vol­ume), i.e. . · u = 0 where u =(u,v,w) is the velocity 
.eld. The equations for the velocity ut = -(u · .)u - .p/. + f (8) are solved for in two parts. First, 
we use this equation to compute an intermediate velocity u * ignoring the pressure term, and then we 
add the pressure (correction) term using u = u *- .t.p/.. (9) The key idea to this splitting method is 
illustrated by taking the divergence of equation 9 to obtain . · u = . · u *- .t. · (.p/.) (10) and then 
realizing that we want . ·u = 0 to enforce mass conserva­tion. Thus the left hand side of equation 10 
should vanish leaving a Poisson equation of the form . · (.p/.)= . · u * /.t (11) that can be solved 
to .nd the pressure needed for updating equation 9. We use a semi-Lagrangian stable .uids approach for 
.nding the * intermediate velocity u and refer the reader to [Stam 1999] and [Fedkiw et al. 2001] for 
the details. Since we use two sets of incom­pressible .ow equations, we need to address the stable .uid 
update when a characteristic traced back from one set of incompressible .ow equations crosses the implicit 
surface and queries the veloci­ties from the other set of incompressible .ow equations. Since the normal 
velocity is discontinuous across the interface, the straight­forward stable .uids approach fails to work. 
Instead, we need to use the balance equation 2 for conservation of mass to correctly interpolate a velocity. 
Suppose we are solving for the hot gaseous products and we in­terpolate across the interface into a region 
where a velocity from the gaseous fuel might incorrectly be used. Instead of using this value, we compute 
a ghost value as follows. First, we compute the normal velocity of the fuel, Vf = u f · n. Then we use 
the balance equation 2 to .nd a ghost value for VG as h VhG = Vf +. f /.h - 1S. (12) Since the tangential 
velocities are continuous across the implicit surface, we combine this new normal velocity with the existing 
tan­gential velocity to obtain G u= VhGn + u f - (u f · n)n (13) h as a ghost value for the velocity 
of the hot gaseous products in the region where only the fuel is de.ned. This ghost velocity can then 
be used to correctly carry out the stable .uids update. Since both n and u f are de.ned throughout the 
region occupied by the fuel, and . f , .h and S are known constants, a ghost cell value for the G hot 
gaseous products, uh , can be found anywhere in the fuel region (even quite far from the interface) by 
simply algebraically evaluat­ing the right hand side of equation 13. [Nguyen et al. 2001] showed that 
this ghost .uid method, invented in [Fedkiw et al. 1999], could be used to compute physically accurate 
engineering simulations of de.agrations. After computing the intermediate velocity u * for both sets 
of in­compressible .ow equations, we solve equation 11 for the pressure and .nally use equation 9 to 
.nd our new velocity .eld. Equation 11 is solved by assembling and solving a linear system of equations 
for the pressure as discussed in more detail in [Foster and Fedkiw 2001] and [Fedkiw et al. 2001]. Once 
again, we need to exercise caution here since the pressure is discontinuous across the inter­face. Using 
the ghost .uid method and equation 3, we can obtain and solve a slightly modi.ed linear system incorporating 
this jump in pressure. We refer the reader to [Nguyen et al. 2001] for ex­plicit details and a demonstration 
of the physical accuracy of this approach in the context of de.agration waves. The temperature affects 
the .uid velocity as hot gases tend to rise due to buoyancy. We use a simple model to account for these 
effects by de.ning external forces that are directly proportional to the temperature fbuoy = a (T - Tair)z, 
(14) where z =(0,0,1) points in the upward vertical direction, Tair is the ambient temperature of the 
air and a is positive constant with the appropriate units. Fire, smoke and air mixtures contain velocity 
.elds with large spatial deviations accompanied by a signi.cant amount of rota­tional and turbulent structure 
on a variety of scales. Nonphysical numerical dissipation damps out these interesting .ow features, so 
we aim to add them back on the coarse grid. We use the vorticity con.nement technique invented by Steinhoff 
(see e.g. [Steinhoff and Underhill 1994]) and used by [Fedkiw et al. 2001] to generate the swirling effects 
for smoke. The .rst step in generating the small scale detail is to identify the vorticity . = .×u as 
the source of this small scale structure. Each small piece of vorticity can be thought of as a paddle 
wheel trying to spin the .ow .eld in a particular di­rection. Normalized vorticity location vectors, 
N = .|.|/|.|.||simply point from lower concentrations of vorticity to higher con­centrations. Using these, 
the magnitude and direction of the vortic­ity con.nement (paddle wheel) force is computed as fcon f = 
eh(N × .), (15) where e > 0 and is used to control the amount of small scale detail added back into the 
.ow .eld. The dependence on h guarantees that as the mesh is re.ned the physically correct solution is 
still obtained. All these quantities can be evaluated in a straightforward fashion as outlined in [Fedkiw 
et al. 2001]. Usually a standard CFL time step restriction dictates that the time step Lt should be limited 
by Lt < h/|u|max where |u|max is the maximum velocity in the .ow .eld. While this is true for our level 
set equation 6 with u replaced by w, the combination of the semi-Lagrangian discretization and the ghost 
.uid method allows us to take a much larger time step for the incompressible .ow equations. We choose 
our incompressible .ow time step to be about .ve times bigger than that dictated by applying the CFL 
condition to the level set equation, and then stably update f using substeps. This reduces the number 
of times one needs to solve for the pressure, which is the most expensive part of the calculation, by 
a factor of .ve. 4.3 Temperature and Density The temperature pro.le has great effect on how we visually 
per­ceive .ames, and we need to generate a temperature time history for .uid elements that behaves as 
shown in .gure 2. Since this .g­ure depicts a time history of the temperature of .uid elements, we need 
a way to track individual .uid elements as they cross over the blue core and rise upward due to buoyancy. 
In particular, we need to know how much time has elapsed since a .uid element has passed through the 
blue core so that we can assign an appropriate temper­ature to it. This is easily accomplished using 
a reaction coordinate variable Y governed by the equation Yt = -(u · .)Y - k, (16) where k is a positive 
constant which we take to be 1 (larger or smaller values can be used to get a good numerical variation 
of Y in the .ame). Ignoring the convection term, Yt = -1 can be solved exactly to obtain Y (t)= -t +Y 
(0). If we set Y (0)= 1 in the region of space occupied by the gaseous fuel and solve equation 16 for 
Y , then the local value of 1 -Y is equal to the total time elapsed since a .uid element crossed over 
the blue reaction core. We solve equation 16 using the semi-Lagrangian stable .uids method to .rst update 
the convection term obtaining an intermedi­ate value Y *. Then we separately integrate the source term 
analyti­cally so it too is stable for large time steps, i.e. Y new = -k.t +Y * . We can now use the values 
of Y to assign temperature values to the .ow. Since Tignition is usually below the visual blackbody emission 
threshold, the temperature we set inside the blue core is usually not important. Therefore, we can set 
T = Tignition for the points inside the blue core. The region between the blue core and the maximum temperature 
in .gure 2 is important since it models the rise in temperature due to the progress of a complex chemical 
reaction (which we do not model for the sake of ef.ciency). Here the animator has a lot of freedom to 
sculpt temperature rise curves and adjust how the mapping corresponds to the local Y values. For example, 
one could use T = Tignition at Y = 1, T = Tmax at Y = .9 and use a linear temperature function for the 
in between values of Y . (.9,1). For large .ames, this temperature rise interval will be compressed too 
close to the blue core for our grid to resolve. In these instances we use the ghost .uid method to set 
T = Tmax for any characteristic that looks across the blue core into the gaseous fuel region. The blue 
core then spits out gas at the maximum temperature that immediately starts to cool off, i.e. there is 
no tem­perature rise region. In fact, we did not .nd it necessary to use the temperature rise region 
in our examples as we are interested in larger scale .ames, but this temperature rise region would be 
useful, for example, when modeling candle. The animator can also sculpt the temperature falloff region 
to the right of .gure 2. However, there is a physically correct, viable (i.e. computationally cheap) 
alternative. For the values of Y in the temperature falloff region, we simply solve 4 T - Tair Tt = 
-(u · .)T - cT(17)Tmax - Tair which is derived from conservation of energy. Similar to equation 16, we 
solve this equation by .rst using the semi-Lagrangian stable .uids method to solve for the convection 
term. Then we integrate the fourth power term analytically to cool down the .ame at a rate governed by 
the cooling constant cT . Similar to the temperature curve in .gure 2, the animator can sculpt a density 
curve for smoke and soot formation. The density should start low and increase as the reaction proceeds. 
In the tem­perature falloff region, the animator can switch from the density curve to a physically correct 
equation .t = -(u · .) . (18) that can (once again) be solved using the semi-Lagrangian stable .uids 
method. Again, we did not .nd it necessary to sculpt densities for our particular examples.  5 Rendering 
of Fire Fire is a participating medium. It is more complex than the types of participating media (e.g. 
smoke and fog) that are typically en­countered in computer graphics since .re emits light. The region 
that creates the light-energy typically has a complex shape, which makes it dif.cult to sample. Another 
complication with .re is that the .re is bright enough that our eyes adapt to its color. This chro­matic 
adaptation is important to account for when displaying .re on a monitor. See [Pattanaik et al. 1998; 
Durand and Dorsey 2000].In this section, we will .rst describe how we simulate the scattering of light 
within a .re-medium. Then, we will detail how to properly integrate the spectral distribution of power 
in the .re and account for chromatic adaptation. 5.1 Light Scattering in a Fire Medium Fire is a blackbody 
radiator and a participating medium. The prop­erties of a participating medium are described by the scattering, 
ab­sorption and emission properties. Speci.cally, we have the scatter­ing coef.cient, ss, the absorption 
coef.cient, sa, and the extinction coef.cient, st = sa + ss. These coef.cients specify the amount of 
scattering, absorption and extinction per unit-distance for a beam of light moving through the medium. 
The spherical distribution of the scattered light at a location is speci.ed by a phase-function, p. We 
use the Henyey-Greenstein phase-function [Henyey and Greenstein 1941] 1 - g2 p(.w· .w')= (19) 4p(1 + 
g2 - 2gw. · w.')1.5 . Here, g . [-1, 1] is the scattering anisotropy of the medium, g > 0 is forward 
scattering, g < 0 is backward scattering, while g = 0 is isotropic scattering. Note that the distribution 
of the scattered light only depends on the angle between the incoming direction, .w, and the outgoing 
direction, .w'. Light transport in participating media is described by an integro­differential equation, 
the radiative transport equation [Siegel and Howell 1981]: (.w· .)L. (x,w= .)+ .) -st (x)L. (x,w ss(x)p(.w,.w')L. 
(x,.w')d.w' + 4p sa(x)Le,. (x,.w). (20) Here, L. is the spectral radiance, and Le,. is the emitted spectral 
radiance. Note that ss, sa, and st vary throughout the medium and therefore depend on the position x. 
We solve Equation 20 to estimate the radiance distribution in the medium by using a stochastic adaptive 
ray marching algorithm which recursively samples multiple scattering. In highly scatter­ing media this 
approach is costly; however, we are concerned about .re which is a blackbody radiator (no scattering, 
only absorption) that creates a low-albedo smoke (the only scattering part of the .re­medium). This makes 
the Monte Carlo ray tracing approach practi­cal. To estimate the radiance along a ray traversing the 
medium, we split the ray into short segments. For a given segment, n, the scat­tering properties of the 
medium are assumed constant, and the radi­ance, Ln, at the start of the segment is computed as: Ln,. 
(x,.w)= e-st .xL(n-1),. (x + .x, .w)+ L. (x,w.')p(.w· .w')ss.x + saLe,. (x).x. (21) This equation is 
evaluated recursively to compute the total radiance at the origin of the ray. .x is the length of the 
segment, Ln-1 is the radiance at the beginning of the next segment, and w.' is a sample direction for 
a new ray that evaluates the indirect illumination in a given direction for the segment. We .nd the sample 
direction by importance sampling the Henyey-Greenstein phase function. Note that we do not explicitly 
sample the .re volume; instead we rely on the Monte Carlo sampling to pick up energy as sample rays hit 
the .re. This strategy is reasonably ef.cient in the presence of the low-albedo smoke generated by the 
.re.  Figure 9: A metal ball passes through and interacts with a gas .ame. The emitted radiance is normally 
ignored in graphics, but for .re it is an essential component. For a blackbody we can compute the emitted 
spectral radiance using Planck s formula: 2C1 Le,. (x)= , (22) . 5(eC2 /(.T ) - 1) where T is the temperature, 
C1 3.7418 · 10-16Wm2, and C2 1.4388 · 10-2moK [Siegel and Howell 1981]. In the next section, we will 
describe how we render .re taking this spectral distribution of emitted radiance into account.  5.2 
Reproducing the Color of Fire Accurately reproducing the colors of .re is critical for a realistic .re 
rendering. The full spectral distribution can be obtained directly by using Planck s formula for spectral 
radiance when performing the ray marching. This spectrum can then be converted to RGB before being displayed 
on a monitor. To get the right colors of .re out of this process it is necessary to take into account 
the fact that our eyes adapt to the spectrum of the .re. To compute the chromatic adaptation for .re, 
we use a von Kries transformation [Fairchild 1998]. We assume that the eye is adapted to the color of 
the spectrum for the maximum temperature present in the .re. We map the spectrum of this white point 
to the LMS cone responsivities (Lw, Mw, Sw). This enables us to map a spectrum to the monitor as follows. 
We .rst integrate the spectrum to .nd the raw XYZ tristimulus values (Xr, Yr, Zr). We then .nd the adapted 
XYZ tristimulus values (Xa, Ya, Za) as: ... ... Xa 1/Lw 00 Xr . Ya . = M-1 . 01/Mw 0 .M. Yr .. (23) Za 
0 01/Sw Zr Here, M maps the XYZ colors to LMS (consult [Fairchild 1998] for the details). Finally, we 
map the adapted XYZ tristimulus values to the monitor RGB space using the monitor white point. In our 
implementation, we integrate the spectrum of the black­body at the source (e.g. when emitted radiance 
is computed); we then map this spectrum to RGB before using it in the ray marcher. This is much faster 
than doing a full spectral participating media simulation, and we found that it is suf.ciently accurate, 
since we already assume that the .re is the dominating light source in the scene when doing the von Kries 
transformation.  6 Results Figure 1, rendered by proprietary software at ILM which is a re­search project 
not yet used in production, shows a frame from a simulation of a .amethrower. We used a domain that was 
8 me­ters long with 160 grid cells in the horizontal direction (h = .05). The .ame was injected at 30m/s 
out of a cylindrical pipe with di­ameter .4m. We used S = .1m/s, .f = 1kg/m3, .h = .01kg/m3, cT = 3000K/s 
and a = .15m/(Ks2). The vorticity con.nement pa­rameter was set to e = 16 for the gaseous fuel and to 
e = 60 for the hot gaseous products. The simulation cost was approximately 3 minutes per frame using 
a Pentium IV. Solid objects are treated by .rst tagging all the voxels inside the object as occupied. 
Then all the occupied voxel cell faces have their velocity set to that of the object. The temperature 
at the center of occupied voxels is set to the object s temperature and the (smoke) density is set to 
zero. Figure 9 shows a metal sphere as it passes through and interacts with a gas .re. Note the re.ection 
of the .re on the surface of the sphere. For more details on object interactions with liquids and gases 
see [Foster and Fedkiw 2001] and [Fedkiw et al. 2001]. Since we have high temperatures (i.e. .re) in 
our .ow .eld, we allow our objects to heat up if their temperature is lower than that of their surroundings. 
We use a simple conduction model where we increase the local temperature of an object depending on the 
surrounding air temperature and object temperature as well as the time step .t. Normally, the value of 
the implicit surface is set to a negative value of h at the center of all voxels occupied by ob­jects 
indicating that there is no available fuel. However, we can easily model ignition for objects we designate 
as .ammable. Once the temperature of a voxel inside an object increases above a pre­de.ned threshold 
indicating ignition, we change the value of the implicit surface in that voxel from -h to h indicating 
that it con­tains fuel. In addition, those voxel s faces have their velocities aug­mented above the object 
velocity by an increment in the direction normal to the object surface indicating that gaseous fuel is 
being injected according to the phase change addressed earlier for solid fuels. In .gure 10, we illustrate 
this technique with a spherical ball that heats up and subsequently catches on .re as it passes through 
the .ame. Both this .ammable ball and the metal ball were com­puted on a 120 × 120 × 120 grid at approximately 
5 minutes per frame. 7 Conclusion We have presented a physically based model for animating and ren­dering 
.re and .ames. We demonstrated that this model could be used to produce realistic looking turbulent .ames 
from both solid and gaseous fuels. We showed plausible interaction of our .re and smoke with objects, 
including ignition of objects by the .ames. 8 Acknowledgment Research supported in part by an ONR YIP 
and PECASE award N00014-01-1-0620, NSF DMS-0106694, NSF ACI-0121288, NSF IIS-0085864 and the DOE ASCI 
Academic Strategic Alliances Pro­gram (LLNL contract B341491). The authors would like to thank Willi 
Geiger, Philippe Rebours, Samir Hoon, Sebastian Marino and Industrial Light + Magic for rendering the 
.amethrower.  References BUKOWSKI, R., AND SEQUIN, C. 1997. Interactive Simulation of Fire in Virtual 
Building Environments. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, 
Annual Conference Series, ACM, 35 44. CHIBA, N., MURAOKA, K., TAKAHASHI, H., AND MIURA, M. 1994. Two 
dimensional Visual Simulation of Flames, Smoke and the Spread of Fire. The Journal of Visualization and 
Computer Animation 5, 37 53. DEVLIN, K., AND CHALMERS, A. 2001. Realistic visualisation of the pompeii 
frescoes. In AFRIGRAPH 2001, ACM SIGGRAPH, A. Chalmers and V. Lalioti, Eds., 43 47. DURAND, F., AND DORSEY,J.2000.InteractiveToneMapping.In 
Proceedings of Eleventh Eurographics Workshop on rendering, 219 230. FAIRCHILD, M. 1998. Color Appearance 
Models. Addison Wesley Longman, Inc. FEDKIW, R., ASLAM, T., MERRIMAN, B., AND OSHER, S. 1999. A Non-oscillatory 
Eulerian Approach to Interfaces in Multima­terial Flows (The Ghost Fluid Method). J. Comput. Phys. 152, 
457. FEDKIW, R., STAM, J., AND JENSEN, H. W. 2001. Visual Sim­ulation of Smoke. In Proceedings of SIGGRAPH 
2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, 
ACM, 15 22. FOSTER, N., AND FEDKIW, R. 2001. Practical Animation of Liq­ uids. In Proceedings of SIGGRAPH 
2001, ACM Press / ACMSIGGRAPH, Computer Graphics Proceedings, Annual Confer­ence Series, ACM, 23 30. 
FOSTER, N., AND METAXAS, D. 1997. Modeling the Motion of a Hot, Turbulent Gas. In Proceedings of SIGGRAPH 
1997, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, An­nual Conference Series, ACM, 181 188. 
GROSSHANDLER, W. 1995. RADCAL: A Narrow-band Model for Radiation Calculations in Combustion Environment. 
NIST Technical Note 1402 (April). HENYEY, L., AND GREENSTEIN, J. 1941. Diffuse radiation in the galaxy. 
Astrophysics Journal 93, 70 83. INAKAGE, M. 1989. A Simple Model of Flames. In Proceedings of Computer 
Graphics International 89, Springer-Verlag, 71 81. MARKSTEIN, G. H. 1964. Nonsteady Flame Propagation. 
Perga­mon, Oxford. MAZARAK, O., MARTINS, C., AND AMANATIDES, J. 1999. Ani­mating Exploding Objects. In 
Proceedings of Graphics Interface 1999, 211 218. MUSGRAVE, F. K. 1997. Great Balls of Fire. In SIGGRAPH 
97 Animation Sketches, Visual Procedings, ACM SIGGRAPH, 259 268. NEFF, M., AND FIUME, E. 1999. A Visual 
Model for Blast Waves and Fracture. In Proceedings of Graphics Interface 1999, 193 202. NGUYEN, D., FEDKIW, 
R., AND KANG, M. 2001. A Boundary Condition Capturing Method for Incompressible Flame Discon­tinuities. 
J. Comput. Phys. 172, 71 98. O BRIEN, J. F., AND HODGINS, J. K. 1999. Graphical Modeling and Animation 
of Brittle Fracture. In Proceedings of SIGGRAPH 1999, ACM Press / ACM SIGGRAPH, Computer Graphics Pro­ceedings, 
Annual Conference Series, ACM, 137 146. OSHER, S., AND SETHIAN, J. A. 1988. Fronts Propagating with Curvature 
Dependent Speed: Algorithms Based on Hamilton-Jacobi Formualtions. J. Comput. Phys. 79, 12. PATTANAIK, 
S., FERWERDA, J. A., FAICHILD, M. D., AND GREENBERG, D. P. 1998. A Multiscale Model of Adaptation and 
Spatial Vision for Realistic Image Display. In Proceedings of SIGGRAPH 1998, ACM Press / ACM SIGGRAPH, 
Computer Graphics Proceedings, Annual Conference Series, ACM, 287 298. PERRY, C., AND PICARD, R. 1994. 
Synthesizing Flames and their Spread. SIGGRAPH 94 Technical Sketches Notes (July). RUSHMEIER, H. E., 
HAMINS, A., AND CHOI, M. 1995. Volume Rendering of Pool Fire Data. IEEE Computer Graphics and Ap­plications 
15, 4, 62 67. RUSHMEIER, H. 1994. Rendering Participating Media: Problems and Solutions from Application 
Areas. In Proceedings of the 5th Eurographics Workshop on Rendering, 35 56. SETHIAN, J. 1996. A Fast 
Marching Level Set Method for Mono­tonically Advancing Fronts. Proc. Nat. Acad. Sci. 93, 1591 1595. SIEGEL, 
R., AND HOWELL, J. 1981. Thermal Radiation Heat Transfer. Hemisphere Publishing Corp., Washington, DC. 
STAM, J., AND FIUME, E. 1995. Depicting Fire and Other Gaseous Phenomena Using Diffusion Process. In 
Proceedings of SIGGRAPH 1995, ACM Press / ACM SIGGRAPH, Com­puter Graphics Proceedings, Annual Conference 
Series, ACM, 129 136. STAM, J. 1999. Stable Fluids. In SIGGRAPH 99 Conference Pro­ceedings, Annual Conference 
Series, ACM Press / ACM SIG-GRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 121 
128. STANIFORTH, A., AND COTE, J. 1991. Semi-Lagrangian Inte­gration Schemes for Atmospheric Models: 
A Review. Monthly Weather Review 119, 2206 2223. STEINHOFF, J., AND UNDERHILL, D. 1994. Modi.cation of 
the Euler Equations for Vorticity Con.nement : Application to the Computation of Interacting Vortex Rings. 
Physics of Fluids 6, 8, 2738 2744. TSITSIKLIS, J. 1995. Ef.cient Algorithms for Globally Optimal Trajectories. 
IEEE Transactions on Automatic Control 40, 1528 1538. TURNS, S. R. 1996. An Introduction to Combustion. 
McGraw-Hill, Inc. YNGVE, G. D., O BRIEN, J. F., AND HODGINS, J. K. 2000. An­imating Explosions. In Proceedings 
of SIGGRAPH 2000, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, An­nual Conference Series, 
ACM, 29 36. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566644</article_id>
		<sort_key>729</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Structural modeling of flames for a production environment]]></title>
		<page_from>729</page_from>
		<page_to>735</page_to>
		<doi_number>10.1145/566570.566644</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566644</url>
		<abstract>
			<par><![CDATA[In this paper we describe a system for animating flames. Stochastic models of flickering and buoyant diffusion provide realistic local appearance while physics-based wind fields and Kolmogorov noise add controllable motion and scale. Procedural mechanisms are developed for animating all aspects of flame behavior including moving sources, combustion spread, flickering, separation and merging, and interaction with stationary objects. At all stages in the process the emphasis is on total artistic and behavioral control while maintaining interactive animation rates. The final system is suitable for a high volume production pipeline.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation systems]]></kw>
			<kw><![CDATA[convection]]></kw>
			<kw><![CDATA[fire]]></kw>
			<kw><![CDATA[flames]]></kw>
			<kw><![CDATA[kolmogorov spectrum]]></kw>
			<kw><![CDATA[physically-based modeling]]></kw>
			<kw><![CDATA[wind fields]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14098546</person_id>
				<author_profile_id><![CDATA[81100259193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arnauld]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lamorlette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39028381</person_id>
				<author_profile_id><![CDATA[81100120776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foster]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>781006</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BEAUDOIN, P. AND PAQUET, S. 2001. Realistic and Controllable Fire Simulation. In Proceedings of Graphics Interface 2001, 159-166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BRODIE, K. and WOOD, J. 2001. Recent Advances in Volume Visualization, Computer Graphics Forum 20, 2, 125-148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHIBA, N., OHKAWA, S., MURAOKA, K., MIURA, M. 1994. Two-dimensional Visual Simulation of Flames, Smoke and the Spread of Fire, Journal of Visualization and Computer Animation 5, 1, 37-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DRYSDALE, D. 1998. An Introduction to Fire Dynamics (2nd Ed.), John Wiley and Sons, ISBN 0 471 97290 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258838</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N. AND METAXAS, D. 1997. Modeling the Motion of a Hot, Turbulent Gas, In Proceedings of ACM SIGGRAPH 1997, Annual Conference Series, ACM, 181-188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>244315</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N. AND METAXAS, D. 1996. Realistic Animation of Liquids, Graphical Models and Image Processing 58, 5, 471-483.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N. AND FEDKIW, R. 2001. Practical Animation of Liquids, In Proceedings of ACM SIGGRAPH 2001, Annual Conference Series, ACM, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FEDKIW, R., STAM, J., JENSEN, H. W. 2001. Visual Simulation of Smoke, In Proceedings of ACM SIGGRAPH 2001, Annual Conference Series, ACM, 15-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90972</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[INAKGE, M. 1990. A Simple Model of Flames. In Proceedings of Computer Graphics International 1990, Springer-Verlag, 71-81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>735072</ref_obj_id>
				<ref_obj_pid>647781</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[NISHITA, T. AND DOBASHI, Y. 2001. Modeling and Rendering of Various Natural Phenomena Consisting of Particles, In Proceedings of Computer Graphics International 2001, 149-156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883416</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MIYAZAKI, R., YOSHIDA, S., DOBASHI, Y., NISHITA, T. 2001. A Method for Modeling Clouds based on Atmospheric Fluid Dynamics, In Proceedings of the 9th Pacific Conference, 363-372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. AND NEYRET, F. 2001. Flow Noise, ACM SIGGRAPH Technical Sketches and Applications, 187.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[PERRY, C. AND PICARD, R. 1994. Synthesizing Flames and their Spreading, In Proceedings of 5th Eurographics Workshop on Animation and Simulation, 56-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357320</ref_obj_id>
				<ref_obj_pid>357318</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[REEVES, W. T. 1983. Particle Systems --- A Technique for Modeling a Class of Fuzzy Objects, ACM Transactions on Graphics 2, 2, 91-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[RUDOLF, M. AND RACZKOWSKI, J. 2000. Modeling the Motion of Dense Smoke in the Wind Field, Computer Graphics Forum 19, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618289</ref_obj_id>
				<ref_obj_pid>616037</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[RUSHMEIER, H., HAMINS, A., CHOI, M. Y. 1995. Volume Rendering of Pool Fire Data, IEEE Computer Graphics & Applications 15, 4, 62-67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[STAM, J. 1999. Stable Fluids, In Proceedings of ACM SIGGRAPH 1999, Annual Conference Series, ACM, 121-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218430</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[STAM, J. AND FIUME, E. 1995. Depicting Fire and Other Gaseous Phenomena Using Diffusion Processes, In Proceedings of ACM SIGGRAPH 1995, Annual Conference Series, ACM, 125-136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166163</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[STAM, J. AND FIUME, E. 1993. Turbulent Wind Fields for Gaseous Phenomena, In proceedings of ACM SIGGRAPH 1993, Annual Conference Series, ACM, 369-376.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344801</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[YNGVE, G., O'BRIEN, J., HODGINS, J. 2000. Animating Explosions, In Proceedings of ACM SIGGRAPH 2000, Annual Conference Series, ACM, 29-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826524</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[YOSHIDA, S. AND NISHITA, T. 2000. Modeling of Smoke Flow Taking Obstacles into Account, In Proceedings of 8th Pacific Conference on Computer Graphics and Applications, 135-145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Structural Modeling of Flames for a Production Environment Arnauld Lamorlette   PDI/DreamWorks ABSTRACT 
In this paper we describe a system for animating flames. Stochastic models of flickering and buoyant 
diffusion provide realistic local appearance while physics-based wind fields and Kolmogorov noise add 
controllable motion and scale. Procedural mechanisms are developed for animating all aspects of flame 
behavior including moving sources, combustion spread, flickering, separation and merging, and interaction 
with stationary objects. At all stages in the process the emphasis is on total artistic and behavioral 
control while maintaining interactive animation rates. The final system is suitable for a high volume 
production pipeline. CR Categories: I.3.7 (Computer Graphics): Three-Dimensional Graphics and Realism 
 Animation; I.3.5 (Computer Graphics): Computational Geometry and Object Modeling Physically Based Modeling. 
Keywords: animation systems, fire, flames, convection, physically­based modeling, wind fields, Kolmogorov 
spectrum. 1. INTRODUCTION Numerical simulations of natural phenomena are now routinely used for animation 
or special effects projects. They are coming of age for computer graphics in the sense that numerical 
components have been added to the base physical models for the sole purpose of controlling and stylizing 
simulations for visual effect. It is no longer enough that numerical techniques are massaged to make 
such methods efficient for animation. They should also incorporate techniques for controlling both the 
look and behavior of the phenomena. Recently, liquids and gases in particular have proved amenable to 
this approach [Foster and Fedkiw 2001; Fedkiw et al. 2001]. That is, efficient numerical methods have 
been combined with physics-based control methods to create general animation systems. It would seem sensible 
then, to apply this same approach to the modeling and animation of fire. Experience suggests however, 
that there exists a subtle difference in the requirements of fire animation over phenomena such as fluids. 
The latter are often required as environment elements. They need to interact with characters and objects 
in a believable way. Rarely are these elements the main actor in a scene. When they are, (the Abyss and 
The Perfect Storm spring to mind) the physics rules are either obeyed closely, Copyright &#38;#169; 2002 
by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use is granted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
Nick Foster PDI/DreamWorks  or can clearly be relaxed because the fluid is already acting atypically. 
Fire on the other hand, is a dramatic element that requires the maximum level of control possible while 
maintaining a believable appearance. We expect fire to look complex and unpredictable, while at the same 
time having a recognizable structure according to the conditions under which it is burning. That complexity 
by itself makes direct numerical simulation of fire much less attractive than for other phenomena for 
three reasons: Numerical simulations scale poorly. As the resolution of a 3D simulation increases, computational 
complexity increases by at least O(n3) [Foster and Metaxas 1996; Stam and Fiume 1995]. The resolution 
required to capture the detail in even a relatively small fire makes simulation expensive.  The number 
of factors that affect the appearance of fire under different circumstances leads to an inter-dependent 
simulation parameter space. It s difficult for developers to place intuitive control functions on top 
of the underlying physical system.  Fire is chaotic. Small changes in initial conditions cause radically 
different results. From an animation standpoint it s difficult to iterate towards a desired visual result. 
 Together, these restrictions make numerical simulation a poor choice as a basis for a large-scale fire 
animation system, where control and efficiency are as equally important as realism. This paper presents 
a different approach to modeling fire. Instead of modeling the physics of combustion, we achieve the 
trade-off between realism, control, and efficiency by recognizing that many of the visual cues that define 
fire phenomena are statistical in nature. We then separate these vital cues from other components that 
we wish to artistically control. This provides us with a number of distinct structural elements. Some 
of these elements are physics­based, but in general they need not be. Each structural element has a number 
of animation parameters associated with it, allowing us to mimic a variety of both real-world and production-world 
fire conditions. Realism is achieved by basing structural state changes on the measured statistical properties 
of real flames. In addition, we define a set of large-scale procedural models that define locally how 
a group of flame structures evolve over time. These include physics-based effects such as fuel combustion, 
diffusion, or convection, as well as pure control mechanisms such as animator­defined time and space 
curves. Together, the structural flame elements and procedural controls form the basis of a system for 
modeling and animating a wide range of believable fire effects extremely efficiently in the context of 
a direction driven animation pipeline. 2. PREVIOUS WORK Previous work on modeling dynamic flames falls 
loosely into two categories: direct numerical simulation and visual modeling. Direct simulations have 
achieved realistic visual results when modeling the rotational motion due to the heat plume around a 
fire [Chiba et al. 1994; Inakge 1990]. Simulation has also proved efficient at modeling the behavior 
of smoke given off by a fire [Rushmeier et al. 1995; Stam and Fiume 1993], and the heat plume from an 
explosion [Yngve et al. 2000]. For the shape and motion of flames however, simulation has not been so 
useful. Numerical models from Computational Fluid Dynamics have not proven amenable to simplification 
without significant loss of detail [Drysdale 1998]. Without that simplification they are an expensive 
option for large­scale animations. Visual modeling has focused on efficiency and control. Particle systems 
are the most widely used fire model [Nishita and Dobashi 2001; Stam and Fiume 1995]. Particles can interact 
with other primitives, are easy to render, and scale linearly (if there are no inter-particle forces). 
The problem of realism falls solely on the shoulders of the animator though. Force fields and procedural 
noise can achieve adequate looking large-scale effects due to convection, but it is very difficult to 
come up with a particle-based model that accurately captures the spatial coherence of real fire. Flame 
coherence has been modeled directly using chains of connected particles [Beaudoin and Paquet 2001; Reeves 
1983]. This retains many of the advantages of particle systems while also allowing animators to treat 
a flame as a high level structural element. 3. ESSENTIAL MODEL The system we have developed to use as 
a general fire animation tool has eight stages. The key to its effectiveness is that many stages can 
be either directly controlled by an animator, or driven by a physics-based model. The components are 
as follows: Individual flame elements are modeled as parametric space curves. Each curve interpolates 
a set of points that define the spine of the flame.  The curves evolve over time according to a combination 
of physics-based, procedural, and hand-defined wind fields. Physical properties are based on statistical 
measurements of natural diffusion flames. The curves are frequently re-sampled to ensure continuity, 
and to provide mechanisms to model flames generated from a moving source.   The curves can break, generating 
independently evolving flames with a limited lifespan. Engineering observations provide heuristics for 
both processes.  A cylindrical profile is used to build an implicit surface representing the oxidization 
region, i.e. the visible part of the flame. Particles are point sampled close to this region using a 
volumetric falloff function.  Procedural noise is applied to the particles in the parameter space of 
the profile. This noise is animated to follow thermal buoyancy.  The particles are transformed into 
the parametric space of the flame s structural curve. A second level of noise, with a Kolmogorov frequency 
spectrum, provides turbulent detail.  The particles are rendered using either a volumetric, or a fast 
painterly method. The color of each particle is adjusted according to color properties of its neighbors, 
allowing flame elements to visually merge in a realistic way.  To complete the system, we define a number 
of procedural controls to govern placement, intensity, lifespan, and evolution in shape, color, size, 
and behavior of the flames.  The result is a general system for efficiently animating a variety of natural 
fire effects. The cost of direct simulation is only incurred when it is desired. Otherwise, the system 
provides complete control over large-scale behavior. 4. STRUCTURAL ELEMENTS The fire animation system 
is built up from single flames modeled on a natural diffusion flame (see Figure 2). Observed statistical 
properties of real flames are used wherever possible to increase the realism of the model. 4.1 Base Curve 
The basic structural element of the flame system is an interpolating B-Spline curve. Each curve represents 
the central spine of a single flame. The flames themselves can merge or split (see Section 4.3) but all 
of the fire phenomena we model are built from these primitives. In the first frame of animation for which 
a particular flame is active, a particle is generated at a fixed point on the burning surface and released 
into a wind field. The particle is advanced in the wind field for a frame using an explicit Euler integration 
method (Runge-Kutta is sufficient and completely stable in this case), and a new particle is generated 
at the surface. The line between these two points is sampled so that n control points are evenly distributed 
along it. For each additional frame of animation, they convect freely within the wind field. After convection, 
an interpolating B-Spline is fitted so that it passes through all the points. This curve is then parametrically 
re­sampled, generating a new set of n points (always keeping the first and last unchanged). This re-sampling 
ensures that whatever the value of n, visual artifacts don't appear in the structure if control points 
cluster together, or as a side effect of large time steps.  4.2 Flame Evolution During the animation, 
the control points for the structural curve evolve within a wind field. This evolution is for providing 
global shape and behavior. Specific local detail due to fuel consumption and turbulence at the combustion 
interface is added later. The motion is built from four main components: convection, diffusion, initial 
motion, and buoyancy. Initial motion is applied only at the very base of the curve. If the fire-generating 
surface has a velocity V, the particle, P0, is given an initial velocity, -V. The flame is then generated 
in a stationary reference frame. Because flames move with the participating media (i.e., with no specific 
inertia), this provides an efficient way of taking account of moving sources. Buoyancy is due to the 
tendency for hotter (less dense) air to rise. The region around the visible part of the flame is relatively 
homogeneous in terms of temperature (at least with respect to the ambient temperature). We chose to model 
buoyancy as a direct linear upward force. The resulting flow is laminar in nature, so we add rotational 
components in the form of simulated wind fields and a noise field generated from a Kolmogorov spectrum. 
The equation of motion for a structural particle, p, is defined by p w x ,t T +V T ,t dx =( )()+d +c 
() (1) p ppp dt where, w(xp,t), is an arbitrary controlling wind field, d(Tp), the motion due to direct 
diffusion, Vp, motion due to movement of the source and, c(Tp, t), the motion due to thermal buoyancy. 
Tp is the temperature of the particle. Diffusion is modeled as random Brownian motion scaled by the temperature 
Tp. Thermal buoyancy is assumed to be constant over the lifetime of the particle, therefore 2 cT,t()=-ßg 
(T -T )t (2) p y0 pp where ßis the coefficient of thermal expansion, gy is the vertical component of 
gravity, T0 is the ambient temperature, and tp is the age of the particle. Equations (1) and (2) allow 
us to combine accurate simulated velocity fields with ad-hoc control fields without producing visual 
discontinuities. 4.3 Separation and Flickering We model flame separation and flickering as a statistical 
process. The classic regions of a free-standing diffusion flame are illustrated in Figure 2. The intermittent 
region of the flame is defined over the range [Hp, Hi]. From its creation a flame develops until it reaches 
Figure 3: Different normalized flame profiles for a candle flame, torch flame, and camp fire flames 
respectively. Hi. At that point we periodically test a random number against the probability function, 
2 2 . V . . . c 2H -H 2 -- .((ip ) ). 1h . h f . . . .. .dh (3) 2p(Hi -Hp ) 2 -8 ()= e Dh to determine 
whether the flame will flicker or separate at a length h. The frequency f, is the approximate breakaway 
rate in Hz, while Vc is the average velocity of the structural control points. From observation [Drysdale 
1998], f = (0.50 ±0.04)(gy/2r)1/2 for circular sources with a radius r. If necessary, D(h) =1 for h > 
Hmax, where Hmax is some artistically chosen limit on flame length. Once separation occurs, a region 
of the structural curve is split off from the top of the flame. This region extends from the top of the 
flame to a randomly selected point below. We could find no measured data that describes the size distribution 
of separated flames. In that absence, this point is selected using a normal distribution function with 
a mean of Hp+(Hi-Hp)/2 and standard deviation of (Hi-Hp)/4. The control points representing the buoyant 
region are fitted with a curve, and re-sampled in the same way as for the parent structure (Section 4.1). 
The number of points split off is not increased back to n in this case however. This prevents additional 
detail appearing in the buoyancy region of the flame where no additional fuel could be added. During 
its lifetime, the structural particles that make up the separated flame follow Eq (1) as before. Without 
modeling the combustion process there s no accurate way to determine the fuel content of the breakaway 
flame. Therefore there s no good model for how long it will remain visible. Each separate flame is given 
a life-span of Ai3, where i is a uniform random variable in the range [0,1], and A is a length scale 
ranging from 1/24th second for small flames up to 2 seconds for a large pool fire. The cube ensures that 
most breakaway flames are short lived. There's no reason why the buoyant flame can't separate again, 
although in general the entrainment region quickly dominates as there's little airborne fuel in a buoyant 
flame except for oxygen starved fires. 4.4 Flame Profiles With the global structure and behavior of 
a flame defined, we now concentrate on the visible shape of the flame itself. A flame is a combustion 
region between the fuel source and an oxidizing agent. The region itself is not clearly defined, so we 
require a model that is essentially volumetric. One such model treats each segment (the line between 
two adjacent control points) of the structural curve as a source for a potential field [Beaudoin and 
Paquet 2001]. The total field for a flame is the summation of the fields for each of its segments. The 
flame is rendered volumetrically using different color gradients for various iso-contours of the potential 
field. This method gives good results for small fires like candle flames, and handles flame merging efficiently. 
Stylization is difficult however, requiring contraction functions for each desired flame shape. In addition, 
turbulent noise has to be built directly into the potential function as there is no subsequent transformation 
stage before rendering. To retain complete control over basic shape, we represent a flame using a rotationally 
symmetric surface based on a simple two­dimensional profile. The profile is taken from a standard library 
and depends on the scale of flame effect that we wish to model (see Figure 3 for examples). We have had 
good results from hand drawn profiles, as well as those derived from photographs. We then define the 
light density of the visible part of the flame as I d( ) = (4) x x 2 -x 2 1 + f () x where xf(x) is the 
closest point to x on the parametric surface defined by rotating the profile, and I is the density of 
combustion at the surface (normalized to one for this work). Equation (4) defines a simple volumetric 
density function for our flame. In order to transform and displace this starting shape into an organic 
and realistic looking flame, we need to transform Eq (4) onto the flame s structural curve. The density 
function is first point sampled volumetrically using a Monte Carlo method. The point samples do not survive 
from frame to frame. The sampling allows us to deform the density function without having to integrate 
it. It is important though, that the rendering method employed (see Section 5) be independent of sample 
density, so that the flame does not appear pointillistic. Once the density function has been approximated 
as particles, we displace and transform them to simulate the chaotic process of flame formation. 4.5 
Local Detail Two levels of structural fluctuation are applied to the point samples that define the visible 
part of the flame. Both animate in time and space. The first directly affects overall shape by displacing 
particles from their original sample positions, while the second simulates air turbulence. We define 
buoyancy noise, which represents the combustion fluctuation at the base of the flame. It propagates up 
the flame profile according to the velocities of the nearest structural particles (Section 4.1). There 
is no real physical data to go on here, but a noise function that visually looks good for this is Flow 
Noise [Perlin and Neyret 2001]. The rate of rotation of the linear noise vectors over time is inversely 
proportional to the diameter of the flame source, i.e., large flames lead to quicker vector rotation. 
The participating medium also causes the flame to distort over time. At this scale, a vector field created 
using a Kolmogorov spectrum exhibits small-scale turbulence and provides visual realism. Kolmogorov noise 
is relatively cheap to calculate and can be generated on a per-frame basis (see [Stam and Fiume 1993] 
for some applications of the Kolmogorov spectrum). Each sample particle is therefore: Displaced away 
from its initial position according to the Flow Noise value that has propagated up the profile. Figure 
4: A cylindrical coordinate system is used to map point samples from the profile space to the space of 
the deformed structural curve. Transformed into structural curve space according to the straightforward 
mapping shown in Figure 4.  Displaced a second time using a vector field generated from a Kolmogorov 
spectrum.  After these transformations, the particles are tested against the transformed profiles of 
their parent flame s neighbors. If a particle is inside a neighboring flame and outside the region defined 
by Eq (4), then that particle is not rendered at all. This gives the appearance that individual flames 
can merge. The Kolmogorov field and global wind field w(xp,t) ensure that merged flames behave in a locally 
similar fashion even though there is nothing in the explicit model to account for merging. The particles 
are then in their final positions, ready for rendering. 5. RENDERING 5.1 Particle Color Apparent flame 
color depends on the types of fuel and oxidizer being consumed together with the temperature of the combustion 
zone (hotter regions are bluer). Instead of trying to calculate the color of the flames that we want 
to model, we find a reference photograph and map the picture onto the two-dimensional profile used for 
flame shape (Figure 3). Particles take their base color directly from this mapping. Obviously, any image 
could be used, giving complete control over the color of the flame. This mapping does not determine the 
intensity of light from the particle, just its base color. 5.2 Incandescence The flames transmit energy 
towards the camera and into the environment. We assume that the visible light transmitted is proportional 
to the heat energy given out by the flame. This is approximated by 1 . 2Af r3 . E =k 0.3m H (5) . . c 
. 2 . h . where m = |vc|r2 is the upward mass flow, .Hc the heat of combustion of the fuel (e.g., 15 
MJkg-1 for wood, 48 MJkg-1 for gasoline), Af is the surface area of the fuel (M2), r is the radius, and 
k is a scale factor for visual control. Each particle then has its incandescence at the camera calculated 
relative to d(xi)Ei = 2 2 (6) n4 (()+l ph2 ) where l is the distance from the center of the flame to 
the camera, and n is the total number of particle samples. The k, in Eq (5) can be automatically adjusted 
to compensate for the energy loss due to the d(xi) term in Eq (6). Equation (5) also approximates the 
total energy given out to the environment. For global illumination purposes, an emitting sphere at the 
center of each flame segment with a radius of h/3, gives reasonable looking lighting. 5.3 Image Creation 
Ideally, the deformed flames should be volume rendered directly. If the potential field approach is used, 
relatively fast methods are available to do this [Brodie and Wood 2001; Rushmeier et al. 1995]. For the 
system described however, volume rendering requires density samples to be projected through two inverse 
transformations to hit the density function, Eq (4). This is elegant, but limits us to using noise functions 
that can be integrated quickly. Instead, we note that due to the coherence in the global and local noise 
fields, sample points that are close together will tend to remain close together under transformation. 
Therefore, if we sample the untransformed profile with sufficient density to eliminate aliasing, then 
the transformed sample points should also not exhibit aliasing. The transforms themselves are inexpensive 
per particle and so total particle count isn t a huge factor in terms of efficiency. We calculate the 
approximate cross-sectional area of the flame as it would appear from the camera, then super-sample the 
profile with sufficient density for complete coverage. The images in this paper were created using a 
sampling rate of around ten particles per pixel. This gives a range of between around a thousand and 
fifty thousand particles per flame. The intensity of each particle is calculated using Eq (6) so the 
total energy of the flame remains constant. The opacity associated with each particle is more difficult 
to calculate however. Real flames are highly transparent. They are so bright in relation to their surrounding 
objects that only extremely bright objects are visible through them. Cinema screens and video monitors 
cannot achieve contrast levels high enough to give that effect unless the background is relatively dark. 
To our knowledge there s no engineering (or photographic) model of apparent opacity in incandescent fluids. 
So, with no justification other than observation we calculate the opacity of a particle as being proportional 
to the relative brightness between it and what is behind it. This is by no means a physically correct 
approach but it appears to work well in practice. The particles are then motion blurred using the instantaneous 
velocities given by Eq (1).  6. PROCEDURAL CONTROL Separating the fire system into a series of independent 
components was driven by the desire to have complete control over the look and behavior of flame animations. 
Animators have direct control over the basic shape and color of the flames (Sections 4.4 and 5.1), as 
well as the scale, flickering, and separation behavior (Section 4.3). In addition, global motion is dictated 
by the summation of different kinds of wind field. Wind fields, whether procedural [Rudolf and Raczkowski 
2000; Yoshida and Nishita 2000] or simulated [Miyazaki et al. 2001] are well understood and effective 
as a control mechanism. The open nature of the system easily allows for many other procedural controls, 
three examples of which follow. 6.1 Object Interaction Interaction between the structural flame elements 
and a stationary object requires a wind field that flows around a representation of that object. We approximate 
the object as a series of solid voxels and simulate the flow of hot gas around it using the method based 
on the Navier-Stokes equations described in [Foster and Metaxas 1997]. This wind field is used in Eq 
(1) to convect the structural control points. The only difference is that after their positions are modified 
they are tested against the object volume and moved outside if there s a conflict. Any ambiguity in the 
direction the point should be moved is resolved using the adjacency of the points themselves. The sample 
particles used for rendering are more difficult to deal with in a realistic way. Generally, the wind 
field naturally carries the flame away from the object, but in cases where the transformed flame profile 
remains partly inside we simply make a depth test against the object during rendering and ignore the 
internal particles. This makes the energy (Eq (6)) of flame segments inaccurate close to an object. However, 
the object itself is usually brightly lit by nearby flames and this is not noticeable. Figure 7 shows 
a rendered sequence of images involving interaction between flames and simple objects. The wind field 
environment had a resolution of 40x40x40 cells, which proved to be sufficient to achieve flow that avoids 
the objects in a believable way. 6.2 Flame Spread Many factors influence how flames spread over an object, 
or jump between objects. Spread rate models rapidly become complex to take into account fuel and oxidizer 
concentrations, properties of the combusting material, atmospheric conditions, angle of attack and so 
on. For simplified behavior, we use the procedural model from [Perry and Picard 1994]. The velocity of 
the flame front can be described by, Tf v s = L (. + hsin f) (7) where f is the relative orientation 
between the flame and the unburned surface, h is the height of the flame, Tf is the temperature of the 
flame, L is the thermal thickness of the burning material, and . is the thermal conductivity of air. 
Values of . and L for different materials are available from tables but it is straightforward to adjust 
them to get a desired speed. For complete animation control, we use a gray-scale image to determine precisely 
when flames become active on a surface. The image is mapped onto the surface of the burning object, and 
the value of the image samples correspond directly with flame activation (or spreading) times. Similar 
maps control both the length of time each flame burns as well as its overall intensity, diameter and 
height. 6.3 Smoke Generation Two measures define the capability of a flame to produce smoke. The first 
is the smoke point . This is the minimum laminar flame height at which smoke first escapes from the flame 
tip. The second is smoke yield . This is a measure of the volume of smoke produced and it correlates 
closely with the radiation emitted from a diffusion flame. We procedurally generate smoke as particles 
above the tip of each flame. The smoke point above the tip is given simply by h, and the density of particles 
(or density associated with each individual particle) is the radiation intensity (Eq (5)) multiplied 
by an arbitrary scale factor. Once generated, the smoke is introduced into a gas simulator [Foster and 
Metaxas 1997] with an initial velocity of Vpn-1 (from the upper control point) and a temperature set 
to maintain the upward buoyancy force specified by Eq (2).  7. RESULTS All the images shown in this 
paper were generated using the system described. Figure 1 shows a sequence of four images from an animation 
of a lit torch being swung through the air. The flames are generated as if the torch is standing still, 
but with an initial base velocity as described in Section 4.2. The control points are influenced by thermal 
buoyancy and Kolmogorov noise. There are 5 flame structures, each rendered using 9000 sample particles. 
The animation as a whole took 2.7 seconds per frame on a Pentium III 700MHz processor. That broke down 
as 0.2 seconds for the dynamic simulation, 0.5 seconds for rendering and 2.0 seconds for B-Spline fitting 
and particle sorting. A more stylized example is the dragon breath animation shown in Figure 5. Here, 
flames are generated at the dragon s mouth with an initial velocity and long lifespan. These flames split 
multiple times using the technique outlined in Section 4.3. Again, five flames are used initially, generating 
over eighty freely moving structures by the end of the sequence. There are a total of 1.5 million sample 
particles distributed according to the size of the flames relative to the camera. Dynamic simulation 
took 3 seconds, rendering 30 seconds, and B-Spline fitting 1 minute per frame. The possibility of individual 
flames merging (Section 4.5) was disabled for artistic preference. A pre-calculated wind field based 
on random forces gives some good internal rotation to the fireball but overall we feel there s still 
a low level of turbulence missing from the animation. Results could be improved by tracking the sample 
particles over time through the Kolmogorov noise, although this would incur more cost during rendering 
to prevent undersampling. Figures 6 and 7 show a selection of images from the large format version of 
the movie Shrek and interaction between flames and stationary objects respectively. The animation system 
was used for all the fire required by that production. Each component of the system has between three 
and six parameters all related to visual behavior (flicker rate and average lifespan for example). While 
the system actually has more parameters than a corresponding direct numerical simulation, they are fairly 
intuitive, and mutually independent. In our experience, aside from learning the simulation tools used 
for wind field creation, an animator can be productive with the system within a week of first using it. 
 8. CONCLUSION We have presented a system for modeling diffusion flames. The main focus has been on efficiency, 
fast animation turnaround, and complete control over visual appearance and behavior. With these goals 
in mind, the system has been built to take advantage of recent advances in direct simulation, as well 
as proven techniques for generating and controlling wind fields. These methods are combined with a novel 
approach for representing the structure of a flame and a procedural animation methodology, to produce 
a comprehensive animation tool for high volume throughput that can dial between realistic and stylistic 
results.  9. REFERENCES BEAUDOIN, P. AND PAQUET, S. 2001. Realistic and Controllable Fire Simulation. 
In Proceedings of Graphics Interface 2001, 159-166. BRODIE, K. and WOOD, J. 2001. Recent Advances in 
Volume Visualization, Computer Graphics Forum 20 , 2, 125-148. CHIBA, N., OHKAWA, S., MURAOKA, K., MIURA, 
M. 1994. Two­dimensional Visual Simulation of Flames, Smoke and the Spread of Fire, Journal of Visualization 
and Computer Animation 5, 1, 37-54. DRYSDALE, D. 1998. An Introduction to Fire Dynamics (2nd Ed.), John 
Wiley and Sons, ISBN 0 471 97290 8. FOSTER, N. AND METAXAS, D. 1997. Modeling the Motion of a Hot, Turbulent 
Gas, In Proceedings of ACM SIGGRAPH 1997, Annual Conference Series, ACM, 181-188. FOSTER, N. AND METAXAS, 
D. 1996. Realistic Animation of Liquids, Graphical Models and Image Processing 58, 5, 471-483. FOSTER, 
N. AND FEDKIW, R. 2001. Practical Animation of Liquids, In Proceedings of ACM SIGGRAPH 2001, Annual Conference 
Series, ACM, 23-30. FEDKIW, R., STAM, J., JENSEN, H. W. 2001. Visual Simulation of Smoke, In Proceedings 
of ACM SIGGRAPH 2001, Annual Conference Series, ACM, 15-22. INAKGE, M. 1990. A Simple Model of Flames. 
In Proceedings of Computer Graphics International 1990, Springer-Verlag, 71-81. NISHITA, T. AND DOBASHI, 
Y. 2001. Modeling and Rendering of Various Natural Phenomena Consisting of Particles, In Proceedings 
of Computer Graphics International 2001, 149-156. MIYAZAKI, R., YOSHIDA, S., DOBASHI, Y., NISHITA, T. 
2001. A Method for Modeling Clouds based on Atmospheric Fluid Dynamics, In Proceedings of the 9th Pacific 
Conference, 363-372. PERLIN, K. AND NEYRET, F. 2001. Flow Noise, ACM SIGGRAPH Technical Sketches and 
Applications, 187. PERRY, C. AND PICARD, R. 1994. Synthesizing Flames and their Spreading, In Proceedings 
of 5th Eurographics Workshop on Animation and Simulation, 56-66. REEVES, W. T. 1983. Particle Systems 
 A Technique for Modeling a Class of Fuzzy Objects, ACM Transactions on Graphics 2, 2, 91-108. RUDOLF, 
M. AND RACZKOWSKI, J. 2000. Modeling the Motion of Dense Smoke in the Wind Field, Computer Graphics Forum 
19, 3. RUSHMEIER, H., HAMINS, A., CHOI, M. Y. 1995. Volume Rendering of Pool Fire Data, IEEE Computer 
Graphics &#38; Applications 15, 4, 62-67. STAM, J. 1999. Stable Fluids, In Proceedings of ACM SIGGRAPH 
1999, Annual Conference Series, ACM, 121-128. STAM, J. AND FIUME, E. 1995. Depicting Fire and Other Gaseous 
Phenomena Using Diffusion Processes, In Proceedings of ACM SIGGRAPH 1995, Annual Conference Series, ACM, 
125-136. STAM, J. AND FIUME, E. 1993. Turbulent Wind Fields for Gaseous Phenomena, In proceedings of 
ACM SIGGRAPH 1993, Annual Conference Series, ACM, 369-376. YNGVE, G., O BRIEN, J., HODGINS, J. 2000. 
Animating Explosions, In Proceedings of ACM SIGGRAPH 2000, Annual Conference Series, ACM, 29-36. YOSHIDA, 
S. AND NISHITA, T. 2000. Modeling of Smoke Flow Taking 8th Obstacles into Account, In Proceedings of 
Pacific Conference on Computer Graphics and Applications, 135-145.  Figure 5: A sequence of images showing 
fully rendered stylized dragon s breath and the structural curves used to model it. Figure 6: The fire 
system in use on a 3D animated feature film.  Figure 7: Interaction between flames and simple stationary 
objects. A wind field simulation resolution of 40x40x40 was used. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566645</article_id>
		<sort_key>736</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Animation and rendering of complex water surfaces]]></title>
		<page_from>736</page_from>
		<page_to>744</page_to>
		<doi_number>10.1145/566570.566645</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566645</url>
		<abstract>
			<par><![CDATA[We present a new method for the animation and rendering of <i>photo-realistic</i> water effects. Our method is designed to produce visually plausible three dimensional effects, for example the pouring of water into a glass (see figure 1) and the breaking of an ocean wave, in a manner which can be used in a computer animation environment. In order to better obtain photorealism in the behavior of the simulated water surface, we introduce a new "thickened" front tracking technique to accurately represent the water surface and a new velocity extrapolation method to move the surface in a smooth, water-like manner. The velocity extrapolation method allows us to provide a degree of control to the surface motion, e.g. to generate a windblown look or to force the water to settle quickly. To ensure that the photorealism of the simulation carries over to the final images, we have integrated our method with an advanced physically based rendering system.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational fluid dynamics]]></kw>
			<kw><![CDATA[implicit surfaces]]></kw>
			<kw><![CDATA[natural phenomena]]></kw>
			<kw><![CDATA[physically based animation]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382429</person_id>
				<author_profile_id><![CDATA[81100622458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Enright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University and Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382467</person_id>
				<author_profile_id><![CDATA[82158642957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marschner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14211105</person_id>
				<author_profile_id><![CDATA[81100612327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fedkiw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University and Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>307368</ref_obj_id>
				<ref_obj_pid>307353</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADALSTEINSSON, D., AND SETHIAN, J. 1999. The fast construction of extension velocities in level set methods. J. Comp. Phys. 148, 2-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>203303</ref_obj_id>
				<ref_obj_pid>203297</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHEN, J., AND LOBO, N. 1994. Toward interactive-rate simulation of fluids with moving obstacles using the navier-stokes equations. Computer Graphics and Image Processing 57, 107-116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>201614</ref_obj_id>
				<ref_obj_pid>201609</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHEN, S., JOHNSON, D., AND RAAD, P. 1995. Velocity boundary conditions for the simulation of free surface fluid flow. J. Comp. Phys. 116, 262-276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CHEN, S., JOHNSON, D., RAAD, P., AND FADDA, D. 1997. The surface marker and micro cell method. Int. J. for Num. Meth. in Fluids 25, 749-778.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641285</ref_obj_id>
				<ref_obj_pid>641282</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[ENRIGHT, D., FEDKIW, R., FERZIGER, J., AND MITCHELL, I. 2002. A hybrid particle level set method for improved interface capturing. J. Comp. Phys. To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND FEDKIW, R. 2001. Practical animation of liquids. In Proceedings of SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>244315</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND METAXAS, D. 1996. Realistic animation of liquids. Graphical Models and Image Processing 58, 471-483.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15894</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A., AND REEVES, W. T. 1986. A simple model of ocean waves. In Computer Graphics (Proceedings of SIGGRAPH 86), 20(4), ACM, 75-84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HARLOW, F., AND WELCH, J. 1965. Numerical calculation of time-dependent viscous incompressible flow of fluid with free surface. Phys. Fluids 8, 2182-2189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S. 1990. Adaptive radiosity textures for bidirectional ray tracing. In Computer Graphics (Proceedings of SIGGRAPH 90), vol. 24, ACM, 145-154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HILTZIK, M. A., AND PHAM, A. 2001. Synthetic actors guild. Los Angeles Times. May 8, 2001, natl. ed.: A1+.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HIRT, C., AND NICHOLS, B. 1981. Volume of fluid (VOF) method for the dynamics of free boundaries. J. Comp. Phys. 39, 201-225.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W. 1995. Importance driven path tracing using the photon map. In Eurographics Rendering Workshop 1995, 326-335.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W. 2001. Realistic Image Synthesis Using Photon Maps. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>363446</ref_obj_id>
				<ref_obj_pid>359189</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[JIANG, G.-S., AND PENG, D. 2000. Weighted eno schemes for hamilton-jacobi equations. SIAM J. Sci. Comput. 21, 2126-2143.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. T. 1986. The rendering equation. In Computer Graphics (Proceedings of SIGGRAPH 86), vol. 20, 143-150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KASS, M., AND MILLER, G. 1990. Rapid, stable fluid dynamics for computer graphics. In Computer Graphics (Proceedings of SIGGRAPH 90), vol. 24, ACM, 49-57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E. P., AND WILLEMS, Y. D. 1993. Bi-directional path tracing. In Proceedings of Compugraphics '93, 145-153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>951109</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MARSCHNER, S., AND LOBB, R. 1994. An evaluation of reconstruction filters for volume rendering. In Proceedings of Visualization 94, IEEE Comput. Soc. Press, 100-107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>30658</ref_obj_id>
				<ref_obj_pid>30657</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MASTEN, G., WATTERBERG, P., AND MAREDA, I. 1987. Fourier synthesis of ocean scenes. IEEE Computer Graphics and Application 7, 16-23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MILLER, G., AND PEARCE, A. 1989. Globular dynamics: A connected particle system for animating viscous fluids. Computers and Graphics 13, 3, 305-309.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192261</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[NISHITA, T., AND NAKAMAE, E. 1994. Method of displaying optical effects within water using accumulation buffer. In Proceedings of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Computer Graphics Proceedings, Annual Conference Series, ACM, 373-381.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791474</ref_obj_id>
				<ref_obj_pid>791214</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[O'BRIEN, J., AND HODGINS, J. 1995. Dynamic simulation of splashing fluids. In Proceedings of Computer Animation 95, 198-205.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[OSHER, S., AND FEDKIW, R. 2002. The Level Set Method and Dynamic Implicit Surfaces. Springer-Verlag, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[OSHER, S., AND SETHIAN, J. 1988. Fronts propagating with curvature dependent speed: Algorithms based on hamiliton-jacobi formulations. J. Comp. Phys. 79, 12-49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15893</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[PEACHEY, D. R. 1986. Modeling waves and surf. In Computer Graphics (Proceedings of SIGGRAPH 86), 20(4), ACM, 65-74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826551</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PREMOZE, S., AND ASHIKHMIN, M. 2000. Rendering natural waters. In The proceedings of Pacific Graphics 2000, 23-30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[RADOVITZKY, R., AND ORTIZ, M. 1998. Lagrangian finite element analysis of newtonian fluid flows. Int. J. Numer. Meth. Engng. 43, 607-619.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SCHACHTER, B. 1980. Long crested wave models. Computer Graphics and Image Processing 12, 187-201.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325510</ref_obj_id>
				<ref_obj_pid>325509</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SETHIAN, J. 1999. Level Set Methods and Fast Marching Methods. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[STAM, J. 1999. Stable fluids. In Proceedings of SIGGRAPH 99, ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, ACM, 121-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., PLATT, J., AND FLEISCHER, K. 1989. Heating and melting deformable models (from goop to glop). In Graphics Interface '89, 219-226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[TESSENDORF, J. 2001. Simulating ocean water. In Simulating Nature: Realistic and Interactive Techniques. SIGGRAPH 2001 Course Notes 47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>35070</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[TS'O, P. Y., AND BARSKY, B. A. 1987. Modeling and rendering waves: Wave-tracing using beta-splines and reflective and refractive texture mapping. ACM Transactions on Graphics 6, 3, 191-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>131953</ref_obj_id>
				<ref_obj_pid>131950</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[UNVERDI, S.-O., AND TRYGGVASON, G. 1992. A front-tracking method for viscous, incompressible, multi-fluid flows. J. Comp. Phys. 100, 25-37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[VEACH, E., AND GUIBAS, L. J. 1994. Bidirectional estimators for light transport. In Eurographics Rendering Workshop 1994 Proceedings, 147-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[VEACH, E., AND GUIBAS, L. J. 1997. Metropolis light transport. In Proceedings of SIGGRAPH 97, ACM SIGGRAPH / Addison Wesley, Computer Graphics Proceedings, Annual Conference Series, ACM, 65-76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97920</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[WATT, M. 1990. Light-water interaction using backward beam tracing. In Computer Graphics (Proceedings of SIGGRAPH 90), vol. 24, ACM, 377-385.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animation and Rendering of Complex Water Surfaces Douglas Enright Stephen Marschner Ronald Fedkiw Stanford 
University Stanford University Stanford University Industrial Light &#38; Magic srm@graphics.stanford.edu 
Industrial Light &#38; Magic enright@stanford.edu fedkiw@cs.stanford.edu Abstract We present a new method 
for the animation and rendering of photo­realistic water effects. Our method is designed to produce visually 
plausible three dimensional effects, for example the pouring of wa­ter into a glass (see .gure 1) and 
the breaking of an ocean wave, in a manner which can be used in a computer animation environment. In 
order to better obtain photorealism in the behavior of the simulated water surface, we introduce a new 
thickened front tracking tech­nique to accurately represent the water surface and a new velocity extrapolation 
method to move the surface in a smooth, water-like manner. The velocity extrapolation method allows us 
to provide a degree of control to the surface motion, e.g. to generate a wind­blown look or to force 
the water to settle quickly. To ensure that the photorealism of the simulation carries over to the .nal 
images, we have integrated our method with an advanced physically based rendering system. CR Categories: 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism Raytracing; Keywords: computational .uid dynamics, implicit surfaces, 
natu­ral phenomena, physically based animation, rendering, volume ren­dering 1 Introduction Water surrounds 
us in our everyday lives. Given the ubiquity of wa­ter and our constant interaction with it, the animation 
and rendering of water poses one of the greatest challenges in computer graphics. The dif.culty of this 
challenge was underscored recently through the use of water effects in a variety of motion pictures including 
the recent feature .lm Shrek where water, mud, beer and milk effects were seen. In response to a question 
concerning what was the single hardest shot in Shrek , DreamWorks SKG principal and producer of Shrek 
, Jeffrey Katzenberg, stated, It s the pouring of milk into a glass. [Hiltzik and Pham 2001]. The above 
quote illustrates the need for photorealistic simulation and rendering of water (and other liquids such 
as milk), especially in the case of complex, three dimensional behavior as seen when water is poured 
into a glass as in .gure 1. A key to achieving this goal is the visually accurate treatment of the surface 
separating the Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 
1-58113-521-1/02/0007 $5.00 Figure 1: Water being poured into a glass (55x120x55 grid cells). water 
from the air. The behavior of this surface provides the visual impression that water is being seen. If 
the numerical simulation method used to model this surface is not robust enough to capture the essence 
of water, then the effect is ruined for the viewer. A va­riety of new techniques for treatment of the 
surface are proposed in this paper in order to provide visually pleasing motion and photo­realistic results. 
We propose a new thickened front tracking approach to model­ing the surface, called the particle level 
set method . It is a hybrid surface tracking method that uses massless marker particles com­bined with 
a dynamic implicit surface. This method was entirely inspired by the hybrid liquid volume model proposed 
in [Foster and Fedkiw 2001], but exhibits signi.cantly more realistic surface be­havior. This effect 
is achieved by focusing on modeling the surface as opposed to the liquid volume as was done by [Foster 
and Fedkiw 2001]. This shift in philosophy away from volume modeling and to­wards surface modeling, is 
the key idea behind our new techniques, resulting in better photorealistic behavior of the water surface. 
We propose a new treatment of the velocity at the surface in order to obtain more visually realistic 
water surface behavior. The mo­tion of both the massless marker particles and the implicit function representing 
the surface is dependent upon the velocities contained on the underlying computational mesh. By extrapolating 
veloci­ties across the water surface and into the region occupied by the air, we obtain more accurate 
and better visual results. In the limit as the computational grid is re.ned, the resulting surface condition 
is identical to the traditional approach of making the velocity di­vergence free, but it gives more visually 
appealing and physically plausible results on coarse grids. Furthermore, this velocity extrap­olation 
procedure allows us to add a degree of control to the behav­ior of the water surface. We can add dampening 
and/or churning effects forcing it to quiet down or splash up faster than would be allowed by a straightforward 
physical simulation. Our new advances can be easily incorporated into a pre-existing Navier-Stokes solver 
for water. In fact, we solve the Navier-Stokes equations for liquid water along the lines of [Foster 
and Fedkiw 2001], in particular using a semi-Lagrangian stable .uid ap­proach introduced to the community 
by Stam [Stam 1999]. Our approach preserves as much of the realistic behavior of the water as possible, 
while at the same time providing a degree of control necessary for its use in a computer animation environment. 
Photorealistic rendering is necessary in order to complete the computational illusion of real water. 
In some ways water is an easy material to render, because unlike many common materials its op­tical properties 
are well understood and are easy to describe. In all but the largest-scale scenes, surface tension prevents 
water surfaces from exhibiting the microscopic features that make re.ection from many other materials 
so complicated. However, water invariably creates situations in which objects are illuminated through 
complex refracting surfaces, which means that the light transport problem that is so easy to state is 
dif.cult to solve. Most widely used render­ing algorithms disregard this sort of illumination or handle 
it using simple approximations, but because water and its illumination ef­fects are so familiar these 
approaches fail to achieve realism. There are several rendering algorithms that can properly account 
for all transport paths, including the illumination through the water sur­face; some examples are path 
tracing [Kajiya 1986], bidirectional path tracing [Heckbert 1990; Lafortune and Willems 1993; Veach and 
Guibas 1994], Metropolis light transport [Veach and Guibas 1997], and photon mapping [Jensen 1995]. In 
our renderings of clear water for this paper we have chosen photon mapping because it is simple and it 
makes it easy to avoid the distracting noise that often arises in pure path sampling algorithms from 
illumination through refracting surfaces. 2 Previous Work Early (and continuing) work by the graphics 
community into the modeling of water phenomenon focused on reduced model rep­resentations of the water 
surface, ranging from Fourier synthesis methods [Masten et al. 1987] to parametric representations of 
the water surface [Schachter 1980; Fournier and Reeves 1986; Peachey 1986; Ts o and Barsky 1987]. The 
last three references are notable in the way they attempt to model realistic wave behavior including 
the change in wave behavior as a function of the depth of the water. Fairly realistic two dimensional 
wave scenery can be developed us­ing these methods including the illusion of breaking waves, but ulti­mately 
they are all constrained by the sinusoidal modeling assump­tion present in each of them. They are unable 
to easily deal with complex three dimensional behaviors such as .ow around objects and dynamically changing 
boundaries. A summary of the above methods and their application to modeling and rendering of ocean wave 
environments can be found in [Tessendorf 2001]. In order to obtain water models which could potentially 
be used in a dynamic animation environment, researchers turned towards using two dimensional approximations 
to the full 3D Navier-Stokes equations. [Kass and Miller 1990] use a linearized form of the 2D shallow 
water equations to obtain a height .eld representation of the water surface. A pressure de.ned height 
.eld formulation was used by [Chen and Lobo 1994] in .uid simulations with moving obstacles. [O Brien 
and Hodgins 1995] used a height model com­bined with a particle system in order to simulate splashing 
liquids. The use of a height .eld gives a three dimensional look to a two dimensional .ow calculation, 
but it constrains the surface to be a function, i.e. the surface passes the vertical line test where 
for each (x,y) position there is at most one z value. The surface of a crash­ing wave or of water being 
poured into a glass does not satisfy the vertical line test. Use of particle systems permits the surface 
to become multivalued. A viscous spring particle representation of a liquid has been proposed by [Miller 
and Pearce 1989]. An alterna­tive molecular dynamics approach to the simulation of particles in the liquid 
phase has been developed by [Terzopoulos et al. 1989]. Particle methods, while quite versatile, can pose 
dif.culties when trying to reconstruct a smooth water surface from the locations of the particles alone. 
The simulation of complex water effects using the full 3D Navier-Stokes equations has been based upon 
the large amount of research done by the computational .uid dynamics community over the past 50 years. 
[Foster and Metaxas 1996] utilized the work of [Harlow and Welch 1965] in developing a 3D Navier-Stokes 
methodology for the realistic animation of liquids. Further CFD enhancements to the traditional marker 
and cell method of Har­low and Welch which allow one to place particles only near the surface can be 
found in [Chen et al. 1997]. A semi-Lagrangian stable .uids treatment of the convection portion of the 
Navier-Stokes equations was introduced to the computer graphics com­munity by [Stam 1999] in order to 
allow the use of signi.cantly larger time steps without hindering stability. [Foster and Fedkiw 2001] 
made signi.cant contributions to the simulation and control of three dimensional .uid simulations through 
the introduction of a hybrid liquid volume model combining implicit surfaces and mass­less marker particles; 
the formulation of plausible boundary con­ditions for moving objects in a liquid; the use of an ef.cient 
iter­ative method to solve for the pressure; and a time step subcycling scheme for the particle and implicit 
surface evolution equations in order to reduce the amount of visual error inherent to the large semi-Lagrangian 
stable .uid time step used for time evolving the .uid velocity and the pressure. The combination of all 
of the above ad­vances in 3D .uid simulation technology along with ever increasing computational resources 
has set the stage for the inclusion of fully 3D .uid animation tools in a production environment. Most 
work on simulating water at small scales has not speci.­cally addressed rendering and has not used methods 
that correctly account for all signi.cant light transport paths. Research on the rendering of illumination 
through water [Watt 1990; Nishita and Nakamae 1994] has used methods based on processing each poly­gon 
of a mesh that represents a fairly smooth water surface, so these methods cannot be used for the very 
complex implicit sur­faces that result from our simulations. For the case of 2D wave .elds in the open 
ocean, approaches motivated by physical cor­rectness have produced excellent results [Premoze and Ashikhmin 
2000; Tessendorf 2001].  3 Simulation Method 3.1 Motivation [Foster and Fedkiw 2001] chose to de.ne 
the liquid volume be­ing simulated as one side of an isocontour of an implicit function, f. The surface 
of the water was de.ned by the f = 0 isocontour with f = 0 representing the water and f > 0 representing 
the air. By using an implicit function representation of the liquid volume, they obtained a smooth, temporally 
coherent liquid surface. They rejected the use of particles alone to represent the liquid surface be­cause 
it is dif.cult to a calculate a visually desirable smooth liquid surface from the discrete particles 
alone. The implicit surface was dynamically evolved in space and time according to the underly­ing liquid 
velocity .u. As shown in [Osher and Sethian 1988], the appropriate equation to do this is ft +.u · .f 
= 0 (1) where ft is the partial derivative of f with respect to time and . = (. /. x, . /. y, . /. z) 
is the gradient operator. An implicit function only approach to modeling the surface will not yield realistic 
surface behavior due to an excessive amount of volume loss on coarse grids. A seminal advance of [Foster 
and Fedkiw 2001] in creating realistic liquids for computer animation is the hybridization of the visually 
pleasing smooth implicit func­tion modeling of the liquid volume with particles that can maintain the 
liquid volume on coarse grids. The inclusion of particles pro­vides a way for capturing the liveliness 
of a real liquid with spray and splashing effects. Curvature was used as an indicator for allow­ing particles 
to in.uence the implicit surface representation of the water. This is a natural choice since small droplets 
of water have very high curvature and dynamic implicit surfaces have dif.culty resolving features with 
sharp corners. Figure 2(a) shows a notched disk that we rotate for one rigid body rotation about the 
point (50,50). The inside of the disk can be thought of as a volume of liquid. Figure 2(b) shows the 
re­sult of using an implicit surface only approach (after one rotation) where both the higher and lower 
corners of the disk are erroneously shaved off causing both loss of visual features and an arti.cially 
viscous look for the liquid. This numerical result was obtained us­ing a highly accurate .fth order WENO 
discretization of equation 1 (see e.g. [Jiang and Peng 2000; Osher and Fedkiw 2002]). For the sake of 
comparison, we note that [Sethian 1999] only proposes second order accurate methods for discretizing 
this equation. Fig­ure 2(c) shows the result obtained with our implementation of the method from [Foster 
and Fedkiw 2001]. The particles inside the disk do not allow the implicit surface to cross over them 
and help to preserve the two corners near the bottom. However, there is lit­tle they can do to stop the 
implicit surface from drifting away from them near the top corners. This represents loss of air or bubbles 
as the method erroneously gains liquid volume. This is not desirable since many complex water motions 
such as wave phenomenon are due in part to differing heights of water columns adjacent to each other. 
Loss of air in a water column reduces the pressure forces from neighboring columns destroying many of 
the dynamic splash­ing effects as well as the overall visually stimulating liveliness of the liquid. 
While the hybrid liquid volume model of Foster and Fedkiw at­tempts to maintain the volume of the liquid 
accurately, it fails to model the air or more generally the opposite side of the liquid sur­face. We 
shift the focus away from maintaining a liquid volume towards maintaining the liquid surface itself. 
An immediate advan­tage of this approach is that it leads to symmetry in particle place­ment. We place 
particles on both sides of the surface and use them to maintain an accurate representation of the surface 
itself regard­less of what may be on one side or the other. The particles are not meant to track volume, 
they are intended to correct errors in the surface representation by the implicit function. In [Enright 
et al. 2002] we showed that this surface only approach leads to the most accurate 3D results for surface 
tracking ever achieved (in both CFD and CG). This was done for analytical test velocity .elds. Fig­ure 
2(d) shows that this new method correctly computes the rigid body rotation for the notched disk preserving 
both the water and the air volumes so that more realistic water motion can be obtained. In this current 
paper, we couple this new method to real velocity .elds and .uid dynamics calculations for the .rst time. 
Represen­tative results of this new method can be seen in .gure 3. A ball is thrown into a tank of water 
with the same tank geometry, grid spacing and ball speed as seen in .gure 4 (courtesy of [Foster and 
Fedkiw 2001]). The resulting splash after the ball impacts the sur­face of the water is dramatically 
different between the two .gures. Our new method produces the well formed, thin sheet one would visually 
expect. Note that the distorted look of the ball in our .g­ure is due to the correct calculation of the 
refraction of light when it passes through the surface of the water. To give an indication of the additional 
computational cost incurred using our new simu­ (a) Initial Notched Disk (b) Implicit Surface Only 
(c) Particles Inside Only (d) Our New Method  Figure 2: Rigid Body Rotation Test lation method, .gure 
3 took about 11 minutes per frame, .gure 4 took about 7 minutes per frame and a level set only solution 
takes about 3 minutes per frame. The rendering time for .gure 3 was approximately 15 minutes per frame. 
 3.2 Particle Level Set Method 3.2.1 Initialization of Particles Two sets of particles are randomly 
placed in a thickened surface region (we use three grid cells on each side of the surface) with positive 
particles in the f > 0 region and negative particles in the f = 0 region. There is no need to place particles 
far away from the surface since the sign of the level set function readily identi.es these regions gaining 
large computational savings. The number of particles placed in each cell is an adjustable parameter that 
can be used to control the amount of resolution, e.g. we use 64 particles per cell for most of our examples. 
Each particle possesses a radius, rp, which is constrained to take a minimum and maximum value based 
upon the size of the underlying computational cells used in the simulation. A minimum radius of .1min(.x,.y,.z) 
and maxi­mum radius of .5min(.x,.y,.z) appear to work well. The radius of a particle changes dynamically 
throughout the simulation, since a particle s location relative to the surface changes. The radius is 
set according to: . rp = . . rmax spf(.xp) rmin if spf(.xp) > rmax if rmin = spf(.xp) = rmax if spf(.xp) 
< rmin , (2) where sp is the sign of the particle (+1 for positive particles and -1 for negative particles). 
This radius adjustment keeps the bound­ary of the spherical particle tangent to the surface whenever 
possi­ble. This fact combined with the overlapping nature of the particle  Figure 3: Our New Method 
(140x110x90 grid cells). spheres allows for an enhanced reconstruction capability of the liq­uid surface. 
 3.2.2 Time Integration The marker particles and the implicit function are separately in­tegrated forward 
in time using a forward Euler time integration scheme. The implicit function is integrated forward using 
equa­tion 1, while the particles are passively advected with the .ow us­ing d.xp/dt = .up, where .up 
is the .uid velocity interpolated to the particle position .xp. 3.2.3 Error Correction of the Implicit 
Surface Identi.cation of Error: The main role of the particles is to de­tect when the implicit surface 
has suffered inaccuracies due to the coarseness of the computational grid in regions with sharp features. 
Particles that are on the wrong side of the interface by more than their radius, as determined by a locally 
interpolated value of f at the particle position .xp, are considered to have escaped their side of the 
interface. This indicates errors in the implicit surface rep­resentation. In smooth, well resolved regions 
of the interface, our dynamic implicit surface is highly accurate and particles do not drift a non-trivial 
distance across the interface. Quanti.cation of Error: We associate a spherical implicit func­tion , 
designated fp, with each particle p whose size is determined by the particle radius, i.e. fp(.x)= sp(rp 
-|.x -.xp|). (3) Any difference in f from fp indicates errors in the implicit function representation 
of the surface. That is, the implicit version of the surface and the particle version of the surface 
disagree. Error Correction: We use escaped positive particles to rebuild the f > 0 region and escaped 
negative particles to rebuild the f = 0 region as de.ned by the implicit function. The reconstruction 
of the implicit surface occurs locally within the cell that each escaped par­ticle currently occupies. 
Using equation 3, the fp values of escaped particles are calculated for the eight grid points on the 
boundary of the cell containing the particle. This value is compared to the cur­rent value of f for each 
grid point and we take the smaller value (in magnitude) which is the value closest to the f = 0 isocontour 
de.ning the surface. We do this for all escaped positive and escaped Figure 4: Foster and Fedkiw 2001 
(140x110x90 grid cells). negative particles. The result is an improved representation of the surface 
of the liquid. 3.2.4 When To Apply Error Correction We apply the error correction method discussed above 
after any computational step in which f has been modi.ed in some way. This occurs when f is integrated 
forward in time and when the implicit function is smoothed to obtain a visually pleasing surface. We 
smooth the implicit surface with an equation of the form ft = -S(ft=0)(|.f |- 1), (4) where t is a .ctitious 
time and S(f ) is a smoothed signed distance function given by f S(f)= . . (5)f 2 +(.x)2 More details 
on this are given in [Foster and Fedkiw 2001]. 3.2.5 Particle Reseeding In complex .ows, a liquid interface 
can be stretched and torn in a dynamic fashion. The use of only an initial seeding of particles will 
not capture these effects well, as regions will form that lack a suf.­cient number of particles to adequately 
perform the error correction step. Periodically, e.g. every 20 frames, we randomly reseed par­ticles 
about the thickened interface to avoid this dilemma. This is done by randomly placing particles near 
the interface, and then using geometric information contained within the implicit function (e.g. the 
direction of the shortest possible path to the surface is given by .N = .f/|.f|) to move the particles 
to their respective domains, f > 0 or f = 0. The goal of this reseeding step is to pre­ serve the initial 
particle resolution of the interface, e.g. 64 particles per cell. Thus, if a given cell has too few or 
too many particles, some can be added or deleted respectively. 3.2.6 A Note on Alternative Methods If 
we felt that preserving the volume of the .uid was absolutely nec­essary in order to obtain visually 
pleasing .uid behavior, we would have chosen to use a volume of .uid (VOF) [Hirt and Nichols 1981] representation 
of the .uid. Although VOF methods explicitly con­serve volume, they produce visually disturbing artifacts 
allowing thin liquid sheets to arti.cially break up and form blobbies and .otsam of liquid. Also, a visually 
desirable smooth .uid interface is dif.cult to construct when using these methods. Another alternative 
is to explicitly discretize the free surface with particles and maintain a connectivity list between 
particles, see e.g. [Unverdi and Tryggvason 1992]. This connectivity list is dif.cult to maintain when 
parts of the free surface break apart or merge together as is often seen in complex .ows of water and 
other liquids. Our approach avoids the especially dif.cult issues associated with maintaining particle 
connectivity information.  3.3 Velocities at the Surface Although the Navier-Stokes equations can be 
used to .nd the ve­locity within the liquid volume, boundary conditions are needed for the velocity on 
the air side near the free surface. These boundary condition velocities are used in updating the Navier-Stokes 
equa­tions, moving the surface, and moving the particles placed near the surface. The velocity at the 
free surface of the water can be deter­mined through the usual enforcement of the conservation of mass 
(volume) constraint, . ·.u = 0, where .u =(u,v, w) is the velocity of the liquid. This equation allows 
us to determine the velocities on all the faces of computational cells that contain the f = 0 isocon­tour. 
Unfortunately, the procedure for doing this is not unique when more than one face of a cell needs a boundary 
condition velocity. A variety of methods have been proposed, e.g. see [Chen et al. 1995] and [Foster 
and Metaxas 1996]. We propose a different approach altogether, the extrapolation of the velocity across 
the surface into the surrounding air. As the com­putational grid is re.ned, this method is equivalent 
to the usual method, but it gives a smoother and more visually pleasing motion of the surface on coarser 
(practical sized) grids. We extrapolate the velocity out a few grid cells into the air, obtaining boundary 
condi­tion velocities in a band of cells on the air side of the surface. This allows us to use higher 
order accurate methods and obtain better results when moving the implicit surface using equation 1 and 
also provides velocities for updating the position of particles on the air side of the surface. Velocity 
extrapolation also assists in the imple­mentation of the semi-Lagrangian stable .uid method, since there 
are times when characteristics generated by this approach look back across the interface (a number of 
cells) into the air region for valid velocities. 3.3.1 Extrapolation Method The equation modeling this 
extrapolation for the x component of the velocity, u, is given by . u = -.N · .u, (6) .t where .N =(nx, 
ny,nz) is a unit vector perpendicular to the implicit surface and t is .ctitious time. A similar equation 
holds for the v and w components of velocity .eld. Since we use an implicit surface to describe the .uid, 
.N = .f /|.f|. Fast methods exist for solving this equation in O(nlogn) time, where n is the number of 
grid points that one needs to extrapolate over, in our case a .ve grid cell thick band on the air side 
of the interface. The fast method is based upon the observation that information in equation 6 propa­gates 
in only one direction away from the surface. This implies that we do not have to revisit previously computed 
values of .(the uext extrapolated velocity) if we perform the calculation in the correct order. The order 
is determined by the value of f allowing us to do an O(nlogn) sorting of the points before beginning 
the calcula­tion. The value of u itself is determined by enforcing the condition at steady state, namely 
.f · .u = 0 where the derivatives are de­termined using previously calculated values of f and u. From 
this scalar equation, a new value of u can be determined, and then we proceed to the next point corresponding 
to the next smallest value of f , etc. Further details of this method are discussed in [Adal­steinsson 
and Sethian 1999]. 3.3.2 Velocity Advection The momentum portion of the Navier-Stokes equations is: 
1 .ut = -.u · ..u + .. · (..u) - .p +.g, (7) . where . is the kinematic viscosity of the .uid, . is the 
density of the .uid, p is the pressure, and .g is an externally applied gravity .eld. We use the semi-Lagrangian 
stable .uids method [Stam 1999] to update the convective portion of this equation, i.e. the .u · ..u 
term. This method calculates the .rst term on the left hand side of equation 7 by following the .uid 
characteristics backwards in time to determine from which computational cell the volume of .uid came, 
and then taking an average of the appropriate veloci­ties there. This allows one to stably take much 
larger time steps than would be allowed using other time advancement schemes for velocity advection. 
A consequence of the now allowed large semi-Lagrangian time step is that near the surface, we might look 
across the interface as many as a few grid cells into the air region to .nd velocities. In a standard 
approach, valid velocities are not de.ned in this region. However, our velocity extrapolation technique 
easily handles this case ensuring that physically plausible velocities exist a few grid cells into the 
air region. In fact, we extrapolate the velocity the maximum distance from the surface that would be 
allowed dur­ing a semi-Lagrangian stable .uids time step guaranteeing that a smooth, physically plausible 
and visually appealing velocity can be found there. 3.3.3 Control Our velocity extrapolation method 
enables us to apply a newly de­vised method for controlling the nature of the surface motion. This is 
done simply by modifying the extrapolated velocities on the air side of the surface. For example, to 
model wind-blown water as a result of air drag, we take a convex combination of the extrapolated velocities 
with a pre-determined wind velocity .eld .u =(1 - a).(8) uext + a.uwind , where .uext is the extrapolated 
velocity, .uwind a desired air-like ve­locity, and 0 = a = 1 the mixing constant. This can be applied 
throughout the surface or only locally in select portions of the com­putational domain as desired. Note 
that setting .= 0 forces uwind churning water to settle down faster with the fastest settling result­ing 
from a = 1. All of the .gures shown in the paper used a = 0, but we demonstrate how a can be used to 
force a poured glass of water to settle more quickly in the accompanying video.  3.4 Summary We divide 
up the computational domain into voxels with the com­ponents of .u stored on the appropriate faces and 
p, . and f stored at the center of each cell. This arrangement of computational vari­ables is the classic 
staggered MAC-style arrangement [Harlow and Welch 1965]. The density of a given cell is given by the 
value of f at the center of the cell. The evolution of .u, p, . and f in a given time step is performed 
in a series of three steps as outlined below: 1. The current surface velocity is smoothly extrapolated 
across the surface into the air region as discussed in section 3.3.1. Appropriate control behavior modi.cations 
to the velocity .eld are made. 2. The water surface and marker particles are integrated forward in time 
via an explicit time step subcycling method with the appropriate corrections to f as described in section 
3.2.3. 3. The velocity .eld is updated with equation 7 using the up­dated values for .. This is done 
by .rst using the semi-Lagrangian stable .uid method to .nd an estimate for the velocity. This estimate 
is further augmented by the viscous and forcing terms. The spatial derivatives used in calculating these 
terms are calculated using a standard centered second order accurate .nite difference scheme. Then a 
system of lin­ear equations is assembled and solved for the pressure in or­der to make this intermediate 
velocity .eld divergence free. The newly calculated pressure is applied as a correction to the intermediate 
velocity in order to fully update the water s velocity .eld. Interaction of the liquid with objects, 
walls, etc. is treated here as well. For this step, we follow exactly the method of [Foster and Fedkiw 
2001] and refer the reader there for more details.  This sequence of steps is repeated until a user 
de.ned stopping point is reached. The time step for each iteration of the above steps is determined using 
the water s velocity to calculate an appropri­ate CFL condition which is approximately .ve times larger 
than the traditional CFL condition used in .uid simulations (the semi-Lagrangian stable .uids method 
allows this). Also, any viscosity related CFL restrictions are locally dealt with by reducing the vis­cosity 
in the offending cells in order to allow for our larger time step. 3.5 Rendering Our results are rendered 
using a physically based Monte Carlo ray tracer capable of handling all types of illumination using photon 
maps and irradiance caching [Jensen 2001]. To integrate our simu­lation system with the renderer we implemented 
a geometry primi­tive that intersects rays with the implicit surface directly by solving for the root 
of the signed distance along the ray. Depending on the accuracy required by a particular scene we use 
either a trilin­ear or a tricubic .lter [Marschner and Lobb 1994] to reconstruct f . The normal is computed 
using trilinearly interpolated central dif­ferences for the trilinear surface, or simply using the derivative 
of the reconstructed tricubic surface. The properties of f have two advantages in the rendering con­text. 
First, intersecting a ray with the surface is much more ef.cient than it would be for an isosurface of 
a general function. The signed distance function provides a lower bound on the distance to the in­tersection 
along the ray, allowing us to take large steps when the current point is far from the surface. Second, 
it is easy to provide for motion blur in the standard distribution ray tracing framework. To compute 
the surface at an intermediate time between two frames we simply interpolate between the two signed distance 
volumes and use the same intersection algorithm unchanged. For the small mo­tions that occur between 
frames the special properties of f are not signi.cantly compromised.  4 Results 4.1 Pouring Water Into 
A Glass Figures 1 and 7 show the high degree of complexity in the water surface. Note the liveliness 
of the surface of the water when the water is initially being poured. The ability to maintain the visu­ally 
pleasing thin sheets of water during the turbulent mixing phase is a consequence of our new method. We 
do not loose any of the .ne detail with regards to the air pockets formed, since we model both sides 
of the water surface. Even though the calculation was performed on a Cartesian computational grid, the 
glass was shaped as a cylinder on the grid, with the grid points outside the cylinder treated as an object 
which does not permit the .uid to interpene­trate it. The glass was modeled as smooth and clear in order 
to highlight the action of the water being poured into the glass. The computational cost was approximately 
8 minutes per frame. To our knowledge, the only other complex, three dimensional simulation of a liquid 
being poured into a glass for computer an­imation is from the Gingerbread Man torture scene in the feature 
.lm Shrek . Milk lacks the transparency of water making it dif.­cult to clearly view the dynamic behavior 
of the milk surface. Also, the modeling of a thick polygonal glass with a rough surface does not provide 
a clear view of the milk making a direct comparison dif.cult. The scene used to render our result contains 
just a simple cylin­drical glass, the simulated water surface, and a few texture mapped polygons for 
the background. Illumination comes from a physically based sky model and two area sources. Because water 
only re.ects and refracts other objects, the scene including the sky is very impor­tant to the appearance. 
The glass and the water together create three dielectric interfaces: glass-air, water-air, and air-water, 
so the inside surface of the glass has two sets of material properties depending on whether it is inside 
or outside the implicitly de.ned surface (which is easy to determine by checking the sign of f ). The 
illumination in the shadow of the glass is provided by a photon map, and illu­mination from the sky on 
diffuse surfaces in the scene is computed using Monte Carlo integration. We also note that milk would 
not present as dif.cult a global illumination problem due to its lack of transparency. Motion blur was 
included for these renderings. The rendering time for the glass was approximately 15 minutes per frame. 
 4.2 Breaking Wave As a second example, we have performed a breaking wave calcu­lation in a numerical 
wave tank as shown in .gure 8. To begin to model this phenomenon, we needed to chose an initial condition 
for the wave. We chose to use the theoretical solution of a solitary wave of .nite amplitude propagating 
without shape change [Radovitzky and Ortiz 1998]. The initial velocities u and v in the x and y direc­tions 
respectively and surface height . are given by: u = gd H sech2 3Hx (9) d 4d3 .. ... . ..3/2 Hy 3H 3H 
v = . 3gd sech2 tanh (10) dd 4d3 x 4d3 x . = d + H sech2 43dH 3 x , (11) where g is the gravitational 
constant 9.8 m/s. For our simulations, we set H = 7 and d = 10. We used the same initial conditions in 
three spatial dimensions, replicating the two dimensional initial conditions along the z-axis. Next, 
we needed to introduce a model of a sloping underwater shelf in order to cause the propagating pulse 
to actually pile up on itself and form a breaking wave. We performed a variety of two di­mensional tests 
to determine the best underwater shelf geometry to generate a visually pleasing breaking wave. Figure 
5 is a sequence of frames from one of the two dimensional tests we ran with the same x-y cross section 
as can be found in our 3D example. We found this prototyping technique to be a fast and easy way to ex­plore 
possible breaking wave behaviors in a fraction of the time it would take to run a fully three dimensional 
test case.   Figure 5: Two Dimensional Breaking Wave After determining the best submerged shelf geometry 
to generate a breaking wave, we generated a slight tilt in the geometry in order to induce a curl in 
the break of the wave and enhance the three dimensional look. A sketch of the shelf geometry used in 
our three dimensional calculations is shown in .gure 6. An incline of slope 1:7 rises up from the sea 
bottom to a depth of 2 m below the surface of the water. Instead of allowing the incline to go all the 
way to the surface we chose to have it .atten out at this depth in order to illustrate the splash up 
effects after the initial breaking of the wave. Next we ran a fully three dimensional simulation, the 
results of which can be seen in .gure 8. The wave has the intended curling effect. The formation of a 
tube of air is clearly seen after the wave splashes down, with the air particles maintaining the tube 
even af­ter the wave begins a secondary splash up. We observe some solely three dimensional effects including 
the .ngering of the breaking wave. The computational cost was approximately 13 minutes per frame. Figure 
6: Submerged Shelf The simulation shown captures the basic phenomena of the breaking wave on a very coarse 
grid, but in real waves there are small-scale features that, while not important to the behavior of the 
large wave, are very important for its appearance. Coupling a 2D simulation for the small scale features 
to the wave simulation is a promising avenue for future work. Once those waves are incorpo­rated it will 
become important to treat diffusion of light through the water more correctly. Also, texture mapping 
of the wave, e.g. use of a Philips spectrum [Tessendorf 2001] or a bump mapping technique [Fournier and 
Reeves 1986], needs to be explored fur­ther. Another challenge will be to augment our method to naturally 
handle spray and foam.  5 Conclusion and Future Work We have presented some novel computational methods 
for en­hanced surface tracking and modeling of the surface motion. Com­bining these leads to the possibility 
of creating photorealistic be­havior in 3D water simulations for the purpose of computer graph­ics animation. 
We discussed some advances in rendering the sim­ulated photorealistic behavior in order to complete the 
illusion that real water is being seen. The computational methods presented can easily be included into 
an existing three dimensional .uid simula­tion animation tool. Also, we believe that our thickened front 
tracking approach should enable texture mapping of an implicitly de.ned .uid surface, and we will pursue 
this as future work. 6 Acknowledgment Research supported in part by an ONR YIP and PECASE award N00014-01-1-0620, 
NSF DMS-0106694, NSF ACI-0121288, NSF IIS-0085864, and the DOE ASCI Academic Strategic Alliances Program 
(LLNL contract B341491). The authors acknowledge Henrik Wann Jensen for the use of his dali rendering 
system and for his help with the extensions that were added to it for this paper. The authors would also 
like to thank Willi Geiger, Andy Hen­drickson, Neil Herzinger, Sebastian Marino, Nick Rasmussen, Philippe 
Rebours, and Industrial Light + Magic for their support and assistance in rendering the breaking wave. 
The third author would like to acknowledge the fruitful collabo­ration with Nick Foster regarding the 
hybridization of particle and level set methods that was published in [Foster and Fedkiw 2001]. Without 
it, neither [Enright et al. 2002] nor this current work would have been possible.  References ADALSTEINSSON, 
D., AND SETHIAN, J. 1999. The fast construc­tion of extension velocities in level set methods. J. Comp. 
Phys. 148, 2 22. CHEN, J., AND LOBO, N. 1994. Toward interactive-rate sim­ulation of .uids with moving 
obstacles using the navier-stokes equations. Computer Graphics and Image Processing 57, 107 116. CHEN, 
S., JOHNSON, D., AND RAAD, P. 1995. Velocity boundary conditions for the simulation of free surface .uid 
.ow. J. Comp. Phys. 116, 262 276. CHEN, S., JOHNSON, D., RAAD, P., AND FADDA, D. 1997. The surface marker 
and micro cell method. Int. J. for Num. Meth. in Fluids 25, 749 778. ENRIGHT, D., FEDKIW, R., FERZIGER, 
J., AND MITCHELL, I. 2002. A hybrid particle level set method for improved interface capturing. J. Comp. 
Phys.. To appear. FOSTER, N., AND FEDKIW, R. 2001. Practical animation of liq­uids. In Proceedings of 
SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference 
Series, ACM, 23 30. FOSTER, N., AND METAXAS, D. 1996. Realistic animation of liquids. Graphical Models 
and Image Processing 58, 471 483. FOURNIER, A., AND REEVES, W. T. 1986. A simple model of ocean waves. 
In Computer Graphics (Proceedings of SIG-GRAPH 86), 20(4), ACM, 75 84. HARLOW, F., AND WELCH, J. 1965. 
Numerical calculation of time-dependent viscous incompressible .ow of .uid with free surface. Phys. Fluids 
8, 2182 2189. HECKBERT, P. S. 1990. Adaptive radiosity textures for bidirec­tional ray tracing. In Computer 
Graphics (Proceedings of SIG-GRAPH 90), vol. 24, ACM, 145 154. HILTZIK, M. A., AND PHAM, A. 2001. Synthetic 
actors guild. Los Angeles Times. May 8, 2001, natl. ed. : A1+. HIRT, C., AND NICHOLS, B. 1981. Volume 
of .uid (VOF) method for the dynamics of free boundaries. J. Comp. Phys. 39, 201 225. JENSEN, H. W. 1995. 
Importance driven path tracing using the photon map. In Eurographics Rendering Workshop 1995, 326 335. 
JENSEN, H. W. 2001. Realistic Image Synthesis Using Photon Maps. A K Peters. JIANG, G.-S., AND PENG, 
D. 2000. Weighted eno schemes for hamilton-jacobi equations. SIAM J. Sci. Comput. 21, 2126 2143. KAJIYA, 
J. T. 1986. The rendering equation. In Computer Graph­ics (Proceedings of SIGGRAPH 86), vol. 20, 143 
150. KASS, M., AND MILLER, G. 1990. Rapid, stable .uid dynamics for computer graphics. In Computer Graphics 
(Proceedings of SIGGRAPH 90), vol. 24, ACM, 49 57. LAFORTUNE, E. P., AND WILLEMS, Y. D. 1993. Bi-directional 
path tracing. In Proceedings of Compugraphics 93, 145 153. MARSCHNER, S., AND LOBB, R. 1994. An evaluation 
of recon­struction .lters for volume rendering. In Proceedings of Visual­ization 94, IEEE Comput. Soc. 
Press, 100 107. MASTEN, G., WATTERBERG, P., AND MAREDA, I. 1987. Fourier synthesis of ocean scenes. IEEE 
Computer Graphics and Appli­cation 7, 16 23. MILLER, G., AND PEARCE, A. 1989. Globular dynamics: A con­nected 
particle system for animating viscous .uids. Computers and Graphics 13, 3, 305 309. NISHITA, T., AND 
NAKAMAE, E. 1994. Method of displaying op­tical effects within water using accumulation buffer. In Proceed­ings 
of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Com­puter Graphics Proceedings, Annual Conference Series, ACM, 
373 381. O BRIEN, J., AND HODGINS, J. 1995. Dynamic simulation of splashing .uids. In Proceedings of 
Computer Animation 95, 198 205. OSHER, S., AND FEDKIW, R. 2002. The Level Set Method and Dynamic Implicit 
Surfaces. Springer-Verlag, New York. OSHER, S., AND SETHIAN, J. 1988. Fronts propagating with cur­vature 
dependent speed: Algorithms based on hamiliton-jacobi formulations. J. Comp. Phys. 79, 12 49. PEACHEY, 
D. R. 1986. Modeling waves and surf. In Computer Graphics (Proceedings of SIGGRAPH 86), 20(4), ACM, 65 
74. PREMOZE, S., AND ASHIKHMIN, M. 2000. Rendering natural waters. In The proceedings of Paci.c Graphics 
2000, 23 30. RADOVITZKY, R., AND ORTIZ, M. 1998. Lagrangian .nite el­ement analysis of newtonian .uid 
.ows. Int. J. Numer. Meth. Engng. 43, 607 619. SCHACHTER, B. 1980. Long crested wave models. Computer 
Graphics and Image Processing 12, 187 201. SETHIAN, J. 1999. Level Set Methods and Fast Marching Methods. 
Cambridge University Press. STAM, J. 1999. Stable .uids. In Proceedings of SIGGRAPH 99, ACM SIGGRAPH 
/ Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, ACM, 121 128. TERZOPOULOS, 
D., PLATT, J., AND FLEISCHER, K. 1989. Heat­ing and melting deformable models (from goop to glop). In 
Graphics Interface 89, 219 226. TESSENDORF, J. 2001. Simulating ocean water. In Simulating Nature: Realistic 
and Interactive Techniques. SIGGRAPH 2001 Course Notes 47. TS O, P. Y., AND BARSKY, B. A. 1987. Modeling 
and render­ing waves: Wave-tracing using beta-splines and re.ective and refractive texture mapping. ACM 
Transactions on Graphics 6, 3, 191 214. UNVERDI, S.-O., AND TRYGGVASON, G. 1992. A front-tracking method 
for viscous, incompressible, multi-.uid .ows. J. Comp. Phys. 100, 25 37. VEACH, E., AND GUIBAS, L. J. 
1994. Bidirectional estimators for light transport. In Eurographics Rendering Workshop 1994 Proceedings, 
147 162. VEACH, E., AND GUIBAS, L. J. 1997. Metropolis light trans­port. In Proceedings of SIGGRAPH 97, 
ACM SIGGRAPH / Addison Wesley, Computer Graphics Proceedings, Annual Con­ference Series, ACM, 65 76. 
WATT, M. 1990. Light-water interaction using backward beam tracing. In Computer Graphics (Proceedings 
of SIGGRAPH 90), vol. 24, ACM, 377 385.  Figure 7: Water being poured into a clear, cylindrical glass 
(55x55x120 grid cells). Our method makes possible the .ne detail seen in the turbulent mixing of the 
water and air. Figure 8: View of wave breaking on a submerged shelf (540x75x120 grid cells). Note the 
ability to properly model the initial breaking (top two frames) and secondary splash up (bottom two frames) 
phases. Rendered by proprietary software at ILM. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566646</article_id>
		<sort_key>745</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Image based flow visualization]]></title>
		<page_from>745</page_from>
		<page_to>754</page_to>
		<doi_number>10.1145/566570.566646</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566646</url>
		<abstract>
			<par><![CDATA[A new method for the visualization of two-dimensional fluid flow is presented. The method is based on the advection and decay of dye. These processes are simulated by defining each frame of a flow animation as a blend between a warped version of the previous image and a number of background images. For the latter a sequence of filtered white noise images is used: filtered in time and space to remove high frequency components. Because all steps are done using images, the method is named Image Based Flow Visualization (<sc>IBFV</sc>). With <sc>IBFV</sc> a wide variety of visualization techniques can be emulated. Flow can be visualized as moving textures with line integral convolution and spot noise. Arrow plots, streamlines, particles, and topological images can be generated by adding extra dye to the image. Unsteady flows, defined on arbitrary meshes, can be handled. <sc>IBFV</sc> achieves a high performance by using standard features of graphics hardware. Typically fifty frames per second are generated using standard graphics cards on PCs. Finally, <sc>IBFV</sc> is easy to understand, analyse, and implement.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[flow visualization]]></kw>
			<kw><![CDATA[line integral convolution]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010370</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P135241</person_id>
				<author_profile_id><![CDATA[81100381561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jarke]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[van Wijk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technische Universiteit Eindhoven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166151</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., AND LEEDOM, L. C. 1993. Imaging vector fields using line integral convolution. In Proceedings of ACM SIGGRAPH 93, Computer Graphics Proceedings, Annual Conference Series, 263-272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>509612</ref_obj_id>
				<ref_obj_pid>509593</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DE LEEUW, W., AND VAN LIERE, R. 1997. Divide and conquer spot noise. In Proceedings SuperComputing'97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833880</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DE LEEUW, W., ANDVAN WIJK, J. 1995. Enhanced spot noise for vector field visualization. In Proceedings IEEE Visualization'95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[HANSEN, P. 1997. Introducing pixel texture. Developer News, 23-26. Silicon Graphics Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300538</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W., WESTERMANN, R., SEIDEL, H.-P., AND ERTL, T. 1999. Applications of pixel textures in visualization and realistic image synthesis. In ACM Symposium on Interactive 3D Graphics, 127-134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>72887</ref_obj_id>
				<ref_obj_pid>72885</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HELMAN, J., AND HESSELINK, L. 1989. Representation and display of vector field topology in fluid flow data sets. Computer 22, 8 (August), 27-36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375235</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[JOBARD, B., ERLEBACHER, G., AND HUSSAINI, M. 2000. Hardware-accelerated texture advection for unsteady flow visualization. In Proceedings IEEE Visualization 2000, 155-162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601678</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[JOBARD, B., ERLEBACHER, G., AND HUSSAINI, M. 2001. Lagrangian-eulerian advection for unsteady flow visualization. In Proceedings IEEE Visualization 2001, 53-60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[MAX, N., AND BECKER, B. 1995. Flow visualization using moving textures. In Proceedings of the ICASW/LaRC Symposium on Visualizing Time-Varying Data, 77-87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[PORTER, T., AND DUFF, T. 1984. Compositing digital images. Computer Graphics 18, 253-259. Proceedings SIGGRAPH'84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614396</ref_obj_id>
				<ref_obj_pid>614270</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[SHEN, H.-W., AND KAO, D. L. 1998. A new line integral convolution algorithm for visualizing time-varying flow fields. IEEE Transactions on Visualization and Computer Graphics 4, 2, 98-108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>236234</ref_obj_id>
				<ref_obj_pid>236226</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[SHEN, H.-W., JOHNSON, C., AND MA, K.-L. 1996. Visualizing vector fields using line integral convolution and dye advection. In Symposium on Volume Visualization, 63-70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218448</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[STALLING, D., AND HEGE, H.-C. 1995. Fast and resolution independent line integral convolution. In Proceedings of ACM SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, 249-256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122751</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[VAN WIJK, J. 1991. Spot noise: Texture synthesis for data visualization. Computer Graphics 25, 309-318. Proceedings ACM SIGGRAPH 91.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[VERSTAPPEN, R., AND VELDMAN, A. 1998. Spectro-consistent discretization of Navier-Stokes: a challenge to RANS and LES. Journal of Engineering Mathematics 34, 1, 163-179.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718500</ref_obj_id>
				<ref_obj_pid>647260</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[WEISKOPF, D., HOPF, M., AND ERTL, T. 2001. Hardware-accelerated visualization of time-varying 2D and 3D vector fields by texture advection via programmable per-pixel operations. In Vision, Modeling, and Visualization VMV '01 Conference Proceedings, 439-446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122719</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[WEJCHERT, J., AND HAUMANN, D. 1991. Animation aerodynamics. Computer Graphics 25, 19-22. Proceedings ACM SIGGRAPH 91.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311549</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[WITTING, P. 1999. Computational fluid dynamics in a traditional animation environment. In Proceedings of ACM SIGGRAPH 99, Computer Graphics Proceedings, Annual Conference Series, 129-136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>260860</ref_obj_id>
				<ref_obj_pid>260832</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Z&#214;CKLER, M., STALLING, D., AND HEGE, H.-C. 1997. Parallel line integral convolution. Parallel Computing 23, 7, 975-989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Based Flow Visualization Jarke J. van Wijk* Technische Universiteit Eindhoven Dept. of Mathematics 
and Computer Science Abstract A new method for the visualization of two-dimensional .uid .ow is presented. 
The method is based on the advection and decay of dye. These processes are simulated by de.ning each 
frame of a .ow an­imation as a blend between a warped version of the previous image and a number of background 
images. For the latter a sequence of .ltered white noise images is used: .ltered in time and space to 
re­move high frequency components. Because all steps are done us­ing images, the method is named Image 
Based Flow Visualization (IBFV). With IBFV a wide variety of visualization techniques can be emulated. 
Flow can be visualized as moving textures with line inte­gral convolution and spot noise. Arrow plots, 
streamlines, particles, and topological images can be generated by adding extra dye to the image. Unsteady 
.ows, de.ned on arbitrary meshes, can be han­dled. IBFV achieves a high performance by using standard 
features of graphics hardware. Typically .fty frames per second are gener­ated using standard graphics 
cards on PCs. Finally, IBFV is easy to understand, analyse, and implement. CR Categories: I.3.3 [Computer 
Graphics]: Picture/Image Gener­ation; I.3.6 [Computer Graphics]: Methodology and Techniques Interaction 
techniques; I.6.6 [Simulation and Modeling]: Simula­tion Output Analysis Keywords: Flow visualization, 
texture mapping, line integral con­volution 1 Introduction Fluid .ow plays a dominant role in many processes 
that are impor­tant to mankind, such as weather, climate, industrial processes, cool­ing, heating, etc. 
Computational Fluid Dynamics (CFD) simula­tions are carried out to achieve a better understanding and 
to improve the ef.ciency and effectivity of manmade artifacts. Visualization is indispensable to achieve 
insight in the large datasets produced by these simulations. Many methods for .ow visualization have 
been developed, ranging from arrow plots and streamlines to dense tex­ture methods, such as Line Integral 
Convolution [Cabral and Lee­dom 1993]. The latter class of methods produces very clear visual­izations 
of two-dimensional .ow, but is computationally expensive. We present a new method for the visualization 
of two­dimensional vector .elds in general and .uid .ow .elds in *e-mail: vanwijk@win.tue.nl Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
particular. The method provides a single framework to generate a wide variety of visualizations of .ow, 
varying from moving particles, streamlines, moving textures, to topological images. Three other features 
of the method are: handling of unsteady .ow, ef.ciency and ease of implementation. More speci.c, all 
512 × 512 images presented in this paper are snapshots from animations of unsteady .ow .elds. The animations 
were generated at up to 50 frames per second (fps) on a notebook computer, and the accom­panying DVD 
contains a simple but complete implementation in about a hundred lines of source code. The method is 
based on a simple concept: Each image is the re­sult of warping the previous image, followed by blending 
with some background image. This process is accelerated by taking advantage of graphics hardware. The 
construction of the background images is crucial to obtain a smooth result. All operations are on images, 
hence we coined the term Image Based Flow Visualization (IBFV) for our method. In the next section related 
work is discussed, and in section 3 the method is described and analysed extensively. The implementation 
and application are presented in section 4, in section 5 the results are discussed. Finally, conclusions 
are drawn. 2 Related work Many methods have been developed to visualize .ow. Arrow plots are a standard 
method, but it is hard to reconstruct the .ow from dis­crete samples. Streamlines and advected particles 
provide more in­sight. A disadvantage of these techniques is that the user has to de­cide where to position 
the startpoints of streamlines and particles, hence important features of the .ow can be overlooked. 
The visualization community has spent much effort in the devel­opment of more effective techniques. Van 
Wijk [1991] introduced the use of texture to visualize .ow. A spot noise texture is generated by inserting 
distorted spots with a random intensity at random loca­tions in the .eld, resulting in a dense texture 
that visualizes data. Cabral and Leedom [1993] introduced Line Integral Convolution (LIC), which gives 
a much better image quality. Per pixel a stream­line is traced, both upstream and downstream, along this 
streamline a random noise texture .eld is sampled and convolved with a .lter. Many extensions to and 
variations on these original methods, and especially LIC, have been published. The main issue is improve­ment 
of the ef.ciency. Both the standard spot noise and the LIC al­gorithm use a more or less brute force 
approach. In spot noise, many spots are required to achieve a dense coverage. In pure LIC, for each pixel 
a number of points on a streamline (typically 20-50) have to be computed. As a result, the computing 
time per frame is in the order of tens of seconds. One approach is to develop more ef.cient al­gorithms. 
Stalling and Hege [1995] achieved a higher performance by using a faster numerical method and a more 
ef.cient, streamline oriented scheme for the integration. Another approach to achieve a higher ef.ciency 
is to exploit hard­ware. Firstly, parallel processing can be used [de Leeuw and van Liere 1997; Z¨ockler 
et al. 1997; Shen and Kao 1998]. Secondly, graphics hardware can be used. De Leeuw et al. [1995] use 
texture SGI Indigo 1 0.02-0.05 SGI Indigo 1 0.23 Stalling and Hege [1995] SGI Onyx 1 4.0 Max and Becker 
[1995] SGI Onyx 8 5.6 De Leeuw et al.[1997] Cray T3D 64 20.0 Z¨ockler et al.[1997] SGI Onyx2 7 0.09 Shen 
and Kao [1998] SGI Octane 1 3.3 Heidrich et al.[1999] SGI Octane 1 2.5 Jobard et al.[2000] SGI Onyx2 
4 4.5 Jobard et al.[2001] PC, GeForce3 1 37.0 Weiskopf et al.[2001] IBFV, 2002 PC, GeForce2 1 49.3 Table 
1: Performance results for texture synthesis in frames per sec­ond. mapped polygons to render the spots. 
Max and Becker [1995] vi­sualized .ow by distorting images. An image is mapped on a rect­angular mesh, 
next either the texture coordinates or the vertex co­ordinates are changed. The mesh is rendered, and 
the mesh is dis­torted further. After a number of frames the distortion of the image accumulates, hence 
a new image is smoothly blended in, using alpha compositing [Porter and Duff 1984]. Heidrich et al. [1999] 
have presented the .rst version of LIC ac­celerated by graphics hardware. The integration of texture 
coordi­nates is delegated to the graphics hardware, and indirect addressing on a per-pixel basis via 
pixel textures [Hansen 1997] is used. They are used to store the velocity .eld and the texture coordinates. 
Pix­els are updated via backward texture advection: The color of a pixel is found by tracing a virtual 
particle backwards. Jobard et al. [2000] have further elaborated on the use of pixel textures. They presented 
a variety of extensions, including the han­dling of unsteady .ow, dye advection and feature extraction. 
Sev­eral problems obtained special attention. Edge effects emerge near in.ow boundaries. This is solved 
by adding new noise via image compositing. White noise is used as background, where per frame 3 % of 
the pixels are inverted to maintain high spatial frequencies at divergent areas. A sequence of frames 
(typically 10) is stored. At each time step the oldest frame is subtracted and the newest is added. All 
in all 18 texture applications per image are used. Re­cently Jobard et al. [2001] presented an alternative 
approach, which is a combination of a Lagrangian and an Eulerian approach: i.e. a mix of particles and 
noise. Again, backward texture advection is used. The method is implemented in a parallel fashion on 
a general purpose computer. Successive frames are blended. Finally, Weiskopf et al. [2001] have used 
the programmable per­pixel operations of a nVIDIA GeForce 3 card. Backward texture advection is used 
to show moving particles. Short pathlines can be shown by combining the four last frames. A summary of 
the performance results for the synthesis of dense textures for .ow visualization reported is given in 
Table 1. Procs. refers to the number of processors used. This table should not be taken too seriously, 
because the results are not completely compara­ble. The resolution of the images varies, some consider 
only steady .ow, and the image quality varies, for instance Weiskopf et al. pro­duce only animations 
of moving particles. Overall, we see that signi.cant progress has been achieved in the synthesis of texture 
for .ow visualization. Unsteady .ows can be handled, framerates in the order of 3 to 40 LIC images per 
second can be achieved. However, to achieve real-time frame rates a super­computer has to be used, or 
image quality has to be sacri.ced. Fram­erates in the order of 1 to 6 fps require special features of 
graphics hardware and/or parallel machines such as an SGI Onyx. For com­parison purposes, IBFV refers 
to the method presented in this article. IBFV uses advection of images via forward texture mapping on 
distorted polygonal meshes [Max and Becker 1995], in combina­tion with blending of successive frames 
[Jobard et al. 2001]. The combination of these two approaches has not been presented before, and we think 
it is close to optimal. In contrast, the use of graphics hardware (f.i. via pixel textures) to calculate 
new texture coordi­nates requires special features of the hardware, introduces quanti­zation problems, 
and requires interpolation of the velocity .eld on a rectangular grid. Advection of a .ow simulation 
mesh matches more naturally with higher level graphics primitives, i.e. polygons instead of pixels, in 
combination with the use of a general purpose processor to perform the necessary calculations. A major 
contribution of IBFV is the de.nition of the background images that are blended in. In other approaches 
[Jobard et al. 2000; Jobard et al. 2001] much work is spent afterwards to eliminate high frequency components. 
The approach used here is simpler: If the original images are low pass .ltered, then there is no need 
for such a step afterwards. 3 Method In this section we start with an informal introduction of IBFV,fol­lowed 
by a more precise description. Various aspects are analysed. In section 4 the model presented here is 
translated into an algorithm and mapped on graphics primitives. A discussion of the results can be found 
in section 5. 3.1 Concept How can we generate a frame sequence that shows an animation of .ow? Suppose 
we already have an image available (.g. 1). Typi­cally, the next frame will look almost the same. If 
we project the im­age on, say, a rectangular, mesh, move each mesh point over a short distance according 
to the local .ow and render this distorted image, we get an even better approximation [Max and Becker 
1995].  image k distort render blend k := k+1 Figure 1: Pipeline image based .ow visualization We could 
continue this process, but then several problems show up. If we distort the mesh further for the next 
frames, the accumu­lating deformation will lead to cells with shapes that are far from rectangular. This 
problem can easily be solved: We just repeat ex­actly the same step as before, each time starting with 
the original mesh. Nevertheless, the original image will be distorted more and more, and it will disappear 
from the viewport, advected by the .ow. This problem can be solved too: For each frame we take some new 
image and blend it with the distorted image [Jobard et al. 2001]. The next question is what new image 
to take. One could use a sequence of random white noise images, just like in standard LIC algorithms 
However, this leads to noisy animations. A better solu­tion is to use pink noise, i.e. to remove high 
frequency components from the background images, both in space and time. The preceding elements are the 
basic ingredients of Image Based Flow Visualization. Flow visualization methods like particle and streamline 
tracking operate in world space, and have geometric ob­jects like points and lines as primitives. The 
Line Integral Convolu­tion method starts from screen space: For each pixel a streamline is traced. We 
take complete images as our basic primitive. This leads to straightforward de.nitions and algorithms, 
which can be mapped effectively on graphics hardware. 3.2 Image generation Consider an unsteady two-dimensional 
vector .eld v(x;t) .IR2 v(x;t) =[vx (x, y;t),vy(x, y;t)]. (1) We assume it has been de.ned for t = 0, 
and for x . S,where S .IR2 . The region S is typically rectangular, but this is not essen­tial. We focus 
on .ow .elds, where v(x;t) represents a velocity, but other applications, such as potential .elds where 
v(x;t) represents a direction and a strength, fall within the scope as well. A pathline is obtained when 
we track the position of a particle in a dynamic .ow .eld. A pathline is the solution of the differential 
equation dp(t)/dt =v(p(t);t), (2) for a given start position p(0). A streamline has an almost identi­cal 
de.nition, except that here the velocity at a .xed time T is con­sidered. For a steady .ow .eld, pathlines 
and streamlines are the same. A .rst order approximation of equation (2) gives the well­known Euler integration 
method: pk+1 =pk +v(pk;t) t, (3) with k .IN and t =k t. In the remainder we use the frame number k as 
unit of time. Suppose we have a .eld F(x;k) that represents some property advected by the .ow. Here F 
represents an image, hence F(x;k) is typically an RGB-triplet. The property is advected just like a par­ticle, 
so F(p(t);t) is constant along a pathline p(t). A .rst order approximation of the transport of F can 
hence be given by: F(pk;k) if pk .S F(pk+1;k +1) =(4) 0 otherwise Here we set F(pk+1;k +1) to 0 (black) 
if the velocity at a previous point pk is unde.ned. Sooner or later most of F(x;k) for x .S will be black; 
for many points pk .S a corresponding start point p0 will be located outside S. To remedy this, at each 
time step we take a convex combination of F and another image G. Speci.cally, F(pk;k) =(1 -a)F(pk-1;k 
-1) +aG(pk;k), (5) where the points pk are de.ned by equation (3), and where a(x;k) . [0,1] de.nes a 
blending mask. For many applications a will be con­stant in time and space. Equation (5) is central. 
It de.nes the image generation process and forms a startpoint for analysis. The recurrency in (5) can 
be eliminated to give k-1 k F(pk;k) =(1 -a)kF(p0;0)+a(1 -a)iG(pk-i;k -i). (6) i=0 The .rst term represents 
the in.uence of the initial picture. This term can be ignored if we either start with a black picture 
F(x;0) or consider a large value for k. Hence we get k-1 k F(pk;k) =a(1 -a)iG(pk-i;k -i). (7) i=0 In 
other words, the color of a point pk of the image is the result of a line integral convolution of a sequence 
of images G(x;i) along a pathline through pk, with an exponential decay convolution .lter a(1 -a)i.A 
low a gives a high correlation along a pathline. This is a Eulerian point of view; We observe what happens 
at a point. We can also adopt a Lagrangian point of view; Consider what happens when we move along with 
a particle. Particles here are advected by the .ow and fade away exponentially. Jobard et al. [2001] 
made the observation that this can be interpreted phys­ically, and hence gives a natural effect. The 
particle moves two­dimensionally, and simultaneously sinks away with constant speed in a semi-transparent 
.uid. The next question is what images G and masks a to use. We .rst consider the synthesis of dense 
textures. A standard way to obtain these is to use random noise. In the next section we discuss spa­tial 
constraints on such noise, in section 3.4 we consider temporal characteristics. Two aspects of IBFV that 
require further attention are edge and contrast problems, which are discussed in section 3.5 and 3.6. 
In section 3.7 the injection of dye is discussed. 3.3 Noise: spatial To simplify the following discussion, 
without loss of generality, we consider a simple steady .ow .eld v(x;t) = [v,0]. As a result, pathlines 
are horizontal lines, and successive points pk are spaced d = v t apart. Furthermore, we consider a pathline 
through the origin, and assume that F and G are scalar functions (i.e. grey scale images). If we de.ne 
f and g as f (x;k) =F([x,0];k) (8) g(x;k) =G([x,0];k) (9) then equation (7) reduces to k-1 k f (x;k) 
=a(1 -a)ig(x -id;k -i) (10) i=0 We consider some special cases for g. First, let s suppose that g is 
a steady, rectangular pulse with width d and unit height, i.e. 1if |x|=d/2 g(x;k) =(11) 0 otherwise 
The corresponding f (x;k) for large k is a sequence of thin pulses, with exponentially decreasing height, 
spaced d apart (.g. 2). Obvi­ously, this is not what we want. Discretization of the time dimension gives 
a sequence of ghost images of the pulse. The desired smooth result is obtained if we set d =v t,and let 
t .0 f (x) =a(1 -a)x/d . (12) What are the requirements on g(x) to obtain such a smooth result? Obviously, 
this is a sampling problem, which can be studied in the frequency domain. A narrow pulse has a .at, white 
noise spectrum in its limit. Sampling such a signal gives aliasing artifacts. As an example, if we set 
g(x;k) =cos(2pnx/d), we get for the steady state f (x) =cos(2pnx/d). Hence, all components with a spatial 
frequency that is an integral multiple of the sampling frequency are not damped at all. Therefore, to 
prevent artifacts, g should not have high frequency components. Figure 2: Stepwise advected pulse We 
next consider this in the spatial domain. Let the function hw(x) represent a triangular pulse 1 -|x|/w 
if |x|=w hw(x)= (13) 0 otherwise and let g(x) = hw(x).For small w, the corresponding f (x) is a sequence 
of triangles with constant width and exponentially di­minishing height. If the triangles overlap, they 
are added up. How can we get a result that approximates the continuous result of equa­tion (12) satisfactorily? 
If we require that f (x)should be at least monotonously decreasing, the requirement we get is simply 
w=d, or, in general w=|v| t. (14) This leads to a convenient way to construct low-pass .ltered white 
noise. We use a linearly interpolated sequence of N random values Gi , with i =0,1,...,N -1, i.e. k g(x)= 
hs(x -is)Gi mod N, (15) where the spacing s satis.es s =|v| t. There are two ways to sat­isfy this constraint. 
First, a large value for s can be used. Figure 3(b) shows an example. The velocity .eld is modeled by 
a source in a linear .ow, moving from left to right. Left of the source a saddle­point is located, where 
the magnitude of the velocity is 0. A station­ary black-and-white noise was used, with s set to three 
pixels (.g. 3(a)). The texture shows both the direction and the magnitude of the local velocity. Concerning 
the latter: in regions with low veloc­ity the texture is isotropic, whereas it is stretched in high velocity 
areas. A second way to satisfy the constraint is to use a modi.ed velocity .eld v.(x;k), with .v(x;k)vmax/|v(x;k)| 
if |v(x;k)|>vmax v(x;k)= (16) v(x;k) otherwise in other words, we clamp the magnitude of the velocitity 
to an up­per limit vmax, with vmax t =s. If weuse a large valueof t and a low value for vmax, most of 
the velocity .eld will be clamped, i.e. magnitude information is ignored and only directional information 
is shown (.g. 3(c)). Or, in LIC terms, the .eld is sampled with points equidistant in geometric space, 
whereas in the previous way the points are equidistant in traveling time. In practice a combi­nation 
of both approaches is most convenient. Fields de.ned with sinks and sources can locally lead to high 
velocities, and the use of v.is a useful safeguard here. Another approach would be to normal­ize the 
velocity by dividing through max(|v|), but for .elds de.ned with sinks and sources this gives poor results. 
If we set vmax to high values, artifacts appear. For .g. 3(d) we set vmax t to nine pixels, and used 
a high value for t.Asa result, components in the background noise with a wavelength of nine, eighteen, 
etc. pixels show up prominently, leading to intricate knitwear like patterns. The scale parameter s can 
be used to in.uence the style of the visualization. In .gure 4 three visualizations of the same .eld 
with Figure 3: Source in linear .ow. From top to bottom: (a) background noise, (b) direction and magnitude, 
(c) direction only, (d) artifacts different settings for s are shown. Dependent on the scale, .ne LIC­style 
images, coarser spot noise images, to fuzzy, smoke-like pat­terns are produced. The latter may seem unattractive, 
but such pat­terns give a lively result when used for animations of unsteady .ow .elds. 3.4 Noise: temporal 
In the preceding section G(x;k)was assumed to be constant in time. This results in static images for 
steady .ow. How can we produce a texture that moves along with the .ow? The obvious answer is to vary 
G(x;k)as a function of time. We analyse this again using the one-dimensional model. We use a set of M 
images as background noise, linearly interpolated in space and discrete in time, i.e. k g(x;k)= hs(x 
-is)Gi mod N;k mod M, (17) with k again representing the frame number. One simple solution would be to 
use for each frame a different set of random values for Gik. This would spoil the animation however, 
because the variation along a path line is too strong. In spectral terms, too much high fre­quency is 
introduced. Another solution is to produce two random images Gi;0 and Gi;N/2, and to derive the other 
images via linear in­terpolation, similar to [Max and Becker 1995] . However, this so­lution lso falls 
short. This texture is not stationary in time, in the sense that each frame has statistically the same 
properties. As an example, the variation in magnitude will be higher for Gi;0 than for Gi;N/4, because 
the latter is an average of two samples. A convenient solution can be derived from the spot noise tech­nique. 
The intensity of each spot varies while it moves, and to each Figure 4: Scaling the texture. From top 
to bottom: (a) s =1.5, (b) s =3.0, (c) s =6.0 spot a different phase is assigned. Here we do the same: 
We con­sider each point Gi of the background texture as a spot or parti­cle that periodically glows up 
and decays according to some pro.le w(t). In other words Gi;k =w((k/M +fi ) mod 1) (18) where fi represents 
a random phase, drawn uniformly from the in­terval [0,1),and where w(t) is de.ned for t .[0,1]. We have 
experimented with various pro.les w(t). The use of cos 2px (.gure 5(a)) gives a result that is too soft 
and too diffuse, especially when seen in animations. A sharp edge in the pro.le is needed to generate 
contrast where the user can focus on. A simple choice is to use a square wave, i.e. w(t) =1if t < 1/2and 
0 else­where (.gure 5(b)). Advantages are that a high contrast is achieved and crisp images are generated. 
Let s have a closer look which pat­terns are generated. Consider a single point, linearly interpolated, 
which is set to 1 and 0 alternatingly for a number of frames. This produces a dashed line, which fades 
away in the direction of the .ow. And also, each dash itself fades away. This is somewhat un­natural: 
A standard convention in particle rendering is that the head should be the brightest and that the tail 
should faint. We can achieve this effect if we use w(t) =ßt .If we set ß<(1 -a)M the pulse falls off 
faster than the decay, and hence a texture with a decreasing intensity upstream is produced (.gure 5(c)). 
However, the exponen­tial decay has one disadvantage: the dynamic range is not used well during a cycle, 
leading to a poor contrast. A convenient alternative here is the sawtooth w(t) =1 -t, which produces 
a similar effect and uses the dynamic range more effectively (.gure 5(d)). The ef­fect is strongest for 
animations, but also in still images the direction of the .ow can be discerned. Finally, we assumed so 
far that for each new frame a dif­ferent background image is used, for frame k we used pattern G(x;k 
mod N). We can also use G(x;L(vgk) mod NJ),where vg denotes the rate of image change. If N is large enough, 
different frequencies for background image variation can be modeled with­out visible artifacts using 
the same set of background images. Figure 5: Different pro.les w(t). From top to bottom: (a) cosine, 
(b) square, (c) exponential decay, (d) saw tooth. 3.5 Contrast One typical problem of .ow visualization 
using texture mapping is the reduced contrast, and, as can be observed from .gure 5, the method presented 
here is no exception. What happens with the con­trast? Consider equation (10). The value of f is a sum 
of scaled samples of g. Let us assume that the values of the samples of g are independent (i.e. spaced 
s apart). We .nd then, using standard rules for summation and scaling of a number of independent variates, 
for the average µf and variance s2 f : µf = µg (19) a s2 f = sg 2 . (20) 2 -a Hence, low values for a 
reduce the contrast signi.cantly. For in­stance, a =0.1 reduces sf with about 77 percent, so a high value 
for sg 2 is welcome. Another result from statistics is that the histogram of the intensities approaches 
a normal distribution, according to the central limit theorem. One way to solve this problem is to postprocess 
the images. We have implemented this: Images can optionally be passed through a histogram equalization. 
For a sequence of textured images the his­togram is fairly stable, so the calculation of the table has 
to be done only once. The result is however somewhat disappointing: Scal­ing up the narrow range of intensities 
gives rise to quantization ar­tifacts. The use of more bits per color component per pixel (eight here) 
could remedy this. Also, other, more sophisticated techniques, like the removal of low pass components 
[Shen and Kao 1998] could be used. Most often however, we .nd that the textures look nice enough on the 
screen, especially when they are used as a background for dye-based techniques, and that the reduction 
in frame rate is not worth the improvement in contrast. For none of the images pre­sented in this paper 
extra contrast enhancement was used, so that the impact on the contrast can be compared for the various 
variations. 3.6 Boundary areas Another issue that deserves extra attention is the treatment of bound­ary 
areas. Suppose that the .ow domain S originally coincides with the image as viewed on the screen, and 
let S. be the distorted .ow domain (.g. 6): S.={x +v t|x .S}. (21) We de.ne the boundary area B as S 
-S. . B S Figure 6: Boundary area B =S -S. The use of eq. (4) and (5) comes down to the following proce­dure: 
1) clear the screen; 2) render the previous image on a warped mesh; 3) blend in a background image. This 
procedure gives rise to artifacts (.g. 7 (a)). Area B is almost black for low a, and this leaks in from 
the boundary into the image. A better solution is to use a modi.ed version of eq. (5) F(pk ;k) =(1 -a)F* 
(pk ;k -1) +aG(pk ) (22) F(pk ;k -1) if pk .B with F* (pk ;k -1) = F(pk-1;k -1) otherwise Stated differently, 
we just do not clear the screen when we generate a new frame. We can show that now the average value 
is constant. Equation (22) can be modeled by setting G(x;t) to 0 for x < 0. The boundary area B is here 
the interval [0, d). The equivalent of eq. (7) is then for k .8 8m-1 kk f (x) =a(1 -a)ig(x -md)+a(1 -a)ig(x 
-id) (23) i=mi=0 with m =Lx/dJ. The boundary area B is not advected, just con­stantly blended with new 
values of g. The average value of f is the same as the average value of g. For the variance s2 f for 
a constant g in time, we .nd however 2(1 -a)2m+1 +a s2 f = sg 2 . (24) 2 -a In the boundary area the 
samples of g are all the same, hence for m = 0 s2 = s2, with increasing m the variance s2 approaches 
fg f sg 2a/(2 -a). In other words, the closer to the in.ow boundary, the higher the contrast. Figure 
7(b) shows this artifact. A linear .ow .eld was used, v t was 6 pixels, the texture scale s also to 6 
pixels, a =0.1. This is a critical setting, near the boundary aliasing arti­facts show up. In practice 
we use lower values vmax t, typically 2-3 pixels, and a time varying G, and as a result the boundary 
artifacts are almost invisible. Also, these artifacts can be removed by using a slightly larger image 
than the image presented on the screen. Figure 7: Boundary artifacts: black boundaries (top), increasing 
contrast towards edge (bottom). 3.7 Dye injection The injection of dye is a convenient metaphor for 
many .ow visu­alization methods. Shen et al. [1996] and Jobard et al. [2000] have already used dye injection 
in the context of LIC. Dye injection .ts well in the model. Injection of dye with color GD and blending 
mask aD to an image F can be modeled as F.(x;k) =(1 -aD (x;k))F(x;k) +aD(x;k)GD (x;k) (25) Multiple dye 
sources can be modeled by repetition of this step. The .nal modi.ed image F.(x;t) is next used in the 
advection step, ad­vecting the injected dye as well. Note that aD (x;k) varies with space. If we want 
to add a circular dye source to the image, we just draw a .lled circle in the image, thereby implicitly 
using a mask that is 1 over the area of the circle and 0 elsewhere. Furthermore, both a and GD can vary 
with time. If they are static, the result is a trace of dye, exponentially decaying. By varying them, 
we get objects that move and distort with the .ow. The same pro.les w(t) as used for the background noise 
can be used, where the length of one cycle can be set independently from that of the background noise. 
  4 Implementation So far we have presented a model, in this section we consider its implementation. 
In section 4.1 an algorithm is presented, in section 4.2 an application is described, and section 4.3 
gives performance results. 4.1 Algorithm All aspects of the model can easily be mapped on graphics primi­tives 
and graphics operations. The image F is represented as a rect­angular array of pixels Fij ,i = 0..NX 
-1, j = 0..NY -1. We use the framebuffer to store the image. The background images G are represented 
by a sequence of images Gkij , with k =0..M -1 and i, j =0..N -1, .lled according to equation (18). Typically 
NX =NY = 512, N = 64, and M = 32. The background patterns only have to be recalculated if the time pro.le 
w(t) has changed, and can be stored in texture memory. For the advection of the image we use a tesselation 
of S. For instance a rectangular mesh, represented by an array Rij , i, j =0..Nm , where for each array 
element the coordinates of the advected grid point are stored. We typically use Nm =100. Other meshes 
could also be used, triangular meshes for instance lend themselves also well to fast implementations. 
If the velocity can be calculated analytically for arbitrary points, the implementor is free to choose 
the most ef.cient mesh; If the velocity is given for a mesh already, it is obviously most convenient 
to use that mesh. Also, time varying meshes can be handled without extra complexity. The algorithm to 
generate frame k of an animation now proceeds as follows. 1. If the .ow .eld has changed, calculate a 
distorted mesh R; 2. Render R on the screen, texture mapped with the previous im­age; 3. Overlay a 
rectangle, texture mapped with noise pattern kmod M, blended with a factor a, whereas the image is weighted 
with 1 - a; 4. Draw dye to be injected; 5. Save the image in texture memory; 6. Draw overlaid graphics. 
 A few words per step. The calculation of the distorted mesh is done per grid point Rij by calculating 
a displacement d = v(Rij, k) t, which is clamped if |d| exceeds a threshold, and adding this dis­placement 
to Rij. In step 2 the texture coordinates are set evenly dis­tributed between 0 and 1. In step 3 the 
background noise is blended into the image. The texture coordinates are set such that the spacing of 
the points of the pattern equals s. Linear interpolation is done by the hardware. In step 4 dye is injected. 
This can be done in many ways, typically by drawing a shape or overlaying an extra image. In step 5 the 
result is copied to texture memory ready for the next frame. Finally, overlaid graphics that should not 
be advected, such as markers and time indicators, are drawn. One little problem of step 4 and 6 is that 
when something is drawn in the boundary region B, this is not removed in step 1 for the next frame. As 
a result, such dye is persistent and leaks into the .ow. A simple solution is just not to render additional 
imagery too close to the boundary of the image or to display the computed image minus its border. Nearly 
all steps match very well with graphics hardware. The main action is the rendering of texture mapped 
quadrilaterals, pos­sibly blended. Modern graphics hardware, such as which can be found in standard PCs 
nowadays, is made exactly for this purpose, and hence high speeds can be achieved. The algorithm can 
be im­plemented using only OpenGL1.1 calls, without extensions, hence the portability is high. As an 
illustration of the simplicity and porta­bility of IBFV, the accompanying DVD contains a minimal but 
com­plete texture based .ow visualization demo in the form of about one hundred lines of C-code. This 
enables the reader to make a quick start and to experiment by varying the parameters and extending vi­sualization 
options. The only step that does not take advantage of the graphics hard­ware is the calculation of the 
distorted mesh. However, this is a great task for the central processor. These calculations are most 
easily done in .oating point arithmetic, and data must be calculated, re­trieved from disk or from a 
network. The use of graphics hardware for this purpose does not pay off.  4.2 Results We have implemented 
IBFV in an interactive .ow simulation and vi­sualization system. The application was implemented in Delphi 
5, using ObjectPascal. It consists of about 3,500 lines of code, most of which concerned user interfacing. 
For the modeling of the .ow a potential .ow model, in line with [Wejchert and Haumann 1991], was used. 
A .ow .eld is de.ned by superposition of a linear .ow  vi(x; t) = . si ri -ri si . d |d|2 , (26) with 
d = x - pi, to the .ow .eld. Figure 8 shows a screen-shot of the user interface. The user can interactively 
add and remove .ow elements and change their prop­erties. To generate a time dependent .ow .eld, to each 
.ow element a velocity is assigned, such that the .ow element moves through the .eld. Each .ow element 
can optionally produce dye, various at­tributes of the circles to be drawn can be set. The image is overlaid 
here with an extra transparent texture, colored according to the mag­nitude of the velocity, similar 
to [Jobard et al. 2000]. Many options for visualization were implemented. With different variations of 
dye injection a wide variety of effects can be achieved. We present these using the same .ow .eld. All 
images have a 512×512 resolution, for the .ow .eld a 100×100 mesh was used. Figure 9 shows two visualizations 
with texture. The scale of the texture can be varied, left we used a coarse texture, on the right a .ne 
texture; by changing t and vmax either both the magnitude and direction of the velocity (left) or only 
the direction can be shown. In the latter version saddle points can be located more easily. A simple 
model for injecting dye is to use a rectangular grid, pos­sibly jittered, and to draw a circle on the 
gridpoints, either continu­ously or intermittently. Figure 10(a) shows that continuous release gives 
arrow-like glyphs. In .gure 10(b) particles are released, using an exponential decay pro.le. This gives 
lively animations. Near the sources (indicated with a red marker) particles are blown away, in other 
areas dye accumulates.  Arbitrary images can be inserted in the .eld. Figure 11 shows what happens 
if a picture of a grid is smeared (continuous release with low a) or warped (release once, with a = 1) 
by the .ow. The method presented here can be used to apply artistic effects on arbi­trary images by adding 
sources, sinks, and vortices, and by overlay­ing texture. Topological analysis [Helman and Hesselink 
1989] aims at seg­menting the .ow area into distinct areas, such that .ow is con­strained within the 
area. Here, streamlines can start only at the boundary or at sources. If we now paint the boundary and 
let each source produce a differently colored dye, automatically a topolog­ical decomposition is achieved 
when the dye is advected (.g. 12). For the .ow model used here this process can be fully automated, because 
sources are modeled and known explicitly, for arbitrary .ow .elds this can be realized by searching for 
points where the velocity magnitude vanishes. Combination of this technique with a soft texture (a = 
0.01 - 0.02, .g. 12(b)) gives an especially at­tractive result. The texture indicates the .ow, the color 
of the area faints with increasing distance from the source or boundary. In.ow from a boundary can be 
studied by painting strips along the boundary (.gure 13). If a pattern for the boundary is used, streamlines 
appear; intermittent painting gives so called time lines. Furthermore, on the right image three user 
positioned dye sources are shown. We have applied the method to visualize the result of CFD simu­lations. 
Figure 14 shows a 2D slice from a 3D time dependent sim­ulation of turbulent .ow around a block [Verstappen 
and Veldman 1998]. A rectilinear 536 × 312 grid with a strongly varying den­sity was used. This grid 
could be handled without special problems. We only added an option to zoom and pan on the data set, such 
that the turbulent .ow can be inspected at various scales. In .gure 14 we zoom in on a corner of the 
block, thereby reveiling intricate de­tail. When suf.ciently close (i.e. about 50 × 50 gridlines in view), 
Figure 12: (a) Topology, (b) Topology with texture zooming and panning on the animated .ow can be done 
smoothly at 30 40 fps. All applications presented here were easy to implement. All that has to be done 
is injection of dye, and the remainder is done by the underlying process. Almost no data structures are 
needed, for in­stance, there is no need to keep track of the position of advected particles. Another 
strong effect of real-time animation is that it be­comes interesting to make all parameters for dye injection 
time­dependent. Positions, colors, shapes, can all be de.ned as functions of time, leading to interesting 
special effects.  4.3 Performance The images shown were made on a Dell Inspiron 8100 notebook PC, running 
Windows 2000, with a Pentium III 866 MHz processor, 256 MB memory, and a nVidia GeForce2Go graphics card 
with 32 MB memory. The following table shows framerates in fps for four differ­ent values of the mesh 
resolution Nm and for three different con.g­urations. A distinction is made between steady .ow (mesh 
requires no recalculation), one moving .ow element (mesh requires recalcu­lation), and seven moving .ow 
elements (the con.guration of .g­ure 9 to 12). As test animation a moving black and white texture was 
used, but there is little difference for the various .ow visualiza­tion styles. The only exceptions are 
the use of a transparent velocity overlay or the use of a constant extra image (see .gure 11(a)), which 
steady one source seven sources mesh 50×50 49.3 49.3 49.3 100×100 49.3 37.0 27.0 200×200 49.3 14.6 9.1 
512×512 18.5 2.5 1.4 Table 2: Frames per second, 512×512 images cost some 10 to 50% extra, dependent 
on the resolution of the mesh or image. What does this table show? Above all, IBFV is fast. Unsteady 
.ow is visualized via moving textures in real-time. Parameters can be changed on the .y, the effect of 
changes is smooth and immediate. The maximum speed of the .ow as shown on the screen is typically in 
the order of 150 pixels per second, hence lively animations result. We have tried our application on 
several other machines. On a PC running Windows 98 with a 350 MHz processor, 128 MB memory, and a graphics 
card with a nVidia TNT processor and 16 MB mem­ory we achieved a frame rate of 22 fps for the standard 
case. In other words, also on somewhat older machines real-time frame rates can be achieved. In the table 
from left to right the amount of work for the proces­sor increases with more moving sources. For steady 
.ow the same mesh can be reused, for dynamic .ow it has to be recalculated. De­pendent on the complexity 
of the .eld (one vs. seven sources) the process slows down. From top to bottom the amount of work for 
the graphics hardware increases with increasing mesh resolution: More coordinates have to be transformed. 
For steady .ow the amount of work for the processor is independent of the mesh resolution, for dy­namic 
.ow it depends on the complexity of the .eld. The impact of variation of the mesh resolution on the result 
is lim­ited. Actually, for the .elds modeled here no differences can be seen in animations. As an example, 
.gure 15 shows a dye source at a dis­tance of 100 pixels from a vortex. The left image was made using 
Nm = 50, for the right Nm = 512 was used. The dye source pro­duces an almost perfect circle. We do see 
that some diffusion shows up, due to resampling of the distorted mesh on the regular pixel grid of the 
image. This smoothing has also a very positive aspect: Stan­dard aliasing artifacts such as jagged edges 
never show up. Anyhow, these images are quite arti.cial, for texture visualization the visible trace 
of a particle is much shorter. We did not perform yet a formal assessment of the accuracy of IBFV, but 
judging from test images such as .g. 15, it does not seem an urgent problem. The use of a more advanced 
ODE solver than the very simple Euler scheme used here could give further improve-ments if necessary. 
The topology method is interesting from the viewpoint of stability. Standard approaches require careful 
pro­gramming, to make sure that streamlines starting from critical points end up in other critical points. 
IBFV produces stable results, also for hard cases like a pure vortex. The result here is that the boundary 
of the vortex area and the remainder of the .eld is blurred and smooth, which seems to be a natural representation 
for this situation.  5 Discussion 5.1 Ef.ciency IBFV is about one to three orders of magnitude faster 
than other meth­ods for the synthesis of dense texture animations of .ow (see ta­ble 1). This ef.ciency 
comes from four different effects. Firstly, graphics hardware is used. The warping, blending, and linear 
inter­polation of images is taken care of by the graphics hardware. Com­munication between graphics hardware 
and processor is limited to the coordinates of warped meshes, frames are copied directly to tex­ture 
memory. Secondly, the number of steps in the generation of a single image is limited. Only two full-screen 
operations are used per frame. Thirdly, frame to frame coherence is exploited. Fourthly, the velocity 
.eld can typically be sampled at a lower resolution than the image resolution, where the central processor 
takes care of velocity .eld calculations. We see no simple options to increase the performance further. 
One limit is the refresh rate of the monitor used. Another limit is the simplicity of the algorithm: 
The inner loop is almost empty. In the near future we will experiment with higher image reso­lutions 
to see if the same frame rates are achieved. Limitations in texture memory will not be a problem. The 
total amount of texture memory required is 3Nx Ny bytes for the image, and 4N2 M bytes for the background 
patterns. For the set-up used here, in total 1.25 MB is used, which .ts well within graphics memory. 
 5.2 Versatility IBFV can be used for arbitrary .ow .elds. So far, we have used IBFV for unsteady .ow 
.elds with high vorticity de.ned for rectangular meshes, but we expect that arbitrary triangular meshes, 
and even time varying meshes can be handled without special measures. IBFV can be used to produce a variety 
of different types of visu­alizations. Various kinds of texture can be produced. These can be de.ned 
with a few parameters: the scale s of the texture, the type of time pro.le w(t), the image variation 
speed vg and the blending fac­tor a. Dependent on these settings, LIC, spot noise, or smoke like textures 
are produced. Changes in parameter settings can be judged in real-time, hence tuning can be done ef.ciently. 
Furthermore, in­jection of dye and images can be used to generate effects like stream­ 5.3 Simplicity 
Maybe the main virtue of IBFV is its simplicity (or elegance?), inde­pendent of the point of view. IBFV 
is based on injection, advection, and decay of dye. It can be described mathematically in a few sim­ple 
equations. It is based on warping and blending images. It can be coded compactly using a few standard 
OpenGL calls. This is attrac­tive for application purposes, but also for its analysis. Issues such as 
contrast and boundary artifacts can be analysed precisely. 5.4 Future work We have applied IBFV to external 
two-dimensional CFD datasets. A next step is the use of the method for three-dimensional simulations. 
Slicing through 3D rectangular data sets does not seem to bring in special problems. Also, generation 
of texture on arbitrary surfaces in 3D should be feasible. A larger step is to transpose the 2D im­ages 
to 3D volumes. In [Weiskopf et al. 2001] such an approach is presented for particles. The ef.ciency will 
depend critically on the capabilities of the graphics hardware. Besides .ow visualization for scienti.c 
purposes, other applica­tions can be foreseen. A system such as presented here is attrac­tive for educational 
purposes. Games and animations could use IBFV to generate smoke and other special effects. The .ow modeling 
method used here is already effective to create and draw fairly ar­bitrary .ow .elds. In combination 
with a more sophisticated CFD method, such as presented in [Witting 1999], a higher realism can be achieved. 
The method produces cyclic imagery (after transient effects) for steady .ow. These can be used as animated 
gifs to liven up web pages. The method can also be used for artistic effects on arbitrary images. Finally, 
also the method itself can be extended. So far we have dealt with injection of dye, decay of dye and 
advection. A similar method could be derived to handle anisotropic diffusion.  6 Conclusions We have 
presented a simple, ef.cient, effective, and versatile method for the visualization of two-dimensional 
.ow. Unsteady .ow on arbitrary meshes is visualized as animations of texture, vec­tors, particles, streamlines, 
and/or advected imagery at .fty frames per second on a PC, using standard features of consumer graphics 
hardware. As there remains little to be desired, we think we are close to having solved the problem of 
visualization of two-dimensional .ow.  Acknowledgements I thank Ion Barosan, Alex Telea, Huub van de 
Wetering, and Frank van Ham (TU/e), for their inspiring and constructive support during the preparation 
of this paper, and Roel Verstappen, Arthur Veldman (RuG), and Wim de Leeuw (CWI) for providing highly 
challenging turbulent .ow simulation data. References CABRAL,B., AND LEEDOM,L.C. 1993. Imagingvector.eldsus­ing 
line integral convolution. In Proceedings of ACM SIGGRAPH 93, Computer Graphics Proceedings, Annual Conference 
Series, 263 272. DE LEEUW,W., AND VAN LIERE, R. 1997. Divide and conquer spot noise. In Proceedings SuperComputing 
97. DE LEEUW,W., AND VAN WIJK, J. 1995. Enhanced spot noise for vector .eld visualization. In Proceedings 
IEEE Visualization 95. HANSEN, P. 1997. Introducing pixel texture. Developer News, 23 26. Silicon Graphics 
Inc. HEIDRICH,W., WESTERMANN,R., SEIDEL,H.-P., AND ERTL, T. 1999. Applications of pixel textures in visualization 
and re­alistic image synthesis. In ACM Symposium on Interactive 3D Graphics, 127 134. HELMAN,J., AND 
HESSELINK, L. 1989. Representation and dis­play of vector .eld topology in .uid .ow data sets. Computer 
22, 8 (August), 27 36. JOBARD,B., ERLEBACHER,G., AND HUSSAINI, M. 2000. Hardware-accelerated texture 
advection for unsteady .ow visu­alization. In Proceedings IEEE Visualization 2000, 155 162. JOBARD,B., 
ERLEBACHER,G., AND HUSSAINI, M. 2001. Lagrangian-eulerian advection for unsteady .ow visualization. In 
Proceedings IEEE Visualization 2001, 53 60. MAX,N., AND BECKER, B. 1995. Flow visualization using mov­ing 
textures. In Proceedings of the ICASW/LaRC Symposium on Visualizing Time-Varying Data, 77 87. PORTER,T., 
AND DUFF, T. 1984. Compositing digital images. Computer Graphics 18, 253 259. Proceedings SIGGRAPH 84. 
SHEN,H.-W., AND KAO, D. L. 1998. A new line integral convo­lution algorithm for visualizing time-varying 
.ow .elds. IEEE Transactions on Visualization and Computer Graphics 4,2,98 108. SHEN,H.-W.,JOHNSON,C., 
AND MA, K.-L. 1996. Visualizing vector .elds using line integral convolution and dye advection. In Symposium 
on Volume Visualization, 63 70. STALLING,D., AND HEGE, H.-C. 1995. Fast and resolution independent line 
integral convolution. In Proceedings of ACM SIGGRAPH 95, Computer Graphics Proceedings, Annual Con­ference 
Series, 249 256. VAN WIJK, J. 1991. Spot noise: Texture synthesis for data visu­alization. Computer Graphics 
25, 309 318. Proceedings ACM SIGGRAPH 91. VERSTAPPEN,R., AND VELDMAN, A. 1998. Spectro-consistent discretization 
of Navier-Stokes: a challenge to RANS and LES. Journal of Engineering Mathematics 34, 1, 163 179. WEISKOPF,D., 
HOPF,M., AND ERTL, T. 2001. Hardware­accelerated visualization of time-varying 2D and 3D vector .elds 
by texture advection via programmable per-pixel operations. In Vision, Modeling, and Visualization VMV 
01 Conference Pro­ceedings, 439 446. WEJCHERT,J., AND HAUMANN, D. 1991. Animation aerody­namics. Computer 
Graphics 25, 19 22. Proceedings ACM SIG-GRAPH 91. WITTING, P. 1999. Computational .uid dynamics in a 
traditional animation environment. In Proceedings of ACM SIGGRAPH 99, Computer Graphics Proceedings, 
Annual Conference Series, 129 136. Z¨OCKLER,M., STALLING,D., AND HEGE, H.-C. 1997. Parallel line integral 
convolution. Parallel Computing 23, 7, 975 989.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>566647</section_id>
		<sort_key>755</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Painting and non-photorealistic graphics]]></section_title>
		<section_page_from>755</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42053465</person_id>
				<author_profile_id><![CDATA[81545854656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>566648</article_id>
		<sort_key>755</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[WYSIWYG NPR]]></title>
		<subtitle><![CDATA[drawing strokes directly on 3D models]]></subtitle>
		<page_from>755</page_from>
		<page_to>762</page_to>
		<doi_number>10.1145/566570.566648</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566648</url>
		<abstract>
			<par><![CDATA[We present a system that lets a designer directly annotate a 3D model with strokes, imparting a personal aesthetic to the non-photorealistic rendering of the object. The artist chooses a "brush" style, then draws strokes over the model from one or more viewpoints. When the system renders the scene from any new viewpoint, it adapts the number and placement of the strokes appropriately to maintain the original look.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[interactive techniques]]></kw>
			<kw><![CDATA[non-photorealism]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P382461</person_id>
				<author_profile_id><![CDATA[81100305590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Kalnins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026749</person_id>
				<author_profile_id><![CDATA[81100387968]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Markosian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P27729</person_id>
				<author_profile_id><![CDATA[81100412576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Meier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P197718</person_id>
				<author_profile_id><![CDATA[81100168324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Kowalski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP310126800</person_id>
				<author_profile_id><![CDATA[81544111256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14044122</person_id>
				<author_profile_id><![CDATA[81100097816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Davidson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43115173</person_id>
				<author_profile_id><![CDATA[81342515444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Webb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024462</person_id>
				<author_profile_id><![CDATA[81100166298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BOURDEV, L. 1998. Rendering Nonphotorealistic Strokes with Temporal and Arc-Length Coherence. Master's thesis, Brown University. www.cs.brown.edu/research/graphics/art/bourdev-thesis.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BOURGUIGNON, D., CANI, M. P., AND DRETTAKIS, G. 2001. Drawing for illustration and annotation in 3D. In Computer Graphics Forum, Blackwell Publishers, vol. 20:3, 114-122.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BRAND, M., AND HERTZMANN, A. 2000. Style machines. Proceedings of SIGGRAPH 2000, 183-192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340927</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COHEN, J. M., HUGHES, J. F., AND ZELEZNIK, R. C. 2000. Harold: A world made of drawings. Proceedings of NPAR 2000, 83-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CURTIS, C. J., ANDERSON, S. E., SEIMS, J. E., FLEISCHER, K. W., AND SALESIN, D. H. 1997. Computer-generated watercolor. Proceedings of SIGGRAPH 97, 421-430.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344792</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DEUSSEN, O., AND STROTHOTTE, T. 2000. Computer-generated pen-and-ink illustration of trees. Proc. of SIGGRAPH 2000, 13-18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732296</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DURAND, F., OSTROMOUKHOV, V., MILLER, M., DURANLEAU, F., AND DORSEY, J. 2001. Decoupling strokes and high-level attributes for interactive traditional drawing. In 12th Eurographics Workshop on Rendering, 71-82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508550</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DURAND, F. 2002. An invitation to discuss computer depiction. Proceedings of NPAR 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A., AND LEUNG, T. K. 1999. Texture synthesis by non-parametric sampling. In IEEE International Conference on Computer Vision, 1033-1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FREEMAN, W. T., TENENBAUM, J. B., AND PASZTOR, E. 1999. An example-based approach to style translation for line drawings. Tech. Rep. TR99-11, MERL, Cambridge, MA. http://www.merl.com/papers/TR99-11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GOOCH, B., AND GOOCH, A. 2001. Non-Photorealistic Rendering. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300526</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GOOCH, B., SLOAN, P.-P. J., GOOCH, A., SHIRLEY, P., AND RIESENFELD, R. 1999. Interactive technical illustration. 1999 ACM Symposium on Interactive 3D Graphics, 31-38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND HAEBERLI, P. 1990. Direct WYSIWYG painting and texturing on 3D shapes. Proc. of SIGGRAPH 90, 215-223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345074</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., AND ZORIN, D. 2000. Illustrating smooth surfaces. Proceedings of SIGGRAPH 2000, 517-526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[IGARASHI, T., MATSUOKA, S., AND TANAKA, H. 1999. Teddy: A sketching interface for 3d freeform design. Proc. of SIGGRAPH 99, 409-416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311607</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KOWALSKI, M. A., MARKOSIAN, L., NORTHRUP, J. D., BOURDEV, L., BARZEL, R., HOLDEN, L. S., AND HUGHES, J. F. 1999. Art-based rendering of fur, grass, and trees. Proc. SIGGRAPH 99, 433-438.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340918</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LAKE, A., MARSHALL, C., HARRIS, M., AND BLACKSTEIN, M. 2000. Stylized rendering techniques for scalable real-time 3D animation. Proceedings of NPAR 2000, 13-20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618268</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LANSDOWN, J., AND SCHOFIELD, S. 1995. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics and Applications 15, 3, 29-37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MARKOSIAN, L., KOWALSKI, M. A., TRYCHIN, S. J., BOURDEV, L. D., GOLDSTEIN, D., AND HUGHES, J. F. 1997. Real-time nonphotorealistic rendering. Proceedings of SIGGRAPH 97, 415-420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340924</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MARKOSIAN, L., MEIER, B. J., KOWALSKI, M. A., HOLDEN, L. S., NORTHRUP, J. D., AND HUGHES, J. F. 2000. Art-based rendering with continuous levels of detail. Proc. of NPAR 2000, 59-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MASUCH, M., SCHLECHTWEG, S., AND SCH&#214;NW&#196;LDER, B. 1997. daLi! --- Drawing Animated Lines! In Proceedings of Simulation und Animation '97, SCS Europe, 87-96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MEIER, B. J. 1996. Painterly rendering for animation. In Proceedings of SIGGRAPH 96, ACM SIGGRAPH / Addison Wesley, Computer Graphics Proceedings, Annual Conference Series, 477-484.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563684</ref_obj_id>
				<ref_obj_pid>563666</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MEIER, B. 1999. Computers for artists who work alone. Computer Graphics 33, 1 (February), 50-51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MITANI, J., SUZUKI, H., AND KIMURA, F. 2000. 3D Sketch: Sketch-based model reconstruction and rendering. Seventh IFIP WG 5.2 Workshop on Geometric Modeling. http://kaemart.unipr.it/geo7/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340920</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[NORTHRUP, J. D., AND MARKOSIAN, L. 2000. Artistic silhouettes: A hybrid approach. Proceedings of NPAR 2000, 31-38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>792857</ref_obj_id>
				<ref_obj_pid>792756</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[POULIN, P., RATIB, K., AND JACQUES, M. 1997. Sketching shadows and highlights to position lights. In Proceedings of Computer Graphics International 97, IEEE Computer Society, 56-63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383328</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PRAUN, E., HOPPE, H., WEBB, M., AND FINKELSTEIN, A. 2001. Real-time hatching. Proceedings of SIGGRAPH 2001, 579-584.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SCH&#214;DL, A., SZELISKI, R., SALISIN, D., AND ESSA, I. 2000. Video textures. Proceedings of SIGGRAPH 2000, 489-498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166135</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SCHOENEMAN, C., DORSEY, J., SMITS, B., ARVO, J., AND GREENBERG, D. 1993. Painting with light. In Proc. of SIGGRAPH 93, 143-146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563685</ref_obj_id>
				<ref_obj_pid>563666</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SEIMS, J. 1999. Putting the artist in the loop. Computer Graphics 33, 1 (February), 52-53.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[STROTHOTTE, T., PREIM, B., RAAB, A., SCHUMANN, J., AND FORSEY, D. R. 1994. How to render frames and influence people. Computer Graphics Forum 13, 3, 455-466.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364342</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[TOLBA, O., DORSEY, J., AND MCMILLAN, L. 2001. A projective drawing system. In ACM Symposium on Interactive 3D Graphics, 25-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[WEI, L.-Y., AND LEVOY, M. 2000. Fast texture synthesis using tree-structured vector quantization. Proc. SIGGRAPH 2000, 479-488.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WINKENBACH, G., AND SALESIN, D. H. 1994. Computer-generated pen-and-ink illustration. Proceedings of SIGGRAPH 94, 91-100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237238</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[ZELEZNIK, R. C., HERNDON, K. P., AND HUGHES, J. F. 1996. SKETCH: An interface for sketching 3D scenes. Proceedings of SIGGRAPH 96, 163-170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WYSIWYG NPR: Drawing Strokes Directly on 3D Models Robert D. Kalnins1 Lee Markosian1 Barbara J. Meier2 
Michael A. Kowalski2 Joseph C. Lee2 Philip L. Davidson1 Matthew Webb1 1Princeton University Abstract 
We present a system that lets a designer directly annotate a 3D model with strokes, imparting a personal 
aesthetic to the non-photorealistic rendering of the object. The artist chooses a brush style, then draws 
strokes over the model from one or more viewpoints. When the system renders the scene from any new viewpoint, 
it adapts the number and placement of the strokes appropriately to maintain the original look. Keywords: 
Interactive techniques, non-photorealism. 1 Introduction Artists and designers apply techniques of 3D 
computer graphics to create images that communicate information for some purpose. Depending on that purpose, 
photorealistic imagery may or may not be preferred. Thus, a growing branch of computer graphics research 
focuses on techniques for producing non-photorealistic renderings (NPR) from 3D models. Strong arguments 
for the usefulness of this line of inquiry are made by many researchers (e.g., [Durand 2002; Lansdown 
and Scho.eld 1995; Meier 1996; Strothotte et al. 1994; Winkenbach and Salesin 1994]). Much of the research 
in NPR has targeted a particular style of imagery and developed algorithms to reproduce that style when 
rendering appropriately-annotated 3D scenes. Relatively little emphasis has been placed on the separate 
problem of how to provide direct, .exible interfaces that a designer can use to make those annotations 
in the .rst place. Instead, the usual approach is to rely on complex scripting or programming. Meier 
[1999] and Seims [1999] argue that e.ective interfaces are essential for these algorithms to be accepted 
by content creators. One reason is that NPR imagery must often re.ect a designer s judgments regarding 
what details to emphasize or omit. Thus, a key challenge facing NPR researchers is to provide algorithms 
coupled with direct user interfaces that together give designers .exible control over the look of a scene. 
In this paper we begin to address this challenge in the context of interactive NPR for 3D models. Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
John F. Hughes2 Adam Finkelstein1 2Brown University Figure 1: Artists directly annotated the same 3D 
teacup model to produce four distinct rendering styles. We present a system called WYSIWYG NPR, for what 
you see is what you get non-photorealistic rendering. We focus on stroke-based rendering algorithms, 
with three main categories of strokes: (1) silhouette and crease lines that form the basis of simple 
line drawings; (2) decal strokes that suggest surface features, and (3) hatching strokes to convey lighting 
and tone. In each case, we provide an interface for direct user control, and real-time rendering algorithms 
to support the required interactivity. The designer can apply strokes in each category with signi.cant 
stylistic variation, and thus in combination achieve a broad range of e.ects, as we demonstrate in both 
.gures and video. In this paper we focus on tools that give the artist control over the look of a scene. 
We also provide limited control over how strokes animate during level of detail transitions, recognizing 
that the ideal system would provide more complete control over the animation of strokes as a design element 
in its own right. The applications for this work are those of NPR, including architectural and product 
design, technical and medical illustration, storytelling (e.g., children s books), games, .ne arts, and 
animation. The main contributions of this paper are to identify the goal of providing direct control 
to the user as being key to NPR, and to demonstrate with a working system the payo. that can result from 
targeting this problem. In support of this goal, we o.er several new NPR algorithms, including improved 
schemes for detecting and rendering silhouettes, an algorithm for synthesizing stroke styles by example, 
methods for view-dependent hatching under artistic control, and an e.cient technique for simulating various 
types of natural media such as pencil or crayon on rough paper.  Figure 2: Example session (L to R): 
load model; shading and paper; stylize outlines; add decals; hatching; labeled features. 2Related Work 
The last decade has seen a blossoming of work on NPR algorithms in a variety of styles. For a survey, 
see Gooch and Gooch [2001]. Much of this work addresses the production of still images, while some systems 
for rendering 3D scenes have addressed the challenge of providing temporal coherence for animations [Deussen 
and Strothotte 2000; Meier 1996]. Our work falls into the latter category, within the subset of systems 
that address interactive rendering (e.g., [Gooch et al. 1999; Kowalski et al. 1999; Lake et al. 2000; 
Markosian et al. 2000; Praun et al. 2001]), wherein the challenge is to maintain temporal coherence, 
using limited run-time computation, when camera paths are not known in advance. Most work in NPR has 
focused on algorithms that are con­trolled by parameter setting or scripting the designer has no direct 
control over where marks are made. An inspiration for our work is the technique for direct WYSIWYG painting 
on 3D surfaces proposed by Hanrahan and Haeberli [1990], which is now available in various commercial 
modeling sys­tems. These tools let the designer paint texture maps directly onto 3D models by projecting 
screen-space paint strokes onto the 3D surface and then into texture space, where they are composited 
with other strokes. Strokes then remain .xed on the surface and do not adapt to changes in lighting or 
viewpoint. In contrast, strokes in our system are automati­cally added, removed, or modi.ed in response 
to changes in lighting or viewing conditions. Our system also draws inspiration from others that pro­vide 
direct drawing interfaces for creating stylized scenes. Arguably, the Holy Grail of NPR research is to 
allow an artist to simply draw in the image plane and thereby express a complete, stylized 3D world. 
The systems of Tolba et al. [2001], Cohen et al. [2000], and Bourguignon et al. [2001], each pursue this 
goal by making simplifying assumptions about the underlying geometry. In contrast, we start with a 3D 
model, and draw directly on it. Our goal is eventually to integrate the WYSIWYG NPR inter­face directly 
into a modeling system, ideally one that uses a drawing interface for constructing geometry, like SKETCH 
[Zeleznik et al. 1996] or Teddy [Igarashi et al. 1999]. Previous e.orts have addressed .nding silhouettes 
on 3D models and rendering them with stylization [Markosian et al. 1997; Masuch et al. 1997; Northrup 
and Markosian 2000]. Other systems have addressed generating hatching or structured patterns of strokes 
on surfaces to convey tone or texture [Hertzmann and Zorin 2000; Mitani et al. 2000; Praun et al. 2001; 
Winkenbach and Salesin 1994]. In our system the challenge is to synthesize strokes with temporal coherence, 
in real time, based on a user-given style, where the user speci.es where particular strokes should be 
placed. 3 An Interaction Example To give a sense of the system from the artist s viewpoint, we now brie.y 
describe the organization of the user interface, and then narrate a step-by-step interaction session. 
To support a direct interface for creating stroke-based NPR styles, we provide a pen and tablet for drawing 
input. Pressure data from the tablet pen can be used to vary stroke attributes such as width and darkness. 
The UI makes use of conventional menus and dialog boxes as needed. The system has three editing modes. 
In the .rst, the artist can position each object and set its base coat (described in Section 4.1). In 
outline mode the artist can draw decals and stylize silhouettes and creases. In hatching mode, he can 
draw hatching strokes. In any mode, the artist can modify the current brush style that a.ects stroke 
properties (see Section 4.2). In practice, the artist will carefully design combinations of parameters 
that together produce a desired look, then save that brush style to use again in future sessions. The 
artist can also select a background color or image for the scene, and control the directional lighting 
that a.ects the toon shaders used for object base coats. All color selections are made via HSV sliders. 
We now describe how an artist annotates a simple 3D model to depict a stylized fruit can. Stages of the 
process are shown in Figure 2; the names of various design elements appear in the rightmost image. 1. 
We begin by loading a model of a can, which is initially displayed in a generic silhouette style with 
a tan base coat and background color. 2. We choose a toon base coat for the model from a previously-created 
list. Next we set the background color to gray, and apply a previously created paper texture to it. We 
adjust the lighting so the right side of the can is lit. 3. From a list of previously saved brushes, 
we choose a black pencil style. We select a silhouette, which highlights in yellow. We draw over the 
highlighted portion with sketchy strokes and then click an apply button to propagate the sketchy style 
to all silhouettes. Next we select a crease where the lip of the can meets the cylindrical part, oversketching 
this and other creases in a sketchy style similar to that of the silhouettes. 4. To add a label to the 
can, we draw decal strokes directly on the surface, moving the viewpoint and changing the color and other 
attributes of the brush as needed. 5. We now switch to hatching mode and draw roughly parallel lines 
where we want shading to appear. To .nish the hatch group, we tap the background. We create a second 
set at an angle to the .rst and tap again. The drawing is done!  4 Rendering Basics The models accepted 
by our system are triangle meshes (possibly animated). A mesh is divided into one or more patches, each 
rendered with several procedural textures, or shaders. One shader draws a base coat, another handles 
creases, a third draws silhouettes, and a fourth applies hatching strokes. 4.1 Background and Base Coat 
The focus of our work is to let the designer annotate a model with strokes, but there are two other elements 
under user control that provide signi.cant .exibility. First, the designer can choose a background color 
or image that .lls the rendering window (e.g. the sky in Figure 8.) Second, the designer can select for 
each patch a base coat a shader that draws the triangles of the patch in some style. Examples in this 
paper include drawing the patch using cartoon (or toon ) shading as in Figure 1, upper left, or in a 
solid color as in Figure 1, upper and lower right. As described by Lake et al. [2000], the designer can 
create custom 1D texture maps containing the color spectrum for each toon shader. Our 1D toon textures 
may include an alpha component to be used by the media simulation algorithm of Section 4.4, as in Figure 
3. In such cases the base coat is typically rendered in two passes: an opaque layer followed by the toon 
shader. 4.2Strokes Our stroke primitive is based on that of Northrup and Markosian [2000]. The main di.erence 
is that the path of the stroke, called the base path, is represented as a Catmull-Rom spline. Elements 
under user control are: color, alpha, width, the degree to which strokes taper at their endpoints, and 
halo (thedegreetowhich astrokeistrimmed back at an endpoint due to occlusion). These parameters are controlled 
by sliders, though width and alpha can be made to vary along the stroke due to pressure data from the 
tablet. The designer may choose an optional 1D texture specifying the image (including alpha) of a cross-section 
of the stroke. (We found that using a 2D texture parameterized by stroke length and width led to temporal 
artifacts arising from abrupt changes in stroke lengths.) The stroke is rendered as a triangle strip 
that follows the base path and matches the varying width, alpha, and color or texture of the stroke. 
As will be described in Section 5, we may further stylize a silhouette or crease stroke using an o.set 
list that represents small-scale wiggles relative to the base path [Markosian et al. 1997]. Each o.set 
records a parametric position along the base path and a perpendicular screen-space displacement from 
that point (in pixels). The o.set list may progress forward and backward along the stroke, and may contain 
breaks. To render the o.set list, we map it along the base path to de.ne one or more triangle strips 
in image space. 4.3 Stroke Visibility The z-bu.er cannot directly compute visibility for strokes because 
the triangle strip for a stylized base path would generally interpenetrate the surface. Instead, we adopt 
the method of Northrup and Markosian [2000], which resolves visibility using an ID reference image. Into 
this o.-screen bu.er, we render the mesh faces, crease edges, silhouette polylines each in a unique 
color. The visible portions of a particular stroke are found by sampling the ID reference image along 
the base path, checking for the correct color. Figure 3: Wide silhouette strokes and a subtle toon shader 
are rendered over a coarse paper texture. 4.4 Media Simulation To simulate natural media, the artist 
can apply a paper e.ect to any semi-transparent scene primitive: background image, toon shader, or stroke. 
We modulate the per-pixel color of the primitive during scan conversion by the texel of a chosen paper 
texture. Conceptually following Curtis et al. [1997] and Durand et al. [2001], the paper texture encodes 
a height .eld h . [0, 1]. At peaks (h = 1) the paper easily catches pigment, whereas in valleys (h = 
0)itresists pigment. We model this process by applying a transfer function to the a component of the 
incoming color. For peaks we use the transfer function p(a)= clamp(2a), and for valleys we use v(a)= 
clamp(2a - 1). For intermediate height h we use the interpolated function t(a)= p(a)h + v(a)(1 - h). 
We composite the incoming color into the framebu.er using the standard over operation with transparency 
t(a). We have implemented this simple algorithm as a pixel program on our graphics card. The paper e.ect 
is clear in Figures 3 and 4. Durand et al. [2001] describe a similar strategy, using a transfer function 
to re-map tones to produce bi-level (halftone) output, and a modi.cation to blur this bi-level output 
for better results on color displays. In contrast, our transfer functions were designed to reproduce 
patterns we observed in scanned images of real crayon and pencil on paper. They are simpler and better 
suited to implementation on current pixel-shading hardware. Because we re-map alpha instead of tone, 
our method more .exibly handles arbitrary colors in both source and destination pixels. Figure 4: Details 
of paper e.ect from Figures 1 and 3. Figure 5: The same cube rendered in two di.erent styles. Crease 
strokes were synthesized from the examples shown.  5 Decals, Creases, and Silhouettes In this section 
we treat the placement, modi.cation and rendering of individual strokes. Section 6 will address level 
of detail issues relating to management of groups of strokes. 5.1 Decal Strokes The simplest annotation 
in our system is the decal stroke. The artist draws a stroke directly onto a surface, and it sticks like 
a decal. These strokes were used to draw the .owers and the butter.y on the upper cups in Figure 1. The 
interface for decal strokes was inspired by the system of Hanrahan and Haeberli [1990], which projects 
strokes into a texture map, and renders them using conventional texture mapping. In contrast, our system 
represents decal strokes as spline curves whose control points are projected onto the surface and rendered 
as described in Section 4.2. Our repre­sentation o.ers three bene.ts over texture mapping. First, decal 
strokes do not require a parameterization of the sur­face. Second, we avoid sampling artifacts at magni.ed 
and oblique views. Third, with texture mapping the apparent brush size depends on the obliquity of the 
surface, whereas we can maintain the screen-space width of strokes to be con­sistent with the artist 
s brush choice under all views. 5.2Crease Strokes The artist may also annotate creases chains of mesh 
edges that follow sharp features on the model. Such features are manually tagged by the modeler or automatically 
found by thresholding dihedral angles in the mesh. The strokes in Figures 5 and 6 all arise from annotated 
creases. When the artist oversketches a chosen crease, perhaps more than once, the system records these 
detail functions in o.set lists as described in Section 4.2. For every vertex of the input stroke, an 
o.set is recorded as a perpendicular pixel displacement from the nearest point along the arc­length parameterized 
crease path. The crease endpoints are extended along the respective tangents to accommodate sketching 
beyond the crease. In subsequent views, the parametric positions of the o.sets remain .xed. However, 
we shrink the o.sets when the model shrinks in screen space, because we found this to appear more natural 
than using .xed-magnitude o.sets. We take the ratio sc of the current screen-space length of the crease 
to that when it was oversketched, and scale the magnitudes of the o.sets by min(sc, 1). I.e., we scale 
them down but never up. Figure 6: Victorian Storefront. Four example strokes were used to synthesize 
strokes along all creases, combining the best of both worlds: direct drawing to create the look, and 
automation to avoid the tedium of sketching every stroke.  5.3 Synthesizing Strokes by Example Since 
some models contain many creases, we provide two techniques for automatically assigning them o.sets based 
on examples provided by the artist. The .rst technique, rubber­stamping, repeats a sequence of example 
o.sets along each crease, using arc-length parameterization. To produce less obviously repetitive strokes 
that still re­semble the example strokes, we provide a second technique that synthesizes new strokes 
from a given set of example strokes. Freeman et al. [1999] describe a method of trans­ferring stylization 
from a set of example strokes to a new line drawing. Their method requires a large set of examples (typically 
over 100), with each example stroke given in each of the various styles to be supported. In contrast, 
the method we describe below works with just a few example strokes (we typically use three or four). 
This is possible be­cause we separate each stroke into an unstylized base path plus detail o.sets as 
described in Section 4.2. We perform one-dimensional texture synthesis on the stroke o.sets using Markovrandom 
.elds, and can apply the result to any new stroke base path. To maintain variety, we synthesize a new 
o.set list for each stroke requiring stylization. Markovrandom .elds have been recently used in graphics 
in other data-synthesis applications [Brand and Hertzmann 2000; Efros and Leung 1999; Wei and Levoy 2000]. 
Our algorithm closely follows the video textures algorithm presented in Sch¨odl et al. [2000]; rather 
than synthesize new sequences of frames from an example video, we synthesize new sequences of stroke 
o.sets from a set of example strokes. The synthesis algorithm constructs a Markovrandom .eld where each 
state corresponds to an o.set in an example stroke; the transition probability between two states is 
a function of the local stroke similarity between the two points. With this method, we generate o.set 
sequences containing features of the example strokes, as shown in Figures 5 and 6. Implementation details 
are given in Appendix A.  Figure 7: The artist chose sepia-toned paper and brown and white conte crayon 
brushes to create this breakfast scene. The toon shading, which is translucent brown in the dark areas 
and completely transparent in the light areas, implies an ink wash. Multiple levels of detail are revealed 
in the free hatching when the camera zooms in for a closer view (right). 5.4 Silhouettes Unlike creases, 
silhouettes are view dependent: their num­ber, locations, and lengths vary from one frame to the next. 
It is not obvious how the artist could annotate individual silhouettes with unique stylizations that 
persist over time. Therefore, our tools let the artist sketch a single proto­type stroke that is rubber-stamped 
wherever silhouettes are found. In most other respects, silhouettes behave like creases from the point 
of view of the artist. We have adapted the silhouette detection algorithm of Markosian et al. [1997], 
which .nds networks of silhouette edges on the mesh. Our goal is to turn visible portions of these networks 
into strokes. While the silhouette generally looks smooth, it often bifurcates and zig-zags in 3D and 
therefore backtracks in the image plane an obstacle for the construction of smooth strokes along the 
path. Northrup and Markosian [2000] propose a set of heuristics for cleaning up the resulting 2D paths 
before forming strokes. Hertzmann and Zorin [2000] use an alternate de.nition of silhouettes that avoids 
such problems. For a given vertex v with normal nv and vector cv to the camera, we de.ne the scalar .eld 
f(v)= nv · cv, extending f to triangle interiors by linear interpolation. Silhouettes are taken to be 
the zero­set of f, yielding clean, closed polylines whose segments traverse faces in the mesh (rather 
than following edges, as in the Markosian and Northrup methods.) Hertzmann and Zorin also describe a 
fast, deterministic method for .nding these silhouettes at runtime, based on a pre-computed data structure. 
A drawback is that their method is not suitable for animated meshes (e.g. Figure 10). Therefore, we use 
their silhouette de.nition, but adapt the stochastic method for .nding the polylines, as follows. Sample 
a small number of faces in the mesh. At each, test the sign of the function f at its three vertices; 
if they are not all the same, then the silhouette must cross this triangle and exit from two of the three 
sides into neighboring triangles; locate those triangles and continue tracing until returning to the 
starting triangle. (If f is exactly zero at a vertex, slightly perturbing the normal removes the degeneracy.) 
We cull back-facing segments by checking if .f points away from the camera, and test visibility of the 
remaining portions as in Section 4.3. Since we render silhouettes with stylization (Section 4.2), assigning 
them consistent parameterization is critical for temporal coherence. This is easy for creases because 
they are .xed on the model. But silhouettes are view-dependent and do not necessarily have correspondence 
from frame to frame. This paper does not fully address the challenge of assigning consistent parameterization 
from one frame to the next. Nevertheless, we adopt a simple heuristic described by Bourdev[1998]. We 
begin by assigning all strokes arc­length parameterization. In each frame, we sample visible silhouettes, 
saving for each sample its surface location and parameter value (with respect to the stylization). In 
the next frame, we project the sample from 3D into the image plane. Then, to .nd nearby silhouette paths, 
we search the ID reference image (Section 4.3) by stepping a few pixels along the surface normal projected 
to image space. If a new path is found, we register the sample s parameter value with it. Since each 
new path generally receives many samples, we use a simple voting and averaging scheme to parameterize 
it; more rigorous analysis of clustering or voting schemes merits future work. Once we have parameterized 
the silhouette paths, we can apply stylization as we do with creases (Section 5.2), with one notable 
di.erence. Because the silhouette paths have varying lengths, we truncate or repeat the stylization to 
fully cover each path. When the artist oversketches a silhouette to provide stylization, the length of 
the oversketched base path is taken as the arc-length period for repeating the o.set pattern. We scale 
the period and magnitude of the silhouette o.sets just as for creases (Section 5.2), except for the de.nition 
of the scaling factor (which for a crease depends on a .xed set of edges). Instead we use the ratio sm 
of the screen-space model diameter to that when the oversketch was performed. We scale the o.set magnitudes 
only when zoomed out, using min(sm, 1), as on this sphere: sm =1/41/21 2 4 Figure 8: A toon shader 
and mobile hatching suggest lighting on the snowman, decal strokes depict a face, and uneven blue crayon 
silhouettes imply bumpy snow. Conical trees are annotated with sawtooth crayon silhouettes plus two layers 
of mobile hatching.   6Hatching Our system provides tools for drawing several forms of hatching groups 
of strokes drawn on the surface that collectively convey tone, material or form. 6.1 Structured Hatching 
In illustrations, perhaps the most common form of hatching could be characterized as a group of roughly 
parallel strokes. We call such annotations structured hatching and leverage their regular arrangement 
to provide automatic level-of­detail (LOD) control. First, the artist draws a group of roughly parallel 
and regularly-spaced strokes from a particular vantage point. Then, he may move the camera closer or 
further to adjust the screen-space stroke density before locking it with a button. In subsequent views, 
the system attempts to maintain the chosen density by successively doubling (or halving) the stroke count 
when the group size doubles (or halves). To compensate for the discrete nature of these LODs, we modulate 
stroke widths between levels. In each frame we calculate a factor sh that approximates the multiplier 
on the original stroke count necessary to maintain density. We take sh to be the ratio of the current 
size of the stroke group (measured transverse to the strokes) to that when it was locked. Figure 9 shows 
the .rst LOD transition as the camera approaches the locked stroke group. The strokes do not double until 
sh reaches the threshold t+ (1.6 here). For sh . [1.0,t+) the strokes thicken as their density falls. 
When sh reaches t+ a brief animation (0.5s by default) begins, during which existing strokes narrow while 
new strokes grow in the interstices, as shown in the middle three images. New stroke paths linearly interpolate 
those of their neighbors. Finally, for sh . (t+, 2.0] strokes thicken again to eventually reach the original 
width. The doubling process repeats analogously for larger sh . [2n , 2n+1]. If the camera path is reversed 
(zooming out) the process is inverted using a threshold t- <t+, providing hysteresis for stability. Thedesignermay 
seta maximum LOD afterwhich strokes simply fatten rather than divide. sh =1.01.25 1.6 -. 1.61.75 2.0 
Figure 9: As the camera approaches a hatch group, level of detail e.ects preserve stroke density. See 
Section 6.1. 6.2Free Hatching For some e.ects, an artist may .nd structured hatching too constraining. 
Therefore, we also provide free hatching with which the artist draws arbitrary arrangements of strokes 
to explicitly build LODs. See, for example, Figure 7. The scheme is simple. As before, the artist draws 
a set of strokes from a particular view, and locks them at the desired density. Then he moves to a closer 
view where higher density is desired and adds more strokes, choosing whether the new stroke group replaces 
or augments the previous. This is repeated until the artist builds su.cient LODs. For novel views, LOD 
transitions are handled using the same policy as for structured hatching. (The user-adjustable constants 
t+ and t- are particularly useful for tuning free­hatching LOD transitions.) However, there is no obvious 
way to measure sh for free hatching, so instead we use the ratio sm based on mesh sizes (Section 5.4). 
When the .rst LOD is drawn, sm = 1. Each additional LOD is associated with the value of sm at the time 
it is drawn.  6.3 Mobile Hatching Artists often use shading near outlines, suggestive of light cast 
from somewhere behind the camera. To achieve this e.ect when an object or camera animates, the hatching 
must move on the surface. Our system provides a tool for annotating the model with such dynamic strokes, 
which we call mobile hatching. This e.ect is used, for example, on the snowman Figure 8, and readers 
with access to the accompanying video can see it in action. Like stationary hatching, described in Sections 
6.1 and 6.2, mobile hatching may also be either structured or free. The artist enters mobile mode and 
then sketches hatch groups as before, but now each group implies a directional light. The model we use 
is simple, and applies equally to hatching suggestive of either highlights or shadows. From the drawn 
group, we infer a light direction opposite to the local surface normal. For highlights, we imagine a 
light source in the usual sense producing this tone. However, for shading strokes, we think of a dark 
light shining darkness on the local surface. As the view changes, mobile hatch groups move over the surface 
consistent with this pseudo lighting rule. Our implementation is presently limited to smooth surface 
regions with roughly uniform uv-parameterization, and we further constrain the motion of mobile hatch 
groups to straight lines in either u or v let s say u.When the artist completes a mobile hatching group, 
the system (1) projects the strokes into uv-space, (2) computes their convex hull, (3) .nds its centroid 
c, and (4) records the normal nc of the surface at c. The parametric path over which hatch groups travel 
is taken to be the line in u passing through c.For new views, we sample the surface normal n(u) along 
the line, and evaluate a Lambertian lighting function (u)= nc · n (u). Prior to obtaining either nc or 
(u), we smooth n(u)with a .lter whose kernel has the extent of the hatch group, to reduce sensitivity 
to minor surface variations. Next, at each local maximum in (u) we create a mobile hatching group whose 
extent is governed by the width of the peak. We omit maxima where (u) is less than a con.dence threshold 
T  1 (we use ) that prevents hatch groups from being placed 2 at insigni.cant local maxima arising 
from the constrained motion of the group. Finally, as (u) approaches T we fade out the group to avoid 
popping. While the dark light model may not be physically accurate, it does yield plausible cartoonish 
lighting e.ects. It is also extremely easy for the artist to specify, and it does not constrain him to 
depict lighting that is globally consistent. Still, the problem of inferring lighting in response to 
hand-drawn shading merits further research. Other researchers, for example, Schoeneman et al. [1993] 
and Poulin et al. [1997], have addressed this problem with greater rigor in the context of photorealism. 
  7 Results and Discussion1 The greatest strength of this system is the degree of control given to the 
artist: the choice of brush styles and paper textures, background and basecoats, and the look and location 
of each mark. In our experience, working with each new style requires a learning period but, as with 
a traditional medium, it becomes natural once mastered. We .nd that complex geometry o.ers detail that 
can be revealed through the more automatic features of our system (e.g., toon shaders or silhouettes), 
whereas simple geometry o.ers a blank slate on which to create new details where none exist. For example, 
in Figure 6, we simpli.ed the appearance of the intricate building with a sparse line drawing representation. 
Four example strokes were used to automatically synthesize detail over the 8,000 crease edges in the 
model. This scene was completed in under .fteen minutes, including time to experiment with media, textures, 
and lighting. In contrast, the snowman scene in Figure 8 called for stroke-based detail to augment its 
simple geometry (spheres and cones). This took about an hour to complete. For interactive exploration, 
the artist may need to design appropriate detail to ensure that a scene is interesting from disparate 
views. In the scene shown in Figure 7, we created three LODs for each object and also annotated the backside, 
so of course the drawing took longer to create than a still image. However, the scene in Figure 8 did 
not have such additional overhead because mobile, structured hatching automatically adapts to a moving 
camera. While most of the images in this paper were captured from interactive exploration of static scenes, 
our system also supports o.ine rendering of scenes with animated geometry, for example the winged character 
in Figure 10. 1The electronic version of this paper contains an Appendix B with additional results. While 
the system supports a diversity of styles, it does not work well for those based on short strokes, such 
as stippling or pointillism. Furthermore, silhouette stylization presently cannot depict features local 
to a particular surface region, since the style applies globally. Also, the system does not yet support 
object interplay such as shadows or re.ections. Table 1 reports model complexity and typical frame rates 
for various models rendered with a 1.5 GHz Pentium IV CPU and Geforce3 GPU. All of the models render 
at interactive frame rates, except for the building which uses a large number of strokes. Stroke count 
has the greatest in.uence on performance, while polygonal complexity has limited impact. Reading back 
the ID reference image (Section 4.3) imposes a signi.cant but .xed overhead, so that trivial models render 
at only roughly 25 fps. Figure Faces (K) Strokes Frames/sec 1: cup 5 25 20 3: statue 120 100 10 6: building 
16 7,000 3 7: breakfast 25 400 9 8: snowman 10 250 11 Table 1: Frame rates and sizes of various models. 
 8 Conclusion and Future Work We have demonstrated a system for drawing stroke-based NPR styles directly 
on 3D models. The system o.ers control over brush and paper styles, as well as the placement of in­dividual 
marks and their view-dependent behavior. Com­bined, these a.ord a degree of aesthetic .exibility not 
found in previous systems for creating stylized 3D scenes. We pre­sented new algorithms for .nding and 
rendering silhouettes, synthesizing stroke detail by example, simulating natural media, and hatching 
with dynamic behavior. As future work, we hope to address some of the limitations of our system and extend 
it, for example, to encompass such styles as stippling and pointillism. We believe that the stroke synthesis 
currently available for annotating creases could be adapted to create regions of hatching strokes or 
other structured patterns based on artist example, thus reducing the tedium of creating every stroke 
manually. We would also like to consider how we can help designers show object-to-object interactions 
such as shadows, and create artistic e.ects like smudging one object into another. Most important is 
to put these tools in the hands of artists. Acknowledgments We thank Carl Marshall and Adam Lake for 
encouragement and advice, and Trina Avery for her super-human edit. This research was supported by Intel 
Labs, the Alfred P. Sloan Foundation, and the NSF (CISE 9875562, EIA 8920219). References Bourdev, L. 
1998. Rendering Nonphotorealistic Strokes with Tempo­ral and Arc-Length Coherence. Master s thesis, Brown 
University. www.cs.brown.edu/research/graphics/art/bourdev-thesis.pdf. Bourguignon, D., Cani, M. P., 
and Drettakis, G. 2001. Drawing for illustration and annotation in 3D. In Computer Graphics Forum, Blackwell 
Publishers, vol. 20:3, 114 122. Brand, M., and Hertzmann, A. 2000. Style machines. Proceedings of SIGGRAPH2000, 
183 192. Cohen, J. M., Hughes, J. F., and Zeleznik, R. C. 2000. Harold: A world made of drawings. Proceedings 
of NPAR 2000, 83 90. Curtis,C. J., Anderson,S. E., Seims, J. E.,Fleischer, K. W., and Salesin, D. H. 
1997. Computer-generated watercolor. Proceedings of SIGGRAPH97, 421 430. Deussen, O., and Strothotte, 
T. 2000. Computer-generated pen­and-ink illustration of trees. Proc. of SIGGRAPH2000, 13 18. Durand, 
F., Ostromoukhov, V., Miller, M., Duranleau, F., and Dorsey, J. 2001. Decoupling strokes and high-level 
attributes for interactive traditional drawing. In 12th Eurographics Workshop on Rendering, 71 82. Durand, 
F. 2002. An invitation to discuss computer depiction. Proceedings of NPAR 2002. Efros, A. A., and Leung, 
T. K. 1999. Texture synthesis by non-parametric sampling. In IEEE International Conference on Computer 
Vision, 1033 1038. Freeman, W. T., Tenenbaum, J. B., and Pasztor, E. 1999. An example-based approach 
to style translation for line drawings. Tech. Rep. TR99-11, MERL, Cambridge, MA. http://www.merl.com/papers/TR99-11. 
Gooch, B., and Gooch, A. 2001. Non-Photorealistic Rendering. A. K. Peters. Gooch, B., Sloan, P.-P. J., 
Gooch, A., Shirley, P., and Riesenfeld, R. 1999. Interactive technical illustration. 1999 ACM Symposium 
on Interactive 3D Graphics, 31 38. Hanrahan, P., and Haeberli, P. 1990. Direct WYSIWYG painting and texturing 
on 3D shapes. Proc. of SIGGRAPH90, 215 223. Hertzmann, A., and Zorin, D. 2000. Illustrating smooth surfaces. 
Proceedings of SIGGRAPH 2000, 517 526. Igarashi, T., Matsuoka, S., and Tanaka, H. 1999. Teddy: A sketching 
interface for 3d freeform design. Proc. of SIGGRAPH99, 409 416. Kowalski, M. A., Markosian, L., Northrup, 
J. D., Bourdev, L., Barzel, R., Holden, L. S., and Hughes, J. F. 1999. Art-based rendering of fur, grass, 
and trees. Proc. SIGGRAPH99, 433 438. Lake, A., Marshall, C., Harris, M., and Blackstein, M. 2000. Stylized 
rendering techniques for scalable real-time 3D animation. Proceedings of NPAR 2000, 13 20. Lansdown, 
J., and Schofield, S. 1995. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer 
Graphics and Applications 15, 3, 29 37. Markosian, L., Kowalski, M. A., Trychin, S. J., Bourdev, L. D., 
Goldstein, D., and Hughes, J. F. 1997. Real-time nonphotorealistic rendering. Proceedings of SIGGRAPH97 
, 415 420. Markosian, L., Meier, B. J., Kowalski, M. A., Holden, L. S., Northrup, J. D., and Hughes, 
J. F. 2000. Art-based rendering with continuous levels of detail. Proc. of NPAR 2000, 59 66. Masuch, 
M., Schlechtweg, S., and Schonw¨ alder, B. ¨1997. daLi! Drawing Animated Lines! In Proceedings of Simulation 
und Animation 97, SCS Europe, 87 96. Meier, B. J. 1996. Painterly rendering for animation. In Proceedings 
of SIGGRAPH96, ACM SIGGRAPH / Addison Wesley, Com­puter Graphics Proceedings, Annual Conference Series, 
477 484. Meier, B. 1999. Computers for artists who work alone. Computer Graphics 33, 1 (February), 50 
51. Mitani, J., Suzuki, H., and Kimura, F. 2000. 3D Sketch: Sketch-based model reconstruction and rendering. 
Seventh IFIP WG 5.2 Work­shop on Geometric Modeling. http://kaemart.unipr.it/geo7/. Northrup, J. D., 
and Markosian, L. 2000. Artistic silhouettes: A hybrid approach. Proceedings of NPAR 2000, 31 38. Poulin, 
P., Ratib, K., and Jacques, M. 1997. Sketching shadows and highlights to position lights. In Proceedings 
of Computer Graphics International 97, IEEE Computer Society, 56 63. Praun, E., Hoppe, H., Webb, M., 
and Finkelstein, A. 2001. Real-time hatching. Proceedings of SIGGRAPH 2001, 579 584. Sch¨ odl, A., Szeliski, 
R., Salisin, D., and Essa, I. 2000. Video textures. Proceedings of SIGGRAPH 2000, 489 498. Schoeneman, 
C., Dorsey, J., Smits, B., Arvo, J., and Greenberg, D. 1993. Painting with light. In Proc. of SIGGRAPH93, 
143 146. Seims, J. 1999. Putting the artist in the loop. Computer Graphics 33, 1 (February), 52 53. Strothotte, 
T., Preim, B., Raab, A., Schumann, J., and Forsey, D. R. 1994. How to render frames and in.uence people. 
Computer Graphics Forum 13, 3, 455 466. Tolba, O., Dorsey, J., and McMillan, L. 2001. A projective drawing 
system. In ACM Symposium on Interactive 3D Graphics, 25 34. Wei, L.-Y., and Levoy, M. 2000. Fast texture 
synthesis using tree­structured vector quantization. Proc. SIGGRAPH2000, 479 488. Winkenbach, G., and 
Salesin, D. H. 1994. Computer-generated pen­and-ink illustration. Proceedings of SIGGRAPH94 , 91 100. 
Zeleznik, R. C., Herndon, K. P., and Hughes, J. F. 1996. SKETCH: An interface for sketching 3D scenes. 
Proceedings of SIGGRAPH 96, 163 170. A Stroke Synthesis We follow the notation of Sch¨odl et al. [2000]. 
The .rst step of the stroke synthesis algorithm is to resample each example stroke into o.sets in the 
normal direction, regularly spaced in four-pixel increments along the length of the stroke (the synthesis 
algorithm does not yet handle breaks or backtracking along the stroke). We then add a separator o.set 
to the end of each example stroke and concatenate the example o.sets together into a single vector y, 
keeping track of the locations of the separator o.sets. We calculate the matrix D,where Dij is the distance 
from yi to yj: ( K,when yj is a separator Dij = 0, when yi is a separator and yj is not |yi - yj| otherwise 
where K is a large constant (we use 104). To take the surrounding o.sets into account, we .lter the distance 
ma- P m trix with a diagonal kernel [w0,... ,wm]: D. = ij k=0 wk · Di-k,j-k, where out-of-range entries 
in D are assumed to be zero. In our implementation, m =4 and wi =1/m. D. represents the di.erence between 
two windows of o.sets, but it does not accurately represent the future cost of tran­sitions. For instance, 
a window may have a low di.erence to windows near the end of an example stroke; if the al­gorithm chose 
to transition to this window, it would have a high probability of prematurely ending the stroke. We want 
to assign high costs to such dead-end windows. To calculate the future costs, we use Q-learning. We initial­ize 
D. to the values in D. . We then iteratively compute D. ij . (D. jk until convergence; we use p =2.0 
ij )p +a mink D. and a =0.999 in our implementation; Sch¨odl et al. [2000] discusses optimizations. Finally, 
we calculate the matrix whose entries Pij are the probability of transitioning from o.set yi to o.set 
yj: Pij . exp(-Di. +1,j/.), where . is a small multiple (we use 0.01) of the average magnitude of the 
entries of D. that are greater than zero and less than the magnitude of the largest example o.set. To 
synthesize a new list of o.sets using y and P ,we .rst choose at random an o.set yi that is the start 
of an example stroke. We then transition to some other o.set j with probability proportional to Pij and 
iterate until the desired number of o.sets are generated.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566649</article_id>
		<sort_key>763</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Painting and rendering textures on unparameterized models]]></title>
		<page_from>763</page_from>
		<page_to>768</page_to>
		<doi_number>10.1145/566570.566649</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566649</url>
		<abstract>
			<par><![CDATA[This paper presents a solution for texture mapping unparameterized models. The quality of a texture on a model is often limited by the model's parameterization into a 2D texture space. For models with complex topologies or complex distributions of structural detail, finding this parameterization can be very difficult and usually must be performed manually through a slow iterative process between the modeler and texture painter. This is especially true of models which carry no natural parameterizations, such as subdivision surfaces or models acquired from 3D scanners. Instead, we remove the 2D parameterization and store the texture in 3D space as a sparse, adaptive octree. Because no parameterization is necessary, textures can be painted on any surface that can be rendered. No mappings between disparate topologies are used, so texture artifacts such as seams and stretching do not exist. Because this method is adaptive, detail is created in the map only where required by the texture painter, conserving memory usage.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[level of detail algorithms]]></kw>
			<kw><![CDATA[paint systems]]></kw>
			<kw><![CDATA[rendering systems]]></kw>
			<kw><![CDATA[spatial data structures]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35023069</person_id>
				<author_profile_id><![CDATA[81339496463]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[(grue)]]></middle_name>
				<last_name><![CDATA[DeBry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thrown Clear Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42052489</person_id>
				<author_profile_id><![CDATA[81341490580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibbs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thrown Clear Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382427</person_id>
				<author_profile_id><![CDATA[81100351766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Devorah]]></first_name>
				<middle_name><![CDATA[DeLeon]]></middle_name>
				<last_name><![CDATA[Petty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thrown Clear Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P317344</person_id>
				<author_profile_id><![CDATA[81100599827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nate]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thrown Clear Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BENNIS, C., V&Eacute:ZIEN, J.-M., IGL&Eacute:SIAS, G., AND GAGALOWICZ, A. 1991. Piecewise surface flattening for non-distorted texture mapping. In Computer Graphics (Proceedings of SIGGRAPH 91), vol. 25, 237-246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566652</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BENSON, D., AND DAVIS, J. 2002. Octree textures. In Proceedings of ACM SIGGRAPH 2002, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 101-106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BERMAN, D. F., BARTELL, J. T., AND SALESIN, D. H. 1994. Multiresolution painting and compositing. In Proceedings of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Orlando, Florida, Computer Graphics Proceedings, Annual Conference Series, 85-90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLINN, J. F., AND NEWELL, M. E. 1976. Texture and reflection in computer generated images. Communications of the ACM 19, 10, 542-547.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BOADA, I., NAVAZO, I., AND SCOPIGNO, R. 2001. Multiresolution volume visualization with a texture-based octree. The Visual Computer 17, 3, 185-197.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>223682</ref_obj_id>
				<ref_obj_pid>223355</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DAILY, J., AND KISS, K. 1995. 3d painting: paradigms for painting in a new dimension. In Proceedings of CHI'95, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FRISKEN, S. F., PERRY, R. N., ROCKWOOD, A. P., AND JONES, T. R. 2000. Adaptively sampled distance fields: A general representation of shape for computer graphics. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, 249-254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614459</ref_obj_id>
				<ref_obj_pid>614278</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HAKER, S., ANGENET, S., TANNENBAUM, A., KIKINIS, R., AND SAPRIO, M. H. 2000. Conformal surface parameterization for texture mapping. Transactions of Visualization and Computer Graphics 6, 2, 181-189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND HAEBERLI, P. E. 1990. Direct wysiwyg painting and texturing on 3d shapes. In Computer Graphics (Proceedings of SIGGRAPH 90), vol. 24, 215-223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375248</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HUNTER, A., AND COHEN, J. D. 2000. Uniform frequency images: adding geometry to images to produce space-efficient textures. In IEEE Visualization 2000, 243-250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364404</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[IGARASHI, T., AND COSGROVE, D. 2001. Adaptive unwrapping for interactive texture painting. In 2001 ACM Symposium on Interactive 3D Graphics, 209-216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383308</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[L&Eacute:VY, B. 2001. Constrained texture mapping for polygonal meshes. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 417-424.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MA, S. D., AND LIN, H. 1988. Optimal texture mapping. In Eurographics '88, 421-428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MAILLOT, J., YAHIA, H., AND VERROUST, A. 1993. Interactive texture mapping. In Proceedings of SIGGRAPH 93, Computer Graphics Proceedings, Annual Conference Series, 27-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311562</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MCCORMACK, J., PERRY, R., FARKAS, K. I., AND JOUPPI, N. P. 1999. Feline: Fast elliptical lines for anisotropic texture mapping. In Proceedings of SIGGRAPH 99, ACM SIGGRAPH / Addison Wesley Longman, Los Angeles, California, Computer Graphics Proceedings, Annual Conference Series, 243-250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PEACHEY, D. R. 1985. Solid texturing of complex surfaces. In Computer Graphics (Proceedings of SIGGRAPH 85), vol. 19, 279-286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1985. An image synthesizer. In Computer Graphics (Proceedings of SIGGRAPH 85), vol. 19, 287-296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344990</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[PIPONI, D., AND BORSHUKOV, G. D. 2000. Seamless texture mapping of subdivision surfaces by model pelting and texture blending. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, Computer Graphics Proceedings, Annual Conference Series, 471-478.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77587</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SAMAT, H. 1990. Applications of Spatial Data Structures: Computer Graphics, Image Processing, and GIS. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383307</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SANDER, P. V., SNYDER, J., GORTLER, S. J., AND HOPPE, H. 2001. Texture mapping progressive meshes. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, 409-416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SLOAN, P.-P. J., WEINSTEIN, D. M., AND BREDERSON, J. D. 1998. Importance driven texture coordinate optimization. Computer Graphics Forum 17, 3, 97-104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808576</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[TAMMINEN, M., AND SAMET, H. 1984. Efficient octree conversion by connectivity labeling. In Computer Graphics (Proceedings of SIGGRAPH 84), vol. 18, 43-51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[WESTERMANN, R., AND ERTL, T. 1997. A multiscale approach to integrated volume segmentation and rendering. Computer Graphics Forum 16, 3 (August), 117-128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197956</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[WILHELMS, J., AND GELDER, A. V. 1994. Multi-dimensional trees for controlled volume rendering and compression. 27-34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. 1983. Pyramidal parametrics. In Computer Graphics (Proceedings of SIGGRAPH 83), vol. 17, 1-11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253314</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[ZHANG, H., AND HOFF, K. 1997. Fast backface culling using normal masks. In Symposium on Interactive 3D Graphics, ACM Press, 103-106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Painting and Rendering Textures on Unparameterized Models David (grue) DeBry Jonathan Gibbs Devorah 
DeLeon Petty Nate Robins Thrown Clear Productions* Abstract This paper presents a solution for texture 
mapping unparameter­ized models. The quality of a texture on a model is often limited by the model s 
parameterization into a 2D texture space. For mod­els with complex topologies or complex distributions 
of structural detail, .nding this parameterization can be very dif.cult and usu­ally must be performed 
manually through a slow iterative process between the modeler and texture painter. This is especially 
true of models which carry no natural parameterizations, such as subdivi­sion surfaces or models acquired 
from 3D scanners. Instead, we remove the 2D parameterization and store the texture in 3D space as a sparse, 
adaptive octree. Because no parameterization is nec­essary, textures can be painted on any surface that 
can be rendered. No mappings between disparate topologies are used, so texture arti­facts such as seams 
and stretching do not exist. Because this method is adaptive, detail is created in the map only where 
required by the texture painter, conserving memory usage. Keywords: Texture Mapping, Paint Systems, Rendering 
Systems, Spatial Data Structures, Level of Detail Algorithms 1 Introduction With the advent of 3D paint 
programs, artists were able to paint tex­ture detail directly onto the surface of models. However, 3D 
paint is limited by the underlying parameterization between the model s ge­ometry and 2D texture space. 
Poor parameterizations result in un­acceptable artifacts such as texture distortion, discontinuities, 
and singularities. Geometric detail does not necessarily correspond to texture de­tail; in fact, the 
opposite is often true. As a result, it becomes an iterative process to adjust the model s parameterization 
to the re­quired texture detail, and .x existing texture data to align with new parameterizations. This 
process can require large amounts of time from both the modeler and texture painter, and is often a bottle­neck 
at CG animation production companies. As a production pro­gresses, camera position and extreme animation 
poses often reveal areas on the model which require more detail in the texture. This requires the parameterization 
to be adjusted, the resolution of the map increased, or both. Our goal is to create a texturing method 
that ful.lls the following criteria: No parameterization should be necessary. A model should inherently 
be paintable. *{grue,jono,devorah,nate}@thrownclear.com Copyright &#38;#169; 2002 by the Association 
for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 Discontinuities should not 
exist. The topology of the texture should match the topology of the model.  Increased texture detail 
should require only a localized in­crease in map resolution. The texture should contain exactly as much 
detail as has been painted.  Memory usage should be similar to existing 2D textures.  After discussing 
related work, we show that storing texture maps in an octree ful.lls these requirements. We then discuss 
painting textures, overcoming dif.culties with some geometry, using these textures in a renderer and 
memory usage. Finally, we show exam­ples of our method and present ideas for future work. 2 Related 
Work Traditional texture mapping applies a 2D image to 3D geometry to represent visual features without 
requiring an increase in the un­derlying geometry [Blinn and Newell 1976]. At render time, these texture 
coordinates are interpolated across the surface and then used to sample the 2D texture. The most critical 
part of this process is arriving at a mapping from the 3D geometry to the 2D texture space. Manual wrapping 
methods include using a set of simple projections or unwrapping the model into a .at sheet (.gure 1). 
Signi.cant work has been done recently on arriving at a mapping automatically [Boada et al. 2001; Haker 
et al. 2000; Igarshi and Cosgrove 2001; L´ evy 2001; Ma and Lin 1988; Malliot et al. 1993; Pedersen 1996; 
Piponi and Borshukov 2000; Sander et al. 2001]. Optimization methods are somewhat successful [Hunter 
and Cohen 2000; Maillot et al. 1993, Bennis et al. 1991], but all have signi.cant limitations. Since 
tex­ture detail has not yet been applied at optimization time, the as­sumption is made that geometric 
detail will correspond to textural detail. This issue can be avoided by reparameterizing a model once 
some texture is applied [Sloan et al. 1998] or by allowing the user to place texture or paint strokes 
interactively and then derive the appropriate mapping function. Unfortunately, all parameterization techniques 
introduce discontinuitues, stretching, and other artifacts due to the problems of mapping between disparate 
topologies. A 3D volumetric texture can be used to avoid the parameteri­zation problem [Peachey 1985; 
Perlin 1985]. However, the mem­ory needs for a complete 3D array of texture data can become pro­hibitive 
when a large amount of surface detail is required. For a surface texture, most of the space in the 3D 
texture is unused. Vol­ume textures are almost exclusively generated procedurally at ren­der time because 
of the high memory requirements. The uniform resolution of image maps can be avoided by using quadtrees 
to allocate memory as needed for localized areas of detail [Berman et al. 1994]. Various space partitioning 
schemes are also used to represent volumes [Frisken et al. 2000; Westermann and Ertl 1997; Wilhelms and 
Van Gelder 1994]. In particular, octrees can store 3D texture data for volume rendering [Boada et al. 
2001] or can be used as an alternate way to represent surfaces, where the geometric detail drives the 
depth of the tree [Tamminen and Samet 1984]. A 3D texturing system similar to this work but developed 
independently also uses octrees to store texture data [Benson and Davis 2002]. Figure 1: Several methods 
of parameterizing an object (a). Auto­matic unwrapping yields (b), iterative packing and relaxing results 
in (c), and (d) was generated by de.ning the parameterization by hand. A 3D paint application allows 
the user to paint a model using all the standard painting and image manipulation tools. The .rst 3D paint 
systems stored the painted color in the vertices of the model [Hanrahan and Haeberli 1990]. Current 3D 
painting systems put the painted colors directly into the 2D texture map using texture coordinates which 
already exist in the model [Daily and Kiss 1995]. 3 Octex: Octree as a Texture Map Instead of storing 
the texture data in a 2D image, we store it in a sparse, adaptive octree [Samat 1990]. An octree is a 
spatial parti­tioning tree where each node is divided evenly into eight children. An octree consisting 
of a single node is constructed around the model before it is transformed or deformed, and a location 
in 3­space is used as the index into the octree. Thus, the model s texture coordinates are the same as 
its vertex coordinates in the model s local space. Since there are no discontinuities or singularities 
in 3­space, this mapping will always be as smooth and continuous as the model itself. With the addition 
of data described in the following sections, we call this structure an octexture, or octex for short. 
The detail in the tree is driven by the detail in the texture. Of the eight children a given node can 
have, only those which contain a portion of the model s surface are ever created. These are called the 
node s potential children. Depending on the details in the texture, only some of the potential children 
will actually be created for a given node. Each leaf node of the octex contains a color sample for every 
texture stored in the tree. A typical model painted for high-end pro­duction work will have at least 
three and as many as ten or more textures painted for it. The model will usually need at least a dif­fuse 
color texture, a specular color texture, and a bump map or a displacement map. The textures are often 
painted to a similar level of detail, therefore storing them all in the same tree signi.cantly reduces 
the total storage overhead. Textures not having some cor­respondence in detail or coverage (i.e., a decal) 
can be broken out into their own tree. A color sample is also stored in parent nodes, but only if the 
par­ent node does not have all its potential children. This sample repre­sents the color for all the 
potential children that have not been cre­ated. When all the potential children exist, they completely 
de.ne the color of the surface in that node and no color sample is stored in the parent node (.gure 2a). 
For texture .ltering during render­ing, an additional color can be stored in all parent nodes. This color 
represents the average of all the potential children s colors. It is important to consider the non-existing 
children as well as the exist­ing children. Because of this .ltered color, the octex now contains the 
3D equivalent of a traditional MIP map [Williams 1983]. When the renderer provides sample area information 
to the octex look-up functions, the .ltered texture data can be used when the sample area covers several 
octex nodes. The use of this data is addressed further in section 6. Consider a model that is initially 
all white and represented by a single white node. If a small red dot is painted somewhere on the model, 
the original white node is given one child which corre­sponds to the part of the model where the dot 
was painted. Nodes are recursively subdivided until the size of the nodes receiving color approximately 
matches the size of the red dot. These leaf nodes are colored red. The parents of that node are outside 
the red dot, so the rest of the model is still white. Figure 2b shows a 2D version of this octex after 
the red dot is added.  4 Painting 3D painting is a natural interface for creating octextures because 
de­tail can easily be added as painting occurs. As the model is painted, we obtain the 3D locations of 
the paint using the z-buffer and in­verse projection matrix. These positions are transformed into the 
model s local space, and hence the octex space, using the inverse viewing matrix. We then iterate over 
the leaf nodes affected by the new paint. Figure 2: 2D example node (a) which has three potential children 
(shaded area) due to where the surface passes through the node. Only one of the potential children exists 
(blue), where paint was applied. The color stored in this node (red) is the color for the two non-existing 
potential children. Octex subdivision (b) when a small red dot is painted into a coarse white octex. 
The shaded areas are leaf nodes. To assure that only the visible surfaces receive paint, in addi­tion 
to the usual clipping and front facing tests, the surface must be transformed to screen space, and compared 
against the depth buffer of the rendered model. When the screen space covered by a given leaf node has 
an area larger than a pixel, and the new paint in that area is not all the same value, then each of the 
node s children that contain part of the sur­face are created. The new children inherit the parent node 
s color, and are iterated. Finally, the paint color is composited over the color already stored in the 
leaf node. Because the octex is subdivided to match the resolution of the painting, detail is only created 
in the map where it is painted (.gure 3). After the paint is applied, nodes are culled if their information 
is redundant. When a child is a leaf node and contains the same color data as the parent, the child is 
removed. Thus, the octex can also shrink in size in a similar manner if detail is removed. The nature 
of this method allows for as much detail as the painter can create, making it easy for too much detail 
to be created. Setting a limit on the depth of the octree or the total number of nodes can prevent painters 
from unwittingly creating an overly large texture. It can be useful to block new nodes from being created 
altogether so a painter can zoom in to a model to tweak existing paint color without increasing detail. 
 Figure 3: A 2D example of applying paint into an octex. The paint .lls a number of pixels from the 
point of view of the camera. The octex is subdivided until the nodes are approximately the size of a 
pixel. The process of culling a child node that is identical to its parent can be broadened to include 
child nodes that are merely similar to their parents. When the color of the child is within some epsilon 
of the parent s color, the child can be removed. The epsilon can vary depending on the depth of the node. 
A primary bene.t of this system is that any type of model which can be rendered can be painted. The geometric 
operations used dur­ing painting are essentially a subset of the operations which would occur during 
rendering. Checking for an intersection between a cube and a surface is well de.ned. It is also necessary 
to clip mod­els to a leaf node. As such, our method works well on a wide variety of surface types: polygons, 
parametric spline surfaces, subdivision surfaces, and so on.  5 Normals in the Octex The octex stores 
the texture volumetrically, but it is only meant to be applied to a surface. It is possible for paint 
to leak to the opposite side of a very thin model. Consider painting on an airplane wing. As the top 
of the wing is painted near the sharp edge, some of the nodes may be large enough to also contain the 
surface on the bottom of the wing (.gure 4). Figure 4: The top of the wing is painted red, the bottom 
blue. The nodes colored yellow contain two surfaces, so the paint there will depend on the order of application. 
To avoid this problem, the normal of the surface receiving paint is written into the leaf node along 
with the paint color. Later, if another paint sample is applied to the node, the normal of the ex­isting 
paint and the normal of the incoming paint are compared. When the normals are close enough, then the 
colors are combined into one color, and the normals are averaged into one normal (.g­ure 5). If they 
are further apart, then the new paint sample and nor­mal are written into the node in addition to the 
original data. Close enough is determined by controls in the 3D paint application, but generally defaults 
to 90 degrees. Just as the deformations of the model during animation must be accounted for by storing 
the undeformed spatial coordinates as tex­ture coordinates, the undeformed normals must also be stored 
in the model. While this increases the size of both the model and the octex, this extra overhead only 
needs to be taken on if the model is thinner than the detail of the paint to be applied. Since an accurate 
normal is not needed, the normal in the oc­texture can be encoded in a small number of bits [Zhang and 
Hoff Figure 5: Nodes with two surfaces contain two color samples, with an associated normal. 1997]. Even 
8 bits per normal can indicate 256 directions which are more than enough for our purposes. 6 Rendering 
Rendering using an octex is very similar to existing techniques us­ing volume textures, making it trivial 
to add this feature to any ex­isting renderer that supports 3D texture coordinates by using an al­ternate 
texture-lookup function. The model s untransformed vertex positions are stored into the 3D texture coordinates 
and interpolated as usual. For deforming models, the undeformed positions from the reference model are 
used. For renderers that support custom shaders, all the octex sampling can be done inside the shader. 
Typ­ical hardware renderers handle the texture lookup directly making support of octextures dif.cult. 
As described in section 3, the .ltered texture data within a node is a 3D extension of a MIP map. The 
simplest way to compute this .ltered color is to average the colors for all the potential children, even 
those which do not exist. This is analogous to a simple box .lter. Instead of weighting the child nodes 
equally, we weight the samples using the area of the surface inside the node, preventing portions of 
the model that just barely intersect an otherwise empty node from being weighted improperly. Other .lters 
could be used by looking into neighboring nodes, but we have not explored using them. In a traditional 
texture, the MIP map level is chosen using either the area of the sample in texture space or the screen-space 
partials of the texture coordinates. This presents problems since the texture coordinates can have discontinuities 
and singularities. The octex has no such artifacts since the index is simply the model s original vertex 
locations in 3-space. When a node is encountered during rendering which has multiple texture samples 
and associated normals, the normal of the deformed surface needs to be transformed back to model space 
to know which texture sample to use. If the model is deformed, the original un­transformed surface normals 
will need to be stored in the model in the same way as the untransformed vertices. 7 Memory Memory requirements 
of the octex will vary in comparison to a 2D texture of similar detail. The storage size of the leaf 
nodes can be exactly the same as the storage size of a pixel in a 2D texture map. However, the size of 
the parent nodes can be quite a bit larger de­pending on the implementation details. A simple implementation 
will require up to eight pointers in the parent nodes. At four bytes each, these nodes are signi.cantly 
larger than the leaf nodes. The overhead of the pointers can be reduced by using a pointerless oc­tree 
[Samat 1991]. These representations use less memory than the traditional pointer-based tree. In addition, 
storing extra data in a leaf node (such as normal information) increases the size of that node. The size 
of an octex will be larger than the best 2D texture in cases where there is a good 2D mapping and when 
the texture is of uniform detail. A simple square is very easy to texture using a 2D image. When the 
texture contains uniform detail and the smallest detail is 0.1% of the area of the square, the 2D texture 
map will require 1 million pixels, approximately 3MB of memory. For the same simple object, the octex 
will be larger. There will still be 1 million leaf nodes using the same 3MB, but there will be some cost 
for the parents. In this case there will be about 1/3 as many parent nodes as leaf nodes since each parent 
node will only have 4 potential children. Since all potential children will exist, those parents will 
not have any colors. Using the simple pointer-based octree, they will require 4 pointers to their children 
which cost an additional 5.3MB. The overhead cost of the tree is variable, depending on the uni­formity 
of texture detail. For uniform textures, all potential children will be created. The number of parent 
nodes in a tree of depth d is: d-1 k nd - 1 . n= n - 1 k=0 where n is the branching factor. When n is 
the maximum of 8, the octex has an overhead of 14% in terms of the number of nodes. In practice, we have 
found that a typical surface mode is locally .at. This means on average there are only about four potential 
children in most of the nodes. In this case, if the tree were built down to a uniform depth, the tree 
overhead would be about 33%. Figure 6: 2D texture map used in .gure 9a. Note the wasted space, distortion 
and discontinuities. A texture is rarely uniform in its detail across the surface. Unlike the octex, 
the traditional 2D texture wastes quite a bit of space to account for the high-detail regions, and space 
will also be lost to areas not used by the parameterization (.gure 6). The overhead of the octex can 
be amortized across multiple textures by storing them in the same tree. When there are N textures in 
the same octex, the overhead cost per texture shrinks by a factor of N. Suppose that the 1 million pixel 
texture above is all white except for one pixel. The 2D texture will still cost 3MB in memory. How­ever, 
the octex will be very small, just one leaf node and 10 parent nodes which can be less than 50 bytes! 
Real-world textures are not so extreme, but highly non-uniform texture detail is very common. In order 
for the cost of the octex to be totally negated, and the oc­tex to actually have fewer nodes than the 
corresponding 2D texture has pixels, we only need a small portion of the model to have one level less 
detail than the most detailed region. Assuming all nodes are of equal size, the break-even point is when: 
d-1 dd . nk + x * n= nk=0 where x is the percentage of leaf nodes which are at the full depth of the 
octex, and n is the branching factor. This turns out to be just: n-d + n - 2 x = n - 1 For any reasonable 
depth, and a typical branching rate of 4, x will be two-thirds. For the octex to have fewer nodes than 
the 2D texture has pixels, just one third of the leaf nodes need to be at one level less than the maximum. 
For example, when texturing a creature, more detail would prob­ably be painted on its face than on its 
feet, even though both areas are likely to have similar geometric complexity. For an octex to be smaller 
in memory than traditional 2D textures, some of the texture needs to have less detail than the most detailed 
regions to negate the overhead cost of the octex. The data structures for parent nodes are typically 
larger than those for a single pixel, thus the octree will usually require more memory than the visually 
equivalent 2D tex­ture. The octex does grow at the same rate as the 2D texture as details are added, 
therefore doesn t have the excessive growth nor­mally associated with volume textures.  8 Examples and 
Results We have implemented these techniques in a 3D paint program and a very simple software scan-line 
renderer. The only octex-speci.c part of the renderer is a shader that loads and samples the octex .les. 
All images use four samples per pixel. All speed measurements were taken on a 1 GHz Pentium III. The 
3D paint program allows the user to load in any model and immediately begin painting. To rapidly preview 
the model, the oc­tex is sampled at the vertices of the model and is drawn using inter­polated vertex 
colors. When painting begins, a high quality image is generated using our scan-line renderer. The user 
can then paint on the model, and the paint is applied to the octex. Figure 7 shows a model with a complex 
topology along with a vi­sualization of its octex at several points during the painting process. Initially 
the texture was roughed in, resulting in a coarse octex. As the texture was re.ned, the nodes in the 
tree become smaller and the structure of the model became evident. The .nal octex has 947,650 nodes, 
715,763 of which are leaf nodes. The average number of children per parent node is 4.08. Figure 8 shows 
an animating model. This is a simple cylinder that is bending to the left and wrinkling a bit as it bends. 
The texture sticks to the model appropriately. Figure 9 shows a creature textured with a 2D map, and 
the same creature textured with an octex. This creature took about 6 hours to model. Preparing the parameterization 
for the standard 2D texture maps initially took 3.5 hours, plus about 2 hours to repeatedly go back and 
adjust them. The mapping was made from an edge cut along the middle of the underside of the creature 
and down each leg, attempting to hide the seam. The texture coordinates were edited by hand to get the 
best mapping. Due to their length, the legs were not getting a large enough portion of the texture map, 
so additional cuts and edits were needed. Sometimes multiple 2D maps are used to avoid wasted space and 
to better .t the geometry at a consistent resolution, but introduce many more discontinuities. All told, 
the mapping process took nearly as long as modeling the geometry. Even in the .nal mapping (.gure 6), 
there are still artifacts due to the parameterization. The creature s octex has approximately 3 million 
nodes, 2.24 million of which are leaf nodes. There are roughly twice as many leaf nodes in the octex 
as there are pixels in the 2D texture map. The average number of children per parent node is 3.92. Not 
all potential children have been created, indicating that the texture de­tail is not uniform. The images 
were rendered at a resolution of 2048x1200. The traditionally mapped images took 1 minute 37 seconds 
to render, while the octex images required 2 minutes 43 Figure 7: Model with complex topology and its 
octex at various stages in the painting process. Note how the octex becomes more detailed as more detail 
is added to the texture. Holes in the re.ned surface represent potential children that have been unnecessary 
to create, as only leaf nodes are shown. The .nal image in the sequence is the actual model with the 
painted texture.  Figure 8: Several frames from an animated sequence of a bending cylinder. The octex 
behaves correctly when applied to a deforming model. seconds. The octex images render 68% slower than 
the traditional texture maps. This represents the increase in time to sample an oc­tex as opposed to 
a simple 2D array. The 2D texture takes 1.85MB losslessly compressed, and the octex is 7.4MB, also compressed 
without data loss. 9 Conclusions and Future Work We have presented a texture storage, painting, and rendering 
tech­nique that is free from the issues of parameterization between dis­parate topologies. We have shown 
how our method allows for the texture to be exactly as detailed as the paint that created it. While the 
structure of the texture data is more expensive than a traditional 2D map, it is signi.cantly cheaper 
than a full 3D volume texture. We consider the expense incurred from the size increase to be well worth 
the time saved by eliminating the solution of iterative param­eterization adjustments. There are several 
areas of expansion and improvement open to this method: Time repainting a model changed during production 
could be reduced if data within the octex were able to shift to match the areas where the model had changed. 
 Filtering and sampling could both be improved. Since the oc­tex represents a 3D extension of MIP mapping, 
an improve­ment on 2D MIP mapping [McCormack et al. 1999] may be applicable. The use of more sophisticated 
.lter kernels will also be required.  Allow paint to be stored not only at the surface position, but 
also at a location displaced along the surface normal. For ex­ample, a creature s skin texture would 
be painted normally, but the texture of muscles and fat under the skin could be painted beneath a model 
s surface. This sub-dermal color could be blended with the skin color during rendering.  10 Acknowledgments 
Thanks to Lawrence Kesteloot, Drew Olbrich, Dave McAllister, Peter-Pike Sloan, Thouis Jones, Daniel Wexler, 
and most especially to Mark Edwards. Thanks also to Gail Currey, Phil Peterson, and Andy Hendrickson 
at ILM, and Aron Warner and Ed Leonard at PDI/DreamWorks for their support, and John Hughes, JP Lewis, 
Vicki Caul.eld, and our referee for all the time and help.   References BENNIS, C., V ´ ESIAS, G., 
AND GAGALOWICZ,EZIEN, J.-M., IGL ´ A. 1991. Piecewise surface .attening for non-distorted texture mapping. 
In Computer Graphics (Proceedings of SIGGRAPH 91), vol. 25, 237 246. BENSON, D., AND DAVIS, J. 2002. 
Octree textures. In Pro­ceedings of ACM SIGGRAPH 2002, ACM Press / ACM SIG-GRAPH, Computer Graphics Proceedings, 
Annual Conference Series, 101 106. BERMAN, D. F., BARTELL, J. T., AND SALESIN, D. H. 1994. Multiresolution 
painting and compositing. In Proceedings of SIGGRAPH 94, ACM SIGGRAPH / ACM Press, Orlando, Florida, 
Computer Graphics Proceedings, Annual Conference Series, 85 90. BLINN, J. F., AND NEWELL, M. E. 1976. 
Texture and re.ection in computer generated images. Communications of the ACM 19, 10, 542 547. BOADA, 
I., NAVAZO, I., AND SCOPIGNO, R. 2001. Multiresolu­tion volume visualization with a texture-based octree. 
The Visual Computer 17, 3, 185 197. DAILY, J., AND KISS, K. 1995. 3d painting: paradigms for paint­ing 
in a new dimension. In Proceedings of CHI 95, ACM Press. FRISKEN, S. F., PERRY, R. N., ROCKWOOD, A. P., 
AND JONES, T. R. 2000. Adaptively sampled distance .elds: A general rep­resentation of shape for computer 
graphics. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addi­son Wesley Longman, Computer 
Graphics Proceedings, Annual Conference Series, 249 254.  Figure 9: Model with (a) a 2D texture and 
with (b) an octexture. HAKER, S., ANGENET, S., TANNENBAUM, A., KIKINIS, R., AND SAPRIO, M. H. 2000. Conformal 
surface parameterization for texture mapping. Transactions of Visualization and Computer Graphics 6, 
2, 181 189. HANRAHAN, P., AND HAEBERLI, P. E. 1990. Direct wysiwyg painting and texturing on 3d shapes. 
In Computer Graphics (Pro­ceedings of SIGGRAPH 90), vol. 24, 215 223. HUNTER, A., AND COHEN, J. D. 2000. 
Uniform frequency im­ages: adding geometry to images to produce space-ef.cient tex­tures. In IEEE Visualization 
2000, 243 250. IGARASHI, T., AND COSGROVE, D. 2001. Adaptive unwrapping for interactive texture painting. 
In 2001 ACM Symposium on Interactive 3D Graphics, 209 216. ´ LEVY, B. 2001. Constrained texture mapping 
for polygonal meshes. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, Computer Graphics 
Proceedings, Annual Conference Series, 417 424. MA, S. D., AND LIN, H. 1988. Optimal texture mapping. 
In Eurographics 88, 421 428. MAILLOT, J., YAHIA, H., AND VERROUST, A. 1993. Interactive texture mapping. 
In Proceedings of SIGGRAPH 93, Computer Graphics Proceedings, Annual Conference Series, 27 34. MCCORMACK, 
J., PERRY, R., FARKAS, K. I., AND JOUPPI, N. P. 1999. Feline: Fast elliptical lines for anisotropic texture 
map­ping. In Proceedings of SIGGRAPH 99, ACM SIGGRAPH / Addison Wesley Longman, Los Angeles, California, 
Computer Graphics Proceedings, Annual Conference Series, 243 250. PEACHEY, D. R. 1985. Solid texturing 
of complex surfaces. In Computer Graphics (Proceedings of SIGGRAPH 85), vol. 19, 279 286. PERLIN, K. 
1985. An image synthesizer. In Computer Graphics (Proceedings of SIGGRAPH 85), vol. 19, 287 296. PIPONI, 
D., AND BORSHUKOV, G. D. 2000. Seamless tex­ture mapping of subdivision surfaces by model pelting and 
tex­ture blending. In Proceedings of ACM SIGGRAPH 2000, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, 
Com­puter Graphics Proceedings, Annual Conference Series, 471 478. SAMAT, H. 1990. Applications of Spatial 
Data Structures: Com­puter Graphics, Image Processing, and GIS. Addison-Wesley. SANDER, P. V., SNYDER, 
J., GORTLER, S. J., AND HOPPE, H. 2001. Texture mapping progressive meshes. In Proceedings of ACM SIGGRAPH 
2001, ACM Press / ACM SIGGRAPH, Com­puter Graphics Proceedings, Annual Conference Series, 409 416. SLOAN, 
P.-P. J., WEINSTEIN, D. M., AND BREDERSON, J. D. 1998. Importance driven texture coordinate optimization. 
Com­puter Graphics Forum 17, 3, 97 104. TAMMINEN, M., AND SAMET, H. 1984. Ef.cient octree conver­sion 
by connectivity labeling. In Computer Graphics (Proceed­ings of SIGGRAPH 84), vol. 18, 43 51. WESTERMANN, 
R., AND ERTL, T. 1997. A multiscale approach to integrated volume segmentation and rendering. Computer 
Graphics Forum 16, 3 (August), 117 128. WILHELMS, J., AND GELDER, A. V. 1994. Multi-dimensional trees 
for controlled volume rendering and compression. 27 34. WILLIAMS, L. 1983. Pyramidal parametrics. In 
Computer Graph­ics (Proceedings of SIGGRAPH 83), vol. 17, 1 11. ZHANG, H., AND HOFF, K. 1997. Fast backface 
culling using normal masks. In Symposium on Interactive 3D Graphics, ACM Press, 103 106.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566650</article_id>
		<sort_key>769</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Stylization and abstraction of photographs]]></title>
		<page_from>769</page_from>
		<page_to>776</page_to>
		<doi_number>10.1145/566570.566650</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566650</url>
		<abstract>
			<par><![CDATA[Good information design depends on clarifying the meaningful structure in an image. We describe a computational approach to stylizing and abstracting photographs that explicitly responds to this design goal. Our system transforms images into a line-drawing style using bold edges and large regions of constant color. To do this, it represents images as a hierarchical structure of parts and boundaries computed using state-of-the-art computer vision. Our system identifies the meaningful elements of this structure using a model of human perception and a record of a user's eye movements in looking at the photo; the system renders a new image using transformations that preserve and highlight these visual elements. Our method thus represents a new alternative for non-photorealistic rendering both in its visual style, in its approach to visual form, and in its techniques for interaction.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[eye-tracking]]></kw>
			<kw><![CDATA[image simplification]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[visual perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Hierarchical</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15034370</person_id>
				<author_profile_id><![CDATA[81100499844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeCarlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14199817</person_id>
				<author_profile_id><![CDATA[81100578485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Santella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383286</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGRAWALA, M., AND STOLTE, C. 2001. Rendering effective route maps: improving usability through generalization. In Proc. of ACM SIGGRAPH 2001, 241-249.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245466</ref_obj_id>
				<ref_obj_pid>245456</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AHUJA, N. 1996. A transform for multiscale image segmentation by integrated edge and region detection. IEEE Trans. on Pattern Analysis and Machine Intelligence 18, 12, 1211-1235.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BURT, P., AND ADELSON, E. 1983. The Laplacian pyramid as a compact image code. IEEE Trans. on Communications 31, 4, 532-540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CAMPBELL, F., AND ROBSON, J. 1968. Application of Fourier analysis to the visibility of gratings. Journal of Physiology 197, 551-566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>848599</ref_obj_id>
				<ref_obj_pid>846227</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHRISTOUDIAS, C., GEORGESCU, B., AND MEER, P. 2002. Synergism in low level vision. In Proc. ICPR 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>513076</ref_obj_id>
				<ref_obj_pid>513073</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COMANICIU, D., AND MEER, P. 2002. Mean shift: A robust approach toward feature space analysis. IEEE Trans. on Pattern Analysis and Machine Intelligence 24, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CURTIS, C. 1999. Non-photorealistic animation. In ACM SIGGRAPH 1999 Course Notes #17 (Section 9).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344792</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DEUSSEN, O., AND STROTHOTTE, T. 2000. Computer-generated pen-and-ink illustration of trees. In Proc. of ACM SIGGRAPH 2000, 13-18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DUCHOWSKI, A., AND VERTEGAAL, R. 2000. Eye-based interaction in graphical systems: Theory and practice. In ACM SIGGRAPH 2000 Course Notes #5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319974</ref_obj_id>
				<ref_obj_pid>2318987</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DUCHOWSKI, A. 2000. Acuity-matching resolution degradation through wavelet coefficient scaling. IEEE Trans. on Image Processing 9, 8 (Aug.), 1437-1440.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732296</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DURAND, F., OSTROMOUKHOV, V., MILLER, M., DURANLEAU, F., AND DORSEY, J. 2001. Decoupling strokes and high-level attributes for interactive traditional drawing. In Proceedings of the 12th Eurographics Workshop on Rendering, 71-82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192223</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FINKELSTEIN, A., AND SALESIN, D. 1994. Multiresolution curves. In Proc. of ACM SIGGRAPH 94, 261-268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[FOLEY, J., VAN DAM, A., FEINER, S., AND HUGHES, J. 1997. Computer Graphics: Principles and Practice, 2nd edition. Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GOOCH, B., AND GOOCH, A. 2001. Non-Photorealistic Rendering. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280950</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GOOCH, A. A., GOOCH, B., SHIRLEY, P., AND COHEN, E. 1998. A non-photorealistic lighting model for automatic technical illustration. In Proc. of ACM SIGGRAPH 98, 447-452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P. 1990. Paint by numbers: Abstract image representations. In Proc. of ACM SIGGRAPH 90, 207-214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[HANDFORD, M. 1987. Where's Waldo? Little, Brown and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[HENDERSON, J. M., AND HOLLINGWORTH, A. 1998. Eye movements during scene viewing: An overview. In Eye Guidance in Reading and Scene Perception, G. Underwood, Ed. Elsevier Science Ltd., 269-293.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618859</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[HERMAN, I., AND DUKE, D. 2001. Minimal graphics. IEEE Computer Graphics and Applications 21, 6, 18-21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345074</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., AND ZORIN, D. 2000. Illustrating smooth surfaces. In Proc. of ACM SIGGRAPH 2000, 517-526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In Proc. of ACM SIGGRAPH 98, 453-460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>735238</ref_obj_id>
				<ref_obj_pid>647781</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A. 2001. Paint by relaxation. In Computer Graphics International, 47-54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[HOFFMAN, D. D. 1998. Visual intelligence: how we create what we see. Norton.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[JUST, M. A., AND CARPENTER, P. A. 1976. Eye fixations and cognitive processes. Cognitive Psychology 8, 441-480.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[KELLY, D. 1984. Retinal inhomogenity: I. spatiotemporal contrast sensitivity. Journal of the Optical Society of America A 74, 1, 107-113.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J. J., M. A. BOUMAN, A. B. D. M., AND SLAPPENDEL, S. 1978. Perimetry of contrast detection thresholds of moving spatial sine wave patterns. II. the far peripheral visual field (eccentricity 0-50). Journal of the Optical Society of America A 68, 6, 850-854.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J. J. 1984. The structure of images. Biological Cybernetics 50, 363-370.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J. J. 1984. What does the occluding contour tell us about solid shape? Perception 13, 321-330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311607</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[KOWALSKI, M. A., MARKOSIAN, L., NORTHRUP, J. D., BOURDEV, L., BARZEL, R., HOLDEN, L. S., AND HUGHES, J. 1999. Art-based rendering of fur, grass, and trees. In Proc. of ACM SIGGRAPH 99, 433-438.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[LEYTON, M. 1992. Symmetry, causality, mind. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>528688</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[LINDEBERG, T. 1994. Scale-Space Theory in Computer Vision. Kluwer Academic Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[LITWINOWICZ, P. 1997. Processing images and video for an impressionist effect. In Proc. of ACM SIGGRAPH 97, 407-414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[MACKWORTH, N., AND MORANDI, A. 1967. The gaze selects informative details within pictures. Perception and Psychophysics 2, 547-552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[MANNOS, J. L., AND SAKRISON, D. J. 1974. The effects of a visual fidelity criterion on the encoding of images. IEEE Trans. on Information Theory 20, 4, 525-536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[MARKOSIAN, L., KOWALSKI, M. A., TRYCHIN, S. J., BOURDEV, L. D., GOLDSTEIN, D., AND HUGHES, J. F. 1997. Real-time nonphotorealistic rendering. In Proc. of ACM SIGGRAPH 97, 415-420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095712</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[MARR, D. 1982. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. W. H. Freeman, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>507491</ref_obj_id>
				<ref_obj_pid>507489</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[MEER, P., AND GEORGESCU, B. 2001. Edge detection with embedded confidence. IEEE Trans. on Pattern Analysis and Machine Intelligence 23, 12, 1351-1365.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[PATTANAIK, S. N., FERWERDA, J. A., FAIRCHILD, M. D., AND GREENBERG, D. P. 1998. A multiscale model of adaptation and spatial vision for realistic image display. In Proc. of ACM SIGGRAPH 98, 287-298.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618852</ref_obj_id>
				<ref_obj_pid>616072</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[REDDY, M. 2001. Perceptually optimized 3D graphics. IEEE Computer Graphics and Applications 21, 5 (September/October), 68-75.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[REGAN, D. 2000. Human Perception of Objects: Early Visual Processing of Spatial Form Defined by Luminance, Color, Texture, Motion and Binocular Disparity. Sinauer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[ROVAMO, J., AND VIRSU, V. 1979. An estimation and application of the human cortical magnification factor. Experimental Brain Research 37, 495-510.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[SAITO, T., AND TAKAHASHI, T. 1990. Comprehensible rendering of 3-D shapes. In Proc. of ACM SlGGRAPH 90, 197-206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508544</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[SANTELLA, A., AND DECARLO, D. 2002. Abstracted painterly renderings using eye-tracking data. In Proc. of the Second International Symp. on Non-photorealistic Animation and Rendering (NPAR).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340923</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[SHIRAISHI, M., AND YAMAGUCHI, Y. 2000. An algorithm for automatic painterly rendering based on local source image approximation. In Proc. of the First International Symp. on Non-photorealistic Animation and Rendering (NPAR), 53-58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>332445</ref_obj_id>
				<ref_obj_pid>332040</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[SIBERT, L. E., AND JACOB, R. J. K. 2000. Evaluation of eye gaze interaction. In Proc. CHI 2000, 281-288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551277</ref_obj_id>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[TRUCCO, E., AND VERRI, A. 1998. Introductory Techniques for 3-D Computer Vision. Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78223</ref_obj_id>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[TUFTE, E. R. 1990. Envisioning Information. Graphics Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>303065</ref_obj_id>
				<ref_obj_pid>302979</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[VERTEGAAL, R. 1999. The gaze groupware system: Mediating joint attention in mutiparty communication and collaboration. In Proc. CHI '99, 294-301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[WINKENBACH, G., AND SALESIN, D. H. 1994. Computer-generated pen-and-ink illustration. In Proc. of ACM SIGGRAPH 94, 91-100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[YARBUS, A. L. 1967. Eye Movements and Vision. Plenum Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[ZEKI, S. 1999. Inner Vision: An Exploration of Art and the Brain. Oxford Univ. Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stylization and Abstraction of Photographs Doug DeCarlo Anthony Santella Department of Computer Science 
&#38; Center for Cognitive Science Rutgers University  Abstract Good information design depends on clarifying 
the meaningful structure in an image. We describe a computational approach to stylizing and abstracting 
photographs that explicitly responds to this design goal. Our system transforms images into a line-drawing 
style using bold edges and large regions of constant color. To do this, it represents images as a hierarchical 
structure of parts and boundaries computed using state-of-the-art computer vision. Our system identi.es 
the meaningful elements of this structure using a model of human perception and a record of a user s 
eye movements in looking at the photo; the system renders a new image using trans­formations that preserve 
and highlight these visual elements. Our method thus represents a new alternative for non-photorealistic 
ren­dering both in its visual style, in its approach to visual form, and in its techniques for interaction. 
CR Categories: I.3.3 [Computer Graphics]: Picture/Image Gen­eration; I.4.10 [Image Processing and Computer 
Vision]: Image Representation Hierarchical Keywords: non-photorealistic rendering, visual perception, 
eye­tracking, image simpli.cation 1 Introduction The success with which people can use visual information 
masks the complex perceptual and cognitive processing that is required. Each time we direct our gaze 
and attention to an image, our visual intelligence interprets what we see by performing sophisticated 
in­ference to organize the visual .eld into coherent regions, to group the regions together as manifestations 
of meaningful objects, and Copyright &#38;#169; 2002 by the Association for Computing Machinery, Inc. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use is 
granted without fee provided that copies are not made or distributed for commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights for components of this work 
owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to 
republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. 
&#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 to explain the objects identities and causal histories 
[Marr 1982; Leyton 1992; Hoffman 1998; Regan 2000]. The fact that looking at a picture so often brings 
an effortless and detailed understanding of a situation testi.es to the precision and subtlety of these 
inferences. Our visual abilities have limits, of course. Good information design depends on strategies 
for reducing the perceptual and cog­nitive effort required to understand an image. When illustrations 
are rendered abstractly, designers can take particularly radical steps to clarify their structure. Tufte 
[1990] for example suggests mak­ing detail as light as possible to keep the main point of a presenta­tion 
perceptually salient, and warns against adding any detail that doesn t contribute to the argument of 
a presentation. Thus expert illustration in instruction manuals portrays .ne detail only on the object 
parts relevant to the current task. When artists purposely invert these heuristics, as in the popular 
Where s Waldo? pictures [Handford 1987] which offer the visual system no salient cues to .nd their distinguished 
character they make extracting visual in­formation acutely demanding. This paper describes a computational 
approach to stylizing and abstracting photographs that responds in explicit terms to the design goal 
of clarifying the meaningful visual structure in an image. Our approach starts from new image representations 
that recognize the visual parts and boundaries inherent in a photograph. These repre­sentations provide 
the scaffolding to preserve and even emphasize key elements of visual form. A human user interacts with 
the sys­tem to identify meaningful content of the image. But no artistic talent is required, nor even 
a mouse: the user simply looks at the image for a short period of time. A perceptual model translates 
the data gathered from an eye-tracker into predictions about which ele­ments of the image representation 
carry important information. The simpli.cation process itself can now apply an ambitious range of transformations, 
including collapsing away details, averaging col­ors across regions, and overlaying bold edges, in a 
way that high­lights the meaningful visual elements. Results are shown above and in Section 5. Since 
we aim for abstraction, not realism, our research falls squarely within the .eld of non-photorealistic 
rendering (NPR) [Gooch and Gooch 2001]. In the remainder of this section, we sit­uate our approach within 
this .eld and clarify the contribution that our approach makes. Then, after a review of relevant research 
in human and machine vision in Section 2, we describe .rst our im­age analysis algorithm in Section 3 
and then our perceptual model and simpli.cation transformations in Section 4. 1.1 Motivations and Contributions 
In the hands of talented artists, abstraction becomes a tool for ef­fective visual communication. Take 
the work of Toulouse-Lautrec, in Figure 1 for example. No account of the poster, advertising the Parisian 
cabaret Moulin Rouge, could omit its exciting and mem­orable content. But the content commands the attention 
it does in no small part because of the directed, simpli.ed organization of the poster: Toulouse-Lautrec 
has rendered the scene with meaningful abstraction. As observed by vision scientists such as Zeki [1999], 
such abstraction results in an image that directs your attention to its most meaningful places and allows 
you to understand the struc­ture there without conscious effort. Such examples make sense of the aims 
of the .eld of non-photorealistic rendering, to produce ab­stract renderings that achieve more effective 
visual communication [Herman and Duke 2001]. Figure 1: Henri de Toulouse-Lautrec s Moulin Rouge La Goulue 
(Lithographic print in four colors, 1891). The organiza­tion of the contents in this poster focuses attention 
on the dancer La Goulue as she performs the Cancan. The use of bright, uniform col­ors distinguishes 
the .gure from the background, while the place­ment of strokes on her dress provides rich information 
about its shape and material. Meanwhile, her dance partner lacks the colors (but has detail strokes), 
and background objects such as the specta­tors, are simply drawn in silhouette. Abstraction depends on 
adopting a rendering style that gives the freedom to omit or remove visual information. Painterly process­ing, 
which abstracts images into collections of brush-strokes [Hae­berli 1990; Litwinowicz 1997; Hertzmann 
1998; Shiraishi and Ya­maguchi 2000], is a notable example of such a style. Our system transforms images 
into a line-drawing style using large regions of constant color; this style is very different from the 
painterly approaches of previous image-based work, and perhaps more closely approximates the style of 
printmaking of Figure 1. A similar visual style was used in the recent .lm Waking Life1 and for producing 
loose and sketchy animation [Curtis 1999]. Once a style is in place, the key problem for interactive 
and auto­matic NPR systems is to direct these resources of style to preserve meaningful visual form, 
while reducing extraneous detail. Visual form describes the relationship between pictures of objects 
and the physical objects themselves. Painterly abstraction can cue visual 1See http://www.wakinglifemovie.com. 
form heuristically by emphasizing parts and boundaries in an im­age through techniques such as aligning 
brush strokes perpendicu­lar to the image gradient [Haeberli 1990], terminating brush strokes at edges 
[Litwinowicz 1997], or drawing in a coarse-to-.ne fash­ion [Hertzmann 1998; Shiraishi and Yamaguchi 2000]. 
NPR ren­dering methods that work from geometric models can cue visual form in more general ways, by detecting 
edges that arise from oc­cluding contours or creases [Saito and Takahashi 1990; Markosian et al. 1997], 
and by determining appropriate directions for hatching [Hertzmann and Zorin 2000]. But models of visual 
form also have a fundamental role in understanding human and machine vision, and even human artistic 
style. For instance, Koenderink [1984b] proves that convex parts of the occluding contour of a smooth 
sur­face correspond to convexities of the surface, and that concave parts of the contour correspond to 
saddles; he then provides an example of D¨urer s engravings that exhibit changes in hatching technique 
where the sign of the contour curvature changed. Our system models visual form using state-of-the-art 
techniques from computer vision to identify the natural parts and boundaries in images [Comaniciu and 
Meer 2002; Meer and Georgescu 2001; Christoudias et al. 2002]. Our system is the .rst to formulate the 
process of abstraction completely in terms of a rich model of visual form. Automatic techniques are more 
limited in their abilities to reduce extraneous detail. This is because automatic techniques cannot as 
of yet identify the meaningful elements of visual form. (Some may ar­gue the problem will never be solved.) 
Selective omission is possi­ble in speci.c domains. The right illumination model can eliminate distracting 
variations in brightness [Gooch et al. 1998]. In drawing trees, texture information can be omitted in 
the center of the tree, es­pecially as it is drawn smaller [Kowalski et al. 1999; Deussen and Strothotte 
2000]. The design of route maps can draw upon rules that embody how people use maps effectively [Agrawala 
and Stolte 2001]. For general image-based techniques the options are few; automatic painterly rendering 
systems simply reduce global reso­lution by using larger brushes [Haeberli 1990; Litwinowicz 1997; Hertzmann 
1998; Shiraishi and Yamaguchi 2000]. Meaningful ab­straction can only be achieved through interaction. 
This paper is no exception. Our approach builds upon methods for manually direct­ing indication in pen-and-ink 
illustration [Winkenbach and Salesin 1994], and for controlling drawing [Durand et al. 2001] or painting 
[Hertzmann 2001] from an image using a hand-painted precision map. More detailed and time-consuming interaction 
is possible, as in the production of Waking Life, which relied on a combination of rotoscoping and other 
animation techniques. For interaction, this paper offers a new choice of modality eye movements [Santella 
and DeCarlo 2002] and contributes new al­gorithms that formalize the link between .xations, perception 
and visual form to use this input effectively. A summary of our process used to transform an image is 
as fol­lows: Instruct a user to look at the image for a short time, obtaining a record of eye movements. 
 Disassemble the image into its constituents of visual form us­ing visual analysis (image segmentation 
and edge detection).  Render the image, preserving the form predicted to be mean­ingful by applying 
a model of human visual perception to the eye-movement data.  We design the system conservatively, so 
that errors in visual analy­sis or .aws in the perceptual model do not noticeably detract from the result. 
Even so, manifestations of their limitations can be quite noticeable for certain images, such as those 
with complex textures. Nevertheless, we expect that advances in computer vision and hu­man vision can 
be used directly, enabling our system to make better and richer decisions.  (a) (b) (c) (d) Figure 2: 
(a) Original image; (b) Detected edges; (c) Color segmentation; contiguous regions of solid color in 
these images represent individual elements of the segmentation; (d) Color segmentation at a coarser scale 
(the image was .rst down-sampled by a factor of 16)  2 Background 2.1 Image Structure and Analysis Our 
approach uses low-level visual processing to form a hierarchi­cal description of the image to be transformed. 
The style of our out­put will be a line drawing uniformly colored regions with black lines. We use algorithms 
for edge detection and image segmenta­tion to gather the information necessary to produce such a display. 
There are a vast number of algorithms available for these processes; in this section, we simply describe 
which we are using and why. Computer vision texts (such as [Trucco and Verri 1998]) provide reviews of 
alternative techniques. For the remainder of this sec­tion, examples of processing will be given for 
the photograph in Figure 2(a), which is a 1024× 768 color image. Edge detection is the process of extracting 
out locations of high contrast in an image that are likely to form the boundary of objects (or their 
parts) in a scene. This process is performed at a particular scale (using a .lter of a speci.c size). 
The Canny edge detector [Trucco and Verri 1998] is a popular choice for many applications, as it typically 
produces cleaner results. We use the robust variant of the Canny detector presented by Meer and Georgescu 
[2001], which additionally uses internal performance assessment to detect faint edges while disregarding 
spurious edges arising in heavily tex­tured regions. Detected edges (using a 5 × 5 .lter) are displayed 
in Figure 2(b); processing took a few seconds. An image segmentation is simply a partition of an image 
into contiguous regions of pixels that have similar appearance, such as color or texture [Trucco and 
Verri 1998]. Each region has aggregate properties associated with it, such as its average color. We choose 
the algorithm described by Comaniciu and Meer [2002] for the ro­bust segmentation of color images, as 
it produces quite clean re­sults. Within this algorithm, colors are represented in the perceptu­ ally 
uniform color space L*uv. [Foley et al. 1997] which produces region boundaries that are more meaningful 
for human observers. The parameters of this algorithm include a spatial radius hs (similar to the radius 
of a .lter), a color difference threshold hr, and the size of the minimum acceptable region M. The output 
of this segmenta­tion algorithm on our test image is shown in Figure 2(c) for hs =7 (in pixel units), 
hr =6.5(in L*uv. units), and M =20 (pixels); processing took slightly over a minute. These two algorithms 
can be combined together into a single sys­tem [Christoudias et al. 2002], yielding even better results; 
edges can be used to predict likely segmentation boundaries, and vice versa. A freely available implementation 
of these algorithms is available at http://www.caip.rutgers.edu/riul . Scale-space theory [Koenderink 
1984a; Lindeberg 1994] pro­vides a description of images in terms of how content across differ­ent resolutions 
(scales) is related. This is formalized with a notion of causality: as images are blurred, smaller features 
come together to form larger objects so that all coarse features have a cause at a .ner scale. This theory 
serves as the basis for our hierarchical repre­sentation of the image, described in Section 3. Our algorithm 
uses segmentation algorithms applied at a variety of scales, and .nds containment relationships between 
their results. 2.2 Visual Perception Our application relies on the fact that human eye movements give 
strong evidence about the location of meaningful content in an im­age. This section brie.y summarizes 
the psychological research about the architecture of human vision that informs our work. We focus in 
particular on perception of static imagery. People can examine only a small visual area at one time, 
and so understand images by scanning them in a series of .xations,when the eye is stabilized at a particular 
point. The eye moves between these .xations in discrete, rapid movements called saccades, typi­cally 
without conscious planning. The eye can move in other ways, such as smoothly pursuing a moving object, 
but the saccades and .xations are the key to understanding static images. Fixations follow the meaningful 
locations in an image closely [Mackworth and Morandi 1967; Henderson and Hollingworth 1998], and their 
durations provide a rough estimate of the process­ing expended on understanding corresponding parts of 
the image [Just and Carpenter 1976]; .xations that land on uninteresting or unimportant objects are very 
short [Henderson and Hollingworth 1998]. Naturally, the information a person needs depends on their task, 
and .xation locations change accordingly [Yarbus 1967; Just and Carpenter 1976]. Within each .xation, 
the .ne detail that will be visible depends on its contrast, its spatial frequency and its eccentricity, 
or angular distance from the center of the .eld of view [Mannos and Sakrison 1974; Koenderink et al. 
1978; Kelly 1984]. Contrast is a relative measure of intensity of a stimulus, as compared to its surroundings 
(it is dimensionless). In psychophysical studies, the typical measure of contrast between two intensities 
l1 and l2 (with l1 being brighter) is the Michelson contrast: l1-l2 [Regan 2000] (which is always be­ 
l1+l2 tween 0 and 1). Contrast sensitivity, which is simply the reciprocal of the contrast, is the typical 
measure used by psychophysicists to gauge human visual performance. Drawing on results from experi­ments 
on the perception of sine gratings (i.e. blurry stripes), Man­nos and Sakrison [1974] provide a contrast 
sensitivity model that describes the maximum contrast sensitivity (or minimum contrast) visible at a 
particular frequency f (in cycles per degree): -(0.144f)1.1 A(f )=1040(0.0192 +0.144 f )e(1) This is 
graphed in Figure 3(a) in log-log scale; frequency-contrast pairs in the shaded region correspond to 
gratings discernible to the human eye. Above this curve, the gratings simply appear as a uni­form gray. 
This particular model has been used in graphics for 400 100 10 1 (a) (b) Figure 3: (a) A contrast sensitivity 
function describes the maximum discernible contrast sensitivity as a function of frequency (of a sinu­soidal 
grating). Values above the curve (which have low contrast) are invisible. (b) Fixations gathered using 
the image in Figure 2(a). Each circle has its center at the estimated location of the .xation, while 
its diameter indicates its duration (the scale in the lower left measures 1 second). producing perceptually 
realistic images of scenes [Pattanaik et al. 1998; Reddy 2001]. Campbell and Robson [1968] adjust this 
model for the viewing of square-wave gratings (i.e. crisp stripes). Color contrast is a much more complicated 
story, however, and is not well understood [Regan 2000]. For colors that only differ in luminance, the 
above model applies reasonably well. When they differ further, sensitivity is typically increased. Further 
psychophysical studies are also required to develop better models for natural scenes as op­posed to simple 
repeating patterns. Contrast sensitivity decreases as a function of eccentricity [Koenderink et al. 1978]. 
A concise model describing this reduc­tion [Rovamo and Virsu 1979] has been used for modeling visual 
acuity (which describes the highest spatial frequency that can be re­solved at maximum contrast) for 
performance-based visualization of 3-D environments [Reddy 2001] or for deciding an appropriate brush 
size in painterly rendering [Santella and DeCarlo 2002]. In terms of the eccentricity angle e (in degrees), 
the sensitivity reduc­tion factor M(e) is 1 at the fovea center and decreases towards 0 with increasing 
e. The resulting contrast sensitivity function that depends on eccentricity is simply A( f )M(e). These 
limits on sensitivity within the visual .eld .t hand-in-hand with the ability of the visual system to 
integrate information with movements of the eyes and head. Thus, we combine information about .xation 
location with information about sensitivity to .ne detail in making decisions about which features in 
an image were prominently visible to a user. The key tool to obtain this information is an eye-tracker 
capable of sampling an observer s point of regard over time. Eye-tracker technology is steadily improving; 
they can now be placed in any work environment and used with just a brief calibration step. Upon viewing 
the image in Figure 2(a) for .ve seconds, our ISCAN ETL­500 eye-tracker (with an RK-464 pan/tilt camera) 
tracks the sub­ject s eye movements. Corresponding .xation locations and dura­tions are detected using 
a velocity threshold [Duchowski and Verte­gaal 2000], and are plotted in Figure 3(b) as circles centered 
at the .xation location; the diameter of the circles is proportional to the duration. Eye-trackers have 
seen appreciable use in human-computer in­teraction research. A common function is as a cursor [Sibert 
and Jacob 2000], either on the screen or in a virtual environment. Other roles include assessing user 
attention in teleconferencing systems [Vertegaal 1999], and using gaze to guide decisions for image degradation 
and compression [Duchowski 2000]. In our case, we use the eye-tracker indirectly in the computer interface. 
Our instructions are simply look at the image ; and viewers do not have to use or attend to the eye-tracker 
or to their eye movements just to the image. People are already adept in lo­cating the desired information 
in images. We aim to exploit this natural ability in our interface, not to distract from it by suggesting 
that the user make potentially unnatural voluntary eye-movements. This paradigm still enables a computer 
system to draw substantial inferences about a user s attention and perception. We expect to see it used 
more widely.  3 Hierarchical Image Representation Our image simpli.cations rest on a hierarchical representation 
of visual form in input images. Each image is analyzed in terms of its constituent regions by performing 
a segmentation at many dif­ferent scales. We depend on regularities in scale-space to assemble this stack 
of image segmentations into a meaningful hierarchy. This section describes how we create this representation 
and how we ex­tract edges using the methods described in Section 2.1. Other multi­scale segmentation 
algorithms already exist in the vision commu­nity [Ahuja 1996]; here we draw on available code to help 
allow our results to be more easily reproduced [Christoudias et al. 2002]. We start with an image pyramid 
[Burt and Adelson 1983], which is a collection of images; each one is down-sampled by a constant . factor 
from the previous one. We use a constant factor of 2(in­stead of the typical value of 2), which produces 
more consistency between structures across levels, and admits a simple algorithm to infer a hierarchy. 
A segmentation is computed for each image in the pyramid (using the parameters: hs = 7, hr = 6.5, M = 
20). Fig­ure 2(d) shows the segmentation result of an image down-sampled by a factor of 16. While the 
alternative of segmenting the original image at a series of increasing spatial resolutions is more faithful 
to scale-space, it is substantially slower. Edges are detected in the original image using a 5 × 5kernel. 
For this application, we have not found it necessary to detect edges at different scales. Through a process 
called edge tracking [Trucco and Verri 1998], detected edge pixels come together to form indi­vidual 
curves, which are each represented as a sequence of pixel locations. This results in a list of edge chains. 
These are the source of the curved strokes drawn in our output. 3.1 Building the Hierarchy We now form 
a hierarchy starting from the regions in the segmen­tation of the bottom image of the pyramid (the largest 
image in the stack). Scale-space theory suggests regions in .ner scale seg­mentations are typically included 
within regions at coarser scales (there are exceptions, however) [Lindeberg 1994]. These contain­ments 
induce a hierarchy of image structures. Figure 4(a) shows an idealized example of such a hierarchy. Regions 
A, B, C and D are detected at a .ne scale, where A and B combine into AB and C and D combine into CD 
at a coarser scale, and all combine into a single region ABCD at an even coarser scale. To represent 
this hierarchy, we can construct a tree (on the right of Figure 4(a)) that documents the containment 
relationships of regions found by seg­menting at various scales. The nodes in the tree contain properties 
of that region, such as its area, boundary, and average color. Noise in the images and artifacts from 
the segmentation prevent this from being a perfect process. Even so, we can de.ne a hier­archy where 
parents are de.ned as the union of the areas of their children regions. In doing this, virtually all 
of the cases are clear­cut, allowing us to use a simple algorithm for building the hierarchy from the 
leaves up. For questionable situations we rely on a sim­ple heuristic to make the choice, with the possibility 
of deferring a choice should it cause an invalid tree (regions must be connected). The algorithm is as 
follows. such that every path from the root to a leaf includes precisely one node from F.) In producing 
the rendering, our system smooths the boundaries of these frontier regions, draws them onto the canvas, 
and then overlays lines using the edge chains. The perceptual model, which relies on eye movement data 
to compute eccentricities, is used to decide where to place the frontier and which lines to draw. A depth-.rst 
search de.nes this frontier; the access to the perceptual model is a boolean function SPLIT(n), which 
determines whether to draw all children of the node n based on the available .xation data (via eccentricities). 
The recursion pro­ ceeds by visiting a node n.If SPLIT(n) is true, then all of its children (a) (b) Figure 
4: (a) A hierarchical segmentation, and its corresponding tree representation; (b) the overlap rule used 
to infer this hierarchy from the segmentation pyramid. Provided with the pyramid of segmentations, a 
leaf is created for each region in the bottom (.nest scale) segmentation of the pyramid. Add each of 
these regions to the active set of regions R (which will be maintained to be those regions which currently 
have no parent).  Proceeding up the pyramid, at level L:  For each active region A E R (which comes 
from a lower level), compute its best potential parent PA.From the set of regions {Bi} on level L which 
overlap with A, PA is selected from this set as the one which maximizes: area(A * Bi) overlap(A,Bi)= 
color(A)- color(Bi) +1 * where colors are expressed in L*uv* . This is depicted in Figure 4(b). Assign 
regions to parents in order of increasing area(A * PA), contingent on it being connected to the children 
PA already has (this prevents the formation of disconnected regions).  When assigned, remove A from 
R and add PA (if not already present); unassigned regions remain in R.  Of the remaining regions in 
R, those under 500 pixels are merged into adjacent regions (as they probably violated scale­space containment). 
A root region that represents the entire image parents the rest.  4 Rendering with a Perceptual Model 
The rendering process works directly from the hierarchical segmen­tation and edge chains that were described 
in the last section. The output is abstracted by pruning the segmentation tree and list of edge chains: 
working from a set of .xations, structure is removed if the perceptual model predicts the user did not 
see it. This percep­tual model extends our previous work [Santella and DeCarlo 2002] in two ways that 
rely on our new representations of visual form. First, the new model uses region structure to judge contrast 
sen­sitivity (instead of acuity). Second, it computes perceptibility of image regions rather than individual 
pixels. This allows the new model to be much more selective in highlighting important image parts and 
boundaries and in discarding extraneous detail. With our new model, a rendering of a line-drawing using 
the hierarchy simply corresponds to drawing those regions on a partic­ular frontier of the segmentation 
tree. (A frontier is a set F of nodes are visited. Otherwise, it is simply marked as a frontier node. 
4.1 Using .xation data Our new model interprets eye-tracking data by reference to our hierarchical description 
of image contents. Our raw data is a time-indexed sequence of points-of-regard measured pas­sively by 
an eye-tracker as the viewer examines the image. We parse this into k .xations [Duchowski and Vertegaal 
2000] { } fi =(xi,yi,ti)| i E [1..k]where (xi,yi)are the image coordinates of the .xation point, and 
ti is its duration. In many cases, eccentricities of regions with respect to a partic­ular .xation are 
solely determined using that .xation. However, estimating the target of each .xation enables sharp delineations 
of detail in the output. Each .xation fi is associated with a target re­gion ni in the segmentation tree 
this represents the coherent part of the image that was viewed. We use the following method to de­termine 
ni, as human vision research currently has little to say about this. Centered at each .xation is a circle 
whose size matches 5 de­grees of the center of the viewer s visual .eld roughly the size of their fovea 
[Regan 2000] (180 pixels across in our setup). We deter­mine ni as the smallest region that substantially 
overlaps this circle: there must be a set of leaf regions within ni that are entirely inside the circle, 
which, taken together, comprise an area greater than half the circle and also greater than half of ni. 
When no such region exists, the target cannot be identi.ed, and ni is set to be the leaf that contains 
the .xation point. The set of nodes N ={ni | i E [1..k]}thus reports the parts of the image that the 
user looked at. For a particular .xation fi and region r,when r is either an an­cestor or descendant 
of ni, then its eccentricity with respect to fi measures the angular distance to the closest pixel in 
r.Otherwise, r is assigned a constant eccentricity eoutside for fi; this provides a parameter that affects 
the level of content in the distant background (we use eoutside =10°). This regime induces discontinuities 
in es­timated eccentricity at part boundaries, which means background information that is adjacent to 
important regions is not inappropri­ately emphasized, as it was in our previous approach [Santella and 
DeCarlo 2002]. 4.2 Region Perceptibility The pruning of the segmentation tree is based on decisions 
made by the perceptual model. In this model, the prominence of a region de­pends on its spatial frequency 
and contrast relative to its surround­ings, as given by the contrast sensitivity threshold (see Section 
2.2). The frequency of a region is estimated as f = 21 D [Reddy 2001], where D is the diameter of the 
smallest enclosing circle. In keep­ing with our understanding of our hierarchical structure as a rep­resentation 
of meaningful relations in the image, we estimate the contrast of a region by a weighted average of the 
Michelson con­trast with its sister regions, where the weights are determined by the relative lengths 
of their common borders (this reduces to an ordinary contrast measure for regions with one sister region). 
In considering color contrast [Regan 2000], we use a slight variation: .c1-c2. * (using colors in L*uv* 
). This reduces to the Michelson .c1.+.c2. contrast in monochromatic cases, and otherwise produces distances 
that steadily increase with perceptual differences in color. A simple model of attention a(ti) used in 
our previous work [Santella and DeCarlo 2002] factors in the .xation duration ti to scale back the sensitivity 
threshold. In effect, it ignores brief .xa­tions that are not indicative of substantial visual processing, 
to ac­commodate the perceptual search required to scan a detailed image. We are now ready to de.ne the 
function SPLIT(n) for the region n. This region is split into its children if at least half of its children 
could have been perceived by any .xation. That is, a child region with frequency f , contrast c, and 
eccentricity ei (for .xation fi)is perceptible when: 1 [() ] < max Amax(f , fmin)· M(ei)· a(ti)(2) ciE[1..k] 
cscale · The lower bound of fmin (defaults to 4 cycles per degree) imposed on the frequencies used by 
the contrast model takes into account that low-frequency square-wave gratings are visible at lower con­trasts 
than sine gratings [Campbell and Robson 1968] (this .at­tens out the left side of the curve in Figure 
3(a)). To enable more substantial simpli.cations, we employ a contrast scaling coef.cient cscale to reduce 
contrast sensitivity (the default value is 0.1). This is a helpful parameter in .ne-tuning the results, 
as it provides a global control for content. 4.3 Region Smoothing Frontier regions form a partition 
of the image. However, the detail level of boundaries is uniformly high, since all boundaries derive 
from the lowest segmentation. Before rendering, the frontier re­gion boundaries are smoothed, so that 
the frequencies present are consistent with the region size. Then, the regions are .lled in with their 
average color. Figure 6(c) demonstrates the effectiveness of smoothing (showing before and after). The 
network of boundaries induced by the frontier regions can be viewed as a set of curves. These curves 
join together at those points where three or more regions touch (or possibly two regions on the image 
border). Interior curves (which are not on an image border) are smoothed using a low pass .lter, where 
the endpoints of the curve are held .xed [Finkelstein and Salesin 1994] (this preserves the network connectivity). 
The assigned frequency f for this curve is the maximum of the two adjoining region frequencies; this 
leads to use of a Gaussian kernel with s = 1 (when .ltering with this 8 f kernel, components with frequency 
f mostly pass through, while those at 4 f are essentially removed). While it is possible for curves to 
cross each other using the .lter, this is unlikely, and is unnotice­able if regions are drawn in coarse-to-.ne 
order. 4.4 Drawing lines With the regions drawn, the lines are placed on top. Lines are drawn using 
a model of visual acuity; this model ignores contrast, and instead uses the maximum perceivable frequency 
of G =50 cycles per degree. Computing the frequency as f = 21 l fora line oflength l, the acuity model 
can predict it was visible if: [] f < max G · M(ei)· a(ti)(3) iE[1..k] The eccentricity ei with respect 
to fi is determined only by the clos­est point on the line to the .xation point (and does not use regions 
in N). Lines shorter than lmin are not drawn. To .lter out spurious lines shorter than 2.5lmin which 
can appear in textured areas, we additionally require them to lie along a frontier region boundary. Lines 
are smoothed in the same manner as the region boundary curves, but instead use a .xed-size .lter (s =3). 
This preserves potentially important detail in lines (recall that they were extracted out using a .xed-size 
kernel). This also affects the placement of long lines so that they are not perfectly aligned with regions. 
The line thickness t depends on the length l, and is de.ned as the af.ne function which maps a range 
of line lengths [lmin,lmax]to a range of line thicknesses [tmin,tmax](above lmax, thicknesses are capped 
at tmax). Lines are drawn in black, and are linearly tapered at each end over the .rst and last third 
of their length (unless the end touches an image boundary). We choose [tmin,tmax]=[3,10]and [lmin,lmax]= 
[15,500]as default values.  5 Results An interaction with our system proceeds as follows. An image is 
selected for transformation, and is displayed on the screen in the presence of an eye-tracker. The user 
is instructed to Look at the image. The image is then displayed for .ve seconds. In the ex­amples that 
follow, all parameters are set to default values unless otherwise listed. Images and eye-movement data 
are available at http://www.cs.rutgers.edu/~decarlo/abstract.html . We present three examples in Figures 
5, 6 and 7. For each exam­ple, building the pyramid and hierarchy took about 3 minutes, and rendering 
took 5 to 10 seconds. The source images are displayed in (a), with the .xations that were collected by 
the eye tracker marked on the lower image. In each case, the line drawing that results is dis­played 
in (b); each of these clearly exhibits meaningful abstraction. The additional renderings in Figure 7(c) 
illustrate the line drawing style without the use of .xation data. Instead, these drawings use a constant 
eccentricity in deciding whether or not to include individ­ual regions. On the top is a drawing that 
maintains .ne detail across the entire image, while on the bottom only coarse structures are pre­served; 
neither clearly contains an obvious subject. This demon­strates our interactive technique: tracking eye 
movements enables meaningful abstraction. However, the images in Figure 7(b) and (c) are still clearly 
produced using the same style.  6 Discussion In this paper we have presented a new alternative for non­photorealistic 
rendering, encompassing: a new visual style using bold edges and large regions of constant color; a new 
approach to visual form for rendering transformations on images, a hierarchi­cal structure that relates 
the meaningful parts in an image across scales; and new techniques for interaction, based on eye-tracking 
and models of perception. Future research can bring improvements to any of these areas. For example, 
the segmenter could be enhanced to use a model of shading. This would reduce the patchiness seen in smoothly 
shaded regions (skin, in particular). More dif.cult, however, is the ap­propriate placement of boundaries 
to indicate shading changes or gradations. The treatment of texture offers a stylistic challenge. Currently, 
simple textures are simply smoothed away (such as the stucco wall in the opening .gure). Complex textures 
are problem­atic (especially when foreshortened), such as the pattern of win­dows in Figure 8. In this 
case, the segmenter lumps together all of the small windows into a single region. While our current seg­menter 
does not model texture, other computer vision research has looked at grouping regions into textured objects. 
But how can a system effectively convey an inferred texture in an abstracted way? The segmentation could 
be enriched with additional aspects of visual form as well. Natural possibilities include the grouping 
of parts into coherent objects, or the status of contours as results of occlusion, shadow or markings. 
For animation, algorithms for the visual tracking of image features and the segmentation of moving objects 
will be required to achieve consistency of elements over  (a) (b) Figure 5: (a) A source image (1024 
× 688) and .xations gathered by the eye-tracker; (b) the resulting line drawing (cscale = 0.1, lmin = 
40).  (a) (b) (c) Figure 6: (a) A source image (1024 × 688) and .xations gathered by the eye-tracker; 
(b) the resulting line drawing (cscale = 0.05, lmin = 40);  (c) region boundaries before and after smoothing. 
   (a) (b) (c) Figure 7: Comparison with and without eye-tracking data for the 768 ×768 image in (a). 
The drawing in (b) uses .xation data, and important details (as seen by the user) are retained (eoutside 
= 40°). The drawings in (c) instead use a constant eccentricity (3° on top, 12° on the bottom image) 
across the entire image so that no meaningful abstraction is performed. (All use cscale = 0.14, lmin 
= 15.) Figure 8: A photograph with a dif.cult texture, and its correspond­ing line drawing (cscale = 
0.175, lmin = 15). time. But how can this be related to the patterns of .xations gath­ered across a series 
of images (whether they are viewed frame-by­frame or at full speed)? Finally, more sophisticated models 
of visual perception can sup­port more accurate decisions of what simpli.cations are possible, and suggest 
more discriminating transformations on regions. In­deed, by providing a controlled means for adapting 
imagery based on a perceptual model, our system may itself serve as a tool for formulating and testing 
such perceptual models. Acknowledgments Thanks to Eileen Kowler, Manish Singh, Peter Meer, Jan Koenderink, 
John Henderson, Chris Christoudias, Va.sek Chv´atal, Cassidy Curtis, and Matthew Stone. Photos in Figures 
5 8 courtesy http://philip.greenspun.com. Partially supported by NSF Instrumentation 9818322.  References 
AGRAWALA,M., AND STOLTE, C. 2001. Rendering effective route maps: improving usability through generalization. 
In Proc. of ACM SIGGRAPH 2001, 241 249. AHUJA, N. 1996. A transform for multiscale image segmentation 
by integrated edge and region detection. IEEE Trans. on Pattern Analysis and Machine Intelligence 18, 
12, 1211 1235. BURT,P., AND ADELSON, E. 1983.TheLaplacianpyramidasacompactimagecode. IEEE Trans. on Communications 
31, 4, 532 540. CAMPBELL,F., AND ROBSON, J. 1968. Application of Fourier analysis to the visi­bility 
of gratings. Journal of Physiology 197, 551 566. CHRISTOUDIAS,C., GEORGESCU,B., AND MEER, P. 2002. Synergism 
in low level vision. In Proc. ICPR 2002. COMANICIU,D., AND MEER, P. 2002. Mean shift: A robust approach 
toward feature space analysis. IEEE Trans. on Pattern Analysis and Machine Intelligence 24,5. CURTIS, 
C. 1999. Non-photorealistic animation. In ACM SIGGRAPH 1999 Course Notes #17 (Section 9). DEUSSEN,O., 
AND STROTHOTTE, T. 2000. Computer-generated pen-and-ink illus­tration of trees. In Proc. of ACM SIGGRAPH 
2000, 13 18. DUCHOWSKI,A., AND VERTEGAAL, R. 2000. Eye-based interaction in graphical systems: Theory 
and practice. In ACM SIGGRAPH 2000 Course Notes #5. DUCHOWSKI, A. 2000. Acuity-matching resolution degradation 
through wavelet coef.cient scaling. IEEE Trans. on Image Processing 9, 8 (Aug.), 1437 1440. DURAND,F., 
OSTROMOUKHOV,V., MILLER,M., DURANLEAU,F., AND DORSEY, J. 2001. Decoupling strokes and high-level attributes 
for interactive traditional drawing. In Proceedings of the 12th Eurographics Workshop on Rendering, 71 
82. FINKELSTEIN,A., AND SALESIN, D. 1994. Multiresolution curves. In Proc. of ACM SIGGRAPH 94, 261 268. 
FOLEY,J., VAN DAM,A., FEINER,S., AND HUGHES, J. 1997. Computer Graphics: Principles and Practice, 2nd 
edition. Addison Wesley. GOOCH,B., AND GOOCH, A. 2001. Non-Photorealistic Rendering. A K Peters. GOOCH,A. 
A., GOOCH,B., SHIRLEY,P., AND COHEN, E. 1998. A non­photorealistic lighting model for automatic technical 
illustration. In Proc. of ACM SIGGRAPH 98, 447 452. HAEBERLI, P. 1990. Paint by numbers: Abstract image 
representations. In Proc. of ACM SIGGRAPH 90, 207 214. HANDFORD, M. 1987. Where s Waldo? Little, Brown 
and Company. HENDERSON,J. M., AND HOLLINGWORTH, A. 1998. Eye movements during scene viewing: An overview. 
In Eye Guidance in Reading and Scene Perception,G. Un­derwood, Ed. Elsevier Science Ltd., 269 293. HERMAN,I., 
AND DUKE, D. 2001. Minimal graphics. IEEE Computer Graphics and Applications 21, 6, 18 21. HERTZMANN,A., 
AND ZORIN, D. 2000. Illustrating smooth surfaces. In Proc. of ACM SIGGRAPH 2000, 517 526. HERTZMANN, 
A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In Proc. of ACM SIGGRAPH 98, 
453 460. HERTZMANN, A. 2001. Paint by relaxation. In Computer Graphics International, 47 54. JUST,M. 
A., AND CARPENTER, P. A. 1976. Eye .xations and cognitive processes. Cognitive Psychology 8, 441 480. 
KELLY, D. 1984. Retinal inhomogenity: I. spatiotemporal contrast sensitivity. Journal of the Optical 
Society of America A 74, 1, 107 113. KOENDERINK,J. J., M.A. BOUMAN,A. B. D.M., AND SLAPPENDEL, S. 1978. 
Perimetry of contrast detection thresholds of moving spatial sine wave patterns. II. the far peripheral 
visual .eld (eccentricity 0-50). Journal of the Optical Society of America A 68, 6, 850 854. KOENDERINK, 
J. J. 1984. The structure of images. Biological Cybernetics 50, 363 370. KOENDERINK, J. J. 1984. What 
does the occluding contour tell us about solid shape? Perception 13, 321 330. KOWALSKI,M. A., MARKOSIAN, 
L., NORTHRUP,J. D., BOURDEV, L., BARZEL, R., HOLDEN,L. S., AND HUGHES, J. 1999. Art-based rendering of 
fur, grass, and trees. In Proc. of ACM SIGGRAPH 99, 433 438. LEYTON, M. 1992. Symmetry, causality, mind. 
MIT Press. LINDEBERG, T. 1994. Scale-Space Theory in Computer Vision. Kluwer Academic Publishers. LITWINOWICZ, 
P. 1997. Processing images and video for an impressionist effect. In Proc. of ACM SIGGRAPH 97, 407 414. 
MACKWORTH,N., AND MORANDI, A. 1967. The gaze selects informative details within pictures. Perception 
and Psychophysics 2, 547 552. MANNOS, J. L., AND SAKRISON, D. J. 1974. The effects of a visual .delity 
criterion on the encoding of images. IEEE Trans. on Information Theory 20, 4, 525 536. MARKOSIAN, L., 
KOWALSKI,M. A., TRYCHIN,S. J., BOURDEV,L. D., GOLD- STEIN,D., AND HUGHES, J. F. 1997. Real-time nonphotorealistic 
rendering. In Proc. of ACM SIGGRAPH 97, 415 420. MARR, D. 1982. Vision: A Computational Investigation 
into the Human Representa­tion and Processing of Visual Information. W.H. Freeman, San Francisco. MEER,P., 
AND GEORGESCU, B. 2001. Edge detection with embedded con.dence. IEEE Trans. on Pattern Analysis and Machine 
Intelligence 23, 12, 1351 1365. PATTANAIK,S. N., FERWERDA,J. A., FAIRCHILD,M. D., AND GREENBERG,D. P. 
1998. A multiscale model of adaptation and spatial vision for realistic image dis­play. In Proc. of ACM 
SIGGRAPH 98, 287 298. REDDY, M. 2001. Perceptually optimized 3D graphics. IEEE Computer Graphics and 
Applications 21, 5 (September/October), 68 75. REGAN, D. 2000. Human Perception of Objects: Early Visual 
Processing of Spa­tial Form De.ned by Luminance, Color, Texture, Motion and Binocular Disparity. Sinauer. 
ROVAMO,J., AND VIRSU, V. 1979. An estimation and application of the human cortical magni.cation factor. 
Experimental Brain Research 37, 495 510. SAITO,T., AND TAKAHASHI, T. 1990. Comprehensible rendering of 
3-D shapes. In Proc. of ACM SIGGRAPH 90, 197 206. SANTELLA,A., AND DECARLO, D. 2002. Abstracted painterly 
renderings using eye­tracking data. In Proc. of the Second International Symp. on Non-photorealistic 
Animation and Rendering (NPAR). SHIRAISHI,M., AND YAMAGUCHI, Y. 2000. An algorithm for automatic painterly 
rendering based on local source image approximation. In Proc. of the First Inter­national Symp. on Non-photorealistic 
Animation and Rendering (NPAR), 53 58. SIBERT, L. E., AND JACOB, R. J. K. 2000. Evaluation of eye gaze 
interaction. In Proc. CHI 2000, 281 288. TRUCCO, E., AND VERRI, A. 1998. Introductory Techniques for 
3-D Computer Vision. Prentice-Hall. TUFTE, E. R. 1990. Envisioning Information. Graphics Press. VERTEGAAL, 
R. 1999. The gaze groupware system: Mediating joint attention in mutiparty communication and collaboration. 
In Proc. CHI 99, 294 301. WINKENBACH,G., AND SALESIN, D. H. 1994. Computer-generated pen-and-ink illustration. 
In Proc. of ACM SIGGRAPH 94, 91 100. YARBUS, A. L. 1967. Eye Movements and Vision. Plenum Press. ZEKI, 
S. 1999. Inner Vision: An Exploration of Art and the Brain. Oxford Univ. Press. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566651</article_id>
		<sort_key>777</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Object-based image editing]]></title>
		<page_from>777</page_from>
		<page_to>784</page_to>
		<doi_number>10.1145/566570.566651</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566651</url>
		<abstract>
			<par><![CDATA[We introduce Object-Based Image Editing (OBIE) for real-time animation and manipulation of static digital photographs. Individual image objects (such as an arm or nose, Figure 1) are selected, scaled, stretched, bent, warped or even deleted (with automatic <i>hole filling</i>) - <i>at the object, rather than the pixel level</i> - using simple gesture motions with a mouse. OBIE gives the user direct, local control over object shape, size, and placement while dramatically reducing the time required to perform image editing tasks.Object selection is performed by manually collecting (subobject) regions detected by a watershed algorithm. Objects are tessellated into a triangular mesh, allowing shape modification to be performed in real time using OpenGL's texture mapping hardware.Through the use of <i>anchor points,</i> the user is able to interactively perform editing operations on a whole object, or just part(s) of an object - including moving, scaling, rotating, stretching, bending, and deleting. <i>Indirect</i> manipulation of object shape is also provided through the use of sliders and Bezier curves. Holes created by movement are filled in real-time based on surrounding texture.When objects stretch or scale, we provide a method for preserving <i>texture granularity</i> or scale. We also present a <i>texture brush,</i> which allows the user to "paint" texture into different parts of an image, using existing image texture(s).OBIE allows the user to perform interactive, high-level editing of image objects in a few seconds to a few ten's of seconds]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[image editing]]></kw>
			<kw><![CDATA[image warping]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39038570</person_id>
				<author_profile_id><![CDATA[81100343021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Barrett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P382417</person_id>
				<author_profile_id><![CDATA[81100057666]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Cheney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADOBE SYSTEMS INCORPORATED 2000. Adobe Photoshop Version 6.0 User Guide.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BEIER, T. AND NEELY, S. 1992. Feature Based Image Metamorphosis. In Computer Graphics (Proceedings of ACM SIGGRAPH 92), 26(2), ACM, 35-42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BERN, M. AND EPPSTEIN, D. 1992. Polynomial-size nonobtuse triangulation of polygons. International Journal of Computational Geometry and Applications 2(3), 241-255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>66134</ref_obj_id>
				<ref_obj_pid>66131</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOOKSTEIN, F. L. 1989. Principal Warps: thin-plate splines and the decomposition of deformations. In IEEE Transactions on PAMI, 11(6), 567-585.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A. AND LEUNG, T. K. 1999. Texture Synthesis by Non-parametric Sampling. In IEEE International Conference on Computer Vision (ICCV 99), Corfu, Greece.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A. AND FREEMAN, W. 2001. Image Quilting for Texture Synthesis and Transfer. In Proceedings of ACM SIGGRAPH 2001, ACM Press/ACM SIGGRAPH, New York. E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 341-346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794800</ref_obj_id>
				<ref_obj_pid>794191</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ELDER, J. H. AND GOLDBERG, R. M. 1998. Image Editing in the Contour Domain. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 98), 374-281.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[GAO, P. AND SEDERBERG, T. W. 1998. A work minimization approach to image morphing, The Visual Computer 14, 390-400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[THE GIMP. http://www.gimp.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HARRISON, P. 2001. A Non-hierarchical Procedure for Re-synthesis of Complex Textures. In Proceedings of Winter School of Computer Graphics 2001, 190-197.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HASBRO, INC. 2001. Mrs. Potato Head. http://www.hasbropreschool.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[MCINERNEY, T. AND TERZOPOULOS, D. 2000. T-Snakes: Topology Adaptive Snakes. In Medical Image Analysis, Vol. 4, 73-91.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E. N., AND BARRETT, W. A. 2001. A Confidence Measure for Boundary Detection and Object Selection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2001), Vol. 1, 477-484.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218442</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E. N. AND BARRETT, W. A. 1995. Intelligent Scissors for Image Composition. In Proceedings of ACM SIGGRAPH 1995, ACM Press/ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 191-198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>305064</ref_obj_id>
				<ref_obj_pid>305061</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E. N. AND BARRETT, W. A. 1998. Interactive Segmentation with Intelligent Scissors. In Graphical/Models and Image Processing, 60(5), 349-384.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E. N. AND BARRETT, W. A. 1999. Toboggan-Based Intelligent Scissors with a Four Parameter Edge Model. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 99), 452-458.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MORTENSEN, E. N., REESE, L. J., AND BARRETT, W. A. 2000. Intelligent Selection Tools. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 99), Vol. II, 776-777.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>73408</ref_obj_id>
				<ref_obj_pid>73393</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MOUNT, D. M., AND SAALFELD, A. 1988. Globally-equiangular Triangulations of Co-circular Point in O(n log n) Time. In Proceedings of the 4th Symposium on Comp. Geometry, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344987</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[PRAUN, E., ET AL. 2000. Lapped Textures. In Proceedings of ACM SIGGRAPH 2000, ACM Press/ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 465-470.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[REESE, L. J. 1999. Intelligent Paint: Region-Based Interactive Image Segmentation. Masters Thesis, Department of Computer Science, Brigham Young University, Provo, UT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344935</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SANDER, P. V., ET AL. 2000. Silhouette Clipping. In Proceedings of ACM SIGGRAPH 2000, ACM Press/ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 327-334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SCANSOFT, INC. 2001. Kai's SuperGoo. http://www.scansoft.com/products/goo.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>673287</ref_obj_id>
				<ref_obj_pid>645908</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SHEWCHUK, J. R. 1996. Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. First Workshop on Applied Computational Geometry, ACM, 124-133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SIBSON, R. 1978. Locally equiangular triangulations. Computer Journal, 21:243-245.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SMITH, M. D. 2002. Sally's Flower. Graphics Lab, Brigham Young University. mike_d_smith@byu.edu. Received by personal communication.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>116707</ref_obj_id>
				<ref_obj_pid>116700</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[VINCENT, L. AND SOILLE, P. 1991. Watersheds in Digital Spaces: An Efficient Algorithm Based on Immersion Simulations. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(6):583-598.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[WEI, L. AND LEVOY, M. 2000. Fast Texture Synthesis using Tree-structured Vector Quantization. In Proceedings of ACM SIGGRAPH 2000, ACM Press/ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 479-488.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[WOLBERG, G. 1998. Image Morphing: a Survey, The Visual Computer 14, 360-372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[XU, Y., ET AL. 2000. Chaos Mosaic: Fast and Memory Efficient Texture Synthesis. Technical Report MSR-TR-2000-32, Microsoft Research.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618825</ref_obj_id>
				<ref_obj_pid>616070</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[YU, XIAOHUA, MORSE, B. S., AND SEDERBERG, T. W. 2001. Image Reconstruction Using Data-Dependent Triangulation. IEEE Computer Graphics and Applications Vol. 21, No. 3, 62-68.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Object-Based Image Editing Figure 1 Animation of a static, digital photo: Mrs. Potato [Hasbro 2001] 
Takes a Bow. (a) Digital photo of a toy on a carpet background. Frames c-e are edited versions of this 
digital photo -not computer graphic models. (b) Image objects (arms, eyes, nose, mouth, ear, tongue, 
body) are selectedin about 27 seconds (~3 sec./object) (c) Object editing: arms are stretched and bent 
in real time using gesture motions with a mouse; ears, eyes,nose mouth and tongue (top of head) are shifted 
down, scaled and warped to simulate foreshortening. Background texture is filled in automatically. (d) 
Object editing continues. (e) Final pose: Foreshortened body occludes shoes; arm occludes body; ears 
occlude arms; texture painting was applied to fix imperfect background filling above eyes. All editing 
operations were performed interactively in a total of about 2 minutes. Abstract We introduce Object-Based 
Image Editing (OBIE) for real-time animation and manipulation of static digital photographs. Individual 
image objects (such as an arm or nose, Figure 1) are selected, scaled, stretched, bent, warped or even 
deleted (with automatic hole filling) -at the object, rather than the pixel level - using simple gesture 
motions with a mouse. OBIE gives the user direct, local control over object shape, size, and placement 
while dramatically reducing the time required to perform image editing tasks. Object selection is performed 
by manually collecting (subobject) regions detected by a watershed algorithm. Objects are tessellated 
into a triangular mesh, allowing shape modification to be performed in real time using OpenGL s texture 
mapping hardware. Through the use of anchor points, the user is able to interactively perform editing 
operations on a whole object, or just part(s) of an object - including moving, scaling, rotating, stretching, 
bending, and deleting. Indirect manipulation of object shape is also provided through the use of sliders 
and Bezier curves. Holes created by move­ment are filled in real-time based on surrounding texture. When 
objects stretch or scale, we provide a method for preserving texture granularity or scale. We also present 
a texture brush, which allows the user to paint texture into different parts of an image, using existing 
image texture(s). OBIE allows the user to perform interactive, high-level editing of image objects in 
a few seconds to a few ten s of seconds Keywords: Image Editing, Image-based rendering, Animation, Texture 
Synthesis, Image Warping 1 INTRODUCTION Object-based editing operations have traditionally been limited 
to well defined graphical objects (circles, rectangles, etc.) created in a drawing or modeling application. 
In contrast, image editing pro­grams, such as Photoshop provide a rich assortment of pixel-based Copyright 
&#38;#169; 2002 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting 
with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM 
Inc., fax +1-212-869-0481 or e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 
editing tools (cloning, pixel brushing, etc.) but limit object-based edit­ing operations, such as scaling, 
warping, rotation or recoloring, to glo­bal manipulation of a bounding box over groups of selected pixels. 
Object-based image editing (OBIE), presented in this paper, allows the user to animate static objects 
in digital photographs with direct, local object (and subobject) control. Tools are created for object 
selection, triangulation, stretching and bending, with texture preservation and background filling, producing 
new gestures and poses with foreshortening and self-occlusion - all interactively. This greatly increases 
the freedom with which image objects can be edited and animated, while dramatically reducing the time 
needed to per­form such editing operations. OBIE tools make a fundamental con­tribution to the problem 
of image editing by changing the granularity of editing operations from the pixel to the object (or subobject) 
level. These tools operate in a way that is more natural to the object s topography, rather than requiring 
that editing operations be limited to a standard rectangular grid. OBIE can also be used to animate objects 
in static digital frames, as shown in Figures 1 and 16. There are three image-editing methods currently 
in use, each of which have their own drawbacks. Pixel-based methods (clone tools, pixel painting, and 
nudge warping [Adobe 2000; GIMP; Scansoft 2001]) push pixels around to produce surprisingly good results, 
but are very time-intensive and do not allow direct, object-level manipula­tion. Region-of-interest (ROI) 
methods such as rectangle-based tools [Adobe 2000; GIMP] limit pixel modification to global manipulation 
of an axis-aligned bounding box and do not update the pixel colors in the region until the mouse movement 
stops. One ROI method [Elder and Goldberg 1998] does allow some object-level control, but requires the 
user to perform lengthy contour grouping operations and to work with disconnected contours vs. well-defined 
regions. Image­based editing methods (such as warping with thin-plate splines and with radial basis functions 
[Beier and Neely 1992; Bookstein 1989; ScanSoft 2001]) affect the entire image, like a rubber sheet, 
but do not allow for efficient, local control of an object s shape independent of surrounding background. 
Recent work in segmentation and semi-automated object selection [McInerney and Terzopoulos 2000; Mortensen 
and Barrett 2001; Mortensen and Barrett 1995; Mortensen and Barrett 1998; Mortensen and Barrett 1999; 
Mortensen et al. 2000; Reese 1999] as well as texture synthesis [Efros and Leung 1999; Efros and Freeman 
2001; Harrison 2001; Praun et al. 2000; Wei and Levoy 2000; Xu et al. 2000] has accelerated the interest 
in object-based image editing. In this paper we integrate into a single, interactive framework, a suite 
of tools for object selection and editing (with automatic hole-filling), that provides real-time feedback 
while preserving the scale and varia­tion native to object and background textures. Objects can be manipulated 
directly with a mouse or indirectly using curve deformers to attenuate length, thickness, and rotational 
bend. We also introduce texture painting for advanced cloning operations and creation of a variety of 
painterly effects. 2OBJECT SELECTION Before performing editing operations, the object is selected and 
broken down into a triangular network that captures the subobject detail. Object selection is based on 
tobogganing, a watershed algorithm [Vincent and Soille 1991] used in recently published interactive seg­mentation 
techniques [Mortensen and Barrett 1999; Mortensen et al. 2000; Reese 1999]. Watershed tobogganing amounts 
to sliding downward in the gradient magnitude image, to points of lowest gra­dient magnitude (base points). 
Pixels which slide to the same base point define catchment basins (~5-15 pixels). An algorithm for auto­mated 
grouping of catchment basins, based on statistical similarity, was first developed for Intelligent Paint 
object selection [Reese 1999], and subsequently applied to Intelligent Scissors [Mortensen and Bar­rett 
1999]. Grouped catchment basins consist of a few ten s to hun­dred s of pixels, referred to in this paper 
as TRAPs (Tobogganed Regions of Accumulated Plateaus). While TRAPs still produce an oversegmented image 
(Figure 2a), they adhere well to object and sub­object edges and correspond nicely to the level of subobject 
detail needed without grouping object and background TRAPs. We begin by computing TRAPs for the entire 
image (Figure 2a). All image pixels are associated with their corresponding TRAPs and labelled accordingly 
for later tagging and object definition. Note that in the zoomed portion of the image (Figure 2b), although 
the object is oversegmented, the TRAPs do a reasonable job of automatically cap­turing the subobject 
detail (fingers, hand, wrist). Object selection is performed by manually collecting the TRAPs belonging 
to the object: when the user clicks the mouse on the image, the TRAP region containing that pixel is 
selected, adding it to the selected object and storing it in the selection list. Figure 2c shows the 
hand TRAPs comprising the object selected in blue. Multiple TRAPs can be selected together by dragging 
a selection box over the object of interest. Deselection can be similarly accom­plished by clicking on 
a TRAP already selected, which toggles it off, and removes it from the selection list. In the case that 
an object will move or shrink, creating a hole, something needs to be done to fill those exposed pixels. 
As will be explained later, surrounding (or background ) TRAPs are used in filling in these holes. When 
an object is selected, participating background TRAPs can be automati­cally selected, using a breadth-first 
connected component algorithm ­dealing with TRAPs instead of pixels as the base components - con­ a 
tinuing outward for as many layers out as requested. The new layers of TRAPs form a background object. 
3 OBJECT REPRESENTATION The object boundary is detected by applying a contour following algorithm to 
the union of the selected TRAPs, treated as a single binary object. In order to manipulate the object 
in real-time with OpenGL we create a Delaunay triangular network from the object boundary and its associated 
TRAPs. Triangulation of the object requires triangular vertices (nodes) along the object boundary as 
well as nodes internal to the object. Boundary nodes are determined by polygonalizing the boundary with 
a simple recursive divide &#38; conquer algorithm. The vertices of the resulting polygon define the boundary 
nodes. The number of boundary nodes is determined by specifying the tolerance level of the polygonal 
line fit. Internal nodes (acting as Steiner points [Bern and Eppstein 1992] for the triangulation) are 
placed at base points (valleys) and where three or more TRAP boundaries (ridges) meet to form junctions 
(Fig­ure 3) in the gradient magnitude image. A Delaunay triangulation is then performed on the object, 
using the specified internal nodes and an algorithm from [Shewchuk 1996], resulting in the creation of 
more optimally-shaped triangles. If a true Delaunay triangulation is not possible, because there are 
not enough input nodes given, then a close approximation to it is found. Alternative, but more computational 
tri­angulation schemes, such as found in [Yu et. al 2001] are also possible. Triangular representation 
of individual TRAPs occurs in the same way by treating TRAPs as objects, in which case the only internal 
node is the base point. Triangular representation of individual TRAPs is necessary for the implementation 
of texture painting (dis­cussed later) where TRAP connectivity is not required. Figure 4 shows the triangulation 
of individual hand TRAPs from Figure 2c overlaid in red. Triangles are used frequently to represent the 
topography of an object, especially in 3D models, and are also used effectively in OBIE. By texture mapping 
these triangles with corresponding Figure 3: TRAP Boundary Junction. Top left TRAP boundary pixels (green), 
right TRAP boundary (blue), bottom TRAP boundary (purple). White dot represents three -TRAP junction, 
where a ridge node is placed for object triangulation. Figure 4: Triangulated TRAPs. Triangles are shown 
in red, with TRAPboundaries from Figure 2b shown in gray. regions of the image, we take advantage of 
OpenGL s hardware acceleration to perform editing operations at interactive rates. 4 OBJECT-BASED EDITING 
OPERATIONS After an object has been selected and triangulated, its shape is ready to be edited. Editing 
operations include traditional linear affine transformations (scale, rotate, translate) as well as new 
non-linear transformations for localized stretching, warping and rotational bend. Objects can be edited 
directly using gesture motions with the mouse, or indirectly by attaching object geometry to curve deformers. 
Included also in our suite of editing tools is object delete with auto­matic filling of the space previously 
occupied by the object. 4.1 Implementation Efficient implementation of object-based editing operations 
requires the support of four basic system components: (1) Conver­sion to, and transformations within 
a local coordinate system (2) Object and background layering (3) Antialiasing within OpenGL (4) Pivot 
point placement for localized warping.  For efficiency and simplicity in carrying out otherwise complex 
object manipulation and editing operations, object vertices are first transformed into a local coordinate 
system. Every subsequent mouse movement calls the current tool s warping function, which warps the object 
s temporary vertices in the local coordinate space. Local coordinate transformation coupled with OpenGL 
rendering of the edited object provides visual update rates of 10-20 frames per second, which is sufficient 
for interactive object editing. Second, the selected object is stored on a separate layer from the rest 
of the image, facilitating overlap with other scene components resulting from the object being pulled 
and stretched separately from the background. Third, because the object is stored on its own layer, edges 
some­times appear jaggy because of aliasing. Using OpenGL, we provide a simple alpha blending fix for 
the edges of the object. The object polygon is drawn multiple times with decreasing thicknesses into 
the background (destination) alpha, summing up the alpha into a ridge with its peak along the actual 
boundary. When the object is blended onto the background, edges take on a feathered look, providing efficient 
and reasonable antialiasing (Figure 5). This provides greater degrees of freedom, as compared to [Sander 
et. al 2000], in the total width of the transition interval (2.5/2 pixels, for example) as well as in 
the functional form of the transition, as specified by each step incre­ment and its associated alpha 
value (e.g. .15, .35, ...). Fourth, the object s pivot point (initially at its center of gravity) is 
adjustable, and can be moved to any location within the object to allow localized warping by stretching 
the object with respect to that pivot point.  4.2 Linear Transformations Object translation, rotation 
and scaling are performed with respect to the pivot point in the traditional way, but at interactive 
rates pro­viding continuous visual feedback. This saves time and eliminates guesswork inherent in other 
iterative, trial-and-error approaches because the feedback is immediate, and the user can verify the 
cor­rectness of the desired result while the object is being edited. 4.3 Background Filling Behind Objects 
When objects are deleted, moved, or otherwise modified in such a way as to leave a hole in the image, 
the hole needs to be filled some­how. Many methods have been proposed to replicate texture to fill holes, 
expand borders of an image, and create tileable texture. Efros and Leung presented a fairly successful 
algorithm [Efros and Leung 1999] that has been built upon in subsequent research [Harrison 2001; Wei 
and Levoy 2000], accelerating the process from hours down to many seconds. While giving some impressive 
results for many types of texture, none of these techniques run fast enough to be interactive with normal 
image sizes. More recently, many algorithms have surfaced that use regions of texture, rather than computing 
each pixel separately. There are vari­ous methods used for connecting these regions of texture, from 
alpha channel blending [Xu et al. 2000], to growing the regions together [Efros and Freeman 2001; Praun 
et al. 2000]. These algo­rithms are faster, but still require excessive computation and/or a region specifying 
phase. Whenever an object s shape is modified, there is a chance that the background behind it will be 
partly or completely exposed. Since our hole-filling processes run at interactive rates, we simply fill 
in the entire space behind an object whenever it is modified. We have imple­mented two of many possible 
algorithms for filling the hole: scale­down filling and random-grid filling. Both methods use texture 
TRAPs in much the same way as the texture-preserving operations below. Scale-down filling uses concentric, 
overlapping displacement of TRAPs to fill toward the center of the object. The scaling is done in steps, 
the number of which are based on the average background TRAP size. At each step in the scale, copies 
of all of the background TRAPs are laid down. Figure 6, parts 2a and 3a illustrate this tech­nique. Random-grid 
filling creates a grid the size of the object s bounding box, with grid points spaced relative to the 
average background TRAP size. A random background TRAP is placed at each intersec­tion in the grid, filling 
the entire grid with overlapping TRAPs. This technique is illustrated in Figure 6, parts 2b and 3b. Since 
both of these algorithms allow TRAPs to land outside of the object hole, they are again masked off by 
the shape of the original object. 4.4 Texture-Preserving Operations When an object is stretched, if 
its texture is high in detail, the tex­ture becomes overly smoothed, looking unnatural and incorrect. 
To fix this, we disconnect the object TRAPs, keeping their sizes con­stant, and warp only their basepoint 
positions. Each TRAP is triangulated separately as described in Section 3. Once the TRAP primitives are 
disconnected, as are the white TRAPs in Figure 7 part 2, rather than warping all of their vertices, we 
leave them rigid at the same scale, and warp only the positions of their basepoints. Since spaces will 
be created between texture TRAPs, we lay down TRAPs continuously with each frame update as we stretch 
the object, as shown in Figure 7, parts 2 &#38; 3. TRAPs continue to build up in lay­ers as the mouse 
is moved so that the user can continue to drag the mouse back and forth until the object is sufficiently 
dense with tex­ture TRAPs. The new TRAPs can be slightly jittered around (moved small random amounts 
in x and y) to increase randomness in the tex­ture, and can be laid down more sparsely if the density 
is too thick. As the region is stretched, the boundary continues to mask off the rest of the stray TRAPs, 
using a mask shown in Figure 7, part 4. Black areas in the mask represent pixels that will not be drawn 
to the screen, and correspond to the pixels outside of the object boundary. Jitter amount (extent of 
the random movements of each TRAP in terms of its bounding box size) and density (percentage of the origi­nal 
TRAPs that will be laid down at each frame), as well as opacity (TRAP transparency), can easily be adjusted 
with sliders in the user interface.  4.5 Non-linear Transformations The non-linear stretching and bending 
tools make use of the local coordinate system transformations discussed earlier and the pivot point placement. 
The pivot point also acts as a thumbtack, to stop all movement in the negative xregions of the object 
s local coordinate system. Figure 8 represents the thumbtack/pivot point as a cyan square near the shoulder 
of the arm. When using the stretch tool, the user grabs any point on the object and uses it as a handle 
to stretch the object with respect to the pivot point. In this way, mouse movement affects both the length 
and width of the object. Movements in the object s local xdirection stretch the object along that xaxis 
for object parts between the han­dle and the pivot point, and movements in its local ydirection increase 
and decrease the thickness of the object. For any given ver­tex in the object, its new position(x',y')is 
computed using Figure 6: Two Different Hole Filling Techniques. The object hole,showing the immediately 
surrounding background TRAPs (1) is filledin one of two ways: (2a-3a) Selected background TRAPs are scaled 
initeratively, and pasted down at each step. When they reach the center,the hole is filled (3a). (2b-3b) 
In the second hole-filling approach,selected background TRAPs are randomly placed in a grid-like fash­ion. 
The grid spacing is based on the average TRAP size in both X and Y. Pasting TRAPs at each grid location 
fills the hole (3b). In both approaches, the surrounding background TRAPs mask off the randomtexture 
TRAPs to the hole s boundary. . x. .. x'= x1+ Ai[]--.--- , y'= y1+ Ai[].-----y-(1) . bx .. by . where 
A[i] is a general attenuation multiplier, b and b are the posi­tive xand ydimensions of the object s 
bounding box, and .xand .y are the xand ychanges in cursor position. Figure 11 (inset curves) and Figure 
13 (bottom) show examples of A[i]. An option is also available which allows the object to preserve area 
as it stretches. As the length increases, the width decreases inversely, and vice versa. The bend tool 
essentially rotates the object, but with a (possibly) non-linear attenuation A[i] towards the thumbtack/pivot 
point, stop­ping all movement in the negative xregions of the object s local coor­dinate system. For 
any given vertex in the object, its new position is computed using xy cos(.Ai[])-sin(.Ai[]) x (2) (x',y')= 
· sin(.Ai[]) cos(.Ai[]) y where .A[i]is the attenuated rotation angle or the rotational bend. An example 
of rotational bending is shown with Mrs. Potato s arm in Figure 8b. The green outlines show where the 
arm used to be, and the red line indicates the excursion caused by .A[i]. Since it can be more intuitive 
for a user to stretch an object as it is bending, rather than performing the tasks separately, we have 
com­bined the two into a single tool. When the cursor-pivot vector is determined for the angle of rotation 
(Figure 9), the ratio of its length to the length of the original (red) vector is used to compute the 
stretch factor. Then the stretched position is passed to the rotation computation, which proceeds in 
the same way as the normal rotation above. For any given vertex in the object, its new position is com­puted 
using:  Figure 7: Texture Preservation Diagram. (1) Original object, with TRAPs adjacent to each other, 
and cursor position as it starts the drag. (2) As the object is expanded, the original (white) TRAPs 
move with thescale, but stay the same size. They lay down a copy of themselves(orange TRAPs) in a random 
location near their current position. (3) Ateach next step of the drag copies (green TRAPs) are laid 
down near thenew positions of each original TRAP. (4) In order to maintain the objectboundary while dragging, 
a binary mask is used in the OpenGL stencilbuffer. White/black mask areas allow pixels to be drawn/not 
drawn,clipping off parts of TRAPs that stray outside the object boundary,such as those in 2 and 3. a 
b Figure 8: Object Bending. (a) Arm selected (green), anchor point (blue),object axis (red). (b) Rotational 
bend using cursor movement and A[i]. vv n -o x'= x.1+ Al[]i--------------- . , y'= y (3) .. bx cos(.A[]i) 
-sin(.A[]i) x' rr (x'',y'')= · (4) sin(.A[]i) cos(.A[]i) y' rr where vn is the cursor-pivot vector length 
(Figure 9), vo is the original vector length (red), Al[i] is the general attenuation multiplier for length, 
Ar[i] is the attenuation multiplier for rotation and .Ar[i] is the attenuated rotation angle. Compounding 
of the attenuation multi­ pliers increases significantly the degrees of freedom with which objects can 
be stretched and warped. Figure 9 diagrams the process of stretching and bending an object. This requires 
two steps computationally, but only a single user inter­action. For example, Figure 10 shows the Potato 
s arm being stretched and bent simultaneously in two different positions, each requiring only a single 
interaction. The green outlines show where the original arm was in relation to the modified arm. The 
red lines show the angle of rotational bend. 5 INDIRECT EDITING WITH CURVE DEFORMERS Section 4 introduced 
the use of A[i] s, general attenuation multipli­ers, to modulate the effect of non-linear warping operations. 
Curve deformers (Figure 11) are one possible way of specifying the A[i] s. After direct editing, additional 
modifications to the object can be made indirectly using curve deformer tools. Curve deformers are implemented 
using a Bezier curve with four control points to inter­actively modify the shape of the curve. Control 
points are con­ Figure 9: Stretching and Bending an Object. (1) Original object: Black square = anchor/pivot 
point, open circle = object handle identifiedwith cursor (black arrow), red line = object axis. (2) Dashed 
line = cur­sor drag (user interaction), gray lines = computation which amountsto a traditional stretch 
and rotation of the object through angle .with the important difference that the stretch and rotation 
are now attenu­ated by Al[i] and Ar[i] (eqs. 3-4). (3) The result is a rotational bend obtained from 
compounding the effects of Al[i] and Ar[i]. a b strained to move only in the y direction. The curve 
is then sampled into a lookup table that defines A[i]. After an object has been stretched or bent, selected 
curves corresponding to length Al[i], width Aw[i], and rotation Ar[i], can be modified, just as in any 
draw­ing application, to reposition, regesture, or otherwise manipulate the object. Figure 11b-f (top) 
show examples of common curve deform­ers with their corresponding effect or fall-off on the original 
object in Figure 11a. Notice that while the length of the object may stay the same, the texture of its 
middle section changes position according to the shape of the corresponding curve deformer. As control 
points on the various curves are moved, the object changes shape interactively. Figure 12a shows a close-up 
similar to Figure 1c, with a default stretch. The hand is somewhat large, which could be useful to show 
sharp perspective. Figure 12b shows how the thickness curve can be used to shape the already stretched 
object to a more natural shape and scale. Any curve deformers, Aj[i] and Ak[i], can be used together. 
For example, Figure 13a shows the arm bent with the default values for both length and rotation fall-off. 
The fingers appear distorted and unnaturally large. Figure 13b shows how the length deformer mini- Figure 
11: Curve Deformers for Indirect Editing. Examples of how editing the curve shape can modify the object 
shown in (a). (b-d)show various fall-off amounts for stretching, and (e,f) show two dif­ferent fall-off 
amounts for bending. ab mizes distortion at the end of the object. Because the deformers can work in 
concert, we can also adjust the rotation fall-off to tuck in, and regesture the elbow and forearm area 
slightly, creating a nicer hand shape as well.  ab c (c) Rotation fall-off deformer is also adjusted 
to tuck in elbow toimprove foreshortening effect, giving user more gesture control. 6 TEXTURE PAINTING 
Texture painting is performed by spraying TRAPs over a region. Various painterly effects are achieved 
by varying the jitter, opacity, scale and density of the TRAPs (Figure 14). TRAPs are selected (in groups) 
and then used as the paint for a texture brush. Similar to the process for texture preservation (Figure 
7), texture TRAPs are laid down as the brush moves in the image. The texture brush can be used to create 
new regions from an existing texture. It also constitutes an effective delete tool. In Figure 15 the 
texture brush accomplishes both purposes. Figure 15 illustrates the process of painting out a horse from 
a field. Texture TRAPs are selected in the upper left part of the field (shown with a red outline in 
Figure 15b), and used to paint out the top part of the horse. More TRAPs are selected in the middle left 
region (Figure 15c) and used to paint the bottom half. Finally, a region of TRAPs in the lower left corner 
is selected and used to ran­domize the seam between the two sections of painted texture, using various 
amounts of scale and jitter. Brush options are provided for better artistic usability, and are attached 
to the GUI by means of sliders. The amount of jitter, opac­ity and density can be adjusted for the brush. 
The brush scale can also be adjusted, which changes the size of each TRAP relative to its original selected 
size. A clone tool provides a similar function to that found in Photoshop, but with the distinctive difference 
that as the mouse moves to paint, the tethered selection moves, sampling new TRAPs each time with the 
powerful variations offered in Figure 14. This tool is particularly effective with the jitter and scale 
options.  b a  7RESULTS Object selection, which typically requires 3-5 seconds, allows OBIE to be 
applied successfully to a variety of images (Figures 16­24). An Athlon MP 1.2 GHz dual processor, with 
an nVidia GeForce 3 graphics card was used for all results. Only a few seconds are required for tool 
and parameter selection. An immense amount of time could be saved in the application of OBIE to 2D animation 
or claymation (Figures 1,16). OBIE extends the reach (Figure 17) of image editing beyond that available 
in pixel-based applications. Figure 20 shows two children choking a goose, with two other techniques 
for texturing out the goose shown in (b) and (c). To really get rid of the goose using OBIE, we first 
delete the head and neck, specifying water as the replacement background. Then we select and delete the 
body and feet, sampling nearby grass as background. This deletion required 10 seconds because it had 
to be done in 2 steps. An extended comparison of OBIE with other popular image-edit­ing software, Adobe 
Photoshop [Adobe 2000], Scansoft s (Kai s) SuperGoo [Scansoft 2001], and Resynthesizer [Harrison 2001], 
a GIMP plug-in, is given in Figures 19, 20, and 22. The other software often produces reasonable results, 
but at the expense of noticeable artifacts or excessive user interaction and/or processing time 8CONCLUSIONS 
Object-based image editing changes fundamentally the way image editing is performed by changing the granularity 
of editing operations from the pixel to the object level, and in a way that provides greater control 
over the object s shape and topography. Individual image objects can be selected and edited, both directly 
and indirectly, in a  a b c (b) Intermediate frame in sequence. Green outlines show the selected objects 
originalpositions. (c) Arm drooped, flower and hand bent, head and hair drooped, smile bent. Note that 
the background was filled in all cases as well. Each object required ~5 seconds toselect and 5-10 seconds 
to bend/move, while preserving frame-to-frame color coherence.Total time per keyframe was ~2 minutes 
compared with ~30 minutes to draw by hand.   ab a b a b    c d (b) Photoshop - 10 seconds, 1 sample 
(clone region); flowers look likea direct copy and are same scale as closer ones. (c) Resynthesizer requires 
map creation for texture sampling. Total time to fill texture (including map making) - 20 minutes. (d) 
OBIE - 10 seconds, 1 startinguser-specified sample region of flower TRAPs. Note slightly smallerback 
layer appears more distant, demonstrating more natural variation. few seconds to a few 10 s of seconds, 
in ways not possible or practical using pixel-based editing or polygonal warping. The visual quality 
of edited images compares favorably with images edited using Photoshop, GIMP s Resynthesizer plug-in, 
or SuperGoo. Additionally, tedious tasks such as selecting new sample points for Photoshop s clone tool, 
and painting to fill in holes, are largely or completely automated. Time taken to perform many sim­ple 
tasks is shortened dramatically, requiring a matter of seconds, compared with a matter of minutes or 
hours using Photoshop or Resynthesizer. Object manipulation and texture painting/hole-filling (for stochastic 
textures) are now interactive tasks, giving the user important visual feedback during mouse interaction. 
Object selection fails (rarely) when image gradients completely disappear at perceived object boundaries, 
because object and back­ground pixels combine into a single TRAP. While this can be overrid­den manually, 
other techniques for object segmentation and decomposition may overcome this automatically and provide 
a coarser, more appropriate level of subobject detail. One area that needs significant improvement is 
background filling. Concentric and gridded filling work reasonably well for stochastic textures but struggle 
with complex backgrounds composed of regular or well-defined structures. Filling to a medial axis and 
exploiting recent work in texture synthesis could significantly improve back­ground filling. Intelligent 
Paint or Intelligent Scissors with sub-pixel accuracy [Mortensen and Barrett 1999] could also be used 
to remove object fringe from background (see Figure 10). While a single anchor point provides significant 
flexibility in object deformation, we would like to also introduce anchor lines or curves, and allow 
the use of multiple anchor points simultaneously. We would also like to extend curve deformers to include 
other object shape properties (area, eccentricity, etc.) - all compoundable, and allow curve deformers 
to affect object shape independent of any initial stretch or scale. Depth-ordered object layers could 
also be used. If the user desires to warp an image on a global scale with smooth continuity, OBIE tools 
would not be the correct choice, because pieces will start to break apart. However, for object-level 
control, OBIE has much to offer over current pixel-based methods. References ADOBE SYSTEMS INCORPORATED 
2000. Adobe Photoshop Version 6.0 User Guide. BEIER, T. AND NEELY, S. 1992. Feature Based Image Metamorphosis. 
In Computer Graph­ics (Proceedings of ACM SIGGRAPH 92), 26(2), ACM, 35-42. BERN, M. AND EPPSTEIN, D. 
1992. Polynomial-size nonobtuse triangulation of polygons. International Journal of Computational Geometry 
and Applications 2(3), 241-255. BOOKSTEIN, F. L. 1989. Principal Warps: thin-plate splines and the decomposition 
of deformations. In IEEE Transactions on PAMI, 11(6), 567-585. EFROS, A. A. AND LEUNG, T. K. 1999. Texture 
Synthesis by Non-parametric Sampling. In IEEE International Conference on Computer Vision (ICCV 99), 
Corfu, Greece. EFROS, A. A. AND FREEMAN, W. 2001. Image Quilting for Texture Synthesis and Trans­fer. 
In Proceedings of ACM SIGGRAPH 2001, ACM Press/ACM SIGGRAPH, New York. E. Fiume, Ed., Computer Graphics 
Proceedings, Annual Conference Series, ACM, 341-346. ELDER, J. H. AND GOLDBERG, R. M. 1998. Image Editing 
in the Contour Domain. In Pro­ceedings of IEEE Conference on Computer Vision and Pattern Recognition 
(CVPR 98), 374-281. GAO, P. AND SEDERBERG, T. W. 1998. A work minimization approach to image mor­phing, 
The Visual Computer 14, 390-400. THE GIMP. http://www.gimp.org. HARRISON, P. 2001. A Non-hierarchical 
Procedure for Re-synthesis of Complex Tex­tures. In Proceedings of Winter School of Computer Graphics 
2001, 190-197. HASBRO, INC. 2001. Mrs. Potato Head. http://www.hasbropreschool.com/ MCINERNEY, T. AND 
TERZOPOULOS, D. 2000. T-Snakes: Topology Adaptive Snakes. In Medical Image Analysis, Vol. 4, 73-91. MORTENSEN, 
E. N., AND BARRETT, W. A. 2001. A Confidence Measure for Boundary Detection and Object Selection. In 
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2001), Vol. 1, 477-484. 
MORTENSEN, E. N. AND BARRETT, W. A. 1995. Intelligent Scissors for Image Composi­tion. In Proceedings 
of ACM SIGGRAPH 1995, ACM Press/ACM SIGGRAPH, Com­puter Graphics Proceedings, Annual Conference Series, 
ACM, 191-198. MORTENSEN, E. N. AND BARRETT, W. A. 1998. Interactive Segmentation with Intelligent Scissors. 
In Graphical Models and Image Processing, 60(5), 349-384. MORTENSEN, E. N. AND BARRETT, W. A. 1999. Toboggan-Based 
Intelligent Scissors with a Four Parameter Edge Model. In Proceedings of IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR 99), 452-458. MORTENSEN, E. N., REESE, L. J., AND BARRETT, W. A. 
2000. Intelligent Selection Tools. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition 
(CVPR 99), Vol. II, 776-777. MOUNT, D. M., AND SAALFELD, A. 1988. Globally-equiangular Triangulations 
of Co-circu­lar Point in O(n log n) Time. In Proceedings of the 4th Symposium on Comp. Geometry, ACM. 
PRAUN, E., ET AL. 2000. Lapped Textures. In Proceedings of ACM SIGGRAPH 2000, ACM Press/ACM SIGGRAPH, 
Computer Graphics Proceedings, Annual Conference Series, ACM, 465-470. REESE, L. J. 1999. Intelligent 
Paint: Region-Based Interactive Image Segmentation. Masters Thesis, Department of Computer Science, Brigham 
Young University, Provo, UT. SANDER, P. V., ET AL. 2000. Silhouette Clipping. In Proceedings of ACM SIGGRAPH 
2000, ACM Press/ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 327-334. 
SCANSOFT, INC. 2001. Kai s SuperGoo. http://www.scansoft.com/products/goo. SHEWCHUK, J. R. 1996. Triangle: 
Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. First Workshop on Applied Computational 
Geometry, ACM, 124-133. SIBSON, R. 1978. Locally equiangular triangulations. Computer Journal, 21:243-245. 
SMITH, M. D. 2002. Sally s Flower. Graphics Lab, Brigham Young University. mike_d_smith@byu.edu. Received 
by personal communication. VINCENT, L. AND SOILLE, P. 1991. Watersheds in Digital Spaces: An Efficient 
Algo­rithm Based on Immersion Simulations. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 
13(6):583-598. WEI, L. AND LEVOY, M. 2000. Fast Texture Synthesis using Tree-structured Vector Quantization. 
In Proceedings of ACM SIGGRAPH 2000, ACM Press/ACM SIG-GRAPH, Computer Graphics Proceedings, Annual Conference 
Series, ACM, 479-488. WOLBERG, G. 1998. Image Morphing: a Survey, The Visual Computer 14, 360-372. XU, 
Y., ET AL. 2000. Chaos Mosaic: Fast and Memory Efficient Texture Synthesis. Tech­nical Report MSR-TR-2000-32, 
Microsoft Research. YU, XIAOHUA, MORSE, B. S., AND SEDERBERG, T. W. 2001. Image Reconstruction Using 
Data-Dependent Triangulation. IEEE Computer Graphics and Applications Vol. 21, No. 3, 62-68.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>566652</article_id>
		<sort_key>785</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-2002</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Octree textures]]></title>
		<page_from>785</page_from>
		<page_to>790</page_to>
		<doi_number>10.1145/566570.566652</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=566652</url>
		<abstract>
			<par><![CDATA[Texturing using a set of two dimensional image maps is an established and widespread practice. However, it has many limitations. Parameterizing a model in texture space can be very difficult, particularly with representations such as implicit surfaces, subdivision surfaces, and very dense or detailed polygonal meshes. This paper proposes the use of a new kind of texture based on an octree, which needs no parameterization other than the surface itself, and yet has similar storage requirements to 2D maps. In addition, it offers adaptive detail, regular sampling over the surface, and continuity across surface boundaries. The paper addresses texture creation, painting, storage, processing, and rendering with octree textures.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[volume texture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14166887</person_id>
				<author_profile_id><![CDATA[81545184756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Benson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light and Magic, San Rafael, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14208685</person_id>
				<author_profile_id><![CDATA[81100603632]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light and Magic, San Rafael, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Akeley. Realityengine graphics. Proceedings of SIGGRAPH 93, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Berman, J. Bartell, and D. Salesin. Multiresolution painting and compositing. Proceedings of SIGGRAPH 94, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal. Polygonization of implicit surfaces. Proceedings of SIGGRAPH 88, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E. E. Catmull. A subdivision algorithm for computer display of curved surfaces. Master's thesis, University of Utah, December 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288224</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[P. Cignoni, C. Montani, C. Rocchini, and R. Scopigno. A general method for preserving attribute values on simplified meshes. IEEE Visualization 1998 Proceedings, pages 59-66, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566649</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. DeBry, J. Gibbs, D. Petty, and N. Robins. Painting and rendering textures on unparameterized models. Proceedings of SIGGRAPH 02, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378484</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[R. Drebin, L. Carpenter, and P. Hanrahan. Volume rendering. Proceedings of SIGGRAPH 88, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. F. Frisken, R. N. Perry, A. P. Rockwood, and T. R. Jones. Adaptively sampled distance fields: A general representation of shape for computer graphics. Proceedings of SIGGRAPH 00, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. Gortler, R. Grzeszczuk, R. Szeliski, and M. Cohen. The lumigraph. Proceedings of SIGGRAPH 96, pages 43-54, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. T. Kajiya and B. P. Von Herzen. Ray tracing volume densities. Proceedings of SIGGRAPH 84, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L. Kobbelt, M. Stamminger, and H. P. Seidel. Using subdivision on hierarchical data to reconstruct radiosity distribution. Proceedings of EUROGRAPHICS 97, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122748</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. Laur and P. Hanrahan. Hierarchical splatting: A progressive refinement algorithm for volume rendering. Proceedings of SIGGRAPH 91, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[D. Lischinski and A. Rappoport. Image-based rendering for non-diffuse synthetic scenes. Rendering Techniques 98, pages 165-174, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J Maillot, Y. Hussein, and A. Verroust. Interactive texture mapping. Proceedings of SIGGRAPH 93, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[M. Maruya. Generating texture map from object-surface texture data. Proceedings of EUROGRAPHICS 95, pages 397-405, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. R. Peachy. Solid texturing of complex surfaces. Proceedings of SIGGRAPH 85, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[K. Perlin. An image synthesizer. Proceedings of SIGGRAPH 85, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344936</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[H. Pfister, M. Zwicker, J. VanBaar, and M. Gross. Surfels:surface elements as rendering primitives. Proceedings of SIGGRAPH 00, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344990</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Dan Piponi and George Borshukov. Texture mapping of subdivision surfaces by model pelting and texture blending. Proceedings of SIGGRAPH 00, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344935</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[P. Sander, X. Gu, S. Gortler, and H. Hoppe. Silhouette clipping. Proceedings of SIGGRAPH 00, pages 327-334, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383307</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[P. Sander, J. Snyder, S. Gortler, and H. Hoppe. Texturing progressive meshes. Proceedings of SIGGRAPH 01, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Segal and K. Akeley. The opengl graphics system: A specification. http://www.opengl.org/Documentations/Specs.html, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Soucy, G. Godin, and M. Rioux. A texture mapping approach for the compression of colored 3d triangulations. The Visual Computer, pages 503-514, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808576</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Tamminen and H. Samet. Efficient octree conversion by connectivity labeling. Proceedings of SIGGRAPH 84, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[G. Turk. Generating textures on arbitrary surfaces using reaction-diffusion. Proceedings of SIGGRAPH 91, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383298</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Li-Yi Wei and Marc Levoy. Texture synthesis over arbitrary manifold surfaces. Proceedings of SIGGRAPH 01, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Pyramidal parametrics. Proceedings of SIGGRAPH 83, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Octree Textures David Benson Joel Davis Industrial Light and Magic San Rafael, CA 94912 Abstract Texturing 
using a set of two dimensional image maps is an estab­lished and widespread practice. However, it has 
many limitations. Parameterizing a model in texture space can be very dif.cult, par­ticularly with representations 
such as implicit surfaces, subdivision surfaces, and very dense or detailed polygonal meshes. This paper 
proposes the use of a new kind of texture based on an octree, which needs no parameterization other than 
the surface itself, and yet has similar storage requirements to 2D maps. In addition, it offers adap­tive 
detail, regular sampling over the surface, and continuity across surface boundaries. The paper addresses 
texture creation, painting, storage, processing, and rendering with octree textures. CR Categories and 
Subject Descriptors: I.3.m [Computer Graph­ics]: Texturing; Additional Key Words: Volume Texture Introduction 
Traditionally, texturing a model with image data requires setting up a 2D parameterization of a model. 
This parameterization is typi­cally set up manually by an artist. This step can be automated with techniques 
such as the work presented by Maillot [14] and recent work by Piponi [19]. These approaches still require 
that dicontinu­ous cuts be introduced in the mapping. In addition, even after a parameterization is constructed, 
some tex­ture distortion, or stretching may be caused by the parameteriza­tion. Techniques such as [21] 
can help minimize this stretching on polygonal meshes. Another problem arises when there is the need 
to have a small area of a large model, such as a decal, have much higher detail than the rest. In some 
cases, this can result in parts of the model needing to be split to accommodate these local area of high 
texture detail. Furthermore, these 2D approaches may not apply to geometric rep­resentations which are 
dif.cult to assign a parameterization to, such as implicit surfaces [3] or Adaptive Distance Fields [8]. 
This paper was motivated by these issues, and presents a solution in a new form of texture as well as 
details of integrating these textures into a production environment. Copyright &#38;#169; 2002 by the 
Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of 
this work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1-212-869-0481 or 
e-mail permissions@acm.org. &#38;#169; 2002 ACM 1-58113-521-1/02/0007 $5.00 2 Related Work 2.1 Previous 
Work Texture mapping was .rst introduced by Catmull [4]. This concept was extended in 1985 with the introduction 
of the concept of solid textures, presented independently by Perlin [17] and Peachey [16]. They realized 
that using a surface s position in 3D space as tex­ture coordinates allowed complex surfaces to be texture 
mapped. This work was mainly concerned with procedural textures extend­ing throughout a volume enclosing 
the surfaces being rendered. Peachy s work [16] also suggested the idea of digitized solid tex­tures 
by scanning slices of a model. He points out that for a regular voxel grid of samples, high resolution 
textures would require huge amounts of memory. This form of solid texture is a volume texture. Akeley 
[1] described a hardware implementation. They have also become part of the OpenGL [22] graphics standard. 
However, their main use has been volume rendering [7]. One piece of related work in volume rendering 
was presented in 1991 by Laur et al. [12]. In this approach, a pyramid of average colors is formed from 
a traditional volume texture, and then an oc­tree is constructed that approximates the original data 
to a given accuracy. This results in a similar data structure to work presented in this paper, but it 
is used as a volume rendering optimization. Octree based images have also been considered in the related 
sub­ject of 3D Image Processing, and work by Tamminen in 1984 [24], introduces this connection. We have 
recently become aware of work by DeBry et al [6], who have independently developed a similar approach 
to texture map­ping using octrees. 2.2 Other Approaches Turk developed a method [25] for using a regular 
distribution of samples spread across a surface to represent a Reaction-Diffusion texture. He suggested 
choosing random samples over a surface, which were then relaxed to get a more even distribution. Texture 
look-up was accomplished by a weighted sum of nearby samples. This contrasts with the more structured 
texture presented in this pa­per, in which the sample positions are implicit in its location in the octree. 
Recent work by P.ster et al. [18], extends this concept to represent the surface itself at the sample 
points and replaces the random sample points by a more uniform set, sampled on a grid pattern (a layered 
depth cube representation [13]), which makes it more closely related to the work in this paper, but focuses 
on using the structure as a representation of the geometry for fast rendering. Recent work by Frisken 
[8] presented the concept of Adaptive Dis­tance Fields for modeling surfaces. In this work, an octree 
of scalar values is used to represent the distance of each sample point from a surface. This structure 
is similar in some ways to the octree texture presented in this paper, but the structure and interpolation 
scheme are quite different. Another approach to texture mapping polygons is used extensively in research 
on mesh simpli.cation [5], [15], [23], [20]. In this ap­proach a texture tile is created for each face 
of a simpli.ed mesh. These tiles need to be padded with boundary information from their neighbors to 
ensure continuity. The tiles are then packed into regu­lar textures. The drawbacks in this form of texture 
mapping are that the sam­pling is once again very uneven, and the discontinuous structure of resulting 
texture maps prevents them from being turned into mipmaps. In addition these textures cannot be completely 
continu­ous, as the technique of padding the boundaries only approximates the real neighboring values. 
Gortler et al. [9] presented a method for reconstructing an image from one dimensional line samples. 
This method consisted of con­structing a lower resolution image, and then up-sampling and com­bining 
with higher resolution samples to .ll in detail. This process has some similarities to the reconstruction 
phase presented here. 3 Octree Texture Structure and Usage 3.1 Conceptual Overview Solid texture mapping 
has been around for some time, and relies on the rest position of a model as the texture coordinates. 
These coordinates can be created automatically, in contrast to the lengthy manual process of assigning 
two dimensional coordinates to sur­faces which are rarely easy to parameterize. While solid textures 
are usually procedural, they can also come from a traditional vol­ume texture. This paper proposes the 
use of octree textures, a variation on the volume texture idea in which only the subset of the volume 
that actually intersects the surface of a model is stored. The textures are sparse because only octree 
cells which intersect an object s surface are stored. The octree structure provides an ef.cient way to 
store and look up these textures. Traditional volume textures are a regular 3D lattice of color sam­ples. 
Their memory usage grows with the cube of the resolution. This has made them impractical for general 
use and con.ned their use to primarily medical and scienti.c imaging [10]. However, solid textures have 
other nice properties. For example, at any particular resolution their samples are uniformly distributed 
across space in the region the surface occupies. In addition, these coordinates are continuous across 
the boundaries between different geometry types, and independent of the actual texture assignments. In 
the visual effects industry, we are mainly concerned with colors on these surfaces, which represent only 
a two dimensional subset of the volume. These textures concentrate all their detail at the sur­face itself, 
and at the limit, behave like two dimensional textures. In other words, at the limit, their size grows 
with the square of the resolution. This means that they are capable of achieving the high resolutions 
required in the visual effects industry without becoming unmanageably large. 3.2 Storage These textures 
may be potentially very large, so the representation must be compact. We chose to represent a node by 
a set of .ags indicating which of the node s children are present, and which are leaves. Then, we store 
an array of pointers to child nodes, and an­other array containing the values of the child nodes that 
are leaves (Figure 1). Since most nodes of the tree will typically be leaf nodes, and since near the 
surface an average of half of a node s children will be Node Leaf Flags Node Pointer 0 Leaf Value 0 Child 
Node Vector Node Pointer 1 Leaf Value 1 Child Leaf Vector Node Pointer 2 Leaf Value 2 Node Pointer 3 
Leaf Value 3 Leaf Value 0 Leaf Value 1 Leaf Value 2 Leaf Value 3 Figure 1 A compact representation of 
the texture octree. Child node pointers and leaf data are stored in two contiguous arrays to mini­mize 
pointer overhead. leaves and contain a color sample, this greatly cuts down on storage required by pointers 
to child nodes. 3.3 Texture Coordinates In most cases, the texture coordinates will simply be the surface 
position. But in some cases, it is convienent to use another param­eterization. For example, when using 
subdivision surfaces, the co­ordinates on the control hull may be used rather than coordinates on the 
limit surface. In this way, a polygonal approximation of the limit surface will be textured consistently 
at any level of subdivi­sion. This introduces a small amount of texture stretching. For a deforming animated 
model, the coordinates of the model s rest pose may be used as texture coordinates to attach the texture 
to the animating model. 3.4 Interpolation To look up a sample and maintain a continous .eld of color, 
tri­linear or tri-cubic interpolation is used. With an octree texture, the resolution varies across the 
texture, so we have the additional problem of how to interpolate between re­gions at different levels 
of detail. Our approach is based on the ideas developed for generating smooth textures from quadtree 
im­ages presented by Kobbelt et al [11]. Their approach uses a subdivi­sion strategy which could be repeatedly 
applied to lower resolution areas until the whole image was at the same high resolution and could then 
be interpolated in the usual way. To look up the texture at a sample point, a small neighborhood is followed 
down from the octree root to the leaves. Our implementa­tion uses a 3x3x3 neighborhood for tri-linear 
interpolation. A 5x5x5 neighborhood could be used for tri-cubic interpolation. Figure 2 A .xed neighborhood 
around a sample point is subdivided to the next higher resolution and .lled or interpolated from the 
octree values. Figure 2 demonstrates how this process works. On the left is the neighborhood at depth 
n, and on the right the neighborhood (out­lined in black) at depth (n+1). The black dot represents that 
sample point, and at each stage it lies at the center of the neighborhood. The higher resolution neighborhood 
(right) is constructed from the lower resolution one (left) by subdividing the cells closest to the sample 
point. Then the sub-cells adjacent to the sample point are gathered together. As none of the cells in 
the new neighborhood can be touching the boundary of the old one, the old neighborhood contains all the 
information necessary to create the new one. The following steps give the rule for subdividing a cell: 
1. If the cell contains empty space, then all children are empty. 2. Otherwise, if the cell contains 
a non-leaf node, then the chil­dren are the node s children. 3. Otherwise, if the cell contains a leaf 
or a subdivided leaf, then form the children from a weighted sum of the neighboring cell s colors. Extrapolate 
for node neighbors which do not have a color.  Figure 3 Texture interpolation between the different 
resolutions of a sample point s surrounding cells is achieved by following a 3x3x3 neighborhood from 
the octree root. The neighborhood is initially set to be empty, except for the central cell, which points 
to the entire octree. As we progress down the tree, the region of the octree represented by the neighborhood 
converges towards the sample point as shown in Figure 3. This process is con­tinued until no cell in 
the neighborhood contains an octree node, i.e. we stop when every cell contains a leaf, a subdivided 
leaf, or empty space. The neighboring colors are then tri-linearly interpolated to give the result. This 
process results in a continuous interpolation of the sample points across regions with varying level 
of detail. However, this al­gorithm is a point sampling of the texture, and it may be necessary to average 
a number of these samples to obtain a .ltered texture look-up. 3.5 Texture Filtering When a textured 
object is drawn at a distance, some kind of mini.­cation .lter is desired to prevent aliasing. The ideal 
.lter is to take the average of many visible texels contained in the area that is being sampled. With 
traditional 2D textures, it is common to use mipmaps [27] or summed area tables to effectively do some 
of this .ltering ahead of time as a preprocessing step. Mipmaps also help reduce the amount of the texture 
that needs to be accessed when the surface is far from the camera, and small in the view, because only 
the lower resolution version of the texture need be accessed. Traditionally this average is simply made 
over a rectangle in a 2D texture s parameter space. Making the average in the texture s space may include 
areas of the texture that are not visible from a distance, and may even include regions that are not 
assigned to any part of the model. Figure 4 On a model with two surfaces of different color nearby but 
facing different directions such as a thin sheet with contrasting colors on each side simply averaging 
the texture values can fail and cause colors to bleed or mix. 3.5.1 Problems With Extending 2D Mipmapping 
The octree texture has a structure similar to a mipmap and it is natu­ral to consider following the same 
procedure to build a 3D mipmap. This idea was explored for volume rendering in [12], but those re­sults 
are not directly applicable to surface rendering. Replacing a neighborhood of voxels in a texture by 
a single larger voxel is in some cases a poor approximation to the ideal average mentioned above, because 
this average may contain colors from neighboring surfaces which are not visible at the higher resolution 
level (Fig­ure 4). Because of this, colors can bleed from nearby. For example, if the back and front 
of a thin model are different colors, the re­sulting average color represents a color that should never 
be seen. Color may also bleed between unconnected, but neighboring, close surfaces. 3.5.2 Normal Flags 
In most cases, these problems can be solved manually by assign­ing the con.icting parts of the model 
to separate octree textures. However, in order to handle a broader range of situations automat­ically, 
the octree nodes can be extended by adding normal .ags for rendering. Our goal is to preserve the behavior 
that we get interpolating the highest levels of the texture. We maintian a set of six normal .ags with 
each node, one for each direction of each axis. A .ag is set for any direction of a node which contains 
a surface with a normal closer to the .ag s direction than to any other. If any axis has both direction 
.ags set then the mini.cation is considered no longer valid (Figure 5), and a look-up is forced to continue 
to a higher resolution node. As an optimization, this rule may be ignored if the samples below the node 
are within some user-speci.ed threshold of each other, and can thus be averaged without producing any 
visible color bleeding.  3.6 File Format Considerations for Rendering In order to allow quick access 
to a lower resolution version of the texture, the tree must be stored in breadth .rst ordering, not depth 
 +X -X +Y -Y +Z -Z Figure 5 For rendering, every octree node contains a normal .ag for each direction 
of each axis. If the normal .ags for both directions of any axis are set, mipmap look-up is forced to 
continue to the node s children. .rst. Unfortunately this requires one or two pointers to be stored for 
each node, pointing to the child leaf and child node positions in the .le. Fortunately, for any section 
of nodes on the same level of the hi­erarchy the pointers will be consecutive, and can be recreated from 
the node s .ags given a starting position for the children and leaves. This means that they can be stored 
in a .le in a compressed form with few pointers, and expanded back up to the two pointer form when reading. 
In our implementation, the octree was also tiled to allow only a subset of the actual texture to be held 
in memory.  4 Painting An octree texture may be generated procedurally, but in this case, one can just 
use the procedural model directly to texture the object. In most cases, an artist will be asked to paint 
the details onto the models, or apply scanned photographs to the model. We must be able to apply this 
technique to an octree texture. 4.1 Overview The simplest form of 3D paint programs involve painting 
on a pro­jection of a model, and then projecting the paint back onto the model. In the two dimensional 
case, the surface geometry is subdi­vided to the resolution of the 2D textures and each of the resulting 
micropolygons projected back onto the painted image to get a color for the corresponding texel [2]. This 
concept extends naturally to three dimensions for octree textures. In this case we need to raster­ize 
the rest geometry into the octree, again keeping the real surface position as a texture coordinate for 
the look up from the painted image. If the octree is kept at a .xed depth, painting is greatly simpli.ed, 
as the hierarchy of the new paint and the existing paint will be iden­tical. However, for an adaptive 
octree, the hierarchy can differ. In practice, it is convenient to .rst create a new paint octree and 
then merge that with the existing octree, rather than try to combine the paint and rearrange the octree 
in one step. This two step process also ensures that blending the new paint into the existing texture 
occurs only once for each voxel when the new paint has an alpha channel. The steps involved in mapping 
paint back onto an octree texture are as follows: 1. Rasterize surfaces into the paint octree, using 
the surface s rest coordinates as its location and its projected position as its texture coordinate to 
look-up a color from the painted image. 2. Where the existing octree resolution is higher than the new 
paint s and the paint s alpha is between zero and one, increase the paint octree s resolution to match 
the texture. 3. Form a boundary layer of empty voxels around the paint, at the same resolution as the 
neighboring paint. The intensities of the boundary don t matter, only its effect on the next step. 4. 
Increase the resolution in the main octree anywhere where the paint octree s resolution is higher. 5. 
Merge the paint octree into the main octree, ignoring the tem­porary boundary layer voxels. 6. Delete 
the paint octree. 7. Cull any voxels in the main octree that are not intersected by any surface.  
4.2 Merging Existing Paint The approach presented by [2] for multiresolution painting can be applied 
to 3D. If an artist paints a transparent wash over some ex­isting detail, their intention is clearly 
to modify the color of the region. If we simply merged in the wash (which is probably at a lower resolution), 
the detail would be lost in the process. In order to preserve it, the resolution of the wash must be 
increased to match the existing detail. Another situation occurs when an artist paints some .ne detail 
over an area which was previously covered by some smoothly interpo­lated low resolution paint. In this 
case, the resolution of the existing low resolution paint must be increased to match the new detail. 
In order to increase the resolution of an octree texture, we need to take into account the interpolation 
scheme being used. If we sim­ply split each leaf into children with the same value, the bound­aries of 
the original voxel will become visible as discontinuities in the resulting texture. This is analogous 
to performing the cor­responding operation on a 2D texture, where the new pixel values would be formed 
by interpolating the lower resolution values. In addition, the effect of an edit on neighboring voxels 
must also be taken into account. Ideally, when the resolution of a voxel is in­creased, the neighboring 
regions should remain unchanged. This can be achieved by keeping the old low resolution leaf around, 
and using it during the lower resolution steps of the reconstruction pro­cess. These resolution increases 
are calculated and applied locally, and not to the texture as a whole. 4.3 Considerations for Painting 
4.3.1 Color Bleeding If the model contains intersecting surfaces, they must be separated out into their 
own octree textures, or a special version of the rest model needs to be constructed in which those parts 
have been moved apart. If these steps are not taken, the intersecting regions will share a common paint 
color. This is mainly a problem if these regions are animating, and these intersection regions become 
ex­posed. 4.3.2 File Format for Painting The sparse octree can be stored very ef.ciently in a consistent 
depth .rst order. This method is outlined in work by M. Samet [24], and the authors show that even further 
compression is possible. Figure 6 Comparision of octree textures at several resolutions to a set of 
2D texture maps. When rendering, the texture should be converted to a .le format like the one suggested 
in Section 3.6 to perform look-ups ef.ciently.  5 Results Our initial work involved converting a model 
that was already painted with a set of traditional textures. The textures were con­verted into octree 
texture without any loss of visual quality or arti­facts. The octree texture took less disk space than 
the large set of 2D images it was generated from. For comparison, a part of a traditionally textured 
version of the droid model in Figure 8 is compared to an octree textured ver­sion of the same model. 
This part of the model was painted with .ve maps: two of size 256x1024, one 1024x512 and one 128x1024. 
The assignments were typical of the way maps are assigned. Max Depth Uncompressed Size 8 (256) 190,189 
9 (512) 740,613 10 (1024) 2,901,789 2D Version 3,534,944 The maps were then converted to octree texture 
at different .xed maximum depths. In this case, the octree texture were not adaptive, so areas of similar 
color were sampled at the same rate as detailed areas. When the octree texture is at the same resolution 
as the largest di­mension of the texture maps, the resulting .le can be smaller than the 2D texture maps. 
The octree texture stores samples only where geometry is present, whereas the traditionally mapped version 
must oversample some areas in order to get other areas sampled at the desired resolution. Of course, 
careful assignment of texture coordinates, or methods to relax the the texture coordinates such as presented 
by [14] can offset this small difference. Additionally, image compression techniques are much more well-studied 
for 2D images. What is important to note is that octree textures have similar storage characteristics 
to sets of 2D textures as the texture resolution increases. To integrate the new textures into our production 
pipeline, we .rst tested it on a small prop (Figure 7). Currently, octree textures are used on characters 
and props that need to be set up quickly, or are dif.cult to assign texture coordinates to. Models that 
use a mix of surface representations also are more straightforward to texture using octree textures. 
The droid model shown in Figure 8 is an example of a model which was painted using octree textures for 
these reasons. Octree textures have also allowed us to experiment with other types of surface representations, 
such as Adaptive Distance Fields and Subdivision Surfaces without worrying about how or even if they 
can be parameterized for 2D texture maps. 6 Known Problems and Future Work 6.1 Painting in 2D The most 
immediate drawback to using octree textures is that one loses the ability to paint on the model in texture 
space, that is, to paint directly on the maps. An approach similar to that which Wei and Levoy [26] used 
to generate a local parameterization for texture synthesis could be used to locally unwrap a region of 
the texture to be painted on. 6.2 Very Close or Intersecting Surfaces Octree textures do not handle 
surfaces in the rest model that are intersecting or very close, such as skin and clothing. The normal 
.ags mentioned in section 3.5.2 help to prevent the mini.cation artifacts that this can cause, but care 
must be taken when setting up a model to either assign different octree textures to intersecting or very 
close surfaces, or to create a special version of the rest model where these surfaces are moved away 
from each other.  Figure 8 Several versions of this droid model were painted using octree textures. 
(Fig. Bridget Goodman) 6.3 Interactive Viewing Currently, no hardware supports 3D textures as anything 
other than a regular 3D grid. Until this, or some similar compressed form of texture can be supported 
in hardware, the texture must be approxi­mated by a set of automatically generated 2D textures if it 
is to be viewed interactively. Also, better techniques to automatically generate such sets of tex­tures 
from an octree texture ef.ciently, and at high quality, would help make it easier to manipulate octree 
textures with existing tools. 6.4 Adaptive Resolution Limits With the ability to add detail at any scale, 
an octree texture can grow to be much larger than is needed to represent a particular set of tex­ture. 
Our solution is to simply allow the artist to adjust the maxi­mum resolution they wish to paint with 
at any given time. However, image processing techniques could be used to adaptively discover the optimal 
resolution of any new paint without manual interven­tion.  7 Summary and Conclusion We have described 
a new approach to storing textures for geomet­ric models by using a sparse octree to store texture information, 
typically color. The approach is useful independent of the surface representation and does not need any 
parameterization other than the surface coordinates themselves. Furthermore, we present some approaches 
to using these textures in a production environment and extend traditional 2D sampling tech­niques to 
render these textures with fewer visual artifacts. We also provide a technique to create the textures 
by painting onto projec­tions of geometry. Our experience using these types of textures in a production 
envi­ronment shows them to be fast and robust, and similar or smaller in disk space to a set of traditional 
2D textures at the same resolu­tion. In addition, there can be a tremendous savings in artist time because 
texture coordinates do not need to be assigned manually for each model. References [1] K. Akeley. Realityengine 
graphics. Proceedings of SIGGRAPH 93, 1993. [2] D. Berman, J. Bartell, and D. Salesin. Multiresolution 
painting and compositing. Proceedings of SIGGRAPH 94, 1994. [3] J. Bloomenthal. Polygonization of implicit 
surfaces. Proceedings of SIGGRAPH 88, 1988. [4] E. E. Catmull. A subdivision algorithm for computer display 
of curved surfaces. Master s thesis, University of Utah, December 1974. [5] P. Cignoni, C. Montani, C. 
Rocchini, and R. Scopigno. A general method for preserving attribute values on simpli.ed meshes. IEEE 
Visualization 1998 Proceedings, pages 59 66, 1998. [6] D. DeBry, J. Gibbs, D. Petty, and N. Robins. Painting 
and rendering textures on unparameterized models. Proceedings of SIGGRAPH 02, 2002. [7] R. Drebin, L. 
Carpenter, and P. Hanrahan. Volume rendering. Pro­ceedings of SIGGRAPH 88, 1988. [8] S. F. Frisken, R. 
N. Perry, A. P. Rockwood, and T. R. Jones. Adap­tively sampled distance .elds: A general representation 
of shape for computer graphics. Proceedings of SIGGRAPH 00, 2000. [9] S. Gortler, R. Grzeszczuk, R. Szeliski, 
and M. Cohen. The lumigraph. Proceedings of SIGGRAPH 96, pages 43 54, 1996. [10] J. T. Kajiya and B. 
P. Von Herzen. Ray tracing volume densities. Proceedings of SIGGRAPH 84, 1984. [11] L. Kobbelt, M. Stamminger, 
and H. P. Seidel. Using subdivision on hierarchical data to reconstruct radiosity distribution. Proceedings 
of EUROGRAPHICS 97, 1997. [12] D. Laur and P. Hanrahan. Hierarchical splatting: A progressive re.ne­ment 
algorithm for volume rendering. Proceedings of SIGGRAPH 91, 1991. [13] D. Lischinski and A. Rappoport. 
Image-based rendering for non­diffuse synthetic scenes. Rendering Techniques 98, pages 165 174, 1998. 
[14] J Maillot, Y. Hussein, and A. Verroust. Interactive texture mapping. Proceedings of SIGGRAPH 93, 
1993. [15] M. Maruya. Generating texture map from object-surface texture data. Proceedings of EUROGRAPHICS 
95, pages 397 405, 1995. [16] D. R. Peachy. Solid texturing of complex surfaces. Proceedings of SIGGRAPH 
85, 1985. [17] K. Perlin. An image synthesizer. Proceedings of SIGGRAPH 85, 1985. [18] H. P.ster, M. 
Zwicker, J. VanBaar, and M. Gross. Surfels:surface ele­ments as rendering primitives. Proceedings of 
SIGGRAPH 00, 2000. [19] Dan Piponi and George Borshukov. Texture mapping of subdivision surfaces by model 
pelting and texture blending. Proceedings of SIG-GRAPH 00, 2000. [20] P. Sander, X. Gu, S. Gortler, and 
H. Hoppe. Silhouette clipping. Pro­ceedings of SIGGRAPH 00, pages 327 334, 2000. [21] P. Sander, J. Snyder, 
S. Gortler, and H. Hoppe. Texturing progressive meshes. Proceedings of SIGGRAPH 01, 2001. [22] M. Segal 
and K. Akeley. The opengl graphics system: A speci.cation. http://www.opengl.org/Documentations/Specs.html, 
1997. [23] M. Soucy, G. Godin, and M. Rioux. A texture mapping approach for the compression of colored 
3d triangulations. The Visual Computer, pages 503 514, 1986. [24] M. Tamminen and H. Samet. Ef.cient 
octree conversion by connec­tivity labeling. Proceedings of SIGGRAPH 84, 1984. [25] G. Turk. Generating 
textures on arbitrary surfaces using reaction­diffusion. Proceedings of SIGGRAPH 91, 1991. [26] Li-Yi 
Wei and Marc Levoy. Texture synthesis over arbitrary manifold surfaces. Proceedings of SIGGRAPH 01, 2001. 
[27] L. Williams. Pyramidal parametrics. Proceedings of SIGGRAPH 83, 1983.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2002</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
