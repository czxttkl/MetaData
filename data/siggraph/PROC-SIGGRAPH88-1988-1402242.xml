<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/01/1988</start_date>
		<end_date>08/05/1988</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1402242</proc_id>
	<acronym>SIGGRAPH '88</acronym>
	<proc_desc>ACM SIGGRAPH 88 panel proceedings</proc_desc>
	<conference_number>1988</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1988</copyright_year>
	<publication_date>08-01-1988</publication_date>
	<pages>480</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>In the beginning, the only function at the SIGGRAPH annual conference was the technical papers session. While the technical papers were the primary focus of the conference, as indeed they are now, many other activities have been added to the conference through the years. Conference goers today know that, besides the papers, there is an equipment exhibition, an art show, a film and video show, about thirty day-long courses, and a complete slate of panels. For all these functions, there is some written record of the event's activities. Papers are published in the Proceedings, there are catalogs for the film and video show and the art show, printed notes are provided for the courses, and exhibition attendees can carry away literature in any volume they can manage. Finally, brief summaries of the panel sessions, written in advance of the conference, appear in the Proceedings.</p> <p>In view of the growing importance of the panels to conference attendees (we often find panels better attended than paper sessions), this seems a disproportionately small record of their proceedings. For SIGGRAPH'86, in Dallas, some first steps were taken to provide technical program attendees with a permanent record of what transpired at the panel sessions. All panel presentations, as well as subsequent intra-panel and inter-audience interactions were audiotaped. After the conference, the tapes were transcribed to computer- readable form and were rough-edited by the Panels Chair to correct words and phrases that were nearly inaudible or were not understood by the transcriber. Following that, the intention was to have each panel chair, eleven in all, further edit their material, obtain appropriate visuals from their panelists, and to return all material to the Panels Chair for publication. Unfortunately, budget uncertainties for SIGGRAPH'86 and a serious underestimate of the work involved precluded these final steps.</p> <p>For S1GGRAPH'88 we had the opportunity (and the budget!) to try again. Just as in 1986, the panel proceedings were audiotaped, most visual material was captured on site, and, with the assistance of the Panels Committee, the panel chairs, and paid staff, the material was transcribed, edited, and printed. A copy of the panels proceedings is being mailed to every person who registered for the technical program. In this way we hope to provide panel attendees with more than just fond memories of the excitement they experienced at the Conference. More important, it is a first step in developing an archive of a highly significant annual event in the field of computer graphics.</p> <p>Finally, we hope that the reader will be both critical and tolerant of this first effort. Tolerant of the mangling of some names, particularly those of the questioners from the audience which we had no way of verifying. Critical feedback is essential at this point. Do you want to see this activity continue? Is it worth the effort for SIGGRAPH to produce panels proceedings and do you find it to be a valuable publication?</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P1101780</person_id>
			<author_profile_id><![CDATA[81547438656]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[R.]]></first_name>
			<middle_name><![CDATA[L.]]></middle_name>
			<last_name><![CDATA[Phillips]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Los Alamos National Laboratory, Los Alamos, NM]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1988</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1402243</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>43</pages>
		<display_no>1</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Screen postscript]]></title>
		<page_from>1</page_from>
		<page_to>43</page_to>
		<doi_number>10.1145/1402242.1402243</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402243</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101751</person_id>
				<author_profile_id><![CDATA[81100125322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geschke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adobe Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101752</person_id>
				<author_profile_id><![CDATA[81100422560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGregor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Equipment Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101753</person_id>
				<author_profile_id><![CDATA[81100031518]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gosling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sun Microsystems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101754</person_id>
				<author_profile_id><![CDATA[81100046940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Leo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hourvitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NeXT, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101755</person_id>
				<author_profile_id><![CDATA[81336487875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Callow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SCREEN POSTSCRIPT Chair: Charles Geschke, Adobe Systems Panelists: Scott McGregor, Digital Equipment 
Corporation James Gosling, Sun Microsystems Leo Hourvitz, NEXT, Inc. Mark Callow, Silicon Graphics SCREEN 
POSTSCRIPT CHARLES GESCHKE: SIGGRAPH is certainly high technology, even the podium moves. It's quite 
outstanding. Well, I want to welcome all of you to this afternoon's session on Display PostScript, or 
as the title indicates, Screen PostScript. I think, judging by the number of people who have shown up, 
it's going to be a very interesting session for all of you, and certainly all of us on the panel are 
excited to have an opportunity to present our ideas in this area that's become a very popular one in 
the industry. I'm joined today on the panel by four co-panelists: Mark Callow, from Silicon Graphics; 
James Gosling, from Sun Microsystems; Leo Hourvitz, from NEXT; and Scott McGregor, from Digital Equipment. 
We've organized the panel today so that each of us will spend ten or twelve minutes giving you our perspective 
on what's happening in the area of Display PostScript and, in general, high performance graphics -- full 
functional graphics for the screen. And we're going to deliberately leave some time at the end of this 
session -- I hope at least a half hour -- during which we'll entertain questions from you in the audience, 
because we want this to be as interactive a session as possible. Therefore, I'd ask you to hold all of 
your questions until the end and we'll try to get to all of them. If we find that there's difficulty 
in responding to all of the questions that you have, those of us who are able will move to a break-out 
room that's been provided for continuing the discussion after the panel. And that's in room 303, just 
down the hall from the auditorium. One more piece of logistics before I begin: following this session, 
I've been asked to announce, there's also going to be a meeting of the PostScript users' group. And that 
meeting's going to be held in room 305, which is, I believe, also just down the hall from the auditorium. 
When Dick Phillips asked me to chair this panel I was delighted, because, of course, the topic that we're 
discussing is something that's of vital interest, both to me personally and to the company that I represent, 
Adobe Systems. The technology of Display PostScript is something that we have actively been working on 
as a graphics technology ever since the inception of our company back in December of 1982. To date, Adobe's 
major activity involving the PostScript language in terms of commercially available products, has been 
in the area of printers and typesetting equipment. Today we have over 25 announced OEM's who produce 
products in this area of printing and typesetting; another dozen or so that will announce during the 
next six to 12 months. One of the reasons that PostScript name in the title of this panel attracts so 
much interest is the amount of penetration into the marketplace of full functional graphics that we've 
seen develop over the last five years. Today, roughly 33 products are being shipped into the field from 
our OEM's -- and that number is growing at the rate of one or two additional printers and typesetters 
a month. By the end of this year, I think there'll be something like 75 developed products, most of them 
beginning to ship to the market. In order to make a standard successful, you not only have to have the 
products that embody the standard, but you have to have people who are willing to use it. So a very active 
part of the business of making PostScript into something that's become a commercially viable and interesting 
technology, has been working with third-party software developers to develop applications that drive 
PostScript printer and typesetter products. And, to date, there are over 1,500 of those. That's the reason, 
I think, the topic of this session is so interesting, as we begin to see this technology move from the 
printed page to the computer graphics interactive display. In the process of developing PostScript technology 
over the last six years, we've learned a number of things about what the corporate customers out there 
in the marketplace and people like yourselves -- who come both from corporations and the university environment 
-- are looking for from those of us who are on the vendor side of the computer systems business. Certainly, 
people are interested in standards. The reason they're interested in standards is that they want to see 
a wide availability of products from multiple vendors that share a common platform, that really give 
them some leverage in the area of price performance. And what I would like to share with you today is, 
given this perception of the marketplace, the sort of the thinking and, as it were, philosophy behind 
developing PostScript for printers and now for displays that we had in mind, to give you the background 
for the development activities that the other four panelists are going to talk about. In any case I'm 
probably no longer in a position to give an in-depth technical talk these days: I tend to spend more 
of my time on the commercial side of my business. But I think understanding what's driving technology, 
from the point of view of what people who are actually buying products want, is an important background 
to understanding why the technology is interesting and to anticipate where it's going in the future. 
We had several goals in mind as we developed Display PostScript. First, we wanted to bring the same imaging 
model to the computer graphics interactive display that we have brought to the printer. We wanted to 
provide the same kind of device independence that had been available in the printer world for the display 
world. That meant, from our point of view, in order to make effective this kind of penetration into a 
commercial market, we had to be operating system and windowing system independent. And, again, as the 
panelists talk, you'll see different viewpoints on exactly how people decide to divide up this technology 
and package it. We believe very much in high technology development, which means we're developing for 
systems that will be generally available in the next 12 to 18 to 24 months. So we're concentrating on 
technology that's on platforms of high-performance processors, like 68020's and 030's, a variety of RISC 
processors, 386's, VAX-class machines. From Adobe's point of view -- when we think and use the term Display 
PostScript, we view it as a piece of operating system's software. -- an operating system for controlling 
the display. We don't anticipate, from a commercial point of view, delivering this technology to the 
market as a retail product; we expect to bring it to the marketplace through major OEM's in 2 the computer 
systems business. And, in most respects, it may end up being quite transparent to the customer who actually 
buys a machine. As a result, we need to co-exist in a variety of different operating system configurations, 
we need to be able to support a broad spectrum of applications, to co-exist with the kind of tool kits 
and application interfaces and windowing systems upon which the manufacturers who bring computer systems 
to the marketplace want to standardize. It's very easy when you hear the concept of Display PostScript 
discussed to be confused between the difference between a Display PostScript system as we think of it 
-- the imaging model that draws images inside these windows -- and a windowing system. When I think of 
a windowing system, I think of a set of software that has the responsibility for dividing up the screen, 
providing a standard mechanism for menus and keyboard handling and controlling network communications 
and the file system, and allocating resources within the operating system, for the application developers. 
At Adobe we consider it to be Display PostScript's responsibility to draw images within those windows. 
High-quality text, high-quality line art -- both color and black and white -- and, of course, continuous 
tone or photographic images. As I said before, one of our major goals was to have an identical imaging 
model on the screen as we have on the printed page with PostScript. The goal of allowing the applications 
developers and, ultimately, the end users, to anticipate no disparity between what you see on the screen 
and what you see on the printed page -- either in terms of quality or function. We wanted to bring the 
same kind of typefaces that have become so popular in the area of printers and typesetting equipment 
to the display marketplace, as well. We recognize there are issues associated with very small point sizes 
at low to medium resolutions on screens, but we provide a seamless interface to our typeface technology 
so that the end user isn't aware of whether a bitmap is used at a small point size or an outline at a 
larger size. Really, when you get right down to it, what we're trying to do is to bring the same kind 
of quality to displays that people have begun to appreciate and value very highly on the printed page. 
In particular, the availability to a developer to develop for a very high-end system with the confidence 
that there is a large installed base of lower capitalized equipment that can take the same kind of software 
and represent it on lower performance monitors -- grey scale or black and white -- even though the original 
application was designed for color. And the reason that this works in the Display PostScript environment 
is that we take over the responsibility for rendering grey, continuous tone or color. So whether you 
have one, two, four, or eight bits per pixel on a monochrome, grey scale monitor, the applications program 
simply specifies percentages of grey and we take over the responsibility of how to render it. Similarly 
with color. Even if you're only prepared to allocate a modest number of bits -- like four bits per color 
-- if one dithers and half-tones, which again is completely done within the Display PostScript system, 
we can define a color map that appears to be full, continuous tone color, and produce very acceptable 
quality line art, as well as photographic images. And, again, bring the kind of device independence that 
has been so important in the printer world to displays. When we think about developing products -- I'm 
sure it's true of all of the people on the panel who have a commercial interest -- you have to think 
about how you're going to sell, how you're going to provide a benefit to the ultimate customer. And we 
think about three customers 3 when we think about a piece of operating systems software like Display 
PostScript. There're the end users, there are our direct customers -- our OEM's -- and there are the 
third-party software developers. From the point of view of end users, as I alluded to before, they're 
very interested in seeing an operating system base for displays that will assure them that the kind of 
applications that they wish to have as end user customers, are going to become easily available across 
a variety of platforms. And they want those platforms to be consistent, but also provided by multiple 
vendors. The typeface solutions that are available in a product like Display PostScript are also of very 
high importance to them because they've made a significant investment in acquiring that typeface technology 
for printing. From the point of view of the OEM's that we service, they sometimes find themselves somewhat 
schizophrenic in the following sense: on the one hand, they recognize the demand for standards. We certainly 
see a lot of activity today in major corporations trying to support standards across the industry. And 
that sometimes comes in conflict with the corporation's desire and natural need to maintain a competitive 
edge. As a vendor to those OEM's, we've tried to maintain a flexibility that will allow them to maintain 
both the standard, and have the ability to add their own proprietary advantage. So, in particular, when 
we offer Display PostScript to an OEM, we give them the kernel of the imaging model in object code format 
to run in their environment, but give them source code interfaces to deal with different windowing systems 
and client libraries and applications interfaces. We give them source interfaces to deal with whatever 
operating system they would like to use. Probably most interestingly, we give them the source code to 
the software for controlling the display devices themselves. So that the OEM is in complete control of 
adding their own proprietary hardware and software to accelerate the kind of graphics performance they 
want to provide to their customers, to control their own development environemnt, and to establish their 
own point along the price performance curve. From the developer's point of view, we've tried to take 
away some of the major complexity that they've experienced in putting together high performance graphics 
systems by giving them a highly leveraged base that they can anticipate will be available across a broad 
spectrum of different products from different vendors. But, in particular, in order to make their job 
effective on interactive displays, we recognize the need to expand and extend the PostScript page description 
language, in such a way as to give them the features that they need for doing high performance and truly 
interactive graphics. There are multiple execution contexts and lightweight processes so you can have 
simultaneous upgrade of multiple windows on the screen. We recognized that in a directly coupled system, 
the binary interface was important, the ability to manipulate and switch very quickly between different 
graphic states, and to cache a variety of information once it's've been rendered at screen resolution. 
These extensions are a way of maintaining and extending the lifetime of the PostScript language for displays. 
And just to sum up very quickly, we have shown some technology demonstrations at a number of shows --we're 
not demonstrating here at SIGGRAPH, unfortunately. The demonstrations run on five different platforms 
from four different vendors, five different 4 CPU's, several different operating systems and display 
interfaces. What's especially interesting about that demonstration is how easy it was for us -- a matter 
of a few man-weeks to port to each of those environments. To date, the only announced customers -- two 
of whom are represented in the panel -- are NEXT, Digital Equipment Corporation, and Scitex. Well, I 
thank you very kindly for your attention. I'm going to turn podium over to Mark Callow from Silicon Graphics, 
who's going to give us a presentation on their activities in the area of Display PostScript. Mark. MARK 
CALLOW: Good afternoon. I work in the Interface and Window Systems groups at Silicon Graphics. While 
at NCGA early this year, I overheard a salesman for a certain vendor of Display PostScript describe NeWS 
as follows: Sun was in such a hurry when they implementated NEWS, that they got the PostScript interpreter 
completely entangled in the window system. With Display PostScript, the interpreter is a separate entity 
from the window system. I'm telling you this now not to cause any rifts between the two PostScript communities, 
but because it shows a basic misunderstanding of the concept of NeWS and its use of PostScript. NeWS 
uses PostScript as a communications protocol making the window server maleable in a way that I find very 
appealing. The input subsystem also draws on the power of PostScript, particularly its dictionaries, 
to great advantage. Let me illustrate. Can you bring up the video, please? The slide showed what's on 
the video so you'd be able to see it a little more clearly. In this picture, can you tell what parts 
were drawn using PostScript? Well, you're probably thinking, PostScript doesn't do 3-D yet, so those 
3-D images weren't drawn using PostScript. And you'd be right. Nevetheless, PostScript played a very 
important part in those images being on the screen. In the window system pictured here that we call 4Sight, 
a PostScript based window server allowed us to provide a standard window system that we could also mold 
to provide compatibility with Silicon Graphics' previous proprietary window system. Let's see how it 
works. In NEWS, images are drawn onto surfaces called canvasses. The server manages a scene of canvasses 
on the screen. We took the NeWS server and we modified it to allow us to create cut-outs on the screen 
into which we can draw the 3-D images. We call these cut-outs GL canvasses. Drawing is done through a 
graphics library that draws direct to the hardware, by- passing the server. The shape of the GL canvas, 
as with any canvas in NEWS, is determined by a PostScript process and can, therefore, be any shape that 
you can imagine. Then, after we had implemented these cut-outs, we added a window management interface 
layer, written in PostScript, that mimicked our old proprietary window system. As a result, existing 
programs can run under radically different window systems after simply being relinked. This is a clear 
demonstration of the power of PostScript in NeWS allowing us to download our own protocol into the window 
server. And, just for good measure, and because our customers were asking for it, we threw in a X-server, 
that, as a GL client, uses these cut- outs to get space on the screen for its own windows and then draws, 
using our graphics library. 5 The GL interface uses two lightweight processes. A lightweight process 
is a thread of PostScript control having its own operand, dictionary, and execution stacks. The first 
lig.htweight process services requests, window management requests, from the client -- such as wm move, 
wm open, win push, win part. These requests create and manipulate the GL canvasses. The interface is 
written as a sub-class of the standard NeWS window manager, using the class programming system that NeWS 
provides through creative use of PostScript dictionaries. PostScript is also used to paint the shape 
of the GL canvas into a set of clipping planes. This prevents the client from drawing outside its cut-out. 
The second lightweight process handles input. It receives events from the NeWS input system, translates 
them into the type of event expected by our GL clients, and inserts them into the client's input queue. 
One particular feature of the NeWS input system made this job particularly easy. Events in NeWS are PostScript 
dictionaries with a number of fields, including the one shown on the slide here -- name, action, location, 
and time stamp. An interest in an event is expressed by creating a template event. Any non-null fields 
in the interest are matched against fields in the real event. A lightweight process executes the "awaitevent" 
primitive, and when "awaitevent" returns, it will be with a real event that matches the interest previously 
expressed. The name and also the action field may be a dictionary or an array. If the name field is a 
dictionary, a real event matches the interest, if the name matches any key in the dictionary. And the 
dictionary value can be an executable array. And this executable array is executed just before the "awaitevent" 
returns. The input lightweight process uses a single interest with a dictionary in the name field. The 
dictionary contains a key for each real event that we are interested in. And the value is an executable 
array that translates the particular type of event that we receive into the graphics library form and 
stuffs it into the client's input queue. This keeps the translation code in very small and easy-to-manage 
fragments, and also keys can be added to or removed from the dictionary at any time. Which means that 
we don't have to have any communication between the input lightweight process and the request lightweight 
process to handle the queueing of various devices. The third lightweight process is created whenever 
the GL client requests pop-up menu service. This is where having PostScript handling both input and output 
really shines. The single PostScript program draws the menu and handles all interaction with the user. 
This is also how menus work for any NeWS client. The GL menu interface is a sub-part of the standard 
NeWS menu class and, again, demonstrates using PostScript to modify the protocol. Let's look at that 
standard menu class. The standard menu package is written in PostScript and downloaded into the server. 
And the single package assures a consistent style for all client programs. The style can be changed by 
loading a different menu package into the server or interactively modifying the package that is currently 
running. Clients don't need to be re-linked. And changes can even affect the client program that's already 
running. I'll demonstrate that for you now. You can bring up the video. Okay. If you look at the menu 
now, we see that it's highlighted with -- you can see it on the video -- that it's highlighted by inverting 
the video and the that the text is fairly small. I can 6 download a little program into the server which 
will change the menu and give me the box around the menu items that are highlighted, and also change 
the font to a very much bigger one. So that's the kind of thing you can do with this system. And another 
very good example of the use of PostScript for both input and output is given by this little calculator 
program created by David LeVally. And this program is entirely written in PostScript and is downloaded 
into the server. And what's interesting about it is because PostScript is handling the input, all of 
the input events are in the same coordinate system that is being used for the imaging and the output. 
So to do this program, this program divides its coordinate space into a five-by-five rectangle, and that 
makes it very easy for us to determine which button you hit because the coordinate space corresponds 
with the buttons on the calculator. I'd like to thank RGB Technology for lending me their scan converter 
so I was able to show you these images. Okay. So, in conclusion, PostScript in a window system can be 
used for far more than imaging. We've seen how PostScript plays an important part in producing 3-D images 
in a windowed environment, even though it isn't actually doing the drawing. And using 4Sight as an example, 
we've seen how PostScript, when properly integrated into the window system, lets it be readily molded. 
We've observed that PostScript can play an important part in putting images on the screen, even when 
PostScript doesn't image. And, lastly, we've seen how integrated PostScript can be used to provide complete 
interactive tools and applications with a readily changed look and feel. The next speaker is James Gosling 
-- a household name in window systems and text editors, and the proud father of a new baby, as well as 
the father of NEWS. JAMES GOSLING: Greetings. This business of having a new baby is more an exercise 
in sleep deprivation than anything else. Can I have my slides? Anyway, when I first thought about talking 
on this panel, the panel's title was Screen PostScript. And I asked myself what I could say about Screen 
PostScript. And I thought that Screen PostScript was a little limiting. So I'll try to talk about that 
more and about why I guess I think that some of the things that Adobe is doing are missing the point. 
Now, these first few slides are just to show you that you can stretch windows, and PostScript really 
does do scaling properly. I actually used PostScript in a PostScript window system to generate these 
slides. I think that I might be unique in that respect. So why is Screen PostScript interesting. As far 
as I'm concerned, PostScript is the greatest thing since sliced bread. When I first saw it, the thing 
that really impressed me was that it has a very sophisticated imaging model. So you can do things like 
draw this face of a clown and have this drawn and it looks beautiful. It's got wonderful curves, interesting 
color. PostScript is ideal for this. This particular drawing was done with a program that's sort of vaguely 
similar to Illustrator 88, that runs on NEWS. The other thing that was interesting about PostScript for 
the screen, is that it provides us with a way to deal with edit correspondence. In our previous window 
system, if you put 7 something on the screen, about the only way that you could get something that looked 
the same out on the printer was to take the bits off the screen and help them out. But this, I think, 
is really sort of the beginning of why Screen PostScript is interesting. There are a number of other 
problems that you can solve using PostScript. What PostScript really does is that it uses communication 
with the programming language between two agents as a way to solve graphical problems. But you can use 
this to solve a large number of other problems, as well. There are a number of sort of synchronization 
issues that you can solve: if you take a look, for instance, at the X- 11 protocol, there are a number 
of very complicated things in it having to do with what they call grabs, that are extremely complicated, 
that are trying to deal with the synchronization issue, and by having a downloaded programming langauge, 
that's completely side-stepped. It's an interesting way to communicate and get various sorts of data 
compression when you send messages down to window systems that are in terms of programs that execute 
it so you can do things like draw grids with significantly less data being sent. Then also you can do 
some very interesting things with interactivity. In a previous window system that I was involved with 
-- a system called Andrew -- one of the things that we noticed was that with application programs, you 
were watching the mouse drag and trying to do animated things, there was a lot of traffic on the net. 
And so NeWS lets you solve some of this. And also, one way that I'm sort of philosophically different 
from some of these other folks is that I believe that PostScript should be ubiquitous. Why shouldn't 
the borders of my window be as beautiful as the contents. So here's the economical, architectural picture 
of NEWS. NeWS is the server process which embodies within it a PostScript intepreter. And it has a number 
of lightweight processes. I was very interested in Adobe's list of extensions to Display PostScript because 
we actually have all of those, including Cache, Pass, and Graphics Context, although we never actually 
really tell people about those. One of the things that we tried to do when we implemented NeWS was to 
avoid adding anything to the graphics model at all because we wanted to preserve printer compatibility. 
Even though you can snapshot graphics contexts and so forth, the fact that you can't use that on a printer 
leads us to discourage people from using that, although it's there because sometimes you have to have 
it. So we have this PostScript interpreter. There are client processes that are talking to it across 
the net. And this protocol that they're speaking is just a binary form of PostScript. One way to think 
of it is that it's an ordinary network protocol where you're saying move to here, line to there, draw 
circle, fill it. Another way to think of it is that the client program is sort of detaching a part of 
itself, setting it to sort of swim down river, and get lodged inside the server and do some work for 
it. So this is the set of things that we had to add to the PostScript language in order to make it useful 
as a window system. As I said before, we tried very hard to stay away from sort of changing the imaging 
model. We wanted to keep that as compatible to printers as possible. But there were a number of things 
that we had to do to deal with the broader scope of the system that it was going to be running in. So 
the very first thing we added was something 8 called Canvasses. This gives you multiple drawing surfaces. 
On a printer, clearly, you've got one drawing surface, so this wasn't an issue. Because you've got multiple 
application programs who all want to deal with the window system at once, you need to have a way for 
them all to be sweeping PostScript at once. So this PostScript system has to deal with multiple PostScript 
contexts running at once. Which is what lightweight processes are. But there are a number of other interesting 
things you can use lightweight processes for, like tracking the mouse, and dealing with menus, and so 
forth. And because we had added lightweight processes, this caused us some problems with storage management, 
so we couldn't go to sort of the safer store stack style that Adobe's PostScript has and we had to implement 
a real garbage collector. I go back and forth on whether or not this is a good idea. It's certainly wonderful 
for people who use it, because they can save data and data can move around and they don't have to worry 
about having bracketed things with save and restore, like blowing their data away. But I'll never implement 
a garbage collector again. I'll let somebody else do it. We had to implement a way to deal with the keyboard 
and the mouse, so this is what became input events. These are little messages that are sent between processes. 
And the keyboard and the mouse sort of look like the virtual processes that are sending these fabricated 
events off. And one of the things that this means is that you can short-circuit computations. Input events 
can be caught by PostScript processes and then responded to immediately without sending the event all 
the way back to the host. It means it's very straight-forward for processes to masquerade as input devices 
or to implement input devices. So, for instance, if you want to add a tablet to your system, you can 
write a PostScript function which opens the file that your tablet is on -- if it's a serial line -- and 
starts reading from it, parses the input, and generates input events. And the system won't be able to 
tell the difference between that and the mouse. And also we added a way to sort of encode the input stream. 
It's sort of a pre-compiled form of PostScript. This was necessary in order to get the kind of performance 
that was necessary for an interactive system because parsing ASCII PostScript was a little too slow. 
But nonetheless, we allow people to intermix sort of encoded binary PostScript and ASCII PostScript, 
and both of them turn out to be very useful. So this is an application that I'd like to use as an example. 
It's an application that -- I couldn't actually find a really useful application that fit in one slide. 
But I found one that fit nicely in two slides. What this application program does is a sort of a little 
magnifying glass. It's called Fat Bits. And it tracks the mouse and gives you an enlarged image of what 
is in this rectangle around the mouse. And you can see that there's this window there with what's underneath 
the cursor. And if your eyes are really good, you can actually see the cursor in the title line -- that's 
actually the "A" from "Application". I can't spell. Anybody who used old versions of Emacs and had to 
deal with an error that occurred knows that I can't spell. So why is this a surprise? Okay. 9 Anyway. 
This application is really fairly simple. There's one PostScript function here which does nothing but 
sample the region around the mouse, and then uses the image canvas primitive to display the region around 
the cursor, which is right here, in this. And this function is getting called regularly just to draw 
this. This right here is just doing some set-up. And this right here is shading the window. This is defining 
the function that gets called to actually update this whenever the window needs to be repainted. And 
then there's this loop that handles input events. Now, one of the nice things about this is because to 
short-circuit the interaction in the server, you can actually track the mouse and update this window 
here in real time. And if we were shuffling things back and forth over the net, I think we'd have a hard 
time updating that in real time. Maybe we could. I don't know. And it makes the application really simple. 
And since it's integrated in with the window system, rather than being sort of tacked on the side, it 
means that from PostScript, we can get at all kinds of interesting window system things, and then we 
can actually create windows. Why bother writing a "C" program when it's so easy in PostScript. So, one 
question I get asked an awful lot is why did we do NEWS. And most of it was sort of a timing question. 
Initially, when we looked at PostScript, PostScript was just a natural, just the obvious thing to use 
for imaging. It's got, you know, the world's most wonderful 2-D imaging model. It's a few bugs, but what 
the hell. We talked to Adobe about it because we wanted to deal with them. But at the time -- and this 
was like three or four years ago -- they only did printers. And we wanted to be able to deal with some 
of these non-graphical interaction problems. So we can track the mouse and give very interesting cursors, 
like this arrow. And we could have the full PostScript imaging model available everywhere. So we can 
have these round windows. The menus are actually windows with little notches cut out of them. And this 
Backgammon game is particularly interesting because all the stones are windows -- they're little round 
windows. And the thing that's really magical about that is that the guy who wrote it animates the motion 
of the pieces and they just glide and they use the PostScript "curveto" primitive to create Beziers. 
They use "pathforall" to follow the Beziers and they move the pieces over these paths and they just flow 
smoothly. So I'm done. The next speaker is Leo Hourvitz of NEXT. LEO HOURVITZ: Hi. Where are my slides, 
I wonder. Contrary to a rumor that was on the slide that was just on the screen, there is no Lou Hourvitz, 
and this is me. And so to put in a little motherhood and apple pie for this panel, PostScript in your 
one- inch system is really great. And the biggest thing -- as everybody has said so far -- is that you 
get one imaging model, and that imaging model is going to work whatever you do. And also, the particular 
one has the benefit of being device independent and also I don't have to create all these great graphics 
that you've seen. Also, it comes with an interpreted extension language. And that's a really handy thing 
to have when you want to draw a grid, a great example of something where that language saves some communication. 
Or, you have some little piece in your system you want to download into the server. 10 But there are 
limits, I think, as to how much you really want to do in PostScript. PostScript is a really great language 
for describing graphics, in combination with that imaging model. That's what it was designed for. It 
does that really well. But, let's face it, PostScript is not my favorite general purpose programming 
language. (APPLAUSE) There are no real local variables in PostScript. You can simulate them with a dictionary 
convention. And there also isn't a structure that is a sort of simple and efficient way to access a bunch 
of collected variables. There's a bunch of control structures in PostScript. There are a couple of things 
that sort of come up missing when you go to write really big routines in PostScript. SWITCH is the biggest 
one of those. And if you've ever looked at PostScript code that tries to simulate that with a lot of 
if/else's, it's not a pretty sight. There's no development environment that comes with PostScript. Again, 
some of the panelists are starting to address that, because there's a need for it; not only as we try 
to write Screen PostScript, but simply to debug complicated printer drivers. But there isn't the set 
of tools that you have in whatever your favorite programming language is. And PostScript is interpreted 
only. Not only does that mean you can't compile it for efficiency, it also means that if you're a third-party 
software vendor and you want to ship something that's written in PostScript, you're shipping source, 
whether you like it or not. I can tell you that some third-party software vendors don't like that idea 
very much. An example of a real complicated application -- I call it real applications -- that people 
use and that most of the keystrokes in all of our computers go to is something on the order of, say, 
a WYSIWYG word processor, or a very complicated text editor. The one thing I'm going to touch on is what 
do you need to know in order to respond to the next user keystroke in that kind of real application. 
Well, this is a partial list of all the data structure that you have to have access to to respond to 
each keystroke. That kind of word processor redraws the entire line on every keystroke. So you need to 
know what all the characters in the paragraph are. You have to have some data structure that you're keeping 
the font sizes and style changes in. Hopefully, you have access to a word processor that'll let you do 
all those fun things. You have to know about the paragraph format; are you justifying it. What kind of 
algorithm are you using to do word breaks because you want to support non-breaking spaces. You need to 
know all the widths of the characters, etc., etc., etc. And, again, this is just a way of pointing out 
that these kind of real applications, which are where most of our keystrokes go, need to have a lot of 
data structures. And so what sort of a conclusion comes from that? Real complex applications probably 
don't want to be written entirely in PostScript. Now, you can. We just saw an example of an application 
that was written in PostScript, that has some real value to it. But do you want to do that. I don't think 
you really do. So the question that we now have to start to answer, as we hopefully all get access to 
systems that have PostScript in them, is this is a new choice that we have. It's what do we write in 
PostScript? Here is one top-level diagram of graphical programs. You have a window system where the windows 
in the graphics package, hopefully, are PostScript. You have the 11 application program that all your 
application developers are writing. And usually you have a tool kit in the middle that's providing these 
user interface objects. And it's usually implemented in a look and feel for the interface. And, in a 
multi-processor environment these days, the window system is in a server process. And the application 
is implemented in a separate process. And so you start to deal with this issue that James was talking 
about of what do you send over the connection and what do you put entirely on one side or the other. 
And the one question I'm going to spend a second on here is where do you want to put the tool kit, or 
what parts of the tool kit do you want to put where. To do that, you're going to think about well, where 
is the tool kit more tightly bound. That is, does the tool kit spend all its time interacting with the 
window system in the graphics package? Or does it spend all its time interacting with the application? 
The application code that was written. One of the things that we've been finding is that the application 
tool kit coupling for these real kinds of applications is much tighter than a simple example brings forth. 
And what I'm going to go through with you is a little bit about menus and what turned out to be true. 
Menus seem like a really good candidate to be real loosely coupled to the application. You should be 
able to define your menu hierarchy. That is, if you have a menu bar, all the things come off of that. 
Or if you have multi-level pop-ups, how they fit together. What key equivalents you have, what the names 
are, etc. And then, you hope the server can handle all of the interaction and the client doesn't have 
to get into the loop at all. That's not quite what happens. Menus are more bound up in the application 
than that kind of system suggests. Again, in these applications that are trying to be very full functionality, 
one thing is that there are always a few menus that are dynamically built -- whether it's a font list 
or a font size list, or a list of the documents you've opened. But that's not a killer. There aren't 
that many of those. The one that really gets to you is when people try to build modeless editors of various 
kinds. One of the techniques they use is they enable and disable the menu items based on the state of 
the application. And if you use an application like MacDraw on the Macintosh, it's actually very complex; 
there's a lot of states that the application is in and lots of menu items are enabled or disabled when 
you do that. And what we found, working with these real application developers, is they expect the menu 
package to be remembering all that, and every time the user does something that changes the application 
state, they go through and enable or disable 20 or 30 menu items. And you say, oh, don't do that. But 
they say, well, I want you to keep track of that. So, what can you do. Either you have to keep track 
of that on the client side and on the server side, or you can try and implement your menus entirely on 
the client side. And I don't think it's a closed issue as to how we want to implement these tool kits. 
But I'm just warning you that applications will actually do a lot of stuff, and it's not easy to separate 
them from these tool kits that they're using a lot. So my conlusions? The PostScript intepreter and the 
PostScript graphics model is a step in the fight direction. Being able to download PostScript code when 
you decide it's an appropriate thing to do is a step forward. But, it isn't clear that applications or 
tool kits really want to be written in the PostScript programming language. Thanks. 12 The next speaker 
is Scott McGregor from DEC. SCOTT MCGREGOR: I'm waiting for my slides. I'd like to pick on the comment 
that Leo made in his conclusion there, that what people really want is a PostScript in their windowing 
system. We've also found that people want a windowing system that is a standard in the industry, and 
they're really looking for something that gives them the portability. So, at Digital, what we've tried 
to do is to integrate the Display PostScript wonderfulness with the X-windowing system which seems to 
be becoming an industry standard, rm going to give you sort of a technical insight into what we've been 
doing to address this and some of the problems we face and why this isn't something that, well, you just 
do and it's real easy. The window system environment is a little different than the printer environment 
which PostScript was originally designed for. For example, the windowing system is interactive, meaning 
that applications are used to putting up a lot of information to the screen in incremental ways, rather 
than doing something like completing an entire page image and writing it out to the printer. Also, a 
windowing system has many jobs at one time. There are many windows, all interactively putting up their 
contents, all doing it at the same time. So you need to design your system a little differently to do 
that. Resources are an issue in a windowing system. You need to share wherever you can because of memory 
size and so forth. So we wanted to look to find a way to share dictionaries and fonts. Resolution differences 
are another program. PostScript is originally designed to work at 300 DPI, or generally high resolution. 
And the font algorithms work especially well at that resolution. Workstations are still lagging -- as 
Chuck pointed out. The printer environment right now, they tend to be 75 dots per inch or 100 dots per 
inch. And you tend to do things a little differently and it affects your algorithms. And the last thing 
is that windows are dynamic. Think of it as a PostScript printer where every time you feed a sheet of 
paper, it's a different size and shape. So you have to be prepared for that kind of thing. Now, at Digital 
we've taken two approaches to this problem, providing two different kinds of solutions. The first one 
is what we call a client-side previewer, and what we're trying to do here is to interpret PostScript 
files on the client side -- if you're familiar with X, that's the side where the application runs. And 
we'd like to generate the bits into a bitmap and ship the bitmap down. Now, this seems sort of clunky 
at first, but it has a couple of advantages. It's aimed at previewing documents. You may have now -- 
say you have a 400 page PostScript document, and you've made a change on page 358 and you quickly want 
to just see, did you get it right. And we want to address the needs of somebody who wants quick turn-around 
to examine a PostScript document. The advantage of using a client-side previewer is it works on any X 
server. It doesn't have to have any special PostScript extensions or anything like that. Any X server 
is capable of 13 displaying a bitmap image, so if you create it on the client side it'll work on, say, 
a PC or a Macintosh or up into the higher end workstations. And the third reason I should point out, 
this is a very easy thing to implement and we think we can get it done very quickly. Here's what the 
previewer looks like inside. At the top there's a shell that sort of controls all these interfaces. And 
it controls things like menus and the file selection that lets the users pick what they want to preview. 
And then you see in the green there, there's a scroll window, and the scroll window has several components: 
the scroll bars which let you move around in, say, a large PostScript document that might be bigger than, 
say, the page. And this image window in the middle. And that's where the actual work is done. That communicates 
with the Display PostScript kernel which you see at the bottom there in purple. And the Display PostScript 
kernel is responsible for interpreting the PostScript and talking to a device interface. And the device 
interface that we've hooked up here is a simple bitmap. So it goes out, writes a bitmap, when it's done 
with a page, it then outputs that page, using the X primitives down to the server. Now, the other solution 
that we're looking at, in terms of a different kind of product, is what we call the X-server extension. 
And this is what we think is the prime candidate for using PostScript. We want to be able to, in the 
server, have all of the PostScript execution facilities, so you can do all of the nice interactive PostScript, 
where if you execute a PostScript operator, you see it immediately. We want to provide this in a fashion 
that allows you to intermix X and PostScript. We think there are a lot of advantages of X, a lot of the 
things are very cleanly designed. We want to have the advantage of that and, at the same time, we want 
to have all of the advantages of the PostScript features. So by doing it this way, we allow you to inermix 
-- in other words, mix and match PostScript commands with X commands. So, for example, you might be doing 
something that's very efficient in X, and then you decide there's something like scaled and rotated text 
that PostScript does very well. You switch into PostScript, just output it, and you're there. We also 
think it's really important to have interactive graphics performance. If it takes a long time to do your 
PostScript, well, it's no good. The way we're doing this is to follow the X extension model. And we've 
done a couple of things and made a few assumptions here. First of all, we're assuming that X and PostScript 
will image to the same drawable. Now drawable is X-speak for either a window or a bitmap that you can 
output to. And we're doing this by having all of the output commands have what we call a triple here 
that specifies the state necessary to do that. It includes the PostScript context. It includes the drawable, 
in other words, the window or bitmap you want at your destination, and the GC, which is X- speak for 
all of the graphic state of X. And the X GC contains the clipping information, for example, the window 
clipping or various client clip paths that the application may have set up. We also feel that an extension 
has to have access to the file system. That is important, especially for PostScript which has a lot of 
file operators. And we also want to take advantage of the X font paths. X has a nice notion for specifying 
how you get at fonts and how to load different fonts. We'd like to make this extension take advantage 
of that. Here's an architectural picture of what we're doing. If you've seen X before, you'll recognize 
this split between the client and the server. In the server side at the bottom, you see 14 there's the 
large part of the X server there, but imbedded within it is something very similar to what you saw in 
the path slide of the previewer. We have the Display PostScript kernel, it talks to a device output package 
down at the bottom there that actually talks to the device itself to put the bits on the screen, and 
it interacts with the operating system, as well. On the client side, we have -- for those familiar with 
X -- the X side interface and all of the tool kit libraries that are layered on top. And we also have 
-- in sort of that reddish shade -- the PS library which lets you get at the PostScript functions. Some 
of the problems you run into in doing something like this are that most of the X servers out there today 
are single-threaded. And you really want, with PostScript, the notion of co-routines; you want to be 
able to do multiple things at a time. A PostScript program can run for an arbitrary length of time, and 
so you really don't want to be stuck while you're doing that. So that gets you into some issues of scheduling: 
when new X commands come in, should you stop the PostScript thing and let those run. Should you wait 
for the PostScript thing to finish. The atomicity concern is when you need to be able to know that a 
PostScript operation has completed. For example, you want to run a complicated PostScript program to 
put something on the screen, and then you want to read the bits back when you're done. In order to do 
that, you need to know when it's done. Color maps is an interesting problem. Most of the workstations 
to do color out there have color maps, and the PostScript model is sort of this ideal of RGB color. And 
that's really great on a 24-plane device. But if you have an eight-plane device, somebody's got to decide 
who owns the color map: does the application get it? ff you're running PostScript, does it get the entire 
color map? And we've tried to work out some things there. And the last thing is that PostScript has this 
model that at any time, it can output text, either because of errors or various other things. And we 
need to find a way to do this. Sort of in a standard "out" mechanism. Some of the solutions we've done 
for this are that we have the notion of a single thread for a lot of the X things, but we use co-routines 
for the PostScript. So, each X connection that somebody opens up -- in other words, any X connection 
that uses PostScript -- operates as a separate co-routine. We support yielding in the scheduler, meaning 
that when the PostScript code is running, it chooses convenient points at which to yield, and we can 
then run other things at the same time. So, the PostScript commands run in yield and the X commands run 
to completion, as required by the X protocol. This solves our atomicity problem a little bit in that 
we've preserved the X model of run-to-completion for X commands. And we have some primitives in there 
to find out when PostScript operations have completed so that they can wait on that, if it's necessary. 
We implement the standard "out" kind of mechanism on top of X events. We pass them back from the server, 
back to the client, and let them deal with them. Also, with color maps, our model is that the client 
really owns this problem and we're thinking along the lines of when the application starts this up, they 
should decide what kind of space they want to allocate to color maps. And we're going to let X do a lot 
of the arbitration on color maps. By the way, if you haven't looked at X in great detail, the color map 
stuff is particularly well done. 15 Well, I promised I wouldn't say too much about marketing, and this 
slide verges on it a little bit. But let me talk about some of the performance we've seen on this. We 
have the PostScript previewer up and running and we've demoed it at a number of trade shows and so forth. 
In general, it takes about a second between every page. So if you want to look at a document, by the 
time you've clicked the next page, it's about a second until it shows up. Now, I'm going to make a little 
advertisement here that you should be using the Adobe structuring conventions because if you don't, and 
you want to see page 356, you're going to wait 356 seconds while it carefully processes every page in 
between. But if you use the structuring comments, then it can jump immediately to that page. The server 
extension we find runs quite fast. We've benchmarked it against some of our X native primitives. It's 
not quite as fast as X. It's almost as fast, though. And that's very encouraging. And it's definitely 
interactive performance, and we're also encouraged by the results using the Adobe Display PostScript 
package. We found it to be visually on the screen as fast as any other implementation we've seen. So 
that's my talk, and I'm going to turn it back to Chuck, and let you have at us here. CHARLES GESCHKE: 
Well, let me first thank all of my co-panelists for, providing the kind of information to you that ought 
to make for some stimulating questions from the audience. And that is the next part of our program now. 
We invite you to ask questions. There are microphones placed around the room. I'm going to ask you to 
use the microphones, and please, one question per person until everyone's had their chance at us. So 
I'll begin here. Q: Hello, I'm Robin Schaufler of Sun Microsystems. I have a question that, I guess, 
is most directed towards Scott McGregor about the PostScript extension to X. Actually, he answered part 
of my question, so thank you very much for that. The first question is that PostScript is defined in 
terms of a byte stream, but X is defined in terms of larger chunks, in the form of requests, with headers 
that you then have to intersperse in the course of your PostScript byte stream. And so the first part 
of my question, then: is that still PostScript if it's interspersed with all these funny X header chunks? 
And then the second part of my question is: how have you integrated the X graphics context with the PostScript 
graphics context? For example, if I change an attribute using the X change graphics context request, 
will that then apply to later PostScript codes that I send down? SCOTI" MC GREGOR: I'm glad you asked 
that question. To say that PostScript is interrupted somehow by X or something like that is really crazy. 
If you look at PostScript, it has spaces between the operators. And is it in any way breaking up PostScript 
and not PostScript because it has those spaces. There's really no problem at all breaking up PostScript 
into its logical operators and passing them down as X requests; that works very naturally. The other 
questions you asked is how do you separate, or how do you merge the context between PostScript and X, 
and that's something we've worked a lot with Adobe on. And we're really happy with the answer. We have 
the ability to -- as I said -- intermix PostScript and X commands to the same window, unlike NEWS, where 
you have different models. And we do that by merging the clip. And it's effectively a composite clip 
between PostScript and X 16 and it's cached so there's high performance on that. So the application really 
sees a good blending between those. CHARLES GESCHKE: Name and affiliation please. Q: My name is Mike 
Russell and I'm from PIXAR. I guess what I'd like to do is sort of admonish you not to have this conflict. 
I don't see any incompatibilities here. I see NeWS opening an extra door to allow interaction in PostScript, 
if you'd like to use it. If you want to stick it on the appfication, fine. I see Display PostScript as 
having some additional, much- needed enhancements to the display of PostScript. Let's add those to NEWS. 
And, furthermore, X and NeWS can run side by side. So to admonish you, let's not have a miniature war 
here over this. There's plenty of room for different ideas. And we'll all be the winners from it. CHARLES 
GESCHKE: Well, thank you. Even though that wasn't a question, let me try and answer it. Because I think 
the point is well taken. And certainly, in the remarks I made at the beginning, I had no intention and 
I hope that I did not, indicate that from Adobe's side there was, in fact, any serious tension. Clearly, 
all of us are in the marketplace to try and make our companies successful. And so, from the point of 
view of how we market and bring our technology to the marketplace, we are, at that level, competitive. 
But in terms of the philosophy of what each of the companies are doing, there's a great deal more agreement 
than disgreement on most remarks. And certainly we had the good taste to try very hard to have James 
Gosling come to Adobe before he decided to go to Sun. So there's certainly no problem on our side with 
that, and I'm sure not on James' as well. Q: My name is Hyam Zemir. I'm with Cricket Software. My question 
about PostScript -- I think it's apparent to me that you didn't intend to do Display PostScript a few 
years ago. And I'm just wondering, don't you have -- everyone here seems to think it's so wonderful -- 
but don't you have any sort of questions about its compatibility with a display driven language, given 
its heritage and the intention of trying to maintain compatibility with printing PostScript. And what 
about maintaining the compatibility with PostScript for printing? CHARLES GESCHKE: Okay. I guess that 
question ends up, in effect, getting addressed to me, although I'll ask any of the other panelists who 
want to add a remark to it as well. It actually is a misconception that we did not want to do Display 
PostScript, even though I'm certain that's the impression James had. Actually, from the very beginning, 
and our very first licensees, many of them actively pursued the feasiblity of PostScript for displays. 
Any of you who have been involved in trying to actually build a company, recognize that you have to be 
very careful to do what you can do in order to build that company successfully. And so, it was for that 
reason, that we chose not to attempt to come to market with Display PostScript any sooner than we did. 
In terms of how we developed the PostScript technology, it was always in the back of our mind. In order 
to develop and extend something, I think you have to work with it for a long time. We were not prepared 
to invest the time at that point to examine the kind of extensions that we wanted to add to the language 
for displays and so forth. Q: I'm Mike Harrison from U.C. Berkley. There is, in fact, another PostScript 
interpreter that some of my students have done, and we're going to be distributing that one pretty soon, 
essentially for free. There will be a nominal charge to research institutions. Now, 17 I was trying 
to find out from the panelists' presentations just how many different PostScript interpreters there are 
around. And if I understood everyone correctly, there are really just the Display PostScript that Adobe 
has and the NeWS that SUN has. And the rest of you are developing extensions, modifications, perhaps 
improvements on that. Is that correct? Or is, in fact, DEC doing a full-fledged PostScript interpreter 
of their own? A: Actually there are a number of PostScript clones, kind of -- I don't think Chuck wants 
to say this, but I'll say it. There are a number of PostScript clone companies out there that are trying 
to make their livelihood on the basis of doing a PostScript implementation. So Adobe and Sun don't exactly 
have a monopoly on interpreters here. A: This is an example of a clone output. MIKE HARRISON: The question 
I really would like to ask of all of you who have PostScript interpreters is the extent to which they're 
complete implementations of the language. What's there, what's missing? I know a few things that are 
missing from NEWS. Certainly there's outline fonts, there's set screens that transfer. Perhaps a few 
other things. There are certainly additions. What about the Display PostScript? Does that have everything 
in it that's in PostSCript ? CHARLES GESCHKE: Let me speak for just a moment on behalf of both NeXT and 
DEC who are licensees of ours. Their implementation is our implementation in terms of the interpreter 
and the graphics imaging model. So in that case, they are identical. Obviously Sun invested, principally 
through the efforts of James, in doing their own implementation of NEWS, and it's well-acknowledged where 
they are with repect to additions and deletions to the language. In terms of the clone vendors, it's 
all over the map. But for the people represented here by the five of us, you are correct, there are really 
only two different competing implementations at the moment. Q: Hello. My name is Ron Jones. I'm from 
Colossal Graphics. We're a service company hoping to survive off of PostScript standards. And I wanted 
to talk to the whole panel about applications for PostScript whereby when I run an Adobe Illustrator, 
it seems to be pretty clear that it adheres to the standard. But when I try to run, like, FreeHand or 
anybody else, they bomb in different applications. I also have NeWS and different other boards. James, 
if you could hold up that poster again. We make large format PostScript. We want Adobe to also address 
large format output because, currently, people say that there's no market for large format output, but 
we did this in less than 15 minutes. And to have some things that you could actually blow up from PostScript 
and get it out in a reasonable fashion, we hope to become similar to what Kodak is to the film market, 
to PostScript. We want to be the output source. My question is, the standards. How are all of you going 
to convince the applications companies to have them maintain the standard, because one application file 
will not run on all different PostScript. A: Are you talking about PostScript from Adobe, or someone 
else? Q: Well, application programs like, say, file formats. For instance, Adobe Illustrator runs perfectly 
because it's written by Adobe to output to PostScript. But we were trying to get more of the application 
companies who say they run Adobe PostScript to output to a laser, but input to a file format that we 
can take and put out. Because we found a lot of programs won't run, even on a laser sometimes.  18 
A: Well, I'm not certain I understand your question, but let me try to answer the question I think you're 
asking. I'm sure it's the case that there are a number of people who have produced application software 
packages -- whether they're outputting to PostScript or anything else -- that have bugs in their software. 
So I know you're not asking about that. If you're asking about applications that come from a variety 
of vendors, all of which should work on all of the PostScript products that we've produced -- certainly 
we stand behind our PostScript products. If you're asking whether applications will run on competitors' 
implementations of PostScript, you've got to go to the competitor and ask them. I'm not certain I've 
answered your question yet. RON JONES: Okay. I'll rephrase the question. This is addressed to James and 
to Mark. Have you gone to the other competitors -- the other application writers -- to, say, make their 
PostScript output files compatible with NeWS previewer? A: We haven't gone to application writers to 
do this, but we're certainly committed to having our PostScript be as compatible as is humanly possible. 
The initial incompatibilities were sort of a side effect of the fact that we were trying to generate 
something that worked with the screen, and we're fixing all of those things, like the lack of outline 
fonts and the lack of half- tone screening. As far as getting application programmers to sort of toe 
the line, peer pressure is about the only thing that we can use. Most of the failures that we've noticed 
-- for instance, in running our PostScript previewer -- are the fact that a number of people don't follow 
the Adobe document structuring conventions, and our previewer uses them to sort of pick apart the document. 
CHARLES GESCHKE: Anyone else want to respond? MARK CALLOW: Yes. We would certainly also like to see the 
PostScript interpreter in use and the Display PostScript be as compatible as possible. And we'll cooperate 
with Sun to help that happen. A: I'd like to add one thing: there's a company whose name I forget, that 
is setting up a PostScript validation suite for people who implement PostScript interpreters. And we're 
in the process of attempting to pass the validation suite. And I would like to see everyone who produces 
a PostScript interpreter, and there are at least two dozen that I've heard of -- I'd like all of them 
to pass. MARK CALLOW: And another problem is that the Red Book is not really a language specification. 
There are some conflicts and areas in there that are unclear. And I think led to some of the early problems. 
: One other thing you referred to that I'll just mention if you're not aware of it, is that there really 
is a file format for spooling out PostScript jobs. That's the document structuring conventions people 
have been referring to. And that is all publicly documented, and it may happen to be that the Adobe Illustrator 
is the only application well behaved enough to implement it. But it's not something that's unpublished. 
Q: Alan Wachs from Xerox. This is perhaps a loaded question aimed at James Gosling. You mentioned earlier 
about you like the imaging model of PostScript, but you were pre- compiling some stuff for performance. 
And I'm wondering, did you consider Interpress in the past, before picking PostScript? JAMES GOSLING: 
At the time, the Interpress spec was locked in a vault and not available for public view. That's changed, 
but it's too late. 19 (LAUGHTER - APPLAUSE) Q: I'm Mike Shantz from Sun Microsystems. And I'm interested 
in possible work that may be ongoing or being thought of in extending PostScript to 3-D. Could the panelists 
say something about what interest or work is being done in extended PostScript to handle 3-D? MARK CALLOW: 
Well, as a company that makes it's main business in 3-D, we naturally have some interest in it. Lately, 
I have been getting a greater desire to have a 3-D PostScript because I've been coming up with all kinds 
of ideas for basically creating what I call a volume manager rather than a window manager. And I'm starting 
to come up with ideas that really cry out for a 3-D window system. So either a 3-D PostScript, or doing 
it more directly using our GL, would be the way for us to do that. As far as actual work on 3-D PostScript, 
I know some work is taking place within Sun Microsystems. I'm not sure how far that's progressed. James 
could tell you more details on that. JAMES GOSLING: Yes. Mike's question was sort of a loaded one because, 
I think two years ago, Mike wrote a proposal for a set of 3-D extensions to PostScript. And they tried 
to be very true to the 2-D model. And I think they would have worked out fairly well. But X11 and PEX 
and all of that sort of exploded, and it was clear that it was time to follow the flow rather than try 
to buck it. So we sort of let it slide, which may have been a mistake, it's hard to tell. But trying 
to stay with the sort of PostScript community is what sort of stopped that. Q: Hi, I'm Nancy Null from 
SRI. We work a lot with scanned images, scanned colored images. And we, use both Sun's and HP's eight 
plane devices. I understand that PostScript doesn't have a full color image model. Using some of these 
window systems that use PostScript, are there any constraints? MARK CALLOW: The PostScript that you were 
looking at before runs a full 24 bit RGB color model, or HSB, according to the PostScript spec. And Adobe 
in Display PostScript has added a CNY-K color model. Quite why it happened in that order, I'm not sure, 
because the CNY-K is clearly more suited to printing than to screens. CHARLES GESCHKE: Actually, to correct 
that slightly, we support three color models. And so I'm not sure what your comment means. There's certainly 
been a color model, and there are color printing products with Adobe PostScript. NANCY NULL: Mostly I'm 
referring to the problems with going from a 24 bit scanned image to eight bit monitor and color maps 
and that sort of thing. Shared color maps and those problems. JAMES GOSLING: In NEWS, we have a set of 
routines that take a -- if you create a 24 bit image, and you just use the image primitive, and you give 
it a depth of 24, it assumes that it's an RGB image -- we added that as sort of an extension to the graphics 
model, because we needed to deal with 24 bit images. And if you do that, it scales and dithers your image 
to the static color map that's loaded. Unfortunately, this tends to not be fast enough to be terrifically 
useful if you've got a large image. Q: Chuck Mishano from Harris Corporation. Leo brought it up, and 
I think there was a general consensus that PostScript isn't exactly everybody's favorite programming 
environment. All deference to HP calculator users and FORTH programmers, I've been an in-fix kind of 
guy since I was about five. I was wondering -- I guess this is to the panel in general -- what 20 availability 
there is for higher level programming models for PostScript? In particular, compilers for higher level 
languages that would compile down to PostScript virtual machines. And also a better -- or any sort of 
debugger, interaction debugger? JAMES GOSLING: Okay, I'd like to answer that one. First of all, I'd like 
to start off by saying I agreed with Leo's talk, 100%. PostScript is probably one of the worst programming 
languages I have ever seen. (LAUGHTER) Writing large applications in PostScript is a serious mistake. 
PostScript should be viewed as a spice and not a staple. A lot of people, unfortunately, have been sort 
of seduced into writing sort of large applications in PostScript. I think because, in some sense, we 
did almost too good a job in NEWS. Namely, we added garbage collection and classes. And, between the 
two of them, people find that a lot easier to deal with than, say, C. But as far as other languages to 
do development in, there's a company called Unipress, which has a product that is a C compiler, which 
generates PostScript code. There have been several people -- including Schumberger in Palo Alto -- who 
have written Lisp compilers that generate PostScript code. In fact, the Lisp system is particularly interesting 
because the way that that works is you write a bunch of Lisp code, and you say, defun this, defun that, 
defun something else. But in your file, if you say defun Pdefun, you write something that looks just 
like any other piece of Lisp, only when you invoke it on the Lisp side, the compiled PostScript code 
gets sent over and executed on the NeWS side. As far as debuggers go, there is one in NEWS. It's woefully 
inadequate. But at leastl you can breakpoint things, catch errors, examine variables and contexts. : 
Let me follow up on James' comment a little bit there. I think we all agree on the panel here that PostScript 
isn't the universal application implementation language and part of the process of housebreaking PostScript 
for applications use is putting these language bindings in place -- either for C or Lisp or other languages. 
: Yes. And you will see a lot of that activity going on in products that are under development. I guess 
having been responsible for at least part of the beginning of this "programming language," perhaps if 
I had my life to live over, I would call it a communication language. Because that really was the design 
of it. It was not designed for doing large-scale programming. So I completely agree with the other panelists 
on that topic. Its intention is to give you some of the flexibility of a programming language, but really 
for communicating between two processes. : One last note on that: anybody interested in debugging PostScript 
-- not that it runs on any of the products from people on this panel -- but there's a program called 
LaserTalk that runs on a Macintosh. That's also a PostScript debugger. A: Thank you, I meant to mention 
that, too. Q: My name's Roy LeBan. I'm from the Ashton-Tate Mac Division. I have my own list of pet extensions 
I'd like to see in PostScript. I'm sure everyone has a few -- things like data structures. Basically 
the question is, is there going to be non company-specific standards committee, as in ACM, that isn't, 
say, totally run by Adobe, where the community as a whole can direct the future of the language? 21 A: 
I would love to see it. You want to volunteer to run it? A: There are actually activities going on -- 
not under the rubrick of PostScript, per se, but in the ISO and ANSI committees on the page description 
language front. BUCK BUCHANAN: Yes, if I may comment on that. The committee is called X3V1, and if anybody 
wants information on it, they can give me a business card and I can put you in contact with the chair 
of that committee. My name is Buck Buchanan and, by the way, I'm also the chair of a PostScript users' 
group meeting immediately following this session. It's just over there in room 305. Q: Hi, Ed Post. Quark, 
Inc. The question is I guess to Charles and Scott. Both X11 and NeWS are implementation independent interfaces 
to graphics. The Display PostScript is being sold to various workstation manufacturers, each of which 
creates a new interface to -- I presume to -- the underlying PostScript printers. Is there any attempt 
by Adobe and by the implementers to define a uniform ... interface to PostScript. SCOTF MCGREGOR: One 
of the things is pushing on Adobe to work with the X- consortium. In fact, we got them to join the X-consortium 
and we'd like to see an open group that defines, on the PS lot, on the client's side. We think that's 
an excellent idea, and I'm sure that Adobe thinks that's a good idea, too. Now, in terms of implementation, 
since Adobe has based their company's livelihood on selling that implementation, I don't think they're 
going to turn over the sources for free in the near future. But there are a lot of clone manufacturers, 
and as we heard today, there are some university efforts that are producing some PostScript interpreters. 
So I think if we standardize on the interface, then we can let people get their implementation from anywhere 
they like. CHARLES GESCHKE: Yes, we're certainly in a position to want to encourage that kind of standardization. 
Having a commitment to actually support as many products as we do makes us want to be quite cautious 
before we standardize on something until we've had a chance to at least gain additional experience before 
we put it out there. Design by committee, in my estimation, never works. Q: My name is Ron Manoff, from 
the University of Western Ontario. My question is for Scott McGregor about his previewer that runs under 
X11. You made the point that it could run with any X server. Given the fact that X servers are not obliged 
to support backing X-rects or big (?) gravity, each time you have the window obscured and then re-exposed, 
you have to re-ship the bitmap. Can you make some comments on what you've done to optimize that? For 
instance, if part of the window is exposed, you can get away with just repainting those parts that are 
exposed, but it's not easy to find out from the X-server what parts of a window are, in fact, exposed. 
SCO'ITF MCGREGOR: Actually, the X-server does you all kinds of favors and tells you precisely the pixels 
that are exposed. And we do take advantage of that and only transfer those bits. Now, let me follow up 
on your question a little more: there are some other clever things you might do to try and compress the 
bits that go down. From a practical point of view, we've found that really isn't necessary, that the 
performance seems to be pretty zippy in our 22 prototypes so far. So we haven't been motivated to spend 
a lot of time on that. But we certainly could do that to help out in the future. JIM BASS: My name is 
Jim Bass, and I'm from Apple Computer. I'm a little bit amused that this is a screen PostScript panel 
and no one's really addressed certain issues and limitations of PostScript when it comes to dealing with 
things interactively on the screen such as, there's no xor mode in black and white, and the limitations 
in color PostScript of no transfer modes other than copy mode. And so I'd really like to address this 
to anyone on the panel who'd like to talk about any extensions they've made, or plan to make, to try 
to really support applications -- trying to use screen PostScript. JAMES GOSLING: One of the things that 
we've had to face was the fact that while the PostScript imaging model is really fairly clean, there 
are all these things that people have gotten used to wanting to be able to do. And the demand for these 
was really terrific. So we ended up actually having to add these to the PostScript imaging mode, and 
they're actually the only thing that we've added. Namely, we added a primitive call, "setrasteropcode", 
which does the obvious thing. In the new version, we have primitives for setting plane masks and loading 
color maps and doing all of these ugly things that you shouldn't want to do anyway. JIM BASS: Unless 
you're trying to do apps or color mixing and transfers. How about anyone else, because my understanding 
is that Adobe is completely opposed to adding new transfer modes and, in fact, that's one of the things 
that makes NeWS not accepted by Adobe. SCOTI" MCGREGOR?: I feel kind of uncomfortable trying to go so 
far as to say pervert the PostScript model by adding a lot of very hardware-specific kinds of things 
in that. For example, xor is sort of this magical operation. And color maps are another example of that. 
And that's why we believe that the combination of X and PostScript really solves a lot of those problems 
with letting you get at hardware specific features through -- : Blend does not color map specific problem. 
When you try to overlay images to get blends and transparencies and the like, that has nothing to do 
with index. That's strictly an operation you want to do to get an end result which the model doesn't 
provide. So only NeWS is the one that's extended it so far. : So the moral of the story is, you use X 
for the unpleasant things and PostScript for the pleasant things. (LAUGHTER - APPLAUSE) MARK CALLOW: 
I'd just like to follow up on this question, especially with regard to xor. Some of the machines that 
Silicon Graphics makes -- it's not a great secret -- don't do xor's very well. And so, we have a natural 
interest in people not using this "setrasteropcode" primitive. One of the ways in which it's avoided, 
and very nicely, is through the model of the overlay canvas that NeWS supports, which basically tells 
the window system that, hey, the guy wants to do a transient thing, and so we can handle that. In fact, 
we have overlay planes, as do a lot of graphic systems that that maps very well into. And I've been more 
interested in people finding new models for doing these things, rather than continuing with some of these 
clunky things. Especially xor is a particularly sore point because it works very badly on color systems. 
23 CHARLES GESCHKE: Okay, two more questions and then if there are continuing conversations, I'm sure 
we'll be up here for a while and move off to the break-out room. MAUREEN STONE: Maureen Stone from Xerox. 
Everyone's been talking about, oh yes, we've got this full color RGB model. But, of course, RGB is not 
a full device independent color specification because the red on my monitor is not the same as the red 
on yours. Is anybody addressing this? Worried about this? Do customers not really care about the fact 
that RGB is not really device independent and that, of course, when you start going off to printers, 
it gets even more complicated. JAMES GOSLING: We're actually worrying about this a lot. It tums out that 
color calibration is basically a nightmare. We have several people, actually, at Sun who are doing nothing 
but thinking about color calibration and we're working with a couple of outside companies in coming up 
with interesting ways to solve it. There is, in particular, a proposal from Kodak to one of the ISO committees 
for a sort of a standard way to view color correction, that they're trying to push. MAUREEN STONE: I'm 
familiar with the Kodak proposal. That's one of the reasons I was asking. I was curious. Because I knew 
Kodak, and also Xerox, are working at the international level of the more precise color specification. 
JAMES GOSLING: Right. And we would love to see one of these adopted. It's a very hard problem. : We'd 
love to see it, as well. You should see the color names demo on our system. Doesn't look anything like 
the colors that it says it's supposed to be. MAUREEN STONE: Difficult thing, those names, isn't it? Chuck, 
what does Adobe think about all that? CHARLES GESCHKE: I really understand how hard of a problem it is, 
since both my grandfather and father were in the printing business, rm not holding my breath. What we 
do in products like Illustrator, is give the user control over adjusting the screen so at least his eye 
can conform to the page on which he's going to print, based on what shirt he's wearing that moming, how 
long his monitor has been on, and how bright the sunlight is that comes in his window. The problem really 
gets down to something that's more like psycho-physics. So, tough problem. We try to give control. But 
I don't know how to standardize it. Q: John Liberty from Eastman Kodak. : I guess we're going to get 
the answer right now. JOHN LIBERTY: No, something different. With all the -- I'd like to get your comments 
on how you feel -- the licensing of spline fonts is going to take place -- with all the workstations 
going out there, the companies and the OEM's and the applications. CHARLES GESCHKE: Well, I'll answer 
for what our plans are and then ask other folks to answer for theirs. As I tried to indicate in my remarks, 
we intend, of course, to support the same typeface library on displays that we've supported on printers, 
so that people who purchase any of the type from the Adobe typeface library, will be able to use them 
on all Display PostScript products that we develop. 24 MARK CALLOW: Silicon Graphics would very much 
like to see more vendors offering spline fonts in particular in a common format. And there are some movements 
in that direction. In particular, Textronics is pursuing, trying to get adopted a standard format for 
these spline fonts. It's a very tricky question because currently, with all these different vendors, 
they all have different licensing conditions and I will be having somebody looking at this in the near 
future and trying to plan for what we should do. JAMES GOSLING: This is actually something that Sun has 
been working on astonishingly hard over the last while. Unfortunately, it's sort of at a kind of a sticky 
state so I can't really say much of anything, other than that I'm happy. (LAUGHTER) CHARLES GESCHKE: 
I think on that note I want to again thank the audience. (APPLAUSE) 25  I i~i~,2:il;,,,;i,i,fdi ' 
 E~ " I J "i I , .lJ ~ I i IIi ,,,  ',"';':'"':;'","'"'".... , 'r 'i"'~',~,I ' .~ 'i I~ ~"', ....................................................... 
' ~! ~!;,~i 1'i. . . . . i~t:.'.l ' b --"J ...:~, b'll', ,.' (;, ,,~I.~ in   h}pul LWP UaeB a elngqe 
Inl@reel wllh a dlllonnry In the/Nam~p Ilold dh;llortnry onlaliltl 0 ks y for each real ovofll w~ lSl,e 
Inlerosted hi the value 18 I! an)~ll ptoeedote Iltal Iraftiflll~t! Iltat I)mltcular lypo of @vent Irllo 
QL for'In ~lld =lutftF II Info IIH| llottt'# Itll)UI qtJeUO kael)tt trantthillort code In entail eaiy 
Io mlmfJi],~' fr,ftO(tlOlll# , I,~zjy~ (Jan be iitdded fo or removed frail1 the dl~llo|'~z~y ~,t any 
tlm~ t+(~quo$t I, WP ~tl queue d+lvl~o~ wllhout corrtmq.Jnlc~)flng wllh Input LWP,  Marc Callow James 
Gosling      
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402244</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>43</pages>
		<display_no>2</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[User interface toolkits]]></title>
		<subtitle><![CDATA[present and future]]></subtitle>
		<page_from>1</page_from>
		<page_to>43</page_to>
		<doi_number>10.1145/1402242.1402244</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402244</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101756</person_id>
				<author_profile_id><![CDATA[81100013136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Myers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101757</person_id>
				<author_profile_id><![CDATA[81100273480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schulert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apollo Computer]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101758</person_id>
				<author_profile_id><![CDATA[81365592948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Smokey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wallace]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Equipment Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101759</person_id>
				<author_profile_id><![CDATA[81100510731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Owen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Densmore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sun Microsystems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101760</person_id>
				<author_profile_id><![CDATA[81100197138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldsmith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apple Computer, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 USER INTERFACE TOOLKITS: PRESENT AND FUTURE Chair: Brad A. Myers, Carnegie Mellon University Panelists: 
Andrew Schulert, Apollo Computer Smokey Wallace, Digital Equipment Corporation Owen Densmore, Sun Microsystems, 
Inc. David Goldsmith, Apple Computer, Inc. USER INTERFACE TOOLKITS: PRESENT AND FUTURE BRAD A MYERS: 
Welcome to the panel on User Interface Toolkits: Present and Future. In the last few years, there's obviously 
been an increasing emphasis on providing very good user interfaces to everybody. But, as we all know, 
they're very difficult to build. Many people thought that by providing a standard window manager and 
a standard graphics model, this would really solve the problem. This has been the motivation for X, Display 
PostScript, and similar things. But, unfortunately, this has not really proved sufficient, and we need 
more support; .more help to build good user interfaces. So one approach to this problem has been to build 
what are generally called user interface toolkits. These are collections of low-level user interface 
components. Typically, a toolkit will include things like menus, scroll bars, buttons, forms, dialogue 
boxes, fields for typing in text strings, keyboard event translation, and sometimes even entire editors. 
This is an example of a Macintosh screen showing examples of some of the different kinds of things you 
might find in a toolkit. Now, there are various names for the things that are in toolkits. Some people 
call them "widgets", some people call them "gadgets". A more formal name is "interaction techniques". 
The various panelists will probably use these different names. The toolkits are becoming more generally 
available. Most application programmers expect to have a toolkit to use. And with almost any window manager, 
you're going to find some sort of toolkit. But there still seems to be a lot of debate over the appropriate 
internal structure and the right way to build toolkits and how the application programmer should be interfacing 
to toolkits. In this talk, I'm going to talk about some general toolkit issues to provide a context for 
the rest of the panel. The first obvious question is where the toolkits fit in. They're part of what 
I'm calling the user interface development environment, by which I mean all the software that has anything 
to do with user interfaces. First, at the bottom, we have the window manager itself, which provides basic 
clipping and input de-multiplexing. On top of that is the imaging model which allows you to draw pictures. 
For those of you who were at the previous panel, Display PostScript is an obvious example of an imaging 
model. But sometimes the imaging model is just built into the window manager, and an example of that 
would be, the X-11 Graphics Package. Now, the toolkit embodies the lowest level user interface routines. 
The best known example of a tool kit is probably the Macintosh Toolbox. The window manager's own user 
interface --the way the user gives commands to the window manager --might be built using the toolkit 
on top of the toolkit. Or, it might be integrated with the window manager itself. So I've shown it above 
in the Figure, but it doesn't have to be. Now, the next layer of user interface software rve called a 
user interface management system, and I'm using this term in a loose way. I'm including any higher level 
tool that helps organize and control the kinds of things you find in toolkits. So this includes conventional 
UIMSs that do dialogue control --and a good example of this is the Open Dialogue package --but I'm also 
including application frameworks -- and an example of this is MacApp. Typically, a UIMS will have other 
software development programs -- such as font editors, icon editors, menu hierarchy editors, screen layout 
programs, etc. Now, the "look and feel" of a user interface is the way that the user interface looks 
and acts to the user. And this is entirely separate from the way that the software is organized. One 
way that you can understand this is that you can get the same look and feel from two different toolkits, 
that are implemented entirely differently. For example, Microsoft Windows Version 2, and the Presentation 
Manager, also from Microsoft, have the same look and feel. A more notorious example, perhaps, is the 
Macintosh and the original Gem user interface, both of which have the same look and feel. They don't 
any more, however. Another example, that you may have heard of, is the "OPEN LOOK" look and feel, which 
has been proposed as a kind of standard look and feel by AT&#38;T, Sun, and Xerox, and it's implemented 
both in C for SunView, and in PostScript for NEWS. So, again, you have an example of the same look and 
feel in two different software packages. And, of course, you can get different looks and feels from the 
same software package, and the obvious example of this is the X Toolkit, which was designed explicitly 
to provide this kind of flexibility. So what are some of the other issues that we can talk about today? 
Well, obviously, the first is the internal structure of the toolkit. Now, one way to organize the toolkit 
is as a library of procedures. Just provide all of the procedures that do what you want and allow the 
application programmer to call whatever is needed. Alternatively, a growing number of toolkits are what 
is called "object-oriented" and the programmer here can get slightly modified behavior by creating sub-classes 
of objects. And the panelists are going to be, probably, discussing why object oriented is a good idea 
and to what extent their toolkit conforms to that idea. Another issue is how the toolkit itself is layered. 
Many of the newer toolkits have a base layer, which is-often called an intrinsics layer, which helps 
you build the toolkit itself. It helps you build the specific widgets, or the specific interaction techniques 
on top of it. The idea of this is to help you support multiple looks and feels at the same time. The 
lower level might be called a toolkit kit, in the sense that it helps you build the toolkit itself. Examples 
of toolkits organized this way are the X toolkit called XTK, the Presentation Manager from Microsoft, 
and the NeWS toolkit. Now, traditionally, user interface software has been divided into lexical, syntactic 
and semantic layers. But this has really proved pretty unsuccessful when applied to user interface toolkits. 
Another issue is to what extent there is a need for higher level tools. It's often been the feeling of 
application programmers that just having a toolkit is not enough. This is because there may be hundreds 
of procedures there and it's not obvious which procedures you're supposed to call at which time, and 
how to get the right kind of user interface out of the toolkit. This is clearly the motivation for the 
higher level UIMS-like tools. MacApp and Open Dialogue are examples of these tools. As an example of 
how these can help, some articles have claimed that MacApp increases the efficiency of the application 
programmer by as much as a factor of five. So this is really a significant aid. A question, obviously, 
for the panelists is whether we need more of this kind of thing and why we don't have them. Now, the 
final question -- I would say the most important question -- is why are we talking about toolkits now? 
Well, there've been lots of panels and papers about UIMSs and window managers in the past, and there've 
been a lot of fights about which window manager to use, but these, in some sense have died down, since 
almost everybody's using X. But 1988 has been characterized -- for better or for worse -- as the year 
of toolkit wars, so I think we can raise the level of discussion one level on the hierarchy and talk 
about the these software tools. There is another motivation, which is, people are trying to write software 
that will work on multiple machines and multiple different systems. With the emerging standards of X 
and maybe the Microsoft Presentation Manager and other standards at different levels, people are expecting 
to write portable software. If your toolkit is not portable, then you might lose out on this feature. 
So this might be some motivation for standardizing at the toolkit level, also. Well, what toolkits are 
there around that are fighting for wide acceptance9. The first one is obviously the Macintosh Toolbox. 
This is the one everybody else's toolkit is compared to. It's organized as a conventional library of 
routines. Now, on top of the Macintosh Toolbox is MacApp, which uses an object oriented Pascal derivative 
language. The Presentation Manager from Microsoft is for PC-class machines, and it's part of their OS-2 
system. The Presentation Manager is claimed to be object-oriented but language independent, and the Microsoft 
people say that it's richer and better, of course, than the Macintosh Toolbox. It's not out yet, but 
they're expecting to have it out in November of '88. Another toolkit is called "Interface Components", 
and that's part of Apollo's Open Dialogue system. It's written for the X window manager and uses C++. 
The NeWS toolkit, by Sun, is for the NeWS window manager and it's PostScript based, it supports the OPEN 
LOOK look and feel, and it's written in an object-oriented style in PostScript. SunView 2 also supports 
the OPEN LOOK look and feel, and runs on Sun computers on X, and was written in C. Another toolkit is 
the Andrew Toolkit, built by Carnegie Mellon University's Information Technology Center on top of X and 
other window managers. It's built on top of a custom object oriented system in C, and is designed to 
support documents inside of other documents, which are called "insets". It's reported that the NeWS toolkit 
is built on some of the ideas in Andrew. Finally, we have the X toolkit, which is called XTK, which has 
been built by DEC and MIT on top of X, and supports various looks and feels. On the panel, we're privileged 
today to have representatives of most of these toolkits, and they're going to try and explain the important 
differences and the important points of each of their toolkits. I'm going to introduce all the speakers 
now, and then we'll just run through them in order. The first speaker will be Smokey Wallace, who is 
a director and co-founder of the Digital Equipment Corporation's Western Software Lab, and is a primary 
inventor of the X toolkit for the X window system. Before that, he was the manager of the team that designed 
and developed the Xerox development environment, used to create the Xerox Star. After that will be Andy 
Schulert, who was one of the two primary designers and implementers of Domain Dialogue at Apollo, and 
he was then the system architect for Open Dialogue, a follow-on product, that is designed to be both 
portable and extensible. He is now a principal software engineer, creating a new interface toolkit for 
On Technology. Then will be Owen Densmore, who is a project leader for the NeWS toolkit effort at Sun 
Microsystems. And then, finally, will be David Goldsmith, who is the manager of the Macintosh Toolbox 
Futures Group at Apple Computer, and he has also been the manager for the development of Apple's MacApp 
system. SMOKEY WALLACE: For those of you who know me or know of my work, you know that I've been in the 
toolkit business ever since I saw Dan Ingalls demonstrate Alan Kay's new Smalltalk system about a bizillion 
years ago. I have a little different twist on the talk of this panel. We talked about it being present 
and future, and I think the real issue is we should be talking about past and future. Because, with two 
notable exceptions, the toolkits we have today are basically the same stuff we've had for about ten years. 
The two significant events are that both X and NeWS implement what I call the network virtual terminal. 
That's a significant step forward in networking and networking technology. I like to say a lot that it 
ups the configurability of the network by an order of magnitude. No longer does the application you run 
have to run on your local workstation, but can, in fact, run anywhere in the network. That's a very, 
very significant thing. The other thing that I think is really important, and I'm not going to talk about 
DEC proprietary layering software on top of XTK today, because of the second, what I think is a very 
important event, is that we actually have a public domain, non-proprietary toolkit implementation. One 
of the things that's dismayed me over the last 15 years, that I've been working on this stuff, is that 
very little of it ever gets out to the masses. It tends to be locked up in vaults as was mentioned in 
the previous thing, by the Xerox Corporation or Sun Microsystems or Apollo or somebody else. And it's 
not pervasive. I think that's a major bug, and I've been spending the last couple of years of my life 
trying to rectify that. In fact, one of the things I want to do right now is -- you can see from my slide. 
What is a toolkit anyway? Brad just mentioned what I consider to be about six of the most disparate software 
packages you could possibly imagine. And they also are all called toolkits. 4 Well, I like to carry 
the analogy forward by trying to define what I think a toolkit is. In fact, the major purpose of my talk 
this morning is to answer these questions: what is a toolkit, what is the XTK toolkit, what were its 
design goals, and what does it do for you, and what does it not do for you. I like to think of toolkits 
basically as providing you an environment for building user interface abstractions and instances. It 
doesn't necessarily dictate to you what those abstractions are, but it probably should come with a sample 
set of those things just so you can get yourself off the ground. Why do I think this is important? The 
reason why I think this is important is one of the things that's happening in the computer business now 
-- particularly in the desktop -- is we're basically moving from the one-MIP paradigm, what I used to 
call the order of magnitude increase in display band-width and computing power that happened basically 
with the Alto. And you see lots and lots of pieces of hardware. And we've sort of been inching away with 
it. There's two to three-MIP machines around now. But what's happening is, in the very, very near future, 
there's going to be a massive explosion in user interface technology of the same magnitude that happened 
when the Alto happened. Ten MIPs on the desk really is another order of magnitude, and I, for one, don't 
believe that user interface technology or innovation is taking advantage of it yet, and I'm not willing 
to jump on to either Mac clones or Apollo domain clones, because I think things are really going to start 
getting exciting right away. So what's XTK? We like to think about it as being essentially the next policy-free 
layer for X. Now, policy-free in much the same sense that X uses it. That doesn't mean that we didn't 
make some decisions for you, but we certainly tried to make as few decisions as we could in trying to 
force our ideas down your throat. Another thing that we did was we did it object oriented, because I 
think anybody who takes a look at user interfaces for a while rediscovers what Alan Kay knew a long, 
long time ago: that it's basic classing and subclassing techniques are just what user interfaces are 
made of. You'll have this little widget and you'll want to make another widget that's just slightly different 
from that. Object oriented stuff just plays right into that. And, oh, by the way, you want to reuse all 
those methods and all that data from the more simple examples. The other thing is that it's written in 
C. We've taken a lot of gaff for that. When I get to my design goals I'll tell you why. And the other 
thing it does is it defines a widget. When you write an object oriented system in C, you basically have 
to define what an object is. And what we did was we defined what a widget is, and basically, a widget 
is a user interface abstraction object. One of the primary, in fact, maybe the primary design goal for 
XTK, was that we weren't going to hedge our bets and do things like what Andrew did and say, well, we'll 
be able to live on a myriad of windowing systems. We applied what we called the good-enough criteria. 
When we took a look at the MIT Athena X system, we said to ourselves, is this a panacea? We said, no. 
We said, it's good enough. It's a good representative tool kit and what we did was jump on the bandwagon 
to make it even better. 5 The other thing that was really important to us -- and as I said it before, 
we wanted to get this technology to the masses. We wanted it to be public domain and we wanted it to 
be on any platform that basically had the horsepower to pull this off. So that's part of the reason why 
we wrote it in C. The other thing you'll notice is that we don't rely on things like threads and multi-processing 
and semaphors and things that aren't in all of the various operating systems you might want to put this 
system on. The other thing is we said, we have real strong ideas about look and feel, but we didn't want 
to lock anybody into ours. We wanted them to be able to do their own. Another big thing that I said before, 
we wanted to be able promote user innovation. And I think this next bullet is the one that's really, 
really important. One of the things I want to be able to do is share my innovation and my ideas with 
other people in the community. I've built these things forever and they die in my systems. And I'd love 
to be able to throw them over the wall, give them to guys at Brown and MIT and at Apple, and have them 
be able to give me things so that I can basically share all that kind of stuff. I think that's a major 
design goal of X and I think we've pulled it off pretty well. And the last thing is that we want to be 
able to separate the form from the function. Okay, so what does XTK do for you? Primarily, it provides 
mechanism. In fact, I like to say that toolkits, in a very real sense, independent of the tools that 
are in them, are actually pure mechanism. Namely, they define the control structures, they define how 
you handle input, they define some mechanism for invoking the application, either using messages or call-backs 
or something like that. And another fundamental thing that XTK does is it tries to separate geometry 
management from the actual window tree itself. It allows that geometry management to be an arbitrated 
thing. It defines the control structure, In fact, we've hedged our bets a little on this one. We like 
the push model: "don't call us, we'll call you", but we don't force that on anybody. If you like a pull 
model, if you're basically a Fortran programmer, we'll support you and let you do that, too. It provides 
the built-in widget classes. Again, that's because of C, and basically they're the building blocks that 
get you off the ground, they provide a shell, a label widget and then a number of what we call compound 
widgets, which basically are recursive widgets. One of the things we really do encourage a lot in XTK 
is recursion. We try to discourage people from doing special purpose clipping things and all that basically 
the underlying window system does for you. Another big thing that is either a feature or a bug, depending 
upon whether you like X or not, XTK really does manage the asynchrony of the X protocol. It basically 
lets you not worry about that monkey business and it optimizes the protocol traffic. If you take a very 
naive approach to X and just fling a packet down the wire for everything you want to do, you'll find 
out very quickly that you run into some fairly serious bottlenecks, so you like to save up operations 
and try to do them. We do a lot of client side cacheing. We've basically made the time space tradeoff 
in terms of time. People don't like using interfaces that don't work well and perform interactively. 
And we've done an awful lot with code reusability versus subclassing, namely you can inherit all the 
methods of your parent. Okay, what doesn't it do for you? And what it doesn't do for you is design your 
system for you and in fact, I can tell you that there is an enormous effort inside of DEC to define our 
view of what we think the system should be, but it is all built on the public domain XTK toolkit, and 
in fact, we are now taking some of our experience with things that we've layered on 6 top of XTK and 
we're taking a look at those as fundamental facilities and we're probably going to put an awful lot of 
those back into the public domain. It does not define the user interface, even though we believe in that, 
and we actually have one and I could show it to you. It does not define the application writer's interface, 
which is very important. I think that that's where most of the serious work in user interface technology 
is today. Somebody really needs to face up to what I call a policy-free application interface. Most applications 
could care less whether a function in invoked by a menu or a command line or by some sort of a slider 
or something. All they really want to do is get notified that the user wants some action performed. The 
other thing is we don't do graphics, we use the X primitives and we don't provide widgets in the purest 
sense of the word, even though there is sample set that comes from MIT and we have another one. Okay, 
just to give you an example of the point I'm making, this is a screen that has all of the MIT widgets 
and in fact, it has the MIT mail writing tool, which is called XMH. It has the MIT clock and it has a 
couple of shell windows. It's got UWM hiding away as a window manager so there's no decoration around 
any of the windows or anything else, so this is a thing using the MIT widgets. If you were to move the 
cursor into any one of those buttons they highlight in a particular way, I wasn't able to show that. 
Now, this is basically the same sort of applications only done with what we call the DEC windows look 
and feel style guide and you'll notice that they all have the same sort of consistent look about them. 
The buttons are different. They actually have different semantics when you use them, so on the same base, 
I was able to implement two completely different sets of widgets that actually implement sort of the 
same user functionality. Now, one of the nice things about XTK and I don't have an example of that, but 
I can freely mix them, I can have an application on my screen that looks like a Mac and I can have another 
one on my screen that looks like a DEC windows thing or looks like an MIT thing. Now, you may say that 
that's not a feature, okay, and I think in most applications, for just plain old application writers 
rather than software developers and user interface freaks like myself, that's probably not a feature. 
Being able to do this for yourself or tailor this thing to have a corporate look and feel is a very, 
very important feature. Okay, and that's all I have to say. ANDREW SCHULERT: My name is Andrew Schulert. 
I work at a small company in Cambridge called On Technology. Before that I worked for about 5 years at 
Apollo Computer. I first worked on a product called DOMAIN/Dialogue, which is a user interface management 
system that runs only on Apollo computers. After we did a couple releases of that, we started to work 
on another product called Open Dialogue, which is similar to DOMAIN/Dialogue in design but is poi'table. 
It runs on other kinds of UNIX boxes and it uses the X Window System. Okay, I want to talk about 2 things 
today. I want to talk about Open Dialogue, to give you a feel for what it is and how you might use it, 
without getting into the nitty gritty details. I also want to get into the distinction between a user 
interface toolkit and a user interface management system. Those are pretty common terms that have been 
bandied about a lot the last 6 or 8 years. I think UIMS is a term that's been used by academics a lot 
and toolkit has been used by the people who are actually doing real work. But I think there actually 
is a distinction to be made and I would like to go into it a little bit. I also think that they are not 
as different in some ways as they might seem. I'll get into that at the very end. Okay, to go over Open 
Dialogue, I'm going to cover a very simple program. It's a program we call square. It's one of the simplest 
non-pathological examples that we have. You'll be thankful for that because I'm going to show you all 
the code that goes into it. It has three areas on a screen. In one window is the top area where you can 
type in a number, a second area where it displays a square of the number that you typed in, and a third 
area where you can click to exit the application. This is the development environment for someone who's 
using Open Dialogue. Look at this as being 3 rows. On the top we have all of the information that the 
application developer provides. In the middle we have the tools that are part of the development environment 
and at the bottom we have some intermediate data bases. If you start with this first database to the 
left, a primitive library, it's basically all of the building blocks that you can use in building an 
application using Open Dialogue. It's all your w!dg.e!s, all of your menus, scroll bars and fields and 
some other stuff. In many cases, the pnmmves that we supply are enough to build your application, but 
we also provide a facility so you can add your own primitives to this set. The way you do that is by 
writing fairly conventional code in C++. Given this primitive library, we need a way of taking these 
primitives and combining them to define a particular interface for a particular application That's the 
function of this box here, the translator. Open Dialogue is designed to support an open-ended set of 
interface design tools, but the only one currently shipped is a translator that takes an interface specification 
as a text file and compiles it into a compiled version of that interface definition. This all happens 
before you run your application. At run time, your application starts up, it calls a runtime library 
to initialize it and to load up the interface definition, and then normally it just passes control to 
the library. The library manages all the interaction with the user based on this interface definition, 
calling back to the application whenever necessary. I told you I was going to show you all of the code 
that goes into this. The next slide is the actual interface specification for square, but rather than 
go into that, I think I'll skip right ahead and show you the same information in a little clearer form. 
This is a schematic version of the interface definition. Open Dialogue encourages a roughly static declarative 
definition of interface. In advance, you define a set of objects that work together to manage the flow 
of information between the user and the application. Every object is of a particular class, we have an 
object-oriented design here, and every object has certain values that it takes in as input and values 
that it generates as output. The way you interconnect these things defines your particular user interface. 
It's useful to divide objects into 3 different categories: on the left are what we call interaction objects. 
These are objects that interact directly with the user to acquire commands and data and to display results. 
So for square, we have 3 of these of interest: we have the field that the user types into, a label that 
displays the result and a button that you can use to say you want to exit the application. On the other 
side, you have application objects. Application objects manage the flow of control and data between Open 
Dialogue and the application. There are two kinds of application objects: There are callback objects 
that, when triggered, collect information from the interface that the user has provided, pass it back 
to the application by calling an application subroutine, take any output parameters from that subroutine, 
and inject them back into the interface. The second kind is return objects. I said that normally the 
application starts off in control and passes control to Open Dialogue by making an event-wait call. When 
a return is triggered, it causes a return back from that event-wait call. In the middle we have what 
we call transformer objects. These are not things that you normally see in a toolkit. The point here 
is that interaction objects are acquiring data from the user in the format that is most convenient to 
the user. The application objects are acquiring data from the application in the form that is most convenient 
to the application. Those two forms don't always match up. It's the function of the transformer objects 
to map between those two requirements. So, in this example, there's one callback routine, a routine that 
actually computes the square of a number, but it expects a number and then it returns a number. On the 
other hand, the field provides a string and the label expects a string. So to map between them, we introduce 
an object that takes a string, converts it into a number, and passes it to the callback. Then, when the 
callback gets the result back, it gives it to this object which converts the number to a string and gives 
it to the label. Transformer objects are not always necessary. When the button is selected, it simply 
tells the return to return. There is no data or transformation required. Okay, this is the development 
environment. What we just covered was the interface specification. When you take that and feed it to 
the translator, you end up with a compiled version of the interface definition. You also end up with 
a header file that contains useful constants and type declarations for your application. At run time, 
we have application code. Most Open Dialogue applications have two parts: One is a main routine that 
starts up in control, initializes Open Dialogue, identifies the interface definition and passes control 
to Open Dialogue. The second part of the application usually contains all of the callbacks, all of the 
application subroutines that are called automatically by Open Dialogue. This is the main routine for 
square. It could be the main routine for a lot of different Open Dialogue applications. It's very simple, 
basically it's four lines that load up the interface definition, wait for an event, and, when you come 
back from the event-wait, since we know that the only way we can get there is if the user asks to terminate 
the application by selecting the exit button, we immediately call the dig_terminate routine to clean 
up and exit gracefully. Then we exit the application. That just leaves the part of the application that 
does the real work. For square, it's very simple. It's one subroutine. It takes a number, computes its 
square, and returns it. This demonstrates one of our design goals. For a large class of applications, 
not only do you want to isolate the application from the particular user interface so you can access 
the same code from various different interaction styles, but also, in many cases, the application doesn't 
even care if there's a user out there. The application wants to provide a facility and whether it's a 
user accessing that facility, or whether it's some other application that's calling it, it doesn't care. 
If we had a more complex application than square, say, a calculator, the application callback might be 
a standard math library that has no conception it is being called by Open Dialogue. It could, in fact, 
just as easily be being called by some other application. Okay, why is this a UIMS and not a toolkit? 
This is a diagram that I did not write. I took it from a paper that Peter Tanner and Bill Buxton wrote 
about 3 or 4 years ago. It's a very good paper that draws out the different components of a UIMS. It's 
not surprising that looks similar to the slide I showed earlier for Open Dialogue. You start with a module 
builder, a way of adding new primitives to a system. In Open Dialogue that's C++. That gives you a set 
of primitives you can use to build an interface. These primitives are glued together using a glue system. 
In Open Dialogue's case, that's the translator that compiles the textual interface definition. That gives 
you a user interface definition that can be used at runtime by a runtime support package. There is one 
other component here: a logger for logging user events to a log file that can be subsequently analyzed 
by an analysis package. We don't have that in Open Dialogue. The main difference between this and most 
toolkits is that most toolkits provide the module builder and the module library, but they're missing 
a comprehensive external user interface definition. They don't have the glue system and they don't have 
the external user interface definition. So what you have with a toolkit is something more like this. 
This might be for instance, in X, the code for a new widget, this is your widget library and, for runtime 
support, this is your XTK library. What have you lost by that? What are the advantages of an external 
interface definition? Well, there are a couple of them. If you have your interface completely defined 
separate from the application then changes to the interface have much less of an effect on the application 
than when your application is defining the interface itself. It also allows specialized tools for defining 
the interface. Rather than defining interface and putting it together at run time using a programming 
language, you can use other tools. That's particularly important if you want to have non-programmers 
designing the interface. It also simplifies your prototyping of interfaces because you can define a complete 
interface without having an application behind it. But of course, things are not black and white. Things 
come in shades of gray, and in fact, if you look at most toolkits, you'll find that they have some form 
of external user interface definition. They're generally not comprehensive, but they do have facilities 
for defining a component of the interface and loading it up at run time, in such a way that you can change 
how that component looks without affecting your application. So, for instance, in the Macintosh toolkit, 
you have resources that can be separately defined and used. In NEWS, using dictionariesas classes allows 
you to define components that can be loaded in on the fly, changing your interface without affecting 
your application. And with X, there's talk, I don't know whether they've pursued this to the point of 
a product, talk of "pickling" widgets where you can define a widget, save it away and load it up again 
later. That would be a useful hook for building a UIMS. But also, Open Dialogue, unlike DOMAIN/Dialogue 
can be used as either a toolkit or a UIMS. When we designed DOMAIN/Dialogue we were very religious. We 
tried to prevent the application from defining the interface at run time and from accessing interaction 
components. We relaxed a bit with Open Dialogue, we allow you to define objects at run time. That means 
that you can have an application that starts up with an interface definition and augments it at run time 
with additional objects. You can also have an application that starts up with no interface definition 
at all and defines it all entirely at run time. The obvious question is why did we do that? If we think 
that a UIMS is the way to go, why did we put toolkit facilities into Open Dialogue? The answer is not 
that we don't think that a UIMS in general is the way to go, that it can't handle all the facilities 
that you need for encapsulating an interface separate from an application. What we did find, though, 
is that providing a complete set of facilities, for modifying and defining an interface in response to 
an application in such a way that the interface is independent of the application, puts a lot of demands 
on the UIMS. For instance, with Open Dialogue, while one can imagine creating new objects in response 
to application results, none of our standard set of components will do that and defining those components 
would be difficult. So, basically, we're covering ourselves. We put in a trap door so that we can stop 
telling customers that they can't do what they want to do. You get to a point where it's easier to allow 
them to do it the wrong way than to convince them do it right and that's what we did. Thank you. The 
next speaker is Owen Densmore. OWEN DENSMORE: Well, let's see, I'm here to bring you koala bears, PostScript, 
zooming windows, lite, a toolkit named after a beer, great movie lines, and lite II, a toolkit kit. I'm 
Owen Densmore, and I'd like to introduce you to some new ideas we're working on in the next generation 
of lite, introduce some controversy into this staid discussion and show why we don't need no stinking 
toolkits. Lite I, the first news toolkit, surprised everyone including us, by being written entirely 
in server resident PostScript. It was well-received because of its simplicity, hence the name lite, and 
flexibility. One year ago, however, lite development was de-emphasized due to the X/News merge effort. 
At that time a small group of NeWS fanatics began a two-month period of intensive, and I might say, clandestine 
design reviews and code readings of lite I. The result was increased distrust of traditional toolkit 
styles and a conviction that a more holistic approach was necessary. We call this notion a toolkit kit. 
These ideas are currently being tested by our first client, the News Development Environment, sometimes 
called NDE. The design group first asked, why did toolkits fail? Toolkits fail because they present a 
static protocol to the window platform. This generally imposes a lock between the client and the window 
platform, denying client access to certain window system capabilities. Toolkits fail because their higher 
layers invalidate lower layers. The macho ISV application writer typically needs to access the lowest 
levels of the toolkit while the casual flower child programmer wants access at the highest level. These 
layers seldom interact well. That's an understatement if I've ever said one. Toolkits fail because they 
are language dependent, disallowing Lisp, Smalltalk and C to all share the same toolkit. Our community 
of users include these languages and more, even Fortran. UNIX toolkits fail because they foresake UNIX 
roots. They're large, monolithic and feel separate from the UNIX model. Well, next, we ask who likes 
their toolkit? Users of interpretive environments like Lisp and Smalltalk love not only the intrpreter 
but the environment, the lightweight processes, programmable events and the like. They appreciate the 
integration, but would like a more open system. Mac users appreciate Mac's minimalism, the minimalist 
approach, because it lets them do everything they need to do. On the other hand, it's hard to do simple 
things. Mac minimalism was a major inspiration for lite I. The Mac ROM is available to every program 
running on the Mac. We feel this is extremely important, and our analog to the ROM for lite is server 
residency. MacApp and lite fixed the negative side of minimalism with classes. Lastly, your UNIX gurus 
love the simplicity of the UNiX model. Then we asked, how do we avoid these failures? First, we replace 
the static protocol with a language which provides complete access to the window system's capabilities. 
This removes the lock between the client and the system. Although the language is PostScript, the interpreter 
itself plays as large a role as the graphics model. Next, we use inheritance based classes rather than 
layered modules. This lets the casual flower child programmer use the leaves of the hierarchy while permitting 
the macho ISV deep access via subclassing. Our use of PostScript as the server resident interface language 
annoys everyone equally, therefore, making us language independent. We try to remain true to our UNIX 
roots by using an interpreted shell, strong interconnectibility with programmable events, being small 
and fight, and keeping simple things simple. So what is a toolkit kit? Well, a toolkit kit has a window 
shell, for one thing, which is to the window platform what the Bourne shell is to the UNIX kernel. This 
is a very important idea for us. PostScript provides both the window graphics model and the shell's interpreter. 
NeWS adds canvases, programmable events, lightweight processes, and other window system elements beyond 
Postscript. Next, the toolkit provides a classing system. We find "super" provides a more flexible hook. 
We were surprised to find that classing also solved the defaults problem. The toolkit kit provides a 
small class hierarchy which implements intrinsics as abstract super classes. This allows applications 
to be independent of look and feel by having the intrinsics choose a default implementation. This hierarchy 
is the toolkit element of the toolkit kit. Why isn't lite a toolkit? Mainly, because it was designed 
to be a server resident kit for building other toolkits. Several really interesting toolkits have been 
made from lite I. We saw one earlier from Silicon Graphics. For example, there is a Lisp group which 
actually compiles Lisp into PostScript. They call this LispScnpt. It's hard to pronounce. Another interesting 
example is the X/News external window manager which, even though it's an X window manager, it's built 
entirely using lite We put lite to non-window usage, too. For example, we can execute NeWS using a portion 
of lite as a standard UNIX filter. One such filter converts PostScript on stdin to a raster file on stdout. 
Another filter actually greps for graphical constructs in PostScript files, coloring circles green, for 
example. Lite uses classes to provide defaulting. Defaults are class variables, which are overridden 
by promoting them to instance variables. Lite notifies a user profile whenever a class is created so 
that the user can change defaults. Instrinsics can create instances of their default implementation. 
The rest of the talk looks at examples of lite II's unusual capabilities. Potentially, long operations 
in NeWS fork very lightweight processes. Because these operations are asynchronous, the system response 
is high. We show you examples here. The first shows interrupting complex redrawing. The second shows 
simultaneous redrawing. In the first example, we initiate a long operation, billions of lines. Here it 
is redrawing. Then we change our mind. Not an unusual thing in computer science. Note, few toolkits allow 
clients to simultaneously paint and pop up a menu. You don't see that very often. Finally, the menu action 
kills the paint process that was running, forking another to replace it. The next example shows three 
such programs redrawing simultaneously. Each one performs a small part of its painting, then pauses. 
At any time, the user can move, resize, or interact in any way with the windows. Here they are completing. 
The next example is the window shell script snap that made these slides. The program is highly functional. 
It reads and writes screen dumps. It can dump the screen or a window or a region. It can catch animated 
sequences and do much more. It fits on two slides and here they are. I hope you got all that. PostScript 
scale and rotate transforms are a powerful system programming tool. For example, lite has a single horizontal 
scroll bar that is rotated for a vertical scroll bar. The next sequence shows how lite satisfies the 
OPEN LOOK scaling requirement using PostScript. The example uses a scroll bar, a little iconic representation 
of it down there, and a simple script --here's the code we're running -- to arbitrarily scale an application 
without its knowing it, even. The application is not involved in any of these pictures. Here's the initial 
application. Here it is a little smaller than you might want to make it shrink. Now we make it a little 
bit larger, and larger, and larger. PostScript is really great. The application is really not involved 
at all in this. At the risk of being sued, the Mac demo example illustrates many facets of lite. It shows 
intrinsic classes allowing applications to be independent of look and feel. As mentioned earlier, that's 
a good thing to be able to do. The application sends "new" default to the intrinsic class rather than 
"new" to a specific class. Mac demo easily implemented a Mac-like look and feel that uses PostScript's 
polymorphism for nontext menus. And it uses Postscript's flexible encoding vector to provide international 
character sets, which I wish the rest of us would do. Here we see a simple scrolling text viewer here, 
and in particular, that's the paragraph we're going to look at. It's a simple 256 character paragraph, 
and we ask for the font to change its encoding. Now, notice that the application is also unaware of the 
bar style menu. There's a bar style menu here that's similar to one that you get sued for nowadays. We 
see that the change in the character set takes place here and we get the standard ISO Latin I so-called 
character encoding. You can replace it with another one if you prefer that. But at least make it international. 
Now we use PostScript procedures rather than strings in the menu. Now if you'll look at the menu down 
here, we have kind of a grayrect menu. All that gives to the menu system is little rectangles as procedures 
that color themselves the appropriate color. And then finally, it grays itself. Our last example is use 
of NeWS programmable events. Windows --there's a window --listens for events that have embedded procedures. 
Here's an event being built and there's a little procedure there and it's going to catch it and then 
the procedure is going to execute inside the window. This is used, for example, to repaint all the windows 
instead of using some sort of -- well, anyway, that's the way we repaint our windows. We also use it 
for even a more important thing, to bounce windows in case you'd like a pinball machine made out of your 
windows. It was even used once to recover a lost source file for a running program. I walked into my 
roommate's office and he was very upset because he just lost a file. He was trying to recover a program. 
His source was lost but was still running. The procedure that we sent the window simply printed out the 
userdict for the application. So this little piece of code saved him three days work. Here we send a 
procedure to all opened windows that forks a process inside the window. This process repeatedly moves 
the window a small amount. We're about to collide there. Notice when they collide, the painter's lightweight 
process starts it repainting, although it's still moving. Because we're using lightweight processes for 
everything here, there is sort of a simultaneity. Everything is equally badly slow in another sense, 
but at least it works. There we get past it. And there it starts repainting itself some more. Summary. 
Toolkits are only a part of the problem. The toolkit kit notion includes an interpreter with lightweight 
processes, events, etc., class orientation, and intrinsic classes, server-resident user interface ROM, 
multi-lingual access, and a window shell. We feel we're getting back to our UNIX roots. We feel rather 
good about it. Finally, the current status is we completed our low level core PostScript programming, 
we're starting on the OPEN LOOK core and completed about half of that. We're working with a preliminary 
Andrew-like or Andrew-based system. It's still fun. And we still don't need no stinking toolkit. Our 
next speaker is David Goldsmith. DAVID GOLDSMITH: The first thing I would like to do is to thank Jim 
Banister and the entire SIGGRAPH slide crew. After my slides were messed up on the way here, they started 
this morning at nine o'clock and created me an entirely new set of slides, and I'd like to thank them 
very much for doing that on such short notice. I'm David Goldsmith from Apple Computer, and the title 
of my talk is, "How to be object oriented and not even know it." I'll be talking a little bit about Apple's 
user interface toolkits. ! I think user interface toolkits are good, with a capital G. Why are they good? 
Well, first of all, they encourage the developer to have one user interface, which was awfully hard sometimes 
in the early days of Macintosh, but it's a little easier today. They also promote a common user interface 
between applications, which makes life easier for the end user. And they also let the developer concentrate 
on the function of their application, which is the reason they're writing the application in the first 
place instead of going out and having a good time. What are the toolkits that Apple has worked on? The 
first was the Lisa toolkit. RIP in this case does not stand for raster image processor, and we won't 
be talking too much about the Lisa toolkit today. The next toolkit that Apple did was the Macintosh toolbox, 
which is fairy well-known. And the most recent one we've worked on is MacApp, which is in some ways an 
heir to the Lisa toolkit. Well, the Macintosh toolbox works by providing a number of standard user interface 
elements which are what give Mac applications their characteristic appearance. And some examples of these 
managers include the window manager, the menu manager, the control manager, the dialogue manager, and 
so forth and so on. The Macintosh Toolbox actually has more managers than a typical department of the 
U.S. government which is a p.retty apt analogy because finding out who's responsible for what can be 
a really surreal experience. How does the Toolbox work? Windows, menus and controls, among others are 
abstractions provided by the Toolbox and those abstractions are made concrete by something called the 
Definition Procedure, or a DefProc which is an implementation of a particular kind of window, menu or 
control. And Apple supplies standard DefProcs to implement the standard variety of windows and so forth. 
Well, this may sound sort of familiar and the reason it's familiar is because it's really object-oriented 
even though the Toolbox doesn't know it's object-oriented. Windows, menus and controls are abstract classes 
and DefProcs provide concrete implementations of those 14 classes. And in fact, some developers, when 
writing their own DefProcs actually turn around and call the DefProcs provided by Apple -- which is a 
crude form of inheritance. Well, there are some problems with the Macintosh Toolbox. One problem is that 
there are a lot of overlapping functions between the various managers. A lot of common concepts, but 
that commonality is not made explicit via inheritance. There's no widget class to tie all those different 
kinds of user interface techniques together. As was mentioned earlier, it's really hard to write a simple 
application because there's no structure provided for putting one together. The developer has to tie 
all these elements together himself. It's difficult to extend: although DefProcs allow you to extend 
the user interface, they're hard to write and not many developers bother. And also, a lot of the standard 
behavior in a Macintosh application falls to the developer to implement. That's something that requires 
going through lots and lots of documentation and even relying a little bit on folklore. And finally, 
there's not a lot of support in the Macintosh Toolbox for interactive user interface techniques like 
dragging icons with the mouse. The Toolbox's philosophy is: "Here's your mouse click. Have a nice day." 
Well, the solution to some of these problems, we feel, is MacApp. MacApp is an object- oriented application 
framework which is used to develop Macintosh applications. It's a library of classes which implement 
the standard features of the Macintosh application -- the way the application starts up, the way it deals 
with menus, windows, documents and so forth. It's written in object Pascal, which is a derivative of 
Pascal which was developed at Apple in conjunction with Nicklaus Wirth, although there is a C++ version 
coming soon. Basically, what it is is a working generic application. It's a faceless application that 
follows the user interface guidefines but doesn't do anything useful, which the developer then customizes. 
One advantage of using our user interface framework is that it follows our guidelines and therefore, 
runs under MultiFinder, A/UX (our UNIX derivative) and lots of other things. MacApp does a lot of other 
things. I'll only mention a few of them here. It manages the application and dispatches events throughout 
it. It takes care of the standard window management features. It takes care of moving and resizing windows. 
It automatically handles scrolfing, which is a big help to developers. It'll handle scrolling both via 
scroll bars and automatically when dragging things with the mouse. And it also has frameworks for undo 
and redo of commands and for mouse tracking for user interface interaction techniques. And it has a lot 
more stuff which I don't have time to go into right now. What the developer has to do in order to use 
MacApp is define some objects or subclass some standard objects to implement the specific behavior of 
their application. They have to be able to draw their document for display on the screen or the printer. 
They have to be able to read and write their documents' files on demand. They have to perform the application-specific 
commands which are unique to their application and they have to handle mouse clicks. The basic MacApp 
philosophy is: "Don't call us. We'll call you." The architecture of MacApp consists of a fairly large 
set of objects, but the primary ones are the ones that handle the major facets of the application. There's 
a TApplication object which is responsible for managing the application and for keeping track of the 
open documents. There's a document object which handles the document state and reading and writing files 
and implementing the standard commands on the Macintosh file menu. There's a window object which handles 
the standard window management problems. 15 There's a view object which is the fundamental unit of display 
in MacApp and is similar to a canvas in NEWS. There's a command object which is the encapsulation of 
mouse-tracking behavior and undo and redo of commands. Plus, there's a lot of other stuff for handling 
scrolling, dialogues, text-editing and so on. There's some things that MacApp doesn't do. It doesn't 
replace the entire Toolbox. Developers still use QuickDraw and the file system directly. You can't extend 
the Toolbox using object-oriented techniques. MacApp doesn't make a provision for defining new kinds 
of windows or menus using objects. So what kind of things can we do in the future to make life better 
for Macintosh developers? One thing we can do is replace some Toolbox features which don't work as well 
today as they did when they first came out, in terms of servicing the needs of applications. For example, 
MacApp 2.0 has its own dialogue system and it doesn't use the Dialogue Manager in the Macintosh ROM at 
all. Another thing we can do is use MacApp to extend some of the features in the Mac Toolbox. MacApp 
2.0 provides for 32 bit coordinates when drawing in a view. QuickDraw, the native Mac Graphics Systems 
only supports 16 bit coordinates. And another thing we can do is provide wrappers for toolbox features. 
For example, although I said on the last slide that you couldn't use objects to implement new kinds of 
menus and windows, some people have built wrappers for definition procedures so that you can do just 
that -- although those aren't part of MacApp yet. So what I conclude here is that object-oriented programming 
is great for user interface toolkits whether the toolkits know they're object-oriented or not because 
of several features. Late binding of object-oriented programming lets the toolkit and the application 
call each other and the toolkit doesn't have to know specifically what the application is doing. It just 
needs to know the abstract classes that it's dealing with, not the specific implementations. Object-oriented 
programming also lets the objects of the application inherit standard behavior from the abstract objects 
in the toolkit. Finally, the data-driven object model fits the event-driven user interface model very 
nicely. To finish up, I will conclude with a somewhat mangled line borrowed from "The Graduate," which 
is: "I have just one word for you: objects." Thank you. BRAD A. MYERS: We'll open the floor for questions. 
You have to use the mikes and you have to state your name and affiliation. JOCK MACKINLAY: Hello. I'm 
Jock Mackinlay. I'm from Xerox PARC and my major concern is designing effective user interfaces. Your 
panel is entitled, "Toolkits: Present and Future." The present toolkits are very good for building user 
interfaces. They let you plug widgets together and build stuff up, but they don't really help with defining 
effective interfaces. What do you think the future is going to hold for that? OWEN DENSMORE: That's interesting. 
The first time we exposed the PostScript classing system, it was at a graphics conference at the Monterey 
workshop and during the conference there were these wonderful demonstrations being done on Silicon Graphics 
machines, with fish swimming around and with wonde~ul 3-dimensional joy sticks. The point I was trying 
to make about object-oriented prograrnmmg, is you'll never get it right, ever. So at least provide a 
structure where you can attempt to do it. 16 So, for example, don't limit people to the kind of user 
interface behavior they can have, but use the classing system to just difference what you want to do 
from what's already there. Oh, by the way, the real way to get the great user interface is let the artist 
do it. We're doing too much engineering. I work with an artist engineer (APPLAUSE) who one day came into 
work and said: "I got so tired of our user interface the other day." And I said, "Oh, God." You know? 
JOCK MACKINLAY: A couple of comments. Have you talked to artists? Have you talked to users? Have you 
studied users? Have you asked what they need for their toolkits? OWEN DENSMORE: Well, I come from Apple 
and before Sun, in the early days of both the Lisa and Macintosh, we witnessed people crying because 
it was so hard to use the system. And yes, I have a lot of compassion for users. I think they deserve 
a lot of compassion. SMOKEY WALLACE: Whether you have artists or not do it, you certainly don't want 
to have programmers do it. OWEN DENSMORE: Absolutely. (APPLAUSE) JOHN LEVINSON: I'm John Levinson from 
WANG. Pardon my ignorance, but I was wondering ff David Goldsmith could say how this all relates to HyperCard. 
DAVID GOLDSMITH: Well, HyperCard constitutes a sort of different direction for investigations into the 
user interface. It was done somewhat separately from our work on the toolbox and MacApp. It is an object-oriented 
system of a sort, although it doesn't fit the strict definition and it has a lot of interesting elements 
of the kind that Owen was discussing for NeWS in terms of using an interpreted language which allows 
a little bit more malleability in putting applications together. HyperCard is very interesting from the 
point of view of those of us working in the toolbox area and I think a lot of the ideas will eventually 
find their way into the more mainstream toolbox. OWEN DENSMORE: Well, actually there's a project being 
defined, who's name is PostCards and I won't say anything more about it. JERRY SHARF: My name is Jerry 
Sharf. I'm with Sterling Software. In the differentiation between UIMSs versus toolkits, do people on 
the panel want to comment on where they see UIMSs going versus toolkits, the construction technique for 
user interfaces. SMOKEY WALLACE: I made a comment on that earlier that I think is very important. You 
need a UIMS because it's the way you manage your user interface and decide which of these abstractions 
and things you want to use. To me, the crucial issue around UIMSs is the application's interface to the 
UIMS, not necessarily which specific UIMS or which UIMS facilities that you use. To me, that's the one 
thing that's allowing application, or preventing application portability across various platforms. ANDREW 
SCHULERT: There's also been a perceived lack of flexibility provided by UIMS and that needs to be addressed. 
I think we're getting better with that, but for the time bein.g, people want to get down at lower levels 
and I think eventually UIMS will be ommpresent, but we're still evolving in that direction. OWEN DENSMORE: 
17 I think there's one really easy guarantee to make about UIMS and that is they're going to fail. We 
don't know how to do it yet. It's too hard a problem and in order to get on to things, what we're going 
to do first is provide you intrinsics, which at least makes abstract the object that you're twiddling. 
DAVID GOLDSMITH: What we're trying to do in MacApp is provide a set of tools for the application developer 
and we don't expect that every application developer is going to be able to use those tools, but we try 
to address as many of their needs as possible. We don't make as much of a distinction between UIMSs and 
toolkits at Apple. It's just a set of tools for the developer to use. BRAD MYERS: Do you object to people 
calling MacApp a "UIMS?" DAVID GOLDSMITH: Not at all. DAVE DICKEY : Dave Dickey, Jet Propulsion Laboratory. 
To get back to the question of specifying user interfaces, it's easy for an artist to draw a representative 
example of what something's going to look like, but it's not easy for artists to determine all the potential 
states that a screen can be in and what the end result of a user action or event is going to be. I'm 
interested in knowing if you have any specific, formal mechanisms you use for specifying the user interface. 
SMOKEY WALLACE: Artists aren't stupid. I mean, a lot of them that I know are damned good computer users 
and they understand about states and things like that. It's just that they come from a background that 
isn't all this monkey business as us computer professionals have. They tend to be much more people and 
aesthetically oriented, which is what a lot about what user interfaces are. So yes, they need some way 
of constructing these things and trying them out and doing the formalism and everything else, but there's 
a lot of systems around. In fact, the Macintosh have probably done a lot of good things for allowing 
people to wear these things Hyper-Card does that very well. ANDREW SCHULERT: But there are generally, 
very few tools though that ... very few tools for programmers, much less an artist. DAVE DICKEY: Like 
for instance, the Macintosh is a very consistent interface. Was that designed using a finite stage transition 
diagram or something like that? Do you have any kind of tools you use for making sure that you have a 
consistent interface or is it just basically, put it together and if you notice an error, go back and 
fix it? DAVID GOLDSMITH: At this point, it's really trial and error. I think that graphical user interfaces 
are young enough and in such a high state of flux that there really isn't a formal theory for describing 
them that wouldn't be immediately made obsolete by advances in the field. So at Apple, we use the model 
where we try something, test it, see if it works or not, go back and fix it. OWEN DENSMORE: The real 
secret is to make it so bloody easy, you don't want it. Another thing is, I tried to design a state machine 
for a folder that was a teapot because here we are at SIGGRAPH, right? wanted to pour the windows out 
of it. I don't think I can do that with any state machine known to mankind. BRAD MYERS: All right. I 
guess I should mention that those of us in academia are working on this problem and my Pefidot System 
is one example where you can draw examples of user interfaces that you want to see, and the system tries 
to guess or create an actual user interface out of your drawings, but this is still very experimental. 
OWEN DENSMORE: Remember, there's only one thing you can guarantee about a heuristic and that is it will 
fail, otherwise it would be an algorithm. (APPLAUSE) DON BART: My name is Don Bart and I'm from the David 
Sarnoff Research Center in Princeton. One of our businesses is developing software to resell that we 
like to port across different platforms. One of the things we've learned from the Macintosh, is that 
it's not nice to have a corporate look that's a Microsoft look or an ALDUS look. On a Macintosh, we want 
it to look like Macintosh software. On a VAX station, we want it to look like DEC software. On a Sun 
workstation, we want it to look like Sun software. The standardization is hurting us. It's not helping 
us. Has anybody thought about an architecture to allow a portable program to require a policy? SMOKEY 
WALLACE: I'd give you a plug. I think XTK at least allows you to build widgets and have completely different 
looks and feels. The thing we beg the question on is we haven't defined the application programmer's 
interface to these things, that in some sense doesn't define how it looks. In fact, one of my pet peeves 
is when you stop ... when the application writers starts specifying look and feel, it stops being an 
application writer and starts being a user interface writer. It's real clear, you've got to decide which 
hat you've got on. Okay? I'll be real candid with you. One of the things I am starting to gear up next 
in the X community is trying to define what I called earlier, the "policy free application interface" 
that would allow you to have this portability. I don't think we know enough about the problem right now 
to make that happen. For example, if you're going to define a function in the user interface, in the 
UIMS, you have to define it in applications specific terms, not user interface specific terms. For example, 
I want to have a file name. Well, you don't want to say pop up a dialogue box to read it and let the 
user do something because I don't give a damn where the user interface management system gets it from. 
It could have it cached; it could read it from a file; it could pipe it in; it could be a VBMS shell; 
it could be a UNIX shell. All I want to do is make a request that says: I need this data. Would you please 
supply it for me? It's your business to figure out how the hell that all happens. ANDREW SCHULERT: That 
is the reason we use Open Dialogue. I mean, it does do that. It ... you can argue about the pros and 
cons of the application interface, but what we have done is we've provided a separation and an indirection 
in there that allows it to have a very wide variety of user interfaces to the same application. JOHN 
CRAWFORD: John Crawford from Microtel Pacific Research. I'd like to add my vote to the person who said 
that the application program interface is really the last, big frontier of this UIMS toolkit issue. However, 
I do have one specific question, or maybe it's more of a gripe with the existing work that's been done 
in UIMSs and toolkits. And that's ... from what I've seen, people tend to use these UIMS and toolkits 
to do things that they do well. In other words, application interfaces and user interfaces are pretty 
easy to build for things like editors. However, a real problem is, for example, real time systems where 
the events that you're watching aren't just the events generated by the user, but they're very important 
events that are generated by the system and the user has to interact with them in real time. Do any of 
you gentlemen have anything to say about how that problem can be addressed with the user interface toolkits 
of the future? OWEN DENSMORE: Well, first of all, you have to ... do you really mean "events?" In other 
words, you want them to behave sort of like events? In that case, they've got to be cast in a programmable 
event model. I think that's clearly a necessary thing. We've seen it time and time again. JOHN CRAWFORD: 
In many cases, you'd like the behavior of the user interface to vary based on events that might be happening 
out there in the real world. For example, if a very important thing happens that an operator might have 
to monitor, you want the user interface to change in some way to adapt to what's going on in the real 
world, just as it adapts to the user's preferences and actions. SMOKEY WALLACE: Let me make a comment 
about that. Inherent in any system that's useful for getting real work done, has to be the notion of 
"cheating." "Loopholes" we used to call them in the Mesa world. The point you're making, is there's a 
whole bunch of real time user interface actions that toolkits aren't going to address for a long, long 
time. In fact, not until we can abstract them to some level that we can understand what the hell they're 
doing and then provide some primitives to let you do that sort of thing --and also, by the way, make 
it behave in a timely fashion. I happen to take the view that toolkits are infinitely useable if they 
do nothing but free you from all the boiler plate and geegaw that you have to surround an appfication 
with. If you have to have a window or some set of windows that you want real-time response in, the real 
time user interface expert must be able to override the system and do whatever the hell he wants to do. 
I happen to believe that that's the way most of these kinds of things you're alluding to will happen 
in the near term, but in the future we'll understand it more, you know, and in a couple of years and 
we'll build facilities that allow mere mortals to do that kind of stuff, too. JOHN CRAWFORD: One parting 
shot. If you really want to make sure your UIMS and toolkits are good, I would suggest that you try them 
out on some hard problems for a change. (APPLAUSE) SMOKEY WALLACE: We thought we were working on the 
hard problems. JON MEADS: I'm Jon Meads. I'm with Bell Northern Research and I'm also the SIGGRAPH observer 
on X3H3 standards committee. I would like to ask the panel their opinions on standards. That was a topic 
that wasn't really addressed in any of their presentations. I have three things I would like them perhaps 
to consider in their comments back. One is the concept of policy-free. It seems to me that you can't 
really be policy-free, totally and completely. An example which I think of very heavily, is that until 
recently, you could not do double clicks in the X Window system. If you don't have the capabilities for 
such basic UI capabilities, you, by definition, are not policy-free. Secondly, the direction seems to 
be towards supporting multiple window managers. The idea is to make applications portable by being able 
to work with multiple window managers. From a user interface point of view, it seems to me patently ridiculous 
for a user to be sitting there, interfacing with three different types of window managers for each of 
the windows on his screen -- if he's using three different applications. We need to be able to interface 
with standard window manager, but how can we do that in a policy-free manner, or at least in some manner 
that doesn't make it dependent upon some specific, corporate style? The third thing to consider is the 
fact that, you know, ugliness goes clear to the bone in user interfaces. When you want to design a user 
interface to a particular thing, widgets aren't always enough. There's a thing called semantic feedback. 
A topic that was debated very heavily at the CHI Conference in Washington, D.C. earlier at a user interface 
panel. That, I think, is very important to fluid interaction. Yet toolkits and servers and window managers 
I know of, do nothing at all to enhance semantic feedback, especially the very deep levels of the user 
interface, down at the response level. So those are all issues I think 20 are very strong, important 
issues regarding standards. I'd like to have the panel comment, if they could, on that. SMOKEY WALLACE: 
I'd like to point out the context shift between this gentleman and the guy that was in the back of the 
room. We had one guy that wanted cutting edge and another guy that wants us to build the forms and pour 
concrete. Those are orthogonal goals. Okay? I agree with him about standards, but one of my comments 
I'd really like to make is about standards bodies. Standards are full of politicians and technological 
historians. Okay? (APPLAUSE) They are very good at taking well-understood things and writing them down 
so that we can all use them. If we can agree upon them, then that's wonderful, but they're not very good 
at doing cutting edge technology. So I think you had a reasonable mix. If you got the message around 
the XTK thing, I was really preaching law and order. Let's define some interfaces we can rely on over 
time. Let's make interoperability testing and verification be a high order thing, rather than new innovation, 
and oh, by the way, allow for open-ended innovation. The rest of my panel members have really been talking 
about sort of the next half step in some of this stuff. I think we're actually doing that sort of in 
concert, so I think we're trying to meet both of those goals and they're conflicting a lot of the time. 
OWEN DENSMORE: The only real standard is a good model. None of the standards organizations know this. 
This is very important. If you have a good enough model, like PostScript, which presents a model rather 
than a standard, it's very, very easy to program in it. It's very easy to predict what it will do under 
new situations. There's just a model. BRAD MYERS: I think we only have time for one more question. SCOTI" 
KIM: Scott Kim, Information Appliance. I wanted to push on the theoretical end, the discussion of user 
interface toolkits and kit-kits. It reminds me of the development of higher level programming languages 
and programming language environments --different levels of addressing the same thing, except this time, 
dealing with interactivity. One speaker already ... one questioner already raised ... or some of it's 
been discussed --about the lack of theory. Continuing to build toolkits on top of widgets, assumptions 
like menus and menus and windows without a more basic theory below that, is ... it lacks a theoretical 
base. I would like to see a more basic theory of interaction. Any thoughts? DAVID GOLDSMITH: Well, Brad, 
when's it going to be ready? At this point, that's an area of active research as I think Brad was indicating 
earlier. In the meantime, the rest of us are trying to get on as best we can. ANDREW SCHULERT: I think 
a main problem of this whole area is that there's an awful lot of data points out there that you can 
look at, but not a lot of theory that pulls all your data points together. BRAD MYERS: I think it's fairly 
well-agreed that the output area -- at least we have some models like PostScript and the models that 
come with your favorite graphic standard, but I think it's well recognized that for input and for handling 
interaction, that really, people don't have any idea what to do. Even the most sophisticated window managers 
today, like NEWS, just go back to giving you essentially, device-dependent information from the mouse. 
So, all you academics and students out there, I think this is a really great area for about 17 Ph.D theses. 
OWEN DENSMORE: I really agree that this is a serious point and I think also it has the flavor of primitiveness 
that it's possible. Also, the main reason I think it's important is that my artist friend keeps harping 
on it and I'm pretty sure he's right, too. SMOKEY WALLACE: Let me give away the fact that I'm really 
an engineer and not a computer scientist. It took me about 10 years of my career to figure that out. 
A lot of people push for the theoretical basis, but let me point out that people drive automobiles and 
fly airplanes and all sorts of things and there isn't a hell of a lot of behavioral theory around how 
they're capable of doing that. Being a pragmatic person, and also being a computer professional, I happen 
to get my kicks out of using these things. I saw what happened when Alan Kay invented Smalltalk and saw 
the 10-fold productivity increase over guy's pushing around punched cards. So I think we are being pragmatists. 
We are sort of taking these steps in an experimental fashion and there isn't a lot of theory -- but it's 
a pretty exciting endeavor. BRAD MYERS: Thank you very much. We'll probably stay up here until the AV 
people kick us out, if you want to talk some more. .... . M 0.. o A oeo oo o a, Lr t "O ~; Toolkit 
Wars / 4 ~ Brad Myers Brad Myers Smokey Wallace Smokey Wallace    , ~f , ..-b~p~ ./ ~' Owen Densmore 
   ll[,..l!JO~    
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402245</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>29</pages>
		<display_no>3</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Parallel processing for computer vision and display]]></title>
		<page_from>1</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/1402242.1402245</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402245</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101761</person_id>
				<author_profile_id><![CDATA[81100553088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rae]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Earnshaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Leeds]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101762</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101763</person_id>
				<author_profile_id><![CDATA[81100251533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Wozny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101764</person_id>
				<author_profile_id><![CDATA[81406596724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Dew]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Leeds]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101765</person_id>
				<author_profile_id><![CDATA[81100435002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tosiyasu]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Kunii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PARALLEL PROCESSING FOR COMPUTER VISION AND DISPLAY Chair: Rae A. Earnshaw, University of Leeds Panelists: 
Henry Fuchs, University of North Carolina at Chapel Hill Michael J. Wozny, Rensselaer Polytechnic Institute 
Peter M. Dew, University of Leeds Tosiyasu L. Kunii, University of Tokyo PARALLEL PROCESSING FOR COMPUTER 
VISION AND DISPLAY RAE EARNSHAW: Good morning everyone. Welcome to the wonderful world of Parallel Processing. 
Thank you very much for coming. It's my pleasure to welcome you, and also to welcome our distinguished 
panelists who I think are all pioneers --or almost pioneers. Professor Kunii from the University of Tokyo, 
Japan. Professor Henry Fuchs, from the University of North Carolina, Chapel Hill. Professor Peter Dew, 
from the University of Leeds, United Kingdom. Dr. Mike Wozny, from Rensselaer Polytechnic Institute, 
who's currently on leave at N.S.F. We've been asked by the SIGGRAPH organizers to not dominate this panel 
session with formal presentations, but to allow plenty of time for audience participation and interaction. 
I've therefore requested that each panelist should limit their opening remarks, their opening statement 
to 10 to 12 minutes, and I shall seek to enforce that. After the sequence of introductory presentations, 
I shall then invite you to comment or ask questions, and participate in the discussion. So please feel 
free to respond provocatively and ask any question you would like. Just one minor point: I think all 
the panelists are consultants with computer companies, so there may be questions to do with technical 
aspects of hardware that they may not be able to answer because of the non-disclosure agreements they 
have signed with particular vendors, so I hope you understand this. Don't worry if we seem rather distant 
-- I can't see any of you because of the lights. And that's a common problem with panels, they appear 
so distant. But we hope to try and break down the barriers and come across to you. When we get to the 
question sessions, I'd be pleased if each of the questioners or anyone wanting to make a comment, would 
identify themselves by name and affiliation. This helps the audience but, more importantly, it's for 
the written record of this particular panel session. Just a few words by way of introduction -- and this 
is a little bit of an addition to the main material of the panel, and you'll see why in a minute. The 
idea for this panel proposal came from a conference that was held at the University of Leeds on the same 
title: "Parallel Processing for Computer Vision and Display". This was co- sponsored by IBM United Kingdom 
Laboratories Ltd, University of Leeds, the British Computer Society, the Computer Graphics Society, and 
Eurographics., So there is some reference material that will shortly be available as a printed publication 
and I d just like to briefly run through what that publication will contain, in case any of you are interested. 
Here are some of the invited papers that were presented at the conference. We endeavored to have a fairly 
good international representation, from the U.S.A., Japan, and from Europe. And you will see that some 
of those invited speakers are actually on the panel today. These are some of the topics that were addressed 
by some of those invited papers: Data Concurrency; 3-D Image Synthesis; Coarse Grain and Fine Grain Parellelism; 
Techniques for Real Time Image Generation; The Ray Casting Machine; And An Approach to Automatic Generation 
of Linear Systolic Array Programs. Just to continue that: Design of a Parallel Processing System for 
Computer Graphics; Micro-Electronic Associative String Process for Cost Effective Computer Vision Systems; 
Graphics Plus Vision is Equal to SIMD Plus MIMD. And a more theoretical contribution: Specification and 
Verification of Synchronous Concurrent Algorithms. And then, Visualization in Scientific Computing. So 
just to summarize this, the themes from the conference - which are the sections in the book - are as 
follows: Processors and Networks: Theory and Practice. Parallel Architectures for Vision and Image Processing. 
Parallel Algorithms for Vision and Image Processing. High- Performance Image Synthesis. And Parallel 
Processing Techniques for the Visualization of 3-D Models. It will be published by Addison Wesley in 
late '88 or early '89. The title is the same title as this panel, and the editors are Peter Dew, Rae 
Earnshaw and Tom Heywood. It's about 500 pages of material and will be priced about $40 to $50. As well 
as the invited papers I just mentioned, there were about 35 reviewed papers that were submitted and these 
are included in the volume. So much for the publication. Now let's move on to the panel this morning. 
The four panelists are going to address these particular topic areas. First of all, Professor Kunii will 
talk about computer graphics and parallel processing. Followed by Professor Henry Fuchs on high- performance 
image generation, high-performance parallel systems. Then, Professor Peter Dew on parallel processing 
for ray tracing. Ending up with Dr. Wozny on parallelism and visualization. I guess there's very little 
else I should say by way of introduction. I just put a few introductory points on this last slide, but 
I think most of the panelists will actually be covering these. I note, personally, an increasing interest 
in parallel processing, and I think what you've heard so far in this conference, both in the technical 
sessions and in the panels has emphasized this point. We note that computer graphics and image processing 
are coming together through the use of analogous techniques and hardware. And the very development of 
new architectures and new types of hardware gives us more power. This power opens up a new range of tools. 
And perhaps thoughts about user interface as well need to be considered. It's my pleasure, then, to introduce 
our first panelist, Professor Kunii, who is the director of the Kunii Laboratory of Computer Science 
at the Unviersity of Tokyo, who will speak to us on computer graphics and parallel processing. TOSIYASU 
L. KUNII: Good morning my friends. I'm the first Alice in the Wonderland of Parallel Processing, according 
to Dr. Earnshaw, speaking on parallel processing ... and I'm the only person coming from the other end 
of the Pacific Ocean. And I will explain the architectural issues for parallel processor design for computer 
graphics and the computer vision. The first thing I'll explain is the requirements for parallel process 
architectural design for computer graphics and computer vision. The second issue that I'll get into in 
more detail will be an example on the parallel processing workload distribution among the processors. 
You have many, many processors connected in certain topology. And there are some processors that are 
short of a job, the other processors have too much job imposed. So we have to distribute the workload, 
hopefully equally among the processors, avoiding communication overhead. That's a necessary thing. And 
so I'll give you an example of such workload distribution. Further topic I'm touching is in part application 
cases we really need parallel processors to be designed. Because it's very costly to have many, many 
processors instead of one to process one job. So I have to make it clear where we need such parallel 
processing, and there will be some examples that I'll show you through the real images, simulating parallel 
processing. Instead of continuing on this way, verbally, what I did was I made five minutes video tape 
for you to see what we're doing and what I propose to you. Please turn on the video tape, please. (AUDIO/VIDEO 
TAPE -- INAUDIBLE) RAE EARNSHAW: Thank you very much, Professor Kunii. The next presenter is Professor 
Henry Fuchs who will talk about high-performance parallel systems. HENRY FUCHS: Hi. I hope you don't 
mind that I took off my coat and am ready to talk to you. But if you think you're warm down there, you 
should see what it's like when you have hot lights on you. What I want to talk to you about today is 
a couple of different topics. As you see here on the contents slide, I want to make a couple of comments 
on parallelism and then I want to tell you about what we've been doing on a parallel interactive graphic 
sYstem called Pixel-Planes in Chapel Hill. I think the most gratifying development in parallel graphic 
systems is that they have finally arrived on the floor in the SIGGRAPH exhibition. If you've gone through 
the floor --and I assume most of you have --you've probably seen the AT&#38;T PIXEL machine, the Silicon 
Graphics IRIS's GT &#38; GTX, the Stellar GS 1000, and some other machines, all of which achieve near 
real-time interactive rates through parallel processing. And the parallel processing they all use is 
dividing the frame buffer in an inter-leaved way among multiple processing elements. This is a scheme 
that a number of research groups for over a decade now have worked on, and I think it's a good example 
of a development of a technology, going from initial conception through university and industrial research 
development through realization in commercial systems. As most of you know, although people realize that 
parallel systems were going to be needed for graphics, the limits of sequential systems weren't reached 
until recently by commercial systems. And most of you know, I'm sure, that those limits have to do with 
the frame buffer access times and the fact that, after about 50 nanoseconds or so, you simply can't push 
frame buffers any faster for any reasonable amount of cost. And so you need to do something in order 
to get more PIXELs into that frame buffer. And as you can see from these commercial vendors, what they 
have chosen is to divide the frame buffer among many units -- anywhere between 16 and 64 units. Well, 
that's going to reach its limits also. It's clear to see, it's 16 or 64 times whatever the frame buffer 
access is there. And so, as you've seen from the papers, there are research designs that are now being 
presented for how to get past the next wall that limits the speed of these interactive graphics systems. 
And by way of starting commentary on this, my personal belief is that what we're going to see coming 
up after the current generation of parallel graphics systems is going to be essentially smart processors 
and memories that are very closely tied together. And I think that we already see some of that in papers 
and some research systems, and I expect that, within another four or five years, we'll start to see that 
in commercial systems. What I want to tell you about then is this slide that some of you have already 
seen. This is a system that we showed at SIGGRAPH two years ago, and is based on a smart frame buffer 
memory system. In fact, this slide is a representation of that system itself. As you can see, everything 
is straight-forward here, from the workstation through the general-purpose graphic processor. What is 
unusual is that the frame buffer itself does essentially all the rendering tasks. And it does that by 
asking the graphics processor to send the description of the primitives, one at a time, to the frame 
buffer, rather than the graphic processor breaking them up doing the scan conversion and visibility calculation 
and the smooth shading. And that description is a set of linear coefficients and operation codes, and 
this format is what we use to represent every possible instruction. So a description of a triangle, for 
instance, would include some eight of these coefficient sets --A, B, C, and instructions. Three for the 
edges and another two for the "Z", actually the same value just done twice. For the "Z" plane, and three 
for the red, green, and blue planes for smooth shading. Every system, of course, has limitations and 
this current one has a limitation of about 35,000 to 40,000 smooth shaded triangles per second. And it 
does, of course, shadow and spheres and transparency and image processing and a number of other things, 
but it has its limits. And the most obvious ones, as many people have noticed, is that if you're broadcasting 
the description of a primitive to this entire frame buffer in a way that's just a higher level version 
of broadcasting an XY value and RGB value to a frame buffer, that is the single access that you have 
to that frame buffer at that particular time. And if that primitive only covers a small part of the image, 
as most primitives tend to, then you're wasting a lot of potential computational power by these frame 
buffer processing elements just sitting there and not doing anything. And so what I want to do is tell 
you, in a few minutes, what we're planning to do for the next system, which we hope to show you some 
time in the future, here at SIGGRAPH. The first thing we want to do is have a lot faster system. That 
is, it's not very exciting to try to increase by a factor of two or three and we think we should have 
a reasonable goal. And we think a factor of 20 is reasonable to shoot for. We want to continue the generality. 
That is, we don't want to put any special hardware for triangles or for spheres or for lines or anything 
else. We want to have a general purpose system as possible insofar as real-time, 3-D image generation 
is concerned. And we want to have a compact realization. That is, we want to have something -- a design 
that we can grow from a small system of a few boards to a large system with perhaps dozens of boards. 
And here's the way we're planning to do it. We're going to break up this frame buffer that we have in 
the current system into a large number of smaller buffers, each one separately controlled and each one 
that could be allocated to different parts of the screen on the fly. So something like, say, 16 boards, 
each with 128 by 128 pixels and a bunch of backing store, and then a rapid way to move the partial images 
that are calculated by these smart small frame buffers into a conventional, passive, high-resolution 
frame buffer. Then, of course, there's the part that most of the papers that are just describing research 
designs don't need to talk about. That is, if you're going to have a high-performance rendering system, 
what are you going to put in front of it to keep it fed. In fact, this is the biggest problem on our 
current system; we can't keep the thing fed. And so we don't have anything brilliant to show you here. 
In fact, the only thing that we could think of to do in terms of a general system, is to have multiple 
copies of the highest performance, floating point processor chip set that we could comfortably get a 
hold of. This is Weitek XL chip set that we're using in our current system, except we only have one of 
these graphic processors now and we expect to have dozens of them in the next system. And then we'll 
have a high-speed ring network in order to take packets from any board to any other board. So what we 
hope to show you, then, is a system which is general purpose, which is able to do a number of different 
things, in a small box that sits next to your desk and is, perhaps, 20 times faster than what you saw 
here a few years ago. I think that's all I have to say fight now, but I'll try to say more controversial 
things later. I brought along a video tape to show you a few things, and I'll show you as much as we 
have time for. Let's show two minutes worth of video tape, and if you could keep the audio down a little 
bit, then I'll try to do a voice-over on the fly. This is a two-minute demo tape that's in the video 
screening room. This is a real-time system. Everything here is as you see it. Notice the shadows going 
all over the place. This is shadow volumes. This is a hall down our current computer science building. 
The shadows were not enabled for this, as you could tell. Now you'll see that there's a cone cutting 
into this telephone. If you'll notice on the sides, the buttons also get cut, and the depressions also 
get cut. (Showing molecular models) Those are real spheres, those are not polygonal approximations. Again, 
this is real time; this is not precalculated. (Showing a 2 window, 2 person tank game) We're only showing 
one of the player boxes, because we didn't have another camera. There's another person playing with another 
joy-stick box. And two different views. Notice the fog? One of these guys is going to get killed right 
now. Do you see that? Thank you. (APPLAUSE) RAE EARNSHAW: Thank you very much, Henry. Henry has more 
on the video if we need it afterwards. The next presenter is Professor Peter Dew from the University 
of Leeds, who is going to talk about parallel algorithms for ray tracing. PETER DEW: I'd like to say 
thank you to SIGGRAPH and Rae Earnshaw for the opportunity to present this material. I've chosen, as 
my topic, to use some of the papers that were presented at the recent conference on ray tracing as a 
way of trying to share with you some of our experiences of trying to program general-purpose VLSI array 
machines. First of all, just to give you a bit of background since most people here don't know me, I 
work with the IBM UK Scientific Center, and we're also working with the Mechanical Engineers at Leeds 
on trying to bring about interactive design for solid modeling. This has been on the go for something 
like nine years, but as yet it is not really getting it out into industry. And maybe parallel processing 
is the key to that technology. And here is the obligatory slide to show you that we can do ray tracing. 
We're experimenting with a Meiko Computing Surface (transputer based computer) in much the same way as 
Professor Kunii showed. If one looks at the papers in the Paralle Processing Conference then there are 
papers that process PIXELS, which is not dissimilar from what Henry's just shown us. And that's based 
upon the SIMD technology. There's a paper by Kedem and Ellis, where he actually implements the Roth ray 
tracing algorithm for CSG objects. They use a processor for each CSG primitive, and then has special 
hardware to combine the results. To me, the most the attractive approach is one that is, essentially, 
a hybrid of that technology. So I would like to submit that one should turn everything round from the 
way that Henry was trying to show us. And one should view, first of all, the general purpose VLSI processor 
array as being the basic computing paradigm. We can add smart frame buffers and do smart things with 
memory. But that really is the basic framework. And the key word, I think, in that slide, is the word, 
flexibility. If you customize too much, you just lose flexibility. To me, the big issue now is, how do 
we program these things. What sort of software models are we going to use. And anybody who's thinking 
about this, I recommend very strongly that they look at the way that they're trying to program a big 
project in England called the Alvy Flagship Project, which is a very large project to try and bring forward 
an IKBS machine. But actually, the most interesting thing is the way in which they've built up software 
models. This is based upon three sorts of levels. First of all, an application level which is machine 
independent. And that, to me, is really important. That's where we should try and express our computations. 
There's then a level where we need to do problem decomposition and that's really where the parallel algorithm 
designers fit in. And what we need, and I think we are starting to firm up on, is abstract machines. 
We can then write compilers to target onto, and if we've got a decent application level, we ought to 
hide parallelism from the user. And that should be the goal of any researcher in this area, in my view. 
The simplest abstract machine and one that we've used and found very effective --although there are some 
bottlenecks in it -- is basically one that processes tasks. A task is just a function, just of piece 
of work that needs to be done. That is queued so that you can feed it into a parallel processor array. 
In reality, it could be a vector processor. In our case, it's a bunch of transputers, but it could be 
a bunch of any sort of basic processor. When the tasks are processed, they come back, and then you can 
generate new tasks. This is a very general paradigm. It's usually implemented in a processor Farm. The 
model seems to be first introduced by Mayod Shepherd and the INMOS people have really pioneered this 
development. This is really attractive. Basically, all your scheduling can be taken out. You don't need 
to worry about load balancing at the algorithmic level; this is done automatically. If the process is 
empty, then it sends a request. That request is then serviced by a set of input tasks streaming in here 
(see slide). The problem is obviously the input and output channels become bottlenecks because that's 
where all the tasks stream in and that's where all the tasks stream out. But there are many ways over 
those sorts of problems. And this sort of architecture that we use for organization is usually the graphics 
module that is a big bottleneck, so we stick that in the middle of the array. But there's no reason why 
you have to one graphics card; you can have several graphics cards. So to summarize that, what people 
have done and what might like to do, is to replicate the whole object model. That's great if you're a 
hardware designer, but it's a trifle expensive, because these memories get very large, as everybody knows. 
But more importantly, you lose spatial locality. The bandwidth, also, in the graphics card is a problem, 
but I think that can basically be solved by replication. Other ways of trying to deal with the large 
memories is, of course, to implement a cache. And that seems a very sensible thing to do in hardware 
rather than trying to do it in software. But that has a nice property because you can exploit spatial 
coherence automatically. So as time goes on, one ought to be able to handle the scheduling automatically, 
and maybe we can even distribute data bases automatically. This continues to hide parallelism from the 
user. I also believe that processor Farm is such a nice model, it's easy for hardware designers to overcome 
quite a lot of the problems. And there's another project that I am also connected with which is the WARP 
project at CMU, and there they essentially are designing to my mind what is the next generation of a 
transputer architecture. So that's just a few remarks on the abstract machine. I think we really need 
to get those defined so that we, as algorithm designers, can start having a firmer basis to target onto. 
If one looks at the algorithmic design, and if one wants to start to try and hide the parallelism, then 
I think the place to start is to read the work on data flow and graph reduction. Quite often, we represent 
scenes either by graphs or, in my case, CSG trees. It is quite a natural extension and, if you look in 
that body of literature, you'll find that it's quite possible to generalize a lot of these concepts. 
The only difference between data flow, as it's traditionally done, and data flow in a transputer, is 
that we just keep the granularity much coarser. And that gets over some of the communication problems 
that they have on the early data flow machines. I think that the way forward is that we need to take 
really good serial algorithms. That means that we have to handle 3-D image space, and that's what we 
have to sub-divide. Also, I don't think there is enough parallelism just in sub-dividing space, you must 
also sub-divide your models. And that means that we have to look for algorithms and there are now those 
about in the literature that do spatial parallelism and structural parallelism, and the trick is to try 
and get those two to work together, so there's always plenty of tasks in which to do the computations. 
And you can either do that, as I've shown you, in the central controller. Or you can do it with distributed 
controller. And there's just a couple of slides just to say a few remarks about that. The central controller 
is, obviously, the first one to use because it's the easiest to conceptualize, and there's a very clear 
abstract model for it. The task here could be defined as the classification of array with a sub-tree. 
Then, when you've done the sub-tree, you need to bring the task back, combine that with other sub-trees 
that you've generated the results for, and, if necessary, fire off additional tasks. Because you've also 
got spatial parallelism, you've also got several of these to do. So you can get parallelism from the 
sub-trees by splitting up the structure of the problem, and from just using spatial parallelism. And 
we've built programs that do that, and they're now working reasonably well. I think that the future, 
though, is not in the central controller. I think we need to look towards distributing that controller. 
There's very interesting work that's probably not known yet by Norman Winterbottom at the IBM UK Scientific 
Center, where essentially he's doing interactive lighting models. And there, he stores within each processor 
a history of the task that that processor needs to perform. And then, when things change, if a processor's 
got no tasks, it can steal task from another processor. And that's starting to do out the distributed 
controlling paradigm, which seems to be a really successful way to look, if we want to do editing where 
we've got a temporal dimension. And I think that area in trying to define more carefully what we mean 
by distributed controller, is an area of quite useful research in the sort of subject that we're looking 
at. Within the PPCVD conference, there was a nice paper looking at the hypercube by Caspari and Scherson. 
There they used a static scheduling algorithm, but there was no reason why one couldn't change that into 
a dynamic algorithm. So just to sum up, I think the starting point now should be, let's look at general 
purpose VLSI ~u'rays. By all means use specialized flame stores, that seems to make good sense to me. 
But the most important thing is now to build the software models and really learn how to program these 
machines. Certainly for ray tracing, I think that we have to look at the 3-D voxel spaces and learn ways 
of effectively bringing together spatial and structural parallelism. I think distributed task models 
offer a lot if we're going to move into interactive temporal design. It would be nice to have cacheing 
mechanisms, because I think that would overcome a lot of difficulties from an application programmer's 
point of view. And clearly, we do need the next generation of processors, which is going to give us faster 
access to shared resources. That I think is very important. And, lastly, though I've not actually said 
it, you do need in your algorithmic design to be able to have a switch which controls the granularity. 
I'd like to hand you over now to Mike Wozny. who's going to do the last part of this session, which is 
on visualization in scientific computing. Which brings everything together. Thank you. (APPLAUSE) MIKE 
WOZNY: Thank you, Peter. I would like to talk about applications, hopefully that will set the requirements 
for future processors. And I want to focus on scientific and engineering visualization. Because what 
we really need is visualization, interactive visualization on the desk, not something that takes a month 
or two to prepare a final version of an image that we want to observe some phenomenon. And here we're 
starting to see some trends; not much in parallel processing at this stage, but we're starting to see 
hardware accelerator boards appear in graphics systems like the Sun TAAC Board. And I'm sure we will 
see more in the future in that area. I have been at NSF for the last two years; I'm ending my two-year 
tour there at the end of this month and going back to RPI, so I feel I have to say something about the 
visualization report that NSF funded. It was not done in my division, but I've been involved in that 
and other efforts internally on visualization. The report, Visualization and Scientific Computing, has 
stirred a lot of interest in our broad-range graphics community. But it has really focused attention 
on a fairly vague and all- encompassing activity, namely, how to use computer graphics for the purpose 
of getting more insight into a problem so that we could reduce the time-to-solution, time-to-discovery, 
or whatever you want to call it. And, when I compare that to what we're looking for at NSF in terms of 
research support, we're trying to fund a fundamental base of concepts in an area. So how do you struggle 
with that vague representation where you want to build a scientific base? What we really have today is 
essentially a set of crayons where we -- in a trial and error manner -- just use them to try to get some 
better understanding of what's going on. In the last two days, I've heard the question asked several 
times: does NSF really understand the visualization report that was done. And my answer is, definitely 
yes. But you have to realize that report was done by a division that funded the supercomputer centers, 
and the goal of that division is to provide a resource for doing research. Not doing the research, but 
providing the resource for doing research. So when I hear Larry Smarr talking about the visualization 
technology, that fits that role that he's looking at. But the majority of NSF funds the research itself, 
the fundamental research. So when I look to visualization issues, I ask questions like, what is your 
hypothesis for proposing a particular research problem. What kind of methodology, or scientific methodology 
are you going to use to attack and solve the hypothesis. Something other than trial and error. If you 
say the hypothesis to improve insight, then how do you quantify that so you have a means of predicting 
beforehand whether a particular visualization approach is going to work or not. And then, if you look 
back and say, where have we really made progress in visualization, or where has visualization improved 
scientific progress. I could name two areas: one was Professor Dave Hoffman at the University of Massachusetts, 
who's a mathematician working on infinite, non-intersecting, minimal surfaces. And basically, he discovered 
the first surface of that type in over 200 years. Only three or four were known and he discovered now 
most of the whole infinite series of them. To get it right, what he needed to prove his mathematics was 
an understanding of the symmetry. He wrote his own ray tracing algorithms and developed this. Certainly 
there is a good use of visualization, but again, was it really graphics research that was pushing the 
field. He basically used the techniques that Turner Whitted developed over ten years ago in the graphics 
community, and tht's when that research was done. Boeing, in developing their 737-200 airplane, had a 
problem with putting larger engines on. They could not put the larger engines under the wing, they had 
to put them in front of the wing. And there was a problem of interacting air flow between the engine 
and the wing and they had to separate this. They knew this problem existed for over 20 years through 
wind-tunnel experimental data. But it was not until they could get on a supercomputer, using particular 
tracers, and to separate the variables, the components of the stream into pieces where they could change 
the shape of the nacelle and actually place the engine a little bit in front of the wing and make it 
reduce drag. Again, particular tracers were used in that application, where we've known this for a long 
time. So we really haven't seen too much very sophisticated graphics being used, but we've seen tremendous 
progress being made in particular scientific applications. So I think there's more to visualization than 
just looking at the images. And, basically, it goes back to things that Dave Evans has said for many, 
many years, where he basically looks at a graphic system for scientific and engineering applications 
as a modeling system. Modeling the real world. And basically, visualization tied back into modeling, 
I believe is where we start getting a higher sense of communication, a higher sense of semantics about 
the relationships that exist. And it is in the area where we have major progress to be made in the future. 
And I think this is where we'll see more and more parallelism being applied in applications. The areas 
that are starting to make some progress are areas of places like computational fluid dynamics. And I 
would just like to show you a few slides that I got from both the Computational Aero Dynamics Laboratory 
at Grumman, who wrote almost all of their own software. Dr. Mike Seclarity (??) and Mr. Mark Mandal are 
the people that were involved in this work. And also from the NASA Ames Research Group, Tom Lazinski's 
graphics group, with people that work with him, Dave Tristand and Trion Levitt. So if I could just briefly 
show you some slides, it will just give you a sense of what's been going on in this area. And again, 
this first slide, what you see is that there's wind-tunnel data and computational data. And if you look 
at this NASA slide, you could see that our numerical models are fairly good. And visualization, in this 
case, doesn't make much sense until you can really almost replicate what's happening in the experimental 
world. And in this case -- and I will second the comment that was made earlier by Rae Earnshaw - -that 
we will see image processing through experimental environments coming together more and more closely 
with graphics synthesis through visual verification, to some degree, of experimental models. Again, what 
you're looking at is pressure distributions, color maps. We've been doing this for a long time in graphics 
areas. I'll just run through some of these. And again, pressure distributions on hypersonic vehicles, 
and one can see just areas of higher pressure. But it's the modeling again, with the visualization which 
is the key aspect. Coordinate systems -- this one doesn't show up too well. But basically, what one needs, 
is slices to look at different pressure distributions. And again, ability to take slices through models 
are fairly generic operators that I would expect in the future ... to some hardware acceleration capability. 
One can see it here, again, with the Grumman slide. And also looking at pieces of it from this NASA slide. 
Very common tools that are used in computational fluid dynamics, and again, other areas where one is 
working with different types of marked cross flow and also different types of enthrapy calculations. 
So essentially, when you're working in this environment, you have many, many different variables that 
come from your dynamic equations and, somehow, your visualization system has to be able to pull those 
variables closer together and be able to take those variables and compare one to another. And again, 
the people at NASA Ames have developed interfaces with slide bars and dials to be able to take something 
like 20 different variables and plot velocity against color kinetic energy, to be able to understand 
relationships that exist. Tracers have been around again for a long time. I mentioned earlier the Grumman 
slide of one of their vehicles. And here, basically, what they did was --instead of using particles --they've 
used ribbons to get a better understanding of what's happening. And all these are fairly basic graphics 
operations, but very powerful tools for interactive modeling. And finally, when you start looking at 
the V/STOL Simulations and you get start getting ground effects, as the vehicle comes close to the ground, 
one has very, very complex dynamics and flow, and one needs tools, more than this type, to be able to 
look at problems of this type. But this is all we have so far. Sorry. I think that was my last slide. 
So that's one area. The computational fluid dynamics. If we look in other areas -- areas like continuum 
mechanics and stress analysis -- what we find is some of the things that I felt that Bob Haber said in 
his tutorial on visualization earlier this week. He started to categorize some of the visualization tools 
that we need for different types of continuum fields. Things like scalar variables, to be handled by 
contour plots or color maps. Vector fields by arrow plots and contraction (??) operators. We look at 
energy instead of velocity, for example. And the problems we start running into is when we start looking 
at higher order fields, tensor fields, like principal stress tensors on objects where it's very hard 
to visualize this. And we really don't have tools to approach that problem. 11 So I feel what we have 
to do in looking at visualization is to get back to some of the basic mathematical operations. I remember 
years ago when AI Barr was at RPI with us, we talked quite a bit to try to take our advanced calculus 
course and base the whole thing on computer graphics. To be able to get a lot of the complex divergence 
and curl opertors, and other operators that you see in mathematics, back onto a graphics display. And 
I see future (??) processing being able to attack particular classes of problems in visualization, and 
give us tools to handle them. When we look at problems, for example, of interest by Noltan (??), I saw 
problems on combustion. How do we really understand what's happening in combustion. How can we model 
it and come up with some kind of a predictive model that we can understand how to design better combustion 
capabilities. We really have no tools for that. But I got excited when I saw Alvy Ray Smith a while ago 
show a cloud where they used a lasar scanner to be able to slice the cloud in pieces and recreate the 
cloud. So, all of a sudden on one side, what I saw was something dealing with image processing taking 
real data. On the other side, we're trying to build models with our graphics synthesis processors. And, 
as I said earlier, I expect some of these things to come together as we see more and more experimental 
models being brought closer to the graphics modeling capabilities I think we'll see more work being 
done in the user interfaces. As I mentioned, the ability to take 20 to 30 variables and cross-plot them 
in any way one needs, from the NASA Ames people. I think we'll see some of those functions starting to 
be put back into a system in a more generic way. And that also means we'll start pulling in graphics 
functions, geometry functions, dynamic functions, inferencing functions -- all built in some way into 
a broader graphic display. I don't think we will start building lots of special-purpose processors for 
bringing these pieces, these functions together. But what we will need is more generic parallel capabilities 
in a way that we could program them in a flexible way to bring out different features that I just mentioned 
in those five areas. That basically concludes my remarks. In summary, what I wanted to do was just present 
for you some of the visualization application areas and try to just talk about what future requirements 
they may bring to the parallel processing environment in computer graphics. Thank you. (APPLAUSE) RAE 
EARNSHAW: Thank you very much, Mike. That concludes the formal presentations from the panelists, and 
I'd like to thank them for keeping to time, so enabling you to have plenty of time for questions and 
interaction. Could I remind you, firstly, of two things: if you wish to make a comment or ask a question, 
please give your name and affiliation. The second thing is all the sessions are being transcribed and 
being mailed out to all technical program registrants in about mid-October. So they've got machines in 
the back which take copies of all the slides and everything so, hopefully, you'll get a fully integrated 
record of everything that happens. Q: It's Charles Clark, Kinder Graph Systems, Ltd., Canada. My question 
is an open question in regards to the high costs associated with taldng a software algorithm and accelerating 
it using parallel processing. When does an algorithm reach that enlightened status where it should become 
parallel-processed or accelerated using parallel processing and when should it be left on the shelf to 
collect dust? PETER DEW: I think that the answer is that, at the moment, it's sensible to use mature 
algorithms because we need to actually understand how to do the job. But I don't see why, with general-purpose 
VLSI arrays, people won't start developing algorithms on those. So it seems a good idea to take some 
mature algorithms, learn how to do it, and then that's your programming base. You see, these things are 
nowhere near as difficult to use as the sort of general mix. Computer scientists think that parallel 
processing is hard. If you go and talk to the engineers, they don't worry about it. HENRY FUCHS: I certainly 
agree with that. I, however, place a stronger hope in people coming up with new algorithms that are particularly 
well-suited for parallel realization. So I think the answer to your question in a nutshell, is that there 
isn't one sure rule, but I think it depends on circumstance, on an individual feeling strongly enough 
about it to champion it to do it, and to then demonstrate it as such. I don't think that it is easily 
determined, fight now, if we went through the top 20 favorite algorithms in a particular category, that 
we can easily determine, fight now, which would best fit into hardware. I think it depends upon insights 
about how you're going to build the hardware. I think that one of the crucial questions is whether you 
take a general-purpose system, like, say, a transputer, and you take as your task the development of 
algorithms that will do the job on that particular existing machinery. Or you take a larger task that 
says you start with a blank piece of paper and you want to achieve a particular goal. I believe that 
there's place for both of them, and I present to you as evidence the fact that when people try doing 
real time, or near real time dynamic interaction, even on massively parallel machines, that they don't 
succeed. And the reason is that there's too many things in there keeping you from doing it. And they 
have to do, not just with computation, but a lot to do with synchronization and the communication. So, 
for instance, we all saw the Meiko Computing Surface here on the exhibit floor, last year I guess. And 
it drew images, ray traced images, sort of coming down the screen. But it wasn't moving around any kind 
of an image 20 times per second. And in relation to the visualization, with what Michael Wozny was saying, 
I believe it's very important to gain understanding, spontaneous understanding of the problem. And that 
calls for exploration. And that calls for dynamic interaction of the user with the system. And that calls 
for systems which can generate images very rapidly. Rapidly enough so you gain insight immediately. And 
that means a high number of frames a second. And it's simply a fact that for $500,000, if you go out 
and buy a Meiko Computing Surface, you won't get 20 frames per second. But for $100,00, you go out and 
buy any of the special-purpose machines -- you know, the ones that I had mentioned -- and you get ten 
to 20 frames per second on fairly complicated data structure. So I think that the reality is, if you 
want fast interaction, you cannot do it fight now with the current generation of general-purpose machines. 
Now I very much believe in general purpose, and I think that we're going toward that. And, so for instance, 
the front end of all systems, I believe, are going to stay general purpose because we don't know enough 
about what we want to do, and there's a lot of different 13 things we want to do. But as we know more 
and more about what we want to do, we encase them in hardware. So as you said, Peter, we encase the frame 
buffer in hardware because we know something about that. I think we're going to encase more and more 
of it in hardware. And we have to do that carefully, and it won't be a single decision. So some people, 
for instance, encase triangle proceessors in hardware. In a pipe. That flat box will render a triangle 
real fast. And if that's all you want to do, if you want to do triangles fast, that's the machine for 
you. If you want to do more general-purpose things, you don't want a machine which will encase triangles 
in hardware. So I think it's a beautiful time we're living in, in which we're seeing a thousand flowers 
bloom, and you can go pick the one is the prettiest to you. RAE EARNSHAW: Thanks so much, Henry. Do any 
of the other panelists wish to add anything on that point? (QUESTIONER NOT IDENTIFIED) Q: There's several 
different approaches people are taking with parallel graphics processors implementing them as scan-line 
processors or polygon processors or working with square PIXEL arrays. And I was wondering if Henry Fuchs, 
perhaps, or the other panelists, would fike to comment on the inter-relation of existing video-RAM technogy 
that encourages you to do everything by scan lines. And how that relates to the ideal architecture you'd 
want to use to implement these processors if you didn't have the problem of trying to work with the existing 
new memories technologies that are available. And if you had a free rein to implement this, any kind 
of a memory architecture you wanted, what would we probably be seeing in commercial systems today. HENRY 
FUCHS: That's a real good question. It takes more thought than I have fight now. I certainly agree with 
you that the memory technology, as well as processor technology, imposes a lot of restrictions on the 
designer of systems. And that the current V-RAM technology makes it more attractive to do scan line organizations, 
because you can do that fast, than other kinds of organizations. Off the top of my head, my guess is 
that a square region orientation would be the most balanced and least apt to give you different performance 
characteristics as you get different scenes. Because, as most of you well know, if you get a scan line 
organization and you get a system or a particular instance of frame in which you have lots of polygons 
on a scan line -- like flight simulators and they're banking and so you have the horizon cutting across 
that particular scan line -- you're going to have lots of processing on that particular scan line. Square 
PIXEL regions are slightly more forgiving. So, off the top of my head, I agree with you, I think square 
regions would be best. You could certainly imagine organizing a dual port RAM, sort of like a video-RAM, 
in which you would have access that would be on square regions rather than on scan lines. And it takes 
some more analysis before we can give a more definite answer. (QUESTIONER NOT IDENTIFIED) Q: This is 
for Dr. Wozny. Is there a reference to describing the NASA Ames visualization work in more detail? MICHAEL 
WOZNY: I haven't seen a reference on it. I basically have heard Tom Lazinski (??) speak on this and I've 
seen some of the slides. I'm sure there is some. I know some of the software, the user interfaces, that 
they've been setting up. Some of their particular tracer programs are available in the public domain. 
I know they've been using IRIS systems, and through the IRIS user group, some of that software is available. 
If you see me later, I'll give you a name that could follow up on. Q: Terry Huntsberger, University of 
South Carolina. This is, I guess, specifically for Professor Kunii. Since computer vision has been thrown 
into this panel discussion, do you think the limit on the transputers of the four (??) !/O channels will 
severely restrict the use of these ... pictures in the vision field. Mostly you've been talking about 
fairly low-level image processing type applications here. And maybe something more sophisticated, like 
a dedicated hyper cube chips, where the number of I/O channels and interconnects are much larger and 
would not restrict the speed of your algorithms. TOSIYASU KUNII: Thank you for valuable comment and question. 
I'm not specifically sticking to one architecture built into the transputers. And I can tell, by experience, 
the problems of using the transputers. The number of connections right now for a chip -- as you know 
-- are four. And they are really in need of more communication capabilities. And we are hoping to see, 
for example, the number increase to at least to eight, to have more connections in committed versions. 
And since our applications are very time-consuming, such as a ski animation using computer visions, and 
also we're also working in ... design to broaden up the area to show customers how gem stones look like 
after being cut, instead of just cut to sell one-cut stones. We can have hundreds of cuts being created. 
And we're combining computer vision to get the image. But we have to get the structural image in, too. 
So the amount of information is really incredible, and not usually thought about. Anyway, I agree tht 
we will need a better architecture in chip design. Also, one factor we have to consider to come up with 
new architectures is the compatibility of the computing, the storage to and the communication of the 
information. We can replace communications and the storage by the processors, because we tell the processors 
to re-compute, instead of sending the information out. So it's really a matter of compromise. And fight 
now, in any chips, that you see the area on the silicon for the processors are only a few percent. So 
if supposedly we increase the area of processors on a chip, then we may be able to get better chips. 
But not much research done on the subject of computer vision workload analysis. It's really a fundamental 
issue. Instead of talking about just one architecture, we have to start from an application, like you 
said, computer vision. Then we analyze the work. Certainly we need to have computer vision chip. And 
we have to start from the real work load analysis of computer vision, before getting into architecture 
or anything. RAE EARNSHAW: Peter, do you want to comment? PETER DEW: I think the answer to your question 
is that the transputer is far too slow, if you want to use the links for low-level vision. It's not really 
been designed for doing that. If you look at the Meiko Computing Surface, and they use a serial bus for 
shipping the image along, and then just use dual ported memory into the transputer. And that's quite 
a sensible hardware hike (??) to deal with in the present situation. If you look at the new chips that 
are coming along, then you'll find that it will be that IO will completely overlap with the computation. 
So you just have more buses. And then one will be able to deal with some of these problems. But it seems 
to me that, with low-level vision, it's clearly a specialized task and it probably fits into the same 
sort of situation as the sort of graphics frame buffer. It's understanding how to balance these things, 
which is what is needed. RAE EARNSHAW: Do you want to comment, Henry? HENRY FUCHS: I want to comment 
on some visualization, but I'll wait. Q: I'm Mat Nutsiman from AT&#38;T, and also I'm associated with 
Stevens Institute of Technology. One of the comments I heard was there are two approaches one could probably 
take with parallel processing: one, the solutions are software which need to be written, as well as the 
hardware, which is presently available. And is it possible, probably, to look at the software problem. 
Because, once we are stuck with the particular hardware, it's usually inflexible. So one of the approaches 
we have really taken is to look at user needs for functional languages for computer graphics to explore 
parallelism. And, in fact, we have presented some papers in Euro-Graphics and we had done some work on 
that for (??) illumination and shading. And I was wondering is anybody else doing some work on exploring 
parallelism using functional langauges, because it makes the work much simpler. And the current languages 
which we have are restricted to one (??) machines. And once you are able to express the problem in a 
functional language, it seems to me that it becomes more flexible. And finally, you can map that problem 
onto your hardware, which could probably exist (??) like a Connection Machine or any other kind of machine 
which is going to appear in future. Are there any comments from the panelists regarding the following? 
PETER DEW: First of all, when I mentioned the Flagship Project, that project is directly involved in 
mapping from functional language through graph reduction techniques, onto a data flow machine that's 
being expressly designed to support parallelism from graph reduction. So you should really look at that 
work. I can certainly give you some references. There's an ICL journal which contains several papers, 
a really nice one, which would kind of help you. It's a big task to go from a functional programming 
language and go through the the route that you've just described. I think that there's a lot of merit 
in that. I'm not sure functional programming languages are powerful enough to do what we want to do. 
That's one worry I would have. But the ideas that I was trying to present was the idea of an abstract 
machine model. think that's a really important clue towards getting portability. I think it's an absolute 
shame if we just write for one particular machine. I think we really do have to think about the portability 
issues. They're very important. Q: I certainly agree with you that functional languages cannot be (??) 
used to produce display on the images. But it could certainly be used as a prototyping mechanism to -- 
A: I personally would rather look towards object-oriented as a paradigm for this sort of application 
level. HENRY FUCHS: There's a group led by Duler (??) Maggow, who's a professor in our department in 
the University of North Carolina at Chapel Hill, tht has been working both on functional languages and 
their parallel hardware realization, for many years now. And has done extensive work showing the wide 
degree of parallelism that's available in many of these programs, expressed like that. And they also 
have a detailed design of a tree-structured machine, that can execute these languages very efficiently. 
PETER DEW: Just one other thing, with functional programming languages, they tend to own their structural 
parallelism. That is, they're just breaking down the problem and trying to do that automatically. It 
is much more difficult for those languages to be able to handle the spatial parallelism. And I think 
that's a problem that's got to be addressed if you're going to do it in the graphics world. RAE EARNSHAW: 
I just want to ask if any of the other panelists want to make a comment about functional languages. Q: 
Neil Ferguson, E Systems, Dallas. This question is addressed, primarily, to Dr. Dew. You mentioned hybrid 
approaches that have processors for both objects and processors for PIXEL level. Could you give some 
examples of tasks that would be idealized for hybrid environment, as opposed to single environment. And, 
secondly, could you address some of the advantages and compromises, in general, for hybrid environment. 
PETER DEW: I think the area where you get in is when you start trying to use space- based structures 
like octrees. When you look at that, really, there isn't enough regularity in the spatial domain to be 
able to partition the problem up. And, therefore, you're forced to go into these hybrid approaches. Anything 
else, and you really don't get efficient processor utilization. There was a second part to that question. 
RAE EARNSHAW: Can you just repeat the second part of the question. PETER DEW: Does that give you the 
answer? I think the paper, the one I mentioned by Caspari and Scherson, is a good example of that. And 
we've done some work ourselves, which I can certainly let you have, where we've combined sort of structural 
and spatial parallelism together and used a hybrid approach. Q: My name is Jeff Shaw. I'm from Vanderbilt 
University Medical Center. And I'm curious about any developments in using neural network techniques 
to do image processing tasks, such as segmentation, specifically. PETER DEW: Do you mean neural nets? 
Neural nets is a very exciting technology that seems to be very appropriate in this sort of (??) recognition 
world. I think it's still very experimental. It certainly seems to be working quite nicely in speech, 
where if you compare sort of what you might call traditional knowledge-based, frame-based approaches 
with the neural nets, then some work at RSRE in England suggest that both do about 75% correct on their 
sort of test samples. How it's going to extend into sort of vision is still very much a reasearch problem, 
because then you're talking about 2-D domain instead of a 1-D domain. MICHAEL WOZNY: I recommend that 
you look at the work of Professor Carver Mead at Cal Tech. Very interesting work there. And in both hardware 
and some software, but both analog and some digital work, specifically for vision. It's very impressive. 
PETER DEW: The other person is Jeff Finton at Toronto University, who writes about the connectionist 
machine. I think that's the best sort of theoretical work to understand what's going on. RAE EARNSHAW: 
Henry, you wanted to make some comments about visualization. HENRY FUCHS: I just want to repeat my support 
for what Michael Wozny had pointed out, that the goal of helping people with visualization should not 
be that they bring a problem that has data on tape and that they give it to a visualization committee, 
and if they feel that it's important enough, then they pass it on to a film-making committee, and the 
film-making committee works with you and two months later you get this beautiful film that you can show 
at a conference. The goal should be making systems which fit on a person's desk, and he uses it as his 
tool for his scientific work. That's the first comment. The second comment is, if any of you people want 
to work on that area to help people in visualization, because you are graphics hardware or software types, 
I highly recommend you find a collaborator or two who really needs to visualize something tough, and 
who needs to do it daily. I recommend somebody like in biochemistry or in medicine. In radiology with 
radiation therapy, like my closest colleague, who has patients coming in every day, he has to treat them 
with radiation therapy by hook or crook. And, you know, more of them are going to die if he doesn't do 
a good job than if he does a good job. He's really interested in getting better understanding Of what 
is happening with the radiation beam and how close it's getting to the spinal cord versus is it really 
covering the cancerous region, and how many RADs are getting to the spinal cord versus how many RADs 
are getting to the brain. Very complicated structural situations. I highly recommend those kinds of collaborators 
because they really, really need to do the job, and they force you to come up with better and better 
solutions all the time. In contrast, somebody who vaguely wants to do something and will come to you 
once in a while to see pretty pictures. That's my exhortation on visualization. (APPLAUSE) Q: Dick Hilmer, 
duPont Central Research. I want to second those coments because rm in exactly that kind of situation. 
I have a bunch of x-ray crystalographers and protein design people who are looking to me to provide (??), 
both in a very exhilarating and very frustrating experience. Exhilarating because they're beginning to 
discover how to do things they couldn't do before because of computer graphics. And frustrating because 
the real problem is not a computer graphics problem, it's a computer modeling problem. Your comments 
about interactivity are absolutely right. They want to sit down and turn the dial and change the parameters 
of their model. Unfortunately, their models currently run for 18 many hours on our CRAY XMP super computer. 
They want it on their desk. And so you're absolutely right about that. And I just wanted to point out 
that the parallel processing should not limit itself to the image generation aspect of it. It should 
expand itself into the actual model that you're trying to produce. : Absolutely. PETER DEW: Now, I think 
that's the exciting bit. Now we do have experimental machines, even if they don't go as fast as we would 
like, which really enables us to get into interactive modeling. And that, to me, is a break-through, 
and that's where we have to put our effort. The rest is just hard work, but the goals are quite exciting. 
RAE EARNSHAW: I'm pleased to see we're all having a heated agreement. My job with the panel is to have 
heated disagreements. Any more questions? Q: I just have one more comment. Dan Bergeron, University of 
New Hampshire. I want to make a comment on the question of what kind of languages are portability. One 
thing I think is very important: C-STAR aims at data parallelism, and its use with the connection machine. 
We've been developing a compiler for that that will generate code for hyper-cube. And have got very preliminary 
but promising results. And it's another way of looking at how do you generate more portable algorithms. 
And that one addresses data parallelism. I encourage people to look at it. PETER DEW: I completely agree 
with that. Trying to develop these compilers which enable us to move across and sort of hide out these 
machines, is a really important technological development. Otherwise, you're just going to spend so much 
time inventing algorithms for a machine that's going to be obsolete in five years time. : All right. 
If somebody develops and algorithm and it's great, and it's for a machine that's only going to be here 
for five years, I'm going to cheer him on, because if it's going to be used for five years, then it's 
a wonderful contribution. I think that the first thing is to make something that's useful, then we'll 
worry about how we can put it on to the next machine. I think what's really important is to get something 
done as soon as possible to make some progress. And if we take advantage of some peculiarity of a machine, 
if it stays around only for five years, great. We'll worry about how to transport to the next machine 
five years from now. PETER DEW: All of us have got a million bucks to spend that way. : Well, when you 
have a million bucks, that's even more reason why you could put it on your machine, because the less 
money you have, the longer you're going to keep your machine, the more useful that's ... (APPLAUSE) PETER 
DEW: ... wonderful idea and he has a different machine. : Yeah, great. First what you need to do is develop 
an effective solution. Then worry about the bloke down the block. The first thing you've got to do is 
make something good for yourself and your colleagues. I want to develop something that's good first, 
in our own laboratory. Then we'll see if it's good for the rest of the universe out there. What I'm worried 
about is if we try to solve problems for the universe out there, we're not going to make as much progress 
as if we try to solve something for the guy in our lab right now, today. How's that for disagreement? 
RAE EARNSHAW: It sounds very pragmatic. Any more questions? : I'm worried about the possibility, because 
I did the same as you said, and I put into the solutions, and after five years, I didn't have any software 
issues compatible running on the current machine. How do you handle that? It's like jumping into the 
ocean and how far you have to swim. : The people that developed your software on your very nice machine 
are going to be the predecessors of the people that are going to be around for your next very nice machine 
five years later. And the important thing, it seems to me, is to make some progress -- by hook or crook, 
you make some progress. And if it turns out that you're using some part of a machine which is very, very 
useful, and it makes that algorithm difficult to port to the next machine, well, maybe that next machine 
isn't good for it. RAE EARNSHAW: Okay, I think on that exciting note we'll draw to a conclusion. (APPLAUSE) 
Thank you very much for coming and my special thanks to the panelists for participating.    
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402246</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>33</pages>
		<display_no>4</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Four paths to computer animation: entertainment, broadcast, education, and science]]></title>
		<subtitle><![CDATA[will their futures converge?]]></subtitle>
		<page_from>1</page_from>
		<page_to>33</page_to>
		<doi_number>10.1145/1402242.1402246</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402246</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101766</person_id>
				<author_profile_id><![CDATA[81365594476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nancy]]></first_name>
				<middle_name><![CDATA[St.]]></middle_name>
				<last_name><![CDATA[John]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pacific Data Images]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101767</person_id>
				<author_profile_id><![CDATA[81365593029]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kovacs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wavefront Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101768</person_id>
				<author_profile_id><![CDATA[81100294395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blinn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jet Propulsion Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101769</person_id>
				<author_profile_id><![CDATA[81100251531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosendahl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pacific Data Images]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101770</person_id>
				<author_profile_id><![CDATA[81100573310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Upson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stellar Computer, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FOUR PATHS TO COMPUTER ANIMATION: ENTERTAINMENT, BROADCAST, EDUCATION, AND SCIENCE--WILL THEIR FUTURES 
CONVERGE? Chair: Nancy St. John, Pacific Data Images Panelists: Bill Kovacs, Wavefront Technologies 
Jim Blinn, Jet Propulsion Laboratory Carl Rosendahl, Pacific Data Images Craig Upson, Stellar Computer, 
Inc. FOUR PATHS TO COMPUTER ANIMATION : ENTERTAINMENT, BROADCAST, EDUCATION, AND SCIENCE--WILL THEIR 
FUTURES CONVERGE? NANCY ST. JOHN: I'd like to start this morning with a little exercise. Do you feel 
up to it? I know its tough when you have been partying for three nights in a row, but let's just try 
this. I'd like all of those people who have had their work seen on any television program or in the media 
to put up their hand.That includes anyone from the commercial and broadcast fields as well as all you 
scientists. Okay. How many of you have ever had your images published in a technical journal or in a 
technical paper of some kind? Hands up. How many of you have ever made slides for a talk using a computer? 
Hands up. Great. Now, how many of you have raised your hand more than once. I guess I can say that I 
have a lot of cross-over people here. And what do I mean by that? Well, have you ever noticed how many 
of your scientific friends may just happen to play music or be really good artists? Or have you ever 
noticed how many of your artist friends actually like to build things or have an interest in the physical 
sciences. And have you ever noticed how the field of computer animation seems to attract a multi-disciplined 
and multi-talented eclectic group of people? And yet, when we talk about computer animation, we talk 
about the evolution of computer graphics applications like scientific graphics, broadcast graphics, educational 
graphics, and business graphics. We talk about broadcast logos versus commercials versus entertainment 
and we have all these disciplines broken up into little subgroups. We talk about how it was in the 60s, 
the 70s, and the 80s and wonder what it's going to be like in the 90s. I guess that's what we're here 
to talk about today. I think we should look at our history and realize that we have always had the same 
problems. In the 60's it was hidden line removal, the 70's were hidden surface removal and paint systems, 
in the 70's and 80's we were dealing with illumination models and today we face the problems of object 
motion and object creation and deformation. One thing is for sure,we all have the same needs. We want 
things to be faster. We want things to be cheaper and we all want our tools to be flexible but robust 
enough to support complex sophisticated projects. So as far as I'm concerned, we don't have a separation. 
We're all the same group and we're all interested in computer animation. And that's why I've invited 
these four speakers today because I think these gentlemen will speak to the question of: Will the various 
areas of computer animation converge? And when you look at the definition of "converge" you can think: 
Will their futures come together or unite in a common interest or focus? The first speaker this morning 
is an electrical engineer or used to be one. Now he's in business and he has a computer animation production 
company nearing its first decade which I think is an amazing accomplishment. He's also a software writer 
and he's written a lot of the software for his company, Pacific Data Images. And he's won a lot of awards. 
He's an animator and he's produced well over 200 jobs. I'd like to introduce my first speaker, Carl Rosendahl, 
President of Pacific Data Images -- CARL ROSENDAHL: Thank you, Nancy. What I'm going to talk about this 
morning in my ten minutes is a little bit about where we've come from, the kind of work we're doing now, 
and where we're going. I divide PDI's markets into three areas: broadcast, advertising and long format. 
I'm planning to talk about each one of these briefly and then discuss where I think it's all going. The 
first area is broadcast. This is the work wedo for networks, cable systems and TV stations and includes 
IDs, program opens, promos and news graphics. This use has perhaps the greatest constraints over what 
we can do. There is a very specific message that needs to get across in a very short amount of time. 
It has to be visually interesting since the viewer is going to see it over and over again for years -- 
it can't be something you are going to tire of quickly. It also has to carry a lot of information. A 
station promo is perhaps the biggest challenge. It's on the screen for about three seconds, and you have 
to be able to read what the show is, what date it's on and what station you get it from -- in three seconds. 
And that's the reason that so much of broadcast graphics has to be so clean and simple and crisp and 
fast. The status of the broadcast industry right now is that it's a very limited growth industry. Basically, 
all the air time that can use computer graphics, is using computer graphics. About three or four years 
ago, it all started being replaced from being film graphics or other forms of animation or graphics. 
And right now, it's all computer animation. Basically, it's a turnover process. You know, new openings, 
new ideas. So it's limited growth. There basically is much business there being done as, I think, will 
ever be done. Of the three markets that we're in, it's also the one with the lowest entry cost. For under 
$100,000 you can get a lot of equipment to do what needs to be done. It's highly competitive and that's 
part of the reason it's such a low entry cost. And the quality is very variable -- depending on who your 
client is, how much money they have, you can really do some bad stuff and get away with it, get paid 
for it, and get on the air. Sad, but true. Advertising uses. Basically, I see four primary things where 
computer graphics is being used in advertising. The first is logos and tags, second is product demos, 
third, character animation, and fourth, environments. These are basically all the options of the ways 
people ask us to use computer graphics to work in their commercial. And obviously, you can mix them together 
in all sorts of ways. There're really two major constraints to dealing with the industry. The most important 
one, from my point of view, is that there are a lot of clients to please. A lot of the jobs that we've 
done, we've done in conjunction with other production companies. For example, the Crest commercial that 
you may have seen in the film show, if you went to it, was done with Charlex which is a production company 
in New York. So they were directing it. We had to please them and they had to please their clients at 
the ad agency which were four, five, six people. And then when they were happy, they had to please the 
people at Proctor and Gamble. It's a very long process to go up, to come back down, before you get the 
feedback you need to go on --as opposed to the broadcast industry where there's usually one or two people 
you're dealing with who are in a decision-making position Also, with advertising, you're limited in the 
amount of screen time you have to about 30 seconds maximum. The status of the advertising industry. Emerging 
from a slump. This is actually a little bit the wrong time to say it because the writer's strike is having 
a pretty devastating effect on a lot of the advertising. But in general, the attitude I see in talking 
to people from the advertising community is they're once again interested in using computer animation. 
Remember three or four years ago, there were all sorts of highly technical, computer animated commercials. 
The pendulum had of swung to high technology. There were a lot of vector graphics, and a lot of using 
computer graphics because it was computer graphics. It was used to make an image for the company that 
they understand technology and use it well. Then the pendulum swung the other way, from high tech to 
high touch -- just people and feelings. The Hallmark Card commercials being perfect examples. Now it's 
swung to kind of the middle ground, I think, where people are using computer graphics not because it's 
high tech and it gives them that kind of an image, but because it helps them tell the story that they 
want to tell. And that's the reason that we want to use computer graphics. There's a real strong interest 
now in using computer animation for doing characters and that's probably the most exciting thing about 
the advertising community to me today. It's a moderately competitive community, very price sensitive 
and there's a moderate entry cost for getting into it. Next: long format. This kind of covers everything 
else. If it's over 30 seconds long, to me, it's a long time. And basically, the areas that we're looking 
at for applications of animation are television programs, motion pictures, music videos and things along 
those lines. The uses in long format are: character animation, environment, special effects. Any one 
of those can be used independently or together. Constraints of long format: money, money, money and visual 
complexity. I've been working for the past year to year and a half to really try and get a lot of people 
in the television industry interested in using computer animation and really taking advantage of what 
we can do these days for them. The first question, every single time is: will it cost me less than what 
I'm doing now? Unfortunately, I have to say, "Well, no, but it'll be really different and really cool 
and people will love it." And they say, "Well, that doesn't matter because we're already losing money 
doing the production we do and we really don't want to lose more." And then visual complexity. It's got 
to be something that's interesting, that's going to draw your eye to it. No one's going to pay us to 
fly letters around for a TV show. You've got to do characters. If you're doing characters, they have 
to be really interesting characters --characters with a lot of emotion and feeling and just visual interest 
to them. If you're doing environments -- no one's going to pay you to do an empty room. You have to do 
a lot of stuff in it. If you're doing effects, you're competing with the real world, real special effects, 
so you better be able to do them better. And all that leads to visual complexity. The status of the long 
format industry. I think this is a very significant time for all of us because this is the year, I feel, 
that we're crossing over the line and becoming economically viable for long format situations and I think 
that's due to a lot of reasons. First, computers are continuing to be faster and cheaper. We can now 
start to economically compute the kind of frames that we need to compute. Second, all of our tools are 
getting better and that just comes through a lot of experience in developing tools and developing better 
tools on top of those. And then, third, there's economies of scale. If you're doing 30 minutes, the expense 
doesn't just go up linearly. There's a lot of work that you can do up front to really save a lot of money 
there. Parts of the industry are really excited about using computer animation. They can get a new look. 
They have a much higher quality and more control than the animation that they're getting by going overseas. 
There are also parts of the industry who are not excited. What can I say? They should be. The most important 
thing about long format to me is that it's the beginning of the evolution of a new form of storytelling. 
We can do new types of characters and new types of stories that are only going to be possible for us 
to tell using this new technique. If you look at any medium for communicating, be it film, traditional 
animation, novels, painting, photography, whatever -- there's an evolution it goes through in creating 
it's own language, it's own form of communicating. If you look at cinematography it went through three 
steps. The first one was what I would call "techno-marvel". You know, Edison creates this thing so you 
can project film and people look at it and they say, "Wow! This is amazing." And they go to the nickelodeons 
and they pay their nickel or their penny to watch a flip book of pictures. They just get blown away by 
it and it's amazing. So there's a stage that it goes through where: "Wow! That's wonderful! Isn't it 
incredible what technology can do?" The next step it goes through is starting to emulate other forms 
of communication. So a lot of the first films, a lot of the stories that were told with film, were done 
by locking down a camera in the middle of a theater and letting a stage play go on it. The camera was 
a static observer of what was going on. Then finally, they discovered, "Hey, we can move the camera. 
We can start it and stop it. We can do editing. We can do camera angles. We can do dramatic lighting." 
And a whole language for communicating with film evolved. And today's it's a very, very specific language 
and you can get books this thick on the language of film. Traditional animation went through a very similar 
process. It started out as being techno- marvel. "I can bring drawings to life!" And people were amazed 
and thrilled. Then they started thinking, "We can actually entertain with it." They drew from other media. 
They drew from this whole language of film that existed. They drew from comics, which is where they all 
came from --newspaper comics, comic strips and such --and started using those. Then they went and started 
evolving their new language, and it's one that we're all looking at very hard today --squash and stretch, 
timing --all these sorts of things. There was now a new language for communicating with traditional animation. 
So this is what we're going through. We've gone through techno-marvel -- "Wow! You can make pictures 
with computers. It's amazing." People would pay us to do that, that's where advertising was a few years 
ago. And now we're starting to draw from all these other mediums. We're drawing from graphics. We're 
drawing from cinematography. We're drawing from traditional animation. We're drawing from physics. We're 
pulling in and trying to experiment with all the languages for communication that are around us right 
now. We're still in that phase of using other mediums, the languages of other mediums, to learn how to 
communicate with this new one. Once we understand those, we can start pushing that in new directions. 
I think over the next few years, we'll really see the beginning of this evolution of using computer graphics 
as a new form of communicating and a new way of telling stories. And that's what we're really interested 
in doing. NANCY ST. JOHN: Thank you Carl. Our second speaker today is a mathematician and a jeweler. 
He was staff scientist at Livermore Laboratories for seven years and a specialist in fluid dynamics. 
He created the planet Jupiter for the film, "2010," and he was an animator, a technical director, a software 
writer, and a scientific producer --all while he was at Digital Productions in L.A. He was the co-creator, 
with myself, of the Scientific Visualization Program at the National Center for Supercomputing Applications 
at the University of Illinois as well as being a research scientist in graphics. And he's currently working 
as a visualization scientist at Stellar Computer. Craig Upson. CRAIG UPSON: Good morning. I'm going to 
talk about visualization in the computational sciences. I thought I'd start with a few examples of some 
older images and then work my way up to the present. Just some rough examples of what the field's been 
doing in the last 10 to 20 years. What I actually did is I went to Bruce Brown, who's been in charge 
of the SIGGRAPH slide show for several years and I went through all his slides to see what older images 
he had. This is an image that came from the University of Utah. My guess is probably mid-70s. And it's 
an early image, probably one of the earliest images of medical imaging, if you will, of a brain. It's 
barely recognizable as we look at different sides of it. It's several different contours of material 
inside a skull. And this is essentially the techniques that we use today for a lot of other images and 
things haven't really changed a whole lot. You know, the complexity has gone up, but in general, the 
methods we use haven't changed dramatically. You look at this skull compared to the ones we see now and 
there's a lot more complexity now, but we're still using the same basic techniques. This is an image 
from the mid-80s. One that can be produced in real time from the medical imaging field. In volume imaging, 
we're getting more and more detail from the data that we're using. And in some cases, we're developing 
new techniques like volume imaging. But in many cases, we're using just the same old techniques. This 
image is a fairly new one, but it certainly is something that could have been produced in the mid-70s, 
of a molecule. And as we look at the progression in the molecular sciences, computational chemistry, 
the ways of representing data really haven't changed a whole lot either. This is from Paul Bash at Harvard. 
It's a ball and stick model with an isopotential surface, a contour surface of the potential field and 
other techniques, such as this from Mike Peak at Scripts, show more detail of a molecular surface. So 
in some sense, I think probably the molecular modeling field has progressed much more than other fields 
in the computational sciences in terms of visual representation forms. This simple 2D contour map in 
black and white is how most computation fluid dynamics scientists represent their data today. It could 
have been done in the 60s and and is still the prevailing method of representing large simulations. This 
is from a 3-dimensional simulation and you get very little information from this one graph, but it none 
the less, gives you very specific information where you might want to look for more detail. But you're 
still getting discreet information. Other techniques are creating a contour surface and this is a fairly 
recent image that was done last year in NCSA at the University of Illinois. And you're getting different 
information now. You're getting all the information along a single thresholded surface of this 3- dimensional 
simulation. But it's the same technique that people at the University of Utah used in the mid-70s and 
another way of representing that is through volume rendering. This is the same simulation of that tornado, 
but a different way of looking at it. If we go to the computational mechanics field, the most common 
way of representing anything still is by wire frame. This is a cylinder that's been dropped on a bar 
and you see the deformation in the center of the cylinder represented by the really dark areas. These 
areas are the different contours of the stress distribution on that cylinder. But this is a technique 
that was used in the 60s and still common today. The same data set, but now shaded with flat shading 
is seen here. This is again, from Bruce Brown at WANG. This was created probably around 1980, maybe '81 
and this is the common techniques that are used today. So in some sense, one conclusion is that we really 
haven't developed a whole lot of new techniques for scientific visualization in the last 20 years. And 
I'm going to try to address that issue in just a minute, but the first thing I'm going to talk about: 
what is scientific visualization. It's really just a new name for a fairly well-established, but perhaps 
somewhat floundering field. Floundering, mainly because it really hasn't kept up with the demands that 
are being generated by computational scientists. Simulations are generating much more data as well. CT 
and MRI scans are producing layer 3D datasets. It's hard to deal with that kind of stuff. So we haven't 
really kept up with the demands in those fields. Let me define at least what I think this field is and 
the processes that a scientist goes through when he's trying to understand a phenomena that he's simulating 
and what the visualization process is throughout that regime. These are essentially the steps that the 
scientist goes through when he simulates something. The first step is researching a method, you know, 
trying to figure out: "What is it I want to solve? .... Now that I've got the program made, I've built 
the simulation code that does this. Now I want to build a simulation -- set up all the parameters that 
govern the phenomena that I'm trying to represent." That's boundary conditions, building a mesh -- that 
kind of stuff. Then I want to pick my favorite super computer or mini supercomputer or whatever, and 
compute on it. Once I'm done with that, well then, what do I do? I have to figure out what I just did 
and that usually takes a long time. That's the analysis step. And then, at least in the way that we do 
things -- we typically go out to some media, like film or video. So, in essence, a scientist will spend 
an awful lot of time going through this loop. And the visualization process here is essentially this, 
of taking the output from the simulation, going through another loop: The analysis step is a loop. Deriving 
the quantities that are interesting to look at, making the translation from raw data to geometric primitives 
(things that can be rendered), making images of those things, playing those images back and then recording 
them on to some media or writing a paper. And so we spend all of our time going through this loop essentially. 
Most of our time, trying to understand what a simulation is doing is spent in this loop. So those are 
the basic steps. Now I'd like to talk about some trends that I see. They fall into two different categories. 
One is cultural trends and the other one is technological trends. In the cultural trends, the first one 
is removing the Hollywood stimga, is that essentially, there's a problem in the computational sciences, 
that people tend to equate good graphics with bad science. People are worried that a scientist can get 
funding if he has a very slick presentation regardless of what the content of the science is. And I think 
that this really... The scientists that typically complain about this the most fall into two different 
categories. There are those scientists that don't have access to visualization tools that really don't 
have the ability to analyze what they're doing. And that's a big problem that we have to fix. Or they 
fall into another category, and that is, that they're not really doing top flight research. The ones 
that are doing top quality work aren't worried about this issue. I think the basic question here is really 
the process of funding. How does an academic researcher get funding? That is, if the grant reviewer can't 
really distinguish mediocre science from fancy graphics, then it's not really the problem of the visual 
representation form. It's a problem in the funding mechanism. We should work on the funding process and 
work on educating grant reviewers and get back to science. Second point is, I think we're going to see 
some demystification of visualization. If you look at it five years ago, this field wasn't esoteric, 
but now there seems to be some overtones of that and I think this really stems from two things. Admittedly, 
it's more complex now. We've got to deal with all this massive amount of data and very complex spatial 
and temporal data. But I think the mystique also stems a little bit from the fact that we now use specialists, 
graphics technologists that help the scientist do something and so the scientist says, "Well, this is 
a little bit more mystical than I thought it was." And I think that'll go away as soon as we get tools 
in the scientists' hands -- good tools. Third one is, building visual vocabularies. Essentially, we have 
a hard time really explaining the graphics and the animations that we create. There really isn't any 
standard ways of representing various phenomena. So scientists spend a lot of time saying, "Well, what 
is that? I don't quite understand what I'm looking at." We don't really have a language for it. We don't 
have a vocabulary. We don't have a syntax. We have no grammar except the molecular modeling people have 
been able to do quite well in that regard. They have their own geometric primitives and a syntax, essentially, 
for communicating their images to each other. And I think that'll change as other fields come up to speed 
also --computational fluid dynamics and structural mechanics, etc. The last point on this slide is getting 
the scientist back in the saddle. By that, I mean what I hinted at in the prior point, is the moving 
away from using graphics people -- graphics technologists, you and me -- that is, that produce animations 
for scientists. We need to get the tools in the scientists' own hands. A good analogy, I think is how 
useful would word processors be if only specialists could use them? They really wouldn't be that very 
useful at all. So we need to get some tools back in the end users' hands. Now I want to talk a little 
bit about technological trends. There are four trends that I see that will be emerging probably in the 
next three to five years, or two to five years. One is common tool kits. That is, you know, shareable 
software tools that are built on hardware and software standards that scientists can use in one environment 
and find in another environment also. Currently if a scientist goes from one institution to the next, 
he ends up relearning how to use all this stuff or rewriting it and that's a big waste of time. And I 
think that this is really going to be fixed. The reason this really hasn't been fixed before is because 
there really wasn't an economic opportunity and I think the economic opportunity now lies with hardware 
vendors actually, rather than software vendors, because hardware vendors have to produce this stuff to 
compete. When the first Cray 1S came out, it was shipped without a compiler. You can't do that now. You 
can't sell machines without compilers. You can't sell the machines without operating systems. And now, 
you can't sell machines without a lot of other stuff, too: soon that list will include visualiztion environments. 
Visualization environments. I think that we'll move toward large integrated suites of common tools. Suites 
which minimize the headaches that you go through in turning data into imagery. The whole process of, 
you know, data management, data manipulation and image creation into coherent suites of tools. And those, 
I'm sure, you'll see in the next year or two. Soon we'll be more interested in modeling rather than rendering. 
Making that translation from data to geometric primitives is probably the next hot of research rather 
than different illumination techniques. How do we build geometric filters that tie surfaces and all that 
kind of stuff. And I think most of the rendering is going to be done in hardware. And that sort of reduces 
the glitz, too, because hardware doesn't have all the capabilities that software does. And the last point 
is: visual co-processing. And by that, I mean essentially something that John von Neuman thought of in 
the mid-1940s: His ideal method of interacting with the computer by what he called "oscilloscopic graphing" 
or making images. Essentially John von Neuman wanted was something like an interactive, numerical ... 
a natural phenomena simulator, just like we have flight simulators. And the question is: why isn't this 
the way of dealing with simulations now? It really hasn't happened and I think the traditional excuse 
has been that hardware just hasn't been fast enough. That's changing. As Carl said, there's a big change 
in economics in terms of machines and so now we're getting machines that have decent graphic speed, bandwidth, 
high compute performance, etc. So now, the question is: well, why haven't people been working on the 
software that's needed to really interact with those machines and anticipate where we're going. It's 
tough to do. So that's essentially my talk and the question that I'd like to pose to you is: Can you 
replace every reference I've made to scientists, with animators? They essentially want the same types 
of things. Animators want to use the same software that they've used before. They're looking for environments 
that are flexible, they want interactivity, and they are moving more and more into simulation to bring 
our realistic dynamics. And so Carl has to teach them how to use this new software and it's the same 
problem, really. I really don't see a whole of lot of difference between the trends that we see in the 
computational sciences and in the entertainment field. Thank you. NANCY ST. JOHN: Thank you Craig. Our 
third speaker today has degrees in both architecture and environmental design. He has written computer 
graphic systems for structural and architectural design. He worked for 6 years for Robert Abel and Associates 
in Los Angeles as a technical director, a software writer, a director and eventually as vice president 
for research and development. And now he's currently one of the co-founders and vice president at Wavefront 
Technologies. Mr. Bill Kovacs. BILL KOVACS: Thank you. I'm glad to be here. My focus and where I'm coming 
from, I think is hopefully representing the people building tools, selling off the shelf products be 
they hardware or software, to solve some of the problems that these gentlemen have been talking about. 
And so I'd like to show you some things we've been doing and talk about some main themes -- those themes 
being visual problems, access to technology and ease of use. So I guess the question I'd pose here is: 
what's wrong with this slide? Well, we did this, I guess for a Christmas card last year. We're very proud 
of it and you can kind of crawl in there to the frame and look at all the reflections going on, but as 
the company's been evolving very rapidly over the last year, the real thing that's wrong with this picture 
is that it doesn't really solve any visual problem and our particular industry is evolving, I feel, from 
a point where ... and this touches on themes that have been mentioned before, from how do we make wonderful, 
pretty pictures to, how do we plug that picture-making capability into solving.., or using the tool to 
solve visual problems. And of course, it's the degree to which those problems are solved that drive the 
size of the animation marketplace. In that vein, is a project we've been working on with Ford and they 
have a classic problem. They're building cars and as you may know, they still build them with clay models. 
And they take some several months to build them. And once they build them, they cover them with mylar, 
then they put them in this room. This is the Ford room. And so they have a sort of standard environment 
with lighting on the top and the same curtains and they look at them in black and white for the same 
reason that Campbell's Soup tasters taste the soup with red lights up on top, so they're not influenced 
by the color. They turn these around and make decisions and then go back and do it all again. And the 
problem of course, is that just prolongs the cycle of getting the product to market. The latest buzz 
word around Detroit, is "time to market". Everybody in Detroit is scrambling, realizing that the Japanese 
have cut about two years off of it. And so there's a very real problem here that's being addressed and 
visual systems are just getting to the point where they can touch on it. I brought a video that shows 
this in motion. We went through a number of studies with this, comparing the real thing to the simulated 
object. And it came fairly close, but still, as you'll see from the side-by-side that we have a way to 
go. There you can see the side-by-side model on the left, which is actually two models and the simulation 
on the fight. Of course some of the inaccuracies are based on the fact that we didn't know enough about 
the room to be able to model it specifically. And you can see that on the computer, when it's seen up 
close, it moves around, does have some small artifacts that can be distracting. But it comes real close. 
Even at that, it's just on the threshold of being good enough to really solve the problem. I mean, this 
is a $10 million-dollar decision they're trying to make here on the quality of the design and they're 
not going to screw around. If there's any chance that this picture is not accurate, they're not going 
to take that chance. They're not going to use the technology. They're going to stay with building models. 
And that's of course, what most of them are doing. I don't think there's really a auto company that's 
set up to really employ the technology directly. They're all experimenting. So these pictures that rll 
show you here are just a few more examples. We're going from just pretty pictures to products in environments, 
looking at the effect of the products on the environments. A significant amount of complexity, of course, 
because you know, real environments have this annoying problem of having lots of detail in them. And 
people who were used to very simple graphics for explaining projects, are now getting into a lot of texture 
mapping and shadow casting. You can see that this satellite dish has got some shadows and the solar panels 
are done quite accurately. And once again, the ante keeps going up in forms of what it takes to do a 
good presentation. Again, an aircraft photograph. High detail. A lot of texture mapping to get realistic 
detail. So I'm going to go very quickly through a little bit of history just to give you some perspective. 
I'm going to talk a little bit now about hardware and cost. And I just wanted to briefly put some of 
that in perspective. The early 60s of course is when it started, certainly research. Certainly, the costs 
were anywhere from exhorbitant to unavailable. In the 70s, people were starting to talk about how images 
could really imitate reality, increase reality -- CAD systems proliferated. Motion was possible. People 
were starting to put matrix multipliers into hardware and in the late 70s, the cost started to drop. 
Dec came out with the Vax. People were starting to talk about image quality and I remember listening 
to the first paper on anti-aliasing and, for some reason, it just sounded ... I thought for a moment, 
it was really kind of going overboard actually, to worry about the value of these pixels at th e edges 
of the polygons. It's obviously now just the commonest feature in off-the-shelf software, but that's 
an indicator of the way the field's progressing. And of course, in the early 1970s, people started to 
use these computers. What comes to mind are all the production houses around the country writing their 
own code, doing this stuff. This is not to say that they were making money on it, but they were having 
a lot of fun and they all had fun until the money ran out. So in the 80s, we saw the combination of Raster 
and Vector -- one box does it all, with video output. Our company couldn't have really existed without 
these kinds of products because this is obviously where Wavefront starts to plug in. The dynamics become 
more and more real and again, these were available off the shelf. Now, as you walk around the floor, 
you're seeing super high quality. Everybody's trying to push the top down. Some people are telling me 
that radiocity is the latest thing. Ray tracing, all sorts of alternate rendering schemes, but lots of 
realism, lots of dynamics, hardware available to do that. And although the last statement on this slide 
is kind of a market hype comment, it really is the case that there are databases out there that are waiting 
to be visualized and that's the goal of a lot of this. What people really have is all sorts of levels 
of realism and the cost goes up with each level of realism. These are pictures that represent the sample 
points on that graph. But what they really want to do is make an image that can be anywhere on that graph 
-- lots of inexpensive pictures, or very few, very fancy pictures and the ability to go anywhere in between. 
And that really kind of defines the ideal product. This is a photograph from a year back. It's still 
fairly current, but this is an indicator of the number of boxes you might have to plug together to do 
computer animation, especially if you were trying to do high quality. And this has to change, obviously. 
It's changing. People are compacting some of these video processes back into the workstations and of 
course, that just makes it easier to get your hands on one piece of hardware that solves the problem. 
Tomorrow? More speed certainly. Networks are obviously here to stay and a defense to sort of breaking 
up the work into pieces. Everybody seems to have rendering. It's obvious. The real question is how do 
you get at it and how do you make those beautiful pictures. And I think that touches on the final issue, 
which is ease of use. My favorite ... this doesn't necessarily look like a lead on slide for ease of 
use, but it's done by one of our clients who is a sculptor who bought this system as kind of a personal 
computer, believe it or not. And two days into his training course, he came into my office and he said, 
"Gee, you know, I've been thinking about this and I don't know if I can do this stuff. There's numbers 
here and I have to ... what's an operating system, really?" And questions like that. And I said, "Well, 
you know, stick it out for a week and, you know, if worse comes to worse, perhaps you can get an apprentice 
and you can focus more on the sculpture and he can focus more on the technology." But lo and behold, 
he sort of got into it more and more and finally developed a feel for it and now I'm seeing his work 
on covers of computer magazines and such. It's really amazing to see. I'd like to make the point that 
the real modern architecture as far as ease of use is concerned, involves an interface that can change. 
If you go into a telephone store and you look at all the telephones -- the Mickey Mouse phone the Trimline, 
all the way down to the one that imitates the first phone, you know, done in wood --you know that they 
all have the same reliable digital and they'll all plug into the network and get the work done. But obviously, 
people buy those different styles. And computer software will evolve to that point where the way the 
product appears has maybe even more to do with its appeal and its attractiveness than the actual guts 
of what's behind the screen. So I feel that's a significant future trend. And also what we're seeing 
that I think is encouraging is that in terms of offering people these kinds of software and hardware 
products, the companies that offer them are maturing. In our particular case, we're now able to offer 
... you're not only seeing primary software, but other companies writing software to aid and augment 
the software of the more mainstream companies. So as these companies are able to not only design the 
product, but provide all the peripheral support and training, to actually bring that to bear on a problem, 
will be progressing a lot more quickly. So thank you very much. NANCY ST. JOHN: Thanks Bill. Our last 
speaker is an artist and a musician. He's produced more frames of animation then Digital Productions, 
Robert Abel, and Cranston Csuri combined. He's produced over eight hours of computer animation just for 
his Mechanical Universe series. He holds degrees in physics,and in computer science. His participation 
in SIGGRAPH conferences includes 15 tutorials, 9 papers and many panels. He received the first computer 
graphic achievement award in1983 -- the highest honor that is bestowed by SIGGRAPH and in the same year, 
he received the NASA Exceptional Service Medal for his contributions to science and to the space effort. 
He's currently at Cal Tech, attempting to produce educational videos for high school mathematics and 
he's finally cutting his roots from JPL in October -- Dr. Jim Blinn. JIM BLINN: We're supposed to be 
talking about four different aspects of uses of computer animation and whether they're going to merge 
together into one happy family in the future. And I'm going to basically say, Gee, I sure hope not -- 
primarily because the other three aspects of computer animation are evil and wicked and the educational 
aspect is good and virtuous. And I hope to make that point here. First of all, let's contrast education 
with advertisement and broadcast. The big problem I have with advertising and largely broadcast-type 
thing is, they're primarily in the business of selling lies. What this means is: they're trying to say 
that their product is inherently better and more worthwhile and more exciting than the competitors' product 
and you should not even consider buying the competitor's product --their particular car is great and 
the other ones are terrible and horrible and of course, actually, you realize that's never true. Cars 
are basically have some minor differences between them, but they're kind of all the same thing. But an 
advertising thing is not supposed to show you that, it's supposed to show you why this one thing is so 
tremendous and wonderful. And when the advertising people start getting into the science and education 
business, I worry to some extent. As was pointed out earlier, to what extent people's scientific theories 
perhaps are going to believed and given validity more on the basis of how good an advertising agency 
they have and how good a flashy video they have, rather than on the basis of evidence or the believability 
of the theory itself. It's something we have to watch out for. Let's compare education, say, to entertainment. 
What I've been doing in the past --educational programs -- has been really fairly simple graphics, which 
is the primary reason I've been able to get away with generating so much of it. If you make simple pictures, 
you can make more of them than complex ones. I find the big difference between education and entertainment 
is that what I'm trying to do is to come up with the simplest possible picture that gets across the idea. 
Whereas, in entertainment, usually they try to come up with the most complex possible picture to get 
across the idea -- keep people entertained. You hear the word "We want to dazzle the viewer," and so 
forth. And I'm surprised that people use the term "dazzle" because that's a perjurative term. It comes 
from the same word as"daze." You're numbing people's senses. You're making them tune out in some fashion 
and I'm trying to open people's minds rather than to dazzle them and blaze their eyes. Let's compare 
education versus scientific animation. This is where one would initially think that there was the most 
in common. And first of all, I want to define how I distinguish between the two. The education game is 
primarily a documentary thing. It's taking some ideas that are already well-known or well-accepted and 
presenting them to people. The people producing it already understand the thing and are just trying to 
share the information with someone else. In scientific research animation, you're experimenting with 
ideas yourself. The person producing it doesn't really know. They've got a theory that they're working 
on. They're not sure if it's right or not. They want to see what the results of the theory are and so 
when they see the animation, it's news to them. A lot of the time it just shows errors in a theory. Now, 
there's a move afoot to bring flashier animation to the science research sort of thing and I'm not entirely 
sure that's a good idea because when you're experimenting with ideas and you're not sure if it's true 
or not, you don't want to lend too much credence to it beyond what it's worth. I mean, this stuff has 
a tremendous longevity. You make this really neat color computer animation and the scientist is playing 
with his theory and discovers maybe there's something wrong with it or changes their mind or something 
as a result of looking at it. But the animation is still around and you know, five years later, some 
television production or some magazine wants it a nifty picture to put in there and they'll scarf it 
up and put it in. And these are theories that aren't even any more held even by the proponents, originally. 
So there's a case for saying that: if you're not really sure of whether your theory is correct or not, 
you should make the thing kind of crude and sketchy-looking so that people who look at it understand 
the fact that this is all just theory and not necessarily what is considered truth. I've done a little 
bit of this in some of the educational animation on a slightly different tack, with the difference between 
an animation that's driven by numerical simulation showing the dynamics correctly versus animation that's 
diagramatic and is supposed to show the big picture rather than the details. And the diagramatic animation 
should make the lines kind of wiggly and sketchy, like it was hand-drawn, rather than mathematically, 
perfectly straight because that implies computer simulation. So anyway, I just wanted to wrap up and 
say that computer graphics has a lot of power to communicate and we just need to think a whole lot about 
using this power wisely and making sure that what we say is what we really want people to go away with. 
Thank you. NANCY ST. JOHN: Thank you Jim.We have finished exactly on time and we'd love to have lots 
of questions. If we could have the lights up in the house so that we can see who we're talking to. And 
would you please come up to one of the microphones on the floor, give us your name and affiliation please. 
(Did not give name) : In Moby Dick, Herman Melville said that "the soul is kind of a fifth wheel." And 
I was wondering if any of you thought that perhaps art is kind of a fifth path to computer animation. 
And in response to those of you who see art as a subset of entertainment, let me just say that I think 
it's the opposite. NANCY ST. JOHN: You mean, why don't we have an artist up here? We should have an artist 
up here. I'm sorry. We're only allowed four people.We blame it on SIGGRAPH. think we have a good representation 
of artists. Perhaps not in the traditional sense, but I can't imagine you telling me that some of these 
gentlemen aren't also artists. And I think that was the whole point of the talk --that we don't give 
people labels. We don't say, "This is a scientist who can't possible do art and this is a person who 
writes software who can't possibly be a scientist." I think that was the idea to kind of eliminate the 
labels. 12 BILL KOVACS: And something that's interesting about the art world is that it's obviously centuries 
old and there have been so many different mediums that also exist --sculpture, painting, drawing. And 
the computer is just one recent one. In some ways I see it as not having quite the same impact as it's 
having on some of these other disciplines. It certainly has its contribution, but it also has its ability 
to distract and all of that. It just hasn't seem to make quite the in-roads as in other areas. CARL ROSENDAHL: 
I think it will. The view that I would have is that it's an evolutionary process. As I said in the talk, 
the first stage you'd have to get through is techno- marvel. Film wasn't an art form for a long time. 
Animation wasn't an art form for a long time. Photography wasn't. Go down the list. You have to experiment 
with it and turn it into one and make it accessible to people who can start using it and make it usable 
to people. From our standpoint, I'd love it if we could sit around and make whatever kinds of films and 
animation that we wanted to make without people telling us what to do. But there's some economic considerations 
involved. And if you want to have the tools available, the high-end tools to really experiment and start 
pushing what you can do there's a trade-off, and you have to have clients and you have to have paying 
customers. We try to do art. We try to do things that express our own feelings, our own interests, without 
the demands of clients behind us. We give our animators time to work on that stuff. We had a piece in 
the film show that was non-commercial. We had some things in the art show. I think it's a valid point. 
I think there should be more artists, but I think it will happen. I think that this is going to get in 
the hands of people who can use it and really do some phenomenal things with it. : I think that we're 
basically lazy people in that we all essentially do try to solve the easy problems first. And I think 
it's a lot easier to get tools, you know, technical tools in a technical person's hands than it is to 
build tools that are designed for artists. That'll come, but we're having a hard time getting the right 
easy-to-use tools in the scientists' hands or in the technologists' hands. JIM BLINN: One thing I'd say 
that's kind of opposite of what I said before and that is, a good aspect of a lot of the commercial stuff 
that's going on is the economies of scale. Artists typically don't have a lot of money and if it was 
only artists who wanted to do videotape editing, it would be incredibly expensive for them because it 
would all have to be technologically designed for them. Whereas, since there's lots of sitcoms and battle 
of the network stars and stuff like that, that supports a video production industry and allows them to 
have big facilities that artists and other people can rent for a lot less than had it been built only 
for them, maybe it's a good idea there's all this stuff around. SCOTT KIM: Information Appliance. I often 
get the feeling that a large part of the industry are artists figuring out a way to also make money while 
their getting access to computers. I wanted to ask each one of you ... if you want to be in any of these 
areas, it has to be something that's meaningful to you if you're going to enjoy it. I want to ask each 
of you, why this work is meaningful to you, personally and how you chose that path you're on. JIM BLINN: 
It's meaningful to me because it makes me feel virtuous. I feel very fortunate in the sense that I've 
been one of the people in the business who's been lucky enough to at least so far, get funding to do 
stuff that I really wanted to do that has what I consider more long-term meaning. I have a lot of respect 
for people who work in animation production houses because you know, when I work day and night for 6 
weeks and generate an animation, it portrays how the theory of relatively works, how the universe is 
structured. And when a lot of people from production companies work day and night for 6 weeks and tear 
their hair out and go crazy, they come up with a tin can commercial. That's true, actually. And it has 
a lot to do with why I'm not working in that industry anymore. I think even when I was working in that 
particular genre, I think my focus and interest was on tool building. What's interesting to me is that 
a lot of the basic things that you see on the floor are not truly new in any fundamental, algorithmic 
way. What distinguishes all these programs and different software tools is just the clever application 
of known techniques to make fundamental thing more easy to use. It's just like a recipe...everybodyhas 
the same ingredients, but you're applying them in different quantities and they could have radically 
different degrees of success depending on your combination of the essential parts. So that's what's challenging 
to me a present. CRAIG UPSON: Well, let's see. I guess if Jim Blinn is in this because it's virtuous, 
I guess I'm in it because I'm curious about how do we really understand what goes on in nature; what 
motivates scientists. And I think I just sort of fell into the field probably like most people have --'essentially, 
when I was going to school, there weren't any classes in computer animation or computer graphics. And 
when I was working doing computational fluid dynamics, you finally realize that you can't really understand 
what you've done. You have no idea. You know how to solve the equations, but you really don't know: did 
I solve them correctly? And so you just sort of ... I just sort of go wander off into one direction and 
think--how do I see what I've done? And then I got captivated by that. And that's where I am now. But 
it's mainly just out of curiosity, I think. CARL ROSENDAHL: My attitude was: I figure I'm a nice guy 
and here's an opportunity where I can be really evil and get away with it. Actually, I got into it out 
of techno-lust and techno-marvel --being someone who is really interested in film-making and just "Wow, 
you can do pictures with computers," but having a degree in electrical engineering and wanting to be 
in the TV industry for whatever poison I got when I lived in Los Angeles growing up. It was the right 
combination of being able to use the background I had to pursue the interests I had. That's changed a 
lot for me in the past few years. The techno-marvel phase has kind of worn off. It used to be I'd make 
a picture back when I was animating and sit and be able to stare at it for an hour and just be amazed. 
Now the things that are exciting and amazing are starting to use it to tell stories, to communicate and 
experiment around and see what kind of new and different things we can do. So I've kind of changed my 
reason for being here and I have to tell you, this industry to me is more exciting today than it ever 
was before. It seems like there're more opportunities now because of all the different applications that 
are starting to be able to use it. And I just find that really interesting and stimulating. NANCY ST. 
JOHN: Well, I'd like the opportunity to answer that question because I'm really proud being the producer 
of Hawaiian Punch --that we made those tin cans. (APPLAUSE) Thanks to Tim McGovern and all the other 
people. And I think the reason why I like being in computer animation is because of the act of creation. 
I come from the film industry and you basically point a camera at a scene and you record it and of course 
you can do some wonderful things artistically, with the lighting and the way you dress that scene. But 
there's something really magical, in a sense, about creating something with the computer and starting 
off with a very small kernel of an idea and have it develop into a board which then develops into all 
these objects which then get colored and lit. And then we move them around and then by the time we're 
finished eight weeks later, there's this whole thing that has come together which wasn't there before. 
And if we hadn't put it there, it probably would never get there because it's not a piece of science. 
It's not a piece of math. It's not something that we can take from the earth and say, well, this is something 
that we've learned from where we live. It's something that just starts as an idea. And I think that's 
what the essence of computer graphics is all about -- it's the fact that we can just take a kernel of 
an idea and create something and put it on a film or videotape and show other people. So I'm proud of 
it. The hell with Jim Blinn (APPLAUSE) BOB HENDRICKS: I'm from the John Hopkins Applied Physics Lab. 
First, I wanted to make a comment. This occurred to me when someone was mentioning about art. I think 
that the convergence or commonality, at least between art and science -- we have a long history of that, 
even Gray's Anatomy --the whole idea of art and science. They have something in common. At their best, 
they're supposed to be trying to convey truth. And they've always been combined or interdependent in 
some ways. I just want to also say that and ask for a discussion about this -- it seems to me that in 
the area of computer graphics, what I'm noticing as converging, is not so much the applications as the 
hardware. I go down through the exhibition, I watch the show. I see something done for broadcast or I 
see something done for science. And they say, "We made this with a Wavefront on a Silicon Graphics" And 
you go to someplace else and they say, "We did this with a Wavefront on Silicon Graphics." Or, I'm not 
trying to focus on any particular piece of hardware, but it seems like the same piece of equipment is 
doing all the different types of work. Could you comment on that? BILL KOVACS: I guess it touches me 
more directly than others. No, I don't ... I think I have just the opposite opinion of it in the sense 
that when I cruise the show these days, they look more and more like toasters everyday. You know what 
I mean? They are looking alike and they're getting into the product, into the stage of products where 
everybody has a lot of the same features and you get into the kind of advertising mode that Jim was talking 
about. You know, we have more Mps. We have more flops. We can do more polys. Hey, we have radiocity. 
So I think there is a lot more diversity that expands the market. It gives people choices. And I think 
there's a lot of diversity out there in software. There's a lot of new software being shown here, both 
by established companies and companies that just poked up and so I think there's a lot of diversity there, 
too. JEFF YOST: From the National Center for Supercomputing Applications. Yeah, I have a question. I 
guess mainly for Jim Blinn. I think using computer graphics to make educational films is a really exciting 
application, but I personally don't know of anybody outside of you that's doing it. I wonder if you see 
the field growing and where the funding is going to come from. JIM BLINN: I wish I knew. NANCY ST. JOHN: 
An excellent question. JIM BLINN: I've done a little bit of reading in the history of animation in general, 
and I found some interesting things. Around 1923, Max Fleischer Studios did an animated film about the 
theory of relativity and when they finished, they discovered that nobody wanted to buy it. It basically 
lost money. And in the 50s, the Bell Telephone funded a series of science programs. And they hired an 
animation company in Hollywood to do little animated characters illustrating the science and the company 
went out of business because they didn't get enough money to pay for their expenses. So that the whole 
thing of education is sort of a grim history. Basically, what I'm doing is sort of riding on the back 
of the industry in terms of everything getting cheaper and moving what I do to PC clones and small scale 
computers -- which is really adequate for what I need to do. We are currently looking for funding at 
this point and we've gotten a lot of suggestions from people here. Hopefully, some day, people will support 
it, but it's always the case -- you know, the tax money and the school boards never have enough money 
to hire teachers at a reasonable salary and so forth. I can say that because both my parents are school 
teachers and we always had this problem, which is kind of puzzling because this is, you know, education. 
It separates you from the muck. It's what makes it possible for you to go out and great things later 
on. And you'd think that it would be more supported than it is. From what I've seen, it's something that 
you only do because you think it's worthwhile, not because you're going to make any money out of it. 
CRAIG UPSON: I see a lot of people trying to expose themselves to the medium and it's still an expensive 
medium as Jim says. And the very fact that you have to get funding, means that there's this chunk of 
resource you need that you can't scrape together personally in order to do sufficient quality of work 
to disseminate it to people. BILL KOVACS: And I think that's just a function of the cost of the tools. 
Unfortunately, most of us up here are trying to do sort of a professional quality product and that isn't 
available on a personal basis, but as soon as it gets to a point where you really don't have to scrape 
that much for the funding, then it gets more like a more accessible medium. DAVE FROSS: My name is Dave 
Fross from Farleigh Dickinson. I'd kind of like to turn the question around and ask you -- I wonder if 
we are past the point of our sort of maximum convergence and if we're diverging onthese various areas. 
Basically, my thought is that you people and the people that you might represent, so to speak, have used 
common technologies, papers from this conference and so forth, to build certain things. And now, it seems 
you're diverging in terms of getting applications and being more specific. JIM BLINN: My interpretation 
of the question is: why should we converge now that we've gotten the basic technology there are many 
diverse paths to enlightenment and we shouldn't necessarily all want to look alike, and at least I would 
agree with that. Once the medium is there, then everybody's imagination comes up with different sorts 
of uses of it. CRAIG UPSON: Well, I think there's a lot of power in two different fields learning from 
each other's mistakes and success stories. And I think that's why I'm interested in the convergence. 
I think your opinion was: well, maybe they've already converged and now they're diverging and I don't 
think that's true at all. I don't. NANCY ST. JOHN: I don't know. I think I'm agree with him. I'm an example 
of a person who went from commercial production to the scientific field which turn out to be a huge mistake 
for me, but it was really a very interesting convergence because the information that was swapped during 
that two-year period, helped me a lot and I think helped the people I dealt with a little bit. I'm definitely 
much richer for the experience and now taking off in a different angle and I think the people I dealt 
with there are doing the same thing again, from their own perspective. So I think I might agree with 
him. DAVID BARTH : Carleton University, Ottawa, Canada. Something I've always been interested in is real 
time animation to be used for education and science. For example, may be a real time mathematics lab 
where you can put in an equation -- let's say a quadratic equation -- and in real time, change the parameters 
of the equation and see the graph change, rotate it in three dimensions and see what it looks like. In 
terms of physics, putting together pieces of metal and so forth in a simulated world, getting an animation 
going and then asking for various forces at various points to see what the forces are. And maybe even 
for entertainment. Supposing you for instance, go to a museum and you have an exhibit that has some little 
characters -- let's say, dinosaurs, that you can control interacfively in real time, and have various 
people sort of tackling each other with dinosaurs. What do you see as the future for real time animation 
for science in education? NANCY ST. JOHN: Well, everybody should have an answer for this one, I think. 
You want to start Craig? CRAIG UPSON: Sure. You know, as I said in my talk, I think that the big reason 
that people haven't been able to do real time --at least their excuse has been that hardware speeds haven't 
been high enough and that's probably a cop-out to say now because hardware speeds are there for a reasonable 
size to medium-size problems that you want to solve. Now it's a question of just making the tools useable 
for that. How do you really interact real time? The hardware will compute it. For at least medium-scale 
simulations, the hardware will compute it in real time. So now it becomes a question of: what do you 
really want to see? How do we interact with it and that really hasn't been solved yet. There are people 
working on it. What is real time? In the 70s, the E&#38;S boxes were real time for what they did and 
still are. And now, we get better and better capabilities at least in terms of rendering power and processing 
speeds that we can accomplish more and more in real time. It will never be fast enough. Whatever you 
want to compute now, it's still going to be 1/10th of what you can get in hardware and whatever you want 
to do in 10 years, hardware will be 10 times, 100 times faster, but still not going to be as fast as 
you want. JIM BLINN: I had one interesting interpretation of the term "real time" at JPL during the voyager 
encounter. The scientists are looking at and analyzing the photographs and want some sort of image processing 
done. Their notion of real time is "overnight." That is to say, they look at something and it happens 
in time for the press conference the next day as opposed to having you take it back to their laboratory 
and study on it for 6 months. So you just have to pick the right people, have the right definition of 
"real time" and you're in great shape, but in general, real time simulations are wonderful. I think they're 
neat and everybody should have them and, you know, there are a lot of things out there that do that already. 
BILL KOVACS: I think what you're talking about is increasing the activity speed in general -- and I know 
that a lot of people in our little market niche are always trying to do that. mean, we've been in business 
for four years and we're only now just releasing a product where you have full control over your colors 
and can place it in a button, get a scene in a few seconds, modify the scene, modify the lights, quickly 
review. And again, that's one of the things that contributes to ease of use, so we're going to see much 
of it I think, by the various software manufacturers as the hardware will support. CARL ROSENDAHL: I 
have a question. Actually, Jim, I was thinking about )our comments and stuff and I think ... I'm a real 
fan of yours and I think that everyone at SIGGRAPH is an enormous fan of yours. The interesting thing 
to me is I think that you're a salesman and you're trying to sell education and you're doing a damn good 
job at it. The reason I love to watch your films is not because I think I'm learning something I didn't 
know before and being refreshed for something I learned before, but I find them really entertaining and 
I enjoy watching them. I think that, yes, visually, they're simple and you're communicating very clearly, 
but there is a real beauty in the way you have graphically shown what you did. You're not using just 
white lines on black. I think your use of color, drop shadows, nice letter forms is wonderful and I think 
that what you've done is taken the best of all the world's -- the best of entertainment, the best of 
graphic design -- and used it for an educational purpose, and I think you're doing a great job at it. 
But I think that in doing that, what you're able to do is sell education much more. I'd be interested 
in seeing the Fleischer film to see, you know, maybe it's just not visually entertaining. And I think 
from my standpoint, if I were making an educational film -- which I would love to do -- I'd want it to 
be visually interesting. I'd want it to be entertaining and something that's going to cut through all 
that dazzle and clutter, to be something that people are going to want to watch. JIM BLINN: I take it 
all back. I now see the error of my ways. I really think that advertising is a great idea and entertainment 
is what we should all do and we should make even vague science theories be as dazzling as possible. Actually, 
in fact, I got a copy of the Fleischer movie and it's incredibly boring to me. But you've got to have 
crude things before you can have sophisticated things. It helped advance the technology to some extent. 
I'm sure. STEVE ROOK: National Optical Astronomy Observatories. To me, the pursuit of science is both 
entertaining and educational and for instance, I would love to be able to take some of Jim Blinn's planetary 
animations and put on a 3-D, head-mounted display and fly through it in real time. This obviously, will 
take some time to happen, but don't you think that we can raise the level of quality of entertainment 
by bringing high quality scientific and educational things like that to the public? JIM BLINN: It depends 
on what you mean by entertainment. I mean, I used to be offended when a lot of things in entertainment 
films happened. Like a computer was represented as sort of talking English to the user and I'd think, 
"That's silly. They can't do that. That's not how you really talk to a computer." But really, the main 
purpose of entertainment is telling a story. It's not teaching you how computers work and if they actually 
had a computer within a movie displaying OS 360 job control language, the audience wouldn't really get 
the story very much. So it's the same sort of thing, as you know, you see a movie about ancient Rome 
and they're all speaking English --should you be offended at that? They didn't speak English back then. 
But you know, they'd have to translate it into terms that the main audience can see. What was I going 
to say? One other interesting ... put on the head-mounted display and fly around the planets, which is 
at first might sound like a neat idea, but one of things that I've sort attempted to portray in the films 
- which is rarely done in entertainment - is the fact that if we had a head-mounted display in this room 
and we put a model of Jupiter in the center and the moons and so forth, around there, it might not look 
nearly as impressive as you'd think because these things, relatively speaking, are very small and very 
far apart. So it's like you'd have a BB in the middle in the center of the room and a little dust mote 
out here and so forth. For example, if you're close enough to one of the moons of any of the planets 
where it would show up as a 18 disk, you're necessarily so far away from any of the others, it would 
just show up as a dot. There aren't any views where you see two moons show up as disks in the sky. Of 
course, that doesn't look neat, so entertainment people put several moons in the sky. So entertainment 
is for a very different purpose than education and sometimes if you try to cram them all in the same 
criteria, neither one of them might work. NANCY ST. JOHN: It seems to me that real time may be exactly 
one of those merge points for all of us because fight now, Carl and I working on a project with Jim Henson 
and we're doing a real time muppet that gets matted into some live action that's being done on a stage. 
It's not very educational, but it's kind of fun. And what's happening is, finally the machines are getting 
fast enough and the software's getting fast enough and we're getting smart enough and everything's kind 
of meeting at one point so that we can do this sort of thing in real time. Computer animation is not 
good at doing animation, and now we're able to take the skills of these very talented puppeteers who 
understand how these things should move and understand how to create emotion in a character. And we're 
able to just record that in real time as well as combine it with the live action as it is taking place 
and the puppeteer can automatically see what he's doing and how he's puppet is interacting with the other 
characters. And so what I'm saying is that we're finally being able to create what we want to do in real 
time. And I think for the scientists, real time is being able to explore within their data in real time. 
They want to watch their simulation going by. The want to stop it. They want to re-run it. They want 
to look at it. They want a close-up on it. They want to be able to interact with the thing that they're 
working with. And I think that's kind of what drives all of us. We'd like to have a much better response 
time and once the software and the hardware and the people are all smart enough all in one point in time, 
I think maybe that might be a merge point for us. And when do we see that? Maybe in the early 90s, from 
what I hear. Right? What do you guys think? JIM BLINN: It sounds good. We seem to be leading to the impression 
that real time is the goal NANCY ST. JOHN: That's not what I meant. I'm just saying, it's kind of nice 
and handy, but it's not the goal. JIM BLINN: I mean, I see it as a difference between ... in music --the 
difference between improvising and composing music and both of them are necessary. I don't like doing 
real time stuff myself because I don't think I'm particularly good at it. Basically, you have to perform 
the animation as it's seen. I rather like the idea of laying it out and fiddling with the timing and 
looking at it and saying, "No. This should be a little bit slower here." And fiddling with the timing 
again and so forth. So for me, real time is not that great a goal. NANCY ST. JOHN: Anyone else have anything 
to say for real time? BILL KOVACS: I had a chance to enjoy recently, some of this stuff that Disney's 
been doing with new technology, I mean, in their theme parks. And the kind of thing you were talking 
about with the head-mounted display, kind of reminds me of the new George Lucas fide there that really 
employs what is now very standard simulation technology. In that case, literally by the same companies 
that build them -- the flight simulators. And I think what's interesting is the transition that's going 
on there. If you look at some of those tides out of Disneyland, you're noticing a transition to a lot 
more use of other technology and media as the preferred way of taking people into a different reality. 
NANCY ST. JOHN: Well, I'm afraid I have to wrap it up. I'm getting the high sign. I'd like to thank you 
all for being a wonderful audience and for being with us here, today.  Carl Rosendahl     
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402247</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>59</pages>
		<display_no>5</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Computer graphics and the changing methodology for artists and designers]]></title>
		<page_from>1</page_from>
		<page_to>59</page_to>
		<doi_number>10.1145/1402242.1402247</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402247</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101771</person_id>
				<author_profile_id><![CDATA[81100533681]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alyce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaprow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The New Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101772</person_id>
				<author_profile_id><![CDATA[81100316584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slayton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cadre Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101773</person_id>
				<author_profile_id><![CDATA[81100431703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Souza]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[WGBH Educational Foundation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101774</person_id>
				<author_profile_id><![CDATA[81100410454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haimes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COMPUTER GRAPHICS AND THE CHANGING METHODOLOGY FOR ARTISTS AND DESIGNERS Chair: Alyce Kaprow, The New 
Studio Panelists: Joel Slayton, Cadre Institute Paul Souza, WGBH Educational Foundation Rob Haimes, Cambridge, 
MA COMPUTER GRAPHICS AND THE CHANGING METHODOLOGY FOR ARTISTS AND DESIGNERS ALYCE KAPROW: Good morning. 
It's a pleasure to be here. A couple of business announcements before I begin. This panel is going to 
meet until about 12:15, I believe it is, and then if there is time or the need for further discussion 
afterwards, there will be a breakout room in 303. We are asked to leave this room immediately afterwards 
so they can prepare for the next panel. This panel is about process, not product. However, we will be 
showing a lot about products, a lot about the things that we all do or the work that we're all involved 
with. It's about art making and design. And as you see Charlie Chaplin on the screen, what we don't 
want is the process to manipulate us -- we want to manipulate the process. I want to take a little 
bit of a survey and because these lights do not permit me to see hands raised, I'd like to use the old 
applause-o-meter. How many of you in the audience are artists or designers -- that does not exclude industrial 
designers, architects, graphic designers, any kind of designer that you would want to put into that category? 
Clap, because I can't see you. (APPLAUSE) How many engaged with computer graphics in any form? (APPLAUSE) 
 How many of you are engaged in the development of the tools? (APPLAUSE) How many of you are not engaged 
in the development of the tools? (APPLAUSE) It's about equal. And how many of you are engaged in the 
marketing of the tools? (APPLAUSE, VERY SLIGHT) --Thank you for coming. You're very brave people. We're 
talking about tools and we're talking about process. As with any tools, we come to the computer with 
a lot of experience and a great deal of opinion. We do not come to this area bfind. The computer works 
differently than the traditional environment, and therefore it gives us different kinds of options. 
It both expands our horizons and limits our choices. What we need to do is understand how it is different 
and how these things can change. From observation and from hands-on experience, we form our new universe. 
We need to understand each other's needs. Developers and users must take time to have a dialogue, to 
understand each other's fantasies. This effort is begun by understanding the process. There's a great 
deal of confusion on the part of developers as to the difference between art and design. Most of the 
systems have been developed for art making -- most of them have been done for picture making, which is 
a very limited spectrum of art making. And now they're being marketed for design. I'm not in any way 
condemning this effort. I'm simply saying that we need to understand those differences. I can only touch 
upon key points. I cannot go into many of the differences -- I don't have enough time to expand these 
thoughts during this panel session. Both art and design are predominantly non-linear. They both rely 
on a personal model of the universe based on experience and taste. If you were to quickly take a look 
at the steps involved, you might say that it was a neat linear flow of activity -- it's not. (refers 
to slide on screen of six- step flowchart) The way that art and design is done is more like that. (refers 
to slide on screen of chaotic flowchart) It's layered, it's chaotic, it's crazy, it's multi-directional, 
and somehow at the bottom comes out a product. That's the way visual people think, that's the way everybody 
thinks. We can't put it into a linear world. The very nature of the activity, its seemingly chaotic and 
layered manner, is the very reason it is successful. The act of art-making and designing is simply not 
one that can be described in a "neat" fashion. It needs elements of disorder and serendipity to allow 
the creativity to grow. Art-making is singular and personal, it's controlled by an individual, it uses 
many media. Design is a singular act with the anticipation of others taking the control. There needs 
to be previsualization and choreography of all steps, understanding the ripple effect of every single 
choice that is made that the designer will no longer control. Production relies on many people to carry 
out the work. They all follow someone else's directions and thoughts and many times production graphic 
artists work in isolation. Different speciafists perform different tasks, all coming together in the 
end at the printing stage. These categories all work together, in concert and sometimes in conflict. 
Many tools and the materials are similar. They often appear the same, and therefore they are assumed 
to be the same. This is where a lot of manufacturers make a mistake in designing computer tools for the 
particular practitioners. The fact is is that these tools may appear to be the same in each of the categories, 
but they are very different in application for one specialist to another. What we need to understand 
is that very reason why some of the tools work in one or two of these categories is the very reason why 
they fail in another category; and that's not bad. The purpose of this panel is to discuss the nature 
of the process. If we understand .the process, as complex as it is, we will then be able to understand 
how to grab hold of the tools we use and to follow our fantasies. Each person on the panel can speak 
for three to six days, let alone 15 minutes. There is no way that we can easily sum up all of our thoughts, 
but we're here to try to touch upon the peaks and not try to go into that much depth, unfortunately. 
So I will not try to make a lengthy introduction to each person. They will introduce themselves as they 
go on, and they will speak briefly about their ideas. I will return with concluding remarks. As I said, 
we can continue this a little bit afterwards with questions and answers. 2 The first speaker is Rob Haimes. 
Rob started out in print-making as an artist, as a photographer -- I do not exclude one from the other. 
He went on to work at the Visible Language Workshop at MIT, and from there worked with SCITEX in product 
development and marketing. He is on his own doing consulting in these fields. I introduce Rob Haimes. 
ROB HAIMES Thanks. I want to make three main points: The blurring between design and prepress (production). 
This is a result of the automation of the design process and the efforts being made by both design system 
and prepress system vendors to extract elements from the design for use in production. Second, the distinction 
between design and art creation, and for the purposes of my talk, I'm going to arbitrarily define art 
as illustration and photography used in print. Third, the distinction between art creation and art processing: 
the creation of art for use in print versus the final processing of that art so that it's color corrected 
and prints well. And I also want to talk about some trends. This slide represents the traditional process. 
The designer creates thumbnails, roughs, and often extremely detailed comps, which are used to get final 
design approval from the client, and then these are essentially thrown away, and the process starts over 
again with the mechanical, the first stage of production. This is where it's evolving to, hopefully, 
with electronic comps that contain information which can be used directly in production, extending the 
control the designer has, and allowing design decisions to be made very late in the process, without 
costing a lot of extra money. To give you an idea of the kinds of problems designers encounter, I want 
to show a few traditional comps- comprehensives, if you're not familiar with the term- created for approval 
purposes only. Here's a fairly rough comp done for Friendly's- it's shown to the client, and the client 
says, well, I really.like the concept, but can we see the background in pink, and the entire comp has 
to be redone. Another comp. The illustration in this case is used to represent the photograph that will 
be taken later. It was, in fact, created by a freelancer outside the agency, hired at some cost. The 
type was set, even thought it's unclear at this point what the actual copy is going to be, except for 
the headline. They've gone to significant expense, and a lot of time was taken to get to this point. 
An extremely fight presentation, which looks almost identical to the printed piece. If the scale of the 
photograph isn't right, you have to go back to the photo house to get a new one. It was all pasted together; 
type was set. The little colored line art elements are created first in black and white, and then sent 
out to get converted into what is essentially custom transfer type. 3 This slide shows the state of the 
art in the early eighties. There was real confusion among design system vendors. In fact, they weren't 
called design systems at that point; they were called paint systems. Pages were thought of as paintings, 
rather than integrated assemblies of separate elements. Everything was pixel information. Revising the 
page was difficult. Even the type was fused to the image and had to be erased before the design could 
be changed. There was also a lack of understanding of where art is created versus where design is done. 
Vendors were trying to sell what were essentially art creation systems directly to designers, while in 
almost all high-end design, art is purchased from some outside source, usually a freelancer. If an agency, 
for instance, is handling a drug account, they're going to go to a specific kind of art creator with 
a specific style that corresponds to the message being conveyed. If they're doing a toy ad, they're going 
to go to a different kind of artist. They want access to a range of creative styles, and it's very unlikely 
that they're going to bring a system inhouse to do art. This is a list of elements making up a typical 
design. On a system, the continuous tone images- photographs and tonal illustrations- have to be handled 
as pixel information. These can be huge files: 32 bits of color and 300 pixels per inch of detail. Page 
geometry, line art, and type, are all essentially object-based, and can be handled very efficiently. 
Control information- scheduling and job tracking data should also be included with a design. On a low-cost 
system, with reasonable measuring tools, a designer can lay out a page with a great deal of freedom, 
but the location of each element in the final design can be sent to a production system with absolute 
accuracy because it's numerically based. The system only has to deal with the basic coordinates of the 
page, where each element is placed, and outlines and color information for the line art and so on. For 
pictures, the designer can make basic cropping, scaling, and placement decisions on low- resolution proxy 
versions, and the high-end system can replace these with print resolution versions, and deal with critical 
color. Here is an example of design done on a Lightspeed system that illustrates how the page is composed 
of separate elements. All of this information can be captured in very efficient form, but more critically, 
in terms of design, these pages are revisable. This is a sequence from around 1984 showing a Scitex Vista 
system doing a Time magazine spread. This system provided one of the best models of what a design system 
should do. You can bring a picture to the screen, and zoom and pan in real time to make your cropping 
and scaling decisions. You have access to rough grids when you're looking at the overview of the page, 
and you get finer detail as you zoom in. You don't have the image cluttered up by the fine grid when 
you're working on a global level. This shows quick masking of an image- dropping the background out. 
You can see that the edge is rough, and the picture is low res. The designer is making the basic decisions 
of how the image works on the page and how it affects the type flowing around it. Later the production 
person uses this outline as a guide to do the critical cutting of the high resolution image. Unfortunately 
the Vista cost several hundred thousand dollars. But what's exciting now is that there's no reason why 
this level of functionality can't be implemented on a Macintosh, and in fact, it's starting to be. The 
Mac is changing the industry tremendously. I would say that two years ago, between all the mainstream 
design system vendors, there were maybe a hundred systems out there being used for print design. More 
than that were bought for that purpose but got shunted into slide making or video operations. At this 
point there are thousands of systems being used, and the vast majority are Macs running something like 
PageMaker or Quark. 4 The Mac has some disadvantages. There is a lot of do-it-yourself integration going 
on, which is OK for pioneers, but probably not for mainstream users. And software like PageMaker is fine, 
but it's really publishing software, and it's not perfectly matched to the requirements of many design 
tasks. But there are some real advantages. The Mac is cheap, relatively, so it's a low risk machine, 
and it's a standard platform: lots of software will run on it, with reassuring similarity of interface 
between packages. It gets you closer to final form, or in some cases gets you final art, if you go to 
a typesetter. In what I would have to characterize as a few experimental cases, you can go directly to 
a prepress system using the kind of interface I've already described. And there is an emphasis on text, 
which can be a big problem for designers because of how changes in the copy can change the design. This 
is an example from Ron Campisi, who was the designer for the Boston Globe. He's now using the Mac exclusively 
to redesign mostly corporate publications, but also newspapers. Design technology is improving, but unfortunately 
what's available doesn't reflect reality. The basic design process as it's understood by most vendors: 
thumbnail to rough to comp, etc. is, in fact, overly linear and idealized. Even if you add approval cycles, 
it's rarely like this. This slide gives a truer picture- far more complex. In a real design environment 
designers work on multiple jobs simultaneously, all at different stages of completion. They need access 
to multiple image and text elements, which may be used in a range of related jobs- ad, point-of- purchase 
piece, and direct mail, for instance- which may be produced by different people. They also need access 
to samples of previous work, or competing designs, or versions of the current design in progress. You 
need workflow and accounting controls. You need to know where the job is and when it's due at the printer, 
and how much time you've put in for a particular client. And the system has to be integrated with whatever 
other equipment exists in the environment, which can be input or output devices, text composition systems, 
or CAD or prepress systems. I want to show a few examples of art creation. Again, this typically takes 
place outside the design environment. It's a harder market to sell to because it's much more diffuse. 
And computers will continue to be only one of a range of available art media. Many of the image creation 
systems were developed originally for TV. There's a nice correspondence between the resolution required 
for TV and that which was available on these systems. When you get into print you need a lot more resolution, 
as I said, typically 300 pixels per inch. If the image is going to appear small, that's not so much of 
a problem, but if it's going to be big, then you're dealing with a file size that's beyond what most 
systems can handle easily, and is certainly beyond what can be displayed all at once. These early low 
resolution images looked like they were created on computers, and the only images like this that ever 
appeared in print for a long time were illustrations for articles on computers. The computer look could 
be exploited. More recent systems, like the Quantel Graphic Paintbox, have been marketed specifically 
to do art creation for print. The Quantel can handle images around 2000 pixels, in full color, with a 
full range of very fast, flexible tools. It allows an artist to move around very quickly in a larger-than- 
screen data space. This freedom of movement is essential, and is lacking on high-end prepress 5 systems, 
which causes them to be thought of more as systems for production rather than as creative tools. As with 
pages, one way of getting around resolution problems in illustration is to use object- based, rather 
than raster-based tools. This is an example from a system originally developed for the presentation graphics 
market, Autographix, that shows how an image can be defined very efficiently as shapes and coordinates 
and color values, and can be passed at any resolution to an output camera or to a prepress system. The 
Mac has changed things in this area as well. This is from USA Today, which is doing just about all of 
the illustration on the Mac. This cow is an extreme example of object based art, done on another presentation 
graphics system, Genigraphics, now being targeted at this market. In addition to resolution, the other 
issue is image processing. Once everything is where you want it in the image, how do you get it out of 
the system and into print. This is an actual piece of a color separation that I cut out and mounted, 
and this is just one of four separations that make up the full-color image. Pixels need to be converted 
to halftone dots, at just the right angle to avoid moire patterns when colors are overlaid, with absolute 
control over the dot shape throughout the tonal range, and with problems such as the spreading of ink 
on a particular press using a particular ink and paper combination compensated for. In addition, a lot 
of color correction has to be done so that the final printed piece matches the transparency, or product 
color, or whatever the client thinks it should look like. A lot of what is done on prepress systems like 
Scitex, Hell, and Crosfield straddles creation and production. Most of what's done is simply cosmetic 
retouching, like this example of the couple on the beach- airbrush out the footprints so it looks pristine. 
But increasingly these systems are being used to extensively manipulate photographs, or to combine images; 
to solve what are essentially creative problems, or to convey a particular idea. So they can also be 
considered image creation tools. Here are some examples. The problem is that these systems are really 
production oriented; are extremely expensive and difficult to use. They are not systems an art director 
is likely to feel comfortable with. So communication between the operator of the system, who's a production 
person and is from one environment, and the art director, who's in a different environment, and who uses 
different terms to describe the desired effects, is likely to be difficult. The problem is one of art 
specification. This slide is of a first proof of a raw scan into a prepress system, marked up by an art 
director. This is a very expensive cromalin proof, produced from separations output from the system. 
This is a number of proofs later, a press proof, meaning separations have again been made, a plate was 
burned, and this was printed on a small press, to get an ink-on-paper proof. The contract proof for most 
national circulation ads is a press proof, because it's obviously the closest to what the ad will look 
like in the magazine. But you'll notice that most of the art director's comments are not about critical 
color correction. Rather, clouds are still being moved around. In other words, all this time and expense 
were gone to and subjective, creative decisions are still being made. Even a low resolution design/paint 
system can be used to allow the art director to make these kinds of decisions up front. The designer 
does this initial combining of images, getting the 6 elements in just the right relationships, and outputs 
a proof which serves as a guide for the operator of the expensive prepress system. The cromalin proof 
comes back with everything in the proper place the first, or maybe the second pass through the system, 
rather than having to go through all those iterations. These are examples from John Waters' studio in 
New York. This is what he put together on a Lightspeed system, and this is a shot of the printed annual 
report. I'm almost out of time. I just want to run very quickly through several emerging trends. First, 
there's obviously a lot more technology. The design and art creation markets are being converged upon 
from both the high end- prepress- and the low end- desktop publishing. The market is fragmenting, with 
focused systems targeting specific applications. There are new, relatively inexpensive scanners for 35mm 
slides, which is most of what's used. There are new output devices, for both proofs and final film. There 
are a number of attempts to lower the cost of color separations- both system and software-only solutions, 
although quality is so far nowhere near that of a high-end system. Second, there are a lot of changing 
business relationships and new business opportunities. Designers are dealing with both clients and their 
suppliers in new ways. This is another slide from Ron Campisi. Instead of finished designs, he supplies 
his clients with templates- formats- in PageMaker, which they use to do the actual layout. Companies 
like The Image Bank, the largest supplier of stock photos, offer libraries of images on videodisc, which 
could be used very effectively with a design system. New system-based art creation studios are opening 
up. One of the most successful is in Hollywood, Electric Paint, doing mostly promotional work for the 
film industry. If a movie isn't doing well and needs to be repositioned, they can turn this work around 
very quickly using a Quantel Graphic Paintbox. For clients, new separation technology will allow very 
specific target marketing. Prepress can be a si .gnificant percentage of the total cost of a job, so 
it has to be justified by printing lots of copies. Significantly lowering the cost of prepress will make 
short run printing feasible. Third, multimedia and hypermedia will play a much larger role in mainstream 
design. Images created on CAD systems for a proposal will evolve as a product evolves through development, 
training, and sales and marketing, and in each phase these will be supported by print or incorporated 
into print materials. This is an example from a Contex system. Container shapes can be created, or input 
from CAD systems, and the package graphics can be wrapped around them. Paul will talk about a Hypercard 
project he's working on. Finally, although AI has had less impact on design and art than it has had in 
other disciplines, there will be greater influence in the future. There was some work done at the Visible 
Language Workshop at MIT using some simple rules to generate possible cover designs for an inhouse magazine, 
with the system working, they emphasize, in partnership with the designer. But these were very rudimentary 
rules having to do only with the formal aspects of the images being used. A lot more work needs to be 
done on dealing with the content of design, both for efforts like this, and for projects like The Image 
Bank videodisc I mentioned, so there can be more subtlety in the search process. In the meantime, Bitstream 
is using some intelligence in the generation of bitmap type from outlines. Scitex has a SmartScanner 
which knows something about the variables of printing, and can learn more from how images are subsequently 
used, working with feedback from the main system. I think a lot more can be done by adapting image enhancement 
techniques from other fields. And several companies are working on "smart" templates. 7 In any case, 
it's clear that within 5 years (and I think this is a conservative estimate because this is a very long 
time), computer-based tools with be absolutely commonplace in design offices. The next speaker will be 
Paul Souza, a designer at WGBH television in Boston. PAUL SOUZA: I work in both print and video environment. 
As a graphic designer I have found computers useful in bridging the gap in both media. I'm going to talk 
about my personal experience with computers over the last eight or so years and see what changes we've 
had as far as methodologies. I will talk about three milestones all related to the NOVA program series, 
which you may be familiar with. First the NOVA open. My primary involvement was in developing the storyboard 
and then going to New York Institute of Technology where I worked with David Geshwind, who was a staff 
member at NYIT. It was a very odd, unusual experience for me in that I was fascinated with computers. 
But when I had the opportunity to see the programmers working, I had no idea what they were doing. If 
you can imagine watching over someone's screen and it looked absolutely Greek -- C code is not a designer's 
dream. But the images that they we're able to create in this environment were absolutely phenomenal, 
beyond anything I could have even imagined in my storyboard. And it opened up to me amazing new horizons 
of what was possible in creating images. Unlike any dream that I've had before, details and visions just 
dancing on the screens, after waiting 20 minutes or more for each one to happen -- after weeks and weeks 
of programming by David and others at New York Tech, we were able to create a series of images which 
when linked together, produced a very satisfying animated sequence. (video shown) Later when we were 
producing a book for the NOVA series we learned the lesson that the images we created on that system, 
if we wanted to use them in print, weren't easily convertible. It seems that the digital information 
could easily be transferred to a Scitex system. Unfortunately, at the time, it was not possible but we 
could use a Scitex system to grab film output, slides of these images, and combine them in this way to 
create a similar image for a book we were producing. It's gone a long way since then. I think Scitex 
now can read images directly from digital data. Next thing I want to talk about what is to me the next 
milestone, and that's personal computers getting involved. A project that to me sort of crystalized that 
change in the way of working was the KnowZone open, a thirteen part series re-packaging NOVA programs 
for a younger audience -- we called it KnowZone. I had the task to produce the open for it, and for the 
first time tried using Macintosh tools to make a 3D object which I could then manipulate and from that 
make a motion test for previewing, planning and showing to the client. 8 This was actually amazing to 
me that I could plan very complex 3D motion using these tools.When I showed this motion test to the animator 
with the storyboard they assured me that they could give me a much better price for producing the animation. 
I was able to do is take the frames and give them a frame by frame motion preview for producing the animation. 
Then I went to Coddbarrett Associates and they used a personal computer, an IBM PC/AT with Cubicomp software 
to produce the animation. (video shown) The interesting thing about that is they produced for us the 
3D elements with a black matt area that we composited other elements that were produced using again another 
IBM PC system, an Aurora 220 system. Images, video and other elements were composited in post-production 
to create this elaborate image. It's amazing to me what in a few years personal computers have allowed 
us to do. The next project is sort of coming back to the other idea of using computer tools for my broad 
application in both print and video. Most of what I end up doing is print -- things like proposals. This 
is a proposal we produced on the Macintosh with PageMaker, three hundred pages, for a notion of doing 
interactive Nova programs -- video disks controlled by a computer. Using PageMaker allowed us to do a 
very elaborate proposal with modest means. This proposal got us enough interest that eventually Apple 
funded a design example, which I can show you right now. The capabilities of using the Macintosh for 
making elaborate print materials at a low cost is really phenomenal -- I think that we're only beginning 
to understand what the potential is. We're now developing a design example of that product called Interactive 
NOVA. Multi-Media using a HyperCard as the controlling device for a videodisc player. The idea is that 
you would have in classroom situations a Macintosh controlling a videodisc with a monitor, where you 
could interact with the monitor and computer screen.You could look at the Nova film for example. Then, 
look at the entire film or parts of the film. Do activities, like field studies, etc., look at certain 
resources or maybe go look at some mini-documentaries, and launch a video sequence. (video shown) And 
if you are bored and let's go to somewhere else -- go back and interactively control those segments, 
or explore the data base, where you could look at animals, habitats and behaviors. Let's say you want 
to know something about insects or birds or mammals -- let's say you want to know something about Rocky 
Mountain Elk -- well, go to the data base in Rocky Mountain Elk, and read something about Rocky Mountain 
Elk. Or we have imbedded film clips. Let's say you've done that search and you want to know something 
a little more specific about Rocky Mountain Elk, so let's say you want to know something about a particular 
habitat, not the California Highland but only in the Rocky Mountains, and let's say something about their 
communicating behavior. 9 (film clip) So we will do a search and find only the special subject cards 
and corresponding film and video clips and the video disk about that... (film clip) You get the idea. 
To me, it's just opening up the possibilities because for the first time I am working on designing the 
way someone interacts with a computer. It's not just using these tools to create images for print and 
video. It is now working.with the computer to establish the way other people will learn from the media. 
It's no longer just print a video but come to a synthesis. To me this is very exciting. It's the egg 
of a new development, it's a shining new bright future. JOEL SLAYTON: Thanks, Paul. Computer visualization 
techniques have dramatically challenged traditional means of communication and consequently forms of 
personal expression. The manner in which we learn, educate and represent ideas has grown more encompassing 
with every advance of media technology. In computer graphics alone, three dimensional modeling, image 
processing, animation, simulation techniques and real time dynamics combined with interactive processing 
are simply redefining how and why of perception and visualization. Many artists, like myself, have responded 
to the emerging media technology with both enthusiasm and a great deal of skepticism. Technical possibilities 
appear unlimited, yet every advance gives rise to difficult aesthetic issues regarding the nature of 
style and content, and it is precisely style and content to which the artist must be concerned and be 
responsible, and thus issues of changing methodology arise. I want to take my short amount of time to 
introduce three specific projects that are in progress. I do it really from a personal point of view, 
not necessarily to advocate any specific approach to the technology or application of the tools. The 
first set of pieces that I'd like to show are some sketches for images that I'll be presenting in an 
exhibition this fall at Santa Clara University in California. These images are intended to be seen on 
monitors in a gallery environment, not as slides or print works. They are produced on low end PC based 
graphics system utilizing very simple software. They combine video with the techniques of frame grabbing, 
additive, subtractive, image manipulation. What is attractive to me primarily about working with this 
kind of technology is that it's easy, it's very straightforward. Anyone can do it. I like low resolution. 
I like limited color. I like fuzzed out fonts. And I like bad graphics. I think they're interesting. 
These kinds of tools are reintroducing the idea of -- in art, anyway -- the notion of multi-media, the 
idea of being able to work across all disciplines. This kind of graphic software is a visual 10 synthesizer 
of sorts. All media is the media. You can be a painter, a videographer, a film- maker. You can work in 
type, you can write poetry. I find that this idea of low end, high end equipment somewhat erroneous and 
not to the point at all. You work with what you have. If you have high end then you pursue it to its 
maximum, if you have low end you do the same. I'm not sure that the technology has anything at all to 
do with the complexity or significance of the imagery that's produced with it. (APPLAUSE) The next piece 
that I would like to show is a research project in progress. We call it Sophie or another word for it 
is Hand. This is an articulated three-dimensional dynamic model of a human hand for interactive control 
of virtual environment spaces -- sounds interesting -- sounds complex and in fact is. This is supported 
in part through the cooperation of the NASA Ames Research Center, Life Sciences Division, specifically 
Scott Fisher's Laboratory dedicated to developing headmounted display technology. Hand is a collaborative 
project involving the expertise of Calvin Chan, a systems and graphics programmer and recent faculty 
at the CADRE Institute of San Jose State University, Lisa Johannson, a research scientist from the Orthoepedic 
Biomechanics Laboratory at Shriner's Hospital in San Francisco, and myself. Hand came about through a 
sort of interesting relationship with the NASA Aims Research Center. I was aware that they were developing 
the headmounted display technology from my association with Scott from past experiences and having gone 
out to visit Nasa and see what they were up to. I found out very quickly that they were developing a 
technique for basically transmitting reality, transmitting presence from one location to another. And 
the idea of building three-dimensional realities and being able to send them in some form or another 
to a helmet in which you'd actually enter into these spaces and move around and investigate them, I found 
very attractive. That interests me a great deal, the idea of being able to create an artificial self, 
so to speak, that could be inside these environments at the same time stimulated our beginnings of the 
research. I'll show this very briefly. This is a robot that was developed by Calvin that basically could 
see and hear, and you'd send it out in the hallways bouncing around and it would transmit data back to 
the helmet so that you would be able to see and experience what the robot was doing. This is the kind 
of approach that NASA is most interested in, I believe. A company called VPL had developed an interactive 
glove device that you could wear and connect to a computer that would allow you to interact with the 
displayed imagery. This glove device could be placed on your hand and it would measure the bending of 
specific angles of the joints in your fingers, and that could be transmitted into a very simple model 
inside the display environment. The problem was that the model was very simple. We decided that it would 
be appropriate to develop a higher resolution model of the human hand that would seem like yours and 
would allow you to interact with the glove in a more articulated way. 11 So we went about building a 
cosmetic prosthesis, digitizing it with a three-dimensional digitizer in a fairly specific way, lots 
of experimentation, trying to figure out how that thing works because it wasn't clear to me at all, and 
finally arriving at a fairly articulated model of the hand in which all the vertices points are strategically 
located, either for surface definition or for rotation about certain axis. That information could be 
split into stereo display and sent into a helmet device. I'm going to show a short tape and leave the 
slide on because it really tries to illustrate the points that I'm most interested in. Can we show the 
first tape, please? This is a simple view of the hand and its structure. I kind of like the pointed finger 
tips. Now all this is done in real time. Here's an environment that was designed so the hand can recognize 
certain classes of objects. Of course, you want to be able to do things that all animators can do, like 
spin things around and do all that at the same time. Here's the helmet display and hand glove. If we 
pipe the data for the hand into the helmet display, it appears I'm inside my own hand flying around. 
Certain gestures allow you to navigate within the display space, so pointing your finger allows you to 
travel across the room, rotating your hand allows you to do somersaults, which is kind of fun. Here's 
a simple view of the hand moving through the environment. Now this is the hand that was developed by 
NASA, and you can see it's very block-oriented. Now we're actually inside Sophie's fingers, flying to 
the front tips of her fingers and then we'll do an about face and fly back. Well, maybe I sound a bit 
confused about it. I guess I am. I don't really know where this project is headed. I don't have a specific 
application in mind. There are lots of potentials for it in a variety of forms, one of which would be 
the animation, the other might be biofeedback for biomedical purposes. The idea of simply building environments 
that you can enter into and fly around and touch things and investigate things represents a whole new 
form of entertainment and broadcast potentials. We'll just have to see how the thing develops and where 
it goes. One thing is absolutely clear to me however, and Paul addressed it somewhat. For the first 
time, I find m.yself desig.ning based on the interactive issues, not simply on the visual ones -- how 
a person is going to interact with data, sound, pictures and text becomes all important. Up on the screen 
on the right, you have kind of a prototype image of one kind of environment in which the hand might interact. 
The idea of books, the ability to touch and reveal more information, investigate by feel, and another 
kind of environment in which one might enter into a transparent piece of architecture and begin investigations 
about its interior. These kinds of things are going to open up a whole range of new possibilities. 
The next project that I would like to introduce is called Zerone. It's an experimental video and music 
composition. It was commissioned by Tash for New Music -- Music in Foon -- Festival in Stucer, Germany. 
This was done in collaboration with Daniel Wyman, a film composer and t2 director of the Digital Recording 
Studio at the CADRE Institute, and Yorgen Branniger, Professor of Electrocustic Music at the University 
of Natal in Dergen, South Africa. Zerone is a 23 minute music and visual composition. First, natural 
sounds of voice were recorded in rural South Africa .... pronouncing high tech words were combined with 
sounds of insects and animals as well as the environment. In San Jose, various digital sampling, processing 
and recording techniques were employed to reconstruct the recording into a very complex musical score. 
The composition was then mapped according to instruments, voice, and frequency modulations to provide 
a guide for storyboard individual sequences. Unfortunately, that's a bad slide. Seven animator programmers 
were assigned to specific tasks to create a range of possible solutions to the desired sequences, utilizing 
a Ridge, mini computer and RasterTech display processor, the sequences were designed to run computationally 
in real time for direct video recording. Eight hours of tape were generated in six weeks. None of us 
had ever made any animation before, by the way. The visual sequences were then selected and edited together. 
The final product is 23 minutes and has over 400 production edits. These were synced to match the rhythm, 
tempo and modulation of the soundtrack. Zerone is non-narrative. It doesn't have a story. It's more like 
a dynamic piece of concrete poetry. At times the sound takes predominance over the visuals and at other 
times the visuals take predominance over the sound. It is in four different languages simultaneously 
and mixed. The piece was first premiered in Stuttgart and since then I've had considerable time to reflect 
upon it and think about where to take it next. It's now being re-edited for interactive videodisc through 
Hypercard interfaces. The user will be able to construct the visual acoustic sequences independently 
and interfere with, or redirect, destroy the nature of the very playback. Could we have that tape, please? 
I'm going to show about two minutes of it. (TAPE SHOWN) (APPLAUSE) Thank you. Just to summarize a bit 
here -- with the proliferation of new media technology, traditional artistic methods are certainly to 
be challenged, permanently transformed, replaced and most likely discarded. It's simply the nature of 
things. On one hand computers have made art easier to produce and more accessible. On the other, some 
of the art ... are collective, complex and difficult to appreciate. The ongoing attempt to break new 
ground technically and aesthetically is likely to continue at all levels. These three examples I hope 
show a rawness, non-slickness, a directness of approach and a sense of experimentation. That's the methodology 
that most interests me, and it has very little to do with the tools. Thank you. 13 (APPLAUSE) And I'd 
like to reintroduce Alyce Kaprow.  ALYCE KAPROW: Computers are objects to think with. They're not necessarily 
only objects to do things with. The computer is a tool with which we analyze problems. It is also a machine 
that contains tools that create solutions. The computer provides us with a way to think differently. 
It gives us new processes with which to explore. It gives us new processes with which to do our work. 
It's a double sided tool. It opens our minds to thinking in new ways, to exploring, and it brings to 
fruition our ideas. Obviously if you're at SIGGRAPH you're at least somewhat convinced of this thinking, 
so I'm up here preaching to the converted, but for the most part I think it's important to sometimes 
state why this is true. We often take the radical power of the computer for granted and as a result, 
we often lose sight of its magic. We fall into the trap of making it commonplace because we do take it 
for granted. Computers are unique tools. They're virtual tools. They can be anything we want them to 
be. They emulate the old, they create new, they synthesize the old and the new with opportunities to 
reorder. Computers are non-physical. We are no longer bound by what we have known always as the real 
world. We can create our own universe. Computers support risk. They allow us to take the opportunity 
to create ideas and develop concepts, and to experiment. They store and retrieve and they give us the 
flexibility to try alternatives and manipulations. They give us the freedom to make bad choices, to make 
mistakes, to make ugly products. We can go and try it again without having to worry about redoing every 
single step or taking so much time. Computers compress the work flow. They give us the opportunity to 
try things. More importantly, the time it takes to do the work is compressed, so the action, the process, 
keeps up with our creative thought. That's a very important part of what the computer can do for us because 
often it takes so long to do things traditionally. Computers are non-linear tools, or at least they are 
not as specifically structured as we have known tools before. They give us the opportunity of random 
access. The coming together of diverse tools and processes under one structure permits synthesis and 
the ability to combine and create. Artists and designers need that nonrlinearity and that non-structure. 
The process of art and design is evolutionary, layered, non-linear, seemingly chaotic. The computer creates 
new models of our universe from which to explore solutions. The computer allows us to order our universe 
differently, to unite tools, to give us new ways to solve problems. The computer helps us think differently. 
They are objects to think with. 14 It is very important for the manufacturers and users to work together 
in developing these products. Non-practitioners of these toolboxes can only begin to understand the applications 
and the subtleties of tool manipulations. Without users to share some insight, the software will remain 
locked in the mentality of the demo, not in the reality of actual use. And we'll never understand how 
far we can push those tools. There must be a dialogue between the developers and the users. And it has 
to be a flexible fluid dialogue, based on mutual respect for each-other's needs. I'll show a couple of 
examples. The computer can draw a circle. How many times have you used a computer circle that is based 
on the center, its origin? Well, as a designer, there's two ways commonly used to draw a circle. One 
is by using a compass -- that's based on a center. Oftentimes I don't want to use the compass because 
I have to use the circle to be inscribed in something or to inscribe some object or shape, so I use a 
template. Now any computer tool that does not understand this difference and limits my choice to the 
circle origin method limits my activity. Another example is the pencil. There are many many computer 
functions that mimic either a pen or a pencil. I'll show you the examples of a number of them. (series 
of slides) Each that I've chosen displays a different use of the same kind of tool, that is a drawing 
tool -- pencil, pen, whatever -- each one of them defines those tools differently. But, it is the artist, 
the user, the creator of the art in each particular case that has gotten around some of the limitations 
of the tools within the system, because the pencil has often been defined simply as something that connects 
two dots and that's all. And so we have to understand how a pencil is held, how the artists manipulate 
that tool. I'm using the pencil because we all can relate to it. I am not limited to this function. Any 
tool can be described in the same manner, anywhere from electronics to paint. All of these (slides) show 
a different idea, a different personality, a different universe from which to make these products. We 
have to understand the functionality of the tools. I can easily say at this point in the industry's maturity 
that if I were to take a very large room, somewhat the size of the exhibit hall downstairs, and load 
in all the available computer equipment to do everything I ever wanted it to do, I could probably get 
any one single task done. So the functions exist. Tasks and process are possible. But there is something 
missing. The functionality of those tools is not necessarily so guaranteed, and certainly the environment 
that is comfortable and accessible to me is absolutely not guaranteed. We have to create an environment 
that will make it work. An environment that is friendly and accessible. One that will invite artists 
and designers to explore and fantasize, and to combine ideas with those of non-artists and non-designers. 
Tools must be intuitive, new and exhilarating, visually exciting and inspiring, responsive, challenging 
-- they must teach us to think differently. An appropriate analogy might be a squash game. You go into 
the squash court and you play squash the very first time. You can hit the ball around and it comes back 
to you and you're delighted. Every time you get in there again, you get better and better at it, and 
the act of playing the game teaches you to play better. Because if you like doing it, you will learn 
how to be better at it simply because it is such an engaging activity. That's the way tools should operate. 
They 15 shouldn't be linear and so specific that we can't use them to begin with or we can't learn from 
them in the end. The tool should invite the beginner into the court and encourage experimentation and 
play; but in a way that teaches us to think like it performs, and to think about the process. Computers 
have to be productive and effort effective. They have to be elegantly interactive, they have to be fun 
and seductive. So many times people developing the tools are fascinated with the technology and the elegance 
of the programming that makes the tools work in the first place. What is missing is some understanding 
of the application of the tools by practitioners. Programming elegance is an important and creative endeavor, 
but the most important condition of the eventual tool is that it makes sense for the intended user, not 
the programmer. In fact, these sexy programming tricks often do nothing more than take up precious resources 
of the computer that should be used for other processes. Computers create or can create a new paradigm 
for creating solutions. To create a new world, one must accept a new model of the universe from which 
to explore. This is true with both the people creating the tools and those using it. It's revolutionary, 
it's not evolutionary. In the 1950's and 60's, there was a movement in art popularly known as HAPPENINGS. 
This was done by Allan Kaprow, the originator of the movement. It threw people into thinking about media 
and process in a different way. I show one particular slide from one of his happenings called Words, 
where words were thrown up in random assortment, where people did not use words the way that we ordinarily 
anticipated using them. Other examples throughout the history of art can be used .... Nam June Paik and 
Charlotte Mormon's TV Cello, Andy Warhol's soup can -- however, I couldn't find a copy of his painting 
so I took a picture of the soup can. I apologize to Andy's estate. The computer can do the same thing 
for us. It creates worlds in which we can take risks, a new paradigm for creating solutions. CAD/CAM 
did that, word processing did that. Other examples can be cited -- I could spend the rest of the day 
citing those examples. Here's two of them at the Media Laboratory at MIT, or then known as the Architecture 
Machine Group , the Aspen Surrogate Travel Project was done, where you could sit at a monitor "drive" 
through the city and into another reality. By touching the magic Winkie Dink Screen (the touch sensitive 
device placed upon the monitor), the traveller was able to visit the place at his/her own speed, with 
a personal map and the ability to pause, browse, and ramble in and out during two seasons of the year. 
Another example is the Movie Manual. This research took a look at the traditional book and said how can 
we make this more dynamic. The Movie Manual put up information on the screen by using video disk technology 
that combined text, moving images, synthesized images, computer graphics and interactive commands that 
allowed you to go through the information in many different ways. A non-linear approach with random access 
to the entire volume at any one time was the key to its success. The combination of media, and the ability 
to combine written, spoken, moving and still information at the viewer's discretion showed us a way to 
change our reality. It's a tremendously dynamic learning tool, as well as a tool that will allow us to 
be totally fascinated with the content of what's there, and it's somewhat like what Paul was talking 
about. 16 The computer is a unique tool in allowing this paradigm to exist. Individuals engaged in this 
revolution must be willing to take the risk and to explore the unfamiliar. While this might sound simplistic, 
it's often not that easy. This has always been a driving force in art making. Without the willingness 
to accept the change and the uncertainty, we only have stagnation, we do not have avant garde. Some will 
argue with me on this point -- I will hold forth with my viewpoint. You can't take the easy way out. 
You can't give technical tools designed for technical people to people who do not understand the language 
without first introducing them to that language. Computers allow us to think in new ways. They teach 
us new concepts and ways to do things. There's about four slides of some of my work. I learned about 
color, I learned about dealing with the color manipulation by moving around color matrices, by moving 
around pixels on the screen. I never did any of this before I touched the computer. The computer allowed 
me to explore, but more importantly, it threw unpredicted and unimagined choices at me that allowed me 
to see things I could never have independently imagined. I'm now presently more involved with understanding 
the methodology of problem solving and visual thinking. I work with people who are developing tools to 
better help them understand what it takes for artists and designers use these tools. I work with people 
who are trying to understand how these tools will help them change their reality. Our past experiences 
with tools and processes give us expectations of new tools and processes. These expectations are almost 
always different than what we will experience once we use the tools. I'd like to show you very quickly 
some work of other people. Most of them are SIGGRAPH slides from past shows and collections. The names 
on the bottom of each will indicated who they're did the work. I simply want to show you how other people 
have thought about this process in the past. Because we come to other machines and tools with expectations 
that are changed, we then create a new model from which to anticipate the next action. This is forever 
an evolutionary process of experience and expectation, with new ideas coming together. There must be 
a critical balance between the defined task and the tool's ability to respond and its ability to create 
serendipity. We cannot throw ourselves into a totally unknown environment and expect to do much of anything 
in the very beginning without learning that vocabulary, without learning about how it works. There must 
be some familiarity and clarity to comprehend the value of the new. We have to anchor ourselves somewhere. 
We can't divorce ourselves from everything that we have learned, nor should we. There must be a balance 
between all of this. What I'm saying is that there is already a change in the way we work. This will 
occur whether we approach it by either waving a flag of enthusiasm or a fist of protest, but there is 
a change. We've all spoken about that. 17 Computers have changed the way we work. All of us on the panel 
understand that. More importantly, it has changed the way we think. There has evolved a new methodology 
in the manner we think and do our work. Try to imagine a world without photographs. Try to divorce every 
recognition of a photograph. You cannot. Try to take TV out of your consciousness -- you cannot. Photography 
has entered our way of thinking from the very concept of the work, even though we might dismiss its product 
or dismiss its use in the final application. It is there nonetheless. It was a revolutionary tool when 
it first started. It continues to be that now. It taught us to think differently, it changed the way 
we experienced the world. It has taught us to see things that aren't really there. It taught us to see 
things that we could not see without photography. It gave us a vocabulary that is without precedent or 
prior example; a new way of dealing with information, a new paradigm. Photography enriches our conceptual 
tool box. The computers have done the same thing to those of us using them, and it will continue to do 
the same thing, as it grows. I'll sum up with just a few slides. This one (slide of image of pool balls 
created by Lucas Films @ 1984) crystalized some ideas for me, even though it's about four years old. 
It said look at this, the computer can create its own reality, even though this is not a photograph. 
It gave me that sense of control, it gave me new ideas and new ways of thinking about the tool. Things 
have gone further than this. As I said, this is a four year old slide. I started out as a photographer 
and a graphic designer doing very straight photography. Nothing was ever manipulated. The purity of the 
image was what the camera found. I got onto the computer and it opened up a world of layenng, it opened 
up a world of exploring collage and manipulation and it changed the way I think. (slides of own artwork) 
It gave me the opportunity to distort and to paint and to combine and to create in new ways than I never 
thought were possible. When I go back into the dark room and leave the computer behind, at least for 
that moment. But, I take with me that new vocabulary. I don't need the computer there to understand what 
it's done, to create a new world, to create new process. It's an object to think with. It has given me 
an entirely new vocabulary with which to think. For that I'm terrifically indebted to the computer and 
anybody who's developed anything on the computer, and I think the panel members will speak the same enthusiasm. 
We're running about ten minutes behind, but I will open this up for questions and I thank you for your 
patience. (APPLAUSE) QUESTIONS AND ANSWERS SESSION: (this was continued, untaped, for another hour and 
a half in the breakout room) If you have questions, could you come up to the microphone, state your name 
and where you're from, and we'll take one at a time until they throw us out of this room and then we 
go into another room. 18 Yes? Q: My name is Hugh Lifson and I teach art at ComeU College in Mt. Vernon, 
Iowa. You used the word risk, and I'm in the process of writing a paper for another panel where my argument 
is that the word risk in some ways can be taken out or perhaps modified and maybe there we would agree. 
That is to say, one of the nice things about using a computer, and I think it's a profound thing, is 
that you can say what you have, try something which may be incredibly cockamamie and then, if it doesn't 
work, of course you can go back. Rembrandt couldn't do that when he wanted to score out parts of three 
crosses, you see. So in one sense, that whole kind of risk feeling that, for example, the abstract expressionist 
spoke about as a kind of protoromantic idea, changes. I agree with you that it's changed not only our 
thinking but our whole kind of sensibility about being artists in that respect, and so the word risk, 
it seems to me, might be a word that might want to either be eliminated or seriously modified in this 
regard. Paradoxically, one can go further because of this ostensible lack of risk. Thank you. ALYCE KAPROW: 
I'd like to comment on that and give everybody else a chance to comment on that. My response is that 
risk can be defined in many, many ways. There's a risk to going on a roller coaster, and that adrenaline 
gives you a rush. I'm not going to talk about my life's history, but I have gone skydiving and let me 
tell you, there's a risk and there's an adrenaline flow like nothing has ever, ever been before, and 
it opens your mind to ways of thinking. So risk is not bad. Risk is good. So I'll keep the word in, but 
I understand what you're saying. Does anybody else want to comment on that? No. PAUL SOUZA: I don't want 
to take any risks. ALYCE KAPROW: Yes, you do. You brought the computer up here. Q: Thank you. This question 
is basically -- ALYCE KAPROW: Name and serial number. Q: That's appropriate. And I work for IBM and 
I work in the media center here in Atlanta, and we're an education facility and we provide educational 
materials internally to the company.  We have a post-production house, a traditional art comp type house 
and a computer CBT type education house, and of late, in the last year, these three groups have converged 
under this banner of computer graphics. Each group has their own computer graphics system, yet there 
needs to be this interpolation of graphics across the house. And I wonder ff you could just comment briefly 
on how I might approach that change and how artists that are traditional artists are now forced, because 
of crunching cycles in production, to get 19 on a computer and this transportation of graphics across 
these different fields -- how that might be going in the future and where you see that going. ALYCE KAPROW: 
Paul, do you want to address that? PAUL SOUZA: I'm discovering a whole new approach to thinking about 
the way I work. As a designer, my job is to present a clear visual solution to a client's problem. What 
I'm finding now is that not only other facilities getting very powerful computer tools, but clients traditionally 
will just talk to me and give me some text on a piece of paper and will talk about the idea, have these 
tools, and they will do more than just talk to me -- they will give me a disk where they've drawn up 
things, where they've really presented things in a lot firmer, even to my opinion, naive way. And my 
first reaction is to really react strongly against it and say they shouldn't be doing this. What I'm 
trying to learn to do is to help them with that new way of working and use it and develop further refine 
their ideas and use it as really a better way of working rather than reacting against it. I think that 
we're at a stage where we're going to be communicating electronically that what we ought to do is work 
towards working together, not trying to separate the functions as we've traditionally tried to do. That 
we should think about it as this is the opportunity to open up these new horizons and put aside our egos 
and help everybody through this step to a much better way of working. ROB HAIMES: I think that's happening, 
what you're describing is happening a lot, and I've been talking to some of the animation or CAD system 
people, and the way that something is developed on one of those systems, you develop something for a 
proposal, and what you've developed for the proposal finds its way into the development of the product 
itself, which finds its way into the training for that product, which f'mds its way into the marketing 
and sales. And print kind of embraces all of that. Print supports that and the form that the print takes 
changes as you go through that process, but there's always the need to get some of those images into 
print and to use some of the moving images to support the print, and the print to support the images, 
so it's really emerging quite a lot. JOEL SLAYTON: Just take slightly the opposite view and I find that 
it doesn't work very well at all. JOEL SLAYTON: Probably because in many cases our laboratory and my 
own work in particular involves the integration of a great deal of imagery and sound and post-production 
techniques that are produced on different kinds of equipment. They simply don't talk to one another, 
they were not designed with that in mind. They exist in different environments. The people who know how 
to use them have a knowledge that's not easily departed to you. And the integration really comes from 
your personal attempts at experimentation, trying to pull the pieces together. I think that's always 
going to be the case. As the technology moves ahead, the integration problems are going to follow with 
it, and if we want to talk about integrating computer graphics on three systems in a particular environment, 
that's one singular issue. 20 If you want to talk about bringing computer graphics from twenty different 
systems into a single work station and piping sound to it and having digital sampling present there, 
everything running through an interface that works properly, you've got a new computer on your hands 
and we damn well need one, I think. So, developers, get with it. We're ready. ALYCE KAPROW: Let's take 
the one in the back. Q: CHUCK HOUSEFUL, Morgan Stanley -- I've been coming to SIGGRAPH and computer art 
shows for longer than I care to admit, and I've always been very disappointed, up until this panel. ALYCE 
KAPROW: Thank you. Q: Because all the panels, again excluding this one, have always focused on the computer 
as a tool which the artist uses to create essentially a finished product which is merely passably experienced 
by the person who experiences it, and this is the first time I've really heard about using the computer 
as a medium, particularly Joel Slayton's talk, where the experience itself depends essentially on the 
computer and depends essentially on the person experiencing it, and the interaction between the two. 
And Joel, you talked about various applications but I didn't hear anything about major a new work of 
art. When are we going to see the first work of art that really depends essentially on the computer and 
the one experienced with the interaction? JOEL SLAYTON: That goes back to the central issue of this whole 
panel, which is methodology. My methodology, like many artist's methodology is I basically don't know 
what I'm doing. That's the bottom line. I go into the studio and I start making things. Or I go into 
a laboratory and I start to produce stuff. I work on what interests me that particular day. I try to 
follow it through the best I can, working with the programming people and the technologists and the engineers 
and so forth. When the new work of art arrives, I hope it will be soon. But I don't have an answer to 
that. Right now what I'm really thinking about in terms of that particular project is to design a so- 
called book environment in which architecture is composed of text and that you read it by traveling through 
it, and explonng it with the hand. What the content of that thing will be and the specific style of it, 
I wouldn't want to venture to say fight this minute. ALYCE KAPROW: I don't know if I can -- I'm answering 
your question in particular, but I'd like to make a very brief comment -- something that hopefully came 
across in what I had to say, though I will restate it briefly. The final product is important to all 
artists and designers. There's no doubt about it. But I'm an artist and a designer, and now a consultant 
in this field, because I love the process. I absolutely adore what I do, and I don't want the computer 
to take that love away. 21 So part of what it is is not necessarily worrying about the product. It's 
going in there and dealing with the process, and to me that's the art. That's the creativity. What comes 
out the other end may or may not be wonderful. I hope that most people consider them at least acceptable. 
But it's the process that I love, and that speaks about what I'm trying to say. I love using the computer 
and I love using the camera as a result. Q: Well, how about making that process available as itself a 
work of art to the final consumer? : To add to that point, there are some earlier examples, but I think 
more and more the system is being used as a system and I know that when I started out doing work on a 
system I was sitting there basically drawing pictures and emulating a very conventional process and I 
knew there was something lacking in that and I think some of the recent stuff, some of the stuff that 
Joel's doing, really extends beyond that kind of limited relationship of the screen with the artist. 
In terms of design, I just wanted to make the point that in terms of automating design and graphic arts, 
you're dealing with processes that have been in place for a long time and are very well established and 
merging computer processes with those processes is very complicated. ALYCE KAPROW: Let's take another 
question. Q: Hello. HELMS MEINER with Cricket Software. I'm ... you'll support me taking a small risk. 
First of all, I'd like to say that I definitely support the process sometimes over the product. Sometimes 
you don't have a choice. But I'd like to draw from this a small parallel between verbal communication 
and graphical communication. It's a little bit disturbing to me to keep hearing the phrase -- things 
like tools for artists. My primary focus -- I design painting and drawing systems. Talking about tools 
for artists is a little bit disturbing to me because if you think about verbal communication pretty much 
it's for everyone, and everyone who can speak has the power to communicate with other people verbally. 
And while it's great that artist is for artists and there are tools for artists, I believe that we should 
be thinking about evolving to the point where we give the power of communicating graphically to everyone, 
not to say that the result will be art, but to say that other people are empowered to communicate visually 
with the same validity that people can communicate verbally, even though each individual person like 
myself is not an orator. ALYCE KAPROW: I have no argument with that. Everybody agree? Way in the back. 
Q: My name is TIM MEYERS from the University of Nebraska, and what I'd like to know is in terms of the 
changing methodology, how much do you think the artists really want tools for traditional methods to 
use the computer to bypass several steps in the traditional methods versus an entirely separate medium 
on its own? ALYCE KAPROW: Synthesis -- that's from my point of view. Or it's emerging. 22 It's not something 
that you necessarily isolate. It should be a tool in the arsenal of a tool box, a way to think, an object 
to think with. It's not separated and it should not be isolated. I use the camera fluidly in everything 
that I do in my fife, whether or not I'm holding a camera, and that's the way I consider the computer 
at this point. l've just been given the time signal. All of these questions are important and very interesting 
to us. We have a breakout room for this purpose and I very, very enthusiastically welcome you to follow 
us to room 303, which is very close by, to continue these questions en masse and we'll go on from there. 
We have not unlimited time, because I don't think the panel members have it, but we certainly have 15, 
20 minutes to talk. (APPLAUSE) 23  Rob Haimes Rob Haime.,                
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402248</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>37</pages>
		<display_no>6</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[What can we learn by benchmarking graphics systems?]]></title>
		<page_from>1</page_from>
		<page_to>37</page_to>
		<doi_number>10.1145/1402242.1402248</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402248</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101775</person_id>
				<author_profile_id><![CDATA[81100454794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ricki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101776</person_id>
				<author_profile_id><![CDATA[81100600320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Broder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitre Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101777</person_id>
				<author_profile_id><![CDATA[81100493909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doctor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Raster Technologies, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101778</person_id>
				<author_profile_id><![CDATA[81365592385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Croll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sun Microsystems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101779</person_id>
				<author_profile_id><![CDATA[81100586999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Turner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whitted]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Numerical Design, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WHAT CAN WE LEARN BY BENCHMARKING GRAPHICS SYSTEMS? Chair: Ricki Blau, University of California Panelists: 
Alan Broder, Mitre Corporation Lou Doctor, Raster Technologies, Inc. Brian Croll, Sun Microsystems, Inc. 
Turner Whitted, Numerical Design, Ltd. WHAT CAN WE LEARN BY BENCHMARKING GRAPHICS SYSTEMS? RICKI BLAU: 
Good morning, I'd like to welcome you to the panel on measuring the performance of graphics systems. 
I'd also like to welcome our panelists. Next to me is Brian Croll from Sun Microsystems and also from 
the Bay Area SIGGRAPH chapter's Technical Interest Group on Performance Evaluation. Next is Lou Doctor 
of Raster Technologies. Lou is also representing the Graphics Performance Characterization group that 
you'll hear more about. The fourth panelist is Turner Whitted from Numerical Design, Ltd., and the last 
panelist is Alan Broder of MITRE. First of all, I'd like to point out that there are many different reasons 
for measuring the performance of a graphics system. What comes to mind first are the customers who want 
to evaluate products that they might be purchasing, but there are a lot of other motivations for benchmarking. 
I personally am a systems designer, and my interest is in improving or tuning designs of architectures 
and software systems. There are other people who are interesting in tuning applications or in comparing 
or evaluating different algorithms that they've proposed. Even though measurement is something that we 
really want to do, we need to keep reminding ourselves that there are lots of difficulties both in constructing 
the vehicles for measuring performance and also in interpreting the results that we get. One of the words 
that comes up a lot is "benchmark," and there are some connotations to this word that I wanted to point 
out. We tend to think of benchmarks as some standard set of tests that can be run. Very often, we associate 
benchmarks with the production of a single figure of merit --some single number that tells you how fast 
the system is running and that leads to easy comparisons between the numbers for different systems. The 
question we want to ask is "How meaningful are these comparisons?" I've asked the panelist to each consider 
the kinds of questions that you see on the slide. The bottom line, is to consider what is fight with 
benchmarks as a means of measuring performance and what is wrong with them. I've each panelist to prepare 
a short position statement about this. We intend to leave a lot of time for questions later, after each 
person has had a chance to speak. I think this is a real important time for talking about these issues 
behind benchmarking, because there's a lot of activity going on in this area fight now. You'll hear about 
much of this activity in the rest of this session. There's research going on in basic methodology and 
in constructing performance tests. Some of this is being done by the group that Brian Croll represents, 
the Bay Area ACM SIGGRAPH Technical Interest Group on Performance Evaluation. They're working on low-level 
benchmarks for graphics functions and on methodological issues. There's a group that Lou Doctor will 
be speaking about, called the Graphics Performance Characterization (or GPC) group, a nationwide group 
of vendors and users who are working on standard methodologies for benchmarking graphic systems. In the 
research end, such as image synthesis and animation, Eric Haines published a set of procedural environments 
in IEEE Computer Graphics and Applications last fall. There is other work going on, also. There's another 
pair of names that inadvertently got left off these slides; Craig Dunwoody and Mark Linton at Stanford 
are working on benchmarks and performance characterization for workstations, especially focusing on window 
systems environments. If you're interested in finding out a bit more about the Graphic Performance Characterization 
group, Ken Anderson will be hosting a Birds-of-a-Feather meeting this afternoon at 3:30 in this building. 
The panelists will talk in this order: I will be going first, followed by Brian Croll, then Lou Doctor. 
The three of us are all working on developing standards or packages for measuring graphics performance. 
Following the benchmarkers, will be the people offering some critiques of benchmarking -- Turner Whitted 
and then Alan Broder. First, I'm going to describe the work I'm doing at Berkeley on measuring and characterizing 
workloads for high end rendering systems. Although I'm working on benchmarks, I really am a sceptic, 
and I'd like to explain the root of my skepticism. Here's the issue -- the key component of a graphics 
benchmark is the database that describes some scene. I think a benchmark or the results that you get 
from a performance experiment, are only as good as this test case or scene is valid. And how is this 
data chosen? Frequently, for high end rendering sorts of environments like ray tracing and image synthesis 
research, scenes are chosen to a great extent because they're available. It takes a lot of effort to 
create the models, among other reasons. Also, if a scene has been used by others, it can be easier to 
compare different systems. I'm not going to knock availability as a criterion for selecting scenes, but 
it's not enough. My suggestion about how to proceed, is that we want to do two things. First of all, 
one way that we can select a scene is to try to choose data that is representative of some real workload 
that we expect to be running on the system. The other thing we can do is try to construct test cases 
that lead to specific performance experiments that test various hypotheses about the system's performance. 
My work can be described as trying to model some workload, and I take both of these approaches. I want 
to go through a case study using the first approach and then outline the second approach. In the first 
approach, we're going to first select some workload of interest. Once we have selected this workload, 
we measure it to understand what are the characteristics of this workload, especially focusing on those 
characteristics that affect performance. For instance, one characteristic for an image synthesis workload 
would be simply the number of primitives in the model. There are various other characteristics, and I've 
gathered statistics and done analyses of these workload characteristics. Once we've done that analysis, 
we select those characteristics that we think have the biggest effect on the performance and try, in 
our benchmarks, to simulate or obtain those desired characteristics under some sort of control. I want 
to give an example of trying to model one kind of workload, a sequence of terrain-like scenes. The picture 
I have up here is from the paper by Fournier and Reeves last year, and it's one scene that I've done 
a lot of measurement and statistical analysis on. I'm going to look at scenes that have complexity characteristics 
similar to those of this scene. For my example here, I'm just going to concentrate on the visible surface 
problem. One critical characteristic affecting the performance of a visible surface algorithm is the 
depth complexity. I'll roughly define the depth complexity as being the number of objects that you have 
to consider at any given point on the image plane. So we'll look at that. As a vehicle for my experiments, 
I've used the Reyes system, as described in the paper by Cook, Carpenter, and Catmull in the SIGGRAPH 
proceedings last year. The picture of the stained-glass knight that Pixar did for the film Young Sherlock 
Holmes, doesn't look like mountains and oceans, but in many ways, it has the same kind of depth complexity 
characteristics as a terrain scene because it's composed of a lot of bicubic patches and it is relatively 
fiat, but with some variation. The monochrome terrain scene that you see is one that I've artificially 
constructed to resemble the other two scenes, the ocean and the stained-glass knight, in its statistical 
properties. The graph shows some approaches to statistical analysis. What you see are "box plots" that 
demonstrate the distribution of the depth complexities in these scenes. There's one boxplot for each 
of the different scenes that I've shown, and the one on the far left is for another of my synthetic scenes 
that I haven't shown you. The box itself contains half of the measured data points. That is, it contains 
the depth complexity for about half of the points that I've sampled on the screen. The whiskers, the 
lines that come up top and bottom of those boxes, show you the range of most of the non-outliers of the 
depth complexity measurements for each scene. The vertical dots show the outlying measurements. The second 
example from the left, "Terrain 3," is the picture that you see on the other screen. Its depth complexity 
distribution does resemble the characteristics of these other terrain scenes, as you can see by visual 
inspection. This is a technique that I'm using for many of the key characteristics to try to test how 
well benchmark scenes actually reproduce the characteristics of scenes in the workload. The real problem 
with this approach ... well, there are a couple of problems. One is simply an implementation problem 
--controlling the characteristics. With these sorts of scenes, I've especially found it problematic to 
control the variability of the characteristics. It's easier to come up with the right mean, but the variability, 
which is of great interest to parallel processing implementations, is very hard to duplicate and control. 
But there's a more fundamental or philosophical problem. And that is, what do we simulate? How do we 
actually choose what is typical? In image synthesis, we've had limited experience with applications. 
It's hard to say from only a few years of experience where this is going and how what is typical now 
will reflect in the future. This is especially a problem because I think that the image synthesis workload 
has always been somewhat constrained by the available technology. I think a lot of what I'm measuring 
tends to be an artifact of what someone can compute at the time. An even more fundamental problem is 
a question raised by Dave Patterson, who has used benchmarks extensively to design RISC architectures 
at Berkeley. He asks: is what is typical necessarily the same thing as what's important to measure? So 
that leads to another point of view, which is to construct deliberate tests where you control your input 
so that you vary one critical parameter at a time, while you try to keep the others constant. This is 
the tetrahedron I showed before. I think it's used quite a lot, especially in the ray tracing community, 
simply because it's available. But as it turns out, you can control the input by varying the amount of 
subdivision in the model. You can keep the screen area constant, and the overall size of the model constant, 
but vary the number of primitives that you have. There are many ways of creating tests and many different 
variables you can control for. These are just some of them. To sum up my work, I'm working on a performance 
measurement toolkit for graphics systems, specifically for a high end image synthesis. It will take both 
approaches to constructing test cases in a way that I hope will be portable and standardizable, as well 
as very flexible for system designers. I'd like to introduce the next speaker now, Brian Croll of Sun. 
BRIAN CROLL: There we go. To start off, what I'd like to share with you is an observation I've made about 
1-1/2 years worth of working in benchmarking performance evaluation --normally rational people lose every 
sense of reality when it comes to benchmarking. Suddenly, people who are very civil become very uncivil 
when they enter a room and start talking about which system's faster. To further digress, the same problem 
happens in baseball. In New York City, there's been about a 20 to 30 year argument over who was better 
-- Mickey Mantle or Willie Mays. And you know, it gets down to the point: how can you go about making 
this type of classification? Who can decide, and how do we make this kind of decision? What criteria 
do we use? Now, what's interesting is that baseball's been pretty successful -- you have a number of 
statistics. Youhave: home runs hit, your batting average, slugging percentages. There are all sorts of 
somewhat valid statistics that you can put together and make some sort of critical evaluation and give 
a basis for the argument. I see the work of performance evaluation as heading towards something that 
baseball's done very successfully. I hope we can do this for the computer industry --using a set of statistics 
to help us argue better and in a more rational manner. The carnage, I believe, will be a lot less visible. 
Now, trying to address this problem, a special local interest group called TIGPE, is investigating this 
whole big problem of benchmarking. When we sat down and tried to figure out how are we going to approach 
this problem, we realized first of all that there's just a ton of different architectures out there in 
graphics. Compare a PC with an add-on board, to an Evans and Sutherland or some flight simulator, a really 
high end system. Compare that to some workstation. It's very difficult to make a valid comparison under 
those circumstances, especially when you're focusing on a specific benchmark and tying it to an architecture. 
The next point that I think everyone out here understands very well, is that it's very difficult to reproduce 
any benchmarking results that we have heard. You know, you hear a number. You go home on your system 
and you can't get the same number. That obviously, is a very large problem with credibility for the vendor, 
and also it makes the customers very unhappy. We want to solve that. The next point is the single figure 
of merit. Once again, when it comes to benchmarking, the world goes crazy. We know that life and the 
world is very complex, and you can't sum up something with one number, but suddenly, for some reason, 
when we look at computers, we want to say: "Okay, that's an 8 and this is a 6. I'm going to buy the 8." 
I think that expectation is not very realistic and I'd like to emphasize that we're trying to move people 
away from that type of an approach. One figure of merit loses too much information, and it can't possibly 
capture the complexity of the problem. This all sums to a sort of a free-for-all, where there's a ton 
of "never-to-exceed" numbers. You're guaranteed you will never go faster than this, but you have no idea 
how fast you really can expect to go. We tried to put together a model that can help solve these problems. 
The key tenet of this model is a black box approach. That solves the problem of multiple architectures. 
We don't really care how you do it. If you have a huge look-up table that cranks 4 out vectors, if it 
can do every single possible vector in the world, and it's fast -- great. You've got a good machine. 
Or, if you have a very compact algorithm that cranks out vectors very quickly, you also have a good machine. 
It doesn't matter how you do it, it's just data in and then, finally, the data out. The next point is 
the reporting section of benchmarking. As we all know, the environment in which you benchmark makes all 
the difference. So what we want to do is establish an excellent set of very, very strict guidelines on 
reporting the results of benchmarks. Finally, to make sure that we don't go the single figure of merit 
route, we'd like to give a comprehensive set of performance evaluation measurements. In other words, 
we want to have the home runs, the slugging percentages and the averages for people to take a look at. 
Now, the fundamental point that I'd like to emphasize again, is the black box approach. We only care 
about input into some graphics machine and what comes out of it. The key point here is: how fast can 
you send information through the black box? Now, you can take two different versions of this. It's fairly 
simple minded. One is the black box where you worry about just the input, the parameters coming in, and 
also the environment in which it's done. Or you can look at it as a sampling problem. Now there, you're 
saying, "Okay, there's a lot of data out there. We're going to sample that data and we have this input 
space from which we're taking that data." For vectors, let's say, it would be the end points of the vectors. 
That gets mapped to an output space, where you have lines actually drawn on the screen. Our goal is to 
measure how quickly you can do that [mapping]. We don't care how well you do it. We just care how quickly 
you do it. There seem to be two approaches that we found to this sampling problem. The first one is where 
you don't really bias the sampling at all. You just say, "Okay, we have all the vectors in the world. 
Let's try to get a pretty good sampling of all of them." We're going to have to distribute the samples 
more or less uniformly throughout the input space, try to cover the whole input space, and get enough 
samples that we can characterize that full input space. For the other side of the model, you can say, 
"We know we're really interested in let's say, electronic CAD. There, it seems that vertical and horizontal 
vectors are particularly important. So, why don't we limit our input sample to vertical and horizontal 
vectors and come up with a set of ratings that is specific to electronic CAD?" There are two really different 
approaches here. One is characterizing the machine with an even distribution of samples. The other side, 
is trying to characterize the particular performance of the machine on the particular application. This 
slide shows the groupings that evolve from this. We have the first level (i.e., low- level) type of benchmark. 
That is, let's say, how many vectors per second can you draw? The next level up is what Ricki was just 
talking about, putting it together and making a specific type picture -- an MCAD or an ECAD picture. 
That goes back to the specific sampling schemes. Then you have the system level where you're including 
input. We can do an input loop or rotate something, let's say. Finally, we have an application level 
where, let's say you wanted to use AUTOCAD, you'd take a benchmark that's specific for AUTOCAD. This 
[slide] has nothing to do with what I'm talking about, but it's SIGGRAPH so you're required to show at 
least one pretty picture. So now, output from this. This is how we're going to report the results. Once 
again, in the theme of losing sanity when it comes to benchmarks, we use graphics for everything in the 
world except for what we do in benchmarking. Why not use graphics to start rating systems? Here is the 
output [of a test] where you vary the angle of a vector and vary the length of the vector. You then plot 
the results, and you get some fairly interesting graphs that are very specific to certain machines. You 
get very different results. You can tell the architecture by the picture you get up there. It seems that 
stereo shops are very good at this kind of thing. When you're buying a speaker, you get a fairly good 
graph with the frequency responses. Why don't we do this for computers? This [slide] shows the kind of 
information you can get from graphing the measurements. Here, it turns out that one computer is very 
fast at drawing horizontal vectors; the other one isn't so good. You can get the comparative results 
here, and it tells you a lot about what's going on in the system. One is very good at the general case; 
the other one is optimized for a specific case. To sum up, what I want to emphasize is -- let's break 
the tendency to want to simplify everything to one number. I think if we can get that far, we're very 
successful. That's going to take an effort on all our parts --both on the vendor's side not to push that 
kind of number and also on the customer's side, not to ask for that kind of a number. The next point 
is that we want comprehensive results -- a lot of information so that you can make a thorough investigation 
in evaluation. And also, with their environment wholly specified. The next point is, in order to make 
this type of benchmarking apply to all systems, we want to make sure that we don't worry about the architecture. 
We make it architecture independent, and worry about the input and then the final picture. It's important 
to realize that vectors per second are very helpful. They help people classify machines in a very, very 
broad area. Drawing the picture is very important because it helps people for a specific application. 
The input loop, where you're changing the state of the picture, is clearly important because it gives 
people a more specific set of evaluations. And then finally, the highest level, putting an application 
on the system and seeing how fast it goes, is clearly, exactly, what the vendor wants. So we converge 
by going up the levels. We get closer to what the user actually wants. With that, I'm getting the hook 
so thank you very much. (APPLAUSE) RICKI BLAU: Thank you, Brian. And our next panelist will be Lou Doctor. 
LOU DOCTOR: Thank you. Okay, Brian touched on what I think are really the two key points here. One of 
them is his stereo shop analogy. The work that he's doing out on the West Coast is essentially analogous 
to being able to look at specifications for a hi-fi system on a data sheet and know that those specifications 
really reflect a standard way of measuring things like distortion or power levels. The work that's going 
on in what we're calling the Graphics Performance Characterization group is somewhat different, and we're 
trying to solve a bit of a different problem. Brian also touched on that when he talked about this "not-to-exceed" 
number. 6 Somebody basically described the situation by saying, "Marketing specifications are those performance 
numbers that the actual hardware is guaranteed not to exceed." If you think about things that way, and 
you think about what's going on in the industry and the chaos, you can see that's the problem we're attempting 
to solve. Brian did touch on that when he called it this "not-to-exceed" number phenomena. As a hardware 
vendor in this industry, I can tell you virtually all of the graphics performance numbers that you hear 
and see are misleading. Not intentionally misleading in most cases, but misleading just the same. There 
are a lot of reasons for that and a lot of reasons that the performance numbers that you see associated 
with graphics systems, don't reflect reality. There are a lot of problems in the industry that have been 
caused by the chaos of having high-end and low-end and mid-range systems that are impossible to evaluate 
by their performance numbers. The problem that we're essentially trying to solve here is that vendors 
and users get forced into writing an awful lot of software in order to make intelligent buying decisions, 
since the numbers that they're being given by vendors are so meaningless today. What we've seen being 
a vendor in this industry for the past seven years, is that the hardware that people have bought -- and 
they're making a significant investment -- rarely performs exactly as they had anticipated. Some things, 
hopefully, are faster and unfortunately, some things frequently turn out to be slower. Another thing 
that I think we've had a lot of success in getting vendors together on, is this whole problem of the 
proprietary instruction sets. But in fact, people's biggest decision when they go to buy a graphics system 
is a software decision, not a hardware one. And if you end up picking the wrong hardware, your biggest 
problem is that you're probably doing a significant investment into a proprietary architecture and then 
can't even change your mind later if you decide that your hardware decision was the wrong one. The biggest 
problem is that by going on the exhibit floor and picking up data sheets and looking at manuals, I think 
that you can use that data to essentially categorize or classify the various graphic terminals and workstations 
and subsystems; it's fairly obvious from the numbers that you're looking at in terms of decimal orders 
of magnitude which are the high-end systems, which are the mid-range systems and which are sort of the 
low-end systems. But I don't think that that's enough data in order to evaluate it today. The biggest 
problem there is that real applications, ones that you'll actually have to write, frequently use whole 
sets of instructions that are not covered by the performance numbers that you see from the vendors. A 
typical example is that you get told essentially ... everyone's using polygons per second or quadrilaterals 
per second or vectors per seconds. In the case of vectors, maybe they don't tell you how long it takes 
to change the color of a polyline from one vector to the next. And maybe, just unfortunately, that happens 
to be 10 times slower than actually drawing it. So now you've got a data sheet that implies that you 
can draw 500,000 vectors per second and you've got a very important specification, like how long does 
it take to change the color of that vector, that gets left out. And when you go to write the real application, 
you sort of get this "gotcha" where the application runs at a quarter or at half the speed you expected. 
So I think that what we've seen is that it's impossible, really, to anticipate all of the possible instructions 
and all of the things that have to go on for people to make intelligent decisions. That's even within 
a given application area -- Brian used the analogy for electrical CAD -- or even with mechanical CAD. 
You talk from user to user, and the kinds of things that they want to display vary very greatly. Some 
users want to look at nurbs. They're expecting to see hardware that can display curves and surfaces directly. 
Other users are just concerned about quadrilaterals. Other users are concerned about triangles. So what's 
going on here? I think that we've taken an approach that was similar to the way that the whole PHIGS+ 
group got started. We had a room full of vendors that were all both saddled with the problem and sort 
of had caused the problem, and we had to take some initiative to help solve it. The solution that we 
have a group of vendors working on now, is what Brian described as the level 2, or picture level benchmark. 
The idea there, is rather than us taking the time to give you all of the possible numbers that you would 
need to make an intelligent evaluation, what the vendors intend to give the users is a specification 
for an interchange format. Users basically can move data on their own and have a standard benchmarking 
program that exists on a variety of workstations and terminals that gives them numbers that they'll understand. 
So, essentially, we push the problem up by doing this piece of effort in this picture level benchmarking 
group and by performing three simple steps. One is defining this interchange format so people can move 
data sets around, and so you don't have to design an interchange format yourself. The vendors are going 
to supply that by implementing it in a reference manner. Each vendor can then go off and make changes 
to it if they want to, but it has a starting point. And by prescribing what the output numbers mean -- 
essentially, a timing methodology - -we can give people an environment for making more intelligent decisions. 
And that's what it's really all about. We're spending a lot of time and money as vendors, and users are 
spending a lot of time and money trying to make the best possible decision. Well, we've got this hurdle 
that we've got to cross so at least the data can be interchanged freely. So I think the key here to what 
we're trying to do is, on the one hand, save that time and money. One of the things we've seen all the 
time, is that the user who only wants to buy one of something doesn't get the same time and attention 
from the vendor as somebody who wants to buy a hundred. If somebody says, "I want to buy 100 workstations," 
vendors drop what they're doing. They start doing benchmarks. They start writing special code. But a 
user who every three years, can buy some capital equipment to do something -- that user doesn't get the 
level of attention when they're buying just one of something and doesn't have an opportunity to get custom-written 
benchmarks. So that user is essentially in the dark right now. But using this interchange format, the 
user will not have to expect the vendor to write software for them in order to evaluate the system. We 
feel that it has an opportunity to really eliminate a lot of bad buying decisions, because you will be 
able to see in advance your own data sets and models up on the screen. So you're not just going to be 
looking at the gears and engine blocks and orbiters that the vendors have in their demos. But you'll 
have an opportunity to see your own data. You can come to your own conclusions about the system's ability 
to perform for you. Another thing which can't really be missed is the fact that most people here tend 
to think in terms of high-end systems. And they think of the nature of the high-end market as always 
pushing that leading edge. But of course, the vast majority of systems that get bought are not high-end 
systems -- they're mid-range and low-end systems. We're trying also to give people an opportunity to 
see side-by-side the differences that they would get if they spent a lot more money for what is essentially 
a high-end system, being able to compare them side-by-side or compare these different price performance 
points. They might be able to see that for only 20% more money, they could get five times the through-put 
or, vice versa, they might have to spent five times as much and only get 20% better performance. I'm 
also a big proponent in the PHIGS+ area. I really believe that one of the other things that's going wrong 
here is that people are quoting specifications that have more to do with, like, the assembly language 
multiply time of a computer, which tends to be an irrelevant specification. As a big proponent of the 
PHIGS and PHIGS+ area, I'm the one that's also keeping people aware of the fact that if someone gives 
you the performance of a system through a proprietary architecture, and if you believe that and take 
it as a given that you'll be able to achieve that -- you'd end up committed to implement all of your 
applications on that proprietary architecture in order to get that level of performance. It's especially 
prevalent in the high-end where all these proprietary architectures have emerged for so long. Again, 
people are really making this much bigger buying decision when they're designing their software than 
when they're buying their hardware. So they'll end up basically getting locked in. It's our point of 
view that a performance specification that is not measured through some standard which is portable across 
vendors is basically irrelevant, and people really have to put their foot down. Force the vendors to 
improve their implementations of the standard. Don't blame the standard. And I think that the portability 
of applications across platforms and being able to move an application from a low-end system to a high-end, 
is really the key to the growth of the whole marketplace. I go to a lot of companies. I'm visiting people 
all the time and explaining my view, which is that the people who are really on the leading edge need 
to be the most concerned with standards and not the least concerned. Because it's only by their being 
concerned about other people having access to their work, will the funding and attention really be paid 
to what's going on the high-end. While there tend to be these packs of what we'll call "graphic snobs," 
just to have some controversy, who basically say, "We don't care about standards because what we're doing 
is really leading edge work," I think that's a very short-sighted view. I think that they have to care 
about that, and they have to care about people's ability to make use of their work on other platforms. 
I think that when the company's making buying decisions, they have to be able to evaluate these other 
platforms in a comparative manner. And that's why the work that's going on in this PLB group is important 
on the picture level benchmark. So that's all I wanted to say. Thank you. (APPLAUSE) RICKI BLAU: Thank 
you. Turner Whitted will be speaking next. TURNER WHITFED: I was supposedly asked to speak so that I 
could disagree with most of what's been said. In fact, I agree with almost everything that's been said 
except for Brian's baseball analogy. Frankly, the vast majority of the baseball user community are only 
interested in a single benchmark, and that's: Who won the World Series? Nevertheless, I am bringing a 
different viewpoint to this in that most of what you're hearing has to do with how one evaluates performance 
in order to make buying decisions or decisions on how to fit existing systems to your own application. 
I'm interested in improving the fit between my application and the vehicle that it runs on. So let me 
describe a little bit about the domain that I'll be referring to. Figure 1. That's not a gratuitous pretty 
picture. That happens to be exactly what I want to talk about. Now, the fact is that we're trying to 
do high-end graphics in low-end systems. So this is to us a typical picture with typical image quality. 
There are other mixes of quality we may deal with, and they're important. In fact, here you see the use 
of [texture, environment] maps and the fact maps effect the performance of graphic systems drastically 
in a way that most people don't talk about yet. Sooner or later, when all the hardware gets so fast that 
that's all we're left to concern ourselves with, then we'll begin to worry about that. But we get a mix 
of that kind of look. We also get a mix of scenes. Here, for instance, is a relatively simpte scene with 
some fairly complex shading. At least, what's visible is simple. It is visibly simple. It's actually 
far more complex than it looks, because there are shadow volumes in the scene which exceed the complexity 
of the remainder of the scene by a factor of about 3. Then there's the notion of a very complex scene, 
in this case, well over a quarter of a million polygons. The shading is run-of-the mill, nothing that 
you wouldn't see ordinarily. But by most measures the depth complexity and the overall scene complexity 
--it's a very complex scene. That's the domain that I want to talk about. Now, let's look at benchmarks 
and, what do benchmarks really measure? In our experience, they mainly measure compilers. In fact, I 
shouldn't admit this in public, but one of the things that we do in proving the performance of our system 
is to just wait for new releases of compilers or new platforms and re- release it with improvement. It 
works. The second thing that a benchmark measures is the performance of programmers. That's something 
that we don't like to quantify. Programmers aren't happy about it, but the clever programmers write faster 
code, by and large. We will make code just as clever as it has to be, so long as it remains portable. 
I don't think anybody's going to disagree with that as a reasonable way to approach that aspect of it. 
Only lastly, do benchmarks measure hardware platforms. What's omitted from this slide, and what's most 
important, is that a benchmark really ends up measuring the match between an application, the programmer's 
knowledge of the application, the programmer's knowledge of the platform, and the platform itself, and 
that's very difficult to analyze. But it's essential to understand it if you want to take advantage of 
newer platforms as they become available. Now, the standard practice ... I mean, the old practice, not 
the more sophisticated approaches that I'm going to propose here. The old practice is just throw a number 
at it and say, we can render X number of kilovectors per second or kilopolygons per second, which is, 
as we've all come to know, a totally misleading gratuitous measure. Some of the proposals for standard 
object collections, frankly, fall in the same category as far as I'm concerned because there are so many 
different ways to render a standard object collection. There are so many questions that haven't been 
answered about, well: how many tricks are you allowed to use in rendering this? How much time are you 
allowed to optinfize your code, etc. There are issues in there that I don't think can be addressed well 
by standard objects. Let me give you an example. Here's the same object rendered twice. We have a low 
quality ... I shouldn't say low quality car. It's a very high-quality car, I'm told by the manufacturer. 
Low-quality rendering of the car and a high quality rendering of the car, and there's a considerable 
difference in the 10 rendering time between the two even though the complexities are identical. Now 
the point here is, they're rendered with the same system. However, we had to turn off all the options 
to get the low quality version. The question that comes up over and over again, "Well, how long did it 
take you to render a low quality version?", because that handles a large percentage of the applications. 
The fact is, a system that can render the high quality picture probably won't render the low-quality 
version nearly as fast as a system that can't render the high quality version. So the question is, in 
trying to design a system or in trying to specify the use of a system, you've got to know a little more 
than just how fast did it make the picture. That's a hard decision for anybody. Will I pay the penalty 
of overhead for that capability of displaying a picture that I'm not going to be displaying often or 
all the time? A very difficult decision. Something that people haven't come to grips with. Okay. What's 
the alternative? Well, there's analysis which I don't believe in either. If you look at some very popular 
algorithms, the z-buffer --and these are probably the two most popular algorithms I've run up against 
recently, z-buffer and ray tracing, both of which are labeled in the classic Sutherland-Sproull-Schumacker 
paper as brute force. I'm probably backwards on which is object space and which is image space. That's 
not terribly important. The fact is, that in spite of their inefficiency or supposed inefficiency from 
an analytic standpoint, they're very popular and, in fact, very practical for their particular uses. 
The reason that analysis doesn't help us here is that it just made too many simplifying assumptions. 
In real applications, those assumptions don't apply. So what are we left with? Well, we like to measure 
things probably more than you might want to measure things. If you're a user, you just want to know the 
numbers. We're a vendor. We want to make the numbers better, so in our case, measurement involves some 
very, very simple techniques. In particular, what we do is hold everything constant and the same and 
change one parameter. In this case, the only thing that's different in all three images is the number 
of pixels. So we can sit there and come up with an expression: Cost = A + B X, where X is the number 
of pixels that are being rendered, B is the slope, and A is the constant overhead for this image. And 
it turns out A and B are quite important to us. We want to minimize them both and we want to understand 
why they're large. Now, we'll do this kind of an analysis over all the important parameters --number 
of polygons, number of edges, number of spans, number of pixels, down to and including, number of texture 
maps used, etc., etc. At that point, what we've got is an indicator of where we ought to be looking for 
improvements. Furthermore, I think that kind of analysis is quite useful to a user as well -- if he sets 
up his test to answer the question of: where am I being hurt on performance? That's the key to understanding. 
I'm not particularly interested in knowing how fast a system is. I'm interested in understanding why 
it's fast or why it's slow. In summary, I don't like benchmarks. I don't like analysis, but common sense 
seems to work pretty well. You don't have to be brilliant to do measurement and combine that measurement 
with a little bit of understanding. You do have to think -- there's no substitute for that. Nobody's 
going to come up and give you numbers that will allow you not to think. Sorry to have to say that, but 
it's quite true. (APPLAUSE) RICKI BLAU: Thank you. And our final speaker is Alan Broder of MITRE. 11 
 ALAN BRODER: I represent a somewhat different background than the other panelists. In the Image Processing 
Technology Center at MITRE, we're not really interested in the conventional kinds of problems that people 
want to benchmark. We're not interested in how fast you can draw vectors. The kind of problems we want 
to look at are interactive, graphical, visualization kinds of things. The kinds of things that vendors 
don't put into their firmware and don't put into their hardware. So what we end up doing in our lab is 
buying the new breed of what is known as "image computers." So let me talk a little bit about them. (slide: 
What's an Image Computer?) So, just a quick summary, in case anybody has been asleep for the last few 
years. Image computers are programmable, very soft, reconflgurable machines. They generally have C compilers. 
In addition, a facility is usually provided for custom, and sometimes pipelined, microcode. Generally, 
the machines have large image memories, high speed scratch pads, and high speed buses. Everybody's familiar 
with the kinds of machines that have been commercially available for the last couple of years. (slide: 
How fast will it run my application?) As an end user, when I see a new "hot box" come out on the market, 
I'm naturally tempted to say, "Well, is it going to run faster than what I've got already? Will it do 
a good job?" One thing you can do, as people have said is "Read the specs." So you have a situation where 
one machine manufacturer is quoting 40 mips and another one's quoting 100 mips. As everybody's probably 
aware, that means just about nothing. The other very tempting alternative is to take conventional computer 
benchmarks, run them on these machines, and try to find out what that tells us. Just as an interesting 
sidenote, there have been researchers who've reported the linear correlation between Whetstone performance 
and ray tracing performance. So that's another piece of information that makes one interested in doing 
these things. (slide: Standard C Language Benchmarks Used) (slide: Machines Tested) Okay. Well, what 
I did was run this suite of benchmarks on all the machines in my lab. The machines that I've got include 
a variety of Suns. We've also got a TAAC-1, which we've had for several months as well as a Pixar. (slide: 
Whetstone) The first benchmark is the Whetstone benchmark. It's well known. It's a floating point benchmark, 
and as you can see, we ran it on all the machines in the lab. The Pixar does pretty poorly, and that's 
no surprise. It doesn't have floating point hardware. The TAAC does okay, but not quite what you'd expect 
after reading the manufacturer's brochures. All the other machines are pretty well known as to what their 
performances are. (slide: Sieve) Okay. The next benchmark is the well known Sieve of Eratosthenes. What 
that benchmark does is compute prime numbers. It's primarily an integer benchmark. And what you see again, 
is that the TAAC isn't making a very good showing. The Pixar is making an evetl worse showing, and the 
Sun 4 stands out very, very highly as a performer. (slide: Dhrystone) 12 The Dhrystone Benchmark is a 
synthetic benchmark which was derived from measurements taken from so-called typical programs. What was 
discovered in developing this benchmark was that typical programs didn't have floating point. Of course, 
in my lab, everything has floating point, but again, looking at the performance -- the TAAC and the Pixar 
are neck and neck in performance. The Sun 4 looks like a much better performer, and the other Suns' performances 
are somewhat less. (slide: Linpack Rolled BLAS) (slide: Linpack Unrolled BLAS) Finally, the Linpack benchmarks. 
We dropped the Pixar from this test because as I said, there wouldn't have been any surprises. The Pixar 
doesn't have floating point hardware, and to port the code to the Pixar would have been a great undertaking. 
We're showing two different results here. One of the results shows the identical C code run on all of 
the machines. That's in the dark blue. Then we modified this C code to declare all the variables as registers 
--every single variable in the program. We recornpiled and ran the experiments again. Those results are 
plotted in light blue. You see that when you do that, the TAAC which was behind the Sun 4 in performance, 
jumps out considerably. We see again that the TAAC is doing better than the Sun 4 -- not quite a factor 
of 2:1. So what does this all tell us? (slide: What Do the Benchmarks Results Mean ?) What do the benchmark 
results mean? I don't think they really mean much. The primary reason is that all of these benchmarks 
are reducing these varied architectures to a very least common denominator. And so really, the benchmarks 
don't let the machines really "show their stuff". (slide: A Better Way to Analyze Performance) We took 
real applications, and we wanted to compare and contrast the relative performances on the applications 
with the results from the conventional benchmarks. The first application from a lab at MITRE was a neural 
network application. We used the heart of the computational code, all written in C. What it primarily 
does is dot-products and look-ups. And we ran a timing experiment on the TAAC and across some of the 
Suns. We took another application, an image transformation application based on the Catmull and Smith's 
algorithm, which is primarily an integer resampling problem, and ran it on the Pixar and all of our Suns. 
Again, we wanted to see how these special purpose image computers would compare on, perhaps, better-suited 
applications. (slide: Neural Network Benchmark) Here's the neural network results. The first three bars 
show the identical C code running on all three machines. The Sun-3 with an FPA, the Sun 4, which has 
a floating point unit built in, and the TAAC-1. And you can see that the TAAC-1 doesn't make such a great 
showing. We then took two lines of C code and wrote a piece of pipelined microcode, which basically computes 
dot products. And we see suddenly that we're doing better than about three times the performance of a 
Sun 4. We extrapolated those results using a special feature of the TAAC to show that we could get almost 
a factor of 4 performance over the Sun 4 on the neural network application. (slide: Image Transformation 
Benchmark) And here's the image transformation benchmark. Again, we ran the identical C code on the first 
three bars, with the exception that there were portions of the code running on the [Pixar] Chap side, 
written in the Chap microcode. And you'll notice right away that there's a factor of three speed up over 
the Sun 4. And when you look at plugging in additional Chaps into the Pixar box, you have greater than 
a factor of 10 speed up over a Sun 4. (slide: Conclusions) So what are the conclusions? The conclusions, 
unfortunately, for the kind of problems we look at and the kinds of things we want our image computers 
to do is that we don't believe that there's going to be a benchmark suite out there that will be able 
to predict the performances on the varieties of applications that we want to look at. And basically, 
our approach is: take an application, learn how to use the machine. Use your mind as, as Turner said, 
and basically, be prepared to squeeze every ounce of performance that you can out of the machines. Basically, 
compiler technology for these varied architectures is simply not where it should be. In order to have 
any understanding of what kind of performance you'll get from one of these machines, you simply have 
to try it (i.e., microcode). I don't believe there's a reliable way to make a prediction. (slide: Who 
I am and How to Reach me) And the next slide there. And that's it, in case you want to send some interesting 
messages. Okay. I think that's it. (APPLAUSE) RICKI BLAU: I'd like to thank all the speakers and just 
remind you, the audience, that everything we said and copies of our visuals will be reproduced in the 
proceedings of the panel sessions which should be mailed out to you this fall. So now we'd like to entertain 
any questions from the audience that you have for either the panel as a whole, or any specific panelist. 
When you ask a question, can you please identify yourself by name and affiliation, so that can go into 
the proceedings as well. MICHAEL ZYDA: I'm Michael Zyda at the Naval Post Graduate School, and what I 
saw was benchmarking for ray tracing, benchmarking for image processing. I don't see anyone talking about 
benchmarking for real time animation types of issues. My work is in visual simulation. I work on Iris's 
at the moment. Have you subgroups working on that type of thing? LOU DOCTOR: I think what you're describing 
is a lot closer to what this Graphics Performance Characterization group is doing with the picture level 
benchmarks, because that will basically allow you to bring a database onto a machine, that reflects the 
kind of terrain and models that you want to see rendered. And you'll be able to specify through the control 
parts of that code, a path for an observer to walk down, or whatever you feel is a kind of a meaningful 
interaction in sequence of that code. The result of that will be a display of essentially the number 
of polygons or quadrilaterals per second that's been rendered and how the machine did. So the work that 
is going on, I think, is going to allow you to be able to evaluate these systems for a visual simulation. 
MICHAEL ZYDA: Yeah, I got a little bit nervous about the picture that Ricki Blau put up. She said, "This 
is a terrain picture." And it's this nice, beautiful, ray traced picture of the ocean. Whereas, many 
people, at least in my area are moving over less realistic terrain, trying to get particular missions 
done, like driving vehicles over DMA data. EUGENE MIYA: Just a question to Alan. Do you have the reference 
to correlating the Whetstone with ray tracing algorithms? That's a reference that I'm not familiar with. 
14 ALAN BRODER: It's not in the literature. This was discussed at a recent Ballistics Research Lab CAD 
symposium. I believe the researcher there is Kennedy, and they reported this in some of their results 
that they presented. EUGENE MIYA: Just as a closing comment in defense of Brian ... we discussed a lot 
of this. I think that in the case of the last two speakers, the issue isn't the system so much. I think 
the issue is trying to understand the nature of applications. I think that's one thing we cannot extract 
out of your guys' heads --whether you're doing CAD or you're doing simulation or something like that. 
It's like you're a manufacturer of rulers and you go mark lines on a piece of wood, then you sell them 
to schools. But the thing is, if your initial inch measurement or if your initial millimeter or whatever 
metric system you're using at the time isn't standardized or isn't consistent at least, then you're selling 
rulers that otherwise have no value. I think that's a big part of the problem. It's not a matter of not 
thinking. I'm not a person who trusts common sense, basically. In the case of your Chap timing, as an 
example, or Turner's comments, you extrapolate speed-ups linearly. In working in parallel processing, 
if there's any one thing that we have seen in the building of SIMD and MIMD processors, things don't 
scale linearly. But I'll talk to you later about Kennedy's reference. Thank you. RICKI BLAU: Would either 
of you like to respond to that? ALAN BRODER: I just want to respond to the extrapolation on the chap. 
I believe that although I haven't got three chap boards on my system, I believe in this case, it's very 
safe to assume it would be a linear scale up in performance by the way the algorithms and code have been 
set up. I think in general, you're correct that you couldn't say that. MIKE VANOVER: IBM in Austin. Historically, 
it seems like to me that in the general CPU case, divergent architectures led to the benchmarks that 
we saw here, the need to compare across broad platforms. However, in the last 10 years, what we've seen 
is the makers of these CPU's optimizing their implementations to the benchmarks to the point of their 
firmware including only the transcendentals that are in the Whetstone benchmark. And any others, by the 
way, you have to do those in software by yourself. How do you propose or what strategy do you propose 
that will prevent the tail here from wagging the dog because it's really an economics issue, not a technical 
issue once the general public gets a hold of X, Y and Z benchmark, and that's all they know from the 
rest of the things being sold. LOU DOCTOR: I'd like to respond to that. I think the only way to handle 
that situation is via this notion of the user bringing their own database and seeing how responsive the 
machine is in allowing them to interact with it. Frankly, I think you've hit upon the exact problem, 
which is that people go off down a rat hole optimizing certain graphic instructions in the graphics area, 
in the interest of coming up with a data sheet that makes the product look as competitive as possible, 
even if for economic reasons they have to ignore a handful of things that are extremely important for 
what most people will actually have to do when they give an application. So I really believe that the 
key to this whole thing is elevating or pushing this thing up to the highest level possible as soon as 
possible, so that users can more easily see their real models on the screens of these sort of subsystems 
and workstations. MIKE VANNOVER: It seems that that presupposes a single platform which we all agree 
would be good, but I was thinking of something before the year 2020, you know, something ... LOU DOCTOR: 
Well, since this was a zing, 1'11 take that one. Yeah, I think, you know, all the systems that are down 
on the exhibit floor are not all compatible today. But you're starting to see the emergence of a sort 
of a checklist of things. Almost all the workstations can run C, and almost all of them are available 
under a reasonably similar operating system. And I think that the one thing that's left is now some compatibility 
in the window system and graphic interface. And I think, although [I'm] from a computer company, it's 
really up to the users to drive this thing. You have to put your foot down and say, "We're not going 
to develop software on proprietary, incompatible architectures." And I think that you'll then be 18 months 
away, not 18 years away, from making the whole thing happen. But I think that the ball is in the user's 
court right now. You know, there are plenty of acceptable implementations of window systems and graphic 
systems now that are going to improve dramatically over time if the users really ... if they really catch 
on with the users. I think the bali's in your court, if you're a user, right now. BRIAN CROLL: Also, 
I'd like to talk a little bit. That whole problem of people writing codes specific for a benchmark or 
microcoding for a benchmark, is really the benchmark's problem, because what they're trying to do is 
get a single figure of merit and that allows people to do that. If, however, you use the graphical means 
that I've talked about a little earlier, you're exercising the machine in a number of different directions 
and in a wide enough range. Using graphics to show the results gets that kind of bandwidth of information 
to the reader of the report. What you can do is make sure that if they do optimize for all the cases 
that you've tested, they have a great machine. That's ultimately what, by designing a good benchmark, 
you want people to do -- optimize for that benchmark. That's assuming, of course, you have a good one 
because then you're going to have a fast machine. So my belief on that is that by combining what Lou 
was just talking about, a picture level benchmark, with a very, very wide range of sample data that you 
can actually see -- I think you can make it so hard to narrow it down and optimize certain specific cases 
that you insure people are actually designing a very good machine. TURNER WHiTTED: Can I object to that? 
JACK BARNES: Jack Barnes, Mass Comp. RICKI BLAU: Excuse me, Turner Whitted wanted to respond to that. 
TURNER WHITTED: The fact is, that if you build a platform that is going to be all things to all people, 
then chances are you built a fairly expensive platform. And the world can't really, not yet, silicon 
isn't free. The world can't afford that quite yet. If you look at what Alan proposes, and that is just 
be willing to do a little work, you get a whole lot more performance on those applications for which 
the platform fits your application. You get a whole lot more performance at less cost, if you're willing 
to do that work. So I'm not convinced that building a platform that covers a really broad benchmark or 
does well on a really broad benchmark, is exactly what you want to do. BRIAN CROLL: Just to clarify what 
I was saying earlier -- I can't disagree with that. think that's very reasonable. What I was getting 
at is by testing a wide range, at least you can find out where they optimize so if things look fishy, 
when you look at the final results, you'll know it. Right now what happens, is we lose all that information 
by saying it's an 8. Why is it an 8? Was it a 1 on something and a 20 on another? You want to know that 
kind of information. RICKI BLAU: Thanks for waiting. The next question. JACK BARNES: Jack Barnes, Masscomp. 
I tend to agree that if you took most of the programs running today under a system, let's just say X 
windows, and you took them of a fairly standard nature, there's been optimization done by virtually every 
programmer at some level because they're not just going to simply call the X routine for every line. 
They've already done the sort(?) or some sort on the data to compact them, to get some through-put out 
of there. And that's a real, live application. Now what most people would want at any level, is to just 
say, "draw a line here," rather than having to put it in an array and then pass it out later on. And 
what you're doing is, if you try to take too many applications fight now and classify them, you're going 
to classify them for the existing hardware model because the hardware model can't take the line at that 
level now. It's already been done. Some work by the programmer to push it into their program that time, 
rather than the hardware. RICKI BLAU: Well, was that a question or do you have a specific question? JACK 
BARNES: Oh, just a statement. How are you going to handle it? In other words, sometimes, you've got to 
make a trade-off. Where does the hardware really count, and where are you allowing the programmer to 
do the optimization? That actually, should be counted as part of the total time to do the hardware task, 
the drawing. LOU DOCTOR: I think I'll take a different twist on that, and raise a point of controversy 
with some of the work that Alan was talking about. Sometimes by optimizing an application or by hand 
microcoding an application around a particular accelerator's architecture, you're basically locking yourself 
into that particular architecture at a time where technology is changing so fast that by the time you 
get done, it's literally obsolete. So, someone who's implementing a "real world application" and literally 
designs it around the architecture of a particular vendor's implementation, I think that's making a bad 
decision. You've definitely seen this in the whole market for these image computer co-processors where 
there were some benchmarks displayed where the Sun 4 outstripped the co- processor. The image computer 
was announced at a time where it was a 10 mips co-processor to a 2 mips CPU and it's being shipped at 
a time where it's a 10 mips co-processor to a 10 mips CPU, and it'll be still shipped when it's a 10 
mips co-processor to a 50 mips CPU. I think what's happened fight now, is that people have gone down 
a rat hole of optimizing things for proprietary architectures that they're just going to have to walk 
out of. I don't think there's any option. JACK BARNES: Oh, I agree with what you're saying, but the reality 
is that graphics is one of those things where the feedback and the smoothness to response is fight there, 
and if you have something that isn't running real time response, it is not acceptable to many customers 
and they just won't by the equipment. That is the real reality of this business, and you will do what 
you have to do to satisfy their needs. LOU DOCTOR: Right. And that's your problem as the vendor, not 
their problem as the customer. That's what I would say. ALAN BRODER: I'd like to respond also to the 
zing. Every application which we've seen ported to our accelerators be it the Pixar or the TAAC, I don't 
believe the "rat hole" is as "ratty" as you would make it appear, because our experience has been that 
the really 17 computationally intense parts of the programs occupy just a few lines of code. I mean, 
we're talking about an inner, inner loop like a dot product which is two lines of C. Provided you structure 
your software the right way, you leave yourself the mechanism for transportable code as well as taking 
advantage of the features on the machine that it runs well on. So I don't think it's a rat hole. I think 
it's using ... making careful and pre-planned use of the features that are available in hardware, and 
I think ... I agree with the other speaker that you do have to do that sometimes because the compilers, 
whoever you are, your compiler can't read your mind and it can't generate a pipelined piece of microcode 
from looking at your algorithm expressed in C. If there are any compilers out there, I haven't seen any 
of them yet that can do that. RICKI BLAU: Any other comments? The next question. GEORGE GORDON: George 
Gordon, MIT. The problem that I have is not that I distrust common sense, but the accountant that reviews 
my proposal does. If he says, "Well, why do you spend 10% more than the low bidder?" He wants a cut-off. 
So you have to have to some sort of a benchmark so you can say, "Well, look. I'll spend 10% more. I get 
20, 30% more on output." So there has to be a sort of single number that you can put on a piece of paper 
to set it up to be approved. Are you approaching that issue? RICKI BLAU: I'm really glad someone was 
here to bring up that point. Any of you vendors have a suggestion? TURNER WHrVrED: As the proponent of 
common sense, I guess I'll have to respond to it. The fact is, that if you characterize the performance 
of a system, you can always fill in all the blanks and come out with a number. The point that was made 
was that it's not sufficient just to come out with the numbers. You've got to go through the characterization. 
But certainly, if I have a great big hairy expression that characterizes my system, feel free to evaluate 
the expression. I would much rather that you evaluate it at a lot of different points, but if you want 
a single number, then by all means, that's the only reason you do that. You don't just want to come up 
with a fancy equation. We're not disagreeing on that. The fact ... the disagreement is whether that one 
number is sufficient, and do you take the one number and then throw away everything that you used to 
get it? And the fact is, you can't. Not if your application changes. Not if any of the assumptions you've 
made have changed and your assumptions always change. Your application always changes a little bit. BRIAN 
CROLL: I think the work that Lou and I are trying to get underway and get the information to you is going 
to help you a lot with your accountant. Because what we're trying to do is give you a certifiable set 
of numbers that you can use and allow you to come up with the final, single benchmark which I think is 
kind of nice because then you have the opportunity to get the number you want for your accountant. And 
you have justification with the numbers that we're going to crank out for you. But seriously, what we're 
really trying to do is give you that information, and a wide range of it, so that you can take a look 
at your application, justify why one system is faster than another, and, if you'd like, condense it into 
a single number. However, what we want to avoid is coming up with that single number for you and telling 
you that this system is going to be this fast for you, specifically. And that's been our approach --to 
make sure you have the data to make a decision. GEORGE GRUEN: My name is George Gruen and I work at EDS. 
And I have a question for Alan and also anybody else who cares to comment. I'm currently involved in 
benchmarking various implementations of PHIGS and the underlying supposition here is that my results 
can be extended to the rather large and varied applications that GM has. My question is: am I wasting 
my time? ALAN BRODER: I'm not sure why I got that question... ' GEORGE GRUEN: Because you suggested using 
a certain application, but I have a huge number of applications. ALAN BRODER Right. I recognize that. 
I don't think you're wasting your time ... it really depends on what kinds of applications they are. 
What I've been trying to say here is, I'm not necessarily disagreeing with the manufacturers who are 
trying to put together benchmarks of standard kinds of graphics operations. What I am saying and trying 
to point out is that there are some things that vendors aren't going to think about. I find myself in 
the dilemma of what to do when there isn't a routine in the PHIGS library to do what I need to do, or 
there isn't a routine or microcoded routine in the vendor's library to do what I need. So the answer 
is, if you can characterize your applications as being able to be built around the PHIGS environment, 
then I'm not disagreeing with that course. BRIAN CROLL: I don't think you're wasting your time. So you 
can relax now and sleep nights. But I would also like to take Tumer's approach of using common sense 
and be aware that you can't make gross generalizations, and I think you have to be aware of the applications 
where it's going to be applied. But if you want to just classify the machines in a gross kind of categorization, 
I think that you'll get some useful information --but keep the common sense in mind. TURNER WHITTED: 
There is extensive history on what you're trying to do because people have been trying to do that for 
years, tens of years now. There are two possible outcomes. One is, that you'll come to a conclusion that 
tries to satisfy everybody and will end up making everybody very unhappy, or that you can get lucky and 
make only a few people unhappy. I think history shows us that you will not make everybody happy, nor 
will you even come close to making everybody happy. LOU DOCTOR: I'd like to just add, I don't think you're 
wasting your time, either. I think you know, it's really only by pushing, again, the problem of delivering 
performance through standards off on the vendors that there is going to be meaningful progress made in 
this whole industry. So I think that you're doing exactly the right thing. I think the reality of the 
situation is that a lot of companies don't have the resources that an EDS might have for performing that 
job on their own. And those companies are going to be relying on the vendors to deliver them those tools. 
So in effect, I think what you've heard here is us talking about developing better sorts of benchmarking 
tools as vendors that we're going to make available to users through a set of common interfaces. And 
if a user or vendor wants to adapt that tool to a proprietary architecture or extend it to make use of 
some special feature, then I think it's basically caveat emptor. If the customer is told candidly what 
he's buying, which is basically a system that only delivers performance through a proprietary set of 
extensions or something, then they're on their own. And that's going to have to be done sometimes. But 
you'd certainly not like to be buying something and then be unpleasantly surprised a year later that 
you can't port your code to anything else without completely rewriting it. RICKI BLAU: Yes? CHARLES CLARK: 
Charles Clark, Intergraph Systems, Ltd., Canada. I think this question's been addressed already, but 
basically, should computer vendors be promoting microcode coding in order to enhance performance, or 
should they be promoting standardization? LOU DOCTOR: You know, I used the term "rat hole" and I really 
believe that. You're looking at an industry that is changing technologically, faster and faster. The 
rate of improvement in terms of raw mips and megaflops that can be thrown at graphics has never been 
changing faster. Why would anybody want to be committing themselves to a particular hardware architecture 
at a time in the industry that is literally chaos? Go down to the exhibit floor -- it's chaos. So why 
would somebody want to lock themselves in for five or ten years on a hardware implementation when on 
a price performance basis, they're going to be able to beat that by a factor of two every single year 
over that ten year period. They're going to be a factor of 1,000 behind by the time they get done. So 
I think that it's basically a no-brainer. TURNER WHITTED: Can I agree with that? The fact is, that users 
shouldn't write microcode. Vendors should write microcode. The question is: what's the important rnicrocode? 
Clearly, as Alan's pointed out, a dot product would be handy. That is, as far as I'm concerned, the vendor's 
responsibility. You're still going to get the benefits, but you don't have to go through the pain. The 
fact is, you shouldn't be writing it, but that doesn't mean you can't take advantage of it. LOU DOCTOR: 
One last thing. It also comes back to ... I use this quote a lot of times, but I think it was Adam Osborne 
who said that, "Ninety percent of computers that are made will never be programmed." You know, thinking 
that everyone is going to ultimately be using application software to do their job. It seems entirely 
obvious to us as vendors, that 99% of graphic systems should never be programmed because it's too difficult, 
time-consuming, costly, agonizing to implement applications. And therefore, you can see that in the evolution 
of this business and to the year 2020, the vast majority of users of useful applications are never going 
to be programming the machines. So this whole discussion, to a certain extent, is kind of irrelevant, 
except for the fact that the reality is that people who are developing applications are going to need 
an application interface that they can count on from year to year and from architecture to architecture 
as these improvements get made. And they're going to have to economize their resources in being able 
to develop applications that are portable. Because what you've got now is basically an incredibly wasteful 
situation for application developers. And that's where I see the industry going in the next 10 years. 
An application developer is only going to be able to support a very small number of versions of their 
software, and they're not going to go down these microcoded rat holes. RICKI BLAU: Any other comments? 
ALAN BRODER: rd just like to make one last comment. My employer, MITRE, is not a commercial firm. We 
don't have a commercial product per se. But I have worked in commercial firms and the real world is, 
let's say, an OEM using a computer from some other computer vendor. I don't believe I've ever seen somebody 
putting out a real product make the decision, "Yeah, we can wait two years for this machine to be two 
times faster and so we'll wait another year." You have to have a product out the door. 20 And I think 
the ugly truth is that microcode can't be avoided. You rarely see it avoided. If somebody wants to get 
a system out the door, they want to meet certain performance specs. It can't be avoided. Yes, in the 
ideal world, I think the vendors should be providing much more microcode support, but again, I don't 
think that the computer vendors are able to predict your needs and I don't think that in the real world, 
you have the luxury of waiting for the order of magnitude of improvements that will come with new technologies. 
RICKI BLAU: I'd like to thank all of the panelists and thank you for being here. remind you once again, 
that if you are interested in the graphics performance characterization work that Lou Doctor talked about, 
there will be that meeting this afternoon at 3:30. Thank you. (APPLAUSE)         
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402249</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>24</pages>
		<display_no>7</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[The reality of computer graphics in the motion picture industry]]></title>
		<page_from>1</page_from>
		<page_to>24</page_to>
		<doi_number>10.1145/1402242.1402249</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402249</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101781</person_id>
				<author_profile_id><![CDATA[81365595781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wahrman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[deGraf/Wahrman, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101782</person_id>
				<author_profile_id><![CDATA[81100291282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hollander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Video Image Associates]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101783</person_id>
				<author_profile_id><![CDATA[81540139756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Backes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Los Angeles, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101784</person_id>
				<author_profile_id><![CDATA[81365597768]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Martha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coolidge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Starbright Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 THE REALITY OF COMPUTER GRAPHICS IN THE MOTION PICTURE INDUSTRY Co-Chair: Michael Wahrman, deGraf/Wahrman, 
Inc. Co-Chair: Richard Hollander, Video Image Associates Panelists: Michael Backes, Los Angeles, CA 
Martha Coolidge, Starbright Productions THE REALITY OF COMPUTER GRAPHICS IN THE MOTION PICTURE INDUSTRY 
 MICHAEL WAHRMAN: Welcome. Welcome to our panel. Welcome to SIGGRAPH 1988. The Reality of Computer Graphics 
in the Motion Picture Industry. We have a couple of replacement panelists today from the program that 
are excellent. The origins of this panel come from several years of going to SIGGRAPH and hearing tremendously 
enthusiastic ideas about computer graphics in the film industry and then going back to Hollywood and 
not seeing that enthusiasm there. So we thought we would produce a panel and bring the buyers of computer 
graphics to SIGGRAPH and present their point of view and their ideas. Today we have Michael Fink of Fink's 
Island Productions. Michael has been visual effects supervisor on the Seventh Sign, Project X, Daryl, 
Buckaroo Bonsai, War Games, and visual effects on Short Circuits, Space Camp, Blade Runner, Close Encounters, 
etc. Martha Coolidge is a director. Her feature credits include Valley Girl, Real Genius, Plain Clothes, 
and television's Sledge Hammer and several Twilight Zones. Michael Backes is a production executive at 
Vestron, home of Dirty Dancing and The Dead. On video tape today, we have several people who were not 
able to come. These people include John Badham, a director; Chris Horner, a production designer; Steve 
Chow, a vice-president of programming of Fox Television; Peter Hyams, a producer and a director; and 
Ralph Winter, a producer and an effects supervisor. What I'd like to do now is to roll video tape one. 
We will be presenting their ideas and attitudes towards computer graphics, and we'll be asking questions 
of the panel, and then you will be getting an opportunity to ask questions of our panel as well. Can 
we roll video tape one, please. Video: John Badham, Director Chris Horner, Production Designer Steve 
Chow, VP of Programming for Fox Television Peter Hyams, Producer and Director Ralph Winter, Producer 
and Supervisor BADHAM: The story is the first thing that I look at, and whatever comes later, is what 
comes later. You know, if there's computer generated imagery and it's fine, and then if it's not well 
it's just not there. It's a lot of fun. And there's probably a lot of new technology to be explored which 
is always fun and the audience likes to see new things. They get a lot of very unusal imagery on things 
like MTV all the time and the younger audience clearly responds to that. The younger audience is going 
to the movies. I think people go the movies to see things they haven't seen before not just to see replays 
of what they've seen before so when you've got something that's pushing out on the envelope of imagery--which 
that kind of imagery is doing, then it's fresh and interesting in feeling. A movie like Tron for example 
which used it brilliantly was missing the key element indeed which was a good story with good characters. 
The movie came to be about the computer imagery, but there are always movies that are sort of about things 
that are other than the things they should be about--they are about songs, the star--Sylvester Stallone--or 
they're about something totally irrelevant to what the people go to see which is a really good story 
and a good character. I don't know what areas it could expand into, whether in the field of animation 
or something that was very highly realistic that we could use. Could you do map paintings? Ultimately 
once you get into really good high definition video and I'm not talking about 1100 lines because I think 
1100 lines is a complete hype, I think it's the Emperor's New Clothes. We should be talking about 2000 
lines, 3000 lines. When you get into that then we can do some quantum leaps forward and suddenly special 
effects is going to take a hugh leap forward, where now the special effects people are struggling with 
film to force it to what they want it to do it should be a lot easier. CHRIS HORNER: Computer generated 
imagery as far as being the star of the show, there just aren't that many stories and vehicles that computer 
graphics are really going to play that big a part where it's going to be ,"Oh, can we get this effect 
by computer generated graphics and if we can we're going to make the movie, and if we can't we're not 
going to make the movie." I mean that situation maybe will arise more and more because the technology 
is coming more and more on stream, but in the end it's Story and The Deal. I mean, can you get the actor 
we want to play and the technical things are almost secondary. Not always, but almost always. So the 
main use, or I would think, a big market for computer generated imagery is, for example, like that problem 
that I mentioned that I had on Miracle Mile where you have a specific problem which is not...visually 
it actually might be to me as a designer a big problem but in terms of the overall scope of the movie 
we might be talking about one-half of one percent of the budget which is ultimately what the producer 
and what the studio, what it comes down to often is just the dollars. And I would think that that is 
an area of computer graphics which people like myself or directors or producers would be very open to 
finding out more about basic problem solving rather than how are we going to dazzle the audience. The 
way it is with effects, this is the way it seems to be with film, and I am sure it's the same way to 
a degree with CGI is that if you can imagine it, it can be done. And then the next question is what's 
the best technique, what's the most cost effective technique. Well, actual technique will come in very 
early depending upon dictates of what the script has in it. For the most part the images that I have 
seen, you can tell that they are machine created. And that is only appropriate for certain kinds of uses. 
What I was saying before about what's different about movies and the requirements of the TV world for 
title sequences or bumpers or commercials is that often you are going for that quick, slick image on 
television and computer graphics are even if they cannot give you the exact reality you are looking for 
it almost doesn't matter because you can still come up with some other 10 second image that is going 
to be some awesome image. In the movies, unless you actually need that particular image, who cares kind 
of. Is the computer graphics industry hoping to get to the place where you can go out and shoot some 
image for real and generate it with a computer and not have people tell the difference? CHOW: Yeah, it's 
expensive. It's prohibitive. The cost of our half-hour programs are in the area of--I mean an economical 
one would be about $80,O00 for a half-hour and a more expensive one would be $150,000 for half-hour, 
so there is not a lot of room for expensive computer graphics. The only way we could justify it is in 
the open where you'd amortize the cost over a number of episodes. No, in my programs we don't really 
need to have the computer generated stuff. Yeah, I think the economics of TV is pretty severe and if 
anything at this point the economics are more severe than they ever have been, so no one is going to 
deficit finance shows. So if your average license fee, and I'm throwing it out, if your license fee for 
a one hour show is $800,000 and there is an overage and Universal or Fox has to pick up that overage, 
the first thing I'd think they'd cut back on is computer animation. They'd rather spend the money on 
writers for scripts and stuff like that. So, I mean everyone hates deficit financing now more than ever 
because the syndication market place is more difficult now than it has been for the last five years so 
is someone going to spend extra money that way? At this time I think the answer is definitely No. I mean 
it would have to be a concept that requires it so intrinsically like Max Headroom or something. Other 
than that it would be the first thing that would be wiped off the budget, 1 would think. If finances 
or budget were not part of the exercise I would certainly be intrigued to do more of it. PETER HYAMS: 
To me graphics means something different. If you talk about computer graphics in the sense of the kinds 
of work that I used in 2010 when we were filling a plethora of video screens with images that were created 
to look like things coming out of computers, then I think it's remarkable. It is dependent upon the talent 
of the people who generated the design. In terms of computer generated imagery to replace things that 
are actually shot, my experience has been that I don't like it at all--for a number of reasons. One, 
it looks like exactly what it is. It looks fake. I spent a lot of time with Nate when he was showing 
me the stuff they were doing for a movie 1 think was called "The Last Starfighter" or something. And 
instead of using miniatures they used computer generated imagery and it looked like a very high tech 
Japanese cartoon. It doesn't have the randomness that reality has. It doesn't have whatever those kind 
of optical accidents that occur when you really photograph something. It doesn't have that. It doesn't 
have the sense of depth, it doesn't have the sense of..., it just looks fake, that's all. But to me, 
if you use computers to generate things that look like they're generated by computers is the proper use 
of them. To use them to somehow or other replace photography, I have yet to see it look good. In 2010, 
I spent about 6 months trying to figure out that image, to the point that I had told MGM that I was not 
going to make the film unless I could solve that image because it seemed that the visual credibility 
of the film hinged on whether or not you could convince people you were near Jupiter. We tried everything. 
My initial inclination was not to go that route for all the reasons that l've stated. So Richard Edland 
and I tried all kinds of wind tunnels and oil tanks and cloud tanks and every possible way we could generate 
real clouds and found ultimately we could never generate anything in the detail that was going to accurately 
map what Jupiter did. You could never really get the red spot exactly where you wanted it or white band 
around it or all of those things. So the next thing we did with all the JPL data that we got we decided 
to map it out and feed that into the computer. The thing that the computer did for us, we were using 
the Cray, the thing that thing did for us which was extraordinary, was the ability to take an image and 
bend it to any contour we wanted. At one point we wanted to actually put a giant hole in it and have 
everything go in and we could do any of those things. To sit there and physically sculpt it the way we 
wanted it sculpted was terrific. I mean, given the technology, I think that's about as good as anybody 
couM do at that time. I think all of us who have spent any time trying to put complex images on the screen 
ultimately come to the realization that there are no limits. So I think any thing can be done. find that 
every time there is a step forward in technology, it's misused and abused because people suddenly think 
it is the answer and it can now replace things and it can't. I think anytime you are kind of on the edge 
of the envelope, so to speak, you are going to encounter all of the aggravations except it's also part 
of the excitement of it. It turns out again not to be the panacea that it's claimed; it turned out to 
be horribly expensive formidably expensive, except I don't know how to have otherwise gotten that particular 
image. I am definitely not looking for some guy to come to me saying, "I can put anything you want into 
the computer and you don't have to photograph it. So if you want to make a picture and, say, the picture 
is set in the desert, you don't have to go to the desert. I can give you the desert in the computer." 
In the first place, I don't think the person could. Secondly, if the person could, and much more important, 
if the person could, what would be in that computer would not be the same as a film maker going to the 
desert and spending time in the desert and looking around and saying, "No, maybe if I move over a little 
here it's better," or "Gee, look at that hill. Look at how the sun moves on that hill...", or "How when 
the wind blows sometimes that little Joshua tree moves." Film is ultimately about people, it's ultimately 
about human beings. It's sometimes also about the settings that the human beings are in and the interact. 
Again there is a tendency l've found where people say "You plug this little thing in and you don't have 
to do what film making is all about." I would like to see the computer used more to generate story boards. 
And I don't mean story boards in the real sense, I meant literally in animating them, having them move 
and seeing what things are really like. I would love to tinker with those ideas--ideas for model shots 
of things I want to see and then we make them and I don't like them and then it's too late. The thing 
that everybody detests about special effects is the amount of time, whether they're done with computers 
or models or zebras, they're terribly expensive and terribly slow and as a film director, film directors 
by nature want their hands on everything. And you give up, some of that is usurped by, you look at a 
story board and the story board looks fine or you say, "No, I want this bigger," or "I want this to the 
left of that." Ultimately then someone goes and takes this very expensive piece of motion control equipment 
and it does what it does and the model flies just that way and you look at it and you say, '7 don't like 
it" and the guy says, "Well, that's what was on the board, and that'll be $35,000 please, or "45,000 
please" and it's too late. I would like to see computers used in those ways where you can actually say 
I want to see what it's going to like if it's going to go like this and turn around and come at you and 
have somebody push a button and have me see that. And I think computers could do that quickly. RICHARD 
HOLLANDER: We have another tape to run fight at this moment. RALPH WINTER: In working on the Star Trek 
pictures, we used a lot of different computer graphics. In Star Trek H, I was involved in post-production 
but we used the genesis effect, which was for some time a standard of what could be done in 525 computer 
graphics and we got a lot of mileage out of that. We used it for three more pictures afterwards. I also 
worked on Explorers and we had two dream pieces that were two one-minute computer generated images. On 
Star Trek IV we used computer graphics and in a variety of pictures we have used computers to generate 
displays and other kinds of tactical displays and other kinds of graphics. It costs a lot and takes a 
long time. It's tough to justify those kinds of expenditures in motion pictures. So at least in the usages 
that we've been through we've tried to minimize that as much as possible, but still we've found ourselves, 
you know with good companies, we've found ourselves right up against the wall on that stuff. I'm talking 
particularly where we've used computer graphics to fill the screen, where it's a completely synthetic 
image and itk expensive. It costs-- as much as everyone says the cost is coming down, it's still very 
expensive. That doesn't mean we're not going to use it, cause we're thinking about and tentatively planning 
to use computer graphics in Star Trek V for a sequence. But again we're trying to control the parameters 
so we protect ourselves. It's within a sequence that is somewhat surreal. It's a sequence that's contained, 
there's boundaries to it. It's not dependent upon principal photography. There's just a number of hedges 
that we have around it to protect ourselves. On the Star Trek picture, for instance, as tactical displays, 
as monitors, as other kinds of graphics that we use. We obviously believe in that and use that a great 
deal, and there's a variety of levels that we use. For some things that are simply eye wash, we use the 
technology that is used in beer signs which gives the impression of maybe computer graphics as eye wash 
wallpaper in the background and that's just a rotating gag, a wheel, that gives you some kind of motion 
but they're crisp and clear because they're made out of film. They're real sharp. For stuff that we need 
to tell a story with and needs to be, it is very important for the characters, there we'll use an outside 
company and rely on their expertise to generate something that is first class, that will hold up if we 
want to push in, but is generally right there within reach of the actors, in focus and is helping us 
to tell the story. In getting information about different, keeping up to date in the computer graphics 
field, of course, there's SIGGRAPH to go to, and see what's happening there. There're other friends I 
have in the business. I'm always interested to see what's happening in other motion pictures--who else 
is using what company for what kind of graphics and general reading material type of stuff. But, you 
get the most out of friends and, in my business, by reading other trade publications that tell you what~ 
happening about computer graphics or techology in movies and I'm interested in that and stay abreast 
of it pretty much. Two areas that we're experimenting with, one with some success and one with not much 
success is story boarding and design. We're using a program called Video Works to do some preliminary 
story boarding, to make story boards come to life. And already we've begun to see some gaps in our thinking 
that that has helped us to solve, but Video Works is not as simple as it's made out to be. It's an inexpensive 
program and I like all those people, but it takes some time to get that stuff in and manipulated. It's 
not quite as straighoeorward that I can sit down with the director and give him the keyboard and the 
mouse and say, "Here you make the shuttle move toward the Enterprise the way you want it to. " It's not 
quite that simple, but we're playing with that and that's making some progress. We're not story boarding 
using Video Works to story board every shot, but simply where movement, timing is of some importance 
and we can give other people shooting the models a better picture of what our thinking is. We're getting 
some success out of that right now. An area I'd like to see more tools developed for us in the motion 
picture business is in terms of designing sets. We're going to a distant location and we need to build 
a set based on certain parameters of what we can see through the camera. And we don't know quite how 
large to build the set. There are some physical limitations. More importantly, it's what you can see 
through the eye piece. An etymorphic what am I going to see with a 75 lens on and how much of the set 
do I really need to have. How far back can I get before I run into trees over here, or a problem here 
or there, and again we're playing with some 3-D software to do that, we're going to a lot of work to 
try to get an answer. Whereas if we could have some other tools that are closer to that we'd be in better 
shape. I think those design phases of the computer can be very helpful, but the tools aren't as simple 
yet for people like me. We're experimenting with something now in terms of some blue matte composites. 
We're going to experiment with high definition TV and possibly digital TV, putting elements t together 
using NTSC technology or high definition technology and then spitting it back on film and if there's 
a way to integrate computer technology in that in a cost effective manner that gives us the imagery that 
we want to get...yeah, I'd be interested in that. If I wanted to go through a blue matte process and 
that cost and time of an optical house of putting it together and can get pretty much the same quality 
in quick cuts that no one will notice, I'm going to embrace that technology and find a way to use it. 
And if there was a way to use CGI in a sense of a synthetic background or a place that you go visit that 
you can expect and use a surrealistic background, yeah, that would be good. At the moment a lot of that 
doesn't hold up to the specifications that we require so that I can go to the Village Theatre in Westwood 
and watch it on a 60ft. screen. The expense of getting something first rate is just prohibitive. Everyone 
says high definition TV is right here. I think that's just a lot further away than what people want to 
think. I think film has amazing qualities about it that are not going away. A lot of military research 
is still done with opticals and optical enhancement using film. I'll be the first one to embrace the 
television technology when it gets ready to shoot movies. It still lacks what film and celluloid can 
do. But let's embrace those things, let's find ways to make those things work together. I think it's 
going to be a meld of those things, not just one or the other. Film's going to be here awhile, it's not 
going to go away. RICHARD HOLLANDER: My name is Richard Hollander, and I guess that the saying is "Ralph's 
opinions on the product mentioned do not represent ACM's or the panel members'". When Michael and I decided 
to do this panel, we wanted to know what the state was, and why there wasn't a lot more being done in 
the motion picture industry. And we went around and asked quite a few people, over the telephone, and 
in person, and with these video interviews What was going on. As we will ask these three individuals 
here. And I found out immediately that all these people had an idea of what CGI was. Four years ago, 
that was not the case. Sometimes their notion of what it was was quite interesting. And it's really different 
now when you go to talk CGI to the outside production community. They all thought computer graphics was 
unique. And it looked like computer graphics. And that meant that they always and still sort of vision 
it as it has a place to go, and that place is when it's called for. And they're not really looking at 
it in other venues. And, unfortunately, when I spoke to all these people, I got bashed in my brain over 
and over again that this was expensive. And I can't tell you how many times I heard this. And I want 
to make sure you get the point: it is definitely expensive. It started to burn my ears so bad, and I 
think our panelists will also agree: it's expensive. I think we're going to go immediately into questions 
with our three panelists here, and maybe they'll say something about expense. And I'll start it off with 
Mike, because he's the closest. And I think that the real question is -- we'll lay all the responsibility 
on you, Mike -- is how are we going to make computer graphics successful in the motion picture industry. 
Is there a chance? MICHAEL FINK: How would I make computer graphics successful in the motion picture 
industry? There's a chance. There's two major things about computer graphics in pictures: there's two 
ways to look at them and I think they're described in the tapes. One of them is as background information 
as part of a story, and the other is as a method to actually synthesize all of the imagery that you see. 
If you think that you have an idea for a motion picture that would use computer graphics to develop imagery, 
that would depict the entire motion picture, or at least the background of a motion picture, if not the 
characters-- but hopefully the characters, too--then that script that you write has got to be better 
than any other script out there. The fact is that computer graphics carries --those dollar signs should 
still be out there - -such an onus that if you have computer graphics that are a the major part of a 
motion picture, and you don't have a script that actually exceeds anybody's expectations in its quality 
-- I mean story and characters -- the picture probably won't get made. People aren't --nobody who finances 
a movie is interested in selling a beautifully generated motion picture made out of graphics. There are 
a lot of beautiful motion pictures out there that never went anywhere, and they didn't bother going to 
graphics. So the first thing that if I was to see a bright future for computer graphics I'd have to see 
it -- if it was used as a basis for making a movie, then that movie's got to be wonderful. I mean, it's 
got to be fantastic. It has to be better than anything else, because it's the only way you'll get the 
enthusiasm going. The other future for computer graphics, which is the future that we're living in right 
now, which is computer graphics as story adjuncts, bits of information that you put up on a screen or 
a video monitor, or behind somebody -- to use Project X as an example; it's a picture that I worked on, 
where we did flight simulation graphics. They were part of the story but they were not the story. There 
were a number of ways we could have made those images. In fact, we talked about them early on. I produced 
about three hours of graphics for that picture. And if you think about how much that would have cost, 
had we had to do it from scratch, it would have been a fortune. Luckily, I had the help from General 
Electric down in Daytona Beach, and they provided a lot of stuff for very little money. And that's why 
that movie got made. Fox literally told me, 20th Century Fox sat in a meeting with me and said, if this 
picture costs as much as you think it could cost because of the graphics, we won't make it. That particular 
piece of graphics was done on a real-time simulator. MICHAEL FINK: That was done on a real-time simulator. 
Real-time is a very valuable tool. If we had not been able to produce the graphics in real time, we would 
not have been able to make the picture in time. If we couldn't have made the picture in time, Fox wouldn't 
have made the picture, no matter how much it cost. So it isn't just money, it's also time. Another picture 
I worked on where graphics was a part of it but not the story was War Games. In War Games, the picture 
literally got finished simply because the picture went into what's called in Hollywood, turn-around, 
twice. Which means the studio held it up because they felt they didn't have enough money to finish it. 
Or they were no longer interested in it. And they let it go to the next higher bidder, and somebody else 
bought it out and then continued to make the movie. And in the two turn-around situations in that picture, 
they continued the computer graphics effort, despite the fact that the picture had been brought to a 
halt. And we made use of that time in turn-around. If we hadn't had it, if the picture had gone on its 
normal production path, we would not have finished in time and I don't know what would have happened, 
no matter how much money we had. And we had some very good people there. So the two hurdles, one is if 
you want to have computer graphics movies, have a movie that is incredibly fantastic story. And if you 
don't have that, then you have to fall back on what computer graphics does now for every movie that uses 
it, which is background. And then you have to deliver it for a price, on time. And that's not easy. WAHRMAN 
: I'd like the panel to perhaps discuss -- in many conversations, I came across the perception of risk 
in using computer graphics. I have one unnamed development executive who gave me a quote. A development 
executive or production executive at a studio is someone who's preparing pictures. And he gave me the 
following quote. He said if the best unit production manager in Hollywood came to him with the most accurate 
budget ever made that contained a substantial portion of computer generated imagery, he would think to 
himself, "Liar, this --bad word deleted -- is trying to get me fired." Is there anyone on the panel who 
wants to comment on that? MARTHA COOLIDGE: I can tell a story. Well, I think maybe appropriate to that 
is a story about the opening sequence in Real Genius. The opening sequence in Real Genius was meant to 
be a computer generated graphic moving image, a sales pitch, basically, made to the men purchasing, or 
hoping to purchase this weapon, that demonstrated precisely how it worked, and it kind of set up the 
whole picture. And it was rather important to me that we see exactly how this weapon would work. And 
it was three minutes long, approximately, and we went out and had it budgeted. the budget came in, the 
initial budget, for over a million dollars. Which, of course, didn't surprise me at all. I was not surprised 
and I felt that's really the real high-tech portion of the picture and that's where we should spend the 
money. But the producer -- the executive producer who's the production executive essentially --said, 
no, no, these things always come in more. You can't trust it, it's unreliable, and we don't know how 
long it's going to take, and we can't spend a million dollars anyway. And the studio completely concurred 
and we struggled and simplified the images, and reworked the entire sequence, and decided maybe to go 
-- instead of with computer generated images -- to go with model shots. Finally got the thing down to 
$500,000, at which point the studio looked at it and said, look, this just isn't worth it. Model shots 
take too long. This is a simple movie, this is not a special effects movie -- little did they know. They 
do qualify these movies as either special effects movies or not, really having nothing to do with whether 
not they have special effects in them, but having to do with how much money they want to spend on them. 
And then said, look, why don't you just cut the entire sequence out of the picture. I went crazy. This 
is a director's typical role, throwing fits, saying you have to set up the danger and the risk in order 
to tell the story. You have no story if you don't know what the danger is. And so then they said, okay, 
we'll give you $40,000, you can do it with a slide show. So at this point, the exec producer and I put 
our heads together -- we had some laser shots. We were planning to do a lot of the laser effects in the 
picture with real lasers, but we did have a budget for optical lasers. We had other computer graphics 
-- what most people in the business think of as computer graphics, meaning graphics on video screens 
that are coming out of computers, in the movie. We had a budget for that. So what we did is we stole 
money out of those two areas of the budget, threw it in with the $40,000, and came up with something 
under $200,000 to go out and find some poor company that would make us model shots for this paltry sum. 
We ended up having these model shots done. It was a very difficult experience because the company under-budgeted 
themselves. And this is the typical problem and why all special effects seem to be unreliable, and particularly 
computer effects, to the budgeting executives in the business. And one of the reasons is they squeeze 
a budget down to the last dime. So you end up promising that you can deliver something for an amount 
of money that you can't possibly do it. And in an amount of time that you can't possibly do it. And there 
we were, waiting right up until the last minute -- really the last week before we released the picture-- 
for these images that went into the very beginning of the picture and ended up, of course, going over 
budget and over time. But, of course, nothing close to the original million dollars budgeted. And, of 
course, nothing approaching the quality that I wanted. And this has been typical every time I've talked 
about using computer graphics in a movie. They're just terrified. And I think one of the reasons really 
you brought up is that, in the movie business, a movie is essentially an idea until it's a go. There 
really is no research and development money put into creating new ideas or fresh ways of approaching 
something. And Mike's story about War Games is unique in the sense that these people were willing to 
put money into developing something while they were still not given a go on the picture. That money had 
to come out of somebody's pocket. The studios don't spend money on a movie until they're ready to make 
it. And once they're ready to make it, then time is of the essence. And they have no time. It's the most 
amazing thing -- there's an expression, hurry up and wait. It's either hurry up or wait -- one or the 
other and sometimes both -- every other day. But once a picture is a go, then time is absolutely critical 
and no one can wait around for new ideas to be presented to them or images to be come up with or anything 
approaching that. And I can tell more stories later. WAHRMAN : Thank you. Michael Backes, when you were 
at Vestron, was there a perception of computer animation as particularly risky? Was there much interest 
in doing a project with computer animation? MICHAEL BACKES: No. I think mainly the perception -- when 
I was production executive -- was that computer graphics were something that was really limited to the 
word processing screen in your secretary's office. I think primarily the reason for that is that the 
company that I worked for did very low-budget films. Low-budget meaning under five to six million dollars 
as a rule. I took several meetings with people who said they were going to put computer graphics in films. 
And it's really unfortunate, because what happens is that usually, when you're doing lower budget films, 
the people who come and pitch computer graphics, don't have a clue what they're talking about. One incident 
that happened was I had a gentleman come and pitch a story to me and he said there are about 15 minutes 
of computer graphics in this film. And I knew how much computer graphics cost. And I looked at him aghast 
and I said, well, what's the budget on this. And he said, well, we're going to do this movie for 3.2 
--$3,200,000. I said, well, what do you have budgeted for the computer graphics? He said, we're going 
to do the computer graphics for about $40,000. And I said, well, what system are you going to use? And 
he said, oh, I found this guy who's got an Apple II and this animation program that you won't believe. 
Anyway, it was really amazing. But my whole feeling, having made a transition from production executive 
over the fence into doing some graphics consultation, is that what Ralph Winter said in his film presentation 
about the ability for computer graphics to get in the door almost immediately as a visualization tool. 
Whether it be story-boarding, animatics, an adjunct to videomatics --animatics and videomatics being 
little previews of what certain shots in the movie will resemble, some of which will be live action, 
some will be composites and opticals and employ motion control. As a previsualization tool, I think that 
is really how Hollywood will become educated as to the nature of computer graphics. I think the picture 
that I'm working on fight now --which is Jim Cameron's new film. Jim is really one of the most savvy 
directors I've ever met in relation to technology. He really knows what's going on. He's also extraordinarily 
suspicious of computer graphics. But he also sees that, as a visualization tool, there is an extraordinary 
amount of potential. And perhaps, if we have some time a little later, I'll discuss, in a little more 
detail how we're using those techniques. HOLLANDER : Well, this all is really depressing. Let's see if 
we can get some opinions on where the future lies -- in a positive sense. BACKES: I personally have a 
couple of opinions about that. I think it's still going to go on. I think that I agree with Michael about 
it. But I think that it's going to have to start making its way on an every-day basis, that it's not 
going to be the hero shot of the movie that's going to make or break CGI in the industry. I think there's 
a lot of other, more mundane efforts, that need to be researched, such as Ralph Winter had indicated 
that he was interested in animatics. So did Peter Hyams. Animatics aren't new, it's just that they aren't 
used. And people aren't taking the effort to try and use them. There are major special effects facilities 
doing motion control work and they don't have animatic capabilities. There were effects facilities that 
had this. Maybe in the near future, that will all change. There's other areas --some post-effects --that 
typically have been done on optical printers and other type devices, mat painting. There's a certain 
amount of things that can be done in the mat painting domain. We're not talking really far future. And 
the same with 3-D backgrounds, which is sort of like another mat painting. But if any of you have any 
specifics and want to jump in on that. FINK : I'll address this because I'm using something on a picture 
right now that I think is actually going to have a big impact. Often, when you're doing a special effects 
film, the director doesn't get to see what certain things look like until they're done --as Ralph addressed, 
and Peter Hyams. Where you give somebody $50,000 and a couple of story boards and send them off and they 
send you back a shot that sometimes is perfect and sometimes bears little or no ressemblance to what 
you originally had intended. And so what we try to do at 20th on the film I'm working on now, is to develop 
a system by which a shot can be previewed, in real time, on a microcomputer. A big order of goods. So 
what we did was we started with Macintosh II's, we found a programmer in the Carolinas who was working 
on a game system that used real time for 3-D techniques. And we gave him a blueprint of one of our sets 
and said, enter this blueprint and make it so that we can just move the mouse and move around the set. 
And it works. It doesn't work in the sense that we don't have the ability to flit the camera. We can 
move in real time back up, move forward, turn around, etc. But it is a visualization technique. The art 
department went ga-ga when they saw it. They really liked it. How useful is it to the film? It's relatively 
useful. It will become more useful when we can simulate a camera more accurately. Then the other thing 
that we've done on the film is that we've computerized almost all our 2-D story boards. They're still 
drawn by an artist, by hand. But what happens is that then we scan them in the computer with 256 levels 
of grey, and then we can manipulate them. We can pull them out, arrange them into different sets. We 
can pull out our wet-for-wet shots or our dry-for-wet shots or our wet-for-dry shots or our motion control 
shots or our CGI shots. Because this movie does have some CGI in it. And it gives you an extraordinary 
amount of power. And everybody who comes in contact with it is extremely impressed with it. Therefore, 
I think that, in the long run, these kinds of visualization techniques will allow a lot more directors 
to become educated as to how CGI works. And they'll trust it. And when they trust it, they'll start to 
use it. MARTHA COOLIDGE: Also, just to throw in --directors will like it a lot better when they can put 
their hands on it and change the story boards themselves. One of the most frustrating or productive relationships 
in the film is the director and the story board artist. All special effects films are story boarded, 
at least in those sequences. And many other films are, tOO. And I know as a director -- I started out 
as an artist, but I don't have time to sit and draw all the pictures. And talking to that artist, and 
then the artist comes back with a series of pictures. I know I'm always scribbling over them and changing 
them. And if I could get story boards which I could really manipulate in a professional way, quickly, 
that would be very effective. And the way that the special effects previewing works -- which the production 
money people could really understand, is -- and I don't think anybody quite mentioned except Peter Hyams 
--is that these shots cost a fortune. These model shots, Space ships and dog fights in space and multi-elements 
including planets and everything. And the problem is that it is often very difficult to visualization 
them. Some directors are not that visual at all. They're very people-oriented. And to judge how exciting 
a shot is going to be is very difficult on still little drawings. They'll draw the -- first the ship 
is here, then it's closer, then it's there. But it's very hard to imagine. Plus, you can even get your 
story boards so you're very happy with them, give them over to the model company, and they come back 
and they say, sorry, we couldn't do it that way, we had to amend the motion in this way. And then you're 
shown this shot that doesn't ressemble the shot that you had in mind at all. And you're still paying 
$40,000 for it. And that's where, if you could preview each one of these shots with the ship, the background, 
the planet, the other space ships, whatever, would be incredibly helpful and productive. Save time, save 
money. As long as it didn't take more time and money to make the preview than it did the shot. In terms 
of the mat painting idea, it just occured to me Richard --and you guys might know more about this -- 
but I know one thing that I've been told in all the mat paintings that I've had done, one thing that's 
very difficult is hard smooth objects in mat paintings. That's a very tough thing for mat painters to 
do. They do it. 11 HOLLANDER: It depends on how they're lit. MARTHA COOLIDGE: Yeah. It does depend on 
how they're lit. But I know that, for example, in Twilight Zone when they were struggling against a schedule 
that they were always falling behind on -- this is in Twilight Zone, the television show, the recent 
one. I did a show where, at the very last minute, we needed a space ship to blow up and they just didn't 
have it. Somehow they came up with an image that was computer generated. It wasn't very detailed. But 
that didn't matter on television. It was a short shot, and it didn't matter, and they put it in there. 
And I was very surprised, but they did it. And I was very happy about it. But this is in the level, I 
know, that the mat painters do not paint the same mats for television as they do for features. Now in 
features, I think several people here mentioned the look in computer generated images is not good enough 
yet. It doesn't satisfy feature directors, in general. It just doesn't intercut well with real life footage 
that has atmosphere, has dirt, has the randomness that Peter Hyams mentioned, whatever, It just doesn't 
quite look fight. And it has to look perfect. It has to intermatch perfectly for people to start accepting 
it as real life. But in television, the whole -- because first you're going over video and, secondly, 
you're going into people's homes, and everybody knows that. And, thirdly, is that huge financial pressure 
of a much smaller budget, always lurking over your shoulder, and shorter time schedules. If there were 
a library of images that you could go to, someone who could supply images quickly and not necessarily 
in the detail required in features, which I think the technology is up to now, I think in terms of the 
technology, at this point in television, it's not the look, it's the money and the time that's prohibitive, 
and not the look. And I think that that's a break-through area. HOLLANDER: Well, sure. I always think 
of these things, what Martha was just saying, as what I call my plastic cabinet analogy. You all remember 
televisions in the -- some of you do, maybe some of you don't --in the '50's and in the '60's, predominantly. 
Or the early Mr. Coffee's even? And they looked like plastic with fake wood grain. In fact, you can still 
find them in your best target and discount stores. My feeling about computer graphics is that it's looked 
at very much the way product designers looked at televisions in the '50's and '60's. They feel that it 
has to look like what everything looks like. Peter Hyams said it over and over again: it doesn't look 
real, so I don't want to use it. The fact is that televisions now, if you look at them -- just look at 
these monitors here -- are looking more and more like the machines that they are, and I think that they 
are finding actually a wider acceptance as something to put in your house as a piece of decoration than 
they used to be. And people are appreciating them as the objects that they are as opposed to the objects 
that they wish they were if they could only be. And movies are the same way. And you can't make a movie 
now with computer graphics that's going to look like a movie that you would make in film, if the imagery 
was the same, given the same imagery. But you can make the computer graphics movie that has a look all 
its own, that's really special. All you have to do is sit through the film show that we saw 12 last 
night -- I saw it last night and some of you have seen it two nights in a row and maybe you're going 
to see it three nights in a row, and I think that's great. The issue there is that a number of the images 
there looked very much machine derived and were astounding. They were interesting images and some of 
them came complete with very interesting cute, funny, riveting, compelling stories. Some of them weren't 
very complex and they were compelling. I think the point here for computer graphics in the future is 
that if -- over the near future, certainly -- is that the desire to make computer graphics into the most 
perfectly rendered vision you've ever seen is not necessarily the thing that's going to get computer 
graphics accepted. But the desire to make computer graphics fill the need to sort of present as itself, 
I don't know how to do that. MARTHA COOLIDGE: I'll say one more thing: actually, it's interesting, based 
on what you're saying. I'm working on a film now with Mike here, which is trying to do that. It's basically 
a movie of computer generated images married with a few live action images. And we're trying to base 
the look of the film on what the computer graphics are capable of, and then, the struggle is how to deal 
with the live portions and marry them. And it's been a very challenging and interesting experience. And 
I'm very hopeful about it. We think that when we do finish this, it will open some eyes, in terms of 
opening people's minds up this way. But I';ve just jotted down a couple of areas. Obviously -- just areas 
where computer graphics really have an edge, if the money and the time and, in certain cases, the look, 
can come down. But obviously, space, that's clear. But historic -- I mean, people may not think about 
that, but you see it's very prohibitive to recreate historic places. Sometimes movies are not even made 
because one can't possibly think of where they would shoot something that looks like the original building 
of the Great Pyramids or the Hanging Gardens or Eden. I mean, incredible places that are famous that 
we know what they look like, but nobody knows how to build them or where to find them. And certainly, 
in that sense, a realistically rendered backdrop or mat painting, something in that order, would be very, 
very productive and helpful. You don't even think about this, but in one of the Raiders, they spent weeks 
removing all the television aerials out of one of those towns in Tunis or whatever, because you can't 
look out over a town in that period and see television aerials. They had to pay all these people to take 
all their television aerials down. Fantasy and dreams --that's a perfect place. Now, we go through periods 
in the movie business where you make fantasy pictures and then you don't. And certainly dream sequences 
-- I think there you start to move into what computer graphics do best and have their own unique look. 
And it could be terrific. Magic effects, of course. And what's nice in the return interest in cartoons 
with Roger Rabbit, I think certainly in the film show, it's quite clear that that's an obvious direction, 
and cartoons have not been that popular recently. And then also areas that you can't get into. Not necessarily 
historic, but an area that you cannot go to. A famous place in Moscow. Or the Pentagon. Or certain areas 
like that where you're doing a very important film, you have to have this area, and it has to be recreated 
in some way. Those are just thoughts. HOLLANDER: One of the items I came up again in my surveys was other 
than cost and time of delivery -- because apparently there are some good quotes about famous disasters 
in delivery on computer graphics. But maybe we won't go into that. There was a lot of discussion of character 
animation. And there was a feeling that characters in computer graphics were not expressive enough. Or 
not cost effective enough. Do you think that makes a difference, or is that an area that has to be addressed. 
MARTHA COOLIDGE: I think it makes sense. You have to understand that animation costs a fortune. One of 
the reasons people haven't done movies which are animated -- except for Disney, and they've, as you can 
see, tapered it down --is because the individual animators, everything costs a fortune. And it was only 
the automation of Xerox reproductioo and all that stuff that made animation possible at all to continue. 
And foreign country labor, which they also used in Roger Rabbit. I don't think you're going to see a 
slew of animated films coming out, even with Roger Rabbit being the hit that it is. You will see a sequel. 
But I think that that is a problem, and I was noticing that last night. Just my own personal opinion. 
I think the more attention to detail -- and I was wondering what it is, and maybe somebody could give 
some information, about computer animated characters often don't quite have the personalization that 
you find in really good cartoons. There are really bad cartoons made by animators, too. A lot of them 
are on Saturday morning. But the best have very, very personalized character traits. And that may be 
more the point you made, Mike, about script and story, and character ideas. It's, to me, the best mar[ying 
of character ideas, execution and story and look, where really is the PIXAR film for me, in terms of 
the movie business. It really, really is so strong in each one of those areas. HOLLANDER: Well, let's 
open up for questions outside. And when you get up to the microphone, please state your name avd any 
affiliation. And anybody have any questions? Q: Robbie Rontrielli, Computer Animation News People. I'm 
just going to answer the point that was just made about character animation and why it really doesn't 
look as good as it could yet. If you go back a couple of years, it looked even worse. And it's getting 
there very quickly. One of the reasons is that we need those traditional animators to tell us how to 
make the computers make the characters look real. The computer animation people are still too busy -- 
and they have to do it this way -- learning how to make everything photo-real. Because until they can 
make it photo-real, they can't take away the unnecessary bits to come out with the essence. Where the 
traditional animator has used that. He spends all day looking at the world around him. And I think it,s 
just a matter of time, and one of the real problems is getting more of those traditional animators used 
to using computers so that they can put that kind of input into it and come out with the images --such 
as John Lassiter and Chris Wedge and some of the other people, who are traditional animators, who are 
now using computers and they are leading that field. So I think it's a matter of time, and everybody 
please try and get animators to use computers because that's what we need. (APPLAUSE) FINK: I think there's 
also another issue there, too. And one thing was raised just briefly: the idea of randomness. Computer 
animation needs more dirt, I think what it is. It has a very sanitary look. I think probably we'll see 
a program from PIXAR soon called, ChapDirt, where you can just throw a little dust on an object and it'll 
look terrific. But I agree. I think in two years, there will be a film at SIGGRAPH, from somewhere, that 
will hit the photo realism. And then I think once the photo realism has become widespread, I think everybody 
will take a couple of steps back and say, now, how can we make that look blurry. And I think that's where's 
the real style. I mean, you definitely see in a lot of what we saw last night and the night before at 
the film shows --see a lot of progress. It's just staggering. And I think it's going to be reached incredibly 
quickly. And then the cost will come down. I have a very optimistic feeling about computer animation 
in the motion picture industry. I'll tell lots of horror stories because there are lots of horror stories. 
But it's still in its absolute infancy. Q: I'm Lance Williams. I'm a computer animator. I'd like to get 
some expansion on the subject of cost. We heard over and over again that computer graphics is just too 
costly for extensive use. And at the same time, it's never going to make an impact until it can handle 
the more prosaic jobs of filming, rather than just the hero shots. People outside the film industry need 
to have a little more information to judge costs by. If you see that a picture cost $12 million, well 
you can sit down and figure out, well, that's $2,200 a second. Live action --even though it happens in 
real time -- is not so very cheap. Now, if the hero shot is the object and you tell the computer graphics 
studio that you're going to have to model the Eiffel Tower, the Brooklyn Bridge, and the Great Pyramids, 
and then we want to blow them up in five seconds, that's going to be expensive. MARTHA COOLIDGE: Well, 
first of all, you have to know that it seems ridiculous, but the money that --well, you can help me, 
too -- but first of all, a lot of money is spent in movies on the people. A huge amount of money is spent 
on the people. That's not just the actors. It's a very labor-intensive business. It's not just the actors 
though, God knows, most of these 20 and plus movies have stoars that cost multi-millions of dollars. 
So you have to -- when you talk about above the line and below the line, your above the line can be way 
more than half of the movie budget. And sometimes you'll have a movie that's, say, $10 million, and it's 
actually being made for $4 million. And that's kind of one of the shocks of the business. The other is 
just equipment, moving people from here to there. And finally, building the sets. And when you finally 
take all that away, that's why we say it's too expensive. It's a very dollar-minded business, and they 
literally count nails. And it's also a business in which they have to watch cost over-run, and particularly 
in the larger budget pictures. For example, in a $4 million picture, it's very difficult to go over budget. 
It's simple. The areas that you're watching are very simple to watch. In a $20 million picture, you can 
go $10 million over so easily because you've committed yourselves to things. HOLLANDER : In weeks. MARTHA 
COOLIDGE: Literally, yeah, several weeks. And you've committed yourself to things that once you start, 
you can't stop. And that's why the production executives are so terrified of anything that is risky, 
experimental, that they can't say, we guarantee that it will be delivered at this time and on this date. 
HOLLANDER: Yeah, I have a lot of those stories. The issue of how much there is to spend on a particular 
CGI effort is very real. And what Martha was saying is true. I don't know if I'm accurate in this, but 
I think that Running Man -- the Arnold Schwartzeneger picture -- which came in at around $30 million, 
actually they had about $8 million to make the movie, by the time they got down to what was left. And 
there's a lot of video -- and I call it video and not computer graphics, although there is quite a bit 
of computer graphics in the picture. But it's shown as kind of video. It's shown as television, so I 
call it video. So it is a big item. You see the $12 million, or $15 million, or $20 million, but you 
look at Rambo III which cost $63 million, and you look at what was on the screen. How much did Stallone 
get -- $15 million, I think, off the top. By the time you start subtracting those figures, there isn't 
much left to make the picture. So it's real. The need to have the stuff delivered on time is real. And 
it is very expensive to do high- quality graphics. If you want to challenge us on that, I'd love to hear 
somebody say that they can do wonderful stuff, 2,000 or 4,000 lines resolution for $300 a minute. We'll 
talk to them. Q: My name is Mark Cantor from Macromind. We're the people who do video works. First of 
all, I want to thank Michael and Richard for having a panel that doesn't say the word B-splines or ray 
tracers. I think that the issue that Lance brought up is really important. The one thing that I try to 
do when I talk about video works is how the animator can use his time better to be an animator and not 
have to be a computer programmer. I think any computer program that forces an animator to have to type 
on a keyboard and type in cryptic computer commands, is really hurting the animator. So that the animator 
has to be a computer programmer. So I think it's important to have animation programs that deal with 
animators like animators. Obviously, Ralph Winter doesn't have a very good video works animator or else 
they'd know about real-time recordings, so Will Shatner can sit there and move his mouse and get the 
Enterprise wherever he wants it to go. : But it's a 3-D movie he wants. MARK CANTOR: That depends on 
the amount he wants to spend. If he wants to spend $200, whatever, he can get something real sleazy real 
quick, if he wants it. Again, I think for the amount of feed-back those directors will be getting, to 
give themselves an idea of what they're going to get, maybe it doesn't have to be in 3-D. That's one 
thing. : I disagree, I think, in lots of cases it does. These are pretty sophisticated people who've 
dealt with story boards before and now they're trying to jump to another domain. And this 3-D notion 
adds something that they like. : It's imperative because motion pictures are about space. They're not 
about planes, they're not on a flat plane. And I think as far as story boarding goes, the way computers 
can go is really away from the concept of a 2-D story board towards the concept of a 3-D story board. 
MARK CANTOR: Okay, so if our aim is to spend $40,000, $50,000 on a shot, but then I also want to be able 
to get an animatic of it, then we better come up with some real-time 3-D graphic chips out there to create 
really cheap workstations so that people can do that for $2,000 or $3,000. Again, it is all about money, 
and if the total cost of the shot is $50,000, then you don't want to spend more than $2,000 or $5,000 
to rough it out, I would think. BACKES: Yeah. In the case of the movies I'm working on right now, I was 
told basically that, yeah, we already have the computers but you're going to have to go out and get all 
of the supplementary hardware to get them to do this kind of 3-D story board stuff. So I went, hat in 
hand, to Tektronix, to Jasmine, to Supermac, to Silicon Beach Software, to Macromind, to a lot of companies, 
and got their support. Because a lot of these companies realized, hey, there is a potential market here. 
The one company that I did not go to was Apple Computer because when I called Apple Computer and said, 
hey, we need computers for this movie -- and I'd just done it on a previous film, dealt with Apple --Apple 
said, well, great, sure, we'd love to give you some computers. Now, first of all, you have to understand 
we have to read and approve the script. At this time, there were eight copies of the script. We had stars 
who were making $3 million a year coming to our office to read the script. And the director said, not 
a chance. So we got no support, actually, from the computer manufacturer itself, but everybody else was 
willing to jump on board. And I think that there's so much future there, and it's going to happen so 
fast, and I think that a lot of people who are in this audience will be surprised what they'll see on 
the screen in the next couple of years that is done, basically, started out from a desk top. With a desk 
top PC. MARK CANTOR: Just to conclude, I think that the real essence here is in the user interface of 
the software so that people like Will Shatner or whoever can animate for them. They ask for it, people 
specifically ask for it, so it's all your job to make it easy to use software. : This is to finish that. 
This is sort of an unfair comparison, because I was using a multi-million dollar system in a very large 
facility. But when I went to General Electric to work on Project X, I was able to sit down at a desk 
with a joy-stick and fly the simulations we needed. I didn't fly them very well, but I could do it. I 
could choose the airplane, and the characteristics of the airplane and speed, maximum speed, whatever 
we needed to determine, we could determine. With a few keystrokes, it was done. And I could sit down, 
and when I did that, it was the first time that I realized that the pre-visualization process that Michael's 
talking about is an incredible tool. That's where I decided that exactly what we've been talking about 
here is what we need. We need, we desperately need, a three-dimensional visualization tool. One that 
allows us to key in what lens we're using, so we can get the right horizontal angle or the right vertical 
angle. These things are real important. That will tell us how big we have to build our sets. It would 
be a fantastic tool and, right now, there isn't anything like that. Q: I'm Alex Singer. I'm a film director. 
And I want to just make a couple of remarks on what we've heard. I had about 143 things to say but the 
people up there, of course, have said it. And the videos we saw covered the terrain marvelously, I thought. 
A couple of things -- what clearly is going to happen is that the major break-through will be in the 
form of a picture that makes money. It will be that simple. Clearly, you've been suffering from the experience 
of Tron and a couple of other goodies -- the Last Star Fighter, and so on - -which suffered from very 
traditional things like bad story, bad film making, and so on, from which they never recovered. If somebody 
had said to you three years ago, how would you like a gigantic hit with cartoons and people. I would 
have laughed at them; everybody would have laughed at them. Well, they're not laughing now. Quite aside 
of what I think of it, we're going to be deluged with disastrous copies that, of course, will go down 
the drain. For reasons that have nothing to do with the technology, nothing to do with the skills whatsoever. 
It'll have to do with the peculiar resonance of cartoon characters and story that are unique in the personal 
history of most of the audience, which cannot be reproduced. They can do sequels, but they cannot reproduce 
it by having, say, Daffy Pig. Or Goofy the weasel. It won't quite work. So that's one thing. We're going 
to see a rush of activity when the first big hit comes through. And I would guess that --my assumption 
is that the film that Martha Coolidge is working on has built into it those things that make valid the 
use of computer generated images. When they are unavoidable and intrinsic to the story, they'll flow 
naturally to what they have to be. A couple of other things: the other areas that Michael, for example, 
was talking about. The slow, the gradual, painstaking, familiarization process with a frightened industry 
that has, incidentally, never, ever spent any money or research and development. On anything, by the 
way. Anything in its entire history. All that stuff comes from outside. It comes from guys working in 
their garages. And did from the turn of the century on. That the process is slow, but will be enormously 
aided by the visualization tools, if it makes you feel any better, there's a small group of directors 
who actually care about this and feel that this entire experience is an analog for the events of the 
next 20 or 30 years, and are making an effort to become computer educated -- computer graphics educated. 
And I'm involved with them and that's why I'm here. And it's interesting that I didn't know about all 
those other people up there who are in the same business I'm in and doing much the same things. But that's 
the way it is, that's the way our business is. It's very fragmented and it goes off in many different 
directions. But, take hope, there is some work out there. (APPLAUSE) : I have to remind you that there 
is a thing called a break-out room. It is provided for you if there is interest in continuing any of 
this discussion after this panel is over. This panel ends in about another 14 minutes, and it has to 
move to the break-out room. It's in room number 303, and it says here it's just down the hall from the 
auditorium. I don't really know which way, but it's close by, I'm sure. Q: My name is Evie Pauley. I'm 
production designer for Walt Disney Imaginary. I want to thank you for being here, you've given us a 
tremendous amount of really valuable information. Regarding your horror stones, handing off story boards 
to production companies and getting back something that looks nothing at all like what you visualized. 
I worked as a free-lance graphics designer for six years and never really worked with the client while 
I was doing my art. An unfortunate but necessary evil, having to actually make changes that always happen. 
But whenever I've done computer graphics, I've had the client sitting there much of the time, watching 
in real time, making comments. I've found that it's enhanced their creative process. Instead of it taking 
longer, it's stepped up production. It's increased my ability to give them what they want. So that's 
one remark on that, it's just unfortunate you haven't had that experience. : You can't do that very often 
in a movie, because movies are basically a parallel process: everything happens at once. : In the case 
of the film that I'm working on fight now, the director can be 3,000 miles away from the people who are 
doing the computer graphics. And actually, we've even investigated the chance of using a satellite up-link 
so that each day everything that was being done by the motion control and CGI houses, could be beamed 
to this location in the wilds of South Carolina. And you talk about trying to convince somebody that 
computer graphics are viable. Wait until you try and convince them that you need to rent Comsat. That's 
a real challenge and, needless to say, didn't work. So we're using a really high-tech solution called 
Federal Express. EVIE PAULEY: We've come a long way, I think, with the demise of the monster CGI production 
companies. There are a lot of small companies that are going to start giving you the prices that you're 
looking for. And the art direction has come a million miles in a good direction. I have to make one remark 
for the audience's benefit: Martha had made a reference that Disney had cut back on their animated features, 
but actually they've stepped up production. Q: My name's Howard Baker. I'm a traditionally trained character 
animator from Cal Arts. And I work at ... images as a character animator. And I guess the thing that 
you're saying is very depressing and I guess that it's going to stick me in commercial animation for 
the rest of my life, I'm going to have to advertise for toothpaste for years to come. But the thing that 
our clients --and I hear that you're asking for also --is the high-end graphics. And it seems like they're 
asking for the content of the imagery to sell your idea much more than the content of what the character 
or something is trying to say. And what I'm saying is talking about the story, you were talking about 
story quality before, also. And it seems like our clients, as well as the movie industry, is asking for 
things that, at this point, are pretty much impossible. You're saying character animation or computer 
graphics is in its infancy. You are asking for very mature answers already. And I'm just wondenng if 
anyone has any interest in actually doing a low-end film, a low budget film using low budget computer 
graphics instead of the high-end flowing hair, flowing things. : You mean the first computer-generated 
blob monster movie? HOWARD BAKER: Not necessarily as much as something that's just simple. Something 
that has --instead of being very, very complex and very realistic, or even just having so much information 
to deal with that you become overwhelmed. I think Michael Wahrman is the perfect person to address this. 
He should talk about this. Michael? As far as character animation. MICHAEL WAHRMAN: I'm going to rephrase 
your question, Howard. Why don't we take the tools as they exist now, find something clever, and make 
a single character in a movie and try and pitch that. Try and make a movie tailored around what the technology 
can do to do that look. HOWARD BAKER: Right. MICHAEL WAHRMAN: I think it's a great idea. You want to 
work on a screen play? MARTHA COOLIDGE: But the story is really critical. It's what Michael said: you've 
got to have a great story, and everything is really script-driven. And then doing a full- length or even 
short cartoon is relatively unheard of in this business. Though certainly, you'll see a few people trying 
now. And this would be the time to do it. : I actually prepped a show which never got made, about four 
years ago, in which there was a computer graphics generated real-time interactive character. That's what 
they wanted, that's what they asked me for. That's not what I could give them in 1984, but we got pretty 
close. At least, this was one picture that was willing to spend the massive amount of six weeks to do 
all the research and development necessary to come up with whatever we needed to create this character. 
But the fact is that this character was a central character in the script, and the script was wonderful. 
And at the last minute, the studio backed out and said they wouldn't handle the picture. And it really 
didn't have anything to do with the character. It is possible to get a picture like you just described 
made. Another script has to come up that's got that kind of a character in it, and somebody who loves 
it enough has got to get it made. MARTHA COOLIDGE: Also, basically, the movie industry is very conservative. 
It's very hard to get people to change the way they do things. It's hard to get animators to go and work. 
I mean, you're a young animator. But it's hard to get the other animators to go work on machines that 
they've never worked on when they're used to pen and ink. And just not to depress you --and it will happen, 
it is happening. But it's going to be years before they're editing features on computers and that seems 
ridiculous. In television now, a large portion of television production is edited on computer editing 
systems, but there are almost no editors who work on these systems. So those very few editors who work 
on those systems get all those jobs. And a lot of other editors just refuse to even learn, and to try 
and convince a feature production company that it would save them time or money to edit on a computerized 
editing system, they just look at you as if you're totally mad. Which is crazy. But the second point 
that they'll bring up to you is, well, we can't get a good feature editor who knows how to work on these 
pieces of equipment. And that's true. My generation grew up working on chems and the flat bed tables, 
and I'm still working with editors who are working on Moviolas. So it's a very slow to change industry 
in terms of using new technology. And it is happening, and there's always the new generation moving in, 
there's no question about that. But to have the -- most of the older generation is not seeking out the 
new technology. WAHRMAN: Howard, when I started working on this panel, Richard and I very cynically proposed 
it and we were very surprised when we got accepted and we're very happy to be here. I expected to find 
much more negative information than we got. And, in fact, I think that we can provide a really interesting 
character --15 or 20 minutes of a character for a reasonable price in a film -- there are people out 
there who are willing to do it. What concerns me is that I got that feed-back a number of times, and 
they're all sort of interested. What concerns me is that it might not be the right property, it might 
be some really bad movie. And I wonder if that would help or hurt. MARTHA COOLIDGE: That would hurt. 
: That would hurt. : If the picture doesn't make money, it doesn't matter how good it is. MARTHA COOLIDGE: 
No, it doesn't matter. Because the thing is that what it is, is that you're talking about justification 
of jobs, and the executive who works with the company that made the movie said, it was that computer 
generated character that killed this picture. It wasn't the fact that the star was addicted to some chemical 
substance. And so, yeah, something new will always be blamed. The fastest way to change something in 
the motion picture industry is to make it cheaper than you can currently do it. That's the fastest way. 
And I know that time is on the computers' sides as far as cost effectiveness goes. And I think that it 
will be 20 cheaper, and when it is cheaper, I think that that is going to be one of the major critical 
mass points. : And Saturday morning cartoons. MARTHA COOLIDGE: What about Saturday morning cartoons? 
I don't know much about them, but they're certainly produced very low budget. And I'm sure that if someone 
came in with a better idea to do that at the same budget or less, they'd be thrilled. : But wait til 
you see the budgets that they currently get. : We have four more minutes, so what I'd like to do is -- 
Q: I'm Stan Kalinosky of Tektronix. And it seems to me that there is a parallel between office automation 
and the potential of automating film production. You pointed out a number of aspects of that. And I'm 
really concerned with -- it seems like films are produced and budgeted on a project basis, but you want 
to capitalize your equipment for film production automation. You want to capitalize that. Do you think 
there's going to be a change of policy in production companies to sort of finance that equipment? : No. 
: No. MARTHA COOLIDGE: No. : No, absolutely not. No. : I don't think so. Sorry, you missed that sale. 
STAN KALINOSKY: So just to follow up on that, I guess the trend would be for -- : We'll take your card, 
anyway. STAN KALINOSKY: So I guess it seems like the solution then is you'd have to have an outside firm 
do it all for you. But then you'd lose the capability of the producers and directors of having the equipment 
on their desk and being used to using it. : Well, there are things called rentals and leases. : There 
are also things called RenderMan. Where you'll see desk top systems having a lot of the same capabilities 
for handling certain kinds of graphical output and input, so that if we can all talk the same language 
in computer graphics to some degree, then the director can sit down with his PC and do a move and say, 
I want this rendered in 48 bits with radiosity and all kinds of great little spline moves and stuff. 
WARHMAN : We're out of time. There will be a break-out room, room 303. Thank you very much for coming. 
(APPLAUSE) Richard Hollander P e t e r t.-t y a m '~ I.. t o(.tucer  Director i, :{ a l t::) h Winter 
t. oducer ~ . ,,~. ......,..:~ral ~l(.)urll ,}" ,,. .~..F I(,ture;::., .t~ ~, Richard Hollander 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402250</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>28</pages>
		<display_no>8</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[X window system]]></title>
		<page_from>1</page_from>
		<page_to>28</page_to>
		<doi_number>10.1145/1402242.1402250</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402250</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101785</person_id>
				<author_profile_id><![CDATA[81100247175]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Champine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Equipment Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101786</person_id>
				<author_profile_id><![CDATA[81100077964]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scheifler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101787</person_id>
				<author_profile_id><![CDATA[81100069208]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gettys]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Equipment Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101788</person_id>
				<author_profile_id><![CDATA[81100499795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Georges]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grinstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Lowell]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101789</person_id>
				<author_profile_id><![CDATA[81100423056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bertram]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herzog]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 X WINDOW SYSTEM Chair: George Champine, Digital Equipment Corporation Panelists: Robert Scheifler, Massachusetts 
Institute of Technology James Gettys, Digital Equipment Corporation Georges Gnnstein, University of Lowell 
Bertram Herzog, The University of Michigan X-WINDOW SYSTEM GEORGE CHAMPINE: Hello. Welcome to the panel 
on the X Window System. My name is George Champine. I am going to be the moderator with you this afternoon. 
I would like to introduce the panel members that we have with us today. These are people who are playing 
a key role in the development of the X Window System. I will introduce them in the order in which they 
will be speaking. First we have Jim Gettys. Jim is a Consulting Engineer at Digital Equipment Corporation 
and he is going to be speaking on the rationale of why the X design is as it is. Second, we have Robert 
Scheifler. Bob is with MIT and is the Director of the X Consortium at MIT and he will speak on the X 
Consortium. Bert Herzog is the Director of the Center for Information Technology Integration at the University 
of Michigan. Belrt will be talking about 3-D extensions to X. Georges Grinstein is from the University 
of Lowell Georges is the Director of the Graphics Research Lab there. Georges will be talking about X 
and the standards community. The format that we will be following this afternoon is that each speaker 
will speak for ten or twelve minutes on their particular topic and will provide a position statement 
on that topic. The first slide I'll be showing you deals with the history of the X Window System. It 
has been said that there are no new ideas in graphics and certainly the idea of a windowing system has 
been around quite some time. The origin of window systems goes all the way back to split screens in an 
ASCII terminal environment. The next major development was on the Alto system at Xerox PARC in the early 
1970's. They had several of the aspects of what we would now consider a modern windowing system at that 
time. However, the activities specifically related to what we now have in the form of X Window System 
started in early 1980's. The system that this all came out of was the V system and then subsequently 
the V Graphics System at Stanford, worked on by Paul Asente and Brian Reid in 1981. The VGTS System then 
lead into the W Window System, W of course standing for Window, also at Stanford. Paul Asente then took 
the W Window System to Sun during 1982 where he further improved it. The W system was brought into the 
Digital Western Research Lab in Palo Alto in 1983 time frame by Paul Asente and Chris Kent, and they 
made further improvements on it at that time. This became the basis for the X Window System. At this 
time Bob Scheifler and Jim Gettys got interested in the W window system. They needed something for their 
respective projects at MIT, and by 1984 were in the process of developing the X Window System for purposes 
of Project Athena, and the Laboratory for Computer Science at MIT. Two years later, in 1986, X Version 
11 was becoming available and of course that is the industrial strength system that people are now using. 
I am associated with Project Athena at MIT. Last fall we put X Version 11 into full production on September 
8 --with 750 work stations and six thousand users. We found out that it works very well and is very responsive. 
The X Window System has become very popular very rapidly.for the reason that X is much more than a simple 
window system. In fact, it is quite inappropriate to think of X as only a window system. It incorporates 
the client/server model and it is used in a network environment. There is a standard protocol which allows 
the decoupling of application programs from the screen management and display of those programs in a 
transparent fashion. This is a major advantage in providing interoperability among various manufacturers 
who previously were not able to interoperate at all. Another very important reason why X has been so 
popular so quickly is that it was designed in a public forum. Everyone who wanted to have input and was 
capable of providing input in a sustained fashion was allowed to participate in the design. Also, X was 
designed from the outset to be a portable system so that it could port very easily to other platforms. 
The people who designed X were foresighted enough to realize that they could not think of every possible 
extension at the beginning. Therefore, they included a standard extension mechanism which is an integral 
part of the X design, and in fact half the opcodes are left unused for extensions. Finally, the strategy 
of putting in mechanism and not policy has made it possible to support a variety of different human interfaces 
on a single platform. In designing the X system there were a number of issues that the designers had 
to deal with. First, there is the question of the proper coordinate system for the display output. The 
options here are world coordinates versus device coordinates. Then there is the issue of server versus 
client communication. The two options here are kernal based windowing systems, which are the older style, 
versus network communication as the newer style which provide decoupling between client and server, and 
provide the interoperability feature mentioned earlier. Another issue is the management of the layout 
of the system --tiled windows versus overlapping windows. There is a question of where is clipping done. 
Is it done by the application or is it done by the screen manager? Where is damage control done? Is it 
done by the screen manager or is it done by the appfication?..How tight is the coupling between the cfient 
and the server? Are they synchronously coupled or is it asynchronous? What sort of structure is provided 
for the windows among themselves? Is it a hierarchical structure or is it a flat structure? And finally, 
what is the nature of the protection which is provided by the window system? Is it based on hard wired 
firewalls between the various applications or is it one based on trust? The X Window System has gone 
far beyond what the original designers ever had in mind. A specific example of that.is the full motion 
video in an X window developed by Project Athena at M1T I am going to show you about a short video tape 
here which provides a number of illustrations of this kind of technology where people have gone far beyond 
the original intent of X. The tape shows a workstation with full motion video in an X window. It is truly 
an X window. It can be managed just like any other X window. The tape was generated by Matt Hodges, also 
at Project Athena. Matt is developing an advanced authoring environment for interactive video which he 
calls the MUSE System. It is an object-oriented authoring environment and it provides synchronization 
among various media streams, as shown by the examples here. In addition to the full motion video, the 
tape shows a scroll bar which is synchronized to the video. In fact, the MUSE authoring environment makes 
it very easy to synchronize various things together --graphics, image, sound and other objects. Also 
included is a small video control panel on the display surface that allows you to stop, start, back up 
and otherwise control the video. You will see a set of mouse sensitive icons, where each icon includes 
a single frame image from the video stream. Each of these objects on the screen is a true X window. X 
is very good at supporting large numbers of windows. We have many applications which can show two or 
three hundred windows on the screen at one time and the performance is truly excellent At this point 
I will run the video. The application is from a course that is being used at MIT this fall on the Athena 
Multi-Media Workstation Platform for purposes of teaching neuroanatomy. The course is being developed 
by Dr. Steve Wertheim, and it will show some segments from that particular course. In the live video 
screen, you see a graphic of the brain, which is rotating. In the lower left hand corner there is a video 
control panel, where the operator has commanded the video to change from forward to stop to back to forward 
again. In this display, the scroll bar is synchronized with the video segment, showing how far it has 
proceeded. In the upper left hand corner, there is a text screen. In the lower left hand corner, there 
is a dialogue box. The dialogue box has prompted the user for input. The user has provided a keyword, 
which is now highlighted, and a search is being run on that keyword in order to pursue a hypertext path 
through the system. This gives you a very brief idea of some of the things that Athena and other people 
are doing. At this time I would like to turn the floor over to Jim Gettys, who will speak about the design 
of X and how the various decisions were made within it. Thank you. JAMES GETTYS: During the first six 
months of the initial X development, Bob Scheifler and I came to consensus about how we'd go about designing 
X. There were quite a few arguments early on, but this is a summary of the general principles we came 
up with. I believe that they are relatively applicable to most areas of system design, and have relatively 
little to do with X, but the X is an interesting example One of the things we learned is that living 
with the results of your mistakes is generally a very bad idea. Typically, it inhibits doing things properly. 
In general, we tried not to do things until we believe we understood what we were doing, and in fact 
in general we tried to avoid doing things until we had enough examples of the problem to do things. An 
example of this is in the Version 11 early design -- some of you may have seen --where there were specifications 
for altemate input devices. When we tried to implement it, we discovered how badly broken our design 
was and we removed that from the Version 11 spec. Of course,there was always too much to do and too little 
time to do it. For Version 11 we decided to restrict ourselves to basic simple graphics, which is to 
say only pixel-oriented things. We believed that this was the minimum that we could get away with. We 
did not believe that it was the right time to attempt to solve 3-D It would take an additional year or 
two to solve that problem, so we decided that was one of the things we were not going to do..We are very 
familiar with things like the PostScript imaging model. We believe that those are wonderful things, although 
we did not understand them very well at the time. Other issues, for example programability in the server, 
which is only really one form of extensibility, we decided to defer. This is not to say we would not 
be completely uninterested in such topics, just that we were not going to do it at that point in time. 
There is the interesting question of what other issues we should have addressed and did not, and there 
we tried to leave the door wide open for further work. In general, the only thing worse than generalizing 
from one example is generalizing from no examples at all. So we were very reluctant to attempt to solve 
things for which the people who were kind enough to cooperate in the design effort did not have good 
experience. Very often, when a single problem occurs for the first time, it may be a symptom of a more 
general problem. We attempted to have more than one instance of a problem before trying to analyze what 
the solution ought to be. Again, in systems design, one has to make certain compromises and what those 
should be are always interesting judgment calls. The tool kit was planned from the beginning. The Andrew 
work already going on. There was work under Version 10 for what is now called XTK. We really designed 
presuming that there would be a tool kit, and we tried to keep it as simple as we could. The classic 
example is that without a tool kit, a "hello world" program is a lot of code and with it there can be 
a dozen lines of code or less In general, on large systems, isolating complexity becomes necessary. The 
number of interactions between components of a large system gets to be huge, so we attempted to separate 
complex things from each other, to limit and fully define the kinds of interactions between them. For 
example, window management --moving, resizing existing windows on the screen --is not wired into the 
base window system. It is provided by an external program that we call a window manager. In general, 
we also tried very hard to keep the base window system simple with just basic fundamentals in the base 
window system and to do as much on the clients as possible. Many people like their own favorite forms 
of window management and that is kept external to the base window system.Bob Scheifler and I (my particular 
involvement was Project Athena) were quite interested in having something that's easy for people to use. 
Bob is in the Laboratory for Computer Science and there people do serious research on such issues. From 
the beginning we basically had to try to provide basic mechanisms rather than the policy. We did not 
believe that a single human interface would serve in an environment as diverse as MIT. There are very 
large amounts of mechanisms in the base window system. We do not expect any single application to use 
it all. And so we leave the look and feel of the human interface to the tool kit window manager. The 
next speaker is going to be Bob Scheifler, who is the Director of the X Consortium. ROBERT SCHEIFLER: 
The X Consortium was formed in January of 1988. The basic goal of the Consortium is to promote cooperation 
within the industry and within academia in developing standard interfaces throughout the X environment. 
MIT is acting as the neutral broker between all the parties concerned, and also acting as the chief architect 
of the design. To date we've been quite successful. Cooperation within the industry has been quite good. 
Politics are basically ignored. We really focus on technical issues. A related goal of the Consortium 
is a commitment to stability. X has moved out of the research environment into the product environment, 
and companies are interested in selling their product , whether it be hardware or software. In the case 
of hardware vendors, they are looking for software vendors to develop applications. ISVs like to see 
a relatively long period of stability before they'll commit product to a platform. So a goal of the Consortium 
is to extend the environment and keep a stable base. The base that we have in place can't change in any 
significant fashion. The Consortium is open to any organization that wants to join --hardware vendors, 
software vendors, end users -- we really encourage organizations to do their own cost benefit analysis 
to figure out whether it's right for them to be involved in the process, in defining interfaces. There 
are other benefits as well. One of our sideline activities is producing software and some view that as 
the more advantageous part than joining in the design. Here is the current list of affiliates. These 
are the full members, big companies and little companies. Most people are represented there, so this 
is not sort of a niche activity. It's really industry-wide. It's a very wide-spread commitment to making 
this go forward. If you were down on the show floor and looked around at all the people with X servers, 
that gives you one idea of where X is going.. This is a diagram of the current X environment. At the 
bottom is protocol, for which there is a standard definition. One level up is the Xlib interface for 
programming in C. That's also a standard in the Consortium. Above that, things branch out. The XT intrinsics 
are also now a standard of the Consortium, and the final versions of that will be out soon. Above that, 
you can view widgets, which are the object oriented things that you actually want to use to build user 
interfaces. Above that UIMS, for which there is currently no active development within the Consortium, 
although various companies that are certainly working in that area. To the left, which seems to be not 
quite visible, are conventions. There is active development going on there and then extensions, which 
sort of permeate the whole thing. A major direction that we are working on is building test suites, both 
for the protocol and for Xlib. There are currently a couple of man-decades that have gone into this development 
effort. It is a combination of work by a dozen or more companies. No single company can really do it. 
That effort will eventually become public when it is finished. The PEX effort will be discussed by Bert 
Herzog. There is work going on in double buffenng and multi-buffering. There is work going on in extension 
for additional input devices. Keyboard and pointer are not sufficient, so there is active work going 
on there in defining what will be a standard. Preliminary work is going on in image processing. The image 
processing community seems a tittle split, and as the questions go on maybe we can get some reactions. 
Many people believe that a standard file format and some client-side processing are adequate, and that 
hardware support and protocol support don't seem to be necessary. There is work going on in fonts, particularly 
right now in font naming, to establish standard conventions for how fonts are named. As George pointed 
out, there is a lot of work going in video and that effort is definitely heating up. There are a number 
of companies interested in that. Conventions -- these are data exchange between clients; cut and paste 
if you will. There are also conventions between clients and window managers so clients can live in the 
environment of lots of different window managers. Tool kit development -- the intrinsics were a major 
focus. There is now preliminary work towards developing a core widget set in terms of functional specification. 
There is also work starting to figure out what the next generation application developmentenvironment 
would be. We are acting as technical liaisons to various standards bodies, including ANSI, which Georges 
Grinstein will talk about. The X Open group in Europe, ANSI, and the National Bureau of Standards for 
the government are all interested in X. BERTRAM HERZOG: My assignment was to discuss what the activities 
of the X3D BOF have been during the last year or so. The X3D BOF(Birds Of A Feather) is truly that. It's 
a group of people --in some cases representing companies, in other cases representing universities and 
yet, in the other cases, simply representing themselves--who gather together to look at the possibility 
of a graphical extension, with respect to the X Window system. The X3D BOF had a very ambitious notion 
of looking at all of the 3-D possibilities.One dominant theme came out of that in the direction of PHIGS. 
The name selected for this PHIGS-related aspect of 3-D was X3D-PEX. There are other proposals. These 
proposals, however, have not gathered the same cumulative interest so far and it remains to see what 
happens to them. It is the PEX portion of X3D that in fact took on a strongest emphasis. And the emphasis 
was to look at the sense of extension. It was accepted that the X Window System was what it is, what 
it did not support was 3-D and the question was how can we provide for that as an extension. Into that 
was woven the support for PHIGS and for what some people called the stable portions of PHIGS+. PHIGS, 
is the Programmer's Hierarchal Interactive Graphics System, and is moving well along in the ANSI/ISO 
standards activity. PHIGS+ is not quite in that same position, but is certainly receiving lots of attention 
and the intent was to include especially the features of shading, lighting, and advanced output primitives. 
This became the consensus of what was to be part of X3D PEX. Most of the activity in this area came from 
folks involved with Digital Equipment Corporation. They presented this information jointly with colleagues 
at Sun at a meeting held at MIT in May of '87. Over fifty people appeared at that time and there was 
much discussion over the days. Consensus developed that in fact we should form a public architecture 
team and appoint a chairman for the activity. It was not formed by X or MIT -- it was formed by consensus 
that people were interested in this activity. The important part that came out of this is the formation 
of what was called the A Team, an architecture team. It was a group of people who volunteered their time 
and their talent to form the next step of taking this initially proposed activity, and solidifying it 
through the process of active meetings.definition of the document. The members of the A team developed 
a document version 3.1 in May of this year. The work was completed and now an implementation team was 
formed. The goal was to cause an implementation to happen. For various and sundry reasons, those ambitions 
weren't realized so other steps were taken. One of them was to decide to take the opportunity of SIGGRAPH 
and demonstrate the PEX running on both Sun and HP workstations. In the future, the X Consortium will 
take responsibility for doing a public implementation of PEX. The funding is being sought from members 
of that consortium and the intention is to use a subcontractor to obtain that. With that, I'll close 
my remarks and turn the podium over to Georges Grinstein. ROBERT SCHEIFLER: In terms of the PEX implementation 
effort, we're very close to beginning a public implementation, and I've been talking to potential sponsors 
and to potential contractors. You'll see an announcement from us in the fairly near future. In terms 
of PEX documents, if you need hard copy and you can't get things over the net, you can always send mail 
to me as well. 7 GEORGE GRINSTEIN: There has been great demand for formal standardization, formal ANSI 
and ISO standardization of much of the X activities. This really is necessary for vendors that sell to 
government installation and government sites. Most European companies must support standards. ANSI has 
started efforts in the standardization of X. We are working on standardizing the protocol and there are 
some activities on the other layers that Bob Scheifler put forth I would like to talk here more about 
the process of standardization in a group so that you at least have an idea of where things are and what 
the two groups are. ANSI stands for the American National Standards Institute. It does not make standards. 
There are certain groups that are accredited for making standards. X3 is one of those groups in the United 
States. X3 develops standards that are then voted forth by its members after public review. Within X3 
is X3H3, which standardizes computer graphics. In the International Standard Organization (ISO) is JTC 
1 --Joint Technical Committee Number 1 which standardizes information systems activities. SC 24 is the 
group that standardizes computer graphics. ANSI is also a member of ISO. Within ISO, each country has 
one vote. X3H3 presents for X3 a U.S. position on certain standards that ANSI then puts forth in ISO 
and votes the U.S. position. Within X3H3 there are currently eight groups that are doing different activities. 
X3H3.1 has finished its activity on PHIGS and is now currently working on PHIGS+. X3H3.2 is working on 
reference models within computer graphics. It works on the device interfaces, Computer Graphics Metafiles 
(CGM) and the Computer Graphics Interface (CGI). CGM is now an ANSI and ISO standard. X3H3.4 works on 
the language bindings to the various functional standards. X3H3.5 is currently working on GKS review 
and the GKS information bulletin. X3H3.6 is a committee that I have that is standardizing window management 
systems. X3H3.7 works on validation, testing and registration of graphical items, and X3H3 8 is a new 
committee that is now working on standardizing an imaging Application Programmer's Interface.(API) Within 
SC 24, the subcommittee that standardizes computer graphics in ISO, there are five working groups. Working 
group 1 standardizes architecture. It works on architecture which includes reference models, user requirements 
and requirements in general of computer graphic systems. It is one of the groups that is involved in 
setting forth new study groups for future standards. There are two study groups this coming year that 
relate to windowing systems; one studying the impact of windowing systems on computer graphics standards, 
and one windowing environments. Working group 2 works on application interfaces such as PHIGS+. Working 
group 3 device interface and metafiles. Working group 4, language bindings --and working group 5, validation 
testing and registration. You can see the correspondence between 8 ANSI and ISO; the same correspondence 
occurs between individual standards making bodies in other nations. The standards process has been known 
to be very lengthy. That's because there are many, many different milestones which involve lots of public 
review, responses to comments, review again, respond to comments, etc. until a consensus is reached. 
Consensus does not imply unanimitity, but it implies more than just a majority. Within ANSI the first 
step is a standing document called SD 3. There is currently an SD 3 for the data stream. There also are 
now activities for an SD 3 for Xlib. After the documents takes form and becomes rather stable, it becomes 
a working draft. It is then forwarded to X3H3, for letter ballot, review and comments. This may repeat 
itself and cycle through several times after which it becomes a draft proposed American National standard. 
At the proper time it is forwarded to several groups which review the process to make sure that it is 
a valid process, that it has undergone full public review, and that there is been adequate time between 
the letter ballots and responses .Finally, it becomes an American national standard. ISO has a very similar 
process. A new work item equivalent to an SD 3 is proposed. After it becomes stable, a working draft 
is proposed and developed through several public reviews andre-evaluations. It then becomes a draft proposed 
standard, then a draft international standard and finally, lastly, an international standard. At each 
of those stages, each member nation has one vote. The U.S. again is directed and guided by X3H3 in its 
position. There are several current standards and evolving standards that X3H3 and ISO are working on. 
GKS, the Graphical Kernal System is one of the first standards that was developed. CGM, the Computer 
Graphics Metafile, PHIGS and PHIGS+, CGI and the X Window Systems are also being developed as standards. 
Language bindings are particularly important for functional interfaces and there are activities and standards, 
in ADA, C, FORTRAN, LISP and PASCAL. I've tried to put forth a simple slide on the advantages and disadvantages 
of standards. This is a very hot topic. It is probably the most controversial of them all, whether something 
should be standardized or not. There are people who spend full time standardizing, there are people who 
spend full time arguing why there should not be standards. In general, there is a benefit and a very 
obvious one to a standard and that is stability, which is a broad base that you can build upon. If you're 
an educator you have one interface, one standard that you can be concerned about and train people on. 
Maintenance is rather simpler. End users benefit in that they have a guaranteed support of one package 
that they can turn to. The disadvantages basically come down to three major issues. The first one is 
that standards tend to be rigid and therefore don't allow one to develop more extensions and other ideas. 
X obviously provides some of those capabilities and is not rigid. In general, standards provide one platform 
that you can build upon, so I think that argument is not a very strong one anymore. Second, standards 
tend to be too general. They are not specific enough for the particular applications you want. That's 
generally true because they have been arrived at through consensus, but they can be manipulated in such 
a way to provide specificity, and as such I think that argument falls also. That they are not timely 
is probably the most direct and perhaps the most true of all the complaints about standards. This is 
a problem of the process. The process for reaching consensus takes time. It's an important process that 
really one does not want to speed up. It is very important that both ANSI and ISO standards be reviewed 
very openly and effectively, and as such this takes time. Thank you. GEORGE CHAMPINE: Okay, that concludes 
the formal remarks part of the session We will now take questions from the floor. STEINHART: My name's 
Jon Steinhart,I'm an independent consultant and a professional trouble maker, so here it is: That was 
a very nice status report, but this is listed as a panel session and in a panel session I usually expect 
speakers to present different views and have a little bit of discussion between them. Since I haven't 
heard any yet, I would like each of the speakers to comment on the one thing they think is wrong with 
X at this time. GE'VrYS: It turns out that most of the mechanism to do non-rectangular windows is in 
the sample server and somebody should go off and implement it and give it back to Bob here. SCHEIFLER: 
The major thing that's wrong with it is that there is still a lot of design work that's still in progress. 
I think once you see things like Display PostScript and PEX and video and a few other things put into 
it, and those things are definitely coming, we'll all be much happier with it. HERZOG: I guess I can 
agree with that in a disagreeable way. The lessons learned from the people who've implemented a fraction 
of PEX just a few days ago need to be consolidated and fed back. GETTYS: My general experience has been 
that it takes a number of tries before you have something that you're happy with in the long run, and 
the people doing the 3-D work are still sort of in their first iteration. I would just urge people not 
to be too eager when people first implement something. X version 11 represents the third major redesign 
of the base system that we're working from. The tool kit is somewhere around the second or third iteration. 
GRINSTEIN : The addition of another drawing package that has to be standardized is a problem RYAN: : 
My name is Kevin Ryan. I'm with Carnegie Mellon University in Biological Sciences and I do image processing. 
You mentioned image processing earlier as what would be standardized in it. The level that you mentioned 
was file file format and some other rather minor thing. What is the other end of that spectrum? SCHEIFLER: 
The other end of that spectrum is figunng out how you virtualize or genedfy various kinds of hardware 
interfaces for the kinds of hardware support for image processing that exist today and will exist in 
the future. It includes how you relate image processing, for example, to video, how you tie all this 
together through a network protocol. It's really figuring out the non-client side or whether any of that 
client side stuff belongs down in the hardware, for example. ROMANOS: My name is Romanos from the University 
of Western Ontario. My question relates to a class of applications that need to do their own rendering 
of graphic objects. Right now it seems the only way is to ship bit maps across the server all the time. 
How much thought was given to these kinds of applications and is there anything in the air for something 
like data compression to make it easier for these kinds of applications to run under X. SCHEIFLER: Certainly 
one component of image processing, the need for various kinds of data compression when you're sending 
images to the server. That kind of capability doesn't exist in the core protocol, partly because we assumed 
that there would be an image processing extension. We planned for it to be handled there in a more apt 
environment, so people are thinking about various kinds of standard and non-standard data compression 
techniques for sending images to the server. MEADS: I'm Jon Meads with Bell Northern Research. I have 
a concern about use of X in the practical real world environment when the applications using it have 
to rely upon trust to prevent themselves from blowing out of the water and so forth. It's been my experience 
that real world requires more of the fire wall protection aspects of things. Most time sharing systems 
tend to require that applications don't blow other applications out of the water and so forth. And I'm 
kind of wondering whether X will survive in the real world under those conditions. What is your philosophy 
and intentions in regard to building fire walls into X for the future? SCHEIFLER: Certainly there are 
some companies who are thinking about security issues, and when they sell to the government and others, 
those become the major concern. There is design work going on on how to include a per resource protection 
mechanism within the server. It doesn't necessarily require any particular protocol modifications to 
do that. If you view the authorization data that sent at connection set up you can interpret that in 
a lot of ways. One of them is to grant you access to particular classes of resources or instances of 
resources. CHAMPINE: One other approach to that, Jon, is to work in a very restricted environment where 
every application has to use the screen manager and then insist that the screen manager build fire walls 
around the applications. So that would mean that you'd have your own screen manager then. ARNOLD:.David 
Arnold I'm convener of ISO SC-24, WG3 at the University of East Anglia in England.I would like to pose 
the moral question to the panelists and in fact, the X activity generally. Going around the exhibition 
at the moment, it is obvious that a very large number of manufacturers are saying they have a standard, 
when in fact we've heard the panel already talking about variations and the climates for change within 
the current document. I'm very concerned this happens with ISO work but when it happens with work which 
is outside ISO, I find it's rather a dubious piece of advertising, shall we say. I noticed for example, 
in the Hewlett-Packard stand, that out of four displays which were showing "commitment to standards" 
only one of them were showing something which was an implementation of a standard. SCHEIFLER: Certainly, 
if you looked around the floor, you saw a lot of people doing additional things that the core protocol 
doesn't support. A lot of them were 3-D things and the reason for that is because we don't have a standard 
there yet. I think if you ask the vendors involved: are they not committed to a standard effort like 
PEX -- the answer is, they are. It's just a question of timeliness in getting their own products out 
the door. ARNOLD: My comment is that they are already marketing it as a standard. SCHEIFLER : A lot of 
people view it as a de facto standard. ARNOLD : But there is not one definition of it. SCHEIFLER : There 
is one definition of it. ARNOLD : There is not one interpretation of that definition. That's the problem. 
SCHEIFLER : I'm not sure how you define "interpretation." CHAMPINE: As a validition suite. SCHEIFLER 
: If you mean a validation suite, there certainly is a test suite underway. The extent to which it becomes 
a conformance suite is a matter is a matter for standards bodies to look at once the standards bodies 
catch up with us. ARNOLD: So I have assurance that there will be opportunity for changes to be introduced 
in X, as a result for the proper review of it within the ISO community? SCHEIFLER: Certainly, we are 
working with the standards committees. We're not trying to be antagonistic to that effort. GRINSTEIN 
: The answer is, yes, David. REISER: My name is John Reiser, Ardent Computer. The answer may be "yes" 
but the answer may be a year and a half in advance of when the standards committee wants to accept it. 
By the time the standards' committee gets around to doing its work, I would expect that X would go through 
another major revision. SCHEIFLER: I hope not. GETTYS: Who's going to do it? We aren't. JOHN REISER: 
Nevertheless, history bears it out that it's quite likely. 12 GETTYS: History has shown that one of 
the longest-lived things is a network protocol. It's almost impossible to ever get rid of one once you've 
defined it. I would be very surprised if X suddenly changed again. It's so incredibly hard to cause everybody 
to agree to a change. REISER: On the other hand, one can always look at DecNet which has gone through 
5 major revisions and one can hardly claim that a DecNet Version 5 system is compatible with a DecNet 
Version 4 system. GETrYS: Well, seeing that Version 5 is not out the door in any case and is ISO based 
in any case -- I don't quite see the point. REISER: The point is that in fact, the various revisions 
have been de facto incompatible because the feature set is much different. GETTYS: Well, we believe that 
this time around, we can do that without having to cause previous versions to stop working. Maybe history 
will show us wrong, but that's our hope. McCORBETI': James McCorbett, Nixdorf Computer. I'm just curious 
if any consideration is being given to RenderMan and how that might fit in. SCHEIFLER: The topic hasn't 
come up in the consortium yet. RenderMan is fairly new to me, so I don't have any specific comments. 
HOLDEN: My name is Ross Holden. I'm wondering if Bob would like to comment on what releases are coming 
up in the planned future and what would the basic content would be and any effect on the stable base 
we've been talking abut. SCHEIFLER: Release 3 is October-ish, this year-ish. There will be a number of 
new back ends for a couple of new servers on a couple of new hardware platforms that some of you will 
be happy about. There will be backing store and save-unders in most of our servers. There will be the 
standard version of the XT intrinsics. We hope there will be a couple of sets of widgets from a couple 
of different parties. There will be all the acronymic window managers that have been floating around 
on the net. There'll be lots of bug fixes. In terms of future releases from us, probably every 6 to 8 
month-ish. In case there are any incompatible changes, Xlib has had a few functions added to it. No incompatible 
changes. The protocol specification has had two semantic changes which you can either view as being clarifications 
of the spec or you can view them as incompatible changes. There are no encoding changes. The incompatible 
changes are in the definition of what a wide arc is and how a linewidth-one cap styles work. STEINHART: 
A question for Georges Grinstein. You gave an interesting overview of the standards process, but you 
didn't say anything about what the status of X in the standards process was. I was wondering if you could 
present that status and in your opinion, go over what the current major issues are. GEORGES GRINSTEIN: 
There is an ST3 now for the X datastream encoding protocol and there is an ST3 that's been circulated 
within X3H3.6 to discuss a standardization of 13 Xlib.This year we will put out a document that will 
be for public review with an X3H3 and we'll probably take one or two cycles. Next year it will be forwarded 
to X3. SCHEIFLER: On the previous question, Bert just pointed out to me that I didn't say anything about 
the PEX demo. The sources to that will become public and will probably be on Release 3. GARY BARNE: Harris 
Computer Systems. I also have a question for Georges. noticed you made remarks concerning an architecture 
committee within the ISO organization that's looking at a reference model. It seems to me that we're 
doing things a little backwards in that we're creating all these standards and we really haven't defined 
how they are all going to tie together. Can you give us a little bit of inside on what's going on in 
that committee? GEORGES GRINSTEIN: There is a reference model that they are working on that has been 
present for quite awhile. The major problem is lack of participation to further that effort and incorporate 
some of the new activities that are going on in the graphics industry. BARNE: Where do you see that activity 
going within the next year or so? GEORGES GRINSTEIN: There are several study groups meeting within the 
next 12 months to further define the reference model and produce output documents. If you are interested, 
I would suggest strongly that you contact me and I'll tell you who in ANSI is doing that activity.ANSI 
itself is producing input documents to the ISO activities. GRAHAM: Jim Graham from Sun Microsystems. 
I am standing here wearing an X11/NeWS T-shirt, so I was interested when I read in the description in 
the program of this panel, you were going to discuss the relationship to NeWS and MS-Windows. You did 
not. Could you please discuss the relationship as you see it. .... CHAMPINE: Our plan was to put people 
that are competent to talk about NeWS in the audience. And I'm very pleased to see we've accomplished 
that GRAHAM: So you really don't have a viewpoint on the relationship. SCHEIFLER: Well, the relationship 
to MS-DOS. I'm not sure what relationship you can state, except that X was explicitly designed to be 
language indepedent and operating system independent. There are certainly instances of X running in the 
DOS environment as well as the UNIX environment, the VMS environment, a couple of LISP environments and 
a couple of other places. As to NEWS, Sun seems to be in the best position to say something about that. 
CHAMPINE: Would you care to make a statement? GRAHAM: In terms of porting to other platforms, there have 
been many ports of NeWS to other platforms, including non-UNIX platforms which will be coming out soon 
and I can't mention specific instances right now. CHAMPINE: One could say that X and NeWS had common 
origins. In fact, in many cases, similar people or the same people have worked on both and there are 
many, many similarities between X and NEWS. SCHEIFLER: You will note that no one up in the panel here 
is wearing a T-shirt bashing somebody elese's window system. SHAUFLER: My name is Robin Shaufler from 
Sun Microsystems. You asked for a statement from somebody from Sun about the relationship between X11 
and NeWS and the way I see it, I tend to regard it as a language issue. If you look around at major computer 
vendors and the operating systems and compilers that they offer, you have your choice of languages. You 
can program in C. You can program in Pascal. You can program in Lisp, Ada, whatever suits your application. 
I think window systems need to move in the same direction. And in fact, a future direction that I would 
like to see them move is to have a greater range of choice between say, a Lisp window system, a Smalltalk 
window system, as well as X-11 and NeWS and perhaps other future window systems Languages for dealing 
with window system concepts, incorporated into a window system are needed that allow all these different 
protocols to share the screen. GETTYS: From my own personal point of view, having spent several years 
building large systems in FORTH, PostScript is not the language I would have chosen as an implementation 
langauge. I very much like the PostScript imaging model, but that is not my view of how one builds applications. 
SCHAUFLER : That's part of my point. GETFYS: From my point of view, Leo Hourvitz in the Screen Postscript 
panel yesterday said it best--his reservations are basically my reservations I believe that very much 
more of toolkit kinds of issues are related to applications than the screen and personally question how 
much that buys you. KEITH BOYER: Ohio State University. Normally, when people will talk about NeWS versus 
X, they talk not so much about the fact that it is PostScript, but that the server is programmable. How 
do you feel about that lack fight now in X and what do you see happening? SCHEIFLER: I think it is a 
lack., it is a lack more in terms of input control than it is in terms of output control, although I 
think once PostScript gets added and it will get added to X, a large segment of the community's complaints 
will go away. Certainly my view of the extension mechanism, is that one of the extensions probably ought 
to be something that makes the server programmable. That is just one kind of extension that you can make 
to a window system. The question is really: what language do you choose? And I didn't want to be in a 
position to try and choose that. BOYER: Is there any serious, current work going on? 15 SCHEIFLER: The 
only serious work is in terms of PostScript. I do not know of any other. There is research going on in 
a couple of different universities. There is one working towards being able to download C++ code into 
an X server. That's one instance that I know about. ROMANOFF: Romanoff from the University of Western 
Ontario. Obviously, there are a whole bunch of styles of writing X applications, some that are good, 
some that are bad. Is it your position that any of the applications shipped with the release, are in 
fact, ideal-- this is in fact the way you want things done? To refine that question a little bit. Do 
you plan to release at least one application and say that this is the way you would like applications 
coded? ROBERT SCHIEFLER: Our intent at this point is not to try ... from MIT staffs point of view to 
tell you what the user interface ought to be. Within the consortium, we are working bottom up toward 
that goal. There is a lot of contention when you get to the higher layers of the atmosphere as to whether 
it's a good thing or a bad thing. We have this core tape and this contrib tape and you should really 
view most of the clients as contrib. We put them in core because it's boring to bring up an X server 
and not be able to mn anything else. Our intent from the MIT staff point of view, is not to produce production 
quality applications. That's what industry is out there to do. DAVID ROSENTHAL: I'm David Rosenthal from 
Sun Microsystems and I would like to encourage people to investigate the problem of how to write good 
programs for X and publish the results. I got a 14-page paper out of trying to write the Hello World 
program. It's a relatively easy area to mine to get publications. And the fact that I could do that, 
indicates that even if you really ... if you really examine even the simplest possible X applications, 
you come up with a lot of interesting design trade-offs, things to consider and a lot more of that material 
is needed. The manuals are reference manuals -- they don't provide this information and a lot of the 
reason why they don't provide this information is simply that nobody has looked any of the individual 
aspects of writing an X program in sufficient detail to be able to write something authoritative about 
it. And what'll happen if you try this is that you choose some simple application, you write it the way 
you like it, and you get some of your friends to write it the way they like it. Don't tell them anything 
about it beforehand and compare the results and you'll find it's pretty easy to get an idea of the right 
directions to go once you've tried comparing several different implementions of the same program. I would 
encourage people to try it. GETTYS: Again, a plug for Dave's paper. He also shows why you use a toolkit. 
Typically, any good toolkit should do most of the work for you. In fact, the line count that I was referring 
to in my talk, was basically taken from the examples in Dave's paper. JOHN RONER: John Roner, National 
Semiconductor. I hope this is probably the simplest question you'll have to answer, but for those of 
us who are just starting to move from our hardware over to software and to investigate X Windows -- how 
do we go about obtaining the data and the software and the rest of the whole package that we need in 
order to evaluate this? SCHEIFLER: It depends on what hardware you have. If you've got a Berkeley UNIX 
based popular workstation, you're welcome to get it from us. The MIT software distribution center, distributes 
all of the source and documentation for a fairly nominal charge. There is no licensing. If you have fairly 
exotic hardware, some of which was on the floor, you can talk to those vendors about the availability 
dates Or if you're willing to dig in and do it yourself, again, you can talk to us. We are basically 
the public distribution point at this point and if you want something that is vendor-hardened, then you 
can talk to the appropriate vendor. MIKE RUSSEL: I'm Mike Russel from Pixar. Bob mentioned a couple of 
times the problem of controlling mage processing via network server. I wonder if you would have any pointers 
to literature where this topic has been discussed or maybe just a general summary of what the problems 
are. ROBERT SCHEIFLER: I guess I won't claim to be an image processing expert, so I'm not sm'e if I have 
anything particular to say there. The imaging work within the X community is fairly new and so far, it 
hasn't really gelled partly because there seems to be so many conflicting opinions about what it is or 
what you want. GETTYS: There are certainly a number of X implementations, some of which were on the floor, 
that also are implemented using shared memory for transport. I believe the Ardent and Stellar implementations 
certainly do that kind of thing. It's certainly for imaging, the kind that helps implementations a whole 
lot. CHAMPINE: I think there are some papers by the people at the University of New Mexico, Albuquerque, 
on a package called X-Vision which you might be able to look at. RICHARD PYER: Richard Pyer with Gould. 
There was talk live video support in X Windows and I was wondering what, if any, work has been done in 
the area of defining a model for handling live video within the X server. CHAMPINE: There is in fact, 
an X video birds-of-a-feather group which is very, very active. They have meetings every couple of months 
or so. I would suggest that you get involved with those people because they are making a great deal of 
progress in supporting video in an X window. RICHARD PYER: Is there a contact person? TODD BRUNHOFF : 
I'm Todd Brunhoff from Tektronix and I'm trying to get this X video going. You can send mail to XVideo@Expo.mit.edu. 
Thank you. And if you'd send mail there, there has been virtually no activity. I've proposed the basic 
framework for how the extension mechanism should work with the X protocol. I haven't got any feedback 
yet, but I've been gone for a month, too. GEORGE CHAMPINE: The X video group does meet every couple of 
months or so, so it is quite active. STEVE BURSER: My name is Steve Burser, I'm with Planning Research 
Corporation. We're a contractor company and we have to ... we'll be transporting a lot of stuff that 
we've written over on to X Windows. Now if we're going to be going from different machines, we're going 
to want to use the same toolkits. Now, you have a toolkit that comes with it, with the 17 distribution, 
but it may not have all the capabilities that you might get from like from a vendor or something. Is 
there any way of, like standardizing. Like you're going to have your buttons and sliders so there are 
certain protocols that us applications writers, and othersystem writers can use to kind of standardize 
ourselves across the board so that if I'm working on a DEC, I can turn around and put it on a Sun or 
an IBM without having to change the interfaces -- so they all look the same. SCHEIFLER: There are a number 
of answers to that. None of them entirely satisfactory. One is that you can do, with the XT intrinsics, 
you can build your own widget set. The intrinsics will port to most platforms and most vendors will be 
supporting the intrinsics on their platform. They may support other things as well. So one would be to 
build your own widget set. Another would be to use a widget set that you expect will be portable or licenseable 
to a number of other platforms. There are certainly a lot of users that have your point of view, and 
a lot of us would like to see a core widget set in terms of a functional specification which each vendor 
might tailor to their own look and feel and add additional functional capabilities. We're trying to move 
slowly in that direction. The problem is that look and feel are not as orthogonal to functional interfaces, 
as you might think. There are often direct impact on the functional interface depending on what kind 
of look and feel that you want on the user interface. It is a fairly hard problem. I don't expect to 
see any overnight miracles there. We defintely understand the problem and are trying to move in that 
direction. GEORGE CHAMPINE: We have time for just one more question before we go to the breakout room 
and we'll take a question over here. CARY ASHBY: I'm Cary Ashby from Evans and Sutherland. I would like 
to hear a little bit more about planned input models, particularly in regards to dials. Are there any 
plans to make dials andother frequently-used graphical input devices part of the core input devices for 
X -- or will they forever remain extension devices? SCHEIFLER: Well, there is nothing to say that in 
the future an extension can't become a standard and perhaps become a required extension. If you look 
at the encoding parts of it, the difference between an extension and core protocol is fairly minimal. 
In the future, you might expect that all servers have to support particular extensions. Certainly, in 
terms of input, we are actively looking at multidimensional devices and dials and all kinds of things. 
GETFYS: It turned out to be a little bit harder than we thought. We made an initial attempt to do the 
design in Version 11 for handling that. And when we got to implementing it, we realized we had done it 
wrong. Rather than trying to live with what was clearly broken, we removed the support from the initial 
core and left it to an extension which people are working on. SCHEIFLER: Even though something at the 
protocol level looks like an extension, the application interface in the programming language can certainly 
paper over that and make it look like any other interfaces just like in the core protocol. GEORGE CHAMPINE: 
Okay, fine. I want to thank everyone for your attention and we will now adjourn to the breakout room. 
(APPLAUSE) Touchstones Don't add mechanism until peal appll~ndon ne~,ds if. lkfin&#38;#169; what problems 
you are solving. And not aolvb18 .... Wait for more lhto exttmp|e of a problem. U,(l~rstand prt)brems 
before solving them. 90% / 10% ~'oJe, l~o]~Ite complexlty, M~'.hanlsm ove, r 1~3]ty, James Gettys 
   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402251</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>30</pages>
		<display_no>9</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Media technology]]></title>
		<page_from>1</page_from>
		<page_to>30</page_to>
		<doi_number>10.1145/1402242.1402251</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402251</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101790</person_id>
				<author_profile_id><![CDATA[81100315933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lippman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101791</person_id>
				<author_profile_id><![CDATA[81336491376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marvin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101792</person_id>
				<author_profile_id><![CDATA[81100216523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zeltzer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101793</person_id>
				<author_profile_id><![CDATA[81100458252]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Walter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bender]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MEDIA TECHNOLOGY Chair: Andrew Lippman, Massachusetts Institute of Technology Panelists: Marvin Minsky, 
Massachusetts Institute of Technology David Zeltzer, Massachusetts Institute of Technology Walter Bender, 
Massachusetts Institute of Technology MEDIA TECHNOLOGY Panel presentation at SIGGRAPH'88 Andrew Lippman 
David Zeltzer Waiter Bender Marvin Minsky MIT Media Laboratory August, 1988 ANDREW LIPPMAN: Good afternoon, 
and welcome to the panel on Media Technology which, I gather, like most of the panels in this room, is 
designed for refugees from radiosity and ray tracing -- words that many people have promised not to use 
on this panel. This afternoon, we have four panelists who will present their views of media technology 
as practiced at the Media Laboratory at MIT. When we began work to form the lab and adopted the name, 
our approach was unique in that we were the only lab that did basic research in communications and information 
technology in a context of creative application. While there were technical labs addressing components 
of media technology and groups that explored applications, we were the only place where the users and 
inventors were in one place and often in one mind. It is fair to say that to the extent that we were 
successful in engendering in our graduates knowledge of both the uses and the technical innards of media, 
they were to proportionately unemployable: No one outside the academic environment understood them. This 
is no longer the case. There are pockets of research and development of themes similar to what we do 
at places as diverse as Apple Computer and Dow Jones; and as geographically distributed as NYU and Stanford. 
There is growing awareness of the notion that computers can participate in communications in novel and 
significant ways through non-linear and content processing, and there is a growing recognition that creation 
of new media requires understanding of the underlying technology as well as human information processing. 
Unfortunately, these pockets are dispersed throughout academia and industry. Rarely are the efforts coupled 
with degree programs and a coherent, complete attack on the problem. The fact that some computer graphics 
programs are hidden in architecture and art departments attests to this, as does the fact that the television 
industry remains distinct from the computing industry. Outside of MIT, there are scattered media experiments 
but little coherence, and glaring omissions. For this reason, the panel on Media Technology is as much 
a review of the work of the MIT lab as a description of the field. I have deliberately selected four 
disparate views of our work for discussion. While we all hold many fundamental principles in common, 
our approach is not at all alike and we rarely all speak with quite the same voice. There is a thread 
tying us together, and that will become clear through the discussion: it involves computing and content 
processing as well as joint and simultaneous understanding of the electrical, cognitive and creative 
processes of learning and information processing, but in practice there are seemingly divergent experiments 
in progress. hope that this panel, through our presentations and your questions will thoroughly explore 
the area. Periodically, media technology intersects and diverges with SIGGRAPH. I can recall roughly 
12 years ago, in 1976, in Philadelphia, where there was a panel on interactivity. At that panel, we addressed 
the definition of interactive systems and the notion of graphical interaction. One of the analogies of 
the time that was used to illustrate the issue was made by Fernando Corbato, who's also a professor at 
MIT and, in fact, invented time sharing. In the course of the discussion, I used Corbato's description 
of interactivity to distinguish between alternation and interaction. Corbato hypothesized a computer 
system that appeared as a turnstile. One entered and presented a deck of punch cards for processing and 
received the result of the "run" as one exited the turnstile on the other side. In those turnstile turned 
rather slowly. But, as time went on, the turnstile could go faster and faster. And you could imagine 
submitting your deck, and one millisecond later, getting the answer. The question we posed in 1976, which 
I think we've successfully answered by today, is whether, ultimately, as the turnstile becomes extremely 
fast, does the system be interactive? And of course, the answer is no. Because you have to fully state 
the question before you get an answer, there is no sense of cooperation with the computer. The response 
is quick, but the entire problem has to be stated before any processing takes place. There is no sense 
of a mutual attack on the problem and no evolution or development of the theory along the way. While 
it may be an abstract historical reference to think about those kinds of things because you always work 
cooperatively with computers, especially in the kinds of applications that most of you are engaged in, 
but it wasn't always so. And the definition of that kind of cooperation and the environment needed to 
implement it, and exactly what the interface would be was, at one time, and remains a matter of study 
and a matter of investigation and exploration. In the intervening years, SIGGRAPH and interactivity diverged. 
Output and database processing grew in prominence as research domains and the mechanisms that generated 
that data base or used the images became distinct areas of research. Now, in 1988, we have come full 
circle, and the uses for graphics systems and the methods by which graphic data bases are designed are 
now as important as their presentation. Hence this panel. Let me tell you a little bit more with some 
pictures. Figure One is a recent chart that outlines the groups that form the Media Laboratory. Perhaps 
the only thing that's misleading about this particular diagram is that it gives you the impression that 
each one of those little bubbles or activities exists independently of the others. That's not the case, 
but the chart would get particularly undescriptive if we tried to demonstrate more exactly how overlaps 
between the several groups function. While there are a great many programs, there are as many interactions 
between them. Perhaps, through analysis of this diagram, you can gain some feel for what we believe Media 
Technology is and how we approach research. In the area of imaging, for example, there are programs that 
address basic image processing (The Advanced Television Research Program), interaction with still and 
moving images (Electronic Publishing and Movies of the Future), cognitive aspects of vision (Vision Research), 
and high level representations of movies (Film/Video.) The people involved include engineers and filmmakers. 
Similarly, work in sound spans cognitive domains of music understanding, through composers working in 
electronic media, to conversational computer systems. One program in place addresses building a computer 
system with which one can play a duet: it listens while it accompanies, and adapts to the interpretation 
you make of the music. As a case in point, in the early 1980's, we developed tele-conferencing systems 
that attempted to transmit the "presence" of the person with which you were conversing instead of the 
image. The idea was that when you became engaged in a distant conversation with that person, you would 
have the overwhelming impression that they were, in fact, there. To implement this, we built models of 
potential conversant's heads in the laboratory and cast masks in the shape of those heads that would 
serve as video projection screens. These masks, or screens, were then mounted in gimbals that allowed 
the replica head to move in synchrony with the person it represented. Onto this mask we then projected 
the image of the remote conferee. There's no ray tracing or radiosity or even B-splines involved in that 
particular experiment, and it can require no video processing at all. However, the effect of the work 
was to take the heads that were typical of teleconferencing systems of the time out of the box that the 
television monitor encased them in and, by correct placement of the "talking heads" around the conference 
table, allow eye contact. When one participant turned to talk to another, their images did likewise; 
the others saw this much as they would were all the people in the same room. The purpose of this illustration 
is to demonstrate a different approach to teleconferencing. Rather than use it as an excuse to investigate 
bit rate reduction of video signals, we used it to explore the essence of remote communications. Both 
the quality and the utility of the picture were subjects of research. In fact, in one demonstration, 
we processed no video at all, but simply animated the image projected on the head by retrieving various 
images of the person speaking controlled by speech analysis alone. I use this example to indicate that 
the teleconferencing system combined speech processing, image processing and computer graphics. In the 
course of the work, we learned about all three topic domains and crossed previously implicit boundaries 
set up between them. The graphics was a sync-sound movie, and the image was three-dimensional. In another 
experiment, we combined position sensing and speech recognition so that each could complement the other 
and provide an interface where the sum was greater than each of the parts: The notion was that a computer 
could not understand a conversation unless it had access to the verbal and physical context. Neither 
alone would be sufficient. The program, called "Put That There," demonstrated that the system could understand 
pronouns by their physical referent. Where you pointed when you said "that" determined what you meant. 
The theme in that work is simultaneity of the interface, using all of the modalities available and, again, 
solving the problem in as many different ways as one can. The moral of the story is: never use one interface 
when two will do. A current program in the lab is the "Movies of the Future Program." The theme of this 
work is the digital representation of moving picture information, and the ultimate goal is a model of 
moving images that can be manipulated with the facility with which we control existing synthetic, completely 
described data bases. This is investigated in the context of two broad application areas: new distribution 
media for traditional movies and the integration of moving images into computer interfaces. The first 
area is of direct interest to some of the supporters of the program -- motion picture distributors such 
as Paramount, Warner Brothers and Columbia - and the second aspect is relevant to personal computers 
where the tasks you require of them take time to accomplish. In this latter case, we anticipate that 
motion can represent the temporal aspect of the computation and create a dynamic visual interface. Part 
of this work involves creation of a "range camera" that senses the distance of the points in the image 
as well as their intensity. By suitable processing of the range images, we can alter the focal point 
and lighting of the image after it is recorded, and ultimately, we can alter the perspective from which 
the scene is rendered. One thesis student is developing a three- dimensional data base from the range 
images by correlating the motion, the image data and the range information in the scene. Each alone is 
ambiguous, but like speech and position in "Put That There", they complement each other and help create 
a more complete model. The range camera derives distance by a technique called "depth from focus." Two 
apertures are used in the lens that provide a short and long focal length image the difference between 
which generate the depth image. This has been both an introduction to the panel and one view of Media 
Technology. The overriding point has been the diversity of the technical approach: conferencing systems 
that combine graphics, speech processing, and image compression; command systems that combine physical 
environment sensing with speech recognition; and moving picture processing systems that require image 
processing, computer graphics and three-dimensional picture analysis. The other three panefists will 
each present their individual views and we will then discuss the issues during a question period. DAVID 
ZELTZER: May I have the first slide, please? I'm going to tell you about my slice of the Media Laboratory, 
which has to do with interacting with animated microworlds. What we're after --and the term is due to 
Marvin Minsky --is something we've been calling an "expressive animation workstation." The idea is that 
we'd like to take the power of computer graphics hardware and software and somehow make it available 
to professionals, educators, consumers and entertainers as a medium for visualizing ideas, for simulation 
and design, for learning, and so on. We take it for granted that this will be a real time high-quality 
graphics medium, and I think it's safe to say that the hardware is rapidly taking us there. We want the 
user to be able to interact with these animated worlds on their own terms. That is, we don't want to 
require people to learn Fortran or Lisp or "C" or some special-purpose animation language. We'd like 
users to be able to interact with the machine on their terms and not on the machine's terms. To do that, 
we need to incorporate a lot of knowledge about what kinds of simulations people are interested in, how 
living creatures interact, and how physical objects interact. We're interested in developing autonomous 
agents, so that if someone wants to use an animation with human figures in it, that person doesn't have 
to write a lot of code to do the transformation hierarchies and do the rendering and so on. By the same 
token, we don't want that person to have to take three or four years to train a simulated human being 
how to walk around, open doors, stand up and sit down. We want figures that know how to do some kind 
of problem solving on their own. To set this in context, let me talk briefly about how I view computer 
animation, and how people have been interacting with animation systems. I think there are basically three 
ways of specifying the behavior of animated figures. I call these modes of interaction guiding, animator 
level and task level animation. We'll look quickly at each of them in turn. Guiding means that the user 
explicitly specifies what every object in the animation is doing from moment to moment. This is terrific 
if you want to have control over all the details of the animation. But, as the simulated worlds that 
we're interested in become more and more complex, that just becomes too tedious and costly to do. This, 
though, is the principal means people currently use for controlling animation. That is, if you want to 
control a jointed figure, you attach a knob or a joy-stick to each of the joints in turn, and twiddle 
the knobs until the right things happen. The next level I call the animator level , and here we provide 
the user with a programming language or special-purpose animation language for defining and controlling 
the objects and events to be animated. This provides the user with the full power of aprogramming language, 
and gives the user the ability to program whatever abstractions and special-purpose mechanisms are required. 
The problem is that we've now made doing animation as hard as programming. We know the problem there. 
It also means that the user has to be either a programmer or has to hire or coerce or cajole a good friend 
who is a programmer to work with them. And that establishes a separation between the user of the system 
and the medium which we'd like to make available to them. Finally, there's a mode that's called task 
level animation. This is a term that's shared with the robotics world, where the problem is to control 
robotic agents by specifying a set of events and some constraints on the behavior of the agent, and letting 
that agent fill in the details as necessary. Let me distinguish here between animation, as many people 
think of it, -- and the kind of animated simulations I'm talking about. First of all, I think of output 
to film and video tape as a rather occasional use, in our case, perhaps, for demonstrating our work for 
sponsors and at conferences. For the most part, we're interested in real-time interaction, right there 
on the desk top with the user. Secondly, lately there's been a lot of interest in developing control 
paradigms for computer animation. This morning, we heard a paper by Witkin and Kass on space~time constraints. 
This way of thinking about animation requires that we know, globally, what the simulated agents are going 
to be up to. We need to know the start conditions and the end conditions and some key constraints in 
the middle. I want to distinguish between that and what we might call forward simulation. Suppose you've 
just designed a new six-legged robot and you want to see if it can, indeed, traverse the terrain it was 
intended for. You would like to place it in the simulated environment and find out if it works. If it 
breaks, fine. You've got to know that. So we're not interested in constraining simulated agents to do 
what we want them to do. Rather, we want to let them do what they want to do. How are we going to get 
there? Well, in my group, we're following three broad lines of research. The kinematics and dynamics 
of motion --in particular we're looking at legged locomotion. We're looking at modeling motor behavior. 
And, lastly, we're interested in natural and easy interaction with these 3-D virtual worlds. Let me quickly 
outline the work that we're doing, without going into great detail on any of it. Here's a single frame 
from a piece showing objects falling and tumbling in a gravity field. I think perhaps the first thing 
that you want to do is include in your simulation all those Newtonian mechanical attributes that we expect 
in a physical world. My earlier work involved modeling human walking using forward kinematics. Currently, 
we're looking at building locomotion models based on biological motor control. We'll show you a video 
tape of that shortly. We're also interested in 3-D interaction. This work has a long historyat the Media 
Lab, and earlier, at the Architecture Machine Group. This is work that was done by my colleagues, Patrick 
Purcell, Delle Maxwell, Carol Ginsberg, and others. Here you see some animation that makes use of speech 
recognition in addition to tracking the position and orientationof the human user. The main idea here 
is something called scripting-by-enactment. That is, ff you want to get an animated figure to do something, 
then you act out the movement and the animated figure will mimic that motion. Later on, this work continued 
in the development of the body suit. Here you see Mark Locasio modeling the latest version --one size 
fits all. I've forgotten how many degrees of freedom it tracks, but there's a ring of LED's around each 
joint in the figure and, not seen in this view, is a pair of cameras which, by comparing the two views, 
can reconstruct 3-D position and orientation information for the figure. Here's the body-tracking glove. 
The glove itself has as many degrees of freedom as the rest of the torso. This work is continuing now 
with a new technology, using fiber-optics. This is the Dataglove made by VPL Research in California. 
It uses fiber-optics that have been treated so that when the finger is bent, the light transmitted through 
the fiber is attenuated. By measuring that attenuation, we can then calculate the joint angle of the 
finger underneath it. So we are measuring data from ten joints -- two knuckles on each finger. In addition, 
there's a Polhemus electromagnetic sensor on the wrist, so we can also get position and orientation information. 
But we're not satisfied with just reaching out and appearing to touch objects -- we want to feel what's 
going on. Here's Alex Ferdman demonstrating a 2-D force feed-back joy-stick, which is due to Max Bahensky, 
Margaret Minsky, and others. I think you get the idea. Not only can you move the joy-stick around, but 
it pushes back. You can simulate a variety of forces reacting to the user's movement as appropriate. 
In conjunction with the mechanical engineering department, we're working on extending this to three dimensions. 
What you see here is a one-dimensional prototype of a force feed-back joy-stick that's actually a hybrid. 
If you want to get high-frequency response, that is, if you want to simulate hitting a brick wall, you 
have to have rather large motors to bring the joystick and the user's hand to a sudden stop. That means 
that the device will no longer fit on your desk- top, and -- what is worse -- if there's a bug in the 
control program, you might break somebody's hand. Naturally we want to avoid that, so, in this device, 
there's a particle brake on one side, and a rather small motor on the other side. The use of the brake, 
then, allows us to do sudden stops without using very large motors. The last thing I want to talk about 
is modeling behavior. What we want to do is understand how biological creatures organize their own routine, 
instinctive behaviors. In graphics, people generally draw on the disciplines of optics and physics to 
understand theinteraction of light with materials. For behavior modeling, we're drawing on the work of 
the ethologists -- in particular Conrad Loren's and Niko Tinbergen, who spent a long time thinking about 
how behavior is organized, although not quite in terms that lend themselves directly to implementing 
algorithms. On the slide you see Tinbergen's postulated view of how behavior is organized hierarchically 
from "higher level drives" on the top -- which might be things like, "I'm hungry," or "I'm thirsty." 
or "I need to go to sleep." At the bottom of the diagram you can see the lowest level of the motor output 
system. The point is that we need to think about how behaviors are organized, how we can build hierarchical 
suites of motor skills, and interconnect them in such a way, that the simulated agents can do certain 
kinds of problem-solving on their own. I don't do all this work by myself and, in fact, it couldn't be 
done without the help of my students. Why don't we roll the video tape. The first tape you'll see is 
a whimsical attempt to show you a bunch of these techniques -- primarily the work of Mike McKenna but 
with a lot of help from a lot of others. So let's roll the tape. Lastly, I want to point out that the 
production of this video tape and one that you saw in the film show yesterday by Bob Sabiston, required 
a close collaboration between the Computer Graphics and Animation Group, the Visible Language Workshop 
and the Electronic Music Studio to put these productions together. WALTER BENDER: There's been a resurgence 
of interest in television over the last few years. It's the first timesince the '50's that the good engineers 
are interested in television. One of the aspects of the lab that Andy mentioned briefly in his introduction 
was our advanced television research program. There's a group of people doing fine work on the quality 
axis of television. How to make the picture better and how to get that picture into your home. There's 
another group called the Audience Research Facility that's trying to understand how consumers and the 
public at large respond to things like high-definition television. If you ask the average person on the 
street -- even around MIT --what's wrong with television, they don't say artifacts, they say, programming. 
Our Audience Research Facility is off-site from campus, though, because you can't really test that at 
a facility in a technical institution. In any case, the Electronic Publishing Group, where I work, is 
more concerned with another axis of television evolution, that of programming alternatives rather than 
solely picture quality: what new degrees of freedom can we invent for television in the next thirty years 
rather than how many new lines of resolution can we bring to the image. You can see that Andy and I work 
closely on some projects. The theme is semantic bandwidth compression, with an emphasis on the word, 
semantic. We take seriously the notion that intelligence (or processing) in the channel can substitute 
for bandwidth, but we regard intelligent channels not simply as those that reduce noise in a picture, 
but instead as those that process content versus signal. As an example, in the early 1980's the group 
created a teleconferencing system where no video was transmitted through the telephone lines at all, 
yet a picture was reconstructed at the receiving end. Some of this work was reported by Lippman at the 
1981 Picture Coding Symposium and ironically, it has been resurrected recently in the form of "Model-based 
Coding." In the system we did in 1981, a gimbal-mounted image of the remote conferee done as a lenticular 
photograph that changed as your viewing angle changed was simply wiggled when the remote speaker spoke. 
These change-image-lenticular photos use the same technology as post cards you once bought where when 
you looked at them from one angle, you saw a person in one position, and as you shook the card, they 
moved, or changed clothes. In our work the gimbal provided the motion, and it was activated by the voice 
signal over the telephone line. We once thought of this as the 17-cent teleconferencing system. We think 
about media coming in three flavors in the Electronic Publishing group. There's traditional ink on paper, 
which is directly accessible. You don't need anything between you and it except, perhaps, eyeglasses. 
It's right there for you. Back, at the beginning of the century, people started experimenting with a 
new kind of media -- radio -- where you needed a box, a signal processor between you and the information. 
There's only really been one new knob added since the '50's, and that's the mute button. But you could 
imagine, now, a computational media --a medium where there's computing in the loop, so you can start 
making decisions based on the content of that signal, not just its electrical characteristics. And what 
the implications of that kind of media is really what we've been exploring for the last ten years. The 
first example I'm going to show you dates back to SIGGRAPH in Seattle in 1980. This is a system called 
the Aspen Movie Map. As Andy likes to say, there are only five people in America who have not seen this 
system. It was even on the Ripley Believe it or Not television show at one point. We used a mechanized 
camera to take one picture every ten feet along every street in the town of Aspen, Colorado, and one 
picture every ten feet around every corner in the town. We envisioned distribution of a data base of 
the town by storing those images on an optical videodisc and regarding the publication as that of a data 
base rather than as a predetermined travelogue or cinematic tour. The town was accessible via images 
taken in the fall and the winter, and you could envision a "season knob" on the television set augmenting 
the traditional channel knob. You could use a control that changed the content within the context of 
the program. That was something that happened locally. The packaging of the material was happening in 
conjunction with you, not necessarily dependent on what was happening in the head end, in the distribution. 
We took it a little bit farther a couple of years later with a project called the Movie Manual. And there, 
it was data as well, assembled on the fly. And there we had a primitive model of the user stored locally, 
so that how that data was put together, what you got, was somewhat determined by who you were and what 
your past interactions with the system might have been. We took that concept a little bit further in 
the next project I'm going to discuss, which is an electronic newspaper. A project called NewsPeek. In 
this project, we developed a more sophisticated model of the user and of the data base. And the idea 
was to take the normal model of how news is distributed and transform it so that each reader received 
a newspaper that reflected his or her own interests and personality rather than a newspaper identical 
to that read by someone else. We tried to model in the machine the functionality of a newspaper editor 
and the reporters. But now, the editor is working for you; he has a readership of one --an individual. 
We tried to present the information in a way that was familiar to the reader. We had a front page to 
our newspaper. It's ironic, every newspaper in the world has a front page. And it is not by coincidence. 
Somebody, at one point, invented the front page. It was a good idea. And it worked. And people used it. 
And yet, there's not, to my knowledge, a single on-line information system that has a front page. There 
are a few that have headline services, and there are a few that have the top of the hierarchy. But none 
of them have a front page. None of them give you that parallel access -- multiple stories on view at 
once. None of them go to the trouble to try to understand what's important, and what's the most important, 
and sort through the information for you. One of the other features of the Newspeek system was that when 
you opened up the newspaper, when you expressed interest in something, the system tried to respond to 
that. So that the example we like to use is page 14. You open up to page 14 for the continuation of an 
article; not only do you get the rest of the article, but you get related articles. You get articles 
that the computer thinks are related to that particular article. Those decisions being made as a joint 
decision between you, the computer, and the editor/publisher at the head end. We extended this notion 
to television. We developed a program called "Network Plus" where the captioning information associate 
with a normal broadcast cued the programs that composed the newspaper to annotate the broadcast with 
additional information that the viewer could query during the program -- a TV program with a "tell me 
more" button. Finally, we're also looking the other way: we're not just looking at the data, but we're 
looking at the user. We've got a system that was inspired -- I hate to say it -- but was inspired by 
the urinals in O'Hare Airport. The urinals in O'Hare Airport know when you're there. And they know when 
you're not there. And they operate accordingly. None of our computers seem to know that. We decided that 
it might be interesting to find out what you could do if the computer knew when you were there and, perhaps, 
where were you were relative to the screen. Whether you were paying attention or not. Whether you were 
looking in the direction of the screen. So we built a presence detector. It's got one bit which just 
says, he's there or she's not there. And then it also tells you an estimate as to how far away you are 
and whether you're looking to the left or center or right on. It's fairly simplistic image-processing, 
but it makes sort of a nifty tool for the interface. So if you're across the room, the computer can interrupt 
you with some voice. Or, as you get closer, it can put more and more dense information into the display. 
Finally, we have one other theme that we've been exploring off and on for a number of years. We still 
don't have the bugs worked out of this one. But this is the smile detector. I think that having a built-in 
smile detector and a built-in blink/stare detector is an essentially part of the interface, and as essential 
as having a keyboard there. And once we have it -- well the smile detector could be a little dangerous. 
But the blink/stare detector, I think, could be quite useful. Thank you very much. MARVIN MINSKY: Everyone 
at this conference is concerned with "media' - that is, with devices or materials through which the minds 
of different people can influence one another. Most of the time, when we write, paint, sculpt, or make 
movies, we're concerned with getting results. We want a certain kind of product, and we focus on the 
small details of getting it done. That's what we do most of the time. In this talk I want to encourage 
our community to think more about the nature of those activities. I'll argue that more thought about 
psychology could lead to better tools for practical arts. We're usually driven by short term goals. When 
we need to produce a movie, or an animation, we naturally tend to think in terms of the final result 
- for example, depicting a certain activity. The traditional way of doing that is to think in terms of 
breaking the scenario down into sequences of pictures or scenes - for example, by conceiving of a story-board 
represented by key frames spaced apart in time. Then we have our work cut out; we have our plan and all 
we need do is to find smooth ways to fill it in. But what are we really trying to do? We talk about communication 
in terms of passing thoughts and ideas from one person's mind to another's. But what do we mean when 
we talk of such things? What are ideas - those things inside minds? And what, for that matter, are the 
things we call minds? How far can we expect to get unless we try to force ourselves to form ideas about 
the things we're trying to do? What does it mean to communicate - for one person to tell an idea to another 
person? Whenever they start to think about that, our scientists and philosophers usually get stuck at 
defining terms like "symbol", "meaning", or "idea". A better way to get around this problem is to think 
instead about what sorts of structures and processes most exist inside a brain. Clearly, human communication 
must involve some family of "construction processes" in which one person manipulates media so as to cause 
new structures to form in another person's brain. How does that happen? It has to be indirect. Even if 
you had some direct way to reach inside and change another person's synapses, that wouldn't do you any 
good, because no one yet knows enough about how brains compute ideas. We're still in the dark ages of 
psychology. I suspect that this is largely because we are all immersed in a humanistic tradition that 
discourages us from trying to comprehend such matters - a world-view in which such subjects as emotion, 
consciousness, and the nature of Self are considered to be beyond the scope of scientific understanding. 
This tradition maintains that although we may be able to understand "reason" because it is rational, 
emotions are too inherently subjective for that, so we must accept them for what they appear to be, and 
not try to understand them. It is less than a century since that conviction started to weaken -for example, 
when workers like Sigmund Freud started to study the jokes and mistakes all of us make in everyday life. 
It is only a generation since Jean Piaget began to look closely at how children develop and Niko Tinbergen 
started to study the behavior of birds. Philosophers had long found themselves baffled by questions about 
the relation of minds to bodies and brains, and no clear image of what they could be even started to 
emerge until until Alan Turing and Warren McCulloch began to speculate about ways to build artificial 
intelligences. Now we can see the relation between mind and brain as simple in principle, although immensely 
complex in detail: minds are simply what brains do - and what they do is construct various kinds of structures 
that support various sorts of processes. And although we still know little about those structures or 
the processes that construct and exploit them, there is much to be gained from making theories about 
such matters - provided that we appreciate our uncertainty and remain ready to learn and change. If we 
each spend some hours or days each year imagining what we might do if we actually did have a good theory 
of how our brains work, that exercise itself will lead to better ideas about ideas. In this as in any 
other field, we can't expect to become proficient without practice; it is hard to become skillful at 
anything unless one first learns to tolerate being awkward. What I'm saying is that it is hard to make 
progress unless one begins with some sort of "top-down" theory. Consider, for example, the field of computer 
vision, which in a way is the inverse of computer animation. All sighted persons are able to move about 
through a home: how do we represent, and interpret all those floors and walls, things and materials, 
animals and people, and relationships between objects, symbols, gestures, and expressions? There are 
so many problems in dealing with this that most workers in the field of computer vision have adopted 
what we call the bottom-up approach. It would be far too difficult," they often maintain, "to directly 
attack the question of how people see," and argue that it is better to begin with lower level processes. 
In the biological realm, for example, one could start by studying the nerve cells of the retina, and 
then go on to understand the following clusters or bundles of cells. Or one can start disregarding the 
brain: by first treating a picture as made of separate pixels and then then aggregating these into features, 
edges, textures, and regions. It certainly sounds sensible first to thoroughly understand the lowest-level 
aspects of picture-things before speculating about the larger-scale procedures involved in discerning 
what pictures depict: boards and bricks, tables and chairs, and people looking at TV sets. But I've come 
to conclude that this approach is not so productive, after year after year of watching many of my colleagues 
trying to find better ways to discern the edges and textures of patches of light. Iask them when they 
expect their machines to be able to distinguish a dog from a cat, or a car from a truck. And every year 
the reply is the same: "Not quite yet, but as soon as we understand a little more about tow level vision, 
it will be easy to move up. The trouble with this is that there they'll always find more low level problems 
to work on. This is what would happen if all the painters became concerned with the chemistry of pigments 
and never returned to their studies of scenes. One can scarcely ever solve high level problems by proceeding 
only upwards from research on lover-level problems. I see the reluctance to compose high level theories 
as having retarded several parts of my own field of artificial intelligence. One must also conceive of 
some high-level plans - to guide that basic research. The "seeing machines of the future will not require 
not only "bottom-up" research; those insights will have to be fused with with separately conceived ideas 
about higher-level representations and processes. There are so many differently useful ways to process 
the input signals that it makes no sense to seek the "best way" to it. Vision involves so many different 
kinds of problems that our brains need whole societies of different methods. The advantage of this diversity 
is that none of those methods has to work well on every problem, so long as a few of them help for each 
problem. But once we look past that signal processing level, we see that we also need theories about 
how to manage the higher level structures and processes involved in seeing those cats and cars and human 
forms. To make machines that make movies from scripts, we'll need yet more theories about how our minds 
work. If you're working with media - that is, with means for changing what others think - you ought to 
have theories in several other domains - about thinking and reasoning, about memory, about emotions, 
and about the nature of communication itself. Suppose, for example, that a certain line in a story-board 
script requires one actor to to arouse a sense of affection (or hostility) in another actor. This demands 
some sort of emotional dialogue - but, today, today we can do little more than make sketches and interpolate. 
Why can't we instruct our drawing machines to think of how to depict such scenes? Some people might argue 
that it is impossible to mechanize such things. But I predict that in the end we'll find that it's actually 
easier to make machines to manipulate emotional gestures and expressions than to do the same for "intellectual" 
matters. I think we find that hard today only because our culture h, as burdened us with,!nadequate images 
of what we are; we have gotten stuck with an incorrect unified field theory - namely, that each of us 
has - or is - a single, unified, central Self. In other words, we have the illusion that inside each 
human person's head lurks another person inside that head who controls or decides what we shall do - 
save when, perhaps, we're confused or ill. What is so bad about that idea? Simply that so long as we 
maintain the concept of a central self we tend to resist attempts to analyze it - that is, to break it 
down into a more complex structure of components and relationships. And our humanists tell us that doing 
that makes one into a beastly "reductionist". We are told that being reductionistic" is somehow evil 
or sinful - a transgression to cost one the loss of one's soul. What can we do about such superstitions? 
My new book, The Society of Mind, (Simon &#38; Schuster, 1988) proposes a variety of more constructive 
theories about various aspects of mental activity. To be sure, only a few of those theories are probably 
right - but the point of the enterprise was less to propose final answers than to present a sort of fantasy, 
a work of psychology-fiction, to suggest an image of what it is like to have such a theory about the 
mind. For despite reductionism's bad repute, it is nevertheless our most powerful way to comprehend complex 
things: by breaking them down into into networks of parts and relationships. Seen from that viewpoint, 
indeed, the traditional image of the single self seems the opposite of ennobling; is that not degrading 
way to approach the brain, that most complex assembly of functioning parts in our universe of experience? 
It is no wonder that the humanist tradition was outraged by Sigmund Freud's reductionist vision of the 
mind: as a system containing several parts, none able to communicate directly with the rest. Yet, as 
I see it, it is just that fragmentation of machinery that accounts for most of the richness of mind, 
because it constrains the different parts of that machinery to employ the same sorts of signals and symbols 
to represent different sorts of wishes and goals. It must be this diversity that forces us to construct 
metaphors which combine new types of knowledge with old, and in that way to make us make connections 
between how we physically interact with worldly objects, how we socially interact with our communities, 
and how we ethically interact with traditions about what we ought and ought not become. In making the 
first mechanistic theories about such matters, Freud's reductionist analyses can be seen as the first 
constructions of artificial intelligence. Yet even that view was too unified. It took another generation 
for researchers like Niko Tinbergen and Konrad Lorenz to envision how what had always been seen as a 
single instinctive, emotional Self could emerge from near-separate specialists. Their careful studies 
of animal behavior led them to see each individual, less as a single (hence unexplainable) whole but 
as a package of smaller minds; one for finding food, another for building a nest, others for keeping 
wet or dry; for defense or attack, for flirtation or flight. In this view every animal can be seen as 
a sort of society composed of dozens or hundreds of smaller components, each kind evolving partly by 
itself, but mainly in the envkonment of the rest of them. Seen through their eyes, we can now envision 
a cat, or a human infant) as a virtual playing ground in which the different members or divisions vie 
with one another in competition for control. Then we can see why the infant might so suddenly switch 
from contentment to rage. At first there is not much cooperation among those components, which mainly 
tend to switch each other off. A young animal rarely seems angry and hungry and affectionate at the same 
time. But in later stages of development of animals with more complex brains, more kinds of compromise 
and cooperation can emerge, as the various divisions learn to deal with one another as well as with the 
outer world. What has this to do with Animated Computer Graphics? Simply, that if you want to make an 
animation machine to express affection (or hostility), that machine must embody a theory of how gestures 
affect a viewer's brain. To begin with, we could start with the Lorenz-Tinbergen idea that each animal 
is born with specific, genetically built-in kinds of signal-detectors that arouse particular parts of 
those specialized internal sub-minds. Evidently, certain kinds of abrupt, jerky gestures - or short, 
loud noises - tend to arouse defensive mechanisms, whereas slower, more graceful motions or sounds -those 
with smoother trajectories -tend to arouse mechanisms involved with comfort, attachment, and sympathy. 
Certainly, it will not suffice for all purposes to think of the mind as simply a package of separate 
instinctive machines. But you must start somewhere - and if you demand too much from your first theory, 
you'll never get very far. Of course, the human mind is not simply a collection of separate specialists; 
that is merely a first- order version of a larger theory in which minds grow from such origins. We can 
develop that concept of personal growth by drawing upon another modem current in psychology. In the same 
era in which most American psychologists had gotten stuck with behavioristic concepts about simple conditioning, 
Jean Piaget and his associates in Geneva were developing more powerful ideas on how children develop, 
stage by stage, into much more sophisticated systems. How do our initial mental "proto-specialists" learn 
to participate in less fragmented kinds of compromise and cooperation? In the final sections of The Society 
of Mind, I propose a more detailed fantasy about how the human nervous system might grow through stages 
eventually to learn to interpret gestures in social terms. Suppose our movie's storyboard requires John 
to give Mary some flowers. How could we make a machine "understand" enough of this to render it acceptably? 
By finding ways to represent that action - where the plural is intentional. We all know how to render 
such an action in the physical realm - for example, in terms of a spatial trajectory in which the flowers 
travel from John's hand to Mary's hand. But we need other representations for expressing its meaning 
in other realms. To express an affectionate mood, those spatial trajectories must use certain types of 
envelopes; hostility requires other forms; and yet other envelopes are appropriate for actions expressing 
respect or reverence - in which John might bow rather than approach. Once equipped with a diverse enough 
variety of representations, our rendering system can modulate each output in accord with several different 
requirements - to interpret the same action in several different experiential realms at once. Animators 
have long understood that social and emotional aspects of trajectories can become so dominant that no 
viewer is disturbed when objects behave in physically impossible ways. Not only in gesture but also in 
words, and in the forms of the objects themselves, the use of multiple representations can permit our 
machines to exploit the power of metaphor, using the selfsame object, action, or structure of plot to 
express different messages at the same time. It almost seems a paradox: that an oversimplified theory 
of the mind as mainly made of separate parts allows one to start building up worlds of endless complexity. 
But you are far more likely to get stuck if you commit yourself, right from the start, to what at first 
may seem to be more humanistic, less mechanical schemes -because models that have no separate parts provide 
no materials that you can manipulate, combine, or develop in different ways. Once you find the freedom 
that comes from using complex reductionist points of view, you can never retum to simplistic acceptance 
of person as thing. Like forbidden fruit, mechanistic analysis is food for ideas that our culture forbids 
- and this is perhaps why it is equally denounced by both humanist and religious instructors. You've 
got to get your movie out, but you don't want to spend your life drawing objects in each frame. We've 
seen wonderful progress in recent years from making models of actors' brains. We see this in such production 
movies as Tron and The last Starfighter and at SIGGRAPH in Karl Sims' walking animals and Craig Reynolds' 
flying birds. No longer do we mindlessly interpolate key frames; we not only describe our actors' bodies 
and articulate their joints, but we're starting to draft designs for their minds. And now that we've 
set out along that path, we'll add more stuff to those primitive brains, provide them with better goals 
and smarter memories, and make them more able to learn - perhaps to join the audience. Eventually, instead 
of having to draw and program yourself, you'll be able to produce a show as directors do, by showing 
and telling the the actors what you want. We can't do much of that yet, today, because neither Artificial 
Intelligence or Psychology is advanced enough. But every member of SIGGRAPH can help, simply by trying 
from time to time to aim more directly toward high-level goals. ANDREW LIPPMAN: We have a little bit 
of time left for questions. Q: Forrest Gudlare, Ford Aerospace, Houston. Wonderful things you all are 
doing. Are any of these things you're doing going to be released to,the public domain? And if so, how 
might one acquire them? MARVIN MINSKY: Everything my students do, practically, is public domain, but 
that's because they're almost useless for the next 17 years, so you can't patent it. ANDREW LIPPMAN: 
We have other reasons why some of the things the other students do are sometimes useless, or rendered 
so. They are mostly released in the form of theses and people rather than as packaged systems. Q: My 
name is Jeff Tanner of IntelliCorp. And I've noticed that there's a lot of good things coming out of 
MIT Media Lab. But how many artists are actually part of your program? And are artists actually directing 
what the new medias are going to be? A: Well, there isn't a simple answer to the question. Part of the 
answer is that many of our students at the laboratory are, in fact, artists who do art and also program 
and do research. We have in the laboratory students in my group (Computer Animation) who are exhibiting 
photographers and video artists, and so on. We have both kinds in the same person. Another part of the 
answer is that it varies with different groups. So, for example, the experimental music studio is quite 
used to inviting composers to come in and work with their systems. In my group, we're interested in programming 
dynamics, inverse kinematics, building rather large systems. So we're mostly interested in people who 
know how to program. Q: My name's Doug McKenna. This is directed to anybody and is prompted by a word 
that Waiter Bender used when he was referring to the smile detector. He said it was dangerous. I'm curious 
if there have been instances when you've experimented with new media, when you came up against some kind 
of ethical problems in using of the new media. WALTER BENDER: I'll start with an answer. I probably shouldn't 
have used that term, but I guess it got a question. We're trying to invent new degrees of freedom. We're 
trying to invent new ways of doing things and looking at things, and I don't think that the work we're 
doing implies that one would make systems in precisely that way. LIPPMAN: Just as printing implies plagiarism 
and image processing allows pictorial falsification, the technology in these cases carries no implicit 
ethics but the implementation does. While this is not always true of technology, it is true in our work 
and communications generally. A: I have a controversial answer to that. Which is, I think there ought 
to be a division of responsibility. There are all sorts of people who have feelings about ethics, and 
I feel that scientists aren't particularly good at that. In fact, I don't think anybody's particularly 
good at that. But that's another matter. So I like the idea that we invent new degrees of freedom and 
new devices and new ways to see where everybody is in the world all the time. And then there should be 
activist groups, graphics experts for social responsibility. And they should heckle us and try to get 
us to stop doing our research. I think if you try to get the people doing the research to think about 
the consequences of it, you will actually do net harm to the whole situation. Q: My name is Tony Ciccala. 
A lot of the technology that you show has been around for a long time. You've been working on these things 
for many years. And yet, we don't see a lot of the ideas in the marketplace. I'm wondering why things 
don't move more quickly into the marketplace, and I'm wondering whether it might have to do with acceptability 
or testing. Are people really happy to have this visual conferencing? Do you test these ideas in a usability-type 
arena? A: I think the answer is that it's just in the last two or three years that terrific graphics 
processors that can do all this stuff -- see, the stuff Andy and Walter were talking about took a lot 
of computers and a long time. And great graphics engines have reached the consumer market. They haven't 
quite reached it, have they. Well, I see the Atari's and Amega's are pretty capable, and machines that 
were a million dollars are not $30,000. So just wait a few minutes. A: There's another answer to that, 
also, in addition to the expense. Most of the things that we do, I guess as Walter said, are done in 
the nature of being a test, and they're not necessarily a compendium of features that you would want 
to embody in a particular product. They're supposed to exhibit a range and try to provoke the design 
of these kinds of products into other areas. And, to that extent, they actually do filter out into the 
design of products.  Q: When I was a kid, there was a practical joke which consisted of a fake fly imbedded 
in a plastic ice cube. I'm starting to feel that some of McCluhan's stuff is coming true, in that we 
are imbedded in this electronic matrix that not only checks who comes in the room and when but also, 
apparently, instructs our speakers to pick up their tempo. I think this is an excellent group to maybe 
expound a little on ff there's any cure for this problem. A: Well, you can pull the plug. Q: But we can't, 
because the plug is what allows us to communicate in an orderly fashion. A: In a lot of ways, one of 
the things that you've seen, or that we've tried to talk about, are systems where some agent of some 
kind -- in this case a computer -- tries to get to know you a little better. And it does that by looking 
at you or listening to you or receiving some inputs from you. And in that manner of getting to know you, 
tries to act a little bit on your behalf. Maybe not very expertly on your behalf yet, but certainly not 
subvertly. And, in fact, in most of the systems that we do, the fact that the machine is listening to 
you is well-known to you and you can use that as an input to it. In other words, you know when reading 
the electronic newspaper that by touching the screen and picking your way through articles, you're in 
effect telling it what articles you'd like to see amplified on more in future editions of the newspaper. 
So you can use that touch deliberately to kind of make those kinds of signals. I suppose there's a certain 
amount of strangeness in that kind of view because, in the end, what you're expecting the computer to 
evolve into is a brother or a wife or a sister or a very close friend. Now, those are the kinds of people 
to whom you occasional withhold things, but occasionally you're quite free with them. And they often 
do act on your behalf and they often act with you in a certain number of ways. And rather than see it 
as being a fly trapped inside of a matrix, I guess the question is really one of perspective. We see 
it as kind of the opposite: that these kinds of agencies can actually free us from the matrices that 
we're otherwise stuck with when you're given things in unresponsive manners. A: Well, I think it's just 
a matter for the laboratories of anti-media technology to worry about how to prevent communication. Q: 
My name's Tim Meyers from the University of Nebraska. And a previous question a few minutes ago, where 
you were saying that your tools aren't good enough is why you don't have very many artists currently 
working with you. And I think that's a mistake because your tools aren't going to be good enough until 
you get the people who are going to use them in working with you and developing them. Because there the 
people who are going to use them --it's severely retarding the development of the tools themselves by 
strictly putting technical people in there and saying, this is how we're going to do it. As the professor 
said, it's going from the bottom up. And it's not going to be as useful or as quickly developed as bringing 
in those people. They're used to using whatever they have at hand, however, difficult it is to use. ZELTZER: 
I said that there were a lot of answers to the question, and I didn't say any of the things that you 
just attributed to me. In the first place, I worked at Ohio State University in the computer graphics 
research group. And we worked --indeed, half of us were computer scientists and the other half were artists, 
and we had a very productive relationship between designers of software and users of software, and I 
think both communities learned a great deal from each other. I also said that many of my students were 
both artists and software developers and physicists. So I think in my own group, that I have both kinds 
of expertise. The reason I said that I thought the tools were too primitive, I felt that it wouldn't 
be fair to invite an outside artist to come and work in residence for some time. I think it's premature 
at the moment. But I don't think there's any lack of help with designing interfaces and user tools. Another 
point to be made is that the work that we're doing in graphics and animation we hope will be of use to 
artists at some point, but that's one piece of what graphics and animation is all about. I think it's 
a sub-set of the total uses which these kinds of animation tools. We're just as interested in getting 
engineers and scientists to use these smarter environments. I will say that, in terms of scientific visualization, 
Russell Kirsch, who's here today, told me some time ago that we have a lot of people around. In fact, 
the human race has been practicing the art of graphic communication for over 40,000 years. And, in terms 
of visualizing large amounts of data and phenomena, I think it's the graphic artists and the visual artists 
that have a lot to offer. I don't think we're quite as hostile to the arts as you seem to imply. Q: My 
name is J. C. Myda. I'm from GMS Technology in San Antonio, Texas. First of all, Mr. Minsky, I'm enjoying 
your book immensely. I'm in the middle of reading your book. I recommend it. I have a problem, however, 
with your top-down approach. I agree with top- down attack of a problem. But often times, I find that 
the top that I chose suddenly is not the top any more while I'm trying to go down. So I have to go up 
again. And how do I get out of such a dilemma? I haven't figured it out over the years. A: Well, I'm 
just worried that sometimes when people go on for many years without certain back-tracking and ask what 
problem was I really trying to solve. Computational linguistics, which probably doesn't bother most of 
you, bothers me a lot because people try to --in that field, there's been a lot of concern with the structure 
of language and the structure of grammar and so forth. And I talk to these people and most of them haven't 
thought -- since they were in grade school or college -- they haven't had that idea that language is 
a bunch of techniques by which the activities in one brain make something in the other brain. And I think 
that idea is in this book in some detail. But isn't it funny that such an idea should be new to people 
in linguistics? I talk to them and they say, oh, I never thought of it that way. And certainly any particular 
strategy you adopt has a 98% chance of being wrong because science is hard. But I was just saying a couple 
of weeks every year, you ought to sort of look at your calendar and say, have I done any high-level thinking 
this year? Q: I'm Orry Olebby from Georgia Tech. This is one of those top-down questions, I guess. And 
it comes in two parts: and the first one may be better directed towards Nicholas Negroponte, but since 
he's not here, I'll ask you guys. Given that the development that media technology is -- we accept it 
as a good thing and expect that maybe we'd like to see it proliferate, what would be the funding base 
that you would recommend looking to, to support the proliferation of educational and research centers 
for media technology? And the second half of that is what would be the national and international socio- 
economic trends that you would point to, to justify this development and proliferation as a good investment? 
A: Nicholas spends all his time, virtually, getting people to donate to this cause. I think a good deal 
of our laboratory's funded from people in Japan who share these visions. It's rather harder to get people 
here to do it. I guess I don't understand the last phrase about justifying it. There's no absolute justification 
for anything, and if you can get a lot of people to think it's good and pay for it, that's fine. A: 
There are probably 37 or 40 justifications for it who will mob you as you head towards the exit and, 
ff not, we'll inspire them to. But graduating some of the students is maybe one of the better socio-economic 
things we can do, and it's justification in its own fight, as far as I'm concerned. Probably should apply 
to the rest of you who are in school, as well.  Q: I just want to follow up on Marvin's comment on justification 
and the idea that if you can get enough people to pay for something, that justifies it. I just want to 
know does he feel that that, then, because they can get enough people to watch it, that justifies what's 
on television these days? A: It's your planet. ANDREW LIPPMAN: Okay, it's 5:00 o'clock and I think that 
gives us a good chance to stop. Thank you very much. i~lodle Labor,~lory Andrew Lippman Andrew Lippman 
  David Zeltzer MOD~L1NO BIIUAVIOll Compu~llUllll tikd~ ttcmiAi~, INmlAlUIIH IHdt~wl~, la otliulx4pd 
klJlrlreklil~ A# e4pINl'u Irel~rkl41m M ~ IkllJa ~lsd llle|m Ini41roonn4~ll~ul d4tllla l;u r~np ~r kkAv~'t 
 David Zeltze David Zeltzer  Walter Bend~ Wal{er Bender  Hardware Strategies for Hardware Strategies 
fo~ Scientific Visualization ScientificVisualization PANE[ INTRODUCTION PANEL IN]RODUCTION Robert Haber 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402252</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>36</pages>
		<display_no>10</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Hardware stategies for scientific visualization]]></title>
		<page_from>1</page_from>
		<page_to>36</page_to>
		<doi_number>10.1145/1402242.1402252</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402252</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101794</person_id>
				<author_profile_id><![CDATA[81100560180]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Urbana-Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101795</person_id>
				<author_profile_id><![CDATA[81365597127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jermoluk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101796</person_id>
				<author_profile_id><![CDATA[81100432746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[DeFanti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Chicago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101797</person_id>
				<author_profile_id><![CDATA[81100493909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lou]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doctor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Raster Technologies, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101798</person_id>
				<author_profile_id><![CDATA[81100028213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stellar Computer, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HARDWARE STATEGIES FOR SCIENTIFIC VISUALIZATION Chair: Robert Haber, University of Illinois at Urbana-Champaign 
Panelists: Tom Jermoluk, Silicon Graphics, Inc. Thomas A. DeFanti, University of Illinois at Chicago 
Lou Doctor, Raster Technologies, Inc. Frank Moss, Stellar Computer, Inc. HARDWARE STRATEGIES FOR SCIENTIFIC 
VISUALIZATION ROBERT HABER: Welcome to the panel, "Hardware Strategies for Scientific Visualization." 
I'm Bob Haber from the National Center for Supercomputing Applications. And before we get started, I'll 
make one procedural announcement. The conference organizers would like to keep the stage area clear after 
the session is over, so they have arranged a breakout room. If you have questions for the speakers, you 
can meet them in Room 303. That will allow time for the quick setup they need for the next panel that 
follows 15 minutes after ours. In fact, this panel and the following panel are companion sessions. They 
both relate to scientific visualization. This one focuses on hardware issues; and the next one will be 
complementary, focusing on software issues. rm going to begin by introducing the whole panel at this 
time. Our first speaker will be Frank Moss. He is the founder and vice president of Software Engineering 
at Stellar Computer. Prior to joining Stellar, Dr. Moss was vice president of Domain Engineering at Apollo 
Computer, Incorporated. He was responsible for the continued development of Apollo's operating system, 
UNIX, local area networks, heterogeneous interconnections and distributed processing architecture. Dr. 
Moss received his BSE degree in aerospace and mechanical sciences from Princeton University in 1971. 
Our second speaker is a change from the printed program. Jim Clark will not be able to make it today 
and in his place, Tom Jermoluk from Silicon Graphics will be speaking. Tom is vice president and general 
manager of the Advanced Systems Division of Silicon Graphics. He is responsible for the development of 
Silicon Graphics' high-end workstations, including graphics and CPU hardware, as well as operating system 
and graphics software. Previously, he was with Hewlett-Packard as head of hardware and software development 
for one of the new RISC computers of the HP precision architecture. He received a BSEE degree from Virginia 
Tech in 1978 and an MS in '79 from Virginia Tech. The third speaker will be Lou Doctor. He is president 
and co- founder of Raster Technologies. He founded Raster Tech in 1981 with Jay Torborg, who is now vice 
president of research and technology. Prior to founding Raster, Lou was a staff member at the Center 
for Interactive Computer Graphics at RPI; where he was involved in research in advanced 3-D computer 
graphics algorithms for computer-aided design. Then, finally, someone who is probably familiar to most 
of you, is Tom DeFanti Tom is an internationally recognized expert in computer graphics, having pioneered 
the development of an interactive graphics programing language for his dissertation at Ohio State. He 
came to the University of Illinois and founded the Circle Graphics Habitat in Chicago. His early efforts 
have evolved over time and, today, the third generation of his programing language, RT- 1, is the basis 
for all the interactive installations and the display of the exhibit, The Interactive Image, which is 
here at SIGGRAPH in the art show. He also co- founded the Electronic Visualization Lab, a joint effort 
of the College of Engineering and the School of Art and Design at the U of I. I think he also should 
be credited for turning SIGGRAPH from a fairly conventional show into a complete zoo dunng his time in 
charge; and we all thank him for that. I think. Before having the panelists start, I'm going to introduce 
three theme questions which I warned the speakers about prior to the panel, and give up a little bit 
of my neutrality by responding somewhat to each of them. That might give each of the speakers something 
else to respond to that they weren't expecting. So the first theme issue is, "What will the overall hardware 
environment be for scientific visualization in three years?" I have chosen that time as a period that 
will not be so far away that we'll just be doing Buck Rogers evaluations, but also as a period that is 
beyond the current product line so our vendors on the panel can't get too commercial. I think one of 
the issues here is, "Which classes of machine will be in the mix?" There's a number of different machine 
types that are now being hyped, I'd say a little bit, by the vendors, and maybe dunng this panel we can 
start to sort them out. Also, other issues we'll be looking at are mass storage, performance, networking 
and how will functionality be distributed among the different types of machines. On this issue, one of 
the questions that we've been facing at the University of Illinois Supercomputer Center is, "What is 
the future of the PC?" And I think to answer that question, "What is a PC?", is the first question you 
have to ask. You can answer that question in a number of different ways; and depending on how you answer 
it, you'll either say that it will not be part of the mix in a few years from now or it will be part 
of the mix in visualization. If you define it in terms of cost, as say, under $10,000 and something that's 
owned by a single individual, I say, yes, that will certainly be there. If you say it's distinguished 
by the type of operating system and software environment, I think the answer is no. I'm sure people will 
disagree with this, but I would at least prefer not to see it there; because having to deal with multiple 
operating systems is a problem. The PC architecture I think also will not be very much distinguished 
from a workstation by that time. So other than cost and ownership, I predict that PCs as a class of machine 
are not going to be distinguished from workstations. Super workstations is a type of machine that's been 
introduced for the firtst time this year, and I know the panelists are going to get into that. So I'm 
not going to make any further comments. One of the things that you want to be looking at in workstation 
architecture - since I think there are some changes coming in this year - are bus designs. We're getting 
away from VME. Components will all be going into parallel, whether CPU, graphics engines or frame buffers. 
And the frame buffers, of course, will have quite a bit more memory associated with them. That, I don't 
think, is too controversial. I think we'll be moving away from the sort of isolated environment around 
supercomputers, middle-tier machines and workstations and PCs, where they're fairly separate in function, 
to a more integrated approach like you see on the slide. You're going to have very high bandwidth networks 
to make that feasible. We'll be installing a 100 megabyte per second network at Illinois this year. There 
will be segregation of the traffic between layers of the newtork. You'll also start to see integrated 
graphics continue to creep up from the PC and workstation level into the middle tier machines, and eventually, 
I'm going to see if I can bang on Cray &#38; Fujitsu and get them to integrate graphics engines and frame 
buffers directly into their machines. Really, the need for integrated graphics is greater as you go up 
than as you go down. It's just history that's responsible for what we have at this point. Mass storage, 
is one thing that's currently the most out of balance in our computing environments. For example, if 
you look at the real prices of the super workstations when you put on a reasonable amount of disk, this 
point becomes clear. Right now we're talking about gigabytes of mass store as what is needed for local 
workstations in supercomputing environments, and terabytes on central machines now -and workstations 
say in a few years from now. The cost is far above and the transfer rates are way below what they need 
to be. The second issue that I'd like the panelists to address is, "What will be the balance between 
hardware and software implementations of visualization functions in the next three years?" In the graphics 
world, we've seen the introduction of a lot of hardware accelerators to execute the compute intensive 
part of the graphics rendering algorithms in real time or in at least balanced time with respect to the 
rest of the process. There is a tradeoff there. If you put something into hardware, you usually give 
up some flexibility. So I think we'll see a continuing debate as time goes on between which functions 
are put onto the general purpose CPU and which ones are going to be implemented in hardware. I'd like 
to see what the panelists think about that issue, where that's going. Also, identify where the key bottlenecks 
are going to be. Part of the response to this a look at what role computer graphics plays in a much braoder 
framework. These two slides show one diagram of the scientific visualization process, where traditional 
computer graphics is really just the last step, the rendenng step. That's down here. There's quite a 
bit of compute intensive work that goes on in data enrichment and mapping your data into some kind of 
visual representation. Those operations are often many times more compute-intensive than the original 
Cray simulation, and also more compute- intensive than the rendenng operation. So it seems that some 
of these steps might require some hardware acceleration in the future as well. To get there, we'll need 
to have a well- defined structure for this visualization process. And as we go forward toward having 
interactive steenng of scientific simulation - so you're not just interacting with your graphics, but 
really interacting with the model, the physical model - then this much bigger process is going to have 
to be addressed. It won't do us any good to have millions of polygons per second rendenng rates if we 
can't do our simulation time steps in less than 10 minutes. I think we're going to have to look at the 
whole scientific simulation process to see where we're going to need hardware accelerators to eliminate 
bottlenecks. The trend is going to be toward a much broader view of scientific visualization than we've 
had up to now. These slides indicate some things to keep in mind with regard to what I was just saying. 
There is one time scale that normalizes everything else in this process. If we're talking about real-time 
animation from a simulation, the animation time per frame is essentially fixed, somewhere between one 
fifteenth and one thirtieth of a second. We should compare the time it takes to do simulation updtaes 
or to transmit data across a network or a bus for rendering operations to animation time; and make sure 
that all of these are in balance. The third issue is, "What are the market projections for scientific 
visualization hardware systems over the next three years?" Visualization is a fairly new field, at least 
in terms of its publicity and the amount of interest that it's been getting in the last couple of years. 
The question for the vendors is, "How much market is really out there and can it support the amount of 
development of new technology that's required?" What types of computer systems will be the main elements 
of the mix? Should they be putting their efforts into super workstations or more modest systems - or 
looking at departmental machines? How will that pan out? Finally, I'll sum up with a discussion of the 
NCSA Rivers project. This is an R&#38;D effort I'm heading at the University of Illinois which is involved 
with both hardware and software development. Here are some of the things we're doing in the hardware 
end. We're looking at very high-performance graphics systems, but also very high-performance network 
systems to tie them together. And we are very interested in hardware-related software standards for workstations 
that support things like PHIGS+. We see standards as very important to ensure that there will be healthy 
visualization market. Standards will allow the very large costs involved in developing visualization 
software tools to be amortized among a larger community. So we are looking at systems that support X 
Windows and NeWS and PHIGS+, well- integrated with the hardware. If the hardware vendors don't support 
those standards with their high-performance engines, I think we're going to do the whole country a tremendous 
disservice. All this bnngs us to the fun part. Finally, after waiting for 12 or 15 years, we are seeing 
raster graphics move into the real- time interactive environment that we've had since the mid- seventies 
in vector graphics. This allows the scientist to move around in the data, to really be at home with it. 
In the case of volumetric data sets, I don't think there is any way you can really understand the whole 
volume of data from a representation in a single frame. You need to be able to feel around in the volume 
in some sense. And the real time capabilities we now see are key to doing that. This is some crack propagation 
work that I did. It wasn't until very recently that we could afford to fly around it. There were views 
I just never would get to see because of the time involved in generating them. So if we could roll the 
videotape, there's a brief piece here that gives you a flavor of what that's like. This was actually 
recorded last Sunday on the exhibition floor here. This was taped in real time - you'll see the interactive 
devices. I should credit Polly Baker, who did the NeWS programing, Ray Idaszak, who did a lot of the 
PHIGS+ programing, and Steve Chall, who also did a lot of work on these systems. Here you see animation 
controls for stop motion, forward and reverse, stepping through step by step. To give you some reference, 
when we were doing this a few years ago, it took about three minutes per frame. So the speedup here is 
on the order of two or three thousand times what we could do a few years ago. Now you'll see the interactive 
controls for panning and rotating and looking at this data from any position. This gives the flavor of 
the kind of environment that I think scientists will be working in in the future. Okay, we can stop there. 
Finally, the last point I would make is that all of this needs to go out across the network. We'd like 
to bring this kind of work out beyond the campus level; and this means that we need a national network 
that can sustain the kind of data rates we're going to be experimenting with this year on the local level. 
The rates are going to have to be up in the realm of 100 megabytes per second, sustained, for a single 
user. I think we need a national infrastructure program, like we had with highways in the 1950s, to build 
a very high- performance research network for the country for the nineties that will give US science 
and industry a competitive edge in the international market. I'll quit there and move on to our next 
speaker, who is Frank Moss from Stellar Computer FRANK MOSS: Thank you very much, Bob. Hype. You haven't 
seen anything yet. Well, thank you. As a software person, or at least having been typecast as that, I 
would like to begin by projecting that the directions and strategies in hardware for visualization will 
not be driven by technology. Rather, they will be driven by this new class of applications that Bob has 
discussed, whereby users intimately observe and interact with complex 3-D models. They dynamically change 
in time to reflect underlying physical and mathematical principles in response to user input. I've heard 
this called by several names, visual simulation. Randy Smith of Xerox PARC has called it realization 
as opposed to visualization, and I think that's a neat enough term. You see many early examples of that 
on the SIGGRAPH floor this week. Now realization applications, if I can call them that for short just 
for this talk, will require a new metric of hardware or system performance that goes well beyond the 
MIPS, MEGAFLOPS and MEGAPOLYS of various types and stripes that we've been talking about. The fixed ideas. 
Consider the general structure of these applications as shown on the left of your screen. Now the underlying 
dynamic analysis will be performed by a numerically intensive computation, abbreviated, NIC. An example 
of this is the Navier-Stokes equation of computational fluid dynamics. The NIC generates dynamically 
changing values of raw data, for example, the values of pressure at points in the 3-D grid. This raw 
data must be converted to viewable graphical renderings by a mapping function, abbreviated as MAP on 
your screen. In our CFD example, the discrete points of pressure must be mapped from a 3-D grid to isosurfaces 
to make them usable and viewable. Finally, the primitives are dynamically rendered as viewable images 
to the user. The user can interact with either the rendering, with the mapping, or with the numerically 
intensive compute. This is the model that we see. By analogy to animation, you can think of the notion 
of a realization frame, which is composed of a single step of a numerically intensive compute, the mapping, 
the rendering, and the all-important data transfers between them. I'm suggesting, just for the sake of 
argument, that you can conceive of trying to maximize the number of realizations per second. This gives 
you the levels of dynamics and the interactivity that will be desired over a broad range of applications 
which have different mixes of computation, of mapping functions, and of graphics. Now given a desire 
to maximize RPS, I'd like to suggest three hardware system strategies. Actually, they're system strategies 
because they involve software as well. I believe that these three strategies will emerge first in the 
form of a new class of computer system, or a new member of the technical computing hierarchy, which has 
come to be known as a graphics supercomputer. You've seen several here at SIGGRAPH this week. They'll 
be distinguished, as I said before, not only by MIPS, MEGAFLOPS and MEGAPOLYS, but by these three architectural 
characteristics which I believe will become more and more common in systems of all types as we move along. 
Let me look at these briefly, one at a time. First, let's deal with the question of bottlenecks in the 
visualization pipeline or the realization pipeline, and the issue of bandwidth. On the left of your screen 
you see the function distribution as it typically appears today. The numerically intensive compute and 
mapping on the supercomputer connected, typically by an ethernet or eventually by higher-bandwidth networks, 
to a graphics workstation which typically runs, perhaps, a piece of the mapping, but primarily, the rendering 
of the display. The user interacts with the rendering and occasionally interacts with the function on 
the supercomputer. Network bandwidth, more or less, is one megabyte per second throughput. Now over the 
next several years, those bandwidths will increase, at least by an order of magnitude with FDDI and by 
several orders of magnitude with special-purpose, high-bandwidth channel type networks like HSC and Ultra-Net. 
But I contend that with growth in the power of supercomputers and the power of graphics, that networks 
will remain a relative bottleneck in the dynamic operation of the realization pipeline. So I believe 
that yet another order of magnitude increase will be required, and that I believe, will be delivered 
as internal bandwidth in very, very high-performance systems. Pieces of the numerically intensive computation, 
denoted here by NIC double prime, will be offloaded from the supercomputer to graphics supercomputer 
kinds of systems; and it will be tightly integrated, with a bandwidth exceeding a gigabyte per second. 
Over the next three years, I see those internal bandwidths increasing even more, and having access to 
even larger memories and larger caches which greatly increase the flexibility and the efficiency of data 
transfer between the various steps of the realization pipeline. I think this is a very, very important 
step and I think it will be reflected in MAP a generation later within networks. Now, the second strategy 
or trend that I'd like to talk about is that of trying to meet a price point. These systems are expensive. 
The capabilities are complex. And we ask the question, how can we reduce costs and drive them down? We 
all want to do that. Well, I believe that an important notion is to recognize the commonality between 
the needs of supercomputing, what I've called NIC and MAP here, and the need for high performance, high 
functionality, 3-D graphics rendering. We can reduce cost by sharing these capabilities. On the left, 
I've depicted by this Venn diagram the capabilities that are typically devoted in a supercomputer to 
a numerically intensive compute and to the mapping. And on the right, I've depicted those that are dedicated 
in a graphics workstation to rendering. Now the trend will be toward more general-purpose functional 
units, indicated in the center by the intersection, and away from specialized special-purpose hardware 
whenever possible or whenever appropriate. For example, at the intersection we see vector floating point 
processing. Typically used for large-scale numerical computation, this can constitute the front end of 
a graphics rendering pipeline and perform functions like translation, clipping and lighting computations. 
Large memories, which are typical to supercomputers, can be used to contain virtual pixel maps of arbitrary 
depth and of arbitrary size and eliminate the need for very large costly frame buffers, etc. There are 
a number of things that can be shared in these systems and it will give rise to the three advantages 
which I've listed. You can have a lever between computation and graphics and allocate your resources 
in your application according to its needs at the time. You may dynamically change the allocation between 
NIC, MAP and rendering, as the application proceeds, or among a number of different applications. We 
can apply the well- known advantages of virtual memory, to graphics: the ability to have arbitrarily 
large images, the ability to have application access to pixels for functions like image processing -all 
transparent to the application programer. And finally, by bringing some of what were previously special- 
purpose, lower-level hardware functions into a general-purpose programing environment, we can expedite 
development. First development by vendors, then development by users. For example, the ability to move 
the mapping function, today performed primarily on the compute piece of the machine, into the pixel processor, 
or into the vector floating point processor shared with the pixel processor. There are great opportunities 
for advancing performance in that way, and for advanced rendering techniques, such as photo-realistic 
rendering and volumetric rendering. And finally, my third trend, if you will, is the trend toward a new 
meaning of parallelism. Parallelism, to date, has been limited to breaking up the loops of a numerically 
intensive computation into smaller pieces, and running them in a fine-grain fashion on parallel processors. 
Well, I believe that with the advent of general-purpose multiple processing units in graphics supercomputers 
and other systems, that we'll be able to greatly increase the rate at which we can execute the realization 
pipeline. At the top, we see a serial execution of the pipeline. I've normalized that to realizations 
per second of one. In the center strip we see what would happen if you allocate different functions of 
the pipeline to different streams of execution synchronized in a way that reduces your time to execution 
and increases your throughput by greater than a factor of two. Again, we are exploiting multiple functional 
units, not only for numerically intensive compute, but for graphics functions. At the bottom, I've illustrated 
how you can increase your performance by even greater factors, if you pay attention to a finer-grain 
level of parallelism within each of the functions (which in a sense has always been applied to numerically 
intensive compute). Here, the theoretical optimum of three is reached. So I really do believe that the 
concept of parallelism will become a much broader, a much more general notion over the next two or three 
years. Now I mentioned before that MIPS, MEGAFLOPS and MEGAPOLYS are not the entire story; that you need 
architectural features that allow applications to tap into those resources, to get the levels of performance 
needed for dynamic visualization. But for those of you who like to play the numbers, I've offered my 
version of a prediction that has a little bit to do with where we think we're going at Stellar, but also 
with where we see the technology and the applications driving things. I've shown price versus year projected 
out to 1992; and you can see that in three years, I see the price of high-end systems moving down through 
the learning curve. I've demonstrated the numbers I have there. I've illustrated the numbers, 200 MIPS, 
80 MEGAFLOPS LINPACK, and significant dynamic rendering capabilities for photo-realistic rendering. I 
also see the kinds of capabilities that we offer today appearing in mid-range machines of half the price, 
beginning at about 1990. So, I'm told to sum up and I've got the yellow light here just in time for the 
last slide. And my message is that MIPS, MEGAFLOPS and MEGAPOLYS are not the bottom line. Unique architectural 
features will appear and, indeed, they'll have to be exploited by capabilities in software, analogous 
to optimizing compilers, that make it easy for scientists and engineers to exploit and tap into this 
great level of power and to write dynamic visualization for realization applications. So that's my talk. 
Thank you very much. HABER: Thank you, Frank. The next speaker is Tom Jermoluk from Silicon Graphics. 
TOM JERMOLUK: Good morning. I guess that I'm the specialized part of the talk this morning, as opposed 
to the general-purpose part of the talk. I'd like to start out with a little bit of a look at the market, 
though. You'll all be pleased to know that we do believe the market will continue to exist in 1992. I 
can't have a graphics talk without a pretty picture. This is "Directions in Visualization," so I thought 
that I'd show you where we would like to go with visualization. Some of the market surveys that we have 
about the size of the three-dimensional visualization market out over the next few years show that we 
are on the underside of the knee of the curve. I think this technology is exploding. Various people that 
you see here, Dataquest and Alex Brown, etc., have estimated this market to be growing fairly exponentially 
over the next few years. Where this market will lead in terms of users, though, is the more interesting 
question. I believe that we've come from an era of the power user, the era of the person who's on the 
leading edge of technology - the animation users, the visual simulation users - and we're moving much 
more into the era of the application users. When you can walk into a Chrysler or a Ford or a GM or a 
Boeing or a Lockheed, people who are actually out there doing real work with systems on every engineer's 
desk, that's when you're getting into the application era, and that's what we believe to be approaching 
today from many of the companies involved with this technology. The more interesting one in the future 
will be the consumers. When and if 3-D visualization gets into the consumer market, that of course will 
significantly change the way that most people think about dealing and interacting with computers. I think 
a large part of what scares off some of the consumer market today is the lack of an interactive realistic 
interface for these systems. The sorts of things which denote the users that we see today, though, is 
a question of interest. The power user typically has no concern but the absolute highest power that they 
can achieve on the system and the most functionality they can get. They rarely care about standards. 
They rarely care about how it looks. They don't care about how much power it takes or how much money 
it costs. They simply have a job that's too large to be done in any other way. The more advanced users 
do care about those things that I mentioned. They're looking to do a job over a long term. They are probably 
people who are not writing applications, but they are modifying things for the rest of a community within 
their company to use; and they have a certain need for broad product lines. They want the high end, they 
want the low end; so they can distribute their software. They want flexibility in the system, to be able 
to provide many different functions and tackle lots of different tasks in their day-to-day work. And 
compatibility across the line is very important, as well as compatibility with other workstations and 
large computer vendors. Then the application user is primarily motivated by price. There's a lot of price 
elasticity in that particular marketplace. Ease of use. They don't particularly care to deal with complex 
graphics languages or UNIX interfaces. They want simple turnkey applications. And they're very interested 
in how many packages are available, much like the early PC users were. Some of these characteristics 
of compatibility, networking, single vendor, are things that large companies look for when they're buying 
across a product line. This is sort of a visual of where we see the bulk of this market. For every workstation 
categorized by the top of the pyramid, say, $75,000 and up, I believe that there's about 30 workstations 
that will go with it for the application users. So about 10% of the market is in super workstations and 
by far the majority is in what we'd refer to in the future as personal 3-D workstations. I'll talk a 
little bit about technology. I think that Frank had a slide much like this. The upper one shows the direction 
that we will go in. We have a multiple CPU architecture, a dedicated graphics subsystem and a display 
subsystem, and that's as contrasted to a general- purpose computer which would have multiple processors 
hooked to a display system. He's right in that the link between those two, as between Crays and graphics 
workstations, has traditionally been a network, some kind of networking communications capability. Now 
the link is migrating more toward a bus, some sort of high-speed bus hooked as a memory interconnect 
between the processor and the display portion of the subsystem. So that's a very interesting trend. A 
number of people are taking that particular thing. I don't believe that there's necessarily any pluses 
or minuses to either one of these. It's more a question of focus, where do you particularly see some 
of these workstations being used. It's our belief that the graphics subsystem is a vital piece for the 
interactivity of 3-D, which is the primary market that we're interested in. Because in fact, the rates 
that are achieved by the specialized hardware and the graphics subsystem aren't attainable on the more 
general- purpose CPUs. In addition, if you're using the bandwidth on your general CPUs for the analysis 
operations, you're not drawing graphics. So you're either doing one or the other, as opposed to the interactive 
scenario where you want to be doing some analysis on the data while you're actually interacting with 
the graphical representation. An example of this is a calculation of the number of MEGAFLOPS required 
to do a polygonal transformation and viewing. If you took 100,000 of these four-sided polygons and did 
the appropriate calculations required to display them - you have the transformation of the vertices and 
the normals, the lighting, the clipping, the projection for perspective and the viewport mapping - that's 
about 46.5 MEGAFLOPS. We believe that it requires a rather specialized architecture to be able to sustain 
that sort of a rate. And if you're sustaining that rate --if you are attempting to sustain that rate 
on your general-purpose side, then those MEGAFLOPS aren't available for the analytical calculations that 
you'd like to have going on. This is the sort of program where you'd like to interact with your analysis 
and your graphics, mapping of the stress points on a piston carved from a cylinder block. You'd like 
to be able to move that around in real time, which obviously, I can't do here. But you'd like to move 
that around and move the cutting plane through, watching where the stress points are in that piston, 
doing the analysis on the host and the graphics display with the graphics subsystem. Where is it going? 
Oh, I'm sorry. A little out of order. This is another representation. I'm in agreement with Frank on 
this point in that graphics data expands exponentially. The data in portion here is described as 100 
surface representation patches, each with 16 control points representing 9.6 kilobytes of data. The data 
out, in actuality, the data that your eye sees, is represented by about 20,000 polygons of 100 pixels 
each, with over 16 million bytes of information. So the problem is, where do you do the expansion of 
that data? If you do it on the general purpose side, that means that the data transfer requirements go 
up significantly over the bus that might also connect you with your memory or your I/O system. That takes 
away bandwidth from the analysis calculations once again. You'd like to have that down in a more specialized 
area that can handle that bus traffic totally in parallel with the rest of your processing. So, where 
is it going? As I look out over the next couple of years, among the things that we see users asking for, 
more and more realism is very high on the list. Phong shading and other advanced lighting techniques. 
Texture and environment mapping is very important for the realism aspects. Anti- aliasing. There's some 
technology reasons why monitors aren't going to a lot higher res very quickly, and to compensate for 
that, many people are interested in anti- aliasing techniques. I think you'll see those a lot more prevalent 
over the next couple of years. And certainly, surface representations to, once again, cut down on the 
amount of data transfer required, if nothing else. More and more performance, smaller polygons, larger 
numbers of polygons. As your requirements for realism go up, the size of your polygon goes down with 
respect to the lighting models and other aspects of the graphical computations. Lower clear times. The 
interactivity that Bob referred to is very important. You want fast screen clear times. You want to be 
able to interact and move those objects on the screen without wasting a lot of your drawing time, and 
you want a lot lower overhead for your interaction. More hardware for window manager support. More hardware 
for interaction with the operating system. And finally, more and more integration. Probably this is most 
important. As we move into the application-ease-of-use area, these are, I believe, the keys to the successful 
company. To integrate things very successfully with the window managers, to have X with a nice 3-D representation, 
NeWS with a good 3-D representation is very important. To have your operating system fully support and 
understand the requirements of the graphics subsystem, the schedulings required. And finally, of course, 
to network these things together very successfully. Because, in fact, there will always be a need for 
several levels of these different types of computers in any significant application. Thank you. HABER: 
Thank you, Tom. The next speaker will be Lou Doctor from Raster Technologies and ALLIANT. LOU DOCTOR: 
And now for something completely different. What I wanted to talk about today is actually quite a twist, 
in contrast to what was talked about by the first panel members. So, rather than telling you that workstations 
are getting faster and more powerful and that you'll be able to do a lot of local computation, I'm going 
to try in my talk to spur a little bit of thought as to what could be - as opposed to what seems to be 
happening - to give you a bit of an indication as to where we're going in the longer term. I think everybody 
is in agreement that today's systems are really sort of too slow. They are too slow in just about every 
respect. A hundred times better computation, a hundred times better graphics. If you could have any system 
you wanted, you would clearly take systems that are a lot more powerful now and make them available to 
users. But the other thing is that they're too expensive. It's pretty obvious that not every scientist 
and engineer can afford a $100,000 and up workstation, and even when they get to $50,000 and up, they're 
still going to be too expensive. The third element is that they're also too impractical. I'm speaking 
as a vendor into this market; and I find it ironic that when you decide to spend 10 times as much for 
a high-end workstation as you spend for your $10,000 and below PC or Mac, the high-end workstation is 
almost completely useless to do anything but high-end graphics. It can't run all the utility software 
and all of the things that scientists and engineers do 90% of the time. So we've looked at that problem 
and we came to the conclusion that the high-end workstation market is going to be a market that basically 
is represented by what I would call islands of computation. If a user is literally isolated, maybe in 
an island in Hawaii or something, maybe they need a high-end workstation because they can't talk to anyone 
and they can't take advantage of other people around them in the organization and the buying power of 
being able to put some of their dollars together. I think that what's going to happen is that you're 
going to see all different kinds of workstations emerging. You've got these commercial 386 based ones 
that are coming out with follow-ons and those are clearly being migrated into what rll call the commercial 
and office market. I think everybody would agree that they're not that interesting for scientific visualization. 
Then you've got a whole family of RISC based architectures for CASE with 68000 instruction sets, or maybe 
RISC instruction sets. Those really don't have the graphics horsepower, and they aren't that interesting 
either for scientific visualization. You've got this market that I think Tom was talking about for what 
I'll call the production CAD user. That's the person that sits in front of their screen all day long 
and does nothing but graphics. They need sub-one- second response time; and they need good local 3-D 
capability. That clearly is a growth market. Companies like IBM, Computervision and Prime are shipping 
tens of thousands of workstations into that market every year and selling them to the automotive companies 
that Tom had talked about. So there is clearly a market there; and that market is clearly growing. But 
now I've taken another twist; and I've defined a new class of workstation that I contend is going to 
emerge as the dominate way of doing scientific visualization over this three- year period that Bob asked 
us to talk about. That's basically using cheap workstations that have essentially a live video capability. 
And rll talk more about that in a minute. So I think that what is going to happen is that we will very 
much see a computing hierarchy, just like Bob had talked about and shown on the slide of his architecture. 
We think that companies or organizations will gang together and pool all of their dollars and make what 
will emerge as a strategic decision to buy a supercomputer. I think people are realizing that the long-range 
implications of supercomputing are so tremendous that the most innovative companies and the companies 
that have that vision are buying supercomputers. So you'll see one or more clustered in a group for an 
entire company. Now I've thrown out a new notion which applies when you have a work group or a department 
that has to use machines. I've invented this new machine type which I'll call a visual server. But a 
visual server is a machine that is connected to a supercomputing network at very high speed, but basically 
gives a department or work group a local capability for both graphics and MEGAFLOPS that is far in excess 
of what each user could afford to buy for themselves. Essentially, a lot of the work that we've done, 
now that we've merged together with ALLIANT, is in trying to look at this marketplace for multi-user 
servers that have very large physical memory systems, all of the characteristics that we talked about 
here previously. But they have specialized processors for graphics, I/O and computation. That's the nature 
of a multi-user system, and that's why it varies quite a bit from what you would design for a workstation 
architecture. I think that the networks are going to advance, not just in their ability to handle digital 
data, because digital data is only one form of the data. But I think that the networks will advance to 
the point where off-the-shelf video technology can be usable for delivering pictures to people sitting 
at multi-media work stations. So you literally could sit in your office on a Mac three years from now, 
pop open a window and get pictures from a remote server or supercomputer. I see that as being the direction 
that the market is going for the vast majority of systems for scientific visualization, because 90% of 
what scientists do does not require the graphics and visualization power of a $100,000 workstation. So 
that's my key point of controversy. So I think that is what we'll do. We've defined this new class of 
machine, a visualization server in some fashion. What are the characteristics of it? Well, number one, 
it interfaces a lot better to existing supercomputers and extremely high-speed networks. I think Bob 
described this 100 gigabit network, or trying to move these extremely large data files around and load 
balance between supercomputers on a network. It's very, very clear that you've got to have that. But 
it also seems clear to me that those network nodes are not going to be practical to go into your Mac 
or workstation directly. In fact, you need impedance matching computers between the high speed networks 
that move data around in supercomputers and the sort of PC or low-end workstation you're going to have 
on your desk. I think that this notion of what I'm calling visualization on demand as a time sharing 
resource is really an extension of the idea of being able to move pictures around rather freely. So it 
is a kind of an odd twist, but it's not so far-fetched that it doesn't make a lot of sense for a lot 
of applications. In effect, what we've said is, we kind of shared this vision. We decided to take a company 
that had the graphics to build a super workstation, and essentially take that super workstation graphics 
and stick it onto a server class machine - where each processor inside of, say, an ALLIANT machine or 
any one of these other server machines, is more powerful than you can buy in the most expensive super 
workstation. And when you have a departmental resource that has, perhaps, 10 times that power, you're 
really giving every user on-demand access to much more performance than would have been practical otherwise. 
So the idea is that basically we demonstrated this capability by hooking up a few screens that were essentially 
direct connected. We designed a system where the memory system of the server was the memory system that 
you build graphics structures in. And we've done a lot of work on the idea of how you build an architecture 
that can support multiple users. We've announced a system that supports from one to 16 visual channels 
off of your server; where every visual channel in this product has the look and feel of a workstation. 
So you can use things like fiber optic links to pull these screens away from the server and give them 
a capability to be brought into somebody's office. So it's sort of an odd twist again. In effect, we've 
replaced ethernet by the memory system of the server for building graphics structures; and then we basically 
ship video to users, as opposed to trying to ship digital data. So I think that what we see is that, 
essentially, these leading workstation vendors are going to have this multi-media capability. It doesn't 
take much to extrapolate what you can see now from companies such as Apple and Sun to imagine that you'll 
be able to have workstations in which you can pop windows. And I think you're going to have some local 
graphics capability also. It's pretty clear that there is going to be some level of graphics that you 
want to be able to generate yourself. But for that really peak demanding visualization requirement that 
you only use once in a while, it's actually more practical to have those images and those image data 
bases rendered and managed by a multi-user server, because of the nature of the problem. The problem 
still seems to be getting more expensive and more powerful, again, taking us toward multi-user systems. 
I think that the kinds of pictures that people will pop up in their windows are going to be essentially 
live video that comes off of these remote servers and also things that they may have in their office, 
like videotape and disk and cable TV services. So they're going to have an ability to get a lot more 
usefulness from their PCs, or super Macs, if you will. I think at the same time, they'll have access 
on demand to the most powerful visualization resources. And that's really a key to my observation about 
the marketplace. So what are the characteristics of this sort of an approach? You get the benefits of 
what I'm calling low cost per seat, and you have all of the usefulness of the existing kind of PC to 
run all the spread sheet and planning and word processing software that people do most of the time, at 
least in the science and engineering area. We're able to take advantage of this large installed base 
of PCs and Macs and whatever to tap into these visual resources. I think that window systems are going 
to be sophisticated enough to handle this notion of mixing together, for example, a live video capability 
along with digital data. I think you're very much going to be able to optimize both the supercomputer 
resources, which are going to remain centralized at least for the next 50 years, and the scientist resources, 
which are going to be decentralized, where people do most of their work in their office sitting in front 
of their PC or workstation. While I'm not by any means forecasting gloom and doom for the high-end workstation 
market, I think it's going to emerge as a very specialized market - only for circumstances where you 
can't tap into these server resources. I think that there are also going to be applications that demand 
a level of real-time performance at the workstation, such as the military and the defense area, that 
are simply going to demand local graphics accelerators. But I don't see it becoming the mainstream. I 
see an approach like this becoming the mainstream, and I see us doing a lot of work together with ALLIANT 
to make that happen. I'm getting the boot here. So again, I think that's my talk and I'll cut it off 
right there. That was my last line anyway. HABER: Thank you very much, Lou. I'm being a little bit of 
a dictator here to keep us on schedule. The last speaker is Tom Defanti, who will take up the "Everyman's" 
point of view. TOM DEFANTI: The people you just heard from, of course, are more or less from what I call 
the conspicuous consumption point of view of computing. I've always --most of you who know me, know that 
I kind of came out of video games. So I have a little different approach to all of this; and I've been 
very interested in getting this stuff out to people who are qmte poor. One way to get 3-D visualization 
to people is what we have on the corner of the stage over there. It's called a phscalogram; and I hope 
if you haven't seen one before in the art show or in one of the 3-D sessions, that you come up and look 
at it. It's a way to involve people who don't happen to have $100,000 workstations to see things in three 
dimensions in a public space. In order to compete with these guys - Stellar and other astronomical-type 
projections, I had to go five years into the future. I think that in five years in the future, we're 
going to have PCs on our desks. Now unfortunately, I used the large letters when I was typing this slide 
in last week. I used the large letters "PC" and I actually meant to use the small letters "pc"; because 
the large letters PC really mean IBM PC and clones. I also noticed this week that the word "artist" has 
become a trademark of somebody; and I don't know how to react to that in my future talks. But I think 
of PCs as being personal computers and this -- I only have one slide. I was able to get all of my thoughts 
onto one slide. -- is essentially what I like about current PCs. I suspect that things that we call PCs 
in the future, regardless of whether they have a 16 bit architecture, will look like this. Another thing 
is that one button I forgot to make this time would read, "I use the same hardware as Jim Blinn." I have 
in fact for many years used PDP-11s and VAX's; and Jim's currently using PCs with Vista boards in them. 
So one can do scientific visualization on very modest equipment. Okay. The things I like about PCs are 
that they're networkable and you can plug them in whenever you want. You can plug them into graphics 
servers, which were actually described in the (NSF) visualization report. One of the nice things, though, 
is you can unplug them. We have the experience of unplugging our Silicon Graphics and Hewlett-Packard 
high-end workstations, and they complain a lot when you unplug the -- have you ever done that, unplug 
the ethemet? They sit there and complain at you. It's really annoying. Configurable. I like the fact 
that there are two zillion people who make PCs; and when I get mad at one vendor I can get one from somewhere 
else. Every part of it is multi-sourced. You can mix and match it. If you don't like a particular board 
that's available, if you just wait a couple of months and keep reading the back of Byte magazine, you'll 
find one that does what you want. So that's kind of nice. You don't have to put pressure on a single 
hardware manufacturer to do what you want them to do. And in science, that's really tough. If you go 
into any big chemistry building, for instance, you'll find out that they blow their own glass and they 
have a machine shop with multi-access milling machines. The reason why they have this is because you 
can't get people to build the kind of stuff you want. You have to do it yourself. So the configurability 
is extremely important. So is the hackability. PCs, as they exist right now, are very easy to hack. There 
are all sorts of high school students and young college students who have these things at home; and you 
can actually let them just have them. They're cheap enough that they can just have them to hack at. This 
is not so true with UNIX. It's relatively difficult to find people who will admit to being able to write 
UNIX drivers. And anytime you want to hook some whacko device or do something slightly different -- which 
is really what science is about -- into your computer, it's very difficult in UNIX. It's not as bad as 
it is to try and hook something into an IBM mainframe or something, but it's really tough. PCs actually 
make pretty good interface units. Really, if you look at even the current 640K space, that's not a bad 
space for human interaction. I mean, just to talk to the human, 640K isn't bad. Everything else you do 
with boards that you plug in. I currently have a PC that has 640K address space and 14 megabytes of graphics 
space on two boards. So that's pretty serious. It's very much like the old LINC-8s, which were essentially 
the first laboratory machines, and the PDP- 1 ls, where people just plug things in. I realize I'm getting 
a little away from the general concept of visualization for computational only, where you're dealing 
with people who are doing all their science on large computers, to people who are mixing real-world things 
in there. But I see those guys as scientists, too. Recordable. Listen to this. Everybody listen, all 
right? If it ain't recordable, it ain't science. It doesn't count. And that's real serious. If you want 
to get scientists' ears, you've got to give them a way of preserving it. You don't get away with saying, 
"Well gee, I did some nice science last week but I forgot to take notes." It doesn't work. There was 
a Journal of Irreproducible Science. We now have a Journal of Unrecordable Science. In order to be recordable, 
it's got to be legit NTSC video. People call everything video; but it's got to be legit NTSC video. And 
most of the stuff out there isn't. Digital people simply don't understand that they can read documents 
and figure out how to do this stuff. Another reason to go for recordability --I mean, people say, "Well, 
I don't need to record it. I can just replay it and see it in high res," and all that sort of stuff. 
How many people out there really feel like they could, if they went back to the labs now, run every program 
that they were able to run three years ago? Anybody? How many people feel like they could play every 
videotape they played three years ago? No problem, right? So, 15 years ago? I can play all the videotapes 
I made 15 years ago. I can't even find the hardware that I was using back then. So if we want to communicate 
this stuff to the future, which is what civilization is about, we probably ought to record some of it. 
I like the fact that PCs are portable. It really opens a whole lot of ways of getting these things out 
to people. You can ship the disk, throw the thing in UPS -- it's not the kind of thing that you want 
to do with an ALLIANT, I would guess, right? -- and have it show up someplace. And boy, there's a lot 
of people out there that can take things out of boxes. You just have your auto boot thing, you plug it 
in and it comes up and does what it's supposed to do. You can take it to the cottage, that kind of stuff. 
That's wonderful. The last big point is affordability. The model that I've kept in my mind for many, 
many years has been the three-year-old Buick. The "I want to have systems that are right at the entry 
level" is right about what it costs to buy a three-year-old Electra 225. That's about $10,000. That's 
the kind of money that we in universities, and I suspect scientists in labs and things, consider to be 
commodities. A junior professor can go into his department chairman and say, "I need $10,000 to buy a 
visualization thing," and he'll probably get it at the end of the year when there's some money left over. 
If he walks in and says, "I need $40,000," then it's, "Well, write a grant." It's a big difference. I 
was real impressed one time about 10 years ago when I went to Livermore and saw they had PDP-1 ls in 
general stores. You could just go write a requisition and one came down the line to you. That's what 
I like. I like the idea that these things are effectively -- in fact, in our lab, they're virtually disposable. 
We don't fix them. We just stick them in the corner and scavenge. It's not worth fixing them. They cost 
too much. Nobody knows how to fix them anyway. Crockett and Tubbs may need a Ferrari, but I think that 
scientists need a three-year-old Buick. Thank you. HABER: Well, I guess that shows hardware types don't 
have to be hard-nosed. I thank the speakers for a series of very interesting presentations. We have about 
half an hour left for questions from the audience: If you'd come up to the microphones, we'll take your 
questions. Q: I'd like to know whether you're going to be addressing image processing, besides just drawing 
polygons -- HABER: Excuse me. Would each questioner please state their name and their affiliation? Q: 
I'm Bill Herbert from the University of Wisconsin. HABER: Thank you. So who on the panel would like to 
take that, the question about addressing image processing? A:Lou Doctor: I think everybody is pretty 
much aware of how important handling image data and being able to process it is. I mean, I think companies 
like PIXAR have shown people that images don't have to be flat anymore. So I think that what you've got 
now is this merging together of architectures to be able to handle both pixel data and geometry. I think 
everybody is also anticipating 3-D voxel data sets for the next step. But I would be very surprised if 
any of the other three hardware panelists are not doing a lot of work in the imaging area, and image 
processing area as well. A:Frank Moss: rll just add to that. I would agree with Lou. I would say that 
one of the key problems is that image processing has been extremely expensive until now. In our approach, 
we're going to try and achieve that within the context of a graphics rendering system by exploring concepts 
like virtual pixel maps that give applications access to pixels in order to provide special-purpose image 
processing; but with the same idea of trying to avoid special- purpose hardware for that capability. 
It won't meet every need. There will still be special- purpose image systems. But, again, I think the 
trend will be toward trying to amortize the cost of hardware and systems. HABER: Question from over here? 
Q: Art Olsen. Molecular Biology, Scripps Clinic. You talked about the computational hardware. Hardware 
is the general title. I'd like to ask the panel in general about input devices which are certainly part 
of the scientific visualization process. In particular, the highly interactive part. And I'd like to 
know what your opinions are of both current input devices and what direction you see them going in the 
next year. Just as an opinion, I think we've taken several steps backwards in input devices. A mouse 
is about the worst thing that I can think of for interacting with my data. A:Lou Doctor: I'll take that 
one. We've actually been doing a lot of things, both with -- although the next worst thing is a knob 
box. But these knob boxes do give. you at least a few different variables to handle. Actually, in cur 
booth at the show, we had a six-axis torque ball. It's a product that came out of Germany; and it is 
essentially software plug-compatible with the knob box - which is sort of a neat feature -and it even 
can be daisy chained. So we'd like to be able to see an environment where scientists can plug together 
their input model and, potentially, even have a multitude of these devices in front of them to grab. 
The six-axis torque ball is a nice thing because it's very intuitive. We also saw the data glove and 
things, but we weren't running that in our booth. A:Tom DeFanti: I'd like to add that if you really want 
to be inspired, Art, I think you ought to spend some time in Myron Kruger's space in the art show. Myron 
has a thing that -- it's actually hard wired -- he's never even put it to PC boards -- that will track 
your finger. If you put up five fingers, it erases the screen. It's got a little critter with 150 rules. 
Myron has done this as an art piece. I think, with substantial support, like a little bit of money, he 
could actually turn this thing into something that people could use and program as a good interactive 
device. Of course, it involves having a video camera on yourself, but I think it would be kind of nice 
to have. A:Bob Haber: I also think that this is all going to become a lot more important in the scientific 
domain. Engineers, for a long time, have had to deal with very complex geometries and model building 
prior to simulation. From what I've seen in the pure science applications, they're either dealing with 
very simple domain geometries to begin with, or they can generate them from a relatively small amount 
of information. I think as time goes on, the scientists will also have to tackle much more difficult 
input problems. I think it's an important question. Okay, next? Q: Olin Lathrup, Cognivision. I thought 
it was very interesting to see the disparity between Lou's vision and the two gentlemen from the high- 
end workstation companies. One way of resolving that, which I didn't see any mention of at all, is to 
look at the relative prices of the various subsystems that go into a high-end workstation. The memory 
subsystem, the CPU, the mass storage and the graphics. I would think that the expense of the subsystems, 
the ones that are more likely to be shared in a back room than the cheaper ones that could probably go 
on people's desks...but I didn't hear any comments on that. I'd like to hear some comments on where you 
think the prices of the individual subsystems are going and which ones you think will dominate. A:Tom 
Jermoluk: I can answer for our particular workstation. Our vision is that the graphics subsystem is still 
the majority of our cost within the workstation because it's the focus of our effort. But that's sort 
of misleading because it depends on what entry point you get into any particular product line. As you 
add more and more processor onto the host side to tackle the tougher applications, the image processing 
applications and computational fluid dynamics and other things, then you'll start to see the compute 
side of the workstation go up in price relative to the graphics performance. So much of that is dependent 
on what the application user needs and where they get into the line. I think that's why it's important 
to have a broad range of product available to meet the breadth of those applications. A:Lou Doctor: The 
other thing that we've seen is that there are two kinds of people out there right now. There are ones 
that are going to buy computational servers, and there are ones that are not. The ones that are going 
to buy computational servers are generally looking at a mix of high-end and low-end workstations, as 
well as their computational server. Since they have to do their visualization on something today, they 
are buying high-end workstations to do it. The alternative approach that I was talking about, where you 
take video right off of your server, you essentially can spend the same amount of money you would have 
spent for several high-end workstations for shared access; and you get the server for free. Because for 
that same amount of money, everybody can have access over the network to the computational resource and 
some number of screens can be made available as a direct connect capability. If you were going to buy 
a server anyway, then it certainly makes a lot of sense to plug graphics screens on to it; and then you 
get the graphics for free. So one way or the other, I think there's a nice approach there by trying to 
connect multi-channel video to high-end servers. A:Frank Moss: Well, I'I1 answer the question a little 
differently than it was asked. It was asked as, what would be the relative cost of the components. But 
I think the fundamental question here is, what is the role of the client-server model, and what role 
machines such as Stellar's are playing in that model, versus Lou's vision. First of all, I think one 
of the things that we find, just to go right to the bottom line, is that a significant number of Stellar 
systems are sold as compute servers. One of the reasons for that is, again, if you're able to solve the 
problem of avoiding special-purpose hardware, then the price/performance for a graphics supercomputer 
system can be very, very effective for a compute application. In fact, in many cases, we find that our 
systems are competitive, or more than competitive, with similarly priced systems that provide compute 
only. So, indeed, I think an important trend will be to see these systems not as single-user systems, 
but as hubs of visualization environments that will provide connectivity to supercomputers. There will 
also be servers of various types to networks of PCs, lower cost PCs and workstations; which incidentally, 
in response to Tom's comments, are very much a part of the picture we didn't discuss today, but I think 
will be tied into a total environment. A:Bob Haber: I think it will be very interesting to see how these 
new supercomputing workstations really pan out in the marketplace. When you listen to the vendors, sometimes 
they call them workstations and sometimes they call them servers. If you look at the size of the boxes, 
they look more like servers, I think. So it will be interesting to see how that really pans out. A last 
point on this question is, I think you can't look just at the individual systems and their prices because 
in all this high- performance hardware world, the chief issue often turns out to be communications. So 
you have to worry about the communications issues between them. You can't just take the individual pieces. 
We'll go on to the next question over here. Q: Marcelli Wein, National Research Council of Canada. If 
I interpret, first of all, that this is a session on architecture strategies, then I could -- I want 
to talk about window systems, X Windows in particular, and observe that the architecture that you're 
describing -- and there is some commonality among the architectures suggested --that the mental model 
of the architecture that the X Windows people carry is at best irrelevant, and at worst, could be a significant 
setback to computing in the US. If you look at the architecture, at the relationships you visualize between 
the numeric server and graphics server, none of them fit the current model. So what's going to happen, 
do you think? A:Lou Doctor: I don't completely agree that it doesn't fit very well into the X Windows 
model, because I think the -- all along, at least in our view of things, the window system is the control 
structure and today's networks are more or less the command and control backbone of the system. For example, 
we've done a lot of work in implementing high-performance graphics and window systems combined, as have 
the other high-end super workstation vendors. In all cases, the window system and the graphics subsystem 
exist more or less side by side and peacefully coexist, meaning the window system tells the graphics 
system when it's doing something that it needs to know about. And I think the same thing will hold true 
with these multi-media systems, that you'll actually be able to run in the context of a Macintosh or 
X Windows environment pretty transparently. A:Bob Haber: Was the gist of the question about how the X 
Windows client-server model is distributed across the network? Was that what you were getting at? Yeah. 
I think that is a legitimate question. I've been giving some thought to distributed processing environments 
for this kind of work, and I haven't reached any firm conclusions, but one of the questions I'm beating 
on is really that kind of client-server split, at the X Windows or even at the PHIGS+ level, is that 
split across the network the right level of granularity for a well-designed software 18 architecture. 
Maybe the next panel will get into this. I think that it may end up that you're doing X Windows; but 
you're doing it on a single processor and that there is data transfer going on at a higher level between 
the modules in your software architecture. I think that's an interesting question. I'm not 100% sure 
what the answer is at this point. A:Frank Moss: I'd like to respond, if I may. Yeah, I'd like to echo 
what Bob just said. I don't think that the current X Windows model necessarily stipulates whether the 
systems are configured in client-server configurations over networks or within a single system. However, 
when you have both your compute piece of the process and your graphical, user interface process in the 
same system, you don't want to bottleneck the performance. Today, the X Windows system specifies a Berkeley-like 
4.2 remote procedure call mechanism between the client and the server. I can tell you that our experience 
has been that it's quite straightforward to replace that connection, without impacting application programs, 
by a shared memory kind of protocol between the client and the server, and you can get high performance. 
In the Stellar system, for example, the software is never a bottleneck to the hardware, even though we're 
performing through X Windows and PHIGS. So it need not be a bottleneck when you're in the same system. 
A:Tom Jermoluk: I guess we have a little bit of disagreement with that, in that our notion is that even 
a shared memory protocol between the graphics process and the server process when you're on a single 
combined workstation is limiting. Limiting in the way that you can do graphics, as well as in the performance. 
So what we think of as each X process on the system has access to the hardware environment and that it 
in fact can control all the input and output and the normal window manager functions of the system. So 
the key there is to provide a hardware architecture such that it's totally transparent to that particular 
client at the time the rest of the mechanical control that the server would have done. That model extends 
well across the network, so that you can actually open the window into the supercomputer, or the window 
into the PC or whatever other sort of system you have holding on. And I think that that will be the more 
interesting area for the 3-D people, at least, in the X and NeWS area to try and standardize, as how 
those models appear across the system. Q: My name is Mike Kaplan and I'm at Ardent Computer. I don't 
want to raise a lot of controversy or anything here, but then I seem more like throwing the baby into 
the pit of worms. My question is specifically for Lou Doctor. His vision of the future, where you have 
centralized computing, very expensive resources and then slightly less expensive, but pretty expensive 
resources which do all these specialized things, and then you have scientists who are -- and these resources 
are all controlled by some central governing authority of some sort, many governing authorities. And 
then you have users who are allocated these kind of things on their desks that tap into it. It sounds 
an awful lot like time sharing to me. And my question is simply, if time sharing was such a desirable 
thing, why did workstations catch on? A:Lou Doctor: The obvious difference to me between my model and 
time sharing is the fact that you can disconnect the network and have a perfectly usable stand-alone 
system in my model. Because the fact is that you're really only relying on the network, if y.ou will, 
or the remote resources for this peak computational throughput, the on-demand reqmrements. And the fact 
is that, within these organizations, I mean like it or not, go to the largest national laboratories and 
walk into every office and you see a Macintosh, and walk into the computer center and you see Crays. 
I realize that the counter-cultural view might be to distribute all of this processing power to every 
user and get rid of the centralized resource; but people that work at large companies and labs that are 
doing scientific visualization are using supercomputers to do them. I personally don't believe that a 
workstation is going to deliver the power of a supercomputer for the next 100 years. Q: My name is Rob 
Wolff from Apple Computer. A couple of comments, I guess. Let me be somewhat consonant with Lou's vision 
in the sense that there will always be a requirement to do something that's beyond the capabilities of 
your own $10,000 workstation. We've seen that for years. However, the power of one of the long-term objectives 
--and hopefully, this growth curve will increase --is to continually bring power to the people. All right, 
so we're old, right? Some of you guys weren't even around. That's the one last night, it wasn't even 
important, we used to talk about this. But it's obviously important to bring more and more power to the 
desktop, and we're seeing this. This is a requirement. If you look at the distribution of time that a 
scientist spends doing various activities, very little of it is spent doing batch processing with a Cray 
or using high-end computers. When the capability is needed, though, you should have it on demand without 
having to go and make some major effort to march off to a computing center or whatever. But let me take 
issue with something that DeFanti said with regard to the hackability of the MS/DOS environment. What 
I've seen over the last 10 years --fortunately, I grew up in a physics environment where we did theoretical 
physics and we didn't worry about which boards worked with which software and whether or not we could 
software hack the particular system and so on, and we spent a lot of our time doing physics. Although 
I strongly believe in an open architecture system where you can tap into the creativities that exist 
in the general industrial, commercial and university population; it's very important to ensure that the 
open architecture doesn't end up causing the scientist who just wants to do their science and doesn't 
want to blow a light bulb or a beaker or something like that. They don't want to spend their time hacking 
hardware and hacking software. If you then extend the concept of distributed computing to the academic 
domain and realize that the end result of research is teaching, you don't want to have your students 
hacking this either. You want to have your students learning. In a very simple environment they can take 
their computer, their personal visualization workstation, rather than PC, and put it on their desk and 
be able to do academic computing and tap into the larger resources and be able to learn in an environment 
without having to worry about whether or not their software and hardware will work together. A:Tom DeFanti: 
I'll answer that. It seems to me that you've failed to form that in the shape of a question. Next contestant, 
please. Q: I never form questions. You know that. A:Tom DeFanti: That's a good question. I don't like 
doing archaeological digs on computers either. I spend an awful lot of time figuring out what people 
really had in mind when they designed these boards, since they're not allowed to document it for ... 
Q: That's because you have MS/DOS, because you have an MS/DOS machine. ATom DeFanti: Right. Well, I spend 
a lot of time doing that. And I don't like that except for the fact that it does give us a way of getting 
there. And what people who come into my lab do is, they say, "Well, how do you do that?" And I hand them 
a stack of purchase orders and they go and Xerox them and send them in and get something that plugs together 
and works. I think that science itself is extremely idiosyncratic, and in fact I sometimes refer to it 
as the fractal nature of science. Scientists, by definition, are hanging out in those places like you 
get in a Mandelbrot set where you keep zooming in and it's still there. And I don't think that anybody 
can, except through extreme flexibility, attempt to address that market. If you give somebody an application 
that's already done, well the next week, they're going to want to do something different and of course 
you can't give them the source code because that's proprietary. So what are they going to do now? Unless 
they're the military, they can't get you to change your code. And that's a real problem. So I really 
think that scientists need - - many scientists, those who are on the edge, need to -- which is all of 
them, really -- need to be able to get at this stuff. The same argument people say about the programing 
rub. They say scientists shouldn't have to program. But I've been checking around here and Japan, unlike 
engineers who use packages, 90% of the people who work with supercomputers do their own programing. I 
don't know whether that means that's the only way to get there, or what. Q:Rob Wolff: Well, it's almost 
-- on my computer, that we're in a transitional phase. For example, we don't question the algorithms 
for sine, cosine, exponential and so on, that come with the math packages that we buy. And at some point 
in time -- I mean, I like sort of the mathemafica approach where I can get to the algorithms that are 
used for particular simulations or modeling ... equations or whatever. A:Bob Haber: I think we're drifting 
away into the software panel here. Q:Rob Wolff: Right ... A:Bob Haber: So maybe we should save this discussion 
for them. Q: David Mathews Morgan, University of Georgia. I agree with Tom DeFanti about the idea that 
if it's not recordable, then it's not really science. And I realize that NTSC video is Tom's way of recording. 
I'd like to get a feeling from the panel as to what they see all the various hard copy devices, where 
they fit in, especially in the university environment where you can't afford to have every person have 
his own, say, hardcopy film recorder or even a $7,000 color thermal recorder. I'd like to know where 
they see that fits in, especially in the university environment. A:Lou Doctor: Not to be in this visual 
server rut, but one of the advantages I think of a centralized resource --Mike Kaplan's probably cringing 
-- is the idea that people can share things like output devices and videotape resources. We've seen a 
lot of progress in the color hardcopier case and in the ability to make videotapes reasonably easy and 
cost effectively. But I think that there is also a need for being able to generate these large image 
and animation data bases and have those data bases be available on demand as well. So that if you did 
an animation last year, you might want to actually have it archived back and be able to play it back. 
Or likewise, if you want to have a videotape or film made, you need to have a resource you can tap on 
the network to have that done for you. So again, while it might seem like gloom and doom to think about 
a centralized resource that someone is actually managing and keeping running for you, I think the reality 
of the situation is that everyone is not going to have a videotape recorder and a hardcopy unit and a 
Dicomed in their office. They're going to have to rely on a resource providing that output capability 
for them. A:Tom DeFanti: Or they could bnng in their home little hand-held video recorder and do it that 
way. No, no. You can plug those things in and it puts out video output. Plug it in and make your videotape. 
A:Bob Haber: I'd make two comments. One is that I hope we're not stuck with NTSC forever because it's 
not such a great medium, and that we might well see digital methods of storing image data as the technology 
improves that is affordable and you could have in your office with cheap playback systems just like your 
stereo. I forgot what my other point is, so I'll go on to the next question. A:Tom DeFanti: Before you 
go on, though, I'd like to point out that one advantage of the approach of having a video server is that 
it's video. You just plug it in and record it. So that's a nice idea, too. Q: Michael Herman, Apricot 
in Canada. I have some background in seismic data processing and have thought about this video workstation, 
live video terminal before. My question to Lou, I guess, is how little computing power can we put in 
that local workstation if we are shipping video? I would think we would get away with even less power 
than a 286 might be required in something like that. A:Lou Doctor: The question was, how cheap can you 
make it if you're just going to have it be kind of a stand-alone sort of video node or something. I mean, 
I think I'll probably actually side with Rob on this one, that what we're calling a PC is getting more 
and more powerful, probably for good reason. I mean, there are still things that I do on my PC that take 
a long time that I'd like to see sped up. So that when you're at that level of cost performance, though, 
you're really riding the exact right place on the technology curve with microprocessor chips and reducing 
memory prices. Because I think what we're seeing, and what seems pretty self-evident, is that while the 
cost per MEGAFLOP and cost per MIP is going down; the cost of the memory system and the cost of a system 
that has high bandwidth is going up or staying the same. So that when people try to build these high-end 
workstations and have to design the memory systems and power and cooling and cache systems to feed these 
RISC CPUs, I think they have real problems getting the cost down. I think when you're designing in the 
under $10,000 class, there's just a lot of things you can do using PC technology to get a lot of performance 
in there, and excellent price/performance. I guess the only other comment I'd like to make is that comments 
about ... disappearing as a time sharing system may be inappropriate. Really, what we're talking about 
is lengthening that video cable that comes out of the back of the CPU and shortening that ethernet cable 
that connects the CPU back to the centr~il resources. So you could imagine essentially 100 PCs in a rack-mounted 
configuration distributing video out, and really, time sharing would be an inappropriate comment there. 
A:Tom Jermoluk: It's an interesting comment, but remember, there's a couple of challenges left to go 
there. When you want to talk about video, you're talking about lots of bandwidth and lots of memory. 
NTSC at 30 hertz is about 16 megabytes a second. So if you want to play a few seconds of animation, that's 
a lot of data bus bandwidth and memory that you'd better have in your system to accomplish that. So I 
don't think you'll see --you'll see a lot of interactivity going on there, you might be able to preview 
single scenes across the network after waiting for a couple of seconds, but you're not going to be dealing 
with any actual preview of animation or motion. So there's some barriers there yet before you get down 
into taking live video or things like that down into the lower cost workstations. So I agree a little 
bit with Lou in the server side of it. That's sort of how we envision that same part of the strategy 
and that you'll have the genlockable capability and the video recorder capability and the RGB printer 
capability hooked to some node shared among a lot of people. And our challenge is to make that as easy 
to use and transparent as possible. A:Lou Doctor: I have a device at home that receives 100 channels 
of five video all in real time. It wasn't really that expensive. So I think that what you have to do 
is, you have to kind of use a tittle bit of imagination here and what I'll call a leap of faith to put 
two technologies that are both very well established and moving very quickly, together. I think that 
while digital data might be perfectly appropriate for shipping data files around, this device that I 
have at home has tremendous technology in it for receiving live video and being able to display it for 
me. I think that that's where you have to put these two critical technologies together. I disagree that 
you want to move live video through the memory system of the device all the time. It's totally inappropriate 
to move animation rate pictures through the memory system of a server or potentially even through the 
memory system of the workstation. What you want to do is simply have those live video pictures appear 
on the screen. So I think we can simplify the problem by taking advantage of another multi-billion dollar 
industry for commercial television technology. A:Tom DeFanti: Dan Sandin likes to point out to people 
that Peoria, has more bandwidth in its cable TV stations than the NSFnet is proposing. A:Bob Haber: Yeah. 
I think this will have to be the last question. Q: Steven Hunt from Imperial College, London. I'd like 
to pick up on something that Thomas DeFanti said. If we're not taking notes, we're not doing science. 
We're also not doing science if we can't publish. That may be a bad thing, but we have to publish. I 
have in mind problems to do with libraries, archiving, referencing. Something has been said about video 
but have to say that I'm from a country where we don't use NTSC. We have a better system. And if you 
change from NTSC to something better in the future, you're going to have the problems we have now. All 
your archive will now be inaccessible to you. How are we going to publish -- if it needs a scientific 
supercomputer to display the data and to understand it, how do we publish the results? A:Tom DeFanti: 
There are digital devices that go quite nicely between NTSC and PAL and they are routinely used and they're 
getting cheaper and cheaper. So I don't think that's a serious problem. In many places in Europe, it's 
really straightforward to find triple standard systems. Of course, the French think they have a better 
system, too. So I don't find that to be a problem. NTSC video isn't great. The preferred method that 
we've been recording lately is on BetaCam, which allows you to record essentially an RGB through a simple 
transcoder, and that's very nice. Because when you play the stuff back on RGB video projectors -- and 
most of them are these days -- or monitors -- most of which take in RGB -- the stuff looks pretty much 
as good as it did originally. So that's a mechanism that I'm currently recommending, so you don't get 
the NTSC zippers and the other kinds of problems that better systems have tried to replace. Q: The problem 
is a bit broader than that. I can't publish a video. The journals don't even accept stereo, usually. 
A:Tom Defanti: Well, that is a historical problem that has to do with people who say they can't allow 
people to publish on video. We even publish them in the SIGGRAPH Video Review and have something like 
35 hours of stuff. And although it doesn't count as a joumal, that largely has to do with that fact that 
ACM doesn't allow SIGs to have journals. And people in other fields, in medicine, are starting to publish 
refereed journals, like in surgery, using video. That argument made a lot of sense 10 years, 15 years 
ago when people didn't have video cassette recorders in their home, but people have them now. So the 
distribution and playback devices are out there. If your library doesn't have one, I think you ought 
to get on their case. A:Lou Doctor: And lastly, I would just say two things. Any solution to this sort 
of visual networking problem has got to be an international solution. So it's pretty clear that we need 
contributions from Europe and Japan and some ability to work internationally. So I don't think -- when 
I say using commercial television technology, you should not necessarily read, "NTSC, poor picture quality, 
US only." You should read, "Large market. A lot of R&#38;D investment. A huge installed base. And the 
ability to leverage very large quantities of product." So I think that you've got to solve all of those 
problems, picture quality and international distribution and the ability to use these networks worldwide. 
Those are all solvable problems. There are some extremely bright engineers that I think can solve that. 
A:Bob Haber: I think for the scientists, our biggest challenge is more of a political one. Right now, 
it's still hard to convince a lot of the conference organizers -other than SIGGRAPH, which is entirely 
visual. You go to a typical scientific conference, you've really got topound the table just to get a 
VCR. So we've got a big battle ahead of us, but we should fight it. I think at this point we'll close 
the panel. And I thank the questioners for their good questions and the panel.    Thomas DeFant ~ 
  = MIPS, MFLOPS, MPOLYS not the tx)t~m line Unique architectural featurm required to max (R~) ^nat~tous 
tm, ftvvm adwm~ required o Frank Moss
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402253</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>36</pages>
		<display_no>11</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Software directions for scientific visualization]]></title>
		<page_from>1</page_from>
		<page_to>36</page_to>
		<doi_number>10.1145/1402242.1402253</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402253</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101799</person_id>
				<author_profile_id><![CDATA[81332489209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bancroft]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA/Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101800</person_id>
				<author_profile_id><![CDATA[81365592525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Roy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wavefront Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101801</person_id>
				<author_profile_id><![CDATA[81337490396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ardent Computer]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101802</person_id>
				<author_profile_id><![CDATA[81318490408]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Al]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lopez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apollo Computer]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101803</person_id>
				<author_profile_id><![CDATA[81100078209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Alvy]]></first_name>
				<middle_name><![CDATA[Ray]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SOFTWARE DIRECTIONS FOR SCIENTIFIC VISUALIZATION Chair: Gordon Bancroft, NASA/Ames Research Center Panelists: 
Roy Hall, Wavefront Technologies Mike Kaplan, Ardent Computer A1 Lopez, Apollo Computer Alvy Ray Smith, 
Pixar SOFTWARE DIRECTIONS FOR SCIENTIFIC VISUALIZATION Introduction, Panel Chair: GORDON BANCROFT (slide 
#1) Let's get started. The title of the panel is "Software Directions for Scientific Visualization." 
First, to get us all thinking along the same lines, here's my "Webster's Dictionary" definition of scientific 
visualization. (slide #2) What I've done here is define what I feel to be some immediate and current 
issues surrounding software directions in scientific visualization. Let's begin with standards. Obviously, 
visualization standards are important, especially with regards to data formats. Software standards are 
also an ongoing issue, and perhaps we'll hear some discussion about this later. This industry is changing 
so quickly that efforts to standardize have not been able to keep pace. There have also been problems 
with standards in scientific visualization areas with regards to extensiblity. The next issue is one 
of "modeling the environment versus modeling the model." The follow- up slides show some examples of 
what we do at NASA/Ames Research Center. And I think you'll agree that what we're after is the environment 
surrounding the model. This has an impact on the next point: scientists are not after photorealism as 
an end goal. A scientist is interested in representing data. Maybe if there were no resource penalties 
for using, say, ray tracing we would use it. But, after all, in a photorealistic world you can't see 
airflow, or pressure, or density.., and that is precisely what we are trying to model at Ames. The problems 
of visualizing data are entirely different than the problems involved in visualizing "photorealistically." 
Here is a more practical example of what I am talking about: I showed a Gauraud-shaded computational 
model of a space shuttle to one of the scientists. He'd been used to looking at wire frame or flat shaded 
models and said, "What happened to my grid?" To a mathematician or a scientist, it's an issue of representing 
scientifically derived information. Shading that is basic to photorealism may actually detract from visualizing 
scientific data. Other issues we really have to start looking at are 3-D techniques and stereo. We don't 
have a good way to interact in 3-D. We've been doing some work with stereo, which goes along with 3-D 
techniques, and found that to be a very effective way of modeling fluid dynamics. (slide #3) This next 
slide is what I consider current/future issues. I think these are things that we need to look at after 
we solve some of the problems that I presented on the last slide. Volumetric visualization is a relatively 
new buzz word in the field of computer graphics. think the other panel members will be talking about 
different ways to do this. Basically, the question is, Do you use fast geometric processors to draw arbitrary 
planes in a volume of data, or do you do that with a frame buffer and some other method where you move 
the scene around and move orthographic planes through that? And that's coupled with the next question, 
Are we going to draw things in immediate mode? That is, we're not going to have display list memory anymore? 
Are we going to have to have software that just draws stuff on the fly because that's the fastest way 
to do it? And that's related to the earlier panel and their discussions about the directions of hardware. 
Distributed graphics processing and distributed processing is a very important issue. Coming up with 
ways to be able to drive scientific analysis from workstations using distributed graphics is something 
that we have to start looking at seriously; we're doing that at NASA/Ames Research Center. In the earlier 
panel, before this one, the subject of video output and input came up. What I'd really like to bring 
up here is the issue that, since we have a digital workstation and we have digital video and these formats 
are defined, a software solution exists to complete this path. We need to be able to get digital video 
from the workstations. (Perhaps the widely accepted D2 digital video format from Sony and Ampex) And 
it doesn't have to be stored. Tom Jermaluk, from Silicon Graphics, was talking about having to store 
NTSC in memory, but it might be enough to just produce it. Then it could be stored on outboard equipment, 
like an Abekas A62 digital video disk/frame store, for later editing and viewing. And then finally, just 
to make a mess out of everything, once we figure out how to do steady data, we've got to figure out how 
to do unsteady data. Unsteady simulations experiment in some 4th dimensional domain, perhaps time. Consider 
an aircraft that is being simulated at a given airspeed, angle of attack, etc. If you take that airplane 
and move it (increase the angle of attack) or change the geometry (move a flap), then we are calculating 
an unsteady phenomenon. The problems surrounding data management and scientific visualization of these 
results is tremendous. I'm just going to run through some slides here that are examples of scientific 
visualization at the NAS (Numerical Aerodynamic Simulation) facility.at NASA/Ames Research Center. (slide 
#4) Here is the shuttle with one side rendered using computational results, and the other side rendered 
with the wind tunnel results. The quantity being displayed here is total pressure on the surface of the 
shuttle with it's SRB's (solid rocket boosters) and external tank attached. I'd like to be able to shade 
this model, and I don't want to do that in RGB space. I want to do that in color index space, or have 
a user definable range of colors. I don't know of a single workstation available today that will do this 
in real time. (slide #5) Here is an example of a rotor stator analysis. We call this hot streak where 
we inject hot gases into a turbine and watch the way they mix downstream. (slide #6) Here's a jet in 
ground effect with particle traces. Particle tracing is a method that we use a lot. (slide #7) Here's 
an F-16 with particle traces again. (slide #8) And here's a rotor stator again, 3-D, with pressure mapped 
onto the blades. Okay, that's the end of my introduction. I'd like to introduce our first speaker who 
is A1 Lopez from Apollo Computer. A1. AL LOPEZ: Good morning. I'm going to be talking about network computing 
and scientific visualization. Network computing is a relatively new term under the umbrella of distributed 
processing. Generally, when we think distributed processing, we think about data communication protocols, 
file access and file transfer protocols. Network computing, on the other hand, tries to address the problem 
of getting the processing out to the data or to the computing resource itself. I'm not an expert on scientific 
visualization. However, I've attended some conferences and found the recurring theme in many of the papers 
which I've read that indicate that one of the problems people are dealing with in this area is that they're 
flooded or actually inundated with torrents of data. There's data just about everywhere on the network 
at many different locations. The problems dealing with this data are: How do you process it? Where do 
you process it? Do you move the data around the network? Or do you try to position it strategically next 
to the computing resource that's going to be processing the data? Then, How do you get the computing 
or that part of the application that is going to operate on that data out to that resource? Here's a 
very simplified model of the scientific visualization environment. You generate raw data through either 
simulation or acquisition, analyze the data, generate more data, and process more data. And,using visualization 
techniques, you look at either the raw data or the processed data through imaging technology or graphics. 
That can also generate more data, like geometry data which then gets rendered to generate image data. 
One of the effects that scientific visualization has had is to provide a higher bandwidth of information 
to the scientists. This bandwidth in turn gives the scientist greater insight on analyzing the data. 
And that insight produces intuition. What a person wants to do with that intuition is sort of follow 
their gut feel. And as a result, they want to control or steer the components of the application, like 
the acquisition, the analysis, and the visualization, and thereby, you have a desire for more interaction. 
Now I moved this slide ahead earlier, but basically, by adding steering into that model, we've completed 
the model and now have the ability to control the visualization analysis and simulation. However, there 
is a very key point that needs to be made here. This is no longer a batch process. You can't run a simulation 
and then some months later run some analysis on it that takes a significant amount of time and then spend 
a couple of weeks generating visuals to further analyze that data through visualization. So basically, 
what we've done by introducing steering into this model is brought these pieces in what needs to be a 
very interactive application. The ways that this application needs to become interactive also generates 
a demand for higher bandwidth communications and also for greater compute speed. Now there are some solutions 
to this. You can take all of these components, put them into one application and run it on an individual's 
personal Cray class computer. Well obviously, there's not a big market, or at least we haven't found 
one for multi-million dollar single user workstations. And in fact, the opposite is quite true. What 
we're finding is a requirement for low end systems that have an easy-to-use interface. Let's look at 
the environment where scientific visualization applications run . It's a networking environment made 
up of local and wide area networks and all types of heterogeneous computers, from supercomputers down 
through mainframes, mini, super workstations and PCs. How can we make an application that's running in 
this environment interactive? Well, one approach is the brute force approach. You just add more computing 
power and add more communications bandwidth. However, one of the problems with this approach is that 
it is in some ways self-defeating in that the same power that you're adding to analyzing and looking 
at the data is also being added to generating more data. And what you've done is basically scaled the 
problem up and created an evenlarger problem. Another part of the solution is to take an architectural 
approach to making that environment interactive through placement allocation and distribution. You want 
to place the data in strategic locations and then move the computing resources out to where the data 
is and where the computing resource is. You want to allocate resources so that the environment is load 
balanced enough to provide almost dedicated resources to the user. In effect, what you want to do is 
give an individual user multiple computers at their disposal. And you want to do this through a method 
of distribution that really harnesses the compute power that's in the network. Let's look at a taxonomy 
of the scientific visualization application. The role of these components, which we've already discussed, 
all of which generate the data that we've discussed, what you'd like to do is take that application and 
move it into that environment and distribute the pieces around the network. You'd like to put the data 
simulation, perhaps, on a supercomputer class machine, acquisition on a mainframe, and some of the visualization 
or any of the algorithms that are highly parallelizable out on a network running on multiple processors 
simultaneously. However, there are two key criteria that need to be considered in doing this. The first 
one is, you want to do this in a way that is transparent to the appfication and to the application developer. 
You don't want to add the burden of dealing with the network and all of the communications protocols 
and data access protocols to the job that the application developer already has to do. You want to provide 
a model that works the same locally as remotely. The other thing that you need to provide is application 
immunity to changes in the network. If you change the placements of data on the network or the allocation 
of resources on the network, you don't want to go back and change the distributed computing application 
in order to adjust to those changes. You really want it to be immune to those sort of changes. Now network 
computing solves both of these problems, transparency and immunity, in the following ways. First of all, 
it's based on a remote procedure called paradigm. Here the calls that are being made to remote procedures 
have stubs that are provided by the environment which marshal the parameters into packets that get transferred 
over the network where they get decomposed or unmarshalled on the remote process and turned into a call 
frame to the subroutine. The subroutine doesn't know if it was called locally or remotely. So therefore, 
you have the transparency both on the client's end and also on the server end. That in and of itself, 
however, is not enough because you don't want people to have to write these stubs and have to deal with 
the remote procedure call mechanism So another component of the environment has to be a description language, 
a network definition language, which we call NIDL in NCS, which basically does that for you. It's a declarative 
language external to the application. You run it through a compiler. It generates the stubs. The stubs 
get bound into your application. And all of the handling of those packets and marshaling and unmarshalling 
of procedure arguments are handled by the remote procedure call environment or the network computing 
environment. This doesn't imply that these things now have to run remotely. In fact, this works just 
as well locally as it does remotely. And that's another important criteria. The other aspect that's very 
useful and provides a lot of flexibility is taking an object oriented approach. You'd like to be able 
to think in terms of operations performed on different types of objects. A good example of this is, if 
you have a program like "Is" in UNIX which lists a directory listing of files, you'd like to be able 
to use that same code and reuse it without modification if what you're asking for is the contents of 
a directory, or if you're asking for the contents of a print server queue. In that case, each of those 
are of a different type and all you need is to be bound or given a handle to different methods which 
know how to extract the contents from either directories or print server queues. And this is done in 
NCS through a client server broker model. The broker basically has the information about where the servers 
are, what operations they support, and where they live and how to give you the handle, give the client 
the handle to access the right set of methods to perform that operation on that type of object. Now we've 
talked about things which give you transparency and things which make it automatic for that transparency 
to exist,like the NIDL compiler, and the flexibility of object orientation. But we haven't talked yet 
about immunity. The immunity needs to be provided by having a data base of information in the environment 
which keeps track of where the servers live once they've registered themselves, and if they get moved, 
where they've been moved to. So you have the new location. And then the client needs to access that information 
through an interface which we call the location broker interface. Now this again gets handled by the 
codes that get generated for the stubs and the application developer does not have to worry about that. 
As we move resources around the network, the location broker data base is updated and the client, the 
next time it runs, simply asks for that capability, that set of operations, on a particular type of object. 
The broker resolves that request and returns back a handle with which you do the bind to the appropriate 
set of methods in the distributed environment. Now we've talked about distributing applications. I want 
to talk just for a couple of minutes about distributing graphics and distributed graphics systems. I 
think that distributed graphics is really in its very early stages right now. We have models of distributed 
graphics built around client server protocols like X-windows, NeWS and PEX. We also have some systems 
that do distributed graphics, in fact some of which use NCS, like the intelligent light package and our 
own ray tracer Apollo, as well as many packages that are doing distributed graphics in their own environment, 
like the Pixar RenderMan which runs on multiple processors. However, we want to generalize this and make 
it more available in a general purpose environment. And I believe the direction that that's going to 
go is towards and object oriented graphics system. An object oriented approach would provide data abstraction. 
You'd like to, say, rotate yourself 30 degrees and not care about whether it's a bit map image that's 
being rotated or some geometry that's being rotated, in which case you'd use different algorithms. The 
application shouldn't have to worry about that. Just issue that command and have the object oriented 
system worry about making the proper binding. You'd also like to be able to provide for user extensibility, 
allow them to add things to the environment which are not currently in the environment and not have to 
wait for a release from the vendor to support that new feature. So an object oriented environment provides 
for that. You'd like to be able to provide distributed access to shared objects that can exist anywhere 
on the network and also be able to invoke objects that themselves are distributed computing objects. 
If you say to an object, "Ray trace yourself," have the method for that object to use the distributed 
computing environment to spawn out copies of the ray tracer on multiple nodes and have that happening 
in parallel. And in order to provide all of this, you really need to consider an object programing model, 
and in fact, there is an object prograrning model that has been developed at Ardent Computer in the Dore 
system, which Mike Kaplan will be speaking about next. I think that in terms of future trends and directions, 
an object oriented graphics system coupled with an object oriented network computing system, such as 
NCS, can really provide the framework for distributed object oriented graphics. In summary --and I apologize 
that I don't have a summary slide -- I just want to stress the following few points. First of all, it's 
not just enough to just move the data faster and process the data faster, not when you've introduced 
steering into the equation. You really need to support interactive distributed applications. Those application 
need to be done in a fashion that makes the network transparent to them, and they need to be immune to 
changes in the network. And the next generation of graphics environments I think will be both object 
oriented and distributed and of great value in the scientific visualization environment. Thank you very 
much. Now, Mike Kaplan from Ardent Computer. MIKE KAPLAN: Hi. I just want to note that you can tell that 
we're the software panel because none of us are wearing suits. Everyone in the hardware panel was wearing 
a suit. I only have five minutes because I'm going to show you a five-minute videotape that shows some 
of the work that has been done, using the tools that I'm going to talk about. So this is going to be 
very fast and I'm going to ask you to visualize the slides that I might have brought. About 2-1/2 years 
ago when I joined Ardent, we had a really big problem in terms of tools. We were building a machine which 
combined mini-supercomputer performance and highly interactive graphics and it was a very closely coupled 
system and it was not a thin wire system where the graphics data and the computational data were separated. 
And we looked around at what was available and decided that none of the models that were currently in 
use, in terms of standards or commercial software, really satisfied that kind of abstraction that we 
wanted to present to the user. So we built a system called Dore, which is an attempt to be a stab at 
this new model of object oriented and distributed graphics. And we believe it's just a first step, but 
at least a framework in which more progress can be made. Primarily, the purpose of Dore is to provide 
an application interface to three dimensional graphics. It is not a rendering system. It is not a modeling 
system. It is not primarily a graphics data base, and it's not a floor wax either. It's really an application 
interface to 3-D graphics. And what that means is that the person who talks to it should not have to 
understanding rendering, where the processing is going on, what decomposition of geometry is being performed 
at a lower level, and what attribute assignment mechanisms are being used. In other words, it should 
be a framework on which manufacturers and laboratories and end users can all build and provide their 
users, or themselves, with a consistent user interface to a lot of functionality. To do this, we felt 
we had to move the level of abstraction much higher than it had been. So the key elements of the system 
are, number one, it's object oriented in a very macro level. In other words, it's still efficient. For 
people who know a lot about Smalltalk and so on, in the past, object orientated at the level of Smalltalk 
tends to be inefficient for computation. So we decided to do it at a macro level so that primitives and 
attributes and views and devices and so on are objects. But it's programed in standard C and can be portable 
to a number of systems. Number two, it's extensible. The user can provide his own attributes and primitives 
and other rendering methods besides the ones that ship would be initial system can be plugged in by other 
vendors, by other computer companies or by the end user. We don't believe that we know how to solve all 
the visualization problems and know what all the primitives are going to be needed in the next five years 
or what kind of visualization rendenng methods are going to be used. But we do feel that there ought 
to be a way that these things can be put together so that you don't have to wait three years for a standards 
committee to decide on it or two years for a group of manufacturers to decide on it or one year for your 
favorite vendor, under a lot of pressure from you, to provide it to you. So it's extensible. It does 
not contain the idea of a renderer. Dore does not say how you render something. It says you hook in different 
renderers and they do the best job that they can for the given devices that they are making output for. 
So for example, we ship a dynamic renderer which drives our real time graphics hardware. It could drive 
other people's as well. And we ship a high quality renderer that makes nice stills. RenderMan interface 
could easily be hooked in to generate -- to do rendenng from the same data base for people who want to 
achieve the effects that RenderMan is specifically set up to handle. Another aspect of this is that we 
do not enforce a display list or a drawing style in the data base. The data base contains a scene description, 
relatively abstract, and based on the kind of primitives that the user uses in his own modeling and/or 
analysis system, and furthermore, it can be closely coupled over a tight interface. So at this time, 
let me start the tape. But before we start the tape, everyone can pull out their entry level Ardent system 
and view it directly if they would care to. The first example is of a simple constraint base model. Here's 
the Ardent national flag waving in the wind. All of these animations were made in the last month and 
a half by people who had never used our system or Dore before. Here's the Siggraph national flag. But 
the analysis application or the constraint base model here and the graphics are very closely coupled. 
The objects are interchanged and the analysis or the data for the meshes and so on are in the forms that 
the computational program wanted it in. Here are some examples of a simulated wind tunnel where we are 
animating particle traces which are computed over the surface of a space shuttle using a panel method, 
and while we're doing this, we can also be rotating the object so that you can get a feel for what's 
happening on all sides of it. The next piece, I believe, shows a 2-D -- okay, this is it moving through 
the particle fields. This is the next piece that shows a helicopter rotor wash which is analysis of the 
vorticity of the air coming off of the blue helicopter blades, and we're also viewing that from a variety 
of angles as multiple frames of data are played back. Next is a 2-D solution where you see it converging 
on a solution of flow over a cylinder, and in the next case, over an airplane wing. And as it converges, 
you'll notice the Bernoulli principle being illustrated. Less pressure on the top. In this case, we take 
the photographic data of Yosemite Valley, map it onto a three dimensional height field, actually make 
a mesh of triangles, color triangles out of that, and then rotate them in real time. Now this kind of 
object is an object which can be added to Dore if it isn't already there. So if you have a particular 
kind of primitive that you use in your analysis all the time, then you can add it to the system and get 
all the benefits of rendering. This is a solution of a wave equation in an L-shaped membrane, and I believe 
that the color has something to do with the data associated with it. I really don't understand what that 
one's about. It's done by a mathematician. The next one is a projection of four dimensional space into 
three dimensional space where the color, position and movement are all indicative of elements of that 
four dimensional space as we're moving through it. I think the most important thing about these animations 
is not that they're pretty or that they show a particular area of science, but that they were all done 
in a month and a half by people who didn't know how to use the system. And on one machine. So there was 
a fair amount of resource contention as well. Now this is a presentation, the first time ever, of a rational 
bi-cubic data spline surface patch work done by Brian Barsky at the University of California. These are 
like nerves, except that unlike nerves, you can have local control of bias and tension. And we're using 
this software to visualize what happens when you change each of the four different controllers, the weighting 
of the nodes, the node spacing, the bias and the tension parameters. The bias and the tension are additional 
parameters provided by the data formulation. So you can see, especially at the end, I believe, the surface 
being pulled tighter and tighter over the control points, but without moving the control points. These 
are the first seven vibrational modes of the tuning fork with finite element analysis of the stresses 
of the fork. I don't think you'd really want to stress a fork this much. It's a rubber tuning fork. This 
is what's called a locking cubes method of doing volume visualization in which we're showing the explosion 
of gas in a rotary engine compartment. So the yellow stuff there is the gas and it's blown up. It's now 
spreading itself through the inside of the compartment. In the next case, we see the actual mechanism 
of the Wankel rotary engine moving in response to the explosion of the gas. And finally -- not finally, 
but almost finally -- in our oil reservoir showing the oil being pumped to the surface by ejecting gas 
from the surface. Now this is a temperature simulation of the Atlantic Ocean. The first one is over a 
six-month period and the next one is over a two-year period. Someone has remarked that if you were extremely 
patient, you could surf on the temperature gradient on the Atlantic Ocean. Here's the two-year simulation. 
This is called geologic time surfing. And finally, this is something one of our students did. He came 
in to take a class on Dore. He did this on the half a day that he was there for the class. And we put 
it in just because we were also impressed that somebody in half a day of training could take their data 
and do this. And here are the credits. 8 Okay. So my time is up, so I'd like to introduce Alvy. I asked 
him what he'd like me to say about him and he just said, "Here's Alvy." ALVY RAY SMITH: Well, I first 
of all want to say that I very much appreciate A1 Lopez's overview. I thought it was quite detailed and 
instructive, to me at least. Several times he mentioned the difference between geometric data and imaging 
data, and that's the main point, the main distinction that I'd like to draw today. This is probably an 
audience that understands the distinction completely, but I have been surprised over the last couple 
of months to find scientists unaware of the distinction. So please allow me to hammer in the distinction 
today as my contribution to this panel on visualization. The idea is very simple. The key word is geometry. 
There are two ways to make pictures, or at least two ways, two general ways to make pictures with computers. 
And one of them is based on geometry. We're all familiar with it. We've done it for years. We take an 
object or a scene or something and describe it in terms of abstract geometrical concepts, such as line, 
polygon, patch, curve, nurb, cylinder, sphere, hyperpatch, etc. A geometric concept that at some point 
gets scan converted into a set of pixels so that it can be looked at. The other one is where you take 
samples of some continuum and place those samples directly onto a display, bypassing any notion of geometry, 
other than the fact the display is square or things like that, or rectangular. It seems obvious, but 
I've had people walk up and say, "You mean you don't have to use geometry to put a picture up on a computer 
screen?" It's like some basic intuition from lay people, at least, non-Siggraphians that computers mean 
mathematics, and mathematics means geometry if you're talking about pictures. So the geometry must be 
involved somewhere. You've got to be kidding. You have to go through geometry somewhere. So, my main 
message is no, there are a lot of pictures. Perhaps in scientific visualization, 90% of the data does 
not come from geometric sources, geometrically described objects in the first place. And I'd like to 
show several examples of those, visualizations of non-geometrically derived data today. Now the first 
thing I'm going to do is show some slides to kind of calibrate you on the videotape that I'll show shortly 
thereafter. This is a puff a smoke, a real puff of smoke that was acquired with a laser beam. The light 
rays bounce off the laser beam and were captured by a CCD array. So this is an example of a volume filling 
set of sample points, data points, representing particle densities in three space, and it's also so complex 
that it would be very difficult to abstract this object with geometry. This is turbulent water flow. 
You'll see all these things move in a moment. This is the main one I wanted to put up on the screen because 
the videotape is not very good. This is the universe. It's a cubed universe about --I don't know, it 
says in the videotape a million light years across, or something like that. A stress analysis throughout 
the volume occupied by a CAD/CAM designed mechanical part. A mathematical function in three space. I 
think this is a Mandelbrot set in quaternian space or something like that. And then some medical images 
that are all derived from CAT scan data. Again, I emphasize that there is no geometry in any of the pictures 
you've just seen, except for one. I'll let you figure out which one it is. That's terrain rendering, 
we'll go back. 9 Okay, good. Would you show the videotape now please? And be ready to kill the sound 
when we get to the Pixar marketing hype. Please play the video. There's one other that I tacked on after 
the Pixar logo that I'd you to see. While we're waiting for that, let me point out the one -- I mentioned 
that one of those data sets was derived from geometrical data, and that was the stress analysis CAD/CAM 
part, was derived from a finite element mesh. The stresses were computed at the nose of a finite element 
mesh and then scan converted into a volume filling set of samples which we then display using our volume 
imaging techniques. This is Freddy. This is the first complete CAT scan reconstruction of a human being, 
showing several different visualization techniques. And I emphasize again that there never was a geometric 
step at all involved in all but one of these visualizations. Okay. Would you stop the tape, please? That's 
all I had to say. The next speaker is Roy Hall, thank you, from Wavefront Technologies. Oh, he's no longer 
from Wavefront Technologies. He's from Cornell University, as of last week, I believe. ROY HALL: Good 
morning. I'd like to talk a little bit about visualization techniques and how they relate to some of 
the commercial software packages that might be available. I think we're seeing kind of an abundance of 
entertainment animation packages that are available. And entertainment animation has done a lot to drive 
commercial availability of packages that let you and I generate images at home on our own little PCs 
or whatever. In the realm of scientific visualization, certainly the problem we're dealing with is substantially 
different than in entertainment. But we are basically trying to communicate information. And I think 
that some of the tools that are available are useful for communicating scientific information also. But 
there are certainly a number of hard questions that need to be asked and need to be solved for those 
tools to become much better.Basically, how do you use the software that's available now to solve the 
visualization problems that you have? Because most of the software in commercial animation is not oriented 
towards scientific visualization. Another question is, what's going to be the future of this software? 
Which way are manufacturers going to orient themselves? And are they going to do products that are going 
to better suit the visualization needs? And probably the most important question is, how do I turn graphics 
and visualization tools into tools that help me look at the real problem I have to solve, rather than 
making graphics itself a more imposing problem than the thing I'm simulating? And this is probably one 
of our greatest challenges fight now. So basically, we want to talk about how we use software that has 
been designed to make pictures like this. And I should have a second carousel there. Well, we'll do it 
without the second carousel. Basically, how do I use software that was generated to make images like 
this? How do I use that software to communicate things like this, which is a scientific visualization 
of some molecular work? Okay, let's look at what we get normally in an animation package. We have a system 
that has some scripting tools that allow us to plan motion over whatever time period. We have some modeling 
tools that let us build geometries and things like that. We have a rendering tool that lets us make an 
image. We have some compositing tools, or really, some post-processing tools that let us do some amount 
of image processing, or combining that image that we've generated with text or other types of things. 
And finally, we have some sort of image output and some tools that let us get it down onto videotape. 
Basically, when we go into the scientific realm and we're talking about visualizing data that's created 
by simulations or by direct measurement, the data that's created becomes a scripting tool and in some 
cases becomes the model. And at some point we need to take this data, this scripting information, or 
this model, and pump it into a renderer. So we're really concerned about what the interfaces look like 
to do that. One of the things that is very much the word of the day is volumetric rendenng. And certainly, 
we've heard a lot of that at this conference. But I think that we find that things like this are perhaps 
inadequate to address all of the needs that we have in visualization. This is a simple volumetric rendenng 
of some storm data that was done at the University of Illinois supercomputer center, and I believe the 
data was generated by Richard Matson. Whoops, wrong data. This was done by Bob Wilhemson. And basically, 
we're looking at a storm vortex. We're looking at the density of rain within that vortex. It's a 3-D 
array of points. It's uni-vadate data that changes over time, and basically an implicit surface is generated 
and normal rendenng techniques are used. These are cosmic strings colliding in the universe. And I have 
no idea what that's all about, but it's by Richard Matson. And again, it's a technique where we're really 
using uni-vadate data and we're watching it vary over time. And we're getting some insight into what's 
going on. But I think the problems have become a bit more interesting in scientific visualization, are 
the cases where our processes are very complex and we have more to look at than a single value that varies 
over the domain space of our solution. In this case, we're looking at a test boring and we're checking 
some of the seismic things that have happened here. I guess they're measunng density and some kind of 
fracture structure. And we basically, the depth is reflected by position along this test bonng. We have 
an orientation as we go around the test bonng. Color tells us something about density, and the depth 
of the surface tells us something else about the structure. So we're adding more elements to the visualization 
... once you get into the multi-variate cases. And I think what you find is that some of the commercial 
animation systems provide a test bed or a prototyping tool to look at new solutions. This is an example 
of a neuron chain that's f'mng and this was done by Mike Miscagni, and basically, we're watching the 
propagation of things as they go around the ring. It's a nng of neurons. This doesn't look anything like 
neurons. I don't have any idea what neurons look like. But basically, what we're trying to understand 
are patterns that have to do with the interactions of these neurons, and the devices that we use can 
take any number of forms. And I think that this is certainly a problem to be treated in research, is 
how to identify the types of forms that let you convey a great deal of information and look at interactions 
between a wide variety of variables at one time. This is some more work that was done at the supercomputing 
center at the University of Illinois by Bob Haber. And here, we're looking at crack propagation. And 
I believe the height field is the axial stress in the material and the color has something to do with 
the sheer stress. As a last example, this is again something that was done in the University of Illinois, 
and it's a very difficult simulation in which an awful lot of information is displayed. Here we're going 
to look at an injection molding problem and we're going to look at the flow of plastic into a mold. The 
simulation was done by Rich Ellison and Ray Eizak and really, the visualization technique, or the consultant 
for visualization was Donna Cox. And here we have the collaboration of scientists to actually create 
the simulation, and an artist to deal with the problem of visualizing and displaying information. We 
have an iconic representation here that describes pla~ic as it's moving into the mold. There is a directional 
component by the onentation of these arrows. The length of the arrows describe the velocity of the material 
as it comes in. There is temperature information that's displayed vertically on each of the little icons. 
There is pressure information that's displayed on the base of the icon grid as plastic moves in. And 
we can see the entire process of this mold filling and we can look at the interrelationships between 
very many different elements, and as it fills, we can see what's going on. I have a video of this that 
I'll show in just a second,~but to sum up, we have multi-variate applications, and as manufacturers of 
software, certainly we would like to provide good software for you to do visualization. But the problem 
is not one that we understand, and in most cases, it's not one that you as scientists understand how 
to deal with this incredible wealth of data that's available and display it in a way that lets you see 
all the relationships you want to see at one time. So I think what we can supply for you now is probably 
only a prototyping tool for those types of future applications. And we're waiting for you to show us 
how to build better tools that are going to let you look at this multi-variate data in meaningful ways. 
So if we could roll the tape real quickly. This is the tape that was generated showing this simulation 
in progress, and we see the plastic moving into the mold. We see the fill patterns, the pressures, temperatures. 
And this was very useful for mold design and better understanding the things that are going on. But it's 
difficult to do this looking at single slices, looking at only a pressure slice, or only a temperature 
slice, or only a velocity slice through the data. And we really need to see all the variables at one 
time. And eventually, we see plastic fill the entire mold. As it fills the mold, eventually, the temperature 
starts to drop, the pressure goes up, and we've reached the final stage for the part. We'll just take 
one more look at it and if we could cut the videotape after this. And I'd like to thank all the people 
at the University of Illinois that supplied me with slides so that I could show this marvelous work. 
Thank you very much. GORDON BANCROFT: At this point, we'd like to open things up to the audience. I had 
some canned questions, but I think we're a little bit short on time, so we'll just go ahead with audience 
questions. Before we do that, they've asked us to tell you they don't want anybody up on the stage after 
the panel, so they can reset it for the next one. So if there are more questions for any of the panel 
members,we'll be in Room 303 immediately following this panel. So, I guess we'll at this point start 
with -- how about the person in the back? Q: Ingrid Carlbom, independent. I would like to comment on 
what Alvy Ray Smith said. You said that now that we have volume data and we don't have to worry about 
geometry, and I would like to disagree with that. It's true that if you only want a visualized data, 
your original data, yes, that's true. But in the medical field, for example, what you then want to do 
is ask questions about this data, what is the size of the tumor, what is the volume of the ... space 
in order to determine when to replace a hip, for example. In the seismic area, you would like to ask 
questions like what is the volume of oil in place or what is the contact area and so on. And now you 
have to worry about geometry and those geometry problems are probably every bit as difficult as those 
we are familiar with. A: ALVY RAY SMITH: Yes. In fact, you're absolutely correct. I failed to make one 
of my points, which was that there are lots of problems for which geometry is absolutely the correct 
way to visualize a solution or possible solution or to provide an intuition about the problem. No question 
about it. I just want to point out that there are numerous large data sets for which the finding of a 
geometrical abstraction in the data actually adds artifact to the data rather than clarifying what the 
data has to offer. For those, then the direct imaging approach is the correct one. They're not in competition 
with each other. One of them is more applicable than the other. BANCROFT: Could we have everyone give 
their name and affiliation before their question, please? Over here. Q: Hi. Gene Miya from the Ames Research 
Center. I don't want to make too gross a generalization, but it seems that the panel in large part is 
telling us what we're going to be getting as opposed to asking us largely what we're going to be needing. 
And this is one of those tail wagging the dogs and solutions looking for problems kind of things. I suspect 
that the reason why this is, is because as people learn about computing, they tend to lose empirical 
ways of doing things. Alvy gave a good case, which was figuring out -- putting a laser through smoke 
puffs. But we have to learn how to take empirical data in various fields. Rob Cook, for instance, is 
a good friend, points out that he has to occasionally go outside and look at scenes, rather than spend 
too much time with the keyboard. And to reiterate what this woman behind is talking about, I think one 
of the most important software directions that we can go to for visualization -- it's not enough, it's 
a good start -- is that we need to be able to take numbers, literally, because that's how you make predictions, 
to how you come up with mathematical models and like, you have to take lengths. You look at an image, 
that's fine. But when you talk ... God, that's nice, but how long is this distance? Or a harder one is, 
what's this angle ... maybe in a bond? What kinds of surface areas are we talking about? What kinds of 
volumes? And I don't see that in any of these tapes, slides, or what other -- because in large part, 
our users don't know. I mean, I don't know myself what I'm going to get sometimes when I take a look 
at data. So you need also to provide, in addition to numbers, you need to provide points of reference. 
One of the things that makes some of Jim Blinn's videos so important is, he has the graph paper in the 
background, you even get some idea of what kinds of distances and ratios and proportions that are involved 
in some of these things, the scale. It's a new field, admittedly. But there have been some computer companies 
who have been at this for a little while, like itty-bitty machines, as an example. I guess what you guys 
are going have to do is, you're going to have to bnng in more scientists to talk to you guys -- I know 
that Mike Mailin (?) was at Pixar for a while -- and get a feel for why they're doing, as opposed to 
just telling us, well, here's the next generation of workstations. Mike. A: MIKE KAPLAN: I think we are 
trying to address that, and we're not trying to address that by knowing what you want to do, because 
I don't know what you want to do. What we're trying to do is to make a framework that gives you a lot 
of functionality and utility and let you add things to it that are appropriate for your application. 
And the idea is to give you that framework that has a lot of functionality that you wouldn't want to 
have to program, like making real time displays work and handling attribute management and hierarchy 
and data base, and then say, look, here's an easy way to plug in, for example, an object that when it's 
displaying itself, 13 also displays an axis and a label, but only when it's between a certain set of 
angles, and when it's beyond those angles, it shows itself. There are a lot of ideas that people have 
come up with for adding things like that, and we're trying to produce the extensibility in a standard 
way, rather than have everyone have to rewrite all the utilities themselves. So I think we're trying 
to address that. A: AL LOPEZ: I'd like to agree with what Mike just said. In fact, I think that one of 
the values of an object orientated approach is that ability to dynamically extend the environment through 
user-written capabilities. I'd like to add also that scientific visualization is really, for workstation 
manufacturers in particular, an emerging market. If you look back about eight years ago when workstations 
first appeared on the scene, ECAD type of applications were running on mainframes and minicomputers and 
when the workstation brought together networking, graphics and microprocessor compute power, memory prices 
dropped, it enabled those applications to move onto a single user network platform, the workstation. 
Now ECAD applications over the last eight years have moved down through the mid range of workstations 
to the low end and actually fallen off the end and are now on PCs as well. And it was over that period 
of time that we learned more about the requirements of ECAD system developers and they learned better 
also how to make those applications work in a workstation environment. The same thing is happening to 
MCAD when the graphics on the workstation became capable of supporting MCAD applications. And now we 
see scientific visualization applications that have been typically running on a variety of systems, including 
supercomputer architectures because of enabling technologies that have been brought into the workstation 
by systems like the Apollo VN10000, the Ardent machine, the Stellar machine, and even the Silicon Graphics 
system. Those applications have now been enabled to move onto a workstation platform. We tend to focus 
a lot on where most of our volume is and as the volume increases, as those applications move down through 
our line, we'll be working closer with scientific visualization applications with scientists together, 
learning how to make those applications work and what requirements they really have. So I expect a lot 
more attention to becoming in this area. BANCROFT: Okay, let's see if we can keep the questions as short 
as possible so everybody can get a chance. Over here. Q: I'm Rob Wolff from Apple, and this is going 
to be a real question. I'm working on this, I'm taking an English course and all that stuff. The question 
is directed towards Mike Kaplan and Roy Hall. As a scientist, I really appreciate the functionality that 
exists with software like Wavefront and Dore. But I don't want to have to take a training class to use 
it. User interface is critically important to scientists in order for them to be able to do science without 
having to go somewhere, take a training class, without having to know C or know UNIX in order to do these 
things. And without the user interface, you essentially don't have the access to all these marvelous 
tools. So the question is, what -- I'm learning. The question is, what work and what efforts do you see 
in your respective areas are being directed towards this? That's a good question. A: ROY HALL: Well, 
I think that we all have to assume that at some point we're going to build up our literacy in different 
areas of graphics, the same way we had to built up our literacy in different areas of programing and 
perhaps had to learn to use C, or at least we had to learn to use somebody who knew how to use C, and 
perhaps we need to find somebody who knows how to use graphics to reduce the problem. I think a great 
deal of efforts that we see, and certainly in the evolution of product that we've seen on the show floor 
over the last two or three years, that the user interface is becoming increasingly easy to deal with. 
Certainly in commercial animation systems, the needs of the commercial animator are substantially different 
than the needs of somebody doing scientific visualization. And as the people who are doing special effects 
and those things need more and more effects, the menuing seems to get increasingly more complicated. 
And perhaps we need to branch off a simpler version for the applications and visualization. And I think 
that we're going to see those in the future. I think we'll see much easier user interfaces, things going 
to a more Mac-like form that 99% of us are familiar with and we can look forward to that in the next 
few years. A: MIKE KAPLAN: Well, I think it's a very good question. And if I can ever get out of the 
trenches of making this stuff run on everything, that's my next project, or one I'm looking at very seriously. 
I believe I have some tools I can work with now that let me make systems that are quite flexible. And 
I believe other people using similar tools -- not necessarily ours -- will have those applications coming, 
and not just applications, but systems you can interact with and easily manipulate and visually manipulate 
data from analysis and steer analysis and do things with 3-D and 2-D on the fly. But first, there's an 
enabling set of tools, and that's what, at least we've been working, both hardware and software. I think 
that other languages, like Smalltalk, may play a part in that area and I certainly think that the user 
interface management systems will play a part in that as well, a networked user interface and computation. 
BANCROFT: Over here. Q: My question is addressed to the panel as a whole. And all the panel members seeming 
to be addressing the visualization-- BANCROFT: Could you give us your name and -- Q: Oh, I'm sorry. 
I'm Gerald Edgar of Boeing in Seattle. And all the panel members addressed visualization of a single 
model. That is, computational fluid dynamics, thermodynamics of plastic injection and the like. And none 
of you have addressed being able to integrate and visualize information from various sources such as 
if you're able to visualize CFD with the stresses imposed by whatever source on whatever object you're 
modeling.  In the ViSC (Visualization in Scientific Computing) report that came out last year, there 
were two lines saying, "Graphics people aren't very good at data base, so we're not going to address 
that." Is there any future directions in this regard that you're aware of to address this problem? A: 
ALVY RAY SMITH: Let me start. I think there's a great deal of interest in integrating data sources, the 
geometric with image sources together into a single visualization. But we're just starting and most people 
don't know how to do it yet. In fact, what the visualization report from NSF was all about, was to encourage 
research in exactly in these issues. One small step towards an example solution is in the medical diagnostic 
imaging domain where several different modalities, as they are called, of data, are to be mixed together. 
For example, CAT scanners and magnetic resonance scanners are two very common types of volume imaging 
devices in medicine and there are places, research places, that are combining images from those two modalities 
into a single visualization very profitably. Profitably in the sense that the referring physician gets 
a better sense of how to deal with his patient than he had with the two modalities separate. A: MIKE 
KAPLAN: I've seen examples, beginning examples of this in the mechanical CAD field. Initially, people 
just did geometry and they did analysis. They showed the analysis as a table of numbers, and then they 
got the idea of attaching the numbers to the geometry with colors, and so on, and motion, and hence introduced 
at least two modalities into that. But I've also seen people in the seismic and mechanical CAD field 
going further than that and showing two or three different effects on parts and so on at the same time. 
And I would hope that that sort of work will go a lot further, now that the technologies are improved, 
to make that real time and to make it easier to produce. BANCROFT: Over here. Q: Art O1son, Molecular 
Biology, Squibbs Clinic. The panel didn't really make very much distinction between what I would call 
working graphics and presentation graphics, in terms of the amount of effort that goes into the scientist's 
working day -- most of it is working graphics -- and also the amount of effort going into the software 
development. There has been very little emphasis, for instance, on the question of tradeoff between rendering 
and dynamic interactivity, and also very little emphasis --you don't see vectors anymore because everybody 
is working with surfaces. If you ask a scientist what he wants, rendering versus interactivity, at least 
in my field, interacfivity is much more desirable. I would like to phrase it as a question. What type 
of work is going on in trying to get whatever types of rendering, whether it be vectors and dots and 
not just surfaces or volume rendering per se, like we saw, just to go faster ... A: None of us want to 
go faster, do we? A: I think that a lot of the developments in speedups that go into the hardware architecture 
is to accelerate even surfaces. Also accelerate the rendering of vectors and the transforms that are 
necessary to redisplay even vector type or wire frame type of renderings or dot type of renderings. 
 In terms of software, I think that one concept that relates to this is the notion of demotion, which 
is that you'd like to be able to keep the rendering separated really from the data base or the geometry 
that you're using with which to render and be able to get a display of that information at varying levels 
of complexity, everything from the wire frame up through a ray trace image or a shaded rendering. And 
I think that the interactivity as a function of how much hardware or the speed or the performance of 
the system that you run, what you'd like to be able to do is really turn a dial on your machine and pick 
the point at which it's interactive enough to satisfy your needs. And if you're on a machine that can 
do that with Gouraud shaded images, and if that meets the needs of your applications, then you can. If 
not, you turn the dial down and demote. And that concept really has to be built into the software systems 
that do the rendering BANCROFT: Okay, over here. Q: O11ie Jones, Apollo Computer. You know, scientists 
are suspicious people and there have been a lot of suspicious things about some of the images we've seen 
today, and I want to address the question to Roy Hall and maybe even to the chair as to what you think 
in the future the responsibility of the provider of visualization software will be to provide things 
like calibration bars to show what each color means and calibration to show whether you're distorting 
vertically and this sort of thing, and to what extent that's going to be left to the individual scientist, 
which of course is the ultimate responsibility. A: ROY HALL: I think that we can address that issue pretty 
simply. And it relates to some comments that were made earlier about the tail wagging the dog, that very 
often, you as scientists are saddled with software that was built for the entertainment industry. Well, 
the entertainment industry really pioneered a lot of the research that generated those techniques and 
that imagery, and many of the software companies, many of the people who are doing that kind of work 
and building that software, came from that environment. And I think it's perhaps naive of you to expect 
us to know what your needs are until you've done sufficient research to describe to us what it is you 
really need. So I think until you know what it is you need, we sure as hell aren't going to know what 
it is. So it's really up to you to do your basic research to describe to us what the solution to your 
problem is, or to somehow get more involved with the manufacturers so that you can indeed tell us that 
we're solving your problem or not solving your problem. Q: How are people going to do that? How do you 
suggest that scientists do that? A: Well, I think we're seeing a great deal of it now at the University 
of Illinois. We're working with them pretty closely and they're demonstrating to us a lot of techniques 
that we never would have thought of because the problems we deal with were very much different. And again, 
as I mentioned earlier, what we can provide fight now is a prototyping tool. And the tools are there 
for you to make pictures that show those things and then to say, you know, we need to do that automatically, 
rather than to have to go through a lot of gyrations. Because in the end, it's our task to make visualization 
easy for you rather than to make it a new problem. But somewhere in between the solution of that task 
and our current state, somebody needs to do a lot of research to describe what the techniques are, because 
we don't know what they are. BANCROFT: Good question. Thanks. Over here. Q: I'm Bruce McCormick at Texas 
A&#38;M, one of the authors of the ViSC report. I have a feeling that the scientific visualization report 
should probably have been entitled scientific visualization and modeling. And looking at Alvy's real 
data coming into there where you really want to be able to bottle it after you've got it for so many 
applications, I get the feeling that the panel is selling what they know, but they're not at all addressing 
the real problem. What they don't know how to do is to do modeling off of that data. It's not computer 
vision. It's mostly volume imagery. I don't see any techniques down on the floor of the exhibit hall 
that show real modeling of real data. It's a very tough problem and I don't see any answers in any of 
the graphics firms that do it and I don't think they're even near it. And I'd like a little honestly 
about the fact that we've got a long, long way to do if we're going to be able to model visual data. 
It's a long way to go. Graphics is a lot of the answer and I agree entirely with the object oriented 
view, but I think you're selling what you know how to do, but you're not talking about the real problem. 
A: MIKE KAPLAN: Well, it's very difficult to talk about things that you don't know about, and when you 
do, people really get on your case. Q: Well, let's take some real cases, let's take the human body, take 
any of the CAT scan issue. A: If you really do know about it, tell us. Q: Well, we gave it to you in 
the visualization report. Take any of the human anatomy and try to do that. I mean, you can scan it in 
CT, in MRI, you can scan it histologically. You can use a laser microscope. You can do all these things 
to get volume data in. Try to model that stuff. A: What you want to do is, you want to create volume 
models yourself. You want to model them as a CAD system would model a set of parts. But you want to do 
them volumetrically. Is that my understanding? Q: Well, the data comes in to you volumetric, but you'd 
like to tear them apart, like they say in the old CAD system, you'd like to put retinas together that 
way. A: There was a system called a Phoenix data system, Phoenix Data Systems that made a machine that 
did something very much like that and let you model voxil data and that company has now gone out of business. 
Q: I talked with Maher just at the meeting here and has gone on in Paris, actually, since then. But that's 
irrelevant. What I'm hearing is, you know how to do graphics. You don't have a clue how to do the image 
analysis, and you're telling us that just a few more twists on the standard old graphics package and 
we're home free. And I just think that's hooey. A: Come on, Bruce. A: AL LOPEZ: I'd like to respond 
to that as well. I don't think that we can expect to be at a point where we have all the answers in the 
very beginning. And I really see scientific visualization, at least in terms of the workstation market, 
is a beginning in emerging technology.  Up until a year or so ago when we started developing supercomputer 
type capabilities into a workstation, we didn't even have the term scientific visualization on our slides 
as a market. The packages that are now being used in scientific visualization, like the Wavefront packages 
and so on, were regarded as packages for the entertainment industry up until only a couple of years ago. 
I basically think that there's a lot of work we've got to do. There's a lot we both have to learn and 
we've got to grow up together, and it's just going to take time, years, and a lot of discussion, a lot 
of applications, working on these systems and a lot of requirements being placed on these systems in 
order to make applications work before we get to the point where we have those answers. A: GORDON BANCROFT: 
I think we have to, at least in some sense, bite off as much as we can chew and solve problems that we 
can identify now and experiment with the solutions to those problems to go on and solve the bigger ones. 
And I agree that we've only just scratched the surface. BANCROFT: Over here? Q: My name is Bob Hendricks 
and I'm from the Johns Hopkins Applied Physics Laboratory. I work in video production at the laboratory 
and our group does video productions that generally serve as status reports to the sponsor for projects 
we're working on and so forth. And I need a 3-D graphics animation system that will create quality images 
that I can use in the video post- production environment. But at the same time, since we are a scientific 
laboratory, I'd like the system to be able to accept technically correct parameters instead of just the 
kind of things that video production people are used to. I'd like to ask directly the panel, going down 
the line and ask you what your opinion is on what the best software is that's available today to address 
both of those needs. A: Gordon Bancroft: I think you'll get four different answers there. A: I think 
the chair should answer that one. A: GORDON BANCROFT: Ah, gee. Don't make me do this. I think in my 
estimation, there are a variety of different animation systems and ones you can build yourself and ones 
you can buy completely put together for you. My gut feeling is that there's probably a certain one that's 
best for each application. I guess if that gets me off the hook, rll go onto the next question. Mike's 
going to tell you that it's Ardent. Just kidding. A: MIKE KAPLAN: Not at all. I think my answer would 
be, I don't know of one that's really great, that you can just pick up and use. But I would say that 
the animation systems that we're seeing for the entertainment industry or from that are probably not 
nearly as applicable as the CAD systems that have been developed over the years. In other words, which 
are not primarily into animation, which are primarily into design and manufacturing. Those systems tend 
to add animation capabilities later and they don't tend to be as fancy as the capabilities that the video 
animation people have done. But they're much more concerned about tolerance and materials and showing 
the real stuff.  So I would say that with the combination of photo realistic rendering or high quality 
real time rendering to these CAD systems and some of the new CAD vendors that are coming up, may provide 
more of an answer than just an animation system. BANCROFT: Over here. Q: ... Geo Systems. When we look 
at the scientific process, what do we do, as a scientist, myself being a physicist? We start measuring 
data. We have measurements. You're providing visualization, and we talk about visualization here, but 
that's only one very small part of the whole scientific process. The next thing we want to do is, we 
want to be able to extract numbers from those data which we are visualizing and then put them back into 
.models and perhaps revisualize them. What is being done about trying to actually grab measurements being 
provided in these computer data bases? If we don't address that issue, you're not addressing the scientist' 
needs. A: ALVY RAY SMITH: Well, in medical --I'll use medicine again as a place of quite active research 
today on extracting measurements from medical imagery. Just as you say, one of the most interesting things 
to, say, an oncologist, is the size of a tumor today on such a such a patient. So what they're doing 
is extracting volume data from their image data. I think that's going on right now in various research 
centers around the country. Q: Do you see a generalized set of tools coming up for that? A: I certainly 
do. I guess what I'm pointing out is that you're absolutely right. People want to extract measurements 
from their data, whether it be geometry based or image based. You can still extract measurements from 
it, and I expect to see package come along very shortly from the research that's being done currently. 
This stuff just started. BANCROFT: I think we have time for one more question. We've got to wrap it up 
here. Over here. Q: Charles ... from the University of Maryland. The computational data when you want 
to visualize, I think that naming the scientist is somewhat misleading. Usually, there are two scientists 
involve d. One who has the original problem, the other who has to solve it, and sometimes they called 
... mathematician. In this case, using finite elements, the model for geometric space is even in the 
finite element form. But you apply and if visually shown otherwise, it's misleading. And whatever that 
applied mathematician's tolerance is, it would be completely wrong visually if you model it differently. 
And I feel that nowhere was mentioned that finite element --it's very much connected to the finite element 
modeling when you might want to visualize it. The other thing, maybe a long-range one, many of the problems 
are not spatial geometry but parametric geometry. I can parametrize shapes geometry, and eventually it 
would lead, on a long-range optimal programming, it might be five years later, and in that sense it could 
be very commercially available, especially if one of the parameters could be a dollar sign. I would like 
to have some comments from the panel in this regard. A: Just to be honest, I missed your second question. 
But I think the first one had to do with, perhaps visualizations are misleading. Q: The scientists, in 
respect when you are considering, the chair mentioned that, what ... you visualize, the model or the 
modeler? So there is a mathematical model which you have to solve. Originally, it was not a mathematical 
model, but it was originating scientist wants to know. Once it gets a mathematical model and solve it, 
then an applied mathematician comes in, because the solution for that mathematical model is again an 
approximation. So your data is not fully accurate. You are getting for visualization. So there are two 
things which you can model here. Model something for the scientist or model how the solution phase goes 
and how accurately the solution goes in the scientific solution. Because finite elements are used. The 
finite element itself specifies, if for example, you represent surface, then -- A: We need a question 
here. Q: I wonder ff ever it was considered that one should contact a finite element analyst and look 
at the representations through him, not just by the originating scientist. A: Is the question, have we 
called in finite element experts? Q: Yes. A: Yes. Well, I showed one example that was exactly based on 
finite element analysis. But I'm not sure that's the answer to your question. Q: But I haven't heard 
that when used that type of element surface volume element, but the finite element defined for the solution. 
20 BANCROFT: I'm afraid we've run out of time here. We've just gone over the time limit. Thank you very 
much for coming. 21   AI Lopez      
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402254</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>48</pages>
		<display_no>12</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Designing effective pictures]]></title>
		<subtitle><![CDATA[is photographic realism the only answer?]]></subtitle>
		<page_from>1</page_from>
		<page_to>48</page_to>
		<doi_number>10.1145/1402242.1402254</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402254</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101804</person_id>
				<author_profile_id><![CDATA[81452617082]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jock]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mackinlay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101805</person_id>
				<author_profile_id><![CDATA[81100427474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101806</person_id>
				<author_profile_id><![CDATA[81100294395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blinn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jet Propulsion Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101807</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101808</person_id>
				<author_profile_id><![CDATA[81100098812]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Margaret]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Hagen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boston University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eglon Hendrik van der Neer (Dutch, 1634--1703), <i>Dutch Interior</i>, 1700. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[John Singleton Copley (American, 1738--1815), <i>Mercy Otis</i>, Detail, 1766. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[William Harnett (American, 1848--1892), <i>Old Models</i>, 1892. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[John Frederick Peto (1854--1941), <i>Poor Man's Store</i>, 1885. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Jan van Huysum (Dutch, 1682--1749), <i>Vase of Flowers</i>, 1730. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA["Club." Cartoon illustrating affordances.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA["No Exit." Cartoon illustrating affordances.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[James J. Gibson, <i>The Senses Considered As Perceptual Systems</i>. Boston: Houghton Mifflin, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[August Toulmouche (French, 1821--1888), <i>Madame de la Poule</i>, 1865. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Claude Monet (French, 1840--1926), <i>Rouen Cathedral in Full Sunlight</i>, 1894. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Claude Monet (French, 1840--1926), <i>Rouen Cathedral: Tour d'Albane, Early Morning</i>. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Caricatures from Perkins, D. N. and Hagen, M. A. Convention, context and caricature. In M. A. Hagen (ed.), <i>The Perception of Pictures, Volume I</i>. New York: Academic Press, 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Rembrandt van Rijn (Dutch, 1606--1696), <i>Artist in Studio</i>. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ryan, T. A. &amp; Schwartz, C. B. Speed of perception as a function of mode of representation. <i>American Journal of Psychology</i>, 1956, 69, p. 60--69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Photograph taken with a 50 mm lens at 18 inches from the near foot, M. A. Hagen.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. M. W. Turner (English, 1775--1851), <i>The Slave Ship</i>, ca. 1840. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[El Greco (Greek (Spain), 1541--1614), <i>Fray Hortensio Felix Paravicino</i>, 1609. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Lilian Westcott Hale (American, 1880--1963), <i>The Old Ring Box</i>, 1907. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[John Singer Sargent (American, 1856--1925), <i>Mrs. Fiske Warren And Her Daughter Rachel</i>, Detail, 1903. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[John Singer Sargent (American, 1856--1925), <i>Mrs. Fiske Warren And Her Daughter, Rachel</i>, 1903. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[John Singleton Copley (American, 1738--1815), <i>Mercy Otis</i>, Detail, 1766. Courtesy, Museum of Fine Arts, Boston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 DESIGNING EFFECTIVE PICTURES: IS PHOTOGRAPHIC REALISM THE ONLY ANSWER? Co-Chair: Jock Mackinlay Co-Chair: 
Steven Feiner Panelists: Jim Blinn, Jet Propulsion Laboratory Donald P. Greenberg, Cornell University 
Margaret A. Hagen, Boston University ACM SIGGRAPH '88 Panel Transcript Designing Effective Pictures: 
Is Photographic Realism the Only Answer? Co-chairs: Steven Feiner and Jock Mackinlay Panelists: Jim 
Blinn, Donald Greenberg, and Margaret Hagen JOCK MACKINLAY: Good afternoon. Steve Feiner and I would 
like to welcome you to the panel "Designing Effective Pictures: Is Photographic Realism the Only Answer?", 
which has only one reasonable response: "Of course not!" The purpose of this panel is to place photorealism 
in context with other approaches for making effective pictures. STEVE FEINER: All of us believe that 
photorealism is, by definition, the technique of choice when it's important to present the exact appearance 
of the world. Indeed, the quest for photographic realism offers us a compelling Turing Test by which 
we can measure our progress, namely, "Does the generated picture look like a photograph of what it's 
supposed to be?" This question has provided a goal that many of us strive for, and, happily, a goal that 
can be easily validated by most viewers. After all, we're all expert judges of photorealism. Photorealism 
is not, however, the be-all and end-all of computer graphics. If it were the final goal, then advances 
in rendering quality and speed, such as those that were presented at Wednesday's fighting model session, 
would ultimately put computer graphics researchers out of business. To put things in perspective, let's 
not forget that our field is computer graphics, not computer photographics. As such, we should follow 
the lead of graphic designers who have studied and practiced the design of effective visual presentations. 
Graphic designers use photographs when they're effective, but often use more abstract pictures. If the 
purpose of a picture is to communicate specific information to its viewers, then a photograph may be 
markedly less effective than a more stylized image. Consider effects such as refractive transparency, 
shadows, detailed object geometry, and precise perspective. These same elements that make for technically 
impressive images can also produce cluttered, hard to understand pictures. Sometimes this may be the 
fault of an inexperienced picture designer, as witness the experiences of any beginning photographer. 
But there are many cases in which the environment itself may have more complexity than need be shown. 
JOCK MACKINLAY: The crux here is that pictures are invariably created for a purpose. Sometimes photographic 
realism is needed. Other times, more abstract designs are more effective. For example, architects and 
engineers often use orthographic projection rather than perspective. They can measure distances directly 
from an orthographic picture without worrying about perspective distortion. Furthermore, effective pictures 
often contain both photoreafistic and stylized aspects. For example, Blinn's animation of the Voyager 
flyby of Saturn was based on a physical model that was altered to make the pictures easier to understand. 
The brightness of the Voyager was increased so that the satellite would not appear in silhouette and 
the planet nngs were thickened to make them more visible. STEVE FEINER: We've assembled a panel that 
will be able to address a number of the important issues associated with the topic of photorealism and 
its place in the context of computer graphics. Each of our panelists, us included, will give an eight- 
minute presentation and then we'll open the floor to discussion. JOCK MACKINLAY: We'll start with Dr. 
Jim Blinn from Cal Tech's Jet Propulsion Lab. Jim has extensive experience in making images that span 
a wide range of styles for educational and scientific applications. Everyone here has either seen, or 
should see, his NASA planetary flybys and cartoon explanations of simple and not-so-simple physics and 
mathematics. Jim will set the stage with examples that illustrate his design principles, showing where 
he has mixed styles to better communicate information. STEVE FEINER: Our second speaker will be Professor 
Don Greenberg. Don is Professor of Computer Graphics and Director of the Program of Computer Graphics 
at Cornell, not to mention the winner of last year's Steven A. Coons Award from SIGGRAPH. Don's group 
has been at the forefront of computer graphics for some twenty years now, most recently doing pioneering 
work on physically based modeling and radiosity approaches. He'll discuss how close we are to making 
photographic realism practical and when it is the most appropriate style. JOCK MACKINLAY: Our third speaker 
is Psychology Professor Margaret Hagen from Boston University, who is an expert on human perception. 
She has analyzed techniques that artists have successfully used to make 2D images of 3D scenes. She will 
discuss some ways their pictures intentionally differ from photographs. STEVE FEINER: Our fourth speaker, 
my Co-Chair, is Dr. Jock Mackinlay from Xerox PARC, who has focused on the problem of automatically designing 
abstract 2D presentations of scientific or statistical data. Jock will describe the graphical techniques 
that are used in such images and how they can be combined to generate complex presentations. JOCK MACKINLAY: 
Our fifth and final speaker is Professor Steve Feiner from Columbia University, whose work includes the 
automatic design of pictures that show how to perform actions in a 3D world. He will discuss a model 
for picture generation that takes into account both content and style. Now, let's welcome Jim Blinn and 
hope he stays in the USA. JIM BLINN: Basically, as they said, most people think that photographic realism 
is not the only answer. The question is where you draw the line - where you should be photographically 
realistic and where you should not. I'm going to go through a little bit of the history of my involvement 
in image making and show various decisions that I've had to make. This is representative of the first 
sort of picture that I was involved in -the simplest sort of computer graphics that there is, line drawings. 
These were done many years ago and they're obviously not particularly realistic, but they were really 
nifty at the time. Some slight modifications of that get you to hidden line elimination which looks a 
bit more realistic. I find when these things are animated they sometimes look more interesting than photorealistic 
pictures because you're used to seeing realistic pictures move around in 3D. You do it every day. But, 
if you see something like this it's obviously a drawing and it moves around on the screen. It kind of 
looks more magical in some sense. The next thing is putting surfaces on shapes and highlights and so 
forth, which again looks more realistic and then you sort of mangle the surface a little bit, putting 
bumps on it. This is the sort of thing that most people wouldn't expect a computer to be able to do, 
but obviously if you show this to somebody they'd say something was wrong. There's no background and 
the perspective is a little bit wrong and that sort of thing, but we're progressing more and more towards 
realism. This is one of the first attempts at doing a Voyager spacecraft, just for reference. I didn't 
expect this to look terrific, but it was just sort of an existence proof that I could put three different 
sorts of shapes on the screen. They were a reference of where I was when I started at JPL. This is more 
like what I'm able to do today, which is a much more realistic image. One of the reasons this looks realistic 
is that people aren't familiar with this sort of subject, so I can do things that I know aren't realistic 
and nobody can tell. This is a real photograph of Saturn, "real" quote unquote. This was sent back by 
a Voyager spacecraft and reconstructed on earth. There are some artifacts introduced by that. Take a 
look at the texture on the surface and the rings and so forth. This is a computer synthesized view of 
roughly the same view. The sun is coming from a slightly different direction and so forth, and they look 
pretty similar to each other, but comparing them side by side you can see some obvious differences that 
are both intentional and unintentional. For example, you can see stars in the background here. There 
are no stars in the background on the real picture because, relatively speaking, the stars are a lot 
dimmer than Saturn and you wouldn't really see them in a photograph like this. But in order to make it 
look more like space, you have to put stars in. You can see that the texture on the surface of Saturn 
matches up pretty well. That's because we took the database from this or a similar picture to generate 
the texture map of Saturn. The details are not accurate, but you can't see them at this resolution. Also, 
the lighting on the rings is different. Again, the intensity of things in all of the space images I've 
done has been adjusted to make most everything be in the intensity range of the screen, because a lot 
of things would be way too dim or way too bright. We're trying to communicate what shape these things 
are, rather than do photometric experiments on them. You also notice that the night side and the shadow 
on the rings is not completely black in this picture, where it obviously is in the real one. Again, this 
is artistic enhancement, so that you can get an idea of the shape even if there's no fight shining on 
it. So the more realistic image is not completely realistic. Here is some vague ray tracing just to say 
I had done it. Getting into other subjects, here's some molecular animation. You can't make real pictures 
of molecules because of various quantum mechanical concerns and so forth, so you represent them in some 
abstract way. This is putting a surface around them and lighting it as though it were a plastic, shiny 
model. The more I learn about quantum mechanics, the more I realize all that stuff is total rubbish. 
But I can't think of anything to replace it with, other than similar things, maybe fuzzier looking. Again, 
you're getting into something for which there is no real image or no real shape, and so photorealism 
in that case by definition doesn't make any sense. Here's an image from The Mechanical Universe. There 
are two styles employed in making this image. A lot of things we had were demonstrations of some physical 
thing happening and an annotation. Any time there's a physical shape that's moving or has inertia or 
weight, it will have a more realistic simulation. But the annotation is done with a whole different program 
that makes constant-width lines in different colors, constant-shaded polygons - that sort of thing. So 
you can tell the difference between the real thing and the annotation. This is a very useful distinction. 
This spinning top is precessing around and shaded with polygons, but it's not the world's most realistic 
top either, for that matter. In particular, the highlight is kind of squarish looking and, for those 
of you who are familiar with how Gouraud shading works, that's what happens if you don't use a lot of 
polygons. I meant to do that, because when the top is spinning, if it's a perfect surface of revolution 
and perfectly smooth, you can't see it spinning - it's perfectly symmetrical. So I found that if I backed 
off some on the polygon count, the highlight sort of shimmers and tipples a little bit, and it gives 
it a sense of motion when it's spinning around that you wouldn't get if it was too realistic. Here are 
some other Mechanical Universe slides. This is a representation of the electric field between a positive 
and negative charge with a surface enclosing them. The field lines in this case are drawn again with 
a constant-width line rather than making them be cylinders in space, so that when they went off into 
the distance they got small, or when you got close to them they turned into giant telephone poles or 
something. That would distract from the idea that the yellow lines are a pure mathematical abstraction. 
If you make the realistic things get bigger or smaller as you move closer to or farther away from them, 
that makes sense, but for the mathematical abstraction you don't want to do that. Here's the theory of 
relativity. This is a space-time diagram. Again, no realistic object. We did a sort of two-dimensional 
cartoon of it and we made it look flat and two-dimensional because when we extrude it in the time dimension 
we wanted to have only three dimensions to deal with. Again, mathematical abstractions, things for which 
there is no real thing. The extrusions are made out of transparent polygons, but the lighting and so 
forth doesn't make them look like they're really made of shiny plastic. It's just something that you 
can see, that there's a trail left behind on these things. Here's some of my realistic cartoon animation. 
Again, we've seen a lot of attempts at realistic human faces, which is an interesting research topic, 
but you can do really interesting cartoons by making them very cartoony. In fact, this is a subject which 
I think computer graphics could use a lot more work on, because you can give the impression just as well. 
Here are more mathematical abstractions, vectors in vector land. There are some realistic touches here, 
some fake shadows in order to give a sense of depth, that they're floating over the ground as it recedes 
in the distance. This is what we're working on now with Mathematica, the mathematics education tapes, 
and we're getting simpler and simpler and more graphic looking and it's basically just colored boxes. 
But that seems to be all that was necessary to get it across. I mean, when I first made these, I said, 
should it have a gradation of a color from the bottom to the top, to make it look a little niftier? Well, 
we're talking about two-dimensional geometry here. All the properties of the plane are the same everywhere 
on the plane, and I don't want to start giving the impression that the plane is different down here than 
it is up here, so a flat background seemed appropriate. There seems to be a progression here. I've gotten 
more and more realistic and now I'm getting more and more graphic. I thought I'd give you a prediction 
of the sort of images I'm planning on making in the nineties. <<blank slide>> So, where do you draw the 
line? I've come to the conclusion that making realistic images is just a phase you go through and after 
a while you get over it and get into something else. But, I also see this in the sense of economy. I 
look at this from the standpoint of an engineer. An engineer is defined as a person who can do for two 
dollars what any damn fool can do for five. Typically, the choice is, I found, fairly simple. If you 
have to deal with non-real objects, abstract objects, obviously realistic images are not so applicable. 
Although you can make them out of realistic models, it looks kind of weird. I've seen people do pictures 
of vectors floating around in space where they looked like solid, rigid things that are shiny and made 
out of plastic and so forth, and it doesn't make the vector look abstract enough. It makes it look like 
there's a plastic cylinder with an arrowhead on it there. To conclude: The only point of photographic 
realism is to enhance the readability of the image. If it doesn't contribute to making the image readable, 
you are probably better off using a cheaper and more graphic technique. Thank you. STEVE FEINER: The 
next speaker will be Professor Don Greenberg of Cornell. DON GREENBERG: I was speaking to Jim Blinn before 
and he told me that realistic image synthesis is a phase one goes through, and I told him that I hope 
when I mature I'll be able to grow up like him. I think I was picked to be the token proponent of photographic 
realism, but I agree with previous comments in that it's not the only answer. The basic question, as 
far as I see it, is, "Is the simulation necessary to represent reality, or is the simulation to provide 
information to the user?" Before we get into potential arguments later on in this session, I have to 
give my interpretation of the term "photographic realism." By this, I mean the most advanced rendering 
techniques that are available in our community, ones which include the global illumination algorithms, 
such as ray tracing and radiosity. They account for inter- reflections, color bleeding, shading, shadowing, 
and texturing, with the goal of making pictures look as close to photographs as possible. But of course 
we're not there yet. I really think we have twenty years to go. As Jim Kajiya once said, if we're really 
there, why do so many computer graphics pictures look so bad? Let me try and explain several situations 
in which photographic realism is very important. The first and obvious one is when realism is necessary 
to make an aesthetic evaluation. Consider automobile design. If we take a look at automobile design, 
there's a clay model upon which a very reflective material is usually laid. The designers then look at 
the model and study all of the light reflections. On the right, is a ray-traced slide by Eric Haines 
of a simulated car. Designers want to see more than just the geometry. They want to see the smoothness 
of the surface, they want to see the sleek, streamlined look, they want to look at the light reflections, 
and these are paramount. One looks at the third derivative continuity, which the eye can pick up, and 
all the multiple reflections. If you went out on the floor yesterday you must have seen dozens of booths 
with shiny ray-traced cars. In architecture we have the same problem. We're trying to evaluate the space 
in a room, which is affected by the shade and shadows as shown by this simple image made with the radiosity 
technique. One can ask the question, "Why is the ceiling always painted white?" or "How does skylighting 
affect the perception in a room?" It's extremely important to understand what the space is going to look 
like before it's designed. We can get more complex with some of these images, as shown by this radiosity 
slide, a view of a simulated museum done by Shenchang Eric Chen, Stuart Feldman, and Julie O'Brien, and 
which was on the cover of this year's conference proceedings. The second and not so obvious reason is 
when photographic realism is necessary to help the user accomplish his task. A long time ago, companies 
like Evans and Sutherland or the people who are in military surveillance recognized the need for textures, 
the depth cues which are necessary for pilot training and to provide the depth perception necessary for 
pilots to judge where they are. There's also a third reason, which we haven't confronted yet, and which 
arises with increased complexity. We're still seeing very, very simple models in our computer- generated 
images because we don't have the modeling tools necessary to measure or model the complexity that we 
want. But what happens when we want to look at a full Volkswagen engine or an entire car? Will the techniques 
which we're using now, which employ direct lighting alone, suffice? I contend no. We did a simple experiment 
with 100 cubes suspended in a box, which is lit from up above. The Gouraud-shaded image of a single light 
source on top of 100 cubes is very confusing, but the radiosity image on your right gives the correct 
impression. I contend that as we finally get modeling tools which will allow us to increase the complexity 
of the environments we evaluate, we will need the higher-quality rendering tools. A fourth reason is 
that it is extremely important to make sure that there are no artifacts, so that we don't have any potential 
misunderstandings. Perhaps I've shown these slides too much, but these are two slides of a sculpture 
by John Ferren in the Hirschorn Museum, which shows the radiosity approach. The artist basically took 
 some sticks, painted their back surfaces in diffusely reflecting colors and their front surfaces a 
diffuse white, and backlighted them, so that inter-reflection has forced the colored light to bounce 
off of the diffuse white front surfaces. The radiosity simulation as shown in this slide reveals the 
correct inter-reflections, whereas a five- hour ray-traced image was pure white. So it's very important 
that we use photorealistic techniques so we can model things as they will appear if they were built. 
Before I get into the next part of my talk, I would like to make the plea that in the future we can obtain 
some standards for photorealism. These standards should measure the simulation against the real or actual 
environment, and only then will our rendering techniques be appropriate. When are abstractions necessary? 
Well, they're certainly necessary to provide communication and interpretation when data must be understood 
and photorealism can, in fact, be harmful. In this sort of situation, the image is representational, 
and frequently in the vocabulary and terminology of the user. Abstractions are not new and they are extremely 
useful. We have an amazing ability to perceive the intentions and meaning of an abstract representation. 
Look at the famous Picasso drawings. Look at how much of an impression we can get of what the figures 
are like. We can understand the whole shape and form and action from just a few lines. In his later life, 
he even abstracted to a greater degree and we can spend many, many hours looking at these abstractions, 
understanding what they meant and appreciating their artistic quality. Molecular biologists use primary, 
secondary, and tertiary structures to present information. Engineers now use color to try and represent 
stresses, and we've seen many of these examples. Architects use figure ground relationships and plan 
drawings with sharp shadows to depict building heights. In any architectural design studio, note all 
the yellow tracing paper which abounds. Architects interact with their designs on this yellow tracing 
paper. It's an abstract representation of what they want, and you can't take this away from them. They 
understand their designs as they deal with the pencil drawings. And they work on all of this tracing 
paper in their own virtual world, understanding what it's going to be. I think that scientific discovery 
in the future will depend on such virtual worlds, ones which abstractly represent the worlds of reality 
but are free from some of the difficulties of precise representation. In order to do this, we must learn 
the peculiar drawing, talking, and representative languages of the particular professions. Furthermore, 
abstractions can communicate even more information. Because of the way we are and the way our minds work, 
we can even associate with them far more meaning than is actually represented. To quote Jessica from 
the movie Who Framed Roger Rabbit, "I'm not bad, I was just drawn this way." JOCK MACKINLAY: Our next 
speaker is Professor Margaret Hagen. MARGARET HAGEN: I am going to show you oil paintings like this Dutch 
interior by van der Neer (ca. 1700), rather than computer-generated images, since showing the latter 
in this company would be hauling coals to Newcastle. I agree with both of the previous speakers in many 
of the essentials, but from the standpoint of a perceptual psychologist. There is no such thing as realism 
in the REYES sense; you can never, ever, render everything you ever saw. Visual representation, like 
visual perception, is always an act of selection. To perform that selection, you can parse visual reality 
in a variety of ways that can be described as different kinds of "perspective": Perspective of Geometry, 
Perspective of Reflectance, and Perspective of Illuminatedness. Perspective of Geometry is well-illustrated 
by this Dutch interior by van der Neer (ca. 1700), Perspective of Reflectance by this Copley, Mercy Otis, 
from 1766, and Perspective of Illuminatedness by this painting, Old Models, by Hamett, from 1892. Traditional 
theory asserts that if you combine all three perspectives in a single image, you will generate a fool-the-eye, 
or trompe-l'oeil, picture. A trompe-l'oeil picture gives the illusion of visual tangibility, of depicted 
surfaces standing out from the image plane. Trompe-l'oeil is considered to be the ultimate end of the 
visual realism continuum - the height of depiction success. Asserting visual tangibility to be the criterion 
of realism is simplistic, because apparent tangibility is only one kind of visual experience, one kind 
of visual percept. Tangibility is not the only realistic selection available for representation. What 
an image maker, an artist, a graphic designer is doing, from the perceptual point of view, is determining 
viewers' subsequent perceptions. Visual realism is essentially the manipulation of the varieties of visual 
experience, of visual perception. Visual perception is manipulated representationally by the selection 
of what to depict. What should the artist/designer select to depict, and when? He/she must depict the 
necessary "affordances." What do I mean by "affordances"? In the ordinary environment, we can often tell 
by vision alone what we can do with what we can see. That is, we perceive the behavioral affordances 
of objects. We pay attention to what we need to pay attention to, in order to do something, to perform 
some task. The job of the artist or graphic designer is to render visual affordances, to structure the 
light to the eye meaningfully for whatever the viewer must do with the image. There are, of course, different 
affordances for different tasks. Holbein painting an image for the King of a prospective Queen depicted 
different affordances than would a designer depicting a machine part in a CAD application. The artist 
must put into the image the information necessary to do a task or make a judgment. Because perception 
is driven by information, we manipulate visual perception by manipulating visual information. I do mean 
visual information, not just visual stimulation. The light to the eye carries information about the world. 
It carries information about surface layout, about the arrangement of surfaces in the world. It carries 
information about substance qualities, about the qualities of substances like fabric and tapestry, wood, 
and ivory piano keys. Sometimes the fight carries information about illumination qualities per se, like 
the depiction of time of day and the changing qualities of sunlight in the well-known pictures of haystacks 
and the cathedral at Rouen by Monet. Simple stimulation, like that occasioned by motion blur, or glare, 
or a camera being out of focus, are not information for the perception of the visible world. They are 
media artifacts or stimulation occasioned by the limiting properties of the organism's effector systems. 
They do not carry information about the world. Visual information tells us what things are, where things 
are, and what you can do with them. Sometimes, manipulating visual information to direct the viewer's 
attention and determining the subsequent perceptions may involve violations of simple realism. These 
violations may take several different forms. You can direct the viewer's attention by violating realism 
by caricaturizing. Caricature, as in these examples, exaggerates a person's distinctive features -at 
the expense of the non-distinctive ones -in order to render the person more recognizable. That is not 
the only function of caricature, but it does work to do that. I know that because I know these two people 
pictured here. One is a colleague, and one is a student. Can you guess which one is which? Take note 
of which is happier. You can do a caricature with greater complexity and generality if you take the shape 
of a baby's head, and a baby's facial feature size relations, for instance, and exaggerate them. You 
can then apply this transformation to an object, like a VW Bug, and obtain judgments from people of "young 
VW" and "cute VW," because babies are perceived to be both "young" and "cute." Roger Rabbit's wife that 
you saw in Don Greenberg's last slide illustrates caricaturized secondary sex characteristics that carry 
affordances, obviously, that are supposed to determine percepts in at least some portion of the viewing 
audience. Political caricature draws on cultural stereotypes like "eyes close together suggests a mean 
person." Violations of realism can direct attention by spotlighting visual information -not a good word 
in this context, I guess. For example, in the painting, Artist in Studio, by Rembrandt, the lighting 
model is incoherent, yet it does a wonderful job of subordinating the image of the artist to the image 
of the work on the easel. You can also isolate visual information in order to direct the viewer's attention. 
In a 1956 study by Ryan and Schwartz comparing the speed of detection of image details -like the position 
of the switch in a picture of a light switch -viewers were fastest with an unshaded line drawing or a 
cartoon, slower with a shaded line drawing, and slowest with a black and white photograph. Violations 
of realism also can be required by the medium. In Western post- Renaissance art, the station point, the 
view point (the camera) is always at a distance that is at least ten times the size of the object depicted, 
following a dictum of Leonardo da Vinci. When the view point is closer than D = 10S, the image does not 
look right to Western viewers, and the ground plane will rise right up off the horizontal, perceptually, 
unless the eye of the viewer is placed exactly at the viewpoint (camera point) -a setup which precludes 
viewing by more than one person at a time. We also have a convention that vertical parallels must remain 
parallel regardless of the projection, and spheres must always project as circles. In computer-generated 
images, where artists and designers do not observe these conventions, you often wind up wanting to nail 
the floor down to the horizontal, and buildings often look like the leaning Tower of Pisa. This does 
not happen all the time, but it does happen too often. Geometry, art, and perception are related, but 
they are not all the same thing. Realism can be violated for aesthetic reasons. Tumer's dramatic paintings 
of fire at sea, for example, blow simple realism right out of the water. Consider E1 Greco, who certainly 
did not exaggerate the body parts of his subjects simply because he was astigmatic, or Lilian Hale who 
didn't work in black and white because she lacked color vision. Aesthetic vision has its own demands. 
My last point is that the trick in realism, it seems to me, is to structure the light to the eye for 
perception without recreating the world, without reproducing the three- dimensional world of objects. 
A notable success in this area is John Singer Sargent's painting of a lady in a satin dress, from 1903. 
Sargent has modeled the structure of the light to the eye from the satin fabric without modeling the 
satin fabric itself. He has captured the visual information in the light from the dress without having 
had to reconstruct a three-dimensional model of the fabric itself. He is painting a pattern of light 
- an intensity pattern -not sculpting a dress of satin. Sculpture and painting are both visual, but they 
require very different kinds of knowledge on the part of the artist. A sculptor can make a statue, and 
photograph it in a lighted room, but in creating such an image he has not made a painting. Where computer 
graphics does not know how to "paint" mathematically, it resorts to mathematically "photographing" a 
lighted, mathematical "sculpture." The efforts are visually impressive, and sometimes aesthetically impressive, 
as well as sometimes even moving, but they do not rival Sargent or Copley, Rembrandt or Turner. They 
do not reflect that level of mastery of visual information for perception. The goal of effective representational 
inaage making, whether you paint in oil or in numbers, is to select and manipulate visual information 
in order to direct the viewer's attention and determine the viewer's perception. To do that requires 
knowing the difference between geometrical optics and ecological optics, and that is what realism really 
is. STEVE FEINER: Our next speaker is Dr. Jock Mackinlay of Xerox PARC. JOCK MACKINLAY: Thank you. Today 
I'm going to describe some research I've done on automatically designing effective pictures. Here are 
two very effective pictures. The picture on the left describes Napoleon's Russian campaign. This picture 
was drawn by Charles Joseph Minard, a French engineer, in 1861, and is included in Edward Tufte's book, 
The Visual Display of Quantitative Information. In the top part of the diagram is a map that describes 
the path that the army took to Moscow in tan and the subsequent retreat back to Poland in black. The 
width of these lines indicate the size of the army. Napoleon started out with 422,000 men and only ended 
up with 10,000 men. In the bottom half of the diagram is a line chart that describes the temperature 
during the retreat. And as you can see, the temperature had a dramatic effect on the size of the Army. 
The picture on the right describes an animation that was shown last year at SIGGRAPH. This animation 
was done by Robert Haber and colleagues at the National Center for Super Computing Applications of the 
lJniversity of Illinois. It describes how a crack in a plate of aluminum grows when you stress the plate. 
Height shows kinetic energy, color shows strain energy, and the little brightly colored bumps you see 
are the tip of the crack as it grows. There are a couple of things to notice about these pictures. First, 
even though this animation was generated with fancy computer graphics such as multiple light sources 
and perspective, it's not a realistic description of a plate of aluminum being stressed. More importantly, 
it's not more effective than Minard's diagram which was done over a century earlier with pen and ink. 
It's not so much how we render effective pictures but how we design them, which brings me to an important 
question. Besides getting the artistically talented, fancy computer graphics technology, how can computers 
help us design effective images? Well, I've done some research that sort of hints at an answer. My research 
goal was to build a system which I call a presentation tool that takes some data and automatically designs 
a graphical presentation. There were three research ideas that lead to the system. The first is that 
graphical presentations are built out of a graphical vocabulary. I based my work on a French graphic 
designer named Jacques Bertin. He observed that infoxrrnation can be encoded by the graphical properties 
of marks, such as their position, color, size, shape, gray level, orientation, and texture. The second 
idea is that you can take this graphical vocabulary and compose it together to generate a presentation. 
I developed three composition operators that are based on the same idea, which is that you can take partial 
presentations and combine them, by merging identical parts. For example, if you have a few scatter plots 
that have the same axes, you can combine the axes. The third idea is that you can then search through 
this space of possible compositions by using effectiveness criteria. This slide here shows the relative 
effectiveness of color for different types of information. In general, effectiveness is quite difficult 
because we don't understand human perception very well. Given these three ideas, I then built a prototype 
system called APT, which stands for "A Presentation Tool." Here's typical input to APT. It's four relations 
that describe the price, mileage, weight and repair record of some automobiles. APT designs a presentation 
in two steps. First, it selects appropriate primitives from the graphical vocabulary for each of the 
relations. Here APT has selected marks positioned on a vertical axis for price, marks positioned on the 
horizontal axis for mileage, the size of the marks for weight, and the color of the marks for the repair 
record. The second step is to compose these primitives into a presentation. In this case, since all the 
marks encode automobiles, APT can use mark composition to generate the scatter plot shown here on the 
fight. Now the interesting thing is that this compositional approach can also be used to describe Minard's 
diagram and the animation that I talked about before. In Minard's case, line width was used to encode 
the number of soldiers, and XY position to encode the location of the Army. Since these lines both encode 
the Army, mark composition can be used to generate the map on the top part of the diagram. A line chart 
is used for the temperature and since that line chart shares a same horizontal axis as the map, single 
axis composition is used to generate the resulting image. In the animation case, XY position encodes 
the plate of aluminum, Z encodes the kinetic energy, the color of the points encodes the strain energy, 
and, as you might guess, mark composition can be used to generate the single image. The animation is 
generated by varying these other values over time. This brings me to three challenges for the SIGGRAPH 
community. First, judge effectiveness, not realism. The challenge here is to develop effectiveness criteria 
for 3D and animated presentations. Second, compose multiple styles. The challenge here is to develop 
hardware and software tools that allow designers to easily compose multiple styles, including realistic 
imagery. The third challenge is to develop design tools. The challenge here is to help people design 
effective images, and the reason this is important is that all the advances that we've made over the 
years in computer graphics technology has been giving designers more and more choices for designing images, 
and they certainly need all the help they can get. Thank you. The final speaker is Professor Steven Feiner. 
STEVE FEINER: So far we've heard about a variety of different pictorial styles, ranging from photorealism 
to more abstract displays of relational data. I'd like to suggest a strawman framework with which we 
can compare and analyze these pictorial styles. To put it another way, I'll try to answer the burning 
question, "Why is a bar chart like a photograph?" A first crack at an answer is, I think, rather straightforward, 
if a bit simplistic, namely, "Because, they're both pictures." But what does that mean? Over the next 
few minutes I'd like to develop the idea of what a picture is in such a way as to allow us to talk about 
the similarities and differences between different pictorial styles. Well, what is a picture? Rather 
than defining a picture as an array of pixels or perhaps as a program in a language like PostScript, 
I'd like to base our definition on some concepts from image synthesis. Think of a picture as being composed 
of three major parts: a set of objects, a camera specification, and a rendering specification. If we 
specify all three parts precisely enough, we can establish a functional relationship between them and 
the array of pixels that they can be used to create. I'll discuss each of these parts in turn. Let's 
begin with the objects. We can think of an object as a set of properties and their values. I'd like to 
distinguish between two different kinds of properties. Visual properties directly affect what an object 
looks like to us. They're the sorts of things that we tell graphic systems about the objects that we'd 
like them to draw. Visual properties include the geometric properties: shape, size, position, and orientation. 
As well, they include the material from which an object is constructed, its surface finish and how much 
light, if any, the object emits. In fact, note how these visual properties generalize the graphical properties 
that Jock mentioned to include arbitrary 3D illuminated objects, rather than 2D symbols. Nonvisual properties, 
on the other hand, include just about everything else we know about an object, for example, its name, 
cost, and importance. This is what's often known as application data as opposed to graphics data in graphics 
packages like Core, GKS, and PHIGS. The camera specification provides the notion of a viewer, distinguishing 
a picture from a mere set of objects. A camera specification includes what graphic packages would call 
the viewing specification, which indicates where the camera is going to be located and what type of projection 
is to be used. We can, of course, supplement this with additional parameters that define lens geometry 
or shutter speed, for example. The rendering specification is, I think, an often neglected component 
of a picture's definition. It corresponds to the sorts of parameters that we would pass to a renderer. 
In fact, the very act of choosing a particular renderer to make a picture defines part of the rendering 
specification, since this choice typically will restrict the kinds of pictures that we can make. The 
rendering specification determines the quality of a picture in terms, for for example, of its resolution, 
and the presence or absence of antialiasing. It specifies whether or not shadows are computed and what 
rendering style should be used, for example, a line drawing or a shaded image. It's important to note 
that we may want to modify both camera and rendering specifications on a per object basis. For example, 
we may indicate whether an object will be allowed to be shadowed by or cast a shadow on other objects. 
I like to use the word "depiction" to refer to the process of defining a picture, to distinguish it from 
the process of rendering a picture once it's been defined. Depiction takes as input a purpose that the 
picture should fulfill, in conjunction with some set of objects in the world being depicted. Purposes 
include entertainment, advertising, or, for many of the pictures that we've been discussing, communicating 
specific information to a viewer in a particular situation. The output of the depiction process is a 
new set of objects, a camera specification, and a rendering specification -in other words, a picture. 
Now, notice how I sort of snuck something in over there, namely that I've intentionally indicated that 
the new set of objects need not be the same as the old set. The new set may well include some objects 
from the old set, but it may also exclude others because they're not relevant to the picture's purpose. 
As well, some old objects may be joined together to make one new composite object that stands for that 
original group. In contrast, we can take one old object and represent it by several new ones, for example, 
in a picture that contains a pair of views of the same object. We may also create new objects totally 
from scratch, such as a coordinate grid on which other objects are positioned. As I'll mention later, 
the visual properties of the picture's objects may be inherited from those of the corresponding original 
set of objects. The more this occurs, the more the picture encodes information about these properties 
in the same familiar way as a photograph. Alternatively, some of these visual properties may be derived 
from nonvisual properties of the original objects, allowing the picture to visualize what would be otherwise 
unseen properties. If we consider each component of a picture as a set of parameters, we can think of 
them as defining a multi-dimensional continuum of pictorial styles. We'll have an orthogonal axis for 
each independent parameter: the objects and their visual properties, the camera specification, and the 
rendering specification. Since our coordinate system in style space, so to speak, needs an origin, I've 
conveniently chosen photorealism. Now, let's see how we can define a photorealistic picture. Here's an 
example of a picture that was recorded directly on film using a portable, real-time, optical computer, 
in 1/250th of a second. Photorealistic images, or actual photographs like the one that you're seeing 
now, use the exact set of original objects, limited only by the bounds of the view volume and the viewing 
projection. These objects maintain all their original visual properties. Photorealistic pictures use 
some form of linear perspective projection as part of their camera specification and, of course, use 
a full, global illumination model for their rendering specification. Now, let's look at some pictures 
that deviate from photorealism. I'll refer to these as examples of abstracted styles since they abstract, 
more or less, the objects, visual properties, or camera and rendering specifications of what would have 
been a photorealistic picture. Consider this bar chart. Only a specific set of objects is shown: a set 
of countries at two points in time, supplemented by some objects that weren't in the original world -labels, 
background, and tick marks. Each one of the countries being represented has none of the visual properties 
of the actual country. Color is used to distinguish time and size in the Y axis is used to indicate wheat 
production, which is a nonvisual property. The camera specification is a simple 2D window to viewport 
mapping, and the rendering specification may use the notion of purely self- luminous objects that don't 
affect other objects and don't even interact with light sources. You can think of these two pictures 
as representing, respectively, the photorealistic origin -the photograph on your left - and something 
rather far from the origin in many of these dimensions -the bar chart on your right. Here's an example 
that's somewhat closer to the origin on several axes than the bar chart. It's an airplane, done by Diane 
Hauser and colleagues at McDonnell Douglas, in which the geometric properties of the object are preserved, 
but the original color of the object is replaced with pseudocoloring to present the nonvisual property 
of pressure. Finally, here is a picture that was generated by APEX, a knowledge-based system that I developed 
that designs pictures whose purpose it is to show how to perform actions in a 3D world. This picture 
is intended to depict to a person that they're supposed to pull out the drawer of the cabinet at the 
middle of these three large cabinets. APEX selects the picture's objects from those in the world, initially 
including only those that participate in the action being performed. It then adds other objects, for 
example, to provide context, to serve as landmarks, or to disambiguate objects already in the picture 
from others that look like them. APEX uses the geometric properties of the objects displayed, modified 
somewhat by algorithms that determine the level of detail to be shown. It creates a new object, the arrow 
that you see at the center of the picture, to show the action of pulling the drawer. The camera specification 
is chosen by the system in such a way as to include more or less of each object in the picture, depending 
upon its role in that picture. The rendering specification indicates that some objects should be rendered 
normally, and that other less important ones should be subdued by modifying how they will be shaded. 
Note that each deviation from the photorealistic origin of our coordinate system, on one or more of these 
axes, may indicate a choice intended to create a more effective picture. Making these choices, either 
by human or machine, is currently an expensive task. It is, however, one that may be paid back in the 
creation of a picture that communicates information effectively, uncluttered by extraneous detail. I'd 
like to leave you with the thought that an important task for computer graphics is to develop a deep 
enough understanding of what makes pictures work, so that we can automate not only their rendering, but 
their design. Thank you. JOCK MACKINLAY: We're now at the question/answer part of the panel, and I hope 
that there will be a lot of discussion, so I ask you to keep your questions short and to the point. I'd 
also like to remind you what you probably already know, that you should state your name and affiliation 
and I'd also like to remind you that, at the end of the panel, we will meet in the back of the room ff 
there are further questions, and head off to the break-out room. Q: Bo Lu Sing from MCC. You cite as 
an example of pictorial effectiveness a graph of Napoleon's army showing the relationship between temperature 
and the distance of the army and so on. What's an effective picture is essentially in the reader's mind. 
And in every picture, the presenter, the person designing the picture, is trying to expose certain relationships. 
I am kind of concerned about how you would go about designing a system that presents or designs effective 
pictures. It's just like saying, "I'm not only going to give you a pencil, but I'm going to give you 
a pencil that designs effective pictures." I'd like you to address that. JOCK MACKINLAY: The intent of 
the research is to explore the kind of knowledge necessary to design effective pictures. I think that 
those insights can also be used by any designer - whether it's machine or human -to do it. I agree with 
you totally that it's the purpose that you need to judge the effectiveness of the picture against. So 
Minard's diagram is very appropriate for one kind of purpose and Guernica by Picasso is very appropriate 
in another case. Q: Roger Craubus, Lawrence Livermore Lab. I've seen quite a few finite element pictures 
up there, and I wasn't sure whether the people showing them were proponents of photorealism or not. But 
a point I'd like to make is that the color and stuff in there is not so much for photorealism, but to 
bring out highlights, to highlight where the peak stresses are, and such. If you use radiosity methods 
on those or shade over that or something, it seems like you're going to smear away or detract from the 
highlights. I'm not sure that is what the scientists want in order to do their analysis. And I'm not 
sure what you are proposing. DON GREENBERG: I can try and comment on it, but I'm not sure I'm really 
answering your question, because I don't think anybody was proposing anything specific. In my last life, 
I was an engineer at one time, and I did work in finite element analysis. We always had a difficult time, 
and we still do, in trying to understand where the key stresses are, and so we use color to represent 
them. Error number one, which is typical in the community, is that we really use very, very poor color 
scales. If anything, we probably should use color scales which are of equal perceptual distance so that 
the observer who's trying to understand what's going on, can understand the relative magnitudes of the 
stresses they're evaluating. More important, perhaps, is argument number two, which is that, right now, 
we just superimpose stress contours on top of the three-dimensional display and most of the time, we're 
looking at relatively simple objects. As the objects get more complex, one is going to have to carry 
the geometric information in the intensity channel and use the hue and saturation channels to represent 
the stress, keeping it separate so you'll still understand the geometry. Q: Bruce Hubbard from Lockheed. 
I'd like to make a comment from the photographer's point of view, and that relates to your use of the 
term "photorealism." A good photograph is its own creation. And it's an entirely new entity that's in 
some way separate from what was photographed. And to say that an effective computer graphic picture is 
the same as an effective photograph and that this is just a one-to-one mapping of points of light from 
something onto film I think misses something very important. A: We agree. Q: The world is full of terrible 
photographs that are good one-to-one mappings and they don't portray what you felt when you took it. 
There's something else that you have to do to it to make that occur. Q: John Crawford from Microtel 
Pacific Research. And I guess if the question is what makes pictures work, one good answer is human intuition, 
or at least the interaction between perception and intuition. On the West Coast, we tend to talk about 
a left brain approach to things and a fight brain approach to things, where one is synthetic and one 
is analytical. The analytical approach deals with schematic representations, whereas the synthetic approach 
can deal with a closer connection with the real thing. Something that can be perceived as being real 
often can be seen to have a much clearer connection with the human intuition. If you take that approach, 
you can say that the more realistic you get, the more intuitive you can be about how you can use this 
image to do something that you need to do. I wonder if the panelists can speak about that kind of connection 
with human intuition through effective pictures? JIM BLINN: My intuition is that it wouldn't work too 
well. Q: How do you see that?  JIM BLINN: Well, sometimes schematic things give information better 
than realistic things. And why that is, I couldn't say. But it's removing distractions, perhaps, as much 
as anything else. We work in a very symbolic area, which is mathematics and representation of mathematics. 
Other things can be represented as well by symbols. STEVE FEINER: I can suggest that if you look at a 
maintenance and repair manual, particularly a well-done maintenance and repair manual, you'll see surprisingly 
few photographs. Very often you'll see line drawings intentionally being used to simplify what might 
otherwise be a much more complex picture, to highlight precisely those things that the person writing 
the manual wants you to pay attention to and do stuff with. JOCK MACKINLAY: I have a comment, too. Could 
you put up slide number 3. Here's an example which is very typical of the sort of thing that you see 
at the film show all the time, of something that isn't something that a scientist is doing. It's for 
entertainment purposes. And yet, there are all sorts of things about this image that are very stylized 
in various interesting ways. So I think the left brain/fight brain sort of notion is rather superficial 
and doesn't really capture the notion that you should understand how humans perceive and what's effective 
in pictures. STEVE FEINER: As another example, I use Maxell disks in my Macintosh. If you look on the 
back of the disk label, it has a set of pictures explaining how to write- disable the disk and telling 
you that you shouldn't touch the disk itself. It's all done with line drawings, not with photographs. 
And you can be sure that it wasn't because Maxell just couldn't afford a photographer. The pictures explain 
things very clearly. They abstract much of the detail of the disk. They don't show lots of little things 
that would detract from the important points that they're trying to get across. Q: Ron Baecker, the University 
of Toronto. This is really a comment, trying, perhaps, to disguise itself as a question. But I'll try 
to be brief. I think the co-chairs are to be congratulated for raising this issue, and I think it's very 
valuable that issues of different kinds of pictorial style - including photorealism, but also line drawings, 
abstract diagrams, cartoons, and their advantages and disadvantages -are raised in a forum such as SIGGRAPH. 
I think, however, our understanding of the advantages and disadvantages of these different approaches 
is still in its infancy. I guess I'd throw out as a challenge to the panel and to the audience that maybe 
one way to enhance this understanding is to try to construct for a particular object -such as a car, 
a room, a mechanical part, or scientific concepts -a space of representations or visualizations or depictions 
that cuts across these styles and that is sensitive to what I think is the fundamental issue, which is 
what is one trying to achieve or communicate. What task is one trying to facilitate with the depiction? 
I mean, is it for purposes of recognition, or for evaluation of strengths or weaknesses of something? 
Or for appreciation? Or for comparison of the features? Or for communication of facts, either quantitative 
or qualitative? To make my challenge more concrete, I hereby issue a verbal call for enhanced teapots 
for next year's SIGGRAPH. This is not just a focus on different shapes of teapots and simply more and 
more photorealisfic teapots. Rather, can one come up with new ways of showing specific objects? For example, 
teapots that span the gamut of all these different styles. Then, maybe one could postulate that they 
are good or bad in various ways because of the extent to which they have certain functions or are an 
answer to certain questions or are useful for certain tasks. I'm curious if the panel has any comments 
on that. JIM BLINN: It would be interesting to try the exercise of coming up with one specific idea which 
you want to portray and trying different pictures to portray that idea. A lot of times, people who do 
this have basically a product in mind or a deadline or something like that and they don't get to do the 
job five different ways. They just have to pick the way, through intuition, that they feel will work 
best and only do it once. I guess it's almost a psychological research area of trying lots of different 
solutions to one problem and see what works best. But it might be a good idea to try to make the challenge 
more concrete in the sense that lots of people are going to, perhaps, try to make different teapots but 
what are they trying to say about the teapot? Are they trying to show its size or its function? If they're 
not trying to say the same thing or illustrate the same thing with the teapot, then all the different 
pictures that people come up with will not be very easily comparable. Q: Well I guess my comment is that 
a space of teapot images designed to answer different questions or done from different points of view 
might be a very interesting exercise. I just mentioned the teapot because it's obviously a theme for 
next year. It could be interesting to have other domains, as well. STEVE FEINER: Part of the submission 
should be an indication of what the purpose of the picture is supposed to be. Q: Leon Soren, Computer 
Aided Planning. My concern is if we are going to use computer graphics as a means of communicating information, 
I think it's important that we know our audience. And it was just barely or briefly mentioned amongst 
the panelists today that perhaps we should have certain kinds of images or certain kinds of information 
presented in certain ways, depending on the audience that the image or the information is meant to communicate 
to. Do you have any comments on that? JOCK MACKINLAY: I have one comment. Your question began with the 
phrase, "if we are going to use our images to communicate" - we certainly always use our images to communicate, 
and so you better be in mind of the audience, because otherwise you're not going to succeed. Images are 
not there just in the abstract; people look at them. STEVE FEINER: I should mention that we were trying 
to say when we talked about a picture's purpose that it included communicating information to a particular 
viewer or set of viewers. For example, although it wasn't clear from the single picture generated by 
APEX that I showed, the objects that APEX includes in a picture depend on what the viewer is assumed 
to know. APEX adds more or fewer objects in more or less detail, depending upon how knowledgeable the 
viewer is assumed to be about the objects being depicted. Q: My name is Lee Moore and I'm with Xerox 
Webster Research Center. I guess my question actually turns out to be a sort of continuation of the last 
topic. And that is, we talked about what you want to present with a particular picture. It really seems 
to be more of what you want the viewer to learn from the particular picture. I think of a possibly apocryphal 
story of a diagram on a box which was to indicate that this box was fragile for shipping. It was a picture 
of a champagne glass with a crack in it. The story goes that this box was just tossed around because 
the viewer in some foreign country assumed that the box contained broken glass. So there is, perhaps, 
the extra cultural parameter or audience parameter, maybe within our own culture, that needs to be thrown 
into the equation. JOCK MACKINLAY: I think I'd like to take a chairperson's prerogative now and ask a 
question which is for each of the panelists. What sort of software advances do you think are required 
for designing effective pictures in the future? Jim? JIM BLINN: I think it's perfectly possible to design 
perfectly nice pictures with the software that exists now. DON GREENBERG: I don't think that's true. 
I think that a major advance in software is needed, particularly in abstract representational terms, 
where the software has to be designed by the future users of the software. Each of the disciplines which 
want to use graphics to display their results or intentions will have to create software which will be 
in the abstract terminology that they use. MARGARET HAGEN: I'd like to see systems that would let somebody 
build a "realistic" image using components that are percept-specific. So if you put this particular aspect 
of geometry in there, you will determine this percept in your viewer. Add this and you'll get that. It's 
complicated, of course, because they interact. But that's what I would like to see instead of something 
that's so ad hoc. STEVE FEINER: One issue that I think also often comes up with photorealism is that 
of cost. I was wondering if the panelists had anything to say about whether it is really worth it. This 
is the question that a lot of people will ask. Is photorealism really worth what might be the extra expense? 
Or is there really a lot of extra expense to it? JIM BLINN: Is it really worth the extra expense? Naaaah. 
DON GREENBERG: Before I give my answer, I'd like it known that I do want to make Jim Blinn a national 
resource. JIM BLINN: These people will have to pay admission to talk to me? DON GREENBERG: I had a wonderful 
experience this past semester, which, to me, revealed something which I haven't quite been able to express. 
I taught a course in architectural design - which is now two lifetimes ago. I had 16 students, and we 
used computer aided design software. They sketched on tracing paper, did drawings, and then put their 
models into the system and displayed them. As their models got more and more refined, the images got 
more and more realistic. Before I taught the course, I was warned that this was a bad year to teach this 
fourth year architectural class because the students were not as proficient as those in previous classes. 
We had our first jury and they put up line drawings and the critics were severe. Step by step, however, 
they started to use more and more of the computer system and less and less of the tracing paper. At the 
end of the semester, the grades were given out. The faculty who teach don't grade the course. It's usually 
graded by external faculty or other faculty within the university. And all but one of the students received 
a B+ or better. This made me start to think that one of two things was occurring: either the students 
had very, very poor communication skills in the abstract sense, and therefore, never got their ideas 
across. Or, in fact, they were superb designers and the photorealism and the graphics techniques which 
were available enabled them to really come up with superb designs, which people could not question any 
longer because they saw it. I don't know where the jury will fall, but it was a fascinating experience 
to see how, in fact, photorealism did help. MARGARET HAGEN: If pursuing photorealism will mean that computer 
graphics will give to the world a better understanding or some understanding, of what the visual information 
for things like silk and satin and lace and fur and velvet and ivory piano keys is, then, of course, 
from a scientific point of view, from the point of view of human interest in visual perception, it has 
to be worth it. Whether it will give us Sargent -I mean, in the new manifestation - I don't know. I mean, 
the way you've defined it, of course, it can't. But again, you took the art out of photography and gave 
us, I guess, a blind camera, which is kind of odd. Sorry, I didn't mean that quite that way. STEVE FEINER: 
I'd like to suggest, in making a quick comment on the question that I asked, that maybe the pendulum 
really swings both ways. In some cases, it is definitely worth what might be extra rendering cost now, 
and what might not be extra rendering cost in the future, to make the more photorealistic picture. But 
I think in other cases, it's definitely worth what might be less rendering cost, but more total cost 
(if we count the design time a person or machine must spend), to make a less photorealistic looking picture, 
that has less stuff in it and, therefore, might be less confusing. Q: Mark Hamburg, Ashton Tate. This 
comment is from my background doing some audio engineering. It's an example of presenting essentially 
the same information in three different modes that are a continuum from an actual photograph to a schematic 
diagram. One frequently finds in mixing board manuals, and so forth, that there'll be a photograph and 
it'll have little diagrams and arrows showing you what all the controls are. It's fine for finding out 
what the controls are, but it doesn't tell you much else; beyond that, you'd have to read the text. In 
between, and on the other extreme - and I'll do the middle after that - they usually have a schematic 
in which everything is abstractly represented and one has to know the language to read it. All the information 
is there; it's just a little bit hard to get to. What I found a lot, working with people who aren't that 
experienced reading them, is the thing that's the easiest to understand is the mixture of the two, which 
uses shaded artist's renderings of the actual controls and includes arrows in between them to show how 
the signal path flows through things. 21 I think right now we're at the stage with computer graphics 
where we can either go all-out and do photorealism fairly well - I won't say fairly easily, but the techniques 
are there for doing it and they're developing - or there are plenty of drawing and illustration packages 
which allow us to do at least reasonably beautiful charts that convey a lot of information. But what's 
lacking and what would be nice to see is more stuff that comes from the middle that allows people to 
pull in, where they want, this piece that at least has some of the photorealistic elements, but is able 
to put it together. Jim Blinn does it in his work, but he has to use two separate packages to pull everything 
together. It would be nice to see more cohesive graphics environments for doing that sort of work. JOCK 
MACKINLAY: Actually, Jim writes the stuff himself. JIM BLINN: It's perfectly cohesive, yes. JOCK MACKINLAY: 
But I agree with you. I think it's very difficult with current tools. You either get realism or a painting 
program or at least a drawing program of some kind. And what I'd like to be able to do, as Steve mentioned 
earlier, on an object-by-object basis, is to decide what properties are appropriate for the image. I 
have to really fight for tools now to do that. JIM BLINN: I'm surprised other systems don't do that. 
I mean, I do that all the time, and it's just a matter of tagging the different primitives in your database 
as to which rendering you use. And I suspect there are other programs that do that, too. Q: Hiam Zemir 
from Cricket Software. I wanted to go back to your original question that you asked about what sort of 
software advances were necessary to help design effective pictures. And I wanted to offer my own opinion. 
But I think you just brought up part of it. I think -if I understand Jim Blinn correctly - you said that 
generally one applies one's intuition to the problem and picks an approach that he believes will work. 
But if software is designed so that the entire gamut of possibilities between abstraction and realism 
are all, say, interactively available, then a designer can find out and in the process, he can use his 
intuition as a starting point. But I think that through the process of experimentation, he may get some 
surprising results in terms of what actually does work for effective communication. JIM BLINN: That's 
certainly worthwhile. On the other side of that, you have to kind of use people for what people are good 
at and computers for what computers are good at. And people are good at intuition. And computers aren't 
generally so hot at it these days. Q: Shawn Atkins, Stalow Systems. To follow directly up on what Jim 
Blinn just said, it seems to me that, in my experience, I have worked with people and I have used machines. 
What is the purpose of replacing perfectly good human artists who are very good at interpreting what 
a person will perceive and needs to see in a picture with a machine? Is that because I can't stand to 
work with a person and would prefer to employ a machine? Or is there is some shortage of good artists 
that we need all these machines to do this artwork intelligently? Desktop publishing sort of proves that 
if you give a guy the tools, he can make ugly pages. You give a person hot wax and a knife, and he can 
make beautiful pages. JOCK MACKINLAY: Different people there, right? When you give the artist a tool, 
the artistically talented will do wonderful things with it. But a lot of the time we're generating images 
for our use, and it's not worth our while to spend a lot of money going off and paying an artistically 
talented person to generate an image. We want to make them as effective as possible. That's part of the 
intuition behind scientific visualization. The scientists are there working, they're finding out things, 
they're making decisions. And we want to give them the best tools we can. We know that tools can either 
hinder or help with the images even if you don't have an artist there at your elbow. STEVE FEINER: I 
think that there are two notions that are important in this context. One of them is customization. For 
example, you might want to have an image that's particularly good for you, given your personal idiosyncrasies. 
For you to go and personally hire an artist, a good artist, to make that image might be more than you 
can afford to do. Second, even if you can hire someone, the artist will take a human amount of time to 
make the picture. There are, however, a lot of applications in which you want to be able to get a picture 
that explains some new data and you want it instantly. And even with the most talented artist, you just 
can't get that kind of turn-around. Automated systems may eventually be able to meet these needs for 
customized, on-the-fly generation of graphics. A: I'd like to add some strong statements about that, 
because I think there's a premise behind your question which is wrong. You seem to be implying that pictures 
are a post-processing mode which is used to do some sort of presentation graphics. There's a whole area 
of computer graphics which is dealing with scientific discovery or design where you want the user to 
interact with the model that the user is trying to understand. If speed is important, maybe an abstract 
representation is far better than a realistic one. But you can't get somebody else involved in the loop. 
It's the scientist or the artist or the designer or whoever who's working with that. And that's extremely 
important. Q: Jerry Marr, University of Oregon. Jock, I think your advice to judge effectiveness is well 
taken, although I would say that effectiveness as a photorealistic image is a valid objective also. I 
think that your intention in this panel session is to emphasize the second word of that two-word statement, 
"effectiveness." But I'd like to say that the first word, "judge," could use a little bit of emphasis 
also. I think it's important to make that judgment, to run experiments, to make a quantitative evaluation 
of the effectiveness. And I was wondering if the panel members had any comments on that. JOCK MACKINLAY: 
That was my first challenge. Q: My name is Scott Harrison, from Skidmore Owings &#38; Merrill Architects 
in New York City. I was very interested to see that there was a discussion of architects by Dr. Greenberg, 
to see that that's actually happening. And the comparison with the world of art was very interesting 
also. I think that architectural imaging is kind of in the stage of the Van Eycks and we're trying to 
reach this perfect image, but we don't really have the technology to model a true environment in which 
men interact completely. But more I see in the way of architecture fight now, we're looking for not only 
an abstraction of a reality, but actually a distortion of reality. Jim Blinn was looking at quantum mechanics 
in which the physics breaks down completely from what we've known it to be. And a lot of architects right 
now want to examine images. which actually deny reality in some way, and I don't really see that progressing 
within the market at the moment. I wonder if anyone sees that need to be coming. JOCK MACKINLAY: Could 
you give us an example? Q: A lot of the work of Zaha Ided, an artist. I'm not exactly sure where she's 
from at the moment. A lot of her work actually is merely presentation graphics in the way of images. 
But it distorts reality altogether using the dimension of time to show her work in. And it's really beyond 
showing an image of a building, but reaching towards more of a graphic depiction of what it could be. 
I don't know, maybe it's a distortion. I thought maybe it was beyond the point of a distortion or an 
abstraction. Q: I'm Harold Zatz, a student at Cal Tech. You have been talking about designing effective 
pictures for visualization. What about designing effective pictures for entertainment? It seemed that 
in the film and video show, the general progress has been towards either photographic realism or imitating 
cartoon styles. Do you see these two as the only answer in entertainment?  JOCK MACKINLAY: This, by 
the way, is a fair question. But we were limited to five panelists, so we didn't include a person from 
the entertainment industry up here, which we otherwise would have. I certainly don't think that there 
are just two data points on effectiveness for the entertainment industry. There's probably a continuum. 
And my challenge about judging effectiveness applies there equally well as for scientific presentation. 
It's just that the judgments are very different. STEVE FEINER: My guess would be that standards for effectiveness 
in entertainment would not only be very different, but would be more difficult to establish. It's easier 
for us to talk quantitatively about things that communicate specific information more effectively and 
harder to come up with the beginnings of a magic box that somehow managed to make something more entertaining. 
I think there's more art involved and there are deeper problems in trying to simulate what's going on, 
all of which are a good deal harder to do than some of the stuff that Jock and I, for example, are exploring. 
Q: Jim Ward from Pacific Data Images. I'd like to address my question to Margaret Hagen. I'm a little 
familiar with your book and know that you sort of have a Gibsonian background. I'm really interested 
in this word "affordances" and in the precepts that you used. We talked a little bit about representational 
type stuff and arrows, and I wonder how that relates to the concept of affordances. Also, I'm interested 
in the idea of emotion and how that can be related to affordances. I guess, like the diagram of the electrical 
field, it seems to me that affordances come from some kind of body sensations or feeling that comes out 
of being human and being in the world. What we're really doing there is getting away from it and replacing 
it with some kind of language. Could you say something about affordances and emotion -I'm sort of lost 
about how the two relate. For instance, in movies like Baghdad Cafe they use color to symbolize certain 
moods. Is there any connection with the concept of affordances? MARGARET HAGEN: Color is not a very reliable 
affordance because it isn't a reliable indicator of state in the environment. There are instances like 
using red to indicate whether an apple is ripe, that will work sometimes, but the absence of red will 
not work. Color perception is unreliable and highly conventional in terms of the kind of cognitive information 
it can carry. Though, there is one thing about Steve's point that I found confusing. When I say "affordances," 
I am talking about the use of animal bodies, not just human bodies. I am saying that for a cockroach 
or a sparrow, the affordances of this particular rigid horizontal surface here are different from the 
behaviors it affords a human animal because we are different and the scale is different, and the effector 
organs are different. So affordances are always specific, so to speak, to a particular species. The other 
thing is that perceptual information -I'll go out on a limb and say it - is generally unlearned. (Of 
course, it cannot all be unlearned. You can learn to discriminate one wine from another, for instance, 
and you are not born knowing how to make such fine discriminations. But what happens in perceptual learning, 
mostly, is getting finer and finer discriminations.) But, reading bar charts and such things is, of course, 
learned. It is not simply perceptual. I mean bar charts, in themselves, don't have any particular perceptual 
meaning. Their meaning, is certainly not meaning or affordance or visual information in the sense that 
it is an unlearned meaningful structure in the light to the eye reflected from the objects of the ordinary 
environment. This kind of meaning is metaphorical or cognitive, and it does not afford behavior in the 
same sense that an opening of a certain size affords egress for an animal of a certain size. Or in the 
sense that this cup affords grasping if you happen to be a creature like me with a grasping organ like 
this hand. Did that answer all of your questions? Q: Completely, thank you. Q: Shelly Lake. Why has 
the fine art community been slow to embrace computer- generated imagery?  JIM BLINN: I expect you'd 
have to ask them that. I sort of see this from a slanted point of view, because I see a lot of fine artists 
who are enthusiastic about it. But those that aren't don't come up to me and talk about it. Q: Tim Turner 
of Research Tfiangle Institute. So many people -computer graphics people -have spent so much effort putting 
motion blur into their pictures, that I was amazed to hear motion blur referred to as an artifact. But 
after thinking about it, it seems that in order to achieve photorealism in computer graphics, people 
find it necessary to emulate what is actually an artifact of the photographic process. It seems to me 
the reason that they must be doing that is because this artifact conveys information in the picture that 
has to do with the relative motion of objects in the scene. And so my question to the panelists is, can 
you think of other examples where artifacts of either a process foreign to computer graphics or the computer 
graphics process itself are actually exploited to make pictures more effective? JIM BLINN: I guess some 
of the things in the scientific visualization community. They have a cloud simulation that I saw here 
that looks kind of ripply or bumpy on the surface -the bumps are an artifact of the polygonalization. 
But the scientists like it that way because that shows them what the scale of the discretization is. 
A lot of times I know, at JPL, the scientists who are analyzing the pictures don't like to have the pixels 
be nicely blended together as we are supposed to do with good reconstruction filters. They like to have 
the pixels be kind of small little dots, so that they can look and see what the resolution of the image 
is, versus the feature that they're trying to analyze, so they know whether the picture is giving them 
accurate enough data. STEVE FEINER: Well, I'm afraid our red light is flashing fight now, meaning that 
we're out of time. I'm delighted that our panelists and all of you in the audience could join us today. 
I think that some interesting questions have been raised. We're all looking forward to seeing more people 
look at some of the difficult problems involved in designing effective images, in addition to rendering 
them. I'd like to remind you that the panel break-out room is Room 303. We'll be there to join you to 
discuss things on a one-to-one basis. Jim Blinn      Hagen, Slide credits (in order shown at talk, 
not 1. Eglon Hendrik van der Neer (Dutch, 1634-1703), Dutch Interior, 1700. Courtesy, Museum of Fine 
Arts, Boston. 2. John Singleton Copley (American, 1738-1815), Mercy Otis, Detail, 1766. Courtesy, Museum 
of Fine Arts, Boston. 3. William Harnett (American, 1848-1892), Old Models, 1892. Courtesy, Museum of 
Fine Arts, Boston. 4. John Frederick Peto (1854-1941), Poor Man's Store, 1885. Courtesy, Museum of Fine 
Arts, Boston. 5. Jan van Huysum (Dutch, 1682-1749), Vase of Flowers, 1730. Courtesy, Museum of Fine 
Arts, Boston. 6. "Club." Cartoon illustrating affordances. 7. "No Exit." Cartoon illustrating affordances. 
 8. James J. Gibson, The Senses Considered As Perceptual Systems. Boston: Houghton Mifflin, 1966. 9. 
August Toulmouche (French, 1821-1888), Madame de la Poule, 1865. Courtesy, Museum of Fine Arts, Boston. 
 10. Claude Monet (French, 1840-1926), Rouen Cathedral in Full Sunlight, 1894. Courtesy, Museum of Fine 
Arts, Boston. 11. Claude Monet (French, 1840-1926), Rouen Cathedral: Tour d'Albane , Early Morning. 
Courtesy, Museum of Fine Arts, Boston. 12. Caricatures from Perkins, D.N. and Hagen, M.A. Convention, 
context and caricature. In M.A. Hagen (ed.), The Percet?tion of Pictures, Volume I. New York: Academic 
Press, 1980. 13. Rembrandt van Rijn (Dutch, 1606-1696), Artist in Studio. Courtesy, Museum of Fine Arts, 
Boston. 14. Ryan, T. A. &#38; Schwartz, C.B. Speed of perception as a function of mode of representation. 
American Journal of Psychology, 1956, 69, p. 60-69. 15. Photograph taken with a 50 mm lens at 18 inches 
from the near foot, M.A. Hagen. 16. J.M.W. Turner (English, 1775-1851), The Slave Ship, ca. 1840. Courtesy, 
Museum  Hagen, Slide credits (in order shown at talk, not of Fine Arts, Boston. 17. E1 Greco (Greek 
(Spain), 1541-1614), Fray Hortensio Felix Paravicino, 1609. Courtesy, Museum of Fine Arts, Boston. 18. 
Lilian Westcott Hale (American, 1880-1963), The Old Ring Box, 1907. Courtesy, Museum of Fine Arts, Boston. 
 19. John Singer Sargent (American, 1856-1925), Mrs. Fiske Warren And Her Daughter Rachel, Detail, 1903. 
Courtesy, Museum of Fine Arts, Boston. 20. John Singer Sargent (American, 1856-1925), Mrs. Fiske Warren 
And Her Daughter, Rachel, 1903. Courtesy, Museum of Fine Arts, Boston. 21. John Singleton Copley (American, 
1738-1815), Mercy Otis, Detail, 1766. Courtesy, Museum of Fine Arts, Boston.    Steven Feiner  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1402255</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>29</pages>
		<display_no>13</display_no>
		<article_publication_date>08-01-1988</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Extending graphics standards to meet industry requirements]]></title>
		<page_from>1</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/1402242.1402255</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1402255</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P1101809</person_id>
				<author_profile_id><![CDATA[81100635048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Albert]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Bunshaft]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101810</person_id>
				<author_profile_id><![CDATA[81100117553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Puk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Puk Consulting Services]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101811</person_id>
				<author_profile_id><![CDATA[81365595937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Laib]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1101812</person_id>
				<author_profile_id><![CDATA[81100207464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Salim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abi-Ezzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 EXTENDING GRAPHICS STANDARDS TO MEET INDUSTRY REQUIREMENTS Chair: Albert J. Bunshafl, IBM Corporation 
Panelists: Richard Puk, Puk Consulting Services Gregory D. Laib, IBM Corporation Salim Abi-Ezzi, Rensselaer 
Polytechnic Institute EXTENDING GRAPHICS STANDARDS TO MEET INDUSTRY REQUIREMENTS ALBERT J. BUNSHAFT: 
Glad to see that people are still here at 3:30, the last session on the last day of the conference. Welcome 
to the panel session, "Extending Graphics Standards to Meet Industry Requirements." What I thought would 
be interesting to do at SIGGRAPH would be to, rather than give a status report on all of the activity 
in the graphics standards arena, to talk about some of the forces that come into play in the real world 
of graphics system consumers trying to use these standards in their particular environments. What we 
have chosen to do is to invite three speakers who have all been involved in the graphics standardization 
process for many years, both in the United States and internationally. I have asked them to talk about 
three different subjects, a little different than the other panels, but that all are related to the same 
theme. Graphics systems users are influenced by many different factors when they go to implement or use 
a particular graphics system to solve their problem. There are "real" graphics standards today, both 
internationally and in the United States. Such systems as GKS are full-fledged international standards. 
There are other standards that are at different stages of the standardization process, both in this country 
and abroad. And there are many systems out there today that are, just by nature of their use throughout 
the graphics industry, de facto standards. These also play a real role in the user's environment. And 
then there are of course also each vendor's, or user's own home- grown or proprietary graphics interface. 
In developing actual applications, it's important that these different systems, where appropriate, be 
able to interact. These are some examples of systems which are either standards today, such as GKS, extensions 
to standards such as GKS 3-D as an official standardized extension to one of these standards. At one 
time there was talk of, there's no official activity of something called GKS 9X, but I use that to characterize 
efforts to extent the GKS standard or to revise the GKS standard in the future. PHIGS is now an officially 
approved international standard, and one of the panelists will be talking about PHIGS+, which has been 
an effort to extend the PHIGS standard. PHIGS+ is just entering the standardization process. Then X windows 
is an example of a system which is a standard today by virtue of its endorsement by a number of vendors. 
It is entering into the official standardization process and it's interesting because, number one, it 
has direct interplay with these other systems. We'd like to be able to run our GKS applications and PHIGS 
applications in windows. Its also interesting because it came about through an entirely different process 
than these other official standards did. It grew up in a different world, so to speak, than the GKS or 
PHIGS systems did. And now we have the task, as graphics systems implementors, to make these things coexist 
and operate gracefully together. One example would be, if you went out on the trade show floor, you saw 
a number of vendors who were showing PHIGS or PHIGS+ implementations or GKS implementations running within 
the X windows environment. That's an example, and you're going to hear about an activity to officially 
define how that should operate. This is a good example of where market forces have taken over and just 
driven this thing forward as far as implementors are concerned. And we have implementations doing what 
the marketplace requires today. From an application developers point of view, coexistence, performance, 
flexibility, are all real world issues for application development, and they're all essential. As graphics 
systems implementors, we have to take all these factors into consideration. The first speaker today will 
be Dr. Richard Puk. I've asked Dick to talk about some recent activities that have changed both the structure 
and the process by which graphics standards will come into existence in the international arena. These 
are results of a very recent meeting which took place in Tucson, and we thought that some of that, interspersed 
with some of Dick's comments on where he sees things going, would be an interesting subject to discuss. 
The second speaker will be Greg Laib. Greg will be talking about the PHIGS+ activities, giving a very 
short overview and status of that, but then talking about what some of the issues might be with continuing 
to extend these systems. There is certainly, everyone would agree, a limit as to how far you can continue 
to add function to these different graphics systems. I don't think anyone would argue that an all-things-for-all-men 
type graphics interface is going to solve a typical user's problems. And so there are some interesting 
issues associated with what we do in a PHIGS environment, at least, beyond PHIGS+. Is it appropriate 
to go beyond PHIGS+? And thirdly, Salim Abi-Ezzi will talk about one of the very important issues with 
coexistence of these different standards environments. 3-D graphics in a windowing environment. There 
is currently activity going on in the X consortium to look at 3-D graphics. People may be familiar with 
X-PEX. Salim will briefly talk about X-PEX and the approach being proposed there, and then contrast that 
with some alternatives, as well as some of his opinions. We're looking forward to having the audience's 
comments after our brief presentations as to some of your experiences and opinions with respect to these 
subjects. So I'd like to ask the first speaker, Dick Puk, please. RICHARD PUK: Thank you A1, and good 
afternoon. It's good to be here, come to nice cool Atlanta after spending two weeks in hot Tucson. It's 
kind of hard to say that. What I'm going to talk about is the fact that the international standards community 
right now is in the throws of trying to decide where they'er going to go next. There has sort of been 
a first generation of standards, work on which is just now beginning to be completed. GKS was completed 
in 1985, CGM in 1986, PHIGS in 1988, GKS 3-D in 1988. Maybe we'll finish CGI in 1989. But I'm not going 
to talk about the first generation. What I'm going to talk about is what happened at the last ISO meeting 
which concerns the approaches to be taken in the future next generation of graphics standards and what 
we need to do to get there. At the meeting, we spent quite a bit of time doing all of these things. We 
spent some time talking of formulating procedures, doing some reference model work, figuring out what 
to do about study periods, study groups. It's almost time for the five-year maintenance cycle on GKS, 
so what do we do there? And there was some interesting work going on in Metafiles. So let's take a look 
at each of these in particular. What we were trying to do with the procedures was to solve some of the 
problems we had in the first generation of standards. Now many of you -- and rightly so -- complained 
about the speed at which standards development seems to progress. Or maybe I should say the lack of speed 
at which standards seem to progress. Standards formulation is very hard because what you're trying to 
do is get consensus from people whose backgrounds are very diverse, both culturally and computer graphics-wise, 
and we spend half of our time just trying to understand each other. And once we understand each other, 
then we spend the other half of our time trying to get agreement. And this is very slow. So what we're 
trying to do with the procedures is to, in essence, automate that process, and also to formalize the 
process so that we have some immediate understandings from an administrative point of view fight up front 
and we won't be spending all of our time trying to decide which standards ought to be compatible, or 
how compatible with other standards. So we've, in essence, identified some procedures that we're going 
to try and formulate in more detail. In particular, the first one is the acquisition and specification 
of requirements. There was a very heavy emphasis at the meeting on bringing a much more formal specification 
of user requirements into the standardization process. I think that there is a general recognition, certainly 
within SC-24 -- that's the international standards group --and also within X3H3, that user requirements 
are something that we have treated perhaps too informally in the past and that we should indeed try to 
be considerably more formal in the future. To go along with this, we feel that we need to have some formal 
procedures for identifying exactly what the constituency for a particular standard or a new work item 
for a standard should be. A new work item also has to define its own relationship to a computer graphics 
reference model. And it also must have an initial architecture up front so that we can figure out -- 
so that we can get a general idea of where it's going to go and what it's competing with, if it's competing 
with anything. And the feeling is that a new work item ought to have an initial draft to .go along with 
it so that there is some starting point from which we can depart without having to invent a totally new 
document. Perhaps the most important work that was accomplished was more, and fairly productive work, 
on developing a reference model for international standards. There is a work item for reference model 
work already, and the end result of that work item will be -- or is supposed to be -- an international 
standard in much the same way that the open system interconnect reference model is an international standard. 
The reference models we've come up with, or the concept, is to have both an external and internal reference 
model, and we're preparing a working draft which will describe both of these. So we need to think about 
that. I encourage more people to work on reference model work. I encourage your companies to support 
it. Well, in order to identify what new work items to push through these new procedures that we're formulating, 
ISO has come up with a new technique called a study period. And the .purpose of a study period, as you 
can see on the slide, is to identify needs and prepare new work 1terns and specific topic areas. And 
at the meeting, we approved five topic areas for study period work. One of these is a new application 
programmer interface for the GKS constituency, with the intent -- which I think is totally unrealistic 
-- of having a new standard in the 1993 time frame. That's only five years from now and I'm not sure 
that we understand GKS well enough to do that. But that's the approved goal for this study period. There 
are also study periods for windowing environments, image analysis, extensions to PHIGS --this is really 
PHIGS+, plus some other stuff, possibly. The study period was not limited to looking at PHIGS+, but the 
expectation is that the study period effort will result in a recommendation that PHIGS+ be an official 
standards project. Meanwhile, we are trying to obtain a new draft of the PHIGS+ document in a form that 
is more appropriate for standardization. Although the current draft is quite nice. In addition, there 
is a study period for extensions to CGM. There are people who say we need a 3-D static picture capture 
metafile and people who say, well, we need something that can represent engineering drawings that have 
these funny connectors at the line joints and so on. So there's a study period underway to look at needs 
in this area. Also approved were some study groups. Now study groups are contrasted to study periods 
in that study groups are really very narrow and they go after a very specific topic. Their purpose is 
to prepare technical reprots which provide material that can be used by standards development groups. 
And we approved four study groups. One for improving the text model. There has been a lot of criticism 
about the text model used in the current generation of standards, especially that the text primitives 
are not self-sizing. They don't specify their own extent. You have to go out and get other information 
in order to figure out how big the text is going to be. There has been some thought that the input model 
needs improving because the current input model doesn't provide you with the intimacy that current windowing 
systems and other current workstation-like graphics is providing -- just in a natural fashion. So there 
is going to be a group looking at that. There is also going to be a group looking at impact of windowing 
environments on graphics standards. And the idea here is to study windowing environments, study the current 
standards and see what we need to make the current standards fit in that environment. An example might 
be sizing commands so that you can tell a window to change its size. And finally, there has been a study 
group that has been approved for product data geometry. Now this has a particular impact on the PHIGS+ 
work because PHIGS+ has some new primitives in it, nurbs and curves of other kinds and quadrilateral 
and triangular meshes and so on, and we're not sure that those are the right primitives or the only primitives 
of those particular kinds that need to be included in PHIGS+. So the study group is intended to not only 
look at those, but look at other possible things that need to be considered. There is also maintenance 
activity underway for GKS. Because of this five-year cycle, we have to start some work. Right now, the 
intent is to limit the GKS revision to be just fix-up type work and extensions in the area that GKS 3-D 
has already been extended, and perhaps some of the CGM primitives and other functionality that might 
be usable in a GKS environment. The result of this work is supposed to be a revision to GKS that is fully 
and strictly upwards compatible. There has also been some talk about adding some addenda to GKS. An addenda 
is a new standard which becomes a new part of an existing standard. So there has been some need expressed 
for a GKS audit trail metafile, a GKS-M, so to speak. Actually, a formalization of Annex E. A CGM 3-D 
static picture capture metafile, which may or may not be an addenda to GKS, but could be an addenda to 
CGM. And PHIGS+ is probably going to be treated as an addenda to PHIGS. 4 And finally, in the metafile 
area, there are two types of metafiles that were identified at this meeting, and this is sort of a reference 
model concept that there was suddenly a realization, and the light bulb lit above the head of everybody, 
and because of this, it's sort of a fundamental understanding of metafile. If you take a look at a functional 
component, for instance, an API or a workstation level interface or a device level interface or maybe 
even a display, if you capture the input to that functional processing step, that is called an audit 
trail. If you capture the state of the functional component at some point in its execution, that is called 
state capture. And there has been agreement within the international community that these are two separate 
and distinct types of metaf'des and they differ quite a bit in the effect and what you try to encode 
into them. An agreement was that the GKSM metafile, for instance, which is a 3-D workstation level metafile 
is really an audit trail metafile and it is not a picture capture metafile. On the other hand, the CGM, 
as it currently is defined, is a state capture metaf'fle at the device level. Well, given all this additional 
work that the standards group has laid out for themselves, we have come to the conclusion that there's 
a lot of work to do. We're trying to get ourselves a little bit more formally organized. And I'm not 
sure we're going to make it, frankly, because we need some fresh blood. I encourage all of you who are 
interested in this to consider participation in the standards activities. We are getting into some much 
more complicated standards, for instance PHIGS+, and we need different kinds of talent on the committees 
to help us to standardize these areas. So I would encourage you to consider participating in standards 
work. These are your standards. We need your input. The next speaker is Greg. GREGORY D. LAIB: Today 
I'd like to present some of the work that has been done toward defining practical extensions to the PHIGS 
standard. In particular, I'll present a brief overview of the PHIGS+ effort and its current status. Following 
this, I will share with you some of my thoughts in regard to PHIGS extensions in general. This will include 
functions that are appropriate for inclusion in PHIGS, when they should be considered for inclusion as 
well as potential pitfalls in defining extensions to the standard. These points are based on some real 
world experience of mine, having participated in the development of a programming interface based on 
PHIGS and having worked with developers of production applications to use this interface. About 18 months 
ago, a group of graphics experts from many different companies began work on a straw man proposal for 
adding shading, lighting, and complex primitives to the emerging PHIGS standard. The motivation for this 
effort stemmed from the emergence of hardware that could perform this function. It was feared by many 
of the vendors that each one of them would define a proprietary interface or proprietary extensions to 
PHIGS to provide applications with access to the new hardware capabilities. These unique interfaces would 
have negatively impacted the portability of applications as well as the acceptance of graphics standards. 
PHIGS was chosen as the base because it appeared to have the best framework for addition of this function 
as well as having received the widest acceptance at that time from the industry. The PHIGS+ charter included 
some very specific goals so that it would not be viewed as being competitive with the formal standardization 
process. One of these goals was that upward compatibility must be maintained so that applications written 
to PHIGS could also be run on PHIGS+ implementations. The second goal was that functionality considered 
and rejected by the PHIGS committee could not be readdressed by the PHIGS+ effort. The original goal 
of the PHIGS+ committee was to have a final specification complete within one year. This was really quite 
aggressive due to the diverse backgrounds of the people involved as well as the lack of any prior effort 
in this area. The formation of the committee started in October of '86. Meetings were held approximately 
every two weeks to try to get an initial draft of some sort of specification out. An early draft of the 
functional specification was distributed to the ANSI PHIGS committee to get early comments back and to 
also keep them abreast of what we were doing. Subsequent iteration on the definition of the function 
proceeded through the spring and summer of '87 resulting in a specification that was disctributed at 
SIGGRAPH last year. At this time, there was also a start of a public review process which lasted until 
approximately October. Comments were then processed through subsequent meetings, the last of which was 
in late October '87. The final specification, which we called PHIGS+ Version 3.0 was completed in January 
of '88. Now, to go through a brief overview of some of the functionality included in PHIGS+. Basically, 
this slide shows the general categories of functionality that were added to PHIGS. It includes high level 
primitives, shading and lighting capabilites, depth cueing effects, prunning and culling of structure 
networks, and some enhanced primitive attributes. Let's go into a little more depth. The high level primitives 
that were added can be divided into two groups. The first group provides a more compact form of the geometry 
specification and also includes some additional information that can be used by the rendering pipeline 
to produce more realistic effects. Some of this optional data includes vertex color and normal information. 
This first group also provides more convenient geometry forms -- includes things like polyline set, which 
is an extended polyline primitive, and an extended fill area set primitive. Both of these new primitives 
accept additional rendering data. A couple of other new primitives, quadrilateral mesh and and triangle 
strip have also been introduced. A quad mesh is a two-dimensional array of vertices where each four adjacent 
points are combined to form a quadrilateral. The final primitive in this category produces a polyhedron. 
The second group of primitives were added to extend the direct forms of geometry that could be represented 
by a PHIGS primitive. These includ parametric curves and surfaces. Nonuniform B-splines as well as uniform 
B-splines and Bezier basis are defined for these primitives. Hooks have been provided so that an implementation 
can provide additional bases if they so choose. Both rational and nonrational forms of these curves may 
also be supported. One of the key primitives added was something we call a trimmed nurb surface, and 
this is a way to render only a portion of a surface, or a portion of the parameter space of the surface. 
This is commonly used in any application area that requires the intersection of surfaces. The automotive 
industry, the aircraft industry, and probably even a lot of the solid modeling work that uses an explicit 
representation can use the trimmed surface representation. The next area of functionality that PHIGS+ 
added included some enhancements to allow shading and lighting calculations. The shading that is directly 
supported or recommended in PHIGS+ includes fiat, Gouraud and Phong shading. The lighting model that's 
recommended by PHIGS+ includes ambient, diffuse, and specular components, and the types of light sources 
that we have introduced include ambient, directional, positional, and what we call spot lights. One of 
the goals of PHIGS+ was to try to allow implementations some flexibility in the types of things they 
can add. These are two of the areas in which there is quite a bit of flexibility for the implementation 
It can add other shading types or other light source types. Some mechanisms were also introduced to PHIGS+ 
to allow applications to specify that a branch of a structure network could be traversed or conditionally 
traversed, or bypassed, if an extent specified by the application after all the transformation is completely 
clipped, or is smaller than a certain size. This is commonly referred to as pruning and culling. In addition, 
primitives whose geometric normals point away from the observer may be rejected and bypassed from subsequent 
rendering also. This is commonly referred to as backface culling. Some of the enhanced attributes that 
were added include the direct specification of color so the color can be specified by the direct color 
components within color space, as opposed to through an indirect method using a color index as in PHIGS. 
Area defining primitives can be treated as one sided or two sided. Two sided primitives have an entirely 
separate set of attributes fro one sided primitives. Area defining primitives can also have additional 
properties associated with them such as specular reflection, color, specular coefficient, as well as 
concentration exponent. The application also has some control over the quality that curves and surfaces 
are rendered, which is sometimes called tessellation. And finally, a new interior style has been added 
that allows new interior styles to be easily added for the rendering of interiors of surfaces. The only 
method here that's been defined so far allows the display of iso-parametric curves for the interior of 
a surface. The current status of PHIGS+, as I stated before, is that the final specification was completed 
in January of this year. Essentially, upon its completion, the PHIGS+ committee was disbanded because 
it had completed its work. As you have probably seen on the exhibition floor, there are many vendors 
now providing or working on PHIGS+ implementations. The formal standardization process, both in the United 
States and in ISO, the international organization, is starting to address the functionality defined in 
PHIGS+. It appears that the earliest possibility for this functionality or this level of functionality 
to be provided in a standard will be in early 1991. However, it is possible, although somewhat aggressive, 
I think, that a stable draft will be available by April of '89. There are many, many other areas of functionality 
that were considered for inclusion in PHIGS+. But a lot of these were deferred for later consideration, 
either because of their complexity or because of lack of a clear understanding of the application requirements 
in those areas. Basically the PHIGS+ committee tried to constrain its work scope to those areas in which 
the application requirements were well understood and the algorithms for them were also understood. One 
of the areas that we discussed a lot but didn't proceed too far on is that of trying to add photo-realism, 
or more realistic rendering to PHIGS. In PHIGS+, it was agreed that only local lighting effects would 
be included. This was so that each primitive could be processed atomically or individually, thus simplifying 
a lot of the semantics in the interface. But for photo-realism or for more realistic image generation, 
we would have to include all the object-object interactions, to obtain reflections, refractions, transparency, 
as well as other things, like depth of field effects and motion blur, for instance. In addition, some 
of the other things that were considered: we looked at at least one proposal each for anti-aliasing and 
for the display and processing of image data. But we decided to defer these so that we could concentrate 
more on the shading and lighting aspects which seemed to be more important for our constituency at that 
time. But recently, in the standardization process, there's been a group formed to start to address the 
area of image display and processing. Also some new primitives were discussed, but deferred. These included 
such things as triangle surface patches, as well as solid primitives. We even discussed whether we should 
include some CSG operations, such as, intersection, union, and difference. Another area that we looked 
at which seems to be required for high-quality rendering is called call-back mechanisms. These were discussed, 
but agreed that they would be too hard to define and probably implement. There were some other extensions 
we tried to define or looked at for data model extensions. These included allowing a portion of a primitive 
to be modified without deleting and re-creating the entire primitives, as is currently required in PHIGS. 
With the introduction of curve and surface primitives, it will be very common for the application to 
want to replace an individual control point within a curve or surface. So this functionality would be 
very nice if it's provided. We also looked at approaches for building high-level primitives from lower-level 
primitives, such as points and lines. And the sharing of geometric information between primitives was 
also discussed in an attempt to try to minimize redundancy and facilitate easy modification of multiple 
primitives. As you can see from the previous list, there are a lot of candidates for inclusion in PHIGS. 
This is only really a beginning list; there are other areas that are being considered, and talked about 
even in the standards area. But I think before we attempt to add these items, we should always ask ourselves 
a couple of key questions, such as, can functionality really be added to PHIGS, to the PHIGS model. Will 
it really fit into the PHIGS model? What ramifications could the addition of that function have on the 
use of PHIGS by other application areas, is it going to affect performance for them? And when is this 
functionality mature enough to actually be included in PHIGS? These are some very important questions 
that we must ask whenever the addition of new function is considered. Before addressing the extensions 
any further, I'd like to make two key points, about base PHIGS itself. First, standard PHIGS is sufficient 
for many application areas. It's sufficient for electrical applications, many of them, as well as 2- 
and 3-D wire frame and even some scientific visualization. The second point that we should understand 
is that, for some applications PHIGS+, and PHIGS, may not be the optimal interface. This isn't to say 
you couldn't use PHIGS for it, but it wouldn't be the optimum solution. For instance, take an area such 
as simulation or the display of multiple frames of simulation data where you may be looking at the results 
of finite element analysis that depict the deformation of some object. In PHIGS, all the data must be 
put into the PHIGS data structures before it can be displayed. For simulations like this, you're only 
going to be viewing each frame one time, so the creation of the PHIGS structure hierarchy, and so on, 
is really not needed in this case, and would probably degrade performance to some extent. The important 
point I think here is that we have to understand and believe, that PHIGS should not try to be everything 
for everybody, because it will die of its own weight if we do that. Extending PHIGS -- let's look at 
some of the tradeoffs in extending PHIGS. First, from a functional perspective. One of the goals of computer 
graphics is to produce machine generated images that rival photographs. There are actually other important 
trends, but this seems to be one of the most popular areas today. But when is a picture actually real 
enough. I think this varies from application to application, from user to user, and even for a given 
user, I believe that it will vary depending upon the task that he's currently performing. For example, 
if a user is trying to modify the geometry of a part, he'll want the interactive performance much more 
than the realism. And he'll only use the realism, or increased realism, up to the point where he perceives 
that the performance is unacceptable. In addition, it's not really clear that a realistic image projected 
onto a 2-D screen is really the easiest way in which to modify a model, especially the geometrical aspects. 
I think it's the user who should actually be able to make the tradeoffs between visualization mode --which 
I mean whether it's wire frame, hidden line removal, hidden surface removal --and the other aspects such 
as realism and performance. The challenge in extending PHIGS is then to add function in such a way that 
the application and user can make these tradeoffs. Also, the function must be added in such a way that 
users who do not require that functionality do not pay a price for it in terms of performance. There 
are many conflicts in trying to satisfy both ends of the functional spectrum -- both the very low end 
and the very high end --with one standard. For example, at the very high end, many of the techniques 
for realistic image generation involve the application very closely. In particular, for the some of the 
high quality rendering, in the middle of rendering a primitive, the application may want to receive control 
to actually modify the color beyond what the rendering pipeline has already performed through all its 
transformations and calculations. This requires some form of call-back mechanism, which is difficult 
and may be impossible to add into the PHIGS model, and would drastically --even if it could be done --change 
implementation approaches and this could possibly affect other application areas that really don't require 
the realism. In my opinion, PHIGS should not address the extremes of the functional spectrum. Of course, 
the spectrum continues to grow or widen as technology and hardware progresses. Therefore, PHIGS will 
also continue to grow and should grow. And for the very ends of the spectrum, I believe specialized interfaces 
may be appropriate. I've already touched on some of the performance tradeoffs in extending PHIGS. There 
are a few important points that we must remember about performance and its relationship to standards. 
First, the acceptance of standard efforts in the area of computer graphics have been hindered by performance 
or, more appropriately, the lack of performance. As I stated before, there are difficult tradeoffs between 
having enough function versus having enough function but not enough performance. Typically, standards 
include much more functionality than is required by any single application. It is also my experience 
that production applications will not use a high-level interface like PHIGS if function and performance 
are not close to what they get by going more directly to the hardware. To understand this, the real reason 
for it is user acceptance and competition. If an application developer who is selling the software believes 
that he can maintain or achieve some competitive edge through higher performance, then he'll bypass any 
high-level interface and access the hardware at a lower level. So our objective must be to try to not 
make him pay a big price for using the high-level interfaces. Finally, I don't think we've reached a 
plateau yet in terms of hardware and software performance which exceeds the model and scene complexity 
that many application areas and users are demanding. Therefore, we cannot afford to be careless about 
adding new functionality at the expense of decreased performance for other application areas. In summary, 
PHIGS can be successfully extended to encompass new areas of functionality, as shown by the recent PHIGS+ 
effort. But care must be taken in defining new extensions due to the many tradeoffs involved, as I've 
touched on today. Introducing extensions can add complexity, possibly unwarranted, for many application 
areas to the program interface and implementation of it. And extensions can also decrease portability 
of applications. As I alluded to in PHIGS+, many factors were left to the implementation to decide. If 
two implementations decide to implement different features, the applications, or at least parts of them, 
may have to be changed to run on a different implementation. Function and performance must be balanced 
in order to address the widest possible application set, as well as achieve acceptance by the industry. 
And, finally, I believe that we should continue to push PHIGS forward as rapidly as possible, but we 
must be careful not to include functionality and their implied algorithms prematurely. Thank you. (APPLAUSE) 
SALIM ABI-EZZI: I really appreciate your stamina. It's a pleasure, it's always a pleasure to present 
to the elite, and there is a uniqueness in this, given that I'm the last speaker in this wonderful week, 
is really exciting. I don't think standards is a very exciting topic in a SIGGRAPH type of event. However, 
I think it's a very timely topic. And, at times, it's controversial. Specifically the problem I'll be 
addressing. I hope that this controversy will fuel us for the next hour and stimulate some discussion. 
The topic rll be addressing: 3-D graphics in a windowing environment. Very timely topic. It's not the 
extension of a graphics standard; rather the coexistence of two standards. And there's a problem there. 
Down at the exhibit floor, you've probably seen plenty of displays with windows. And some of these windows 
had smooth shaded 3-D graphics images. That's exactly the problem I'm trying to address. I should have 
had a slide that shows a few windows with some graphics, but I know you know what I'm talking about. 
The overview of my presentation is simple: I will first present the problem. Second, rll discuss a few 
solutions that are being pursued by industry. And finally, I'll conclude with my own opinion. I hope 
the discussions that will follow that will be other opinions or comments on my opinion. What is the problem? 
Two major components of technical workstations are 3-D graphics and windowing capability. And I'd like 
to fit them together. So there you hit a problem of resource sharing. There are windowing standards --not 
just one --well, proposed standards. The obvious ones are X and NEWS. There may be more in the future. 
And there are several 3-D graphics systems, as well: PHIGS, GKS 3-D, PHIGS+. And maybe some proprietary 
3-D graphics systems that users may be interested with. The problem of resource sharing here, I give 
three examples. One is sharing the display and its corresponding frame buffer. I give an example of having 
an animation in one window and that window is partially covered. So there is an interesting problem there, 
because a 3-D graphics engine is generating the frames in real time, however, some other window is partially 
covering the 3-D graphics window. And that other window is under the control of the windowing system. 
That's the essence of the problem. Another form of the problem is sharing input. You have one physical 
mouse. In one instance it's doing pick of the 3-D model, in another instance it may be doing re-sizing 
of a window. So also I would like a mechanism to arbitrate this single physical resource between two 
different systems -- software systems if you will. And the last is a color look-up table. This is not 
a problem in case you have a true color device, meaning a 24-bit deep frame buffer. That's not the case 
in the majority of the applications or of the devices. Usually you have a color look-up table, with a 
limited number of entries, and you'd like to share that across different windows. So that's a problem 
of concurrent resource sharing. I'm approaching the problem from a general point of view in the sense 
that I already mentioned a number of windowing systems, a number of 3-D graphic systems, and there are 
other systems as well, such as image processing that contend for the same resources. And also I view 
image processing as a separate system. And keep in mind that there are different communities of experts 
working on these different systems. What's X 3-D? It's an industry committee that was formally established 
June of last year. There was a meeting at M.I.T., around 60 attendees. And some interesting results came 
out of that meeting. One thing I'd like to mention is that the intention of X 3-D is just to pursue solutions 
in an X environment. At the time, three proposals were presented. X-PEX proposal, X 3-D GI proposal, 
peer-to-peer interface proposal, and I will spend some time comparing and contrasting between these different 
proposals. X 3-D itself, as a committee, is an umbrella under which there could exist different task 
groups each pursuing one of these proposals. At this point, only the X-PEX task group exists and is active. 
This is the only proposal that is being pursued. What are the status of the different approaches to the 
solution. X-PEX, as I said, is heavily worked on. Currently there is a revision 3.10 document which is 
fairly clean and fakly stable. There is an intent to come up with a public implementation. I think the 
X consortium is close to making a decision as to who to contract for the public implementation of X-PEX. 
X 3-D GI: there was a proposal by Ardent that was submitted but never got any momentum. Peer-to-peer 
--you see that implemented in different graphics packages. Nearly every single product that you had on 
the exhibit floor that had 3-D graphics with windowing, used some sort of peer-to-peer interface. I do 
know that there was an X-PEX demo, but by excluding that, nearly every other package followed the peer-to-peer 
approach. Again, I'll explain what that is. Here I give examples. SUN PHIGS with SUN VIEW for example. 
That chose some peer- to-peer interface. Vax PHIGS with X windows, R-PHIGS --which is Raster Technology's 
PHIGS with NEWS, also used some sort of a peer-to-peer interface. This is an adaptation of a chart that 
was suggested in the June meeting. It basically shows which portion of the graphics pipeline is ported 
to the X server, based on the alternative approaches. So as you see, it starts with the application which 
is running some simulation or has some data base. Then running a visualization process of that data base. 
So the yellow boxes are processes while the white boxes are depositories of data. So you see that, after 
the visualization step, you're generating graphical data, such as PHIGS structures, which in turn, gets 
traversed, transformed, clipped, and finally, it gets rendered into a frame buffer. Notice that what 
X-PEX is doing is basically using the X extension facility to add PHIGS to the X server. Basically the 
PHIGS/PHIGS+ functionality. And that's why I labeled it X- PEX. So the X server takes over after the 
visualization step. The storage of the PHIGS data plus processing of that data now is part of the X server. 
So that's what this chart is trying to show. In contrast to that, the X 3-D GI approach takes over after 
transforming and clipping and basically handles the rendering. While the peer-to-peer allows the graphics 
system to do the whole pipeline, including the rendering, and then worries about pixels. Basically, it 
takes pixels from the graphics system and incorporates them with the X server. Here I list some of the 
positive aspects of the X-PEX approach. And again, the X-PEX approach lumps PHIGS with the X server, 
and extends X with PHIGS/PHIGS+ functionality. So that's an implicit solution to the concurrency problem. 
You still have to worry about the concurrency problem. It's just that now it's within one entity; namely 
the X server, because that server now exclusively owns the frame buffer plus the mouse. One nice feature 
of the X-PEX approach is that it extends X's network invisibility features to PHIGS+. And that network 
invisibility feature is through a reliable byte stream. So, automatically, such a feature allows distributed 
applications over a network to exploit PHIGS functionality. And that's sort of nice because you'd like 
your application to be distributed and still be able to use the PHIGS functionality. Another nice feature 
of the X-PEX approach is that it extends X's availability to multiple concurrent applications to PHIGS+ 
applications. So X is available to distributed applications on a network as well as to concurrent applications, 
whether they're distributed or centralized. And these very nice features now are automatically available 
to a PHIGS user. That's nice. Let's jump to the negative features of the X-PEX approach. One conceptual 
problem with the approach is that it's lumping two gigantic systems together. And they are two different 
systems. The only relationship between them is the problem of concurrency. I don't think this is enough 
justification to lump them together. PHIGS itself includes around 350 routines, PHIGS+ adds another few 
hundreds to that. So you end up with a gigantic set of extensions to the X server. That's my problem 
with the approach. Another thing about the X-PEX approach is that it is a costly process. There's a very 
hard- working team that's been working on the spec since June, basically, and even before June, to come 
up with a draft document. So it's a very costly approach. And it only solves an instance of the problem. 
It solves the combination of X with PHIGS. It doesn't address NeWS and GKS and all these variations of 
the general problem as suggested. Now I'd like to pull back that note I made previously that different 
teams of experts work on different systems. You have the windowing team of experts and you have the 3-D 
graphics team of experts. By lumping the two systems together, there's a lack of objectivity on the side 
of the team that's doing the extensions. We saw the effect of this in the form of trying to fix PHIGS 
while adding it to X. This is a natural form of behavior. We all know that PHIGS is not perfect, but 
we'd like not to add a new standard on the list. We'd like to just add it as is. And we had some problems 
with this. Another thing is that setting policy, which is very related to item number two, in the sense 
that an X user may be fully satisfied with GKS 3-D. So by only having something like X-PEX, you're forcing 
the user to use PHIGS, and its policies. Ironically that is something that the X community tried to stay 
away from. It tried to stay away from setting policy. X 3-D GI, in contrast, is a very low-level set 
of extensions to X windows. Basically adds around 15 routines to X, using the X extension mechanism. 
And by that, it maintains the ownership of the frame buffer by the X server. So the graphics system, 
again, is still doing the traversal, the transformation, and clipping of the data. Its just that the 
end rendering step is now part of the X server. X 3-D GI, again, does not handle the input, does not 
address the input at all. And it is a dead proposal at this point because it never gained momentum. Now 
peer-to-peer approach, which is my favorite and I will speak for. The essence of it is that it keeps 
the two systems separate. It simply defines the protocol of interaction between the two systems, and 
that protocol could be extended to other systems such as image processing. And it's a very clean approach. 
That's an example of the situation with a peer-to-peer interface. Here we have a number of applications 
distributed over a network, and the yellow boxes are entities within the workstation that the scientists 
or the engineers are using. And notice that such an engineer, or such applications have a need for a 
combination of these three systems. There's no question about the simultaneous need for these systems. 
I'd like to definitely have windows. One window I'm doing 3-D graphics and I may take a 3-D synthesized 
image and do some image processing on it. So I'm simultaneously in need of these three boxes. However, 
the peer-to-peer approach keeps these boxes separate. What you see here, which is in red, doesn't show 
very much, is an abstraction of this protocol I'm suggesting, I'm proposing. It is a protocol that may 
have a master/slave relationship --the windowing system plays the role of a master, and the other systems 
are as slaves. And they interchange pixels in one way or another. This is a very low-level type of protocol. 
If you're attempting to define a standard protocol that would work across different windowing systems, 
different 3-D graphics systems, and other types of systems, then you want to be careful and define it 
in the logical sense; you don't want to dictate technology because, again, it's very low level, it's 
a very low-level piece of hardware. There are different ways to implement such a protocol, and things 
like you hear from Stellar, the concept of virtual buffers, virtual frame buffers. It's a very powerful 
concept for such a solution. Because you want to make sure that you define it in a logical, high-level 
implementation, independent sense. And I think it is possible. I did get a chance to interact with people 
this week on this idea, and I got a lot of positive comments. In fact, Ardent does support a peer-to-peer 
approach. And what's happening is that each company coming up with it's product, have to solve these 
problems; there's no way around it. And each is doing the peer-to-peer interface in their own way, and 
solving the same problem. So that's my concluding slide. I presented the problem of coexistence of windowing 
and 3- D graphics, and the problem is of concurrent resource sharing. And then I showed that there is 
a range of solutions to deal with this problem. And finally I spoke for the peer-to-peer approach because 
of its simplicity, cleanliness, and this is the most general. Thank you. (APPLAUSE) ALBERT BUNSHAFT: 
We'd be happy to take any questions from the floor. Q: My name is Lou Leopold, I'm with Perceptronics, 
out in Woodland Hills, California. First I'd like to state an observation and then throw out an open 
question to the panel. My approach in generating application programs in 3-D graphics has been to use 
standards as a guide to rapidly prototype my application but not to drive it. Once it's been implemented, 
I've always tuned my application to performance, given whatever resources I've had available. Now, with 
that in mind, I'd like to throw out my question: I'm a little concerned with something that I had heard 
earlier in the week with regard to PHIGS. I understand that the PHIGS archive format has been changed 
substantially between the October 1987 draft and the finally-accepted standard in 1988. Could someone 
elaborate, at a high level, if this is correct, number one. And how they would affect my application, 
based on the October 1987 draft to the new draft. DICK PUK: The archive file format was cleaned up considerably. 
However, the effect of it is probably not going to be very much on the 1987 draft. The same items are 
in there. What we did is we added some separators where there was some confusion in how to parse the 
clear text. And we cleaned up the documentation of part 3, the format of the document. However, I do 
not feel that there are substantial changes to the content of that particular encoding, or of the archive 
file in general. Q: Thank you. Q: Mark Hatrick, Ardent Computer. I have a question for Salim regarding 
the peer-to-peer approach, which indeed we do use. One of the nice things about it you've pointed out. 
But one of the problems that many people have seen is it's frame machine specific. Have you any ideas 
of how that might be resolved? SALIM ABI-EZZI: I agree that the solution is very hardware specific, and 
that's why I stressed the point that any solution has to be in logical terms. And I agree it's not a 
simple problem. But it is do-able, from a first look at it. Q: Jon Meads, I'm with Bell Northern Research. 
And I will take objection to the peer-to- peer approach. It may be do-able, but it's a complete hack. 
It's the results of people not having an adequate reference model to understand what the functionality 
of the parts of the system are. As a matter of fact, the name of it -- peer-to-peer -- is an obfuscation 
and a lie. You're not dealing with peers; you're dealing with two systems or subsystems which have totally 
different functionality requirements and needs. A good reference model would place them in perspective 
and you'd be able to develop your systems appropriately. 14 But one is an image drawing package and the 
other one is a screen control package -- they are not peers. The concept that you think they can cooperate 
along that line is something that's bound to fail in the long run in practical reality. You cannot have 
an application, working on a peer-to-peer basis with an operating system, which is effectively what the 
window system is, for the user's screen. SALIM ABI-EZZI: I don't agree with you, John, that it is not 
a realistic approach. Simply because, just look at the products that exist at this point. And if the 
products, existence of these products prove anything, is they prove the simplicity of the approach and 
the success of the approach. In terms of a reference model, I did suggest one. And in terms of the difference 
between the peers, I did suggest that by saying that one is a master and the other is a slave. So they 
are peers, but they're not on equal grounds. There is a master and there is a slave. But for cleanliness 
sake, they are peers. JON MEADS: I believe that you'll find that you'll always be doing an individual 
implementation on a peer-to-peer interface, and you'll have device-specific and system-specific systems. 
You won't have any that's universal or really allows for portability of code. DICK PUK: I think one of 
the reasons why the peer interface is what is currently available is because it is easy to implement 
and these are companies, in many cases, who need to get something up quickly because they've got to start 
earning revenue to pay back their venture capitalists. In the long run, what you want to do is abstract 
your data at a much higher level so that the transmission of that data across the --what is in essence 
-- the bottleneck of the system, the network, is to compact it into a much higher-level representation 
and then re-expand it once it gets down below. ALBERT BUNSHAFT: One comment I would have is that this 
is a perfect example, I think, of a comment that I made in the introduction with respect to these standards 
evolving sort of out of different communities. I think that no one would argue that a standards reference 
model is not necessary. Unfortunately, standards have grown up in different communities without an agreed-upon 
reference model. One comment I would make about X windows is that it's being referred to as an operating 
type system or something at that level. Unfortunately, I think it didn't go quite far enough in separating, 
in its own definition, the window management type capabilities it provides from the window content. And 
if we could do that, or if X more cleanly defined that, we might have a better model to fit these graphic 
systems in with the windowing system. Q: David Arnold. I'm ISO SC-24 WG-3 Convenor of Metafiles and Interfaces. 
I support, wholeheartedly, what you say about separation of intent. And it comes back, actually, to the 
talk about reference models. The one thing I think was missing from Dick's report on the Tucson SC-24 
meeting was that I think substantial progress was made on reference model work. And a very different 
approach to defining both the reference model and standards in the future was presented, based around 
the notion of components and framework. Now, I feel that has a substantial beneft, both to the timeliness 
of development and also to the manageability of the process. I don't know if Dick would like to comment 
on that or not. RICHARD PUK: Well, I think you're fight and I'm glad you pointed that out, Dave. We did 
make substantial progress. There are really two types of reference models that are going to be included 
in the reference model standard. One is the external reference model, which will allow the outside world 
to figure out how it interfaces to the computer graphics world, and how the different sections of the 
computer graphics world fit together. And the internal reference model, which talks about the architecture 
of individual standards, and how all those kinds of things fit together and inter-relate. Components 
and framework was just one of the techniques used to talk about how to express the internal reference 
model. In addition, the external reference model, we came up with an initial design for that and some 
initial words to describe the concepts. We're still working on the exact words in the form of the document 
for both the external and the internal reference model. We're hoping that we'll have a draft out in a 
few months. DAVID ARNOLD: The other thing - I would like to see more emphasis put on in panels such as 
this, and that's the relationship between the formal standards process and the advantages to the user. 
In other words, the area of certification of implementations and so on. We rarely hear what the current 
status is of what you can get a certificate to say it is a conforming implementation and so on. It's 
certainly true that you can't get it for anything at the moment other than GKS 2-D. There are various 
procedures being proposed to allow certification properly of standards implementations. Anybody who is 
buying a package, ought to be asking whether or not it's really conforming. ALBERT BUNSHAFT: I'd like 
to throw out a comment that people might like to make their opinions known on before I take the next 
question. And it has to do with input. I'd be very curious to hear anyone's opinions on the input models 
provided, since we're on this subject of windowing systems versus, or in conjunction with graphics standards. 
I think one important question has to do with the input models that the two provide, and how different 
they are. I'd like to hear comments on the applicability of those input models and how people view using 
them, or view the models coexisting. Q: My name is John Bloom from Chevron Northfield Research. Just 
based on Salim's presentation, not on any real in-depth knowledge, the X 3-D GI seemed to have a lot 
of the benefits of the peer-to-peer in being easy to implement. And a lot of the benefits of X-PEX in 
sort of fitting into the reference model pretty well. And the only comments you made about it were lack 
of momentum, which hardly seems like a technical judgment. Is there a problem in the standards process 
that politics gets in the way of functionality, or is there something else wrong with the proposal, or 
why was is dismissed with such a comment? SALIM ABI-EZZI: It's partly politics, of course. As a fact, 
when it comes to standards, I think what happened was there was a mature X-PEX document at the June meeting. 
And that made people more interested with something that already existed and started investing resources 
in it and never spent the time or effort to consider something as simple as X 3-D GI. Again, it was proposed 
by Ardent Computer. JOHN BLOOM: I've heard some fairly long time lines for when X-PEX might really be 
available ... years. And the X 3-D GI is out, or could be done very, very quickly. SALIM ABI-EZZI: Yes, 
that's true. In fact, I did look at the draft proposal and it looks complete, as is, because it's so 
simple. So it does exist. But there was nobody that took it and formalized it, or discussed it. But it's 
there. But, as I mentioned, even Ardent went back to the peer-to-peer approach. There are reasons for 
that. DICK PUK: I think one of the things --I think Salim over-emphasizes the -- just to be controversial 
here, I think Salim over-emphasizes the difficulty of adding the X-PEX functionality to the server. I 
think that it would be fairly easy for it to be implemented if you already have a PHIGS implementation 
in the server. And in the case of the example implementations that are available, that were available 
on the exhibit floor, all three of those companies have PHIGS running as something in their workstations 
and I think all of that technology can be grabbed onto by the people who implement the X-PEX server protocol. 
SALIM ABI-EZZI: The problem arises -- I mean, the proof that there is a problem or that it is complex 
is the simple fact that it took so long to define it, what X-PEX is. The problem arises from trying to 
represent PHIGS entities in X terminology. You have PHIGS bundle tables. In X you have something approximately 
like bundled tables, and it doesn't work well with bundled tables. And you start hitting these conceptual 
problems. Just one point - I realized that neither X nor X-PEX is actually -- well, actually a standard 
today. X and only X and the X-11 protocol is being considered as an ANSI standard at this time. Q: My 
name's John Steinhart. I'm an independent consultant and trouble-maker at Panels. I just wanted to make 
two comments that in regards to the peer-to-peer interface, if you consider window systems to be operating 
systems, you can see a very good example of the peer-to-peer interface working well in UNIX and the relationship 
between shells and applications. So it actually does work and can work and work very well. Second of 
all, a comment on X. I see Bob Shiefler is in front of me, so I probably shouldn't make this comment. 
But I think you have to -- in my opinion, X is an unfinished piece of research that is being hurriedly 
pushed into production by a number of companies and, as a result, a number of things about its interfaces 
are unfinished and not worked out very well. And I don't think it's a particularly outstanding example 
from which to judge the entire approach. Q: I'm Mike Stroyant from Hewlett Packard. Actually, I don't 
have a question, I have an answer for the other question about X 3-D GI. There are some serious disadvantages 
in that for some vendors in that it separates a lot of the functionality on the wrong side of the X networking 
connection. You will find many vendors have hardware to do rendering from supplying control points or, 
at least, polygon points. And using that low definition did not allow the kind of hardware acceleration 
that a peer interface or the X-PEX effort will allow. Q: Bob Schiefler, M.I.T. The previous speaker 
basically said what I was going to say about X 3-D GI. So I won't comment any further on that. Previous 
comments on separation of graphics and windowing in X, I think I have two comments on that: one is, I 
think, for the most part, they are fairly cleanly separated in X and part of this complaint about it 
is, I think, a lack of understanding or having actually studied what's in X.  I also think that if you 
look at the existing graphics standards, one of the problems they've run into is believing that they 
are separate. And it's exactly this interaction between output and windowing that's caused problems with 
trying to take existing graphics standards and move them into the modem world. On the point on input, 
I think there's room for both kinds of models that are there today. certainly think that the kinds of 
high-level input that you see in PHIGS and GKS are things that applications need. But I think that there's 
need for being able to base those on lower-level input models; such as exist in X and NEWS. So I don't 
see that it's one or the other. I see one being built on top of the other. Q: I'm Mike Eckberg from Sun 
Microsystems. Is there any graphics standards work being done in object-ofiented programming methodology? 
Or will there be in the future? A: I can't answer for the future fight now. There's certainly no impetus 
apparent within the standards community. Many people feel that PHIGS is an appropriate platform being 
used within an object-oriented framework. A lot of effort has gone into insuring that the PHIGS functionality 
is sort of made up of orthogonal pieces so that it can, indeed, work that way. I would encourage the 
users who have a requirement for a standard for object-oriented type of environments, if you feel that 
your current standards are not doing the job for you, then you need to raise your voice and let it be 
known. There is nothing in the standards community that would prohibit such a standard from being developed, 
but there's got to be some pressure from the user community before anything will happen. Right now, there 
is nothing happening in this area. SALIM ABI-EZZI: I have a comment on that: I think the object-oriented 
paradigm is clearly very strong. However, the idea of PHIGS started way before, maybe eight years ago. 
So I envision any new approach to an API for graphics to be based on the object-ofiented paradigm, and 
I see such an approach as very different from the PHIGS approach. It will be a number of years before 
that happens. DICK PUK: I might add one other point. We've seen a lot of papers over the last several 
SIGGRAPH's on how to render, realistically, some very strange and beautiful things, like flowers and 
trees and rocks and waves and clouds and all sorts of stuff. Those are all objects, potentially. And 
it might be possible, some time, a long time in the future, for a standard which really is trying to 
emphasize realistic, photo-realistic images, to actually be one which, in essence, accumulates all of 
these different techniques that are being prepared. But they're still being prepared and perhaps it's 
too early to have that level of a standard. Although I believe that one will exist some time in the future, 
where the future is going to be as fast as technology drives it. ALBERT BUNSHAFT: I think it's important 
to remember that although PHIGS is a "advanced" standard as graphics standards go today, it embodies 
practices that have been being used in computer graphics since the 1960's. Hierarchical, display list 
type graphics which PHIGS provides, is not something which was invented in the 1980's. Therefore, I would 
agree with the previous comment that implies it's going to take some time for practice in this area to 
coalesce to the point of making it appropriate to pursue a standard. SALIM ABI-EZZI: Yes. I'd like to 
add that the single most important thing in the future of the object-oriented paradigms is dynamic extendability. 
For example, something like fractiles, what Dick suggested, could be added at a later time. But the tools 
are there. The foundation is there within the object-oriented paradigm. Just define a new object with 
the new methods, in this case it's a fractile. So that's a very desirable feature in a standard that 
is to live for a long period of time. ALBERT BUNSHAFT: I think an interesting subject for discussion, 
possibly, for a future time would be the appropriateness of inventing richer and richer higher-level 
type graphics standards as opposed to lower-level interfaces, as was shown in some of the charts today. 
We're not even sure we chose the right primitives for something as simple as GKS 2-D, where the text 
model is not appropriate in many cases. So I think that's an interesting and probably lively discussion 
that continues to go on. Q: I'm Don Meers. I'm with the Minnesota Super Computer Center. I'm amazed at 
the quality and the speed with which the PHIGS+ group got their report together. And I was wondering 
-- can they teach the other standards groups anything? DICK PUK: Just to comment on that, PHIGS+, unfortunately, 
is not a standard. If that had been a standards-making body, they would not have been able to make as 
much progress in the time that they had. Part of the problem has to do with a very formal way of operating. 
And standards development is, indeed, a part-time voluntary operation. Those people were working full-time 
with full company support. And if we could get our standards committees that same level of commitment 
from the companies, we'd make a lot faster progress. SALIM ABI-EZZI: I think the ad hoc activity complements 
the standardization activity in a very nice way. And the PHIGS+ experience is a perfect example of that. 
You do something that the market needs and then have the standardization people worry about it and polish 
it, which may take another three to four years. ALBERT BUNSHAFT: Okay, if there are no other questions, 
I'd like to thank everyone for attending, and hope you enjoyed the week at SIGGRAPH. Thank you very much. 
(APPLAUSE)   Gregory Laib  tl.. :,,,,ll~,,~l-~ding PHIGS -Functi~li~.~l~iii,!~; ...... Tile trend 
in grapllics is low~~r(i more realistic visualization Tile goals of some function~l domains conflict 
* Interactive modlflc~tion Performance Photo-realism * Quality ~;l~:,,clftc applicatior~ are~)~; f.t~::qu}r~i; 
spech)Hzed e~h:;t~:,f,,~, Gregory Laib   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1988</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
