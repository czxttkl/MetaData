<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/26/2010</start_date>
		<end_date>07/30/2010</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1837101</proc_id>
	<acronym>SIGGRAPH '10</acronym>
	<proc_desc>ACM SIGGRAPH 2010 Courses</proc_desc>
	<conference_number>2010</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0395-8</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2010</copyright_year>
	<publication_date>07-26-2010</publication_date>
	<pages>1132</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>In SIGGRAPH 2010 Courses, attendees learn from the experts in the field and gain inside knowledge that is critical to career advancement. Courses are short (1.5 hours) or half-day (3.25 hours) structured sessions that often include elements of interactive demonstration, performance, or other imaginative approaches to teaching.</p> <p>The spectrum of Courses ranges from an introduction to the foundations of computer graphics and interactive techniques for those new to the field to advanced instruction on the most current techniques and topics. Courses include core curricula taught by invited instructors as well as Courses selected from juried proposals.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2264987</person_id>
			<author_profile_id><![CDATA[81341494013]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[James]]></first_name>
			<middle_name><![CDATA[L.]]></middle_name>
			<last_name><![CDATA[Mohler]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Purdue University]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1837102</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>168</pages>
		<display_no>1</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Advanced techniques in real-time hair rendering and simulation]]></title>
		<page_from>1</page_from>
		<page_to>168</page_to>
		<doi_number>10.1145/1837101.1837102</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837102</url>
		<abstract>
			<par><![CDATA[<p>Hair rendering and simulation have always been challenging tasks, especially in real-time. Due to their high computational demands, they have been vastly omitted in real-time applications and studied by a relatively small group of graphics researchers and programmers. With recent advancements in both graphics hardware and software methods, real-time hair rendering and simulation are now possible with reasonable performance and quality. However, achieving acceptable levels of performance and quality requires specific expertise and experience in real-time hair rendering. The aim of this course is to bring the accumulated knowledge in research and technology demos to real world software such as video games and other commercial or research oriented real-time applications. We begin with explaining the fundamental techniques for real-time hair rendering and then present alternative approaches along with tips and tricks to achieve better performance and/or quality. We also provide an overview of various hair simulation techniques and present implementation details of the most efficient techniques suitable for real-time applications. Moreover, we provide example source codes as a part of our lecture notes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264988</person_id>
				<author_profile_id><![CDATA[81319505055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cem]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yuksel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University Cyber Radiance]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264989</person_id>
				<author_profile_id><![CDATA[81309513319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sarah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tariq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alter, J. S., 2004. Hair generation and other natural phenomena with surface derived control volumes in computer graphics and animation. U.S. Patent 6720962.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134021</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Anjyo, K., Usami, Y., and Kurihara, T. 1992. A simple method for extracting the natural beauty of hair. In <i>SIGGRAPH '92: Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 111--120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., and Rom, R. J. 1974. A class of local interpolating splines. In <i>Computer Aided Geometric Design</i>, Academic Press, Orlando, FL, USA, 317--326.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545273</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chang, J. T., Jin, J., and Yu, Y. 2002. A practical model for hair mutual interactions. In <i>SCA '02: Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, ACM, New York, NY, USA, 73--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chen, L.-H., Saeyor, S., Dohi, H., and Ishizuka, M. 1999. A system of 3d hair style synthesis based on the wisp model. <i>The Visual Computer 15</i>, 4, 159--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042390</ref_obj_id>
				<ref_obj_pid>1042201</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Choe, B., and Ko, H.-S. 2005. A statistical wisp model and pseudophysical approaches for interactive hairstyle generation. <i>IEEE Transactions on Visualization and Computer Graphics 11</i>, 2, 160--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Daldegan, A., Thalmann, N. M., Kurihara, T., and Thalmann, D. 1993. An integrated system for modeling, animating and rendering hair. In <i>Eurographics '93</i>, Blackwell Publishers, Oxford, UK, R. J. Hubbold and R. Juan, Eds., Eurographics, 211--221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>775496</ref_obj_id>
				<ref_obj_pid>775492</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Floater, M. S. 2003. Mean value coordinates. <i>Computer Aided Geometric Design 20</i>, 1, 19--27.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1384439</ref_obj_id>
				<ref_obj_pid>1384429</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Fu, H., Wei, Y., Tai, C.-L., and Quan, L. 2007. Sketching hairstyles. In <i>SBIM '07: Proceedings of the 4th Eurographics Workshop on Sketch Based Interfaces and Modeling</i>, ACM, New York, NY, USA, 31--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Grabli, S., Sillion, F., Marschner, S. R., and Lengyel, J. E. 2002. Image-based hair capture by inverse lighting. In <i>Proc. Graphics Interface</i>, 51--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hadap, S., and Magnenat-Thalmann, N. 2000. Interactive hair styler based on fluid flow. In <i>Eurographics Workshop on Computer Animation and Simulation 2000</i>, Springer, 87--99.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872905</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kim, T.-Y., and Neumann, U. 2000. A thin shell volume for modeling human hair. In <i>CA '00: Proceedings of the Computer Animation</i>, IEEE Computer Society, Washington, DC, USA, 104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566627</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kim, T.-Y., and Neumann, U. 2002. Interactive multiresolution hair modeling and editing. <i>ACM Transactions on Graphics (Proc. of SIGGRAPH 2002) 21</i>, 3, 620--629.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776362</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Koh, C. K., and Huang, Z. 2001. A simple physics model to animate human hair modeled in 2d strips in real time. In <i>Proceedings of the Eurographic workshop on Computer animation and simulation</i>, Springer-Verlag New York, Inc., New York, NY, USA, 127--138.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Kong, W., Takahashi, H., and Nakajima, M. 1997. Generation of 3d hair model from multiple pictures. In <i>Proceedings of Multimedia Modeling</i>, 183--196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508127</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lee, D. W., and Ko, H. S. 2001. Natural hairstyle modeling and animation. <i>Graphical Models 63</i>, 2, 67--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946922</ref_obj_id>
				<ref_obj_pid>946250</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Liang, W., and Huang, Z. 2003. An enhanced framework for real-time hair animation. In <i>PG '03: Proceedings of the 11th Pacific Conference on Computer Graphics and Applications</i>, IEEE Computer Society, Washington, DC, USA, 467.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Malik, S. 2005. A sketching interface for modeling and editing hairstyles. In <i>SBIM '05: Proceedings of the 2nd Eurographics Workshop on Sketch Based Interfaces and Modeling</i>, 185--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1252495</ref_obj_id>
				<ref_obj_pid>1251973</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mao, X., Isobe, S., Anjyo, K., and Imamiya, A. 2005. Sketchy hairstyles. In <i>CGI '05: Proceedings of the Computer Graphics International 2005</i>, IEEE Computer Society, Washington, DC, USA, 142--147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009577</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Noble, P., and Tang, W. 2004. Modelling and animating cartoon hair with nurbs surfaces. In <i>CGI '04: Proceedings of the Computer Graphics International</i>, IEEE Computer Society, Washington, DC, USA, 60--67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015784</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Paris, S., Hector M. Brice n., and Sillion, F. X. 2004. Capture of hair geometry from multiple images. <i>ACM Transactions on Graphics (Proc. of SIGGRAPH 2004) 23</i>, 3, 712--719.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360629</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Paris, S., Chang, W., Kozhushnyan, O. I., Jarosz, W., Matusik, W., Zwicker, M., and Durand, F. 2008. Hair photobooth: geometric and photometric acquisition of real hairstyles. <i>ACM Transactions on Graphics (Proc. of SIGGRAPH 2008) 27</i>, 3, Article 30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>865018</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Shewchuk, J. R. 1994. An introduction to the conjugate gradient method without the agonizing pain. Tech. rep., Pittsburgh, PA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>985839</ref_obj_id>
				<ref_obj_pid>985821</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Wang, T., and Yang, X. D. 2004. Hair design based on the hierarchical cluster hair model. <i>Geometric modeling: techniques, applications, systems and tools</i>, 330--359.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531362</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Wang, L., Yu, Y., Zhou, K., and Guo, B. 2009. Example-based hair geometry synthesis. <i>ACM Transactions on Graphics (Proc. of SIGGRAPH 2009) 28</i>, 3, Article 56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263263</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Ward, K., Bertails, F., Kim, T.-Y., Marschner, S. R., Cani, M.-P., and Lin, M. C. 2007. A survey on hair modeling: Styling, simulation, and rendering. <i>IEEE Transactions on Visualization and Computer Graphics 13</i>, 2, 213--234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246983</ref_obj_id>
				<ref_obj_pid>1246981</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Ward, K., Galoppo, N., and Lin, M. 2007. Interactive virtual hair salon. <i>Presence: Teleoperators and Virtual Environments 16</i>, 3, 237--251.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073267</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Wei, Y., Ofek, E., Quan, L., and Shum, H.-Y. 2005. Modeling hair from multiple views. <i>ACM Transactions on Graphics (Proc. of SIGGRAPH 2005) 24</i>, 3, 816--820.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1264166</ref_obj_id>
				<ref_obj_pid>1263551</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Wither, J., Bertails, F., and Cani, M.-P. 2007. Realistic hair from a sketch. In <i>International Conference on Shape Modeling and Applications</i>, IEEE, Lyon, France, IEEE, 33--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618822</ref_obj_id>
				<ref_obj_pid>616070</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Xu, Z., and Yang, X. D. 2001. V-hairstudio: An interactive tool for hair design. <i>IEEE Computer Graphics and Applications 21</i>, 3, 36--43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345147</ref_obj_id>
				<ref_obj_pid>345140</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Yang, X. D., Xu, Z., Wang, T., and Yang, J. 2000. The cluster hair model. <i>Graphical Models 62</i>, 2, 85--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883447</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Yu, Y. 2001. Modeling realistic virtual hairstyles. In <i>PG '01: Proc. of the 9th Pacific Conference on Comp. Graphics and Applications</i>, IEEE Computer Society, Washington, DC, USA, 295.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383555</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{AL04} Aila T., Laine S.: Alias-free shadow maps. In <i>Eurographics Symp. on Rendering</i> (2004), pp. 161--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089521</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{BMC05} Bertails F., M&#233;nier C., Cani M.-P.: A practical self-shadowing algorithm for interactive hair animation. In <i>Proc. Graphics Interface</i> (2005), pp. 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111424</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{ED06} Eisemann E., D&#233;coret X.: Fast scene voxelization and applications. In <i>Symposium on Interactive 3D Graphics and Games</i> (2006), pp. 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122370</ref_obj_id>
				<ref_obj_pid>1121991</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{GMT05} Gupta R., Magnenat-Thalmann N.: Scattering-based interactive hair rendering. In <i>Comp. Aided Design and Comp. Graphics</i> (2005), pp. 489--496.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009600</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{KHS04} Koster M., Haber J., Seidel H.-P.: Real-time rendering of human hair using programmable graphics hardware. In <i>Proceedings of the Computer Graphics International (CGI'04)</i> (2004), pp. 248--256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{KK89} Kajiya J. T., Kay T. L.: Rendering fur with three dimensional textures. In <i>Proceedings of SIGGRAPH 1989</i> (1989), pp. 271--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{KN01} Kim T.-Y., Neumann U.: Opacity shadow maps. In <i>12th Eurographics Workshop on Rendering Techniques</i> (2001), pp. 177--182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{LV00} Lokovic T., Veach E.: Deep shadow maps. In <i>Proceedings of SIGGRAPH 2000</i> (2000), pp. 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383558</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{MKBvR04} Mertens T., Kautz J., Bekaert P., van Reeth F.: A self-shadow algorithm for dynamic hair using clustered densities. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i> (2004), pp. 173--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141995</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{MM06} Moon J. T., Marschner S. R.: Simulating multiple scattering in hair using a photon mapping approach. In <i>Proceedings of SIGGRAPH 2006</i> (2006), pp. 1067--1074.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263263</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{WBK*07} Ward K., Bertails F., Kim T.-Y., Marschner S. R., Cani M.-P., Lin M.: A survey on hair modeling: Styling, simulation, and rendering. <i>IEEE TVCG 13</i>, 2 (Mar-Apr 2007), 213--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Wil78} Williams L.: Casting curved shadows on curved surfaces. In <i>SIGGRAPH '78</i> (1978), pp. 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383936</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[
{XLJP06} Xu S., Lau F. C., Jiang H., Pan Y.: A novel method for fast and high-quality rendering of hair. In <i>Proc. EGSR'06</i> (2006), pp. 331--341.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{ZSW04} Zinke A., Sobottka G., Weber A.: Photorealistic rendering of blond hair. In <i>Vision, Modeling, and Visualization 2004</i> (2004), pp. 191--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089521</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Bertails, F., M&#233;nier, C., and Cani, M.-P. 2005. A practical self-shadowing algorithm for interactive hair animation. In <i>Graphics Interface</i>, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122370</ref_obj_id>
				<ref_obj_pid>1121991</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Gupta, R., and Magnenat-Thalmann, N. 2005. Scattering-based interactive hair rendering. In <i>Comp. Aided Design and Comp. Graphics</i>, 489--496.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283908</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Hadwiger, M., Kratz, A., Sigg, C., and B&#252;hler, K. 2006. Gpu-accelerated deep shadow maps for direct volume rendering. In <i>Proceedings of Graphics Hardware 2006</i>, 49--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Kim, T.-Y., and Neumann, U. 2001. Opacity shadow maps. In <i>Eurographics Rendering Workshop</i>, 177--182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776766</ref_obj_id>
				<ref_obj_pid>776751</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Kniss, J., Premoze, S., Hansen, C., Shirly, P., and McPherson, A. 2003. A model for volume lighting and modeling. <i>IEEE Trans. on Vis. and Comp. Graphics 9</i>, 2, 150--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T., and Veach, E. 2000. Deep shadow maps. In <i>Proceedings of SIGGRAPH 2000</i>, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Marschner, S. R., Jensen, H. W., Cammarano, M., Worley, S., and Hanrahan, P. 2003. Light scattering from human hair fibers. <i>ACM Transactions on Graphics 22</i>, 3, 780--791. SIGGRAPH 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383558</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Mertens, T., Kautz, J., Bekaert, P., and Reeth, F. V. 2004. A self-shadow algorithm for dynamic hair using density clustering. In <i>Eurographics Symposium on Rendering</i>, 173--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141995</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Moon, J. T., and Marschner, S. R. 2006. Simulating multiple scattering in hair using a photon mapping approach. <i>ACM Transactions on Graphics 25</i>, 3, 1067--1074. SIGGRAPH 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Premoze, S., Ashikhmin, M., Ramamoorthi, R., and Nayar, S. 2004. Practical rendering of multiple scattering effects in participating media. In <i>Eurographics Symp. on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263263</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Ward, K., Bertails, F., Kim, T.-Y., Marschner, S. R., Cani, M.-P., and Lin, M. 2007. A survey on hair modeling: Styling, simulation, and rendering. <i>IEEE Transactions on Visualization and Computer Graphics 13</i>, 2, 213--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383936</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[
Xu, S., Lau, F. C., Jiang, H., and Pan, Y. 2006. A novel method for fast and high-quality rendering of hair. In <i>Proc. of the 17th Eurographics Symp. on Rendering</i>, 331--341, 440.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Yuksel, C., and Keyser, J. 2008. Deep opacity maps. <i>Computer Graphics Forum (Proc. of EUROGRAPHICS 2008) 27</i>, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338591</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Yuksel, C., Akleman, E., and Keyser, J. 2007. Practical global illumination for hair rendering. In <i>Pacific Graphics 2007</i>, 415--418.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Zinke, A., and Weber, A. 2006. Global illumination for fiber based geometries. In <i>Electronic proceedings of the Ibero American Symposium on Computer Graphics (SIACG 2006)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263273</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Zinke, A., and Weber, A. 2007. Light scattering from filaments. <i>IEEE Trans. on Vis. and Comp. Graphics 13</i>, 2, 342--356.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Zinke, A., Sobottka, G., and Weber, A. 2004. Photorealistic rendering of blond hair. In <i>Vision, Modeling, and Visualization (VMV) 2004</i>, 191--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advanced Techniques in Real-time Hair Rendering and Simulation SIGGRAPH 2010 Course Notes Lecturers: 
Cem Yuksel Texas A&#38;M University Cyber Radiance Sarah Tariq NVIDIA The latest version of the course 
notes can be found at http://www.cemyuksel.com/?x=RealtimeHairCourseNotesSiggraph2010 http://www.sarahtariq.com/HairCourseNotes_SIGGRAPH2010.pdf 
  Abstract Hair rendering and simulation have always been challenging tasks, especially in real-time. 
Due to their high computational demands, they have been vastly omitted in real-time applications and 
studied by a relatively small group of graphics researchers and programmers. With recent advancements 
in both graphics hardware and software methods, real-time hair rendering and simulation are now possible 
with reasonable performance and quality. However, achieving acceptable levels of performance and quality 
requires specific expertise and experience in real-time hair rendering. The aim of this course is to 
bring the accumulated knowledge in research and technology demos to real world software such as video 
games and other commercial or research oriented real-time applications. We begin with explaining the 
fundamental techniques for real-time hair rendering and then present alternative approaches along with 
tips and tricks to achieve better performance and/or quality. We also provide an overview of various 
hair simulation techniques and present implementation details of the most efficient techniques suitable 
for real-time applications. Moreover, we provide example source codes as a part of our lecture notes. 
 Latest Version of the Course Notes The latest version of the course notes can be found at http://www.cemyuksel.com/?x=RealtimeHairCourseNotesSiggraph2010 
http://www.sarahtariq.com/HairCourseNotes_SIGGRAPH2010.pdf Lecturers Cem Yuksel Texas A&#38;M University 
Cyber Radiance Cem Yuksel is the founder of Cyber Radiance LLC and is receiving his Ph.D. from Texas 
A&#38;M University in 2010. He designed and programmed Hair Farm, a leading hair software plugin for 
3ds Max, used by various production studios and individual artists. His published research work on hair 
includes hair modeling with hair meshes, curve formulations, real-time hair shadows, real-time computation 
of multiple scattering in hair, and efficient global illumination techniques for hair. His other published 
graphics research includes methods like mesh colors for efficiently storing color data on arbitrary meshes 
and wave particles for real-time water simulation. cem@cemyuksel.com www.cemyuksel.com www.cyberradiance.com 
 Sarah Tariq NVIDIA Sarah Tariq is a software engineer on NVIDIA's Developer Technology team, where she 
works primarily on implementing new rendering and simulation techniques that exploit the latest graphics 
hardware, and helping game developers to incorporate these techniques. During her time at NVIDIA she 
has been involved in the development of several game titles for the PC, including Hellgate: London, Supreme 
Commander and Dark Void, and has helped optimize several other titles. She has presented talks at various 
conferences, including SIGGRAPH and GDC. Before joining NVIDIA, Sarah pursued graduate studies at Georgia 
Tech, where she worked on research projects including subsurface reflectance capture of skin. stariq@nvidia.com 
www.sarahtariq.com www.nvidia.com  Course Overview 8:30 am Introduction and Fundamentals of CG Hair 
[Yuksel] 8:40 am Data Management and Rendering Pipeline [Yuksel and Tariq] -Overview -Rendering Hair 
as Poly Lines -Rendering Hair as Camera Facing Polygons -Managing Dynamic Hair Data -Efficiently Sending 
Data to GPU -Generating Hair on the GPU 9:15 am Transparency and Antialiasing [Yuksel and Tariq] -Transparency 
and Blending -Depth Sorting for Blending -Avoiding Sorting -Antialiasing -Best Practices for Better Performance 
and Quality 9:30 am Hair Shading [Yuksel] -Kajiya-Kay Shading Model for Hair -Physically Based Hair Shading 
-Improving Shading Performance on the GPU 10:10 am Q &#38; A [Yuksel and Tariq] 10:15 am Break 10:30 
am Hair Shadows [Yuksel and Tariq] -Shadow Maps -Transparent Shadow Mapping for Hair -Shadow Filtering 
-Simplified Shadow Maps for High Performance 11:05 am Multiple Scattering in Hair [Yuksel] -Introduction 
to Multiple Scattering -Dual Scattering Approximation -Implementation Notes 11:25 am Hair Dynamics for 
Real-time Applications [Yuksel and Tariq] -Overview of Hair Simulation Techniques -Fast Simulation of 
Hair on the GPU -Handling Hair-Hair Interaction -Efficient Collision Detection and Handling for Hair 
-Simulating Hair with Hair Meshes 12:10 pm Conclusion / Q &#38; A [Yuksel and Tariq]  Introduction and 
Fundamentals of CG Hair Hair is known to be an extremely important visual component of virtual characters. 
However, hair rendering has been avoided or substituted by extremely simplified and often highly unrealistic 
polygonal approximation in most real-time graphics applications. With the advancement of the graphics 
hardware, we believe that the time has come to handle hair rendering properly in real-time graphics applications. 
In this course, we overview crucial components of hair rendering and simulation for real-time applications 
and discuss how high performance results can be achieved with high quality images. Human hair, especially 
when it is long, often forms an extremely complicated geometric structure. A person can have over 100 
thousand hair strands and each them is an extremely thin fiber and can form rather complicated shapes. 
Therefore, a full hair model of a person presents various challenges in efficiently handling this complicated 
structure, in terms of rendering and simulation, as well as modeling. The geometric complexity of hair 
also makes it difficult to realistically render it as a large surface, just like any other object. While 
such approaches are commonly used in practice, it is very difficult, if possible at all, to get realistic 
results with surface approximations of hair for most hair models. In this course, we do not talk about 
"fake" hair rendering approaches like these, but concentrate on properly rendering hair similar to the 
approaches used in offline hair rendering. We also explain how hair can be rendered efficiently to serve 
the needs of a real-time application. In computer graphics a hair strand is often represented by a curve 
with some thickness. This representation ignores the details of the tubular shape of hair fibers, which 
is often unimportant, because, in most cases, the projected thickness of a hair strand is less than the 
size of a pixel on the screen. With this representation, a hair model in computer graphics is essentially 
a collection of curves. Due to the large number of hair strands in a general hair model, it is often 
undesirable to explicitly model each and every hair strand. Therefore, most hair modeling techniques 
provide some form of modeling only a portion of all hair strands (key hairs) and populating the rest 
of the hair model based on the explicitly modeled ones. There are two main approaches for generating 
hair strands from the modeled subset. Single Strand Interpolation creates hair strands around each explicitly 
modeled curve based on the shape of the curve. Wisps and generalized cylinder based techniques use this 
approach for generating a complete hair model. Multi Strand Interpolation creates hair strands in-between 
a number of explicitly modeled curves by interpolating their shapes. A recent approach for modeling and 
representing hair is the hair mesh structure, which permits modeling of hair similar to polygonal modeling. 
A hair mesh is essentially a volumetric structure and the hair strands are generated inside this volume 
following the shape and topology of the hair mesh.    The other option instead of rendering hair directly 
as lines is to expand the line segments into camera facing quads That is for each line segment expand 
it into two triangles facing the camera facing quads. That is, for each line segment, expand it into 
two triangles facing the camera. This expansion is pretty easy to do in the Geometry shader. We specify 
the input primitive for the geometry shader to be a line (two vertices), from these we can figure out 
the tangent, and taking the cross product of that with the eye vector gives us the two axis to expand 
the quad: //creating the four Float3 Tangent = normalize(vertex[1].Position vertex[0].Position); Float3 
sideVec = normalize(cross(eyeVec, tangent)); float4x3 pos; float3 width0 = sideVec * 0.5 * width * vertex[0].width; 
float3 width1 = sideVec * 0.5 * width * vertex[1].width; pos[0] = vertex[0].Position -width0; pos[1] 
= vertex[0].Position + width0; pos[2] = vertex[1].Position -width1; pos[3] = vertex[1].Position + width1; 
 Rendering hair as triangle strips has a number of advantages. The hair strips can have a real world 
thickness, and this thickness can vary both amongst strands and along the length of a given strand. This 
allows us to have different levels of detail in different parts of the head, for example the hair near 
the front of the face can be thinner (with more strands) and the hair near the center of the head (these 
are strands that will probably get occluded by other strands on top) can be thicker (to better occlude 
the scalp). Changing the thickness of the hair along its length allows us to taper it towards the bottom, 
which is important for a realistic look. Finally, we don t have to worry about changing the line width 
as the hair moves closer or further from the camera. Rendering triangles can be costly however, both 
because of the additional amount of geometry that we have to either pass to or create on the GPU and 
because of the more costly rasterization.  Many of the methods used in literature and in practice simulate 
hair interactions on only a subset of the hair, which are called guide strands, and then render a larger 
number of hair strands either by interpolating between the guide strands or by clumping rendered hair 
to guide strands.    diagrams  Indexing into the attribute buffers Need additional buffers that 
map the instanceID of the rendered hair strand to the correct offset in the attribute buffers Variable 
density of hair across the head also has to be considered in this indexing Makes efficiently changing 
the amount of hair rendered (for example for LOD) hard  We have talked about how to efficiently send 
all the hair vertices to be rendered on the GPU. However, given the large amount of data that we are 
trying to render, it is actually more efficient to generate this data directly on the GPU. The newest 
generation of GPUs introduce functionality to create large amounts of data directly on the hardware; 
the tessellation engine. In this section we are going to be talking about how to use the tessellation 
engine to easily and efficiently create hair strands for rendering. Note that GPUs have had functionality 
for creating data, the Geometry Shader, for some amount of time. However, the Geometry shader is meant 
for only small amounts of data expansion, for example for expanding a line into a quad. It is not meant 
for, and is certainly not efficient at creating the amounts of geometry that we would like to create 
for all the hair strands. There are a number of reasons why the tessellation engine is useful for creating 
hair for rendering. The most important advantage is that it is faster to create data using the tessellation 
engine than it is to create data on the CPU and then upload it to the GPU, or even to render dummy vertices 
on the GPU and then evaluate them in the vertex shader as we were discussing in the previous section. 
It is also easier to create hair using the Tessellation engine than using the dummy vertex method. Using 
the Tessellation engine we can have very fine grained and continuous control over the level of detail. 
 The tessellation engine has three stages, two of them are programmable (the hull and domain shader) 
and one of The tessellation engine has three stages, two of them are programmable (the hull and domain 
shader) and one of the is fixed function (the tessellator). The Hull Shader is the first new stage and 
it comes after the vertex shader. The Hull shader takes as input a patch -an input primitive which is 
a collection of vertices with no implied topology. In this stage we can compute any per patch attributes, 
transform the input control points to a different basis, and compute tessellation factors. Tessellation 
factors are floating point values which tell the hardware how many new vertices you would like to create 
for each patch, and are necessary outputs of the Hull Shader. The next stage is the Tessellator, which 
is fixed function. The tessellator only takes as input the tessellation factors (specified by the Hull 
Shader) and the tessellation domain (which can be quads, triangles or isolines) and it creates a semi-regular 
tessellation pattern for each patch. Note that the tessellator does not actually create any vertex data 
 this data has to be calculated by the programmer in the Domain Shader. The Domain shader is the last 
stage in the tessellation engine, and it is at this point that we actually create the vertex data for 
the tessellated surface. The Domain shader is run once for each final vertex. In this stage we get as 
input the parametric uvw coordinates of the surface, along with any control point data passed from the 
Hull shader. Using these inputs we can calculate the attributes of the final vertex. To render hair 
we are going to be using the Isoline tessellation Domain. In this mode the hardware tessellator creates 
a number of iso lines (connected line segments) with a multiple line segments per line. Both the number 
of isolines and the number of segments per isoline can be specified by the programmer in the Hull Shader. 
The actual positions and attributes of each tessellated vertex are evaluated using the Domain Shader. 
The output from the Tessellation engine is a set of line segments which can be rendered directly as lines, 
or they can be rendered as triangles by expanding them to camera facing quads using the geometry shader. 
 This is an overview of a pass to create and render data using the tessellation engine and the clump 
based interpolation method. Our input is a patch, which is a section of a guide strand. The hull shader 
computes the tessellation factors for this patch, which are the number of iso lines that we would like, 
and the number of line segments per isoline. This data is passed to the tessellator. The Hull shader 
also calculates any per patch data which might be needed by the Domain Shader. The Tessellator generates 
the topology requested by the Hull shader. Finally the Domain Shader is invoked once per final tessellated 
vertex. It is passed the parametric uv coordinates for the vertex, and all the data output from the Hull 
Shader. The input to our rendering pass is a set of guide strands that have been simulated and tessellated 
(to make them smooth). The reason we choose to tessellate the strands first is that we are going to be 
using these tessellated strands for creating both types of final interpolated strands -computing this 
tessellated data once and reusing it for both types of interpolation saves time. The data that we have 
for each vertex includes its position, tangent, length (specified as distance from the root in world 
space units) and local coordinate frames. This data is stored on the GPU as buffers using structure of 
arrays format (we have separate buffers for position, tangent etc). Since we are going to be binding 
our data as texture buffers which can be sampled in the Hull or Domain shader we don t need to bind any 
data to the input assembler as vertex buffers (or for that matter index buffers or input layout): unsigned 
int stride = 0; unsigned int offset = 0; ID3D11Buffer* buffer[] = { NULL }; pd3dContext->IASetVertexBuffers(0, 
1, buffer, &#38;stride, &#38;offset); pd3dContext->IASetInputLayout(NULL); In addition, we set the primitive 
topology to be a patch with a single control point. After this we call just call draw, with the total 
number of guide strands as input. We might have to call this draw call more than one time depending 
on how many isolines and line segments we want per patch (covered in the next slide).  Dividing input 
guide strands into patches: Ideally we would like to specify a patch to be one complete guide hair (and 
the tessellation engine would then create the specified number of final hair for this patch). However, 
there is a hardware limit of maximum 64 isolines per patch and maximum 64 segments per isoline, so if 
we want to create more than 64 segments per isoline or more than 64 isolines per guide strand we have 
to partition the guide strand into multiple patches. For each patch that we render the hardware will 
create a specified number of output isolines. In this example the number of isolines created for a patch 
is based on two things the floating point LOD specified globably (calculated based on the distance of 
the head to the camera) and the local density of hair that an artist has created. This local density 
is provided as a texture that is mapped to the scalp. Each guide strand has texture coordinates that 
can be used to lookup the local density. The Hull Shader is split into two parts, the main shader which 
operates once on each input control point, and the patch constant function which is invoked once per 
patch. In our implementation we are loading control point data for a patch from a buffer, so the input 
to the hull shader is a dummy patch consisting of a single control point. Since we have only one input 
control point we are using the patch constant function for all of the computation and our main shader 
is null. This listing shows the Patch Constant Shader, which calculates the tessellation factors and 
other data for each patch. Output.Edges is the tessellation factors, its semantic is SV_TessFactor. Edges[0] 
is the amount of iso lines that we would like created for this patch, and Edges[1] is the amount of segments 
that we would like in each line. As we mentioned before, the number of isolines per patch and the number 
of segments per isoline cannot exceed 64, so the calculation of both Edges[0] and Edges[1] has to take 
this into account.  The Domain Shader is invoked for each final vertex that is created. As input to 
the domain shader we get the patch constant data and the per patch control point data that we had output 
in the Hull shader. We also get as input a number of system generated values, including SV_DomainLocation 
which gives the parametric uv coordinates of the current vertex in the tessellated patch. We also get 
the id of the patch that we are operating on (SV_PrimitiveID). Using these values we can figure out which 
vertex and which strand we are operating on. These indices can then be used to look up the guide strand 
attributes and also the random offsets that we are going to use to offset this generated strand from 
the input guide strand.  Being able to dynamically reduce the complexity of the rendered hair (and 
thus increase the performance) is very important for real time applications. The level of detail for 
hair can be based on the distance of the hair/head from the camera, the importance of the character, 
the probable occlusion of the patch or any other factor. Using the tessellation engine we can change 
the LOD per patch by changing the number iso-lines or the number of segments per line. In the images 
on the right we are showing the same hair style under two levels of detail. At the top we have the hair 
rendered at full LOD, and at the bottom we have scaled down the rendering and increased performance by 
2x. In this case we are creating and rendering less hair strands, although we are making the strands 
a bit wider so that there is no visible reduction in the density of hair. At the bottom right we show 
what that lower level of detail would look like if you zoomed in. Instead of linearly decreasing the 
amount of hair strands as LOD decreases we can also have an artist create density and thickness maps 
for different discrete LODs, and then blend between them to determine the amount of hair and its thickness 
for a particular LOD. This way we can use the computational resources available (the limited number of 
hair that we can create) in the region where the artists feel they will have the highest impact.  An 
important part of creating realistic hair is having randomness between hair strands. Without this we 
get a look that is too smooth and synthetic, as shown in the top figure. Adding randomness to strands 
gives a more natural look (bottom). Since our model for rendering hair is based on interpolating many 
children hair from a small set of guide hair, we need to introduce this randomness at the interpolated 
hair level. The method we outline here is similar to that presented by [Choe and Ko 2005]. We pre-compute 
a small set of smooth random deviations and apply these to the interpolation coordinates of the interpolated 
hair. (Without loss of generality) the slide above shows how deviations are applied for clump based hair. 
Creating Deviations We create two types of deviations for the hair. The first, applied to a large number 
of the hair strands, is small deviations near the tips. In our images we have applied this deviation 
to 30% of the hair strands. The second, applied to only a small percent of the hair strands (for example 
10%), is deviation all along the strands. This second type of deviation is what you see highlighted in 
the red box.    Tae-Yong Kim, Algorithms for Hardware Accelerated Hair Rendering Game Tech 2003 
   Satish, Harris and Garland. Designing Efficient Sorting Algorithms for Manycore GPUs Sintorn, Assarson, 
Olsson and Billeter. Radix sort of line primitives in CUDA for realtime Self-Shadowing and Transparency 
of Hair  Stochastic Transparency, Eric Enderton, Erik Sintorn, Peter Shirley, David Luebke. I3D 2010 
  High Quality Antialiasing. Tristan Lorach, 2007. http://developer.download.nvidia.com/SDK/10.5/opengl/src/FroggyAA/doc/FroggyA 
A.pdf Note that with MSAA edges of polygons are antialiased but interiors of polygons are not. Nick 
Thibieroz, Deferred Shading with Multisampling Anti-Aliasing in DirectX10. ShaderX7   Volume Rendering 
Techniques, Milan Ikits, Joe Kniss, Aaron Lefohn, Charles Hansen. Chapter 39, section 39.5.1, GPU Gems: 
Programming Techniques, Tips, and Tricks for Real-Time Graphics(2004). This technique allows us to accumulate 
the shadowing from the light at the same time at that we are alpha blending the slices Since hair strands 
are very thin, when shading hair we assume that the projected thickness of a hair strand on the screen 
is smaller than the size of a pixel. With this assumption, we ignore the shading variations along the 
thickness of a hair strand and compute the shading function for the whole thickness of the hair strand. 
Therefore, we cannot use the surface normal of a hair strand fiber for shading hair with this assumption. 
Instead, we use the tangent direction of the hair strand and consider the hair strand as a thin tube 
that is aligned with this direction.                Bounces between         
      Iteratively satisfy distance constraints between Iteratively satisfy distance constraints 
between vertices To satisfy one distance constraint we need to modify positions of the two vertices it 
connects To satisfy many springs in parallel we partition VB into disjoint sets of vertices by using 
two index buffers IndexBuffer 1: (0,1) (2,3) (4,5) . IndexBuffer 2: (0,0) (1,2) (3,4) . One iteration 
consists of two passes; first satisfying spring conditions between pairs of vertices given by IndexBuffer 
1 then the same for IndexBuffer2         As discussed previously, there are different methods 
available for interpolating hair each with its own advantages. Interpolated hairs can be created along 
the length of a single guide hair (clump based interpolation), or they can be created by combining multiple 
guide hair (for example barycentric interpolation between three guide hair). The advantage of using the 
latter approach is that it provides a good coverage of the scalp and hair volume with relatively few 
hairs. Unfortunately, one of the drawbacks of this approach is that it is likely to produce interpolated 
hairs that penetrate the body or other collision objects. This is demonstrated in the top figure, where 
interpolated strands are going through the collision obstacles of the head and body. In this section 
we describe a method for efficiently detecting and avoiding cases where multi-guide hair interpolation 
leads to hair penetration into objects. As we see in Figure(a) the guide hairs avoid the collision obstacle 
since they are explicitly simulated. The interpolated hairs however are only produced using the positions 
of the guide hair and have no physical simulation. In Figure(b) we see the results of interpolation using 
only a single guide hair to create a given interpolated hair. The interpolated hairs penetrate the collision 
object slightly, but largely follow their guide hair. In Figure(c) however we see much worse penetration 
of interpolated hair even though the guide hairs are not penetrating the object. This is because the 
interpolated hairs are created by averaging the positions of many guide hair and this process gives us 
no guarantees that the interpolated positions will not go straight through the collision object (for 
example the head). In order to avoid these collisions we first detect the vertices where collisions 
occur and then switch those vertices to a more conservative interpolation approach, like the single strand 
interpolation. It is important to note that it is not sufficient to address only those vertices in the 
interpolated hair strands that actually undergo a penetration; altering their positions necessitates 
that we alter the positions of all vertices beneath them (and some above them) as well. This is demonstrated 
in figures above. If, as in figure(b), we only change the interpolation method of those vertices directly 
undergoing a collision the remaining hair strand below the modified vertices looks un-natural in its 
original position. Instead, we need to modify the interpolation mode of all vertices that are undergoing 
a collision or are below such vertices, as in Figure(c). In order to identify hair vertices that are 
below other object-penetrating vertices we do a pre-pass. In this pre-pass we render all the interpolated 
hair to a texture; all vertices of an interpolated hair strand are rendered to the same pixel. For each 
hair vertex we output its ID (number of vertices that separate the current vertex from the hair root) 
if that vertex collides with a collision object. Otherwise we output a large constant. This rendering 
pass is performed with minimum blending. The result of the pass is a texture that encodes for each interpolated 
hair strand whether any of its vertices intersect a collision object, and if they do, what is the first 
vertex that does so. We can then use this texture to switch the interpolation mode of any vertex of an 
intersecting hair strand below the first intersecting vertex, as in the figure on the left. In the Figures 
above we can see the result of this step visualized on the hair. Here we render each vertex as red if 
its ID is greater than or equal to the minimum ID that we wrote out in the pre-pass. We can then use 
our pre-calcuated collision texture to correctly switch the interpolation mode of interpolated hair when 
we are creating them. In the shader where we calculate the interpolated hair position we read the texture 
to determine if the current vertex is above or below the first intersecting vertex of the strand. If 
the current vertex is below the first intersecting vertex we use the single strand interpolation method 
to calculate its position. We also employ a blending zone of several vertices above the first intersecting 
vertex to slowly blend between the two interpolation modes hence avoiding a sharp transition To appear 
in theACM SIGGRAPH conference proceedings Hair Meshes Cem Yuksel * Scott Schaefer John Keyser Cyber 
Radiance Texas A&#38;M University Texas A&#38;M University  Figure 1: Anexample hair mesh model and 
the .nal hair modelgenerated using this hair mesh andprocedural styling operations. Abstract Despite 
the visual importance of hair and the attention paid to hair modeling in the graphics research, modeling 
realistic hair still remains a very challenging task that can be performed by very few artists. In this 
paper we present hair meshes, a new method for modeling hair that aims to bring hair modeling as close 
as possible to modeling polygonal surfaces. This new approach provides artists with direct control of 
the overall shape of the hair, giving them the ability to model the exact hair shape they desire. We 
use the hair mesh structure for modeling the hair volume with topological constraints that allow us 
to automatically and uniquely trace the path of individual hair strands through this volume. We also 
de.ne a set of topological operations for creating hair meshes that maintain these constraints. Furthermore, 
we provide a method for hiding the volumetric structure of the hair mesh from the end user, thus allowing 
artists to concentrate on manipulating the outer surface of the hair as a polygonal surface. We explain 
and show examples of how hair meshes can be used to generate individual hair strands for a wide variety 
of realistic hair styles. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object 
Modeling Curve, surface, solid, and object representations; Keywords: Hair modeling, hair mesh, volume 
modeling * e-mail: cem@cemyuksel.com e-mail: schaefer@cse.tamu.edu e-mail: keyser@cse.tamu.edu 1 Introduction 
Hair is an extremely important visual component of virtual characters. Therefore, it is crucial to equip 
artists with powerful tools that can help them sculpt the exact hair model they desire. Unfortunately, 
realistic hair models may require hundreds of thousands of hair strands, formed into exceptionally complicated 
geometric structures. The characteristics of individual hairs, the styling products applied to hairs, 
and the physical forces affecting the hairs all impact the overall look. Hence, a hair modeling tool 
should be powerful enough to handle a wide range of hair styles, simple enough that the tremendous complexity 
of the model is hidden from the user, and controllable such that artists can easily express their desired 
outcome. Despite a considerable amount of research and a variety of implementations over the past two 
decades, hair modeling still remains an open challenge; there is no solution that is widely accepted 
in the graphics industry. To reduce the complexity of hair modeling, almost all existing approaches 
generate .ne details of the hair model through procedural techniques. These procedural tools relieve 
the burden of dealing with every individual hair strand, allowing artists to concentrate on the overall 
look of the hair model. Thus, the main modeling effort on the part of the artist is in de.ning the global 
shape of the hair. Even though hair is made up of many thin strands, we often interpret hair models 
as a surface. Therefore, the shape of this outer surface is important when modeling a particular hair 
style. Existing hair modeling approaches either de.ne the shape of the hair model indirectly through 
various parameters or concentrate on the shapes of individual hair strands or bundles. In either case, 
the outer surface of the hair model is not explicitly de.ned. For this reason many skilled artists .rst 
model the outer surface with standard surface modeling tools and then use this surface as a guide while 
modeling hairs, attempting to place hairs in positions that will match that surface. This indirect control 
can be rather time consuming, especially when an artist desires to change the shape of the hair surface 
later. In this paper we present a new alternative, hair meshes, that aims to bring hair modeling as close 
as possible to modeling polygonal meshes. Figure 1 shows an example hair mesh and the hair model 1 To 
appear in theACM SIGGRAPH conference proceedings created using this hair mesh. A hair mesh represents 
the entire volume of hair with topological constraints that allow us to easily trace the path of hairs 
from the scalp through the volume. The user has the ability to explicitly control the topological connections 
within the hair mesh, thus allowing creation of a wide range of possible hair models. A user will typically 
only interact with the external surface of the mesh volume, using this surface to explicitly control 
the shape of the hair. Internal vertices of the mesh volume are automatically placed based on the external 
surface. Since internal vertices do not need to be manipulated directly, artists who are already skilled 
at polygonal surface modeling can easily model hair with the .exibility and direct control of polygonal 
structures. 2 Related Work There is a large body of previous research on virtual hair. In this section 
we brie.y overview most related methods. We recommend the reader refer toWard et al. [2007a] fora recent,extensive 
survey of hair methods in Computer Graphics. The high geometric complexity of hair and the wide variety 
of realworld hair styles make hair modeling a challenging task. Therefore, most hair modeling techniques 
are based on controlling collections of hair strands at once. Perhaps the simplest approach to hair modeling 
is representing hairs as parametric surfaces (e.g. NURBS) called strips [Koh and Huang 2001; Liang and 
Huang 2003; Noble and Tang 2004]. Using texture mapping with alpha channels, these surfaces look like 
a .at group of hair strands. Even though these techniques can be improved by adding thickness to this 
surface [Kim and Neumann 2000], theyare very limited in terms of the models these methods can represent 
and are not suitable for realistic hair modeling. A common hair modeling technique is to use wisps or 
generalized cylinders to control mostly cylindrical bundles of hairs with 3D curves [Chen et al. 1999; 
Yang et al. 2000; Xu and Yang 2001]. While these approaches are especially good at modeling hair styles 
with well de.ned clusters, it is often dif.cult and time consuming to shape a collection of wisp curves. 
Even making simple changes to an existing hair model can be exhausting depending on the number of curves 
to be edited. Multi-resolution approaches [Kim and Neumann 2002; Wang and Yang 2004] can improve the 
modeling process, yet this improvement depends on the complexity of the hair style and how close the 
desired hairstyle is to the types supported in the system. Researchers have also tried using different 
physically-based techniques to shape hair strands. Anjyo et al. [1992] simulated the effect of gravity 
to .nd the rest poses of hair strands. Hadap and Magnenat-Thalmann [2000] modeled hairs as streamlines 
from a .uid dynamics simulation around the head. Yu [2001] used 3D vector .elds to shape hairs by placing 
vector .eld primitives, and Choe andKo [2005] appliedvector .elds with constraintsto shape wisps. While 
various hair types can be modeled withthese approaches, just like other simulation methods, they can 
be dif.cult to control in a precise manner. Capturing a hair model from images [Kong et al. 1997; Grabli 
et al. 2002; Paris et al. 2004; Wei et al. 2005] is another alternative used to automate the virtual 
hair modeling process. Even though the recent methods [Paris et al. 2008] are very promising in terms 
of the visual realism of the results, these methods do not incorporate any artistic control. Sketch based 
interfaces are also used for modeling hair [Malik 2005], both for cartoon hairstyles [Mao et al. 2005] 
and more realistic models [Wither et al. 2007]. Recently, Fu et al. [Fu et al. 2007] proposed a sketch 
based interface to build a vector .eld, which is (a) Hair Mesh (b) Generated Hairs (c) Styled Hairs 
Figure 2: (a)2Drepresentationofa hair mesh,(b) hairsgenerated from this hair mesh, and (c) hairs after 
procedural styling operations. The green lines correspond to faces of the root layer, the tip layer 
is colored purple. Blue hair mesh vertices are external, and red are internal. then used to generate 
individual hair strands. While these techniques are practical for quickly generating a hairstyle, they 
are very dif.cult to control for achieving a desired outcome precisely. Other hair modeling approaches 
include explicitly modeling individual hair strands [Daldegan et al. 1993; Lee and Ko 2001] or a number 
of guide hairs [Alter 2004]. An interesting recent approach aims to simulate a real-world hair dressing 
session using haptic controls and physically-based simulation [Ward et al. 2007b]. Recently, Wang et 
al. [2009] proposed a method for generating a new hair model based on a given hair model. 3 Hair Mesh 
Modeling In our approach, hair modeling begins by de.ning the outer surface of the hair model. This outer 
surface is used to create a meshed volume, the hair mesh (see Figure 2a). The hair mesh structure is 
then used to generate individual hair strands (Figure 2b). As in most existing techniques, .ne details 
of the hair strands are subsequently de.ned through procedural styling operations (Figure 2c), which 
we will discuss at the end of this section. 3.1 The Hair Mesh Structure Ahair mesh is a 3D mesh describing 
the volume of space enclosing the hair. It consists of layers of polygonal meshes, which typically contain 
quadrilateral or triangularfaces,but we place no restrictions on the types of polygons used. Let F k 
bea setoffaces for layer k. We refer to thefaces in F 0 as the root layer of the hair mesh (highlighted 
green in Figure 2a) and the polygons in this layer exactly match the scalp model. This is the surface 
we will grow hairs from and eachface in F 0 will correspondtoabundleof hairs. To create a path for each 
hair, we place a number of additional layers on top of the root layer, such that each face Fjk at layer 
k has a one-to-one correspondence to a face Fjk+1 at the next layer. Connecting thetwocorrespondingfaces 
Fjk and Fjk+1 , we form a prism such that thesefaces arethetwo basefacesoftheprism.We referto the collection 
of such prisms starting at Fj 0 and connecting tofaces Fjk (where k = 0)as a bundle, Fj . If for any 
face Fjk the correspondingface Fjk+1 of the next layer does not exist, hair strands of thisbundle terminate 
at layer k. We refer to the termination layer of bundleFj as nj and call the surface composed of all 
of the Fjnj the 2 To appear in theACM SIGGRAPH conference proceedings Figure 3: Ahair knot model and 
its hair mesh. The explicit control of the hair shape provided by hair meshes makes modeling suchhairstyles 
as easy as modeling any other surface. tip layer, which is highlighted in purple in Figure 2a. Even though 
thereisa one-to-one mapping betweenfacesatdifferent layers,the faces adjacenttoagivenfacemay change from 
oneleveltothenext (e.g. the mesh can split, separatingbundles as in Figure 2a). Given this correspondence 
between layers, we can create a path for each hair from the root layer. For each point on the root layer, 
we compute the barycentric coordinates of that point with respect to the face Fj 0 containing it using 
mean value coordinates [Floater 2003]. We then trace the path of a hair growing from that point through 
the volume by applying these barycentric weights at every corresponding face in the bundle up to the 
tip layer. Finally, we connect these points together using C1 Catmull-Rom splines [1974] to generate 
the .nal hair. In the simplest case, all layers of a hair mesh have exactly the same topology. However, 
as illustrated earlier, this is not a requirement. The mesh is valid as long as each face has a corresponding 
face on all subsequent layers. Therefore, a vertex can be connected to one vertex (if the topology is 
locally the same) or to multiple vertices (if the topology changes) on a neighboring layer. To keep the 
hair mesh structure simple, we permit only a one-tomany mapping of vertices from one layer to the next 
layer (note: faces are always a one-to-one mapping), though a many-to-many mapping of vertices can potentially 
generate a valid hair mesh. This simple restriction ensures many useful properties, such as edges of 
faces at one layer cannot collapse at the next layer, two faces at any layer can be neighbors (i.e. share 
a vertex) only if they are neighbors at the root layer, and the genus of the root layer is the same as 
that of the set ofbundles. 3.2 Topological Operations Users must be given controls that provide a wide 
variety of modeling operations, but at the same time these modeling operations must preserve the topological 
constraints on the mesh. The input to our system is a polygonal object that we would like to grow hairs 
on. This object forms the root layer F 0 of our hair mesh. In the beginning (when the hair mesh has no 
layers), the root and the tip layers coincide (i.e. nj =0 .j). A user interacting with the mesh will 
typically model the hair by growing the layers out from the root layer, specifying geometric and topological 
changes in each layer. To perform this modeling, the following operations are supported for creating 
and modifying the hair meshes: Face Extrude: This is our primary operation to create new layers. Faceextrusions 
are only permitted from the current tip layerfaces, astheextrusionsofsidefaceswouldnot generatevalidhair 
meshes. nj nj +1 For each face F to be extruded, a new face F is created, jj thereby generating a new 
prism in the hair mesh. This is typically the very .rst operation we use to extend the root layer. Face 
Delete: This operation deletes the face at the tip layer of a bundle, thereby removing the last prism. 
When the root and the tip layers coincide(nj =0), a face delete operation is equivalent to deleting a 
particular face from the root (and thus no hairs will be created for that region). Layer Insert: We use 
this operation to create new layers in between two intermediate layers. Though layer insert could be 
de.ned as a local operation, to enforce one-to-manymapping of vertices we perform the same operation 
on allfaces that are topologically connected in the layer at which the operation is applied. The new 
layer is inserted before the layer that is selected. Layer Remove: Similar to layer insertion, layer 
removal affects all the topologically connected faces of a layer. When the layer to be removed is the 
tip layer, this operation is identical toface delete(s). The root layer cannot be removed as it would 
mean deleting the root object. Edge and Vertex Separate: Vertices and edges shared by more than one face 
in a single layer can be topologically separated. This topological separation creates multiple edges/vertices 
that are topologically separated but are geometrically coincident. Subsequent modeling operations may 
move these points geometrically. If the separated vertex or edge is not at the tip layer, all corresponding 
vertices or edges above this layer are also separated to ensure oneto-many mapping of vertices. Edge 
and Vertex Weld: The weld operation is the inverse of a separate operation, and topologically joins the 
vertices or edges at the same layer. To respect one-to-many mapping of vertices, this operation can only 
weld vertices that correspond to the same vertex at the root layer. Furthermore, all corresponding vertices 
below this layer are also welded. Face and Edge Divide and Subdivision: Splitting and subdivision of 
faces can be easily de.ned over the hair mesh. This includes standard approaches such as Catmull-Clark 
or Loop subdivision. Any subdivision operation applied to a face must be propagated throughout the entirebundle 
for thatface. In addition, since subdivision may modify adjacentfaces, allbundles adjacent to thatbundle 
in the root layer may also be affected. Note that subdivision is supported only on the layers of the 
hair mesh, not on the quadrilateralfaces that form the sides of the prisms in the hair mesh surface 
(layer insertion provides a similar effect, there). 3.3 Geometrical Operations The vertices of the hair 
mesh are described as either external vertices, which lie on the outer surface of the mesh, or internal 
vertices. This classi.cationis illustratedin Figure2a, whereexternalvertices are drawn in blue, and internal 
vertices in red. Note that several 3 To appear in theACM SIGGRAPH conference proceedings  In general, 
the user can explicitly position all these vertices. However, a large number of internal vertices may 
be generated during construction of the hair mesh. Since the number of external vertices is proportional 
to surface area and the interior to volume, the number of internal vertices may dominate the total number 
of vertices in complex hair models. These vertices are necessary to determine the path of a hair and 
provide adjacency information for hair bundles. However, these internal vertices are problematic for 
the user because they lie inside the enclosed volume of the hair mesh, making them hard to see, especially 
when the hair mesh is visualized as a surface. Therefore, we provide the option to hide these internal 
vertices and instead place them automatically based on the positions of the external vertices. Internal 
vertex placement is a part of the modeling process and is executed every time the user moves or creates 
a group of external vertices. Some of these operations (moving external vertices) can affect a large 
number of internal vertices. For this reason, it is critical that the internal vertex placement algorithm 
be performed very quickly, so as not to interrupt the modeling process. While many techniques may be 
used to place internal vertices, we choose a simple constrained quadratic minimization. We will de.ne 
an error metric in terms of the positions of all mesh vertices, .x the external vertices, and solve for 
the positions of internal vertices that minimize this metric. Our choice of error metric is motivated 
by physical properties of hair. Namely, hair strands should have a similar shape as nearby hair strands 
(later we will apply styling operations to differentiate nearby hairs as explained in Section 3.4). For 
each extruded prism between faces Fjk and Fjk+1 , we can approximate the hair direction locally using 
the edges of the prism in the extrusion direction. Let V k be the ith vertex of the face F k . j,i j 
Then, an edge of the prism is given by the vertices V k j,i , j,i and V k+1 and the hair direction locally 
along the edge is V k+1 - V k We j,i j,i. want to minimize the difference in the local hair direction 
between adjacent edges along the extrusion direction of the prism. Ifwe sumoverallofthequadfaces that 
formthe sidesofthe prisms in the volume, the resulting minimization is of the form nj LLL 2 k+1 kk+1 
k min (Vj,i -Vj,i) -(Vj,i+1 -Vj,i+1), jk=1 i subject to the constraint that the external vertices (or 
anyothers the user wishes to .x) remain unchanged. We can easily minimize this quadratic, which has a 
unique minimum, using Conjugate Gradients [Shewchuk 1994]. Since we use the current positions of the 
internal vertices as a starting point for this minimization, Conjugate Gradients typically converges 
to a solution in only a few iterations and is quite fast. The red vertices in Figure 2a show the result 
of a 2D version of this optimization with the blue, external vertices as constraints. 4 Figure 5: A futuristic 
hairbun model and its hair mesh. 3.4 Hair Styling Hair mesh modeling can be thought of as an initial 
stage of modeling hair. The hair mesh de.nes the overall shape of the hair model and the hair strands 
we generate conform to that model. However, realistic hair is not always straight and many existing hair 
modeling techniques can be applied to the hair strands to improve the realism of the hair or reproduce 
speci.c hair styles. In our system, all hair modeling operations applied to the hair strands after they 
are generated from the hair mesh are called styling operations. Procedural hair styling forms one group 
of such operations. These operations typically deform the hair by moving the vertices of the hair strands 
using a combination of procedural noise and trigonometric functions with various parameters. The functions 
can be directly computed using the 3D position of each hair strand vertex. However, this makes the hair 
style, which is applied using these procedural operations, very sensitive to the initial 3D positions 
of hair strand vertices. As a result, even minor modi.cations to the hair mesh may signi.cantly alter 
the shapes of some hair strands. To avoid this undesired behavior, one can de.ne these procedural operations 
in the canonical space of a hair strand as in [Yu 2001] or using the barycentric embedding of a hair 
strand within the hair mesh. In our system we use the later approach. In addition to procedural operations, 
we can easily combine our hair modeling system with some previous hair modeling techniques that use wisps. 
We achieve this by generating wisp curves from the hair mesh similar to generating hair strand curves. 
In this case, individual hair strands are not directly generated from the hair mesh, but the wisp curves 
along with a number of parameters are used to populate .nal hair strands. Note that wisp curves themselves 
can go through procedural styling operations orexplicit user modi.cations before generating the hair 
strands. Similarly, we can combine our hair mesh modeling approach with multiresolution hair modeling 
[Kim and Neumann 2002] by generating .rst level generalized cylinders using the hair mesh. Higher level 
generalized cylinders and .nally individual hair strands are then generated from the .rst level generalized 
cylinders as described in [Kim and Neumann 2002]. This approach replaces the most laborious stage of 
multiresolution hair modeling (as stated by Kim and Neumann [2002]) with hair mesh modeling.  4 Results 
and Discussion To demonstrate the capabilities of our hair mesh modeling approach we present various 
hair models produced using our system. Figure 1 shows a typical hair model with its hair mesh. While 
similar hair models can be prepared with many previous techniques, the main advantage of the hair mesh 
is the ability to control the hair shape by directly manipulating the outer surface. To appear in theACM 
SIGGRAPH conference proceedings  Figures3,4, and5 show different hair models withbuns and knots. Such 
models are very dif.cult to prepare with most previous techniques,but with hair meshes, modeling these 
hairstyles are no more dif.cult than modeling the outer surface using any standard polygonal modeling 
tool. Figure6showsa more complicatedbun model. Notice the .ne detail of the bun and the explicit control 
of the hair shape and direction available to the artist. Depending on the complexity of the desired hair 
model, modeling using hair meshes can take as short a time as a couple of minutes. For example, the simple 
hair model shown in Figure 7 can be prepared in a couple of minutes. While preparing a more complicated 
hair model can take signi.cantly longer, the explicit control provided by hair meshes makes it easy 
to edit the model and produce the desired variation. Figure 7: Asimple hair model prepared within a 
couple of minutes. Even though hair meshes are designed to model hair by letting the user specify only 
the outer surface of the hair, they are also useful when an artist desires to control the hair shape 
within the hair volume. Figure 8 shows such a complicated hair mesh model where the artistexplicitly 
shapes each hairbundle. Figure9 shows anexampleof combining hair mesh modeling with wisp based hair modeling. 
Here the hair mesh in Figure1is used to generate 200 wisp curves, instead of individual hair strands. 
Final hair strands are then generated from these wisp curves as in Choe andKo [2005]. One useful property 
of our hair mesh modeling approach is the complete separation of large and small scale details. While 
large scale details that de.ne the global shape of the hair model are controlled using the hair mesh, 
.ne details are introduced during the styling process. Therefore, the same hair mesh can be used to generate 
different types of hair styles as shown in Figure 10. This technique can also be used for introducing 
signi.cant style variations within a hair model by generating one set of hair strands from a hair mesh 
and applying one style, then using the same hair mesh to generate another set of hair strands with a 
different style applied. By generating multiple sets of hairs with this procedure, style Figure 8: A 
complicated hair mesh model and the hair generated from this hair mesh. variations of a hair model can 
be easily represented. The union of these sets form the .nal hair model, and the global shape of the 
hair model is explicitly controlled by a single hair mesh. This feature is especially useful when modeling 
realistic hairs with rich variations such as frizzy hair and .y-aways. Figure 11 shows such an example. 
Note that in many previous hair modeling techniques, introducing these frizzy strands can be dif.cult 
or even impossible. We designed the hair mesh modeling approach such that styling operations are reserved 
for small scale details only and larger details are explicitly modeled using the hair mesh. However, 
in our system there is no restriction on the user side to forbid using styling operations for large 
variations as well. When the style variations are exaggerated, the perceived surface of hair formed by 
the .nal hair strands can deviate from the surface de.ned by the hair mesh. This deviation is especially 
undesirable when the hair mesh is used for explicitly avoiding intersections of hairs with surrounding 
objects. Note that undesired intersections can also be automatically avoided at the hair strand level 
using the technique described by Kim and Neumann [2002]. Hair mesh modeling merely provides a high level 
structure to easily de.ne the global shape of a hair model. Unfortunately, it is not possible to claim 
for any hair modeling technique that it can produce hair models that cannot be modeled using previous 
techniques, since theoretically speaking all hair models can be produced by ex- To appear in theACM 
SIGGRAPH conference proceedings plicitly modeling the hair strands. Moreover, the real power of hair 
meshesis not thefact thatvarious different hair models canbe prepared with this approach, either. Most 
existing techniques permit a wide variety of hair styles to be generated. However, the lack of explicit 
control over the global hair shape makes the existing techniques dif.cult, if not impossible, to use 
to achieve the exact hair model one aims for. On the other hand, hair meshes convert the volumetric hair 
modeling problem to a surface modeling problem. This signi.cantly reduces the high complexity of volume 
modeling and brings hair modeling closer to standard polygonal surface modeling. As a result, hair meshes 
offer a familiar interface to experienced modelers and make it very easy for them to sculpt the exact 
hair models they desire. We have also tried using hair meshes for simulating hair. We follow an approach 
similar to that introduced by Chang et al. [2002]. Instead of picking representative hair strands (i.e. 
guide hairs), we form an articulated rigid body chain directly from the edges of the hair mesh that connect 
the vertices of one layer F k to the next layer F k+1 . Figure 12 shows example frames captured from 
our hair mesh simulation system. We observed that physical simulations using hair meshes can produce 
seemingly natural hair-hair interactions with high performance. The hair mesh in Figure 12 includes 
30 rigid body links and 120 chains, and the simulation runs at 92 fps on a 2.14 GHz Intel Core 2 Duo 
processor with a single thread (note that such a simulation can be trivially multi-threaded). 5 Conclusions 
and Future Directions We have introduced hair meshes for modeling hair using polygonal surface tools. 
Our technique allows an artist to create complex hair styles easily by providing explicit control over 
the overall shape of the hair surface. By automatically placing internal vertices, the Figure 11: Frizzy 
strandsgenerated directly from the hair mesh as an additional hair group on top of the hairs from Figure 
9. The close-up view on the right shows the effect of frizzy strands. artist can concentrate on the outer 
surface shape of the hair, which signi.cantly simpli.es the hair modeling process without limiting direct 
control over the hair model shape. We believe there are more applications of hair meshes than just modeling. 
An example of using hair meshes for hair simulation is presented in the previous section. Furthermore, 
real-time rendering of deforming hair such as the animation produced by our hair simulation can be accelerated 
using hair meshes. Since the hair is completely determined by the geometry of the hair mesh, the hair 
geometry can be synthesized on the GPU simply by sending the deformed positions of the hair mesh vertices, 
thereby signi.cantly saving graphics bus bandwidth. Hair meshes may also be used to approximate shadow 
and ambient occlusion computations.  Acknowledgements We would like to thank Lee Perry-Smith (www.ir-ltd.net) 
for providing the models and producing most of the hairstyles in this paper. This work was supported 
in part by NSF grant CCF-07024099. References ALTER, J. S., 2004. Hair generation and other natural 
phenomena with surface derived control volumes in computer graphics and animation. U.S.Patent 6720962. 
ANJYO, K., USAMI, Y., AND KURIHARA, T. 1992. A simple method for extracting the natural beauty of hair. 
In SIG-GRAPH 92: Proceedings of the 19th annual conference on Computer graphics and interactive techniques,ACM, 
NewYork, NY, USA, 111 120. CATMULL, E., AND ROM, R. J. 1974. A class of local interpolating splines. 
In Computer Aided Geometric Design, Academic Press, Orlando, FL, USA, 317 326. CHANG, J. T., JIN, J., 
AND YU, Y. 2002. A practical model for hair mutual interactions. In SCA 02: Proceedings of the 2002 ACM 
SIGGRAPH/Eurographics Symposium on Computer Animation,ACM, NewYork, NY, USA, 73 80. CHEN, L.-H., SAEYOR, 
S., DOHI, H., AND ISHIZUKA, M. 1999. Asystem of 3d hair style synthesis based on the wisp model. The 
Visual Computer 15, 4, 159 170. 6 To appear in theACM SIGGRAPH conference proceedings CHOE, B., AND 
KO, H.-S. 2005. A statistical wisp model and pseudophysical approaches for interactive hairstyle generation. 
IEEETransactions onVisualization and Computer Graphics 11, 2, 160 170. DALDEGAN, A., THALMANN, N. M., 
KURIHARA, T., AND THALMANN, D. 1993. An integrated system for modeling, animating and rendering hair. 
In Eurographics 93, Blackwell Publishers, Oxford, UK, R. J. Hubbold and R. Juan, Eds., Eurographics, 
211 221. FLOATER, M. S. 2003. Mean value coordinates. Computer Aided Geometric Design 20, 1, 19 27. FU, 
H., WEI, Y., TAI, C.-L., AND QUAN, L. 2007. Sketching hairstyles. In SBIM 07: Proceedings of the 4th 
Eurographics Workshop on SketchBased Interfaces and Modeling,ACM, New York, NY, USA, 31 36. GRABLI, S., 
SILLION, F., MARSCHNER, S. R., AND LENGYEL, J. E. 2002. Image-based hair capture by inverse lighting. 
In Proc. Graphics Interface, 51 58. HADAP, S., AND MAGNENAT-THALMANN, N. 2000. Interactive hair styler 
based on .uid .ow. In Eurographics Workshop on Computer Animation and Simulation 2000, Springer, 87 99. 
KIM, T.-Y., AND NEUMANN, U. 2000. A thin shell volume for modeling human hair. In CA 00: Proceedings 
of the Computer Animation, IEEE Computer Society, Washington, DC, USA, 104. KIM, T.-Y., AND NEUMANN, 
U. 2002. Interactive multiresolution hair modeling and editing. ACM Transactions on Graphics (Proc. 
of SIGGRAPH 2002) 21, 3, 620 629. KOH, C. K., AND HUANG, Z. 2001. A simple physics model to animate human 
hair modeled in 2d strips in real time. In Proceedings of the Eurographic workshop on Computer animation 
and simulation, Springer-Verlag NewYork, Inc., NewYork, NY, USA, 127 138. KONG, W., TAKAHASHI, H., AND 
NAKAJIMA, M. 1997. Generation of 3d hair model from multiple pictures. In Proceedings of Multimedia 
Modeling, 183 196. LEE, D. W., AND KO, H. S. 2001. Natural hairstyle modeling and animation. Graphical 
Models 63, 2, 67 85. LIANG, W., AND HUANG, Z. 2003. An enhanced framework for real-time hair animation. 
In PG 03: Proceedings of the 11thPaci.c Conference on Computer Graphics and Applications, IEEE Computer 
Society,Washington, DC, USA, 467. MALIK, S. 2005. A sketching interface for modeling and editing hairstyles. 
In SBIM 05: Proceedings of the 2nd Eurographics Workshop on Sketch Based Interfaces and Modeling, 185 
194. MAO, X., ISOBE, S., ANJYO, K., AND IMAMIYA, A. 2005. Sketchy hairstyles. In CGI 05: Proceedings 
of the Computer Graphics International 2005, IEEE Computer Society,Washington, DC, USA, 142 147. NOBLE, 
P., AND TANG, W. 2004. Modelling and animating cartoon hair with nurbs surfaces. In CGI 04: Proceedings 
of the Computer Graphics International, IEEE Computer Society, Washington, DC, USA, 60 67. PARIS, S., 
HECTOR M. BRICE N., AND SILLION, F. X. 2004. Capture of hair geometry from multiple images. ACM Transactions 
on Graphics (Proc. of SIGGRAPH 2004) 23, 3, 712 719. PARIS, S., CHANG, W., KOZHUSHNYAN, O. I., JAROSZ, 
W., MATUSIK, W., ZWICKER, M., AND DURAND, F. 2008. Hair photobooth: geometric and photometric acquisition 
of real hairstyles. ACMTransactions on Graphics (Proc. of SIGGRAPH 2008) 27, 3, Article 30. SHEWCHUK, 
J. R. 1994. An introduction to the conjugate gradient method without the agonizing pain. Tech. rep., 
Pittsburgh, PA, USA. WANG, T., AND YANG, X. D. 2004. Hair design based on the hierarchical cluster hair 
model. Geometric modeling: techniques, applications, systems and tools, 330 359. WANG, L., YU, Y., ZHOU, 
K., AND GUO, B. 2009. Examplebased hair geometry synthesis. ACM Transactions on Graphics (Proc. of SIGGRAPH 
2009) 28, 3, Article 56. WARD, K., BERTAILS, F., KIM, T.-Y., MARSCHNER, S. R., CANI, M.-P., AND LIN, 
M. C. 2007. A survey on hair modeling: Styling, simulation, and rendering. IEEE Transactions on Visualization 
and Computer Graphics 13, 2, 213 234. WARD, K., GALOPPO, N., AND LIN, M. 2007. Interactive virtual hair 
salon. Presence: Teleoperators and Virtual Environments 16, 3, 237 251. WEI,Y.,OFEK,E.,QUAN,L., AND SHUM,H.-Y. 
2005. Modeling hair from multiple views. ACM Transactions on Graphics (Proc. of SIGGRAPH 2005) 24, 3, 
816 820. WITHER,J.,BERTAILS,F., AND CANI,M.-P. 2007. Realistic hair from a sketch. In International Conference 
on Shape Modeling and Applications, IEEE,Lyon, France, IEEE, 33 42. XU, Z., AND YANG, X. D. 2001. V-hairstudio: 
An interactive tool for hair design. IEEE Computer Graphics and Applications 21, 3, 36 43. YANG, X. D., 
XU, Z., WANG, T., AND YANG, J. 2000. The cluster hair model. Graphical Models 62, 2, 85 103. YU, Y. 2001. 
Modeling realistic virtual hairstyles. In PG 01: Proc. of the 9thPaci.c Conference on Comp. Graphics 
and Applications, IEEE Computer Society,Washington, DC, USA, 295. 7 EUROGRAPHICS 2008/G. Drettakis andR. 
Scopigno Volume 27 (2008), Number2 (Guest Editors)  Deep Opacity Maps CemYuksel1 and JohnKeyser2 Departmentof 
Computer Science,Texas A&#38;M University 1cem@cemyuksel.com 2keyser@cs.tamu.edu No shadows Opacity 
Shadow Maps Opacity Shadow Maps Density Clustering Deep Opacity Maps 16 layers 128 layers 4layers 3 layers 
(154 fps) (81 fps) (2.3 fps) (73 fps) (114 fps) Figure 1: Layering artifacts of Opacity Shadow Maps 
are visible even with 128 layers, while Density Clustering has artifacts dueto inaccuracies.DeepOpacityMapswithonly3layerscangeneratean 
artifactfreeimagewiththehighestframerate. Abstract We present a new methodfor rapidly computing shadows 
from semi-transparent objects like hair. Our deep opacity maps method extends the concept of opacity 
shadow maps by using a depth map to obtain a per pixel distribution of opacity layers. This approach 
eliminates the layering artifacts of opacity shadow maps and requires far fewer layers to achieve high 
quality shadow computation. Furthermore, it is faster than the density clustering technique, and produces 
less noise with comparable shadow quality. We provide qualitative comparisons to these previous methods 
and give performance results. Our algorithm is easy to implement, faster, and more memory ef.cient, enablingustogeneratehigh 
qualityhair shadowsinreal-timeusinggraphicshardwareona standardPC. Categories and Subject Descriptors 
(according toACM CCS): I.3.7 [Computer Graphics]: Color, shading, shadowing, and texture Keywords: shadow 
maps, semi-transparent shadows, hair shadows, real-time shadows, GPU algorithms 1. Introduction Self-shadowing 
is an essential visual element for rendering semi-transparent objects like hair, fur, smoke, and clouds. 
However, handling the transparencycomponent is either inef.cient or not possible for simple shadowing 
techniques. Various algorithmshave beenproposedto address this issue bothforof.inerendering[LV00,AL04]and 
interactive/realtime rendering[KN01,MKBvR04].In this paper we present . 2008 The Author(s) Journal compilation 
. 2008 The Eurographics Association and BlackwellPublishing Ltd. cPublished by Blackwell Publishing, 
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main Street, Malden, MA 02148, USA. the deep opacity 
maps method, which allows real-timehair rendering with dynamic lighting and semi-transparent shadows. 
This new method isfaster than the previous ones and produces artifact free shadows (Figure 1). Even though 
we focus on hair shadows, our method is applicable to other semi-transparent objects. The deep opacity 
maps method combines shadow mapping[Wil78]and opacity shadow maps[KN01]to give a CemYuksel&#38;JohnKeyser/Deep 
Opacity Maps better distribution of opacity layers.We .rst render the hair geometry as opaque primitives 
from the light s view, recording the depth values on a shadow map. Next we render an opacity map from 
the light s view similar to opacity shadow maps. The novelty of our algorithm lies in the way that the 
opacity layers are distributed using the depth map to create opacity layers that vary in depth from 
the light source on a per-pixel basis. Unlike previous interactive/real-time transparent shadowing techniques[KN01,MKBvR04], 
this new layer distribution guarantees that the direct illumination coming from the light source without 
being shadowed is captured correctly. This property of deep opacity maps eliminates the layering artifacts 
that are apparent in opacity shadow maps. Moreover,far fewer layers are necessary to generate high quality 
shadows. The layering artifacts of the previous methods are especially signi.cant in animated sequences 
or straight hair models. Figure 1 shows a comparison of our deep opacity maps algorithm to the previous 
methods. Layering artifacts in opacity shadow maps[KN01]are signi.cant when 16 layers are used, and 
even with 128layers, diagonal dark stripes are still visible on the left side of the hair model. Density 
clustering[MKBvR04]producesa good approximation to the overall shadows, but still suffers from visible 
artifacts around the specular region. However, our deep opacity maps method can produce an artifact-free 
image with fewer layers anditis signi.cantlyfaster. The next section describes the previous methods. 
The details of our deep opacity maps algorithm are explained in Section3.We present the resultsof our 
methodin Section 4, and we discuss the advantages and limitations in Section 5 before concludingin Section 
6. 2. Related Work Most shadow computation techniques developed for hair are based on Shadow Maps [Wil78]. 
In the .rst pass of shadow mapping, shadow casting objects are rendered from the light s point of view 
and depth values are stored in a depth map. While rendering the scene from the camera view in the second 
pass, to check if a point is in shadow, one .rst .nds the corresponding pixel of the shadow map, and 
compares the depth of the point to the value in the depth map. Theresultofthis comparisonisabinarydecision,soshadow 
maps cannot be used for transparent shadows. Deep Shadow Maps [LV00]is a high quality method for of.ine 
rendering. Each pixel of a deep shadow map stores a 1D approximate transmittance function along the corresponding 
light direction.To compute the transmittance function, semi-transparent objects are rendered from the 
light s point of view and a list of fragments is stored for each pixel. The transmittance function de.ned 
by these fragments is then compressed into a piecewise linear function of approximate transmittance. 
The value of the transmittance function starts decreasing after the depth value of the .rst fragment 
in the corresponding light direction. The shadow value at anypointisfound similartoshadowmaps,butthistimethe 
depth value is used to compute the transmittance function at the corresponding pixel of the deep shadow 
map, which is then converted to a shadow value. The Alias-free Shadow Maps [AL04] method is another 
of.ine technique that can generate high quality semitransparent shadows. In this method, rasterization 
of the .nal image takes place before the shadow map generation to .nd the 3D positions corresponding 
to every pixel in the .nal image. Then, shadows at these points are computed from the light s point of 
view, handling one occluding piece of geometry at a time. Opacity Shadow Maps [KN01] is essentially 
a simpler version of deep shadow maps that is designed for interactive hair rendering. It .rst computes 
a number of planes that slice the hair volume into layers (Figure 2a). These planes are perpendicular 
to the light direction and are identi.ed by their distances from the light source (i.e. depth value). 
The opacitymapisthen computedby renderingthehair structure fromthelight sview.Aseparate renderingpassis 
performed for each slice by clipping the hair geometry against the separating planes. The hair density 
for each pixel of the opacity map is computed using additional blending on graphics hardware. The slices 
are rendered in order starting from the slice nearesttothelight source,andthevalueoftheprevious slice 
is accumulated to the next one. Once all the layers are rendered, this opacity map can be used to .nd 
the transmittance from the occlusion value at anypoint using linear interpolation of the occlusion 
values at the neighboring slices. Depending on the number of layers used, the quality of opacity shadowmaps 
canbe muchlower thandeep shadowmaps, since the interpolation of the opacities between layers generates 
layering artifacts on the hair. These artifacts remain visible unless a large number of layers are used. 
Mertens et. al. proposed the Density Clustering approach [MKBvR04]to adjust the sizes and the positions 
of opacity layers separately for each pixel of the shadow map. It uses k-means clustering to compute 
the centers of the opacity layers. Then, each hair fragment is assigned to the opacity layer with the 
nearest center, and the standard deviation of the v opacity layer times 3is used as the size of the layer. 
Once the opacity layers are positioned, the hair geometry is rendered once again from the light s point 
of view and the opacity value of each layer is recorded. In general, density clustering generates better 
opacity layerdistributions than opacity shadow maps,butitalso introduces other complications and limitations. 
Density clustering s major limitation is that it cannotbeextendedtohaveahigh numberof layersdue to the 
way that the layering is computed, and it is only suitable fora small numberof clusters (the original 
paper[MKBvR04]suggests4 clusters). Moreover, k-means clustering is an iterative method and each iteration 
requires a separate c . 2008 The Author(s) Journal compilation c . 2008 The Eurographics Association 
and Blackwell Publishing Ltd. CemYuksel&#38;JohnKeyser/Deep Opacity Maps (a) Opacity Shadow Maps (b) 
Deep OpacityMaps Figure 2: Opacity shadow maps use regularly spaced planar layers. Our deep opacity maps 
use fewer layers, conforming to the shape of the hair model. pass that renders the whole hair geometry. 
The ef.ciencyof the clustering depends on the initial choice of the opacity layer centers. Even if only 
a single pass of k-means clustering is performed, the density clustering method requires 4 passes to 
generate the shadow map. Finally, like opacity shadow maps, density clustering cannot guarantee that 
unshadowed direct illumination is captured correctly since the .rst opacity layer canbegin before the 
.rsthair fragment. The deep opacity maps method presented in this paper has advantages over these prior 
methods. It guarantees that the direct illumination of the surface hairs is calculated correctly. Unlike 
opacity shadow maps, opacity interpolation occurs within the hair volume, thus hiding possible layering 
artifacts. Unlike density clustering, deep opacity maps can easily use arbitrary numbers of layers (though 
usually3 layers are suf.cient). Comparing to both density clustering and opacity shadow maps, deep opacity 
maps achieve signi.cantly higher frame rates for comparablequality. Other previous methods include extensions 
of opacity shadow maps [KHS04], voxel based shadows [BMC05, ED06, GMT05], precomputed approaches [XLJP06], 
and physically basedof.ine methods[ZSW04,MM06].Fora more complete presentation of the previous methods 
please refer toWard et al.[WBK*07]. 3. Deep Opacity Maps Algorithm Our algorithm uses two passes to prepare 
the deep opacity map, and the .nal image is rendered in an additional pass, using this map to compute 
shadows. The .rst step prepares the separators between the opacity layers.We rendera depth mapof the 
hair as seen from the light source. This gives us, for each pixel of the depth map, the depth z0 at which 
the hair geometry begins. Starting from this depth value, we divide the hair volume within the pixel 
into K layers such that each layer lies from z0+ dk-1 to z0+ dk where d0 = 0, dk-1 < dk and1 =k=K. Note 
that the spacing dk -dk-1 (layer size) does not have to be constant. Even though we use the same dk 
values for each pixel, . 2008 The Author(s) Journal compilation c. 2008 The Eurographics Association 
and BlackwellPublishing Ltd. z0 varies by pixel, so the separators between the layers take the shapeof 
the hair structure (Figure 2). Note that the light sourcein this setup canbea point oradirectional light. 
The second step renders the opacity map using the depth map computed in the previous step. This requires 
rendering the hair only once and all computation occurs within the fragment shader. As each hair is 
rendered, we read the value of z0 from the depth map and .nd the depth values of the layers on the .y.We 
assign the opacity contribution of the fragment to the layer that the fragmentfalls in and to all the 
other layers behind it. The total opacity of a layer at a pixel is the sum of all contributing fragments. 
We represent the opacity map by associating each color channel with a different layer, and accumulate 
the opacities using additive blending on the graphics hardware. We reserve one color channel for the 
depth value, so that it is stored in the same texture with opacities. Therefore, using a single colorvalue 
with four channels, we can represent three opacity layers. By enabling multiple draw buffers we can output 
multiple colors per pixel to represent more than three layers(n drawbuffers allow4n -1 layers). Obviously, 
using more than three layers will also require multiple texture lookups during .nal rendering. One disadvantage 
of using a small number of layers with deep opacity maps is that it can be more dif.cult to ensure all 
points in the hair volume are assigned to a layer. In particular, points beyond the end of the last 
layer z0+ dk do not correspond to anylayer (shaded region in Figure 2b).We have a few options: ignore 
these points (thus, they will not cast shadows), include these points in the last layer (thus, they cast 
shadows on themselves), or ensure that the last layer lies beyond the hair volume by either increasing 
the layer sizes or the number of layers. While the last option might seem ideal, it can lead to unnecessary 
extra layers that add little visual bene.t at more computational cost, since the light intensity beyond 
a certain point in the hair volume is expected tovanish.We found that the second option, mapping these 
points onto the last layer, usuallygavereasonable results. Note that our algorithm uses the depth map 
only for computing the starting points of layers, not for a binary decision of in or out of shadow. 
Thus, unlike standard shadow mapping, deep opacity maps do not require high precision depth maps.Forthe 
scenesinourexperiments,we found that using an 8-bit depth map visually performs the same asa16-bit .oating 
point depth map. 4. Results To demonstrate the effectiveness of our approach we compare the results 
of our deep opacity maps algorithm to optimized implementations of opacity shadow maps and density 
clustering.Weextended theimplementationof opacity shadow maps with up to16layers to simultaneously generate 
the opacity layersina single passby using multiple draw CemYuksel&#38;JohnKeyser/Deep Opacity Maps No 
shadows Opacity Shadow Maps Opacity Shadow Maps Density Clustering Deep Opacity Maps 8layers 256 layers 
4layers 3 layers (140 fps) (88 fps) (0.6 fps) (47 fps) (74 fps) Figure 3: Dark hair model with over 
one million line segments. The Opacity Shadow Maps method requires many layers to eliminate layering 
artifacts, Density Clustering approximates the shadows with some noise, while the Deep Opacity Maps methodgenerates 
an artifactfree image with higherframerate. Opacity Shadow Maps -256 layers Density Clustering -4layers 
 Deep Opacity Maps -3 layers Figure 4: Enlarged images fromFigure 3 comparison. buffers, as opposed to 
the multi-pass implementation proposedinthe original method[KN01].Wealso introducedan additional pre-computation 
pass to density clustering, which computes the layer limits before the .nal image rendering, and achieved 
higher performance by optimizing the shadow lookup. All images presented in this paper were captured 
from our real-time hair rendering system using a standard PCwitha2.13GHz Core2 Duo processor and GeForce 
8800 graphics card. We used line drawing for rendering the hair models, and the Kajiya-Kay shading model[KK89]because 
of its simplicity. Antialiasing in the .nal image was handled on the graphics hardware using multi-sampling.We 
did not use antialiasingwhen generatingopacity maps.To achieveafake transparency effect in the .nal 
image we divided the set of hair strands into three disjoint subsets. Each one of these subsetsis rendered 
separatelywithno transparencyonaseparate image. The resulting images are then blended to produce the 
.nal frame. Figure1 showsa straight hair model with 150 thousand line segments. The opacity shadow maps 
method with 16 layers produces severe artifacts of diagonal dark stripes that correspond to the beginning 
of each opacity layer. Though signi.cantly reduced, these artifacts are still visible even when the number 
of layers is increased to 128. Density clustering, on the other hand, produces a good approximation 
to the overall illumination using 4 layers, however it suffers from a different kind of layering artifact 
visible around the specular region due to its inaccuracy. Our deep opacity maps technique produces an 
artifact free image with plausible shadow estimate using only3 layers, and it is signi.cantlyfaster 
than the other methods. Figures 3 and 5 show two different complex hair styles with over one million 
and 1.5 million line segments respectively. On both of these models, the opacity shadow maps method 
with 8 layers produces dark stripes as interpolation artifacts between layers. When 256 layers are used 
with opacity shadow maps, layering artifacts visually disappear and the resulting shadows approach the 
correct values,but rendering one frame takes about two seconds. On the other hand, density clustering 
manages to produce a close approximation using only 4 layers. However, the inaccuracies of density clustering 
produce some noise in the illumination that are clearly visiblein the enlarged images (Figures 4and 6)and 
animated sequences. The deep opacity maps method manages to create an artifact free image with smooth 
illumination changesover the hair surface with signi.cantly higher frame rates and less memory consumption. 
Figure7demonstratesthatdeep opacitymapscanbeused in conjunction with traditional shadow maps. In this 
image of a hairy teapot, the shadow map handles the opaque shadows due to the teapot and the deep opacity 
map handles semi-transparent shadows due to the hair strands. Both the hair model and the teapot cast 
shadows onto each other as well as on the ground plane. c . 2008 The Author(s) Journal compilation . 
2008 The Eurographics Association and Blackwell Publishing Ltd. c CemYuksel&#38;JohnKeyser/Deep Opacity 
Maps  No shadows Opacity Shadow Maps Opacity Shadow Maps Density Clustering Deep Opacity Maps 8layers 
256 layers 4layers 3 layers (104 fps) (65 fps) (0.5 fps) (37 fps) (50 fps) Figure 5: Curly hair model 
with over 1.5 million line segments. Layering artifacts of Opacity Shadow Maps with 8 layers are apparent 
on the top of the hair model. They disappear with 256 layers with low frame rates. Density Clustering 
generates a noisyapproximation,whiletheDeepOpacityMapsmethodgeneratesan artifactfreeimagewithhigherframerate. 
Opacity Shadow Maps -256 layers Density Clustering -4layers Deep Opacity Maps -3 layers Figure 6: Enlarged 
images fromFigure 5 comparison. Figure 8 shows clustered strands rendered using deep opacity maps.InFigure 
8a,3layersarenotenoughtocover the whole illuminated volume, and dark regions in appear where the strands 
beyond the last layer cast shadows onto themselves. By increasing the number of layers, as in Figure8b, 
incorrect self-shadowing canbe eliminated. This can alsobeachievedby increasingthelayersizesasinFigure 
8c; however, this also reduces the shadow accuracy. As can be seen from these images, deep opacity maps 
can generate high quality shadows with non-uniformhair models. 5. Discussion The main advantage of our 
method is that by shaping the opacity layers, we capture direct illumination correctly while eliminating 
visual layering artifacts by moving interpolation between layers to within the hair volume. This lets 
us hide Figure 7: A hairy teapot rendered using deep opacity maps with traditional shadow maps for opaque 
shadows. possible inaccuracies while also allowing high quality results with fewer layers. Unlikedensity 
clustering, which tries to approximate the whole transmittance function, we concentrate the accuracy 
on the beginning of the transmittance decay (where the shadow begins). By doing so, we aim to be more 
accurate around the illuminated surface of the hair volume the part of the hair that is most likely to 
appear in the .nal image and where inaccuracies would be most noticeable. Since we require very few 
layers, all information can be storedina small numberoftextures(a singletexturefor3 layers). This makes 
our algorithm memory ef.cient and also reduces the load on the fragment shader. The extreme simplicity 
of our approach allows us to prepare the opacity map with only2render passes, and only one of these 
passes uses blending. Opacity shadow maps of just a few layers can be generated in only a single pass, 
however visual plausibility requires many more layers. . 2008 The Author(s) Journal compilation . 2008 
The Eurographics Association and BlackwellPublishing Ltd. c CemYuksel&#38;JohnKeyser/Deep Opacity Maps 
 (a)3layers (b)7layers (c)3(larger) layers Figure 8: Clustered strands with deep opacity maps using different 
number of layers and different layer sizes. For the examples in this paper we used linearly increasing 
layer sizes with deep opacity maps. This choice of layer distribution provides high accuracy around bright 
regions wherethe transmittancebeginstodecay,whilekeeping other layers large enough to cover the illuminated 
part of the hair volume withafew numberof layers.Varying layer sizes can also be used for opacity shadow 
maps, but this would not provide any visual enhancement, since there is no heuristic that can reduce 
the layering artifacts by changing layer sizes without increasing the number of layers. In our implementation 
we observed minor .ickering due to aliased line drawing we used while rendering depth and opacity maps. 
Applying a smoothing .lter to the depth and opacity maps reduced this problem,but did not completely 
remove it. In our experiments we found that using multisampling for shadow computations, a standard 
technique used for smoothing shadow maps, produced better results with additional computation cost. 6. 
Conclusion We have introduced the deep opacity maps method, which usesadepthmapto achieveper-pixel layeringofthe 
opacity map for real-time computation of semi-transparent shadows. We compared both quality and performance 
of our method to the previous real-time/interactive semi-transparent shadowing techniques. Our results 
show that deep opacity maps arefast and can generate high quality shadows with minimal memory consumption. 
Our algorithm does not have any restrictions on the hair model or hair data structure. Since it does 
not need anypre-computation, it can be used when rendering animated dynamic hair or anyother semi-transparent 
object that can be represented by simple primitives. References [AL04] AILA T.,LAINE S.: Alias-free shadow 
maps. In Eurographics Symp. on Rendering (2004), pp. 161 166. [BMC05] BERTAILS F.,MNIER C.,CANI M.-P.: 
A practical self-shadowing algorithm for interactive hair an imation. In Proc. Graphics Interface (2005), 
pp. 71 78. [ED06] EISEMANN E., DCORET X.: Fast scene voxelization and applications. In Symposium on 
Interactive 3D Graphics and Games (2006), pp. 71 78. [GMT05] GUPTA R., MAGNENAT-THALMANN N.: Scattering-based 
interactive hair rendering. In Comp. Aided Design and Comp. Graphics (2005), pp. 489 496. [KHS04] KOSTER 
M.,HABER J.,SEIDEL H.-P.: Realtime rendering of human hair using programmable graphics hardware. In 
Proceedings of the Computer Graphics International (CGI 04) (2004), pp. 248 256. [KK89] KAJIYA J. T., 
KAY T. L.: Rendering fur with three dimensional textures. In Proceedings of SIGGRAPH 1989 (1989), pp. 
271 280. [KN01] KIM T.-Y., NEUMANN U.: Opacity shadow maps. In 12th Eurographics Workshop on Rendering 
Techniques (2001), pp. 177 182. [LV00] LOKOVIC T.,VEACH E.: Deep shadow maps. In Proceedings of SIGGRAPH 
2000 (2000), pp. 385 392. [MKBvR04] MERTENS T., KAUTZ J., BEKAERT P.,VAN REETH F.:Aself-shadow algorithm 
for dynamic hair using clustered densities. In Proceedings of Eurographics Symposium on Rendering 2004 
(2004), pp. 173 178. [MM06] MOON J. T., MARSCHNER S. R.: Simulating multiple scattering in hair using 
a photon mapping approach. In Proceedings of SIGGRAPH 2006 (2006), pp. 1067 1074. [WBK*07] WARD K., BERTAILS 
F., KIM T.-Y., MARSCHNER S.R.,CANI M.-P.,LIN M.:Asurvey on hair modeling: Styling, simulation, and rendering. 
IEEE TVCG 13,2(Mar-Apr 2007), 213 34. [Wil78] WILLIAMS L.: Casting curved shadows on curved surfaces. 
In SIGGRAPH 78 (1978), pp. 270 274. [XLJP06] XUS.,LAU F.C.,JIANG H.,PAN Y.:Anovel method for fast and 
high-quality rendering of hair. In Proc. EGSR 06 (2006), pp. 331 341. [ZSW04] ZINKE A.,SOBOTTKA G.,WEBER 
A.: Photorealistic rendering of blond hair. In Vision, Modeling, and Visualization 2004 (2004), pp. 
191 198. c . 2008 The Author(s) Journal compilation . 2008 The Eurographics Association and Blackwell 
Publishing Ltd. c Dual Scattering Approximation for Fast Multiple Scattering in Hair Arno Zinke * CemYuksel 
AndreasWeber JohnKeyser Institutf ur Informatik II Dept. of Computer Science ur Informatik II Institutf 
Dept. of Computer Science Universitat Bonn Texas A&#38;M University Universitat Bonn Texas A&#38;M 
University Path tracing (7.8 hours) Of.ine dual scattering (5.2 minutes) Real-time dual scattering (14 
fps) Using only single scattering (20 fps) Single scattering + diffuse (20 fps) Kajiya-Kay shading model 
(20 fps) Figure 1: Comparison of our method to path tracing and existing hair shading methods with deep 
opacity maps. Our dual scattering approximations (of.ine ray shooting and real-time GPU-based implementations) 
achieve close results to the path tracing reference without any parameter adjustment and with signi.cantly 
improved rendering times. Using single scattering only fails to produce the correct hair color. Adding 
an ad-hoc diffuse component or Kajiya-Kay shading model also fails to achieve the same realism, even 
after hand tweaking the diffuse color and shadow opacity to match the reference. The hair model has 50K 
strands and 1.4M line segments. Abstract When rendering light colored hair, multiple .ber scattering 
is essential for the right perception of the overall hair color. In this context, we present a novel 
technique to ef.ciently approximate multiple .ber scattering for a full head of human hair or a similar 
.ber based geometry. In contrast to previous ad-hoc approaches, our method relies on the physically accurate 
concept of the Bidirectional Scat * e-mail: zinke@cs.uni-bonn.de.de e-mail: cem@cemyuksel.com e-mail: 
weber@cs.uni-bonn.de.de e-mail: keyser@cs.tamu.edu tering Distribution Functions and gives physically 
plausible results with no need for parameter tweaking. We show that complex scattering effects can be 
approximated very well by using aggressive simpli.cations based on this theoretical model. When compared 
to unbiased Monte-Carlo path tracing, our approximations preserve photo-realism in most settingsbut with 
rendering times at least twoorders of magnitude lower. Time and space complexity are much lower compared 
to photon mapping-based techniques and we can even achieve realistic results in real-time on a standard 
PC with consumer graphics hardware. CR Categories: I.3.7[Computer Graphics]: Three-Dimensional Graphics 
and Realism Color, shading, shadowing, and texture Keywords: Hair rendering, multiple scattering, GPU 
algorithms 1 Introduction Accounting for multiple scattering is a key factor in the realistic rendering 
of human hair. Particularly in dense, light-colored hair, multiple scattering provides a critical component 
of the hair color, similar to the effect seen in subsurface scattering of translucent materials. Unfortunately, 
the high geometric complexity of hair models coupled with the complexity of the light interaction in 
hair volumes makes computing this multiple scattering effect dif.cult. Even for the simplest case of 
computing illumination due to a single point light source, contributions of several different light 
paths need to be determined to achieve reasonable visual accuracy. Recently, two different photon mapping 
based methods have been proposed to accelerate the multiple scattering computation in hair. Even though 
these methods present a signi.cant speed improvement as compared to brute force path tracing, they still 
require hours to compute a single frame and a large amount of memory to store the photon map. On the 
other hand, in real-time graphics the multiple scattering property of hair is overly simpli.ed and treated 
as transparent shadows (completely ignoring the effect of the circular shape of hair .bers) coupled 
with an ad-hoc diffuse component. This extreme simpli.cation prevents the use of physically based hair 
shaders, and gives hair a dull appearance. Furthermore, rigorous parameter tweaking is necessary to 
achieve acceptable results, the realism of which is always questionable. In this paper, we introduce 
the concept of dual scattering, which splits the multiple scattering computation into two components: 
global multiple scattering and local multiple scattering. The global multiple scattering component aims 
to compute the light traveling through the hair volume and reaching the neighborhood of the point of 
interest, while local multiple scattering accounts for the scattering events within this neighborhood. 
Exploiting physically based scattering properties of real hair .bers, we introduce several theoretical 
simpli.cations for computing both of these components; this permits extremely ef.cient multiple scattering 
computations with minimal accuracy compromise. As a result of our aggressive theoretical simpli.cations, 
the implementation of global multiple scattering becomes very similar to standard semi-transparent hair 
shadowing techniques, while local multiple scattering is modeled as a material property derived from 
hair .ber properties. Thetoprowof Figure1 comparesdifferent implementationsof our dual scattering method 
to path tracing. The path tracing image takes 7.8 hours to compute, while by using dual scattering we 
can reduce this time to 5.2 minutes in our of.ine implementation or even 14 frames per second with our 
GPU implementation. The precomputation time required for all dual scattering implementations is only 
a few seconds. As can be seen from these images, we can maintain visual accuracy with signi.cant improvement 
in computation time. Just as important, all calculations are based on computable or measurable values, 
so there is no unintuitive parameter tweaking (such as smoothing radius) that existed in previous physically 
based techniques. The bottom row of Figure 1 shows the results of single scattering only and existing 
hair shading methods with semi-transparent shadows using deep opacity maps. Even after rigorousparameter 
tweaking of ad-hoc diffuse color used by the shaders andthe opacity value used for the shadow computation, 
the results still differ signi.cantly from the path tracing reference. This is mainly because several 
important multiple scattering effects like directionality, color shifts, and successive blurring cannot 
be modeled by these oversimpli.ed formulations. Therefore, the realism that we achieve using our dual 
scattering approximation goes beyond existing real-time hair rendering methods. Note that dual scattering 
is not an ad-hoc addition for enhancing real-timehair rendering;itisaphysically-based simulation, the 
simplicity of which permits real-time implementation. In the next section we provide a brief summary 
of the previous work and the terminology we use throughout the paper. The theory of dual scattering is 
introduced in Section 3. We present the details of our different implementations in Section 4 and our 
results in Section5. Finally, we conclude witha shortdiscussionin Section6. 2 Background There is a 
large body of work on hair modeling and rendering in computer graphics. In this section we only review 
the techniques that are most relevant to our approach and we concentrate on physically based methods. 
For a more comprehensive overview of hair rendering please refer to the recent survey paper of Ward et 
al. [2007]. 2.1 Prior Work Marschner et al. [2003] presented the seminal work in the physically-based 
rendering of human hair. Their paper developed a far-.eld scattering model for hair based on measurements 
of actual hair. They modeled hair .bers as dielectric cylinders with colored interiors, and their model 
was able to account for important single scattering effects, such as multiple highlights, and deliver 
realistic results for dark colored hair. Further work showed that multiple .ber scattering is essential 
for correct perception of hair color, particularly for light colored hair. New models [Zinke et al. 2004; 
Moon and Marschner 2006; Zinke and Weber 2007] were developed to generalize the approach of Marschner 
et al. [2003] to account for these multiple scattering effects. In order to solve this more general 
illumination problem, methods such as path tracing can be used, though this often leads to prohibitive 
running times. Both Moon and Marschner[2006] and Zinke and Weber [2006] use photon mapping approaches 
to estimate the multiple scattering. Although both methods can produce accurate results similar to path 
tracing in many situations, they are computationally costly and require high resolution (memory consuming) 
photon maps. These methods cannot be made interactive. Yuksel et al. [2007] have also proposed an alternative 
projectionbased method for rendering global illumination for .bers. However, this approach makes several 
simpli.cations, e.g. neglecting inter-re.ections, that reduce accuracy. Anumber of other techniques have 
addressed some aspects of multiple scattering effects for hair volumes, especially in the context of 
self-shadowing for interactive hair rendering [Lokovic and Veach 2000; Kim and Neumann 2001; Mertens 
et al. 2004; Bertails et al. 2005; Xu et al. 2006; Hadwiger et al. 2006; Yuksel and Keyser 2008]. However, 
the main drawback of most of these methods is that they use ad-hoc simpli.cations and non-physical parameters, 
which cannot be derived from physical hair .ber properties. While plausible images can be produced by 
these methods in some situations, parameters need to be repeatedly tweaked to be appropriate for a particular 
scene and lighting condition. An exception is the work of Gupta and Magnenat-Thalmann [2005], which provides 
a more complex scattering-based approach. However, their approach is purely density-based and uses an 
ad-hoc volumetric scattering function without any physical basis. Therefore it cannot capture important 
phenomena such as directionality of multiple scattering, successive blurring, or subtle color shifting 
effects. For simulating volumetric light transport, cheap analytical multiple scattering models have 
been presented [Kniss et al. 2003; Premoze et al. 2004]. By taking into account optical properties of 
a participating mediaPremoze et al. [2004] use practical approximations to ef.ciently compute important 
features of multiply scattered light, such as spatial and angular spread of an incident beam. The radiative 
transfer is computed based on only a few prototypic path samples. However, even though this work is related 
in spirit to our approach, it has not yet been demonstrated to work with spatially varying and highly 
anisotropic scattering of hair .bers. Figure 2: De.nition of angles and directions 2.2 Terminology 
and Notation The discussion in this paper includes numerous symbols and terms that might not be familiar 
to readers. We follow the terminology and notation used by Marschner et al. [2003] and Zinke and Weber 
[2007]. While we would refer the reader to those sources for more complete descriptions, we give a brief 
review and overview of terminology here. Referring to Figure 2, consider the tangent to the hair .ber 
(i.e. a vector running down the central .ber axis), u. The normal plane is the plane perpendicular to 
u. Directions within this normal plane are referred to as azimuthal angles, and are expressed with the 
symbol f. An angle formed with respect to the normal plane is called the longitudinal inclination, and 
is expressed with the symbol .. For a point on the .ber, a direction is expressed by the symbol .. Thus, 
anydirection .a is equivalently expressed by the longitudinal inclination .a and the azimuthal angle 
fa. The amount of incoming light from one given direction, .i, scattered in another given outgoing direction, 
.o, is expressed by a Bidirectional Scattering Distribution Function. For a hair .ber, Zinke andWeber 
[2007] de.nedavery general Bidirectional Fiber Scattering Distribution Function that incorporated .ber 
geometry in addition to incident and output angles. By assuming the viewer and light source are suf.ciently 
far from the hair .ber, one can ignore the hair geometry and local (near-.eld) variations in geometry, 
resulting in a simpli.ed Bidirectional Curves Scattering Distribution Function (BCSDF), fs(.i,.o). The 
scattering function de.ned by Marschner et al. [2003] is one possible BCSDF. A hair .ber is generally 
thought of as a cylinder, and as light hits these cylinder boundaries, it can either be transmitted (T) 
or re.ected (R). Thus, TT refers to the light passing into then out of the .ber in a generally forward 
direction, while R and TRT refer to backward re.ections, either before (R) or after (TRT) passing through 
the .ber. Computing a BCSDF generally involves computing these three components, and more complex interactions 
(e.g. TRRT) are ignored. We follow both Marschner et al. [2003] and Zinke and Weber [2007] in using 
a formulation for the BCSDF as a product of a longitudinal function, Mt, and an azimuthal function, Nt 
for each of the three re.ection types t . (R,TT,TRT). M is modeled as a normalized Gaussian function 
g(at,(t)2)with mean at and standard deviation t speci.ed according to measured properties of hairs. 
N is precomputed in a 2D table of difference angles . =(.o - .i)/2 and f = fo - fi. See Marschner et 
al. s paper [2003] for more details on these computations. Note that in this paper, g(a,b)refers to 
a unit area Gaussian function de.ned in variable a with a zero mean and variance b.  3 Dual Scattering 
In this section we present the theoretical foundations of the dual scattering approximation. We simplify 
the computation of the complicated physical multiple scattering phenomena using properties of real human 
hair .bers and realistic human hair models. As a result of these simpli.cations, we achieve a physically-based 
formulation for multiple scattering, which can be implemented in a very similar way to standard hair 
shadowing techniques. Similar to Moon and Marschner [2006], in our rendering system we use one dimensional 
.bers, disregarding the illumination variation across the width of a hair strand. In this form, the general 
rendering equation for outgoing radiance Lo towards a direction .o at a point x can be written as 1 Lo(x,.o)= 
Li(x,.i)fs(.i,.o)cos .i d.i , (1) O where Li(x,.i) is the incident radiance from direction .i, fs(.i,.o)is 
the BCSDF of a hair .ber, and O is the set of all directions over the sphere. The incident radiance 
function Li(x,.i) includes all light paths scattered inside the hair volume such that 1 Li(x,.i)= Ld(.d).(x,.d,.i)d.d 
, (2) O where Ld(.d) is the incoming radiance from outside the hair volume from direction .d (assuming 
distant illumination), and .(x,.d,.i)is the multiple scattering function denoting the fraction of light 
entering the hair volume from direction .d that is scattered inside the hair volume and .nally arriving 
at point x from direction .i. The main concept behind the dual scattering method is to approximate the 
multiple scattering function as a combination of two components: global multiple scattering and local 
multiple scattering. The global multiple scattering function .G is used to compute the irradiance arriving 
at the neighborhood of a point x inside the hair volume, and the local multiple scattering function .L 
approximates the multiple scattering of this irradiance within the local neighborhood of x (Figure 3). 
Therefore, the multiple scattering function becomes the sum of global multiple scattering and global 
multiple scattering that goes through further local multiple scattering .(x,.d,.i)=.G(x,.d,.i)1+.L(x,.d,.i). 
(3) For the sake of simplicity we explain our dual scattering method assuming that the subject hair model 
is illuminated by a single directional light source. At the end of this section we explain how this 
approach can be extended for other light source types, multiple light sources, image based lighting, 
and global illumination. 3.1 Global Multiple Scattering Global multiple scattering is especially important 
for light colored hair types as they permit outside illumination to penetrate Figure 3: Based on intersections 
(red dots) along the shadow path (dashed line), the multiple scatteringdistributions .L and .G and the 
resulting outgoing radiance Lo are locally estimated. deeper into the hair volume. According to the measurements 
of Marschner et al. [2003], light scattering from human hair .bers is strongly anisotropic in the longitudinal 
direction, while it is much less anisotropic in the azimuthal direction. Because of this rather wide 
azimuthal scattering property of human hair, the computation of global multiple scattering should handle 
several different rather complicated light paths. On the other hand, the energy of light arriving at 
a point does not depend on the actual path, but the quality of the scattering events along the path. 
Assuming statistically independent scattering events, all other geometrical properties of the cluster 
(such as the distance between .bers) can be neglected. Moreover, a realistic human hair model presents 
strong local similarity in the orientation of neighboring hair .bers, therefore the probabilities of 
different light paths exhibit this similarity. Based on these observations, in our dual scattering method 
we simplify the computationofglobal multiple scatteringbyexploring the light scattering properties along 
only a single light path, namely the shadow path (in the direction .d), and use the information we gather 
for approximating the contributions of other possible paths (Figure 3). Along the shadow path, we classify 
all possible scattering directions of a scattering event into two groups: Forward Scattering and Backward 
Scattering, which correspond to all directions in the front and back half of the scattering cone relative 
to the original light source. Note that the strong TT component of hair .ber scattering is included 
in the front half-cone. Therefore, for light colored hair models forward scattering is signi.cantly 
stronger than backward scattering. Furthermore, light paths that go through a backward scattering before 
reaching the neighborhood of point x scatter away from this point, and they need another backward scattering 
to reverse their direction. As a result, for light colored hair types only a very small portion of global 
multiple scattering includes backward scattering. In our formulation we disregard these double (or more) 
backward scattered paths, approximating the global multiple scattering by only front scattered radiance. 
(Note that backward scattered paths are ignored only for the global multiple scattering computation and 
theywill be included in local multiple scattering). As a result, we approximate the global multiple scattering 
as .G(x,.d,.i) Tf (x,.d)Sf (x,.d,.i), (4) where Tf (x,.d)is the total transmittance and Sf (x,.d,.i)accounts 
for the spread of global multiple scattering into different directions. Therefore, to compute the global 
multiple scattering we need to evaluate these two functions. 3.1.1 Forward Scattering Transmittance The 
transmittance function Tf (x,.d)gives the total attenuation of a front scattered light path arriving 
at the point x. Therefore, the transmittance function depends on the number of scattering events n along 
the shadow path and the average attenuation af (.d)caused by each forward scattering event as n Tf (x,.d)= 
df (x,.d) af .dk , (5) k=1 where df (x,.d)is the density factor and .dk is the longitudinal inclination 
at the kth scattering event. Note that if n =0, the point x is illuminated directly and the transmittance 
function is set to 1. We use the density factor df (x,.d)to account for the fact that not all points 
x are located inside a dense cluster. Thus, the front scattered irradiance on x comes from only a subset 
of all directions as shown in Figure 4. Although the density factor theoretically depends on the hair 
density and the speci.c location of x, in practice we simply Figure 4: Cross section of a hair cluster. 
The .ber at x is not fully surrounded by other strands, so it can receive multiple scattered radiance 
only from the shaded sections. df accounts for .bers in the orange region, while db accounts for .bers 
in the blue region. use a constant value (between 0 and 1) to approximate this factor based on the overall 
hair density. In our experiments we found that for realistic human hair models densityfactors between 
0.6 and 0.8 give realistic results (as compared toapath tracing reference). For allexamplesinthis paper,the 
densityfactorissetto0.7. We compute the average attenuation af (.d)directly from the .ber scattering 
function fs as the total radiance on the front hemisphere due to isotropic irradiance along the specular 
cone: p 11 1 2 af (.d)= fs ((.d,f),.)cos.d dfd., (6)p -p Of 2 where Of is all directions over the front 
hemisphere and .d is the inclination of direct illumination at the scattering event. 3.1.2 Forward Scattering 
Spread The spread function Sf (x,.d,.i)approximates the .nal angular distribution of front scattered 
light to .nd the probability of radiance coming to the point x from direction .i. Because of the wide 
azimuthal scattering property of hair .bers, front scattered radiance quickly becomes almost isotropic 
in the azimuthal direction after only a few scattering events. However, in the longitudinal direction 
front scattered spread is still rather anisotropic. Therefore, we represent our spread function using 
a constant term s f for the azimuthal spread and a narrow Gaussian distribution function g for the longitudinal 
component: s f (fd,fi) 2 Sf (x,.d,.i)= g.d + .i ,sf (x,.d) , (7) cos.d where s f (fd,fi)is 1/pfor forward 
scattering directions and zero for backward scattering, and f (x,.d)is the total variance of for s2 
ward scattering in the longitudinal directions. Since the BCSDF ofa .ber is representedbya Gaussian distribution(M)in 
longitudinal directions, we can compute the total variance as the sum of variances of all scattering 
events along the shadow path n s2 2 .k f (x,.d)= fd , (8) k=1  where f 2(.dk)is the average longitudinal 
forward scattering variance of the kth scattering event, which is directly taken from the BCSDF of the 
hair .ber. Note that for a single directional light source when n =0, i.e. when the .ber is being illuminated 
directly, the spread function becomes a delta function d(.d -.i).  3.2 Local Multiple Scattering The 
local multiple scattering function accounts for the multiple scattering events within the neighborhood 
of the point x. Since light paths that go through only forward scattering are included in the global 
multiple scattering function, light paths of the local multiple scattering function must include at 
least one backward scattering. Because of this backward scattering, local multiple scattering is mostly 
smooth with subtle changes over the hair volume, yet it signi.cantly affects the visible hair color especially 
for light colored hair types. In our dualscattering method, we combine local multiple scattering and 
the BCSDF of the hair .bers, approximating the result with a densityfactor db and backscattering function 
fback as .L(x,.d,.i)fs(.i,.o) db(x,.d)fback(.i,.o). (9) Similar to forward scattering density factor 
df , backward scattering density factor db accounts for the hair density around the point x, and in 
practice we approximate it using a constant density term equal to df , which we set to 0.7. Note that 
fback is not a function of x, which means that it is modeled as a material property. We formulate fback 
as a product of an average backscattering attenuation function, Ab, and an average spread function, 
Sb, estimating  the bidirectional multiple backscattering distribution function of a hair cluster as 
2  fback(.i,.o)= Ab(.)Sb(.i,.o), (10) cos. where . =(.o -.i)/2is the difference angle of incident and 
outgoing inclinations and an additional cos . factor accounts for the fact that light is roughly scattered 
to a cone as in [Marschner et al. 2003]. To evaluate the effect of local multiple scattering we need 
 to compute average backscattering attenuation Ab(.)and average  backscattering spread Sb(.i,.o). Note 
that our formulation of .G and .Lfs have a similar structure; however, these two expressions are conceptually 
different: models an angular radiance distri .G bution whereas fback serves as a curve scattering term 
(BCSDF). 3.2.1 Average Backscattering Attenuation The average backscattering attenuation is computed 
for a point x inside a cluster of disciplined hair1. Since realistic hair models have strong similarity 
among neighboring hair strands, average attenuation computed for a disciplined hair cluster is a good 
approximation to be used in local multiple scattering. Furthermore, we ignore the slight change in the 
longitudinal inclination angle due to backward scattering events assuming that the absolute value of 
the longitudinal inclination |.|is the same for all .bers in the cluster. Local multiple scattering 
needs to account for the portion of light paths after they reach the neighborhood of point x. We consider 
only paths with an odd number of backward scattering events in this part of the light path. When there 
are an even number of backward scattering events in this part of the light path, the path points away 
from the light and does not return back to x; thus, such paths would not contribute to localmultiple 
scattering. Average backscattering attenuation for all light paths that include a single backward scattering 
is 8 L 2 ab a  A1(.)= ab af 2i = f 2 , (11) 1-a f i=1 where af (.) is the average forward scattering 
attenuation, and ab(.) is the average backward scattering attenuation. It is computed similar to Equation 
6 from .ber BCSDF for isotropic irradiance along the specular cone as p 11 1 2 ab(.d)= fs ((.d,f),.)cos.d 
dfd., (12)p -p Ob 2 1All hair .bers of a disciplined hair cluster share the same tangent where Ob is 
all directions over the back hemisphere. In other words, we consider all paths for which light forward 
scatters through i .bers, then backward scatters once, then again forward scatters all the way back through 
the same i .bers. Note that the special case with no forward scattering(i =0)is not included here, since 
it is handled separately within the single scattering computation. The average backscattering attenuation 
for light paths with three backward scattering is approximated by the analytical solution of the following 
triple sum: 8 i-1 8 LLL 32 aa 3 2(i-j-1+k) bf A3(.)= ab af = 3 . (13) 1-af 2 i=1 j=0 k=j+1 Here, 
we consider each possible case where light forward scatters through i .bers, backscatters, forward scatters 
through j<i .bers, backscatters, and forward scatters through k >j .bers before backscattering one last 
time and forward scattering all the way back through the i-j-1+k.bers again. Since ab(.)is small for 
human hair .bers, we disregard the paths with more than 3 backward scattering events, approximating 
the average backscattering attenuation as the sum of equations 11 and 13  Ab(.)= A1(.)+ A3(.). (14) 
 3.2.2 Average Backscattering Spread Similar to the forward scattering spread function in Equation 7, 
we represent backscattering spread as the product of a constant azimuthal term s b and a Gaussian function 
for the longitudinal spread as s b(fi,fo)  Sb(.i,.o)= g.o + .i -.b(.),sb 2(.) , (15) cos . where s 
b(fi,fo) is 1/p for backward scattering directions and  zero for forward scattering, .b(.)is the average 
longitudinal shift caused by the scattering events, and b (.)is the average longitudi s2 nal variance 
for backscattering. As in the computation of average backscattering attenuation, we consider all possible 
paths with one and three backward scattering events, and we compute average longitudinal shift using 
a weighted average of shifts due to all possible light paths 8 L 3 L ab 2i abm .b = af (2iaf + ab)+ 
af (3ab + maf ),  Ab Ab i=1 i,j,k ..8 .i-1 .8 where denotes , m is 2(i-j-1+k), i,j,k i=1 j=0 k=j+1 
and af (.)and ab(.)are average forward and backward scattering shifts, taken from the BCSDF of the 
hair .ber. Similarly, the average backscattering standard deviation is computed as 8 L 3 L ab 2i abm 
sb = af2if 2 + b 2 + af3b 2 + mf 2 ,  Ab Ab i=1 i,j,k 2 2 where f (.)and b (.)are average 
forward and backward scattering variances of the hair .ber BCSDF. In these formulations, we approximate 
the sum of the Gaussian functions of the individual scattering spreads by a single Gaussian function 
with a combined  mean (.b) and standard deviation (sb). This is a good approximation since for realistic 
hair BCSDF, longitudinal shifts are small and individual scattering lobes have a comparable standard 
deviation. In practice, we use the following analytical approximations2 2These analytical approximations 
are numerical .ts based on a power series expansion with respect to ab up to an order of three. to these 
sums given above: path (along the shadow path), all implementations of global multiple scattering are 
very similar to different semi-transparent hair 2(1-a 2 f )2 22 +4a  shadowing techniques. However, 
unlike these non-physical shad 2a a fb  .b b ab 1- +af (16) owing approaches that aim to compute 
a transmittance (or an opac f )2 2 f )3 (1-a(1-a ity) function along the shadow path, in our dual 
scattering method we compute front scattering transmittance Tf (x,.d)and front scat ab 2 2f 3 +  
2 a 2 2 +3 2 + bb f b tering variance s 2 f (x,.d), along with the fraction of direct illu 2 sb 
 1+0.7a . (17) f 2f +3b 3 b ab + a mination that reaches the point of interest without being blocked 
 3.3 General Rendering Equation The equations we derived up to here assume that the hair model is illuminated 
by a single directional light source such that .d is constant. However, our formulation can be extended 
to arbitrary light sources as long as estimating all global scattering based on the shadow path is still 
practicable. We can rewrite the general form of the rendering equation incorporating the terms for global 
and local multiple scattering as 1 LG Lo(x,.o)= i (x,.i)f(.i,.o)cos.i d.i , (18) (shadowed) by other 
hair strands. 4.1.1 Ray Shooting Ray shooting is the simplest implementation of the global multiple 
scattering function. The procedure is very similar to ray traced shadow computation. To .nd the forward 
scattering transmittance and spread at point x due to illumination from direction .d, we shoot a ray 
from x in the direction .d. If the ray does not intersect with any hair strands, Tf is taken as 1, sf 
is set to zero, and the direct illumination fraction becomes 1. If there is an intersection with a hair 
strand, the direct illumination fraction becomes zero and we update the transmittance and variance values 
using equations 5 and 8. While computing the transmittance (Equation 5) we use a one dimensional lookup 
table for af (.d), which is precomputedby O O where numerical integration of Equation 6. The ray shooting 
method pro 1 vides an accurate dual scattering approximation, however multiple LG i (x,.i)= Ld(x,.d).G(x,.d,.i)d.d 
(19) ray samples per pixel are needed to eliminate sampling noise.  4.1.2 Forward Scattering Maps is 
the globally multiple scattered light entering the hair volume from all directions before scattering 
towards x in direction .i, and To accelerate the global multiple scattering computation we implemented 
a two pass approach. We begin by generating a voxel grid f(.i,.o)= fs(.i,.o)+dbfback(.i,.o) (20) that 
encloses the hair geometry. Each voxel in this grid keeps Tf and s 2 f values along with a direct illumination 
fraction. In the .rst pass we trace rays from each light source towards thegrid, comput ing Tf and s 
2 f values along the ray. The values of each voxel are determined by the average of the values along 
the rays that inter sect with the voxel. In the second pass we render the hair geometry is the BCSDF 
including both single hair .ber scattering and backscattering from a collection of .bers. Hence, this 
general ren dering equation can handle multiple light sources, general shaped area lights, image based 
lighting from all directions, and full global illumination solution including multiple scattering in 
hair. and .nd the global multiple scattering values using linear interpolation from the voxel grid. 
In our implementation we also applied  4 Implementation For an ef.cient implementation of the dual 
scattering approximation we rewrite the general rendering equation, changing the order of the integrals 
in equations 18 and 19 as 1 Lo(x,.o)= Ld(x,.d)F(x,.d,.o)d.d , (21) multi-sampling to further smooth the 
results. Using forward scattering maps signi.cantly improves the rendering time as compared to the ray 
shooting method. Furthermore, this voxel grid can keep global multiple scattering information of multiple 
light sources within the same data structure; therefore, the performancegain of forward scattering maps 
becomes more significant as the number of light sources increase. On the other hand, O the accuracy 
and performance of forward scattering maps depend on the map resolution (i.e. the extent of a voxel). 
For all examples where presented in this paper the map resolution is 0.5cm. 1 G F(x,.d,.o)= .(x,.d,.i)f(.i,.o)cos.i 
d.i . (22) 4.1.3 GPU Implementation O Since Ld(x,.d)is known (incoming radiance), all we need to compute 
is the function F(x,.d,.o) to .nd the outgoing radiance. This computation has two main steps: the .rst 
stepgathers the necessary information to compute the global multiple scattering function .G(x,.d,.i)and 
the second one is the shading step that computes the above integral. 4.1 Computing Global Multiple 
Scattering The global multiple scattering function can be computed in many different ways, with the choice 
of method determining the speed and accuracy of the overall implementation. Since global multiple scattering 
is estimated by analyzing a single prototype light In our GPU implementation we used a similar approach 
to forward scattering maps based on the deep opacity maps method [Yuksel and Keyser 2008] developed for 
computing semi-transparent hair shadows. We chose the deep opacity maps method over other possible alternatives, 
since it generates high quality results with minimum computation and memory cost. Similar to deep opacity 
maps, in the .rst pass we render a depth map from the light s point of view. This depth map is used in 
the second pass to shape the map layers conforming to the shape of the hair style. Therefore, the hair 
volume illuminated by the light source can be accurately sampled using a very small number of layers. 
In the third and .nal pass, we render the .nal hair image using the map to .nd the global multiple scattering 
values. Insteadofkeepingasingle opacityvalueforeachmappixelassuggestedby the deep opacity maps method, 
we store7 values: Tf and sf values for each RGB color channel and the direct illumination fraction. 
Therefore, for every layer we need two textures to store all 7 values. Due to the ef.ciency of the deep 
opacity maps approach, we can achieve high quality results with as few as 4 layers, which are stored 
in8 textures. The latest consumer graphics cards support generation of all these 8 textures in a single 
pass using multiple drawbuffers. However, in our (earlier generation) GPU implementation we output 4 
textures per pass and generate our maps using two passes (one additional pass as compared to the original 
deep opacity maps implementation). 4.2 Shading Computation Once the global multiple scattering information 
is gathered, the shading step computes the rest of the integral in Equation 22. This step is the same 
for all implementations of dual scattering. We compute Equation 22 differently depending on whether 
or not the point x receives illumination directly without being blocked (shadowed) by other hair strands, 
which is determined by the direct illumination fraction value of the global multiple scattering information. 
When point x receives illumination directly (i.e. no scattering events occur along the shadow path) the 
integral in Equation 22 evaluates to f(.d,.o)cos.d. Here f(.d,.o) is the sum of the single scattering 
component and the average backscattering component fback as given in Equation 20. The computation of 
the single scattering component is identical to [Marschner et al. 2003] and [Zinke et al. 2004], such 
that the longitudinal scattering function M, which is modeled as a Gaussian function, is multiplied by 
a precomputed azimuthal scattering function N. The backscattering component fback is computed from Equation 
10 using precomputed s2 tables for Ab(.), .b(.), and b (.). When point x is blocked by other hair 
strands, it receives illumination via global multiple scattering. In this case the azimuthal forward 
scattering spread on x becomes isotropic and the longitudinal forward scattering spread is represented 
by a narrow Gaussian function as given in Equation 7. For ef.cient computation of Equation 22, instead 
of using numerical integration techniques, we approximate the result by combining azimuthal and longitudinal 
components of Sf and f. The combination of longitudinal components can be easily approximated for narrow 
Gaussian functions by using combined variances 2 and 2 and fback respectively, s back for fs such 
that 2 22 s (x,.d,.i)= sf (x,.d)+(.),and (23) 2 22 back(x,.d,.i)= sf (x,.d)+sb (.), (24) where 
2(.)is the variance of the scattering lobe of the BCSDF3. In the azimuthal direction, fback is already 
isotropic; therefore, isotropic forward scattering does not affect this component. However, the azimuthal 
component N of fs changes under isotropic azimuthal irradiance. We precompute this component NG and 
store it in a 2D table similar to N using numerical integration of p 1 2 NG(.,f)= s f (f)N(.,f ' )df 
' . (25) -p 2 Note that this formulation ignores the elipticity of hair .bers. However, this approximation 
is still accurate enough even for elliptic hair .bers, since s f =1/pis isotropic (constant) and NG is 
averaging fs along the front half cone. 3The BCSDF has three lobes, one for each component: R, TT, and 
TRT Table 1: Precomputed tables used for shading Ab(.)Function Equation 14 Reference Use local multiple 
scattering .b(.) Equation 16 local multiple scattering s2 b (.) Equation 17 local multiple scattering 
NG(.,f) Equation 25 BCSDF due to forward scattering F(Tf,sf2 , directFraction) // Compute local multiple 
scattering contribution s2 s22 fback . 2Ab(.)g(.d + .o - .b(.), (.)+)/(p cos .) bf // Compute BCSDF 
of the .ber due to direct illumination MR,TT,TRT . g(. - aR,TT,TRT,2 ) R,TT,TRT fdirect . MRNR(.,f)+ 
MTT NTT(.,f)+ MTRTNTRT (.,f) Fdirect fdirect s . directFraction + db fback s // Compute BCSDF of the 
.ber due to forward scatterd illumination similarly MG . g(. - aR,TT,TRT,2 +s2 ) R,TT,TRT R,TT,TRT 
f fscatter s R R TT TTTRTTRT . MGNG(.,f)+ MG NG (.,f)+ MG NG (.,f) Fscatter fscatter . (Tf - directFraction) 
df + pdb fback s // Combine direct and forward scattered components return (Fdirect + Fscatter )cos 
.i Figure 5: Pseudo code of our dual scattering shader. a and  values are measured hair characteristics 
(part of the BCSDF) and computation of M and N is discussed in Section 2.2. Other precomputed tables 
are given inTable 1. To accelerate most computations we use small precomputed tables that store complicated 
hair .ber properties. These tables are listed inTable 1. The precomputations are simply numerical integrations 
of the given equations. In addition to these tables, we use a precomputed table for the azimuthalpart 
of single .ber scattering, N(.,f), which can be computed using techniques described in [Marschner et 
al. 2003] or [Zinke andWeber 2007]. Figure 5 shows the pseudo code for computing Equation 22 using this 
procedure. This pseudo code is a simple extension to the BCSDF shading computation; therefore, our method 
can be easily integrated into existing physically based hair rendering systems.  5 Results To test the 
validity of our simpli.cations, we compared the results of our dual scattering approximation directly 
to path tracing. Figure 6 shows such a comparison for a disciplined cluster of blond hair, where the 
single scattering component is excluded to compare the results of multiple scattering only. As can be 
seen from these images, our approximation generates very similar results to path tracing regardless of 
the incident light direction. Although there are subtle differences compared to the results obtained 
with unbiased path tracing, the general look is very similar and irregularities that occur in real hairstyles 
tend to conceal such errors. The graph in Figure 7 shows the comparison of radiance values in the middle 
of the clusters in Figure 6. Here, the average BRDF of multiple scattered light is shown for longitudinal 
and azimuthal sweeps along the cluster for various absorption coef.cients. As can be seen from this graph, 
the azimuthal component ofbackward scattering (white region of the left graph) is almost isotropic, 
while the longitudinal component resembles a Gaussian distribution as modeled in our formulation. Even 
though the approximation slightly overshoots the accurate simulation for very low coef.cients, the results 
indicate that our approach is a viable approximation for scattering from a cluster. dual scattering 
path tracing dual scattering path tracingdual scattering path tracing Figure 6: Comparison (multiple 
scattering only) of dual scattering using ray shooting to path tracing for a cube shaped disciplined 
hair cluster consisting of 64K hair .bers illuminated by a single directional light source (light direction 
is shown on the left). We also tested the accuracyof our dual scattering method for different hair .ber 
scattering properties. Figure8 and Figure9 show that dual scattering provides a good approximation for 
various hair colors and longitudinal Gaussian widths (standard deviations). Slight changes of the optical 
properties of a .ber (such as absorption) may have a drastic impact on the overall hair color. As can 
been in these .gures, our approximation handles such variations correctly. Despite the common intuition, 
it is more dif.cult to perceive errors in complicated models (where detail can hide problems), while 
even the slightest errors show up on simple models. We show a complicated hair model example in Figure 
10. Since dual scattering only exploits local symmetry in the hair model, most of our assumptions still 
hold and we can produce a close approximation to the path tracing reference with signi.cantly improved 
rendering times. Figure 7: The average BRDF (multiple scattering only) at a blond hair cluster computed 
for the red, green and blue components of light blond hair (see Figure 6) using path tracing (solid lines) 
and dual scattering approximation (dashed lines). The scene is illuminated by a directional front light 
source which means that for f 0 (white area) the results are dominated by purely local multiple scattering(fback)since 
no visible .bers are shadowed. path tracing reference dual scattering (ray shooting) Figure 8: Comparison 
for various hair colors with RGB absorption coef.cients (from left to right) (0.03,0.07,0.15), (0.15,0.2,0.3), 
(0.2,0.3,0.5), and (0.3,0.6,1.2). path tracing reference dual scattering (ray shooting) Figure 9: Comparison 
for varying longitudinal widths R, TT , and T RT of the BCSDF (from left to right) (4, 5, 7.5), (8, 
10, 15), (16, 20, 30) Figure 11 compares the of.ine implementations of dual scattering to path tracing 
and single scattering as well as to photon mapping (using ray based global illumination [Zinke andWeber 
2006]) techniques. As can be seen from these images, while single scattering produces a dark image that 
fails to reproduce the correct color of hair, all other methods produce similar results to path tracing, 
while of.ine implementations of dual scattering signi.cantly improve the rendering speed. Figure 12presents 
captured frames from our GPU based implementation of dual scattering, and comparisons to of.ine dual 
scattering (ray shooting) and path tracing. The .gure shows three scattering components (single, global 
multiple, and local multiple) that produce the full dual scattering solution when combined. For this 
hair style the performance of the GPU implementation of dual scattering (12 fps) is comparable to the 
performance of a deep opacity maps [YukselandKeyser2008] implementation(18fps). Usingextended multiple 
draw buffers, the additional pass we use for GPU-based dual scattering canbe eliminated, whichwould signi.cantly 
reduce the performancegap between our method andexisting non-physical real-time solutions. 6 Discussion 
and Conclusion Dual scatteringoffersaphysically basedsimpli.cationtothe complicated phenomenon of multiple 
scattering in hair, exploiting scattering properties of hair .bers (such as narrow scattering along 
longitudinal directions and wide scattering along azimuthal directions) and general characteristics 
of human hair models (such as local sim Single Scattering Only Path Tracing Reference Dual Scattering 
Dual Scattering Dual Scattering (of.ine) (of.ine) (ray shooting) (forward scattering map) (GPU-based) 
3 minutes 22 hours 9.6 minutes 4.6 minutes 5.8 fps  0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Figure 
13: The effect of forward and backward scattering density factors df (x,.d)and db(x,.d). Values of 0.0 
are equivalent to single scattering only.For most human hair stylesvalues between 0.6 and 0.8generate 
close approximations to path tracing. ilarity of .ber directions). By splitting multiple scattering into 
a local and a global part and using different approximations regarding the directionality of scattering 
from hair .bers, our dual scattering approach is orders of magnitude faster than other accurate techniques. 
We justify our simpli.cations by not only the theory but also several comparisons to ground truth (path 
tracing) in both experimental setups and realistic cases. It is important to note that all of the computations 
described rely on physically-based values. All parameters are either fundamental to the virtual scene 
(e.g. directions such as .i), or are characteristics of real hair that can be accurately measured and 
described (e.g. the a and  values, and the BCSDF description). The only user adjustable term is the 
densityfactor(df and db), but even this has a physical meaning that limits the range of choices and, 
in theory, it could be computed precisely. Figure 13 shows the effect of changing density parameters. 
The multiple scattering computation simpli.cations introduced in this paper are based on theoretical 
approximations rather than adhoc appearance-based formulations. As a result, despite the aggressive 
simpli.cations we have made, we obtain close approximations with no parameter adjustment and muchfaster 
computations. On the other hand, one can come up with special hair models that would break some of our 
assumptions. For instance, if the mean path length is large (as in sparse hairstyles) or if attenuation 
coef.cients are very small, the global structure of a hairstyle tends to play an important role that 
biases the results. Furthermore, the assumption that neighboring hair strands exhibit a similar structure 
can be violated in chaotic hair models. However, even for complicated cases our results look plausible 
and close to reference images with signi.cantly improved computation times. One limitation of our formulation 
arises from the fact that we use the shadow path as a prototype for all signi.cant multiple scattered 
paths. Therefore, when there is a strong spatial variation in illumination, this prototype path assumption 
may be violated. For example, a hard shadow edge falling across the hair creates a sharp illumination 
change, hence our prototype path approximation would be less accurate along the shadow boundary. We believe 
that separating local and global multiple scattering is a very general principle that is applicable not 
only in the realm of hair rendering, but also for other highly scattering quasi-homogeneous structures 
such as snow, clouds or woven textiles.  Acknowledgements We would like to thank Murat Afsar for the 
head model, and Anton Andriyenko for the hair model in Figures 8, 9 and 11. We would also like to thank 
the anonymous reviewers for their helpful comments. This work is supported in part by NSF grant CCR-0220047 
and DeutscheForschungsgemeinschaft under grantWe1945/3-2. References BERTAILS, F., M A practi- ENIER,C., 
AND CANI,M.-P. 2005. cal self-shadowing algorithm for interactive hair animation. In Graphics Interface, 
71 78. GUPTA, R., AND MAGNENAT-THALMANN, N. 2005. Scatteringbased interactive hair rendering. In Comp. 
Aided Design and Comp. Graphics, 489 496. HADWIGER, M., KRATZ, A., SIGG, C., AND B UHLER, K. 2006. Gpu-accelerated 
deep shadow maps for direct volume rendering. In Proceedings of Graphics Hardware 2006, 49 52. KIM, T.-Y., 
AND NEUMANN, U. 2001. Opacity shadow maps. In Eurographics RenderingWorkshop, 177 182. KNISS, J., PREMOZE, 
S., HANSEN, C., SHIRLY, P., AND MCPHERSON, A. 2003. A model for volume lighting and modeling. IEEETrans. 
onVis. and Comp. Graphics9, 2, 150 162.  single scattering only   dual scattering (forward scattering 
maps) -7 minutes Figure 11: Comparison of a blond hairstyle (87K strands) viewed from three different 
perspectives (illuminated by three directional light sources). LOKOVIC, T., AND VEACH, E. 2000. Deep 
shadow maps. In Proceedings of SIGGRAPH 2000, 385 392. MARSCHNER, S.R., JENSEN, H. W., CAMMARANO, M., 
WOR-LEY, S., AND HANRAHAN, P. 2003. Light scattering from human hair .bers. ACMTransactions on Graphics 
22, 3, 780 791. SIGGRAPH 2003. MERTENS,T.,KAUTZ,J.,BEKAERT,P., AND REETH,F.V. 2004. A self-shadow algorithm 
for dynamic hair using density clustering. In Eurographics Symposium on Rendering, 173 178. MOON, J. 
T., AND MARSCHNER, S. R. 2006. Simulating multiple scattering in hair using a photon mapping approach. 
ACM Transactions on Graphics 25, 3, 1067 1074. SIGGRAPH 2006. PREMOZE, S., ASHIKHMIN, M., RAMAMOORTHI, 
R., AND NA-YAR, S. 2004. Practical rendering of multiple scattering effects in participating media. In 
Eurographics Symp. on Rendering. WARD, K., BERTAILS, F., KIM, T.-Y., MARSCHNER, S. R., CANI, M.-P., AND 
LIN, M. 2007. A survey on hair model path tracing dual scattering (of.ine) dual scattering (real-time) 
single scattering global multiple scattering local multiple scattering Figure 12: Components of dual 
scattering method captured from our real-time implementation (12 fps) and comparisons to of.ine dual 
scattering using ray shooting (11.2 minutes) and path tracing reference (11.8 hours). The hair model 
has 50K strands and 2.4M line segments. ing: Styling, simulation, and rendering. IEEE Transactions on 
Visualization and Computer Graphics 13, 2, 213 34. XU, S., LAU, F. C., JIANG, H., AND PAN, Y. 2006. A 
novel method for fast and high-quality rendering of hair. In Proc. of the 17th Eurographics Symp. on 
Rendering, 331 341, 440. YUKSEL, C., AND KEYSER, J. 2008. Deep opacity maps. Computer GraphicsForum 
(Proc. of EUROGRAPHICS 2008) 27, 2. YUKSEL, C., AKLEMAN, E., AND KEYSER, J. 2007. Practical global illumination 
for hair rendering. In Paci.c Graphics 2007, 415 418. ZINKE, A., AND WEBER, A. 2006. Global illumination 
for .ber based geometries. In Electronic proceedings of the Ibero American Symposium on Computer Graphics(SIACG 
2006). ZINKE, A., AND WEBER, A. 2007. Light scattering from .laments. IEEETrans. onVis. and Comp. Graphics 
13, 2, 342 356. ZINKE, A., SOBOTTKA, G., AND WEBER, A. 2004. Photorealistic rendering of blond hair. 
In Vision, Modeling, and Visualization (VMV) 2004, 191 198.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837103</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>78</pages>
		<display_no>2</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[An introduction to 3D spatial interaction with video game motion controllers]]></title>
		<page_from>1</page_from>
		<page_to>78</page_to>
		<doi_number>10.1145/1837101.1837103</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837103</url>
		<abstract>
			<par><![CDATA[<p>3D spatial interfaces [Bowman et al. 2004] give users the ability to spatially interact with 3D virtual worlds because they provide natural mappings from human movement to interface controls. These interfaces, common in virtual and augmented reality applications, give users, rich, immersive, and interactive experiences that can mimic the real world or provide magical, larger than life interaction metaphors [Katzourin et al. 2006].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264990</person_id>
				<author_profile_id><![CDATA[81100283513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[LaViola]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264991</person_id>
				<author_profile_id><![CDATA[81100484036]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Marks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Computer Entertainment America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, R., and Bishop, G. 1994. Improving static and dynamic registration in an optical see-through hmd. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1536527</ref_obj_id>
				<ref_obj_pid>1536513</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bott, J., Crowley, J., and LaViola, J. 2009. Exploring 3d gestural interfaces for music creation in video games. In <i>Proceedings of The Fourth International Conference on the Foundations of Digital Games 2009</i>, 18--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253301</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., and Hodges, L. F. 1997. An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 35--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836072</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Koller, D., and Hodges, L. F. 1997. Travel in immersive virtual environments: An evaluation of viewpoint motion control techniques. In <i>Proceedings of the Virtual Reality Annual International Symposium</i>, 45--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618556</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Wineman, J., Hodges, L. F., and Allison, D. 1998. Designing animal habitats within an immersive ve. <i>IEEE Comput. Graph. Appl. 18</i>, 5, 9--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993837</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bowman, D. A., Kruijff, E., LaViola, J. J., and Poupyrev, I. 2004. <i>3D User Interfaces: Theory and Practice</i>. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1581092</ref_obj_id>
				<ref_obj_pid>1581073</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Charbonneau, E., Miller, A., Wingrave, C., and LaViola, Jr., J. J. 2009. Understanding visual interfaces for the next generation of dance-based rhythm video games. In <i>Sandbox '09: Proceedings of the 2009 ACM SIGGRAPH Symposium on Video Games</i>, ACM Press, 119--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147199</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Conner, B. D., Snibbe, S. S., Herndon, K. P., Robbins, D. C., Zeleznik, R. C., and van Dam, A. 1992. Three-dimensional widgets. In <i>SI3D '92: Proceedings of the 1992 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 183--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Crassidis, J. L., and Markley, F. L. 2003. Unscented filtering for spacecraft attitude estimation. <i>Journal of Guidance, Control, and Dynamics 26</i>, 4, 536--542.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>954544</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Duda, R. O., Hart, P. E., and Stork, D. G. 2001. <i>Pattern Classification</i>. John Wiley and Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>168657</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Feiner, S., MacIntyre, B., Haupt, M., and Solomon, E. 1993. Windows on the world: 2d windows for 3d augmented reality. In <i>UIST '93: Proceedings of the 6th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 145--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Friedman, J. H. 1996. Another approach to polychotomous classification. Tech. rep., Department of Statistics, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Giansanti, D., Macellari, V., and Maccioni, G. 2003. Is it feasible to reconstruct body segment 3-d position and orientation using accelerometric data. <i>IEEE Trans. Biomed. Eng 50</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Gunturk, B. K., Glotzbach, J., Altunbasak, Y., and Schafer, R. W. 2005. Demosaicking: color filter array interpolation. <i>IEEE Signal processing magazine 22</i>, 44--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192501</ref_obj_id>
				<ref_obj_pid>192426</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hinckley, K., Pausch, R., Goble, J. C., and Kassell, N. F. 1994. A survey of design issues in spatial input. In <i>Proc. ACM UIST'94 Symposium on User Interface Software &amp; Technology</i>, ACM, 213--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2196253</ref_obj_id>
				<ref_obj_pid>2195920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[
Hoffman, M., Varcholik, P., and LaViola, J. 2010. Breaking the status quo: Improving 3d gesture recognition with spatially convenient input devices. In <i>IEEE Virtual Reality 2010</i>, IEEE Press, 59--66.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Intel. 1999. Video as input (vai) white paper. Tech. rep., Intel Architecture Labs.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Julier, S., and Uhlmann, J. 1997. A new extension of the kalman filter to nonlinear systems. In <i>Int. Symp. Aerospace/Defense Sensing, Simul. and Controls</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858134</ref_obj_id>
				<ref_obj_pid>857202</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Kato, H., and Billinghurst, M. 1999. Marker tracking and hmd calibration for a video-based augmented reality conferencing system. In <i>IWAR 99: Proceedings of the 2nd IEEE and ACM International Workshop on Augmented Reality</i>, 85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187631</ref_obj_id>
				<ref_obj_pid>1187619</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Katzourin, M., Ignatoff, D., Quirk, L., LaViola, J., and Jenkins, O. C. 2006. Swordplay: Innovating game development through vr. <i>IEEE Computer Graphics and Applications 26</i>, 6, 15--19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1328241</ref_obj_id>
				<ref_obj_pid>1328202</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Kratz, L., Smith, M., and Lee, F. J. 2007. Wiizards: 3d gesture recognition for game play input. In <i>Future Play '07: Proceedings of the 2007 conference on Future Play</i>, ACM, New York, NY, USA, 209--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313289</ref_obj_id>
				<ref_obj_pid>1313055</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. J., and Zeleznik, R. C. 2007. A practical approach for writer-dependent symbol recognition using a writer-independent symbol recognizer. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 29</i>, 11, 1917--1926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. 2000. Msvt: A virtual reality-based multimodal scientific visualization tool. In <i>Proceedings of the Third IASTED International Conference on Computer Graphics and Imaging</i>, 1--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>769976</ref_obj_id>
				<ref_obj_pid>769953</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[LaViola, J. J. 2003. Double exponential smoothing: an alternative to kalman filter-based predictive tracking. In <i>EGVE '03: Proceedings of the workshop on Virtual environments 2003</i>, ACM, New York, NY, USA, 199--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1449423</ref_obj_id>
				<ref_obj_pid>1449377</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Lee, J. 2008. Hacking the nintendo wii remote. <i>Pervasive Computing, IEEE 7</i>, 3 (July-Sept.), 39--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344341</ref_obj_id>
				<ref_obj_pid>344326</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Luinge, H., Veltink, P., and Baten, C. 1999. Estimating orientation with gyroscopes and accelerometers. <i>Technology and Health Care 7</i>, 6, 455--459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Mapes, D., and Moshell, M. 1995. A two-handed interface for object manipulation in virtual environments. <i>Presence: Teleoper. Virtual Environ. 4</i>, 4, 403--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Marks, R., Scovill, T., and Michaud-Wideman, C. 2001. Enhanced reality: A new frontier for computer entertainment. In <i>SIGGRAPH 2001: ConferenceAbstracts and Applications</i>, ACM, 117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Marks, R., Deshpande, R., Kokkevis, V., Larsen, E., Michaud-Wideman, C., and Sargaison, S. 2003. Real-time motion capture for interactive entertainment. In <i>SIGGRAPH 2003: Emerging Technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Marks, R. 2000. Medieval chamber. In <i>SIGGRAPH 2000: Emerging Technologies</i>, ACM, etech58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258747</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Mine, M. R., Brooks, Jr., F. P., and Sequin, C. H. 1997. Moving objects in space: exploiting proprioception in virtual-environment interaction. In <i>SIGGRAPH '97: Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 19--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897820</ref_obj_id>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Mine, M. 1995. Virtual environment interaction techniques. Tech. rep., UNC Chapel Hill CS Dept.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218495</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Pausch, R., Burnette, T., Brockway, D., and Weiblen, M. E. 1995. Navigation and locomotion in virtual worlds via flight into hand-held miniatures. In <i>SIGGRAPH '95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 399--400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253303</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Pierce, J. S., Forsberg, A. S., Conway, M. J., Hong, S., Zeleznik, R. C., and Mine, M. R. 1997. Image plane interaction techniques in 3d immersive environments. In <i>SI3D '97: Proceedings of the 1997 symposium on Interactive 3D graphics</i>, ACM, New York, NY, USA, 39--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237102</ref_obj_id>
				<ref_obj_pid>237091</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Poupyrev, I., Billinghurst, M., Weghorst, S., and Ichikawa, T. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in vr. In <i>UIST '96: Proceedings of the 9th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, 79--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531517</ref_obj_id>
				<ref_obj_pid>1531514</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Rehm, M., Bee, N., and Andr&#233;, E. 2008. Wave like an egyptian: accelerometer based gesture recognition for culture specific interactions. In <i>BCS-HCI '08: Proceedings of the 22nd British HCI Group Annual Conference on HCI 2008</i>, British Computer Society, Swinton, UK, UK, 13--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>354667</ref_obj_id>
				<ref_obj_pid>354666</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Rekimoto, J., and Ayatsuka, Y. 2000. Cybercode: designing augmented reality environments with visual tags. In <i>DARE 2000: Proceedings of the 2000 ACM Conference on Designing Augmented Reality Environments</i>, ACM, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122753</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Rubine, D. 1991. Specifying gestures by example. In <i>SIGGRAPH '91: Proceedings of the 18th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 329--337.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1624417</ref_obj_id>
				<ref_obj_pid>1624312</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Schapire, R. E. 1999. A brief introduction to boosting. In <i>IJCAI '99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</i>, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1401--1406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1347395</ref_obj_id>
				<ref_obj_pid>1347390</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Schl&#246;mer, T., Poppinga, B., Henze, N., and Boll, S. 2008. Gesture recognition with a wii controller. In <i>TEI '08: Proceedings of the 2nd international conference on Tangible and embedded interaction</i>, ACM, New York, NY, USA, 11--14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1274966</ref_obj_id>
				<ref_obj_pid>1274940</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Shirai, A., Geslin, E., and Richir, S. 2007. Wiimedia: motion analysis methods and applications using a consumer video game controller. In <i>Sandbox '07: Proceedings of the 2007 ACM SIGGRAPH symposium on Video games</i>, ACM, New York, NY, USA, 133--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409076</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Shiratori, T., and Hodgins, J. K. 2008. Accelerometer-based user interfaces for the control of a physically simulated character. <i>ACM Trans. Graph. 27</i>, 5, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Shivaram, G., and Seetharaman, G. 1998. A new technique for finding the optical center of cameras. In <i>ICIP 98: Proceedings of 1998 International Conference on Image Processing</i>, vol. 2, 167--171.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1520648</ref_obj_id>
				<ref_obj_pid>1520340</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Silva, M. G., and Bowman, D. A. 2009. Body-based interaction for desktop games. In <i>CHI EA '09: Proceedings of the 27th international conference extended abstracts on Human factors in computing systems</i>, ACM, New York, NY, USA, 4249--4254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>223938</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Stoakley, R., Conway, M. J., and Pausch, R. 1995. Virtual reality on a wim: interactive worlds in miniature. In <i>CHI '95: Proceedings of the SIGCHI conference on Human factors in computing systems</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Wan, E. A., and Van Der Merwe, R. 2002. The unscented kalman filter for nonlinear estimation. 153--158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Williamson, R., and Andrews, B. 2001. Detecting absolute human knee angle and angular velocity using accelerometers and rate gyroscopes. <i>Medical and Biological Engineering and Computing 39</i>, 3, 294--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2013903</ref_obj_id>
				<ref_obj_pid>2013876</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[
Williamson, B., Wingrave, C., and Laviola, J. 2010. Realnav: Exploring natural user interfaces for locomotion in video games. In <i>Proceedings of IEEE Symposium on 3D User Interfaces 2010</i>, IEEE Computer Society, 3 -- 10.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1749428</ref_obj_id>
				<ref_obj_pid>1749397</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Wingrave, C., Williamson, B., Varcholik, P., Rose, J., Miller, A., Charbonneau, E., Bott, J., and LaViola, J. 2010. Wii remote and beyond: Using spatially convenient devices for 3duis. <i>IEEE Computer Graphics and Applications 30</i>, 2, 71--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>215647</ref_obj_id>
				<ref_obj_pid>215585</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Wloka, M. M., and Greenfield, E. 1995. The virtual tricorder: a uniform interface for virtual reality. In <i>UIST '95: Proceedings of the 8th annual ACM symposium on User interface and software technology</i>, ACM, New York, NY, USA, 39--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Introduction to 3D Spatial Interaction with Video Game Motion Controllers ASIGGRAPH 2010 Course 
Tuesday,July, 27, 2010 2:00PM -5:15 PM Joseph J. LaViola Jr. Richard L. Marks School of EECS SonyComputer 
Entertainment US R&#38;D University of Central Florida SonyComputer Entertainment America jjl@eecs.ucf.edu 
Richard Marks@Playstation.sony.com Contents 1 Course Introduction 1 1.1 CourseTopics ......................................... 
1 1.2 Course Schedule ....................................... 2 1.3 Speaker Biographies ..................................... 
4 1.4 GameUI -Past,Present,andFuture ............................. 6 1.4.1 Input ControlandGameComplexity......................... 
8 1.4.2 3DUIinGamesToday ................................ 9 1.4.3VideoGamesandModern Controllers ........................ 
9 2 Common Tasks in 3D User Interfaces 11 2.1 Navigation........................................... 
12 2.1.1 Gaze-Directed Steering ................................ 13 2.1.2 Pointing........................................ 
13 2.1.3 Map-basedTravel................................... 13 2.1.4 GrabbingtheAir ................................... 
14 2.2 Selection ........................................... 15 2.2.1 TheVirtualHand ................................... 
15 2.2.2 Ray-Casting ..................................... 15 2.2.3 OcclusionTechniques ................................ 
16 2.2.4 Arm-Extension.................................... 16 2.3 Manipulation ......................................... 
17 2.3.1 HOMER ....................................... 18 2.3.2 Scaled-WorldGrab .................................. 
18 2.3.3World-in-Miniature .................................. 19 2.4 System Control ........................................ 
20 2.4.1 GraphicalMenus ................................... 20 2.4.2Voice Commands ................................... 
22 2.4.3 Gesturesand Postures ................................ 22 2.4.4Tools ......................................... 
23 3 3D Interfaces with 2D and 3D Cameras 25 3.1 EyeToy ............................................ 
25 3.1.1 Motion Detection ................................... 25 3.1.2 ColorTracking .................................... 
26 3.2 PlayStationEye ........................................ 28 3.2.1FaceTracking..................................... 
29 3.2.2 MarkerTracking ................................... 30 3.3 3D Cameras .......................................... 
30 3.3.1TechnologyOverview ................................ 31 3.3.2Foreground/Background Separation 
......................... 32 3.3.3 AugmentedReality .................................. 33 3.3.4 Real-time 
MotionCapture .............................. 33 4 Working with the Nintendo Wiimote 36 4.1WiimoteBasics 
........................................ 36 4.1.1 ConnectingtheWiimote ............................... 
36 4.1.2 Framesof Reference ................................. 37 4.1.3 The SensorBar Connection ............................. 
38 4.1.4 3-Axis Accelerometer ................................ 40 4.1.5Wii MotionPlus .................................... 
40 4.2 Design Considerations .................................... 41 4.2.1Waggling Motions .................................. 
41 4.2.2 Compensationby Story .............................. 41 4.3 InterpretingWiimoteData .................................. 
42 4.3.1 Integration ...................................... 42 4.3.2 Recognitionthrough Heuristics 
........................... 43 4.4 DesigningforUniversal3D Interaction ............................ 
45 4.4.1 Selection ....................................... 45 4.4.2 Manipulation..................................... 
46 4.4.3Travel......................................... 47 4.4.4 System ControlandSymbolicInput ......................... 
48 4.5 Case Studies .......................................... 49 4.5.1 RealNav ........................................ 
49 4.5.2 OneManBand .................................... 49 4.5.3 RealDance ...................................... 
50 4.5.4WoWNavigation ................................... 51 4.5.5Wiisoccer ....................................... 
52 5 3D Spatial Interaction with the PlayStation Move 53 5.1 PlayStationMove Characteristics ............................... 
53 5.2 PlayStationMoveTechnology ................................ 55 5.2.1 Image Analysis .................................... 
55 5.2.2 Sensor Fusion ..................................... 56 5.3 SpatialInputvs. Buttons ................................... 
56 5.4 1-to-1DataMappingvs.OtherMappings .......................... 57 5.4.1 1-to-1 Bene.tsand Challenges 
............................ 58 5.4.2 Other PossibleData Mappings ............................ 59 6 
3D Gesture Recognition Techniques 60 6.1 Linear Classi.er ........................................ 60 
 6.2 AdaBoost Classi.er ...................................... 61 6.2.1Weak LearnerFormulation .............................. 
62 6.2.2 AdaBoost Algorithm ................................. 62 6.3 FeatureSet .......................................... 
63 6.4 GestureSet .......................................... 64 6.5 3D GestureData Collection .................................. 
64 6.6 3D Gesture Recognition Experiments ............................ 66 6.6.1 User Dependent Recognition 
Results ........................ 67 6.6.2 User Independent Recognition Results ........................ 
69 6.7 Discussion........................................... 70 1 CourseIntroduction 3Dspatialinterfaces[Bowmanetal.2004]giveuserstheabilitytospatially 
interactwith3D virtualworlds because they provide natural mappings from human movement to interface controls. 
These interfaces, commonin virtual and augmented reality applications,give users, rich, immersive, and 
interactiveexperiences that can mimic the real world or provide magical, larger than life interaction 
metaphors [Katzourin et al. 2006]. 3D interfaces utilize motion sensing, physical input, and spatial 
interaction techniques to effectively controlhighlydynamic virtual content.Withtheadventofthe NintendoWii,EyeToy,andahostof 
soon-tobe-released peripherals such as the PlayStation Move motion controller, Microsoft s Natal, and 
Sixense s TrueMotion,gamedevelopers, researchersand hobbyists arefaced withthe challengeof comingup with 
compelling interface techniquesandgameplay mechanicsthatmakeuseofthis technology[Wingraveetal. 2010]. 
Researchers in the .elds of virtual and augmented reality as well as 3D user interfaces have been working 
on 3D interaction for nearly two decades. The techniques and interaction styles and metaphors developedover 
this timein these communities are directly applicabletogames thatmake useof motion controller hardware. 
1.1 CourseTopics In this course, presenters will demystifytheworkingsof modernday videogame motion controllersand 
provideathoroughoverviewofthe techniques, strategies,and algorithmsusedin creating3D interfacesfor tasks 
such as 2D and 3D navigation, object selection and manipulation, gesture-based application control, and 
character control. We will discuss the strengths and limitations of various motion controller sensing 
technologies, found in today s and soon-to-be-released peripherals including accelerometers, gyroscopes, 
and 2D and 3D depth cameras. We also will present techniques for compensating for their de.ciencies including 
gesture recognition and non-isomorphic control-to-display mappings. Attendees will receive valuable information 
on how to apply existing 3D user interface techniques using commercial motion controllers as well as 
develop their own techniques. 1.2 Course Schedule 1. Welcome, Introduction,&#38; Roadmap 20 minutes 
-LaViola (a) Motivate the importance of the topic (b) Introduce presenters (c) GameUI -Past, Present 
and Future  2. CommonTasksin3D User Interfaces 30 minutes -LaViola (a) Selection and manipulation techniques 
 (b) Travel techniques (c) System control techniques  3. 3D Interfaces with 2D and 3D Cameras 45 minutes 
-Marks (a) EyeToy i. Motion detection ii. Color tracking (b) PlayStation Eye i. Marker tracking ii. 
Face tracking (c) 3D cameras  i. Technology overview ii. Foreground/background separation iii. Augmented 
Reality iv. Real-time motion capture 4.Working with the NintendoWiimote 40 minutes -LaViola (a) Device 
characteristics (b) Device mathematics (c) Home-brewWiimote software (d) Heuristic motion analysis 
 5. 3D Spatial Interaction with the PlayStation Move 45 minutes -Marks (a) PlayStation Move characteristics 
 (b) PlayStation Move technology (c) Spatial input vs.buttons (d) 1-to-1 data mapping vs. other mappings 
 6. 3D Gesture RecognitionTechniques 30 minutes -LaViola (a) 3D Gestures (b) Linear and AdaBoost classi.ers 
 (c) A25 gesture open source dataset  1.3 Speaker Biographies Joseph J. LaViola Jr. Assistant Professor, 
University of Central Florida University of Central Florida School of EECS 4000 Central Florida Blvd. 
Orlando, FL 32816 (407)882-2285 (voice) (401)823-5419 (fax) jjl@eecs.ucf.edu (email) Joseph J. LaViola 
Jr. is an assistant professor in the School of Electrical Engineering and Computer Science and directs 
the Interactive Systems and User Experience Lab at the University of Central Florida. He is also an adjunct 
assistant research professor in the Computer Science Department at Brown University. His primary research 
interests include pen-based interactive computing, 3D spatial interfaces for videogames, predictive motion 
tracking, multimodal interaction in virtual environments, and user interfaceevaluation. Hiswork has 
appearedin journals such asACMTOCHI, IEEEPAMI, Presence, and IEEE Computer Graphics&#38;Applications, 
and he has presented research at conferences includingACM SIGGRAPH,ACM CHI, theACM Symposium on Interactive 
3D Graphics, IEEEVirtual Reality, and EurographicsVirtual Environments. He has also co-authored 3D User 
Interfaces: Theory and Practice, the.rst comprehensivebookon3D user interfaces. In2009,hewonanNSF CareerAwardto 
conduct research on mathematical sketching. Joseph received a Sc.M. in Computer Science in 2000, a Sc.M. 
in Applied Mathematics in 2001, and a Ph.D. in Computer Science in 2005 from Brown University. Richard 
L. Marks Senior Researcher SonyComputer Entertainment US R&#38;D SonyComputer Entertainment America 
US R&#38;D Division 919 E. Hillsdale Blvd. Foster City, CA 94404 (650)655-8000 (voice) (650)655-8060 
(fax) richard marks@playstation.sony.com (email) Richard Marks is currently a Senior Researcher in Sony 
s US R&#38;D group, investigating new interactive user experiences. Inspired in 1999 by the unveiling 
of PlayStation 2, he joined PlayStation R&#38;D to investigatetheuseoflivevideoinputforgaming.Hedevelopedthe 
technologyfortheEyeToycameraand worked closely with the SonyLondon studio to make it a successful product 
selling over 10 million units. Richard later helped create the PlayStation Eye camera and a computer 
vision SDK for the PlayStation 3. Concurrently, he explored the use of 3D cameras and the new experiences 
theycould enable. Most recently, he has been working on Sony s new motion controller, which combines 
both visual and inertial sensingtoenable3D interaction. RichardreceivedaB.S.inAvionicsfromMITin1990,thenreceivedhis 
Ph.D. in 1996 from Stanford University in the .eld of underwater robotics. 1.4 Game UI  debuted in 
1972. Figure 1). Figure 1: Donkey Kong: (a) the arcade and (b) ColecoVision game console versions. Game 
console graphics quickly approached the quality of arcade games. In the 90s, things changed. Consoles 
becamefaster, with better graphics and sound than arcadegames. If you bought one of these consoles (or 
your parents bought one for you), you didn thave to put a quarter Figure 2: Arcade games that used morerealistic 
input strategies: (a) BeachHead, (b)FootballPower, (c) Manx TT, and (d) Dance Dance Revolution. Suchinnovative 
interfaces helped arcade games compete with PCs and game consoles. During the 90s, VR technology appeared 
in videogames. VR-basedgames helpedkeep arcades alive because theyincluded more advanced interfaces employing 
such technologies as stereoscopic vision, head andbody tracking,and3D spatial interaction. Thesegames 
appeared mainlyin arcadesand entertainment centers. Oneof the .rstVRgameswas Dactyl Nightmare, whichWIndustries/Virtualitydevelopedin 
the early 90saspartofa suiteofVRgames. Players entereda pod-like structure,putona head-mounted display, 
andusedatrackedjoystickto interactwiththevirtualworld.Thepodprotectedusersfromwalkingaround in thephysicalworld. 
Thegames themselves were somewhat primitive,but the immersiveexperience hadn t been seen beforeina video 
arcadegame. OneVR entertainment center was BattleTech, based on the BattleTech universe. The .rst one 
opened in Chicago in 1990. It aimed to provide an immersive experiencewhereseveral userscouldplay simultaneouslynotonlyatasingle 
locationbutalsonetworked across all BattleTech centers. Although these typesof videogames provided more 
interesting interactiveexperiences than consoles, the costofupkeep,lackof throughput,costofplaypergaming 
session,and continuedimprovementofgame consolesledtothe demiseof most arcades. Arcades stillexist,butthey 
renowhere nearas popularas they wereinthe late 70sand 80s.Todaythey re often coupled withfamily entertainment 
centers, bars, bowling alleys, and other smallvenues. However, the video arcadegame sevolution shows 
that as time passed,thesegameshadtoprovide userswithhigherlevelsof interaction-notjustusinga controlstickand 
a setofbuttons-tokeep them interested and coming back for more. 1.4.1 Input Control and Game Complexity 
Ifweexaminetheevolutionofgame consolesfromthe 70stotoday, wecansee profoundimprovements in computer graphics, 
sound, arti.cial intelligence, and storytelling. However, a closer look shows that thesegames have become 
more complex, in terms of not only story, graphics, sounds, and so onbut also gameplay. As videogames 
became more complexbygiving usersa greatervarietyof thingstodoand controls to master, the input devices 
for controlling them stayed relatively constant. If we look at PC games, the argument is clear: mouse 
andkeyboard have been the predominant control devices since PC gaming started in the early 80s. One could 
argue that consolegame input devices have improved and gotten more useful since the Atari 2600 s simple 
directionaljoystickandbutton. Game controllers certainlyhave gotten more complex,but not necessarily 
better. As new generationsofgame consoles emerged, the controllers simply added more buttons and joysticks 
to previous versions. Forexample,the1983 NintendoFamicomusedagamepadwith directionbuttons(to replacethe 
directionaljoystick)andfourbuttons;theSegaMasterSystemhada similardesign.In1994,when Nintendo introducedthe 
.rst3Dgame console (Nintendo64),the controllerhada directionaljoystick,a directional pad, and 10buttons. 
Finally, the SonyPlayStation introduced the DualShock controller, which has two analogjoysticks,a directional 
pad, and10buttons. As controller complexity increased, so did the complexityofagame s control scheme. 
These schemes allowedformoreexpression,atthecostofmakingthegamesdif.culttolearnand master.So,theinterfaces 
became tailored to hard-coregamers, often alienating the casual player.Forexample, each newversion of 
John MaddenFootball has gotten more realistic and improved how you control the football players. However, 
thegame now has so manychoices and controls to master that it is dif.cult to remember them all, effectively 
limiting what can be done duringgameplay. It s clear thatgame interfaces must become easierto use while 
maintainingthehighlevelsofexpressionand controlof modern consoleandPCgames. 1.4.2 3DUIin GamesToday 
Glancing at videogame history shows three trends that lead us to whyvideogames are moving toward using 
modern videogame controllers. First, oncegame consoles had better graphics and sound, had more interesting 
stories,andlet usersplay much longer, arcadegameshadtogive players something thatthey couldn t get on 
consoles: innovative interfaces that provided more natural means of expression. So, to competeina market 
where consumers can choose from severalgaming platforms, better graphicswas no longerakeyto staying on 
top. More naturalgameplay, as we ve seen with the NintendoWii s success, keeps people wanting more. Second, 
more complicated videogames and videogame controllersgave users moreexpressivepower but alienated casualgamers. 
In manycases, thesegames use somewhat abstract control schemes, when we consider the mappings between 
control mechanisms that are easy to perform naturally and spatially (running, jumping, punching, kicking,andsoon)anda 
seriesofbutton presses.Tobringbackthe casual gamer and improveoverallgameplay,3D spatial interactionis 
the natural next step. Third, the technology to make 3D spatial interaction mainstream and not just a 
gimmick has arrived. Consolegameshadpreviously incorporatedadvancedgame interfaces(inthelate 80sandearly 
90s)with devices such as the Nintento UForce, Mattel PowerGlove, and Sega3D glasses. However,poor technology 
and lackof support fromgamedevelopers caused their early demise.Today,faster and cheaper sensors, faster 
processors that can perform complex tracking and recognition, and the need to reducegame control complexity 
have .nally made 3D spatial interaction feasible. Current videogames employ3D spatial interactionin threeways. 
First, as the EyeToyshowed, simple vision-based tracking can let players use their bodies to controlgame 
characters. Second, realistic 3D spatial interfaces based on activephysical props-speci.cally, guitars 
and drum sets-givegamers the ability to interact as if they were in a real rock band. Guitar Hero and 
Rock Band are interesting examples of thistypeof realistic control scheme because people are willingtobuy 
thesedevicesfor use withjust one game.Tenyearsago,noonewouldhave believedthatpeoplewouldspend$90to$200onasinglegame 
with a speci.c controller. (This supports the possibility of more arcade-style interfaces from the 90s 
making their way into the home.) Finally, and probably the most important, is Nintendo s approach with 
itsWii andWiimote. TheWiimote is one of the most signi.cant technological innovations in 3D spatial interaction 
forgaming. It not only acts asagamepadbut also makesgames accessible to the casualgamer because it can 
sense 3D motion. It showed that consumers were willing to accept this type of interface deviceand,asaresult,weseethat3D 
motionsensing interfacesaregoingtobeapartofeverygaming platform. 1.4.3 Video Games and Modern Controllers 
Given the plethora of new 3D motion sensing controllers, it is important to have a solid understanding 
of theirstrengthsand weaknessesandhowtheycanbeeffectivelyusedtocreatevideogameinterfaces.Work onthis 
understandinghasalreadybegun.Forexample,in2006,SwordPlaywasdeveloped(seeFigure3), a videogame prototypein 
which users .ght enemies usinga sword and shield,abow and arrow, anda set of spells they can sketch with 
the sword. This work aimed to leverage existing 3DUI techniques in the contextofa videogame. Thegamewas 
playedina four-sided stereoscopic display(Cave Automatic Virtual Environment) and with 6-DOF trackers. 
Unfortunately, most people probably don t own a foursided CAVE or an expensive 6-DOF tracking system. 
The project was successful in that it showed what might be possible in the future. Well, the future is 
now and the remaining sections of these course notes Figure 3: Ascreen shot of SwordPlay, a video game 
prototype used to explore how 3D UIs can .t in a gaming environment. willprovide useful informationonhowtoworkwiththe 
latest videogame motion controller technologies.  2 CommonTasksin3D User Interfaces Beforeexamining 
modernday videogame motion controllers, we .rst will takea brief tourof the techniques used for common 
tasks in 3D spatial interaction. What is 3D spatial interaction anyway? As starting point, we can say 
that a 3D user interface (3D spatial interaction) is a UI that involves human computer interaction where 
the user s tasks are carried out in a 3D spatial context with 3D input devices or 2D input devices with 
direct mappings to 3D. In other words, 3D UIs involve input devices and interaction techniques for effectively 
controlling highly dynamic 3D computer-generated content, and there s noexceptionwhenit comesto videogames. 
Figure4showsexample3DUIs. A3D videogameworld can use oneof three basic approachesto interaction. The 
.rst maps2D input andbuttondevices,suchasthekeyboardand mouse,joysticks,andgame controllerstogame elementsin 
the 3D world. This is the traditional approach; it s how people have been interacting with 3D (and 2D) 
videogames since their inception manyyears ago. The second approach simulates the realworld using replicasofexistingdevicesorphysical 
props. Commonexamples include steering wheels,light guns,and musical instruments (for example, the guitar 
in Guitar Hero). These devices don tnecessarily provide 3D interaction in thegamebut do provide 3D input 
devices that enable more realisticgameplay. The third approach is true spatial 3D tracking of the user 
s motion and gestures, where users interact in and control elementsofthe3Dgamingworld with their bodies. 
This control can come through vision-baseddevices suchasthe PlayStationEyeand motion-sensingdevicessuchas 
NintendoWii Remotes(Wiimotes)and the PlayStation Move motion controller. Figure 4: Two examples of a 
3D UI. The image on the left shows a user interacting with a world-inminiature and the image on the 
right shows a user navigating using body-based controls. There are essentially four basic 3D interaction 
tasks that are found in most complex3D applications [Bowman et al. 2004]. Actually, there is a .fth 
task called symbolic input, the ability to enter alphanumeric charactersina3D environment,but we will 
not discussit here. Obviously, there are other tasks which are speci.c to an application domain,but these 
basicbuilding blocks can often be combined to let users perform more complex tasks. These tasks include 
navigation, selection, manipulation, and system control. Navigation is the most common VE task, and 
is consists of two components. Travel is the motor component of navigation, and just refers to physical 
movement from place to place. Way.nding is the cognitive or decision-making component of navigation, 
and it asks the questions, where am I? , where doI wanttogo? , howdoIget there? ,andsoon. Selection is 
simply the speci.cation of an object or a set of objects for some purpose. Manipulation refers to the 
speci.cation of object properties (most often position and orientation, but also other attributes). Selection 
and manipulation are often used together, but selection may be a stand-alone task. For example, the user 
may select an object in order to apply a command such as delete to that object. System control is the 
task of changing the system state or the mode of interaction. This is usually done with some type of 
command to the system (either explicit or implicit). Examples in 2D systems include menus and command-line 
interfaces. It is often the case that a system control technique is composed of the other three tasks 
(e.g. a menu command involves selection), but it s also useful to consider it separately since special 
techniques have been developed for it and it is quite common. There are two contrasting themes that are 
common when thinking about 3D spatial interfaces; the real and the magical. The real theme or style tries 
to bring real world interaction into the 3D environment. Thus, the goal is to try to mimic physical world 
interactions in the virtual world. Examples include direct manipulation interfaces, such as swinging 
a golf club or baseball bat or using the hand to pick up virtual objects. The magical theme or style 
goesbeyond the realworld into the realmoffantasy and science .ction. Magical techniques are only limitedby 
the imagination andexamples include spell casting, .ying, and moving virtual objects with levitation. 
Two technical approaches used in the creation of both real and magical 3D spatial interaction techniques 
are referred to as isomorphism and non-isomorphism. Isomorphism refers to a one to one mapping between 
the motion controller and the corresponding object in the virtual word. For example, if the motion controller 
moves 1.5 feet along the x axis, a virtual object moves the same distance in the virtual world. On the 
other hand, non-isomorphism refers to ability to scale the input so that the control-to-display ratio 
isnotequalto one.Forexample,ifthe motion controlleris rotated30degreesabouttheyaxis,the virtual objectmay 
rotate60degrees abouttheyaxis. Non-isomorphismisaverypowerful approachto3D spatial interaction because 
it lends itself to magical interfaces and can potentially give the user more control in the virtual world. 
In this section, we will review several common techniques used to perform these basic tasks. Note that 
it is beyond the scope of this lecture to go into great detail on these techniques or on the concepts 
of 3D UIs in general. The reader shouldexamine 3D User Interfaces: Theory and Practice fora rigorous 
treatment of the subject [Bowman et al. 2004]. Also note for this section, we assume (for the techniques 
where it makes a difference) that y is the vertical axis in the world coordinate system. 2.1 Navigation 
The motor component of navigation is known as travel (e.g., viewpoint movement). There are several issues 
to consider when dealing with travel in 3D UIs. One such issue is the control of velocity and/or acceleration. 
There are many methods for doing this, including gesture, speech controls, sliders, etc. Another issue 
is that of world rotation. In systems that are only partially spatially surrounding (e.g. a 4walledCAVE,orasinglescreen),theusermustbeableto 
rotatetheworldorhisviewoftheworldinorder to navigate. In fully surrounding systems (e.g. with an HMD 
or 6-sided CAVE) this is not necessary since the visuals completely surround the user. Next, one must 
consider whether motion should be constrained inanyway,forexamplebymaintainingaconstant heightorbyfollowingthe 
terrain. Finally,atthelowestlevel, the conditions of input must be considered -that is, when and how 
does motion begin and end (click to start/stop, press to start, release to stop, stop automatically at 
target location, etc.)? Four of the more common 3D travel techniques aregaze-directed steering, pointing, 
map-based travel, and grabbing the air . 2.1.1 Gaze-Directed Steering Gaze-directed steering is probably 
the most common 3D travel technique, although the term gaze is really misleading. Usually noeye trackingis 
being performed, so the directionofgazeis inferred from theheadtracker orientation.Thisisasimpletechnique,bothtoimplementandtouse,butitissomewhat 
limitedinthatyou cannotlook aroundwhilemoving[Mine1995]. Potentialexamplesofgaze-directed steeringin 
videogameswouldbe controllingvehiclesortraveling aroundtheworldin real-time strategy games. Toimplementgaze-directed 
steering, typicallyacallback functionis setup thatexecutes before each frame is rendered. Within this 
callback, .rst obtain the head tracker information (usually in the form of a 4x4 matrix). This matrix 
gives you a transformation between the base tracker coordinate system and the head tracker coordinate 
system. By also considering the transformation between the world coordinate system and the base tracker 
coordinates (if any), you can get the total composite transformation. Now, consider the vector (0, 0, 
-1) in head tracker space (the negative z-axis, which usually points out the front of the tracker). This 
vector, expressed in world coordinates, is the direction you want to move. Normalize this vector, multiply 
it by the speed, and then translate the viewpoint by this amount in world coordinates. Note: current 
velocity isin units/frame. Ifyouwant truevelocity (units/second),you mustkeep track of the time between 
frames and then translate the viewpoint by an amount proportional to that time. 2.1.2 Pointing Pointing 
is also a steering technique (where the user continuously speci.es the direction of motion). In this 
case, the hand s orientation is used to determine direction. This technique is somewhat harder to learn 
for some users,but is more .exible thangaze-directed steering [Bowman et al. 1997]. Pointing is implementedinexactlythe 
samewayasgaze-directed steering,excepta hand trackeris used insteadof the head tracker. Pointing could 
be used to decouple line of sight and direction of motion in .rst and third person shootergames. 2.1.3 
Map-basedTravel Figure 5: Dragging a user icon to move to a new location in the world. The map-based 
travel technique is a target-based technique. The user is represented as an icon on a 2D map of the 
environment. To travel, the user drags this icon to a new position on the map (see Figure 5). When the 
icon is dropped, the system smoothly animates the user from the current location to the new location 
indicatedbytheicon[Bowmanetal.1998]. Map-basedtravelcouldbeusedto augmentmanyof the2Dgame maps currently 
foundin manygame genres. To implement this technique, two things must be known about the way the map 
relates to the world. First, we need to know the scalefactor, the ratio between the map and the virtualworld. 
Second, we need to know which point on the map represents the origin of the world coordinate system. 
We assume here that the map model is originally aligned with the world (i.e. the x direction on the map, 
in its local coordinate system, representsthex directionintheworld coordinate system). Whenthe user pressesthebuttonand 
is intersecting the user icon on the map, then the icon needs to be moved with the stylus each frame. 
One cannot simply attach the icon to the stylus, because we want the icon to remain on the map even if 
the stylus does not.Todo this, we .rst .nd the positionof the stylusin the map coordinate system. This 
may require a transformation between coordinate systems, since the stylus is not a child of the map. 
The x and z coordinatesofthestyluspositionarethepointtowhichtheiconshouldbemoved.Wedonotcoverhere what 
happens if the stylus is dragged offthe map,but the user icon should stick to the side of the map untilthe 
stylusismoved back insidethemap boundaries, sincewe don twantthe usertomove outsidethe world. Whenthebuttonis 
released,weneedto calculatethe desired positionoftheviewpointintheworld. This position is calculated 
using a transformation from the map coordinate system to the world coordinate system, which is detailed 
here. First, .nd the offset in the map coordinate system from the point correspondingtotheworld origin. 
Then,dividebythemap scale(ifthemapis1/100thesizeoftheworld,this corresponds to multiplying by 100). This 
gives us the x and z coordinates of the desired viewpoint position. Sincethemapis2D,wecan tgetaycoordinatefromit. 
Therefore,the technique shouldhave some way of calculating the desired height at the new viewpoint. In 
the simplest case, this might be constant. In othercases,itmightbebasedontheterrainheightatthat locationorsomeotherfactors. 
Once we know the desired viewpoint, we have to set up the animation of the viewpoint. The move vector 
m represents the amount of translation to do each frame (we are assuming a linear path). To .nd m, we 
subtract the desired position from the current position (the total movement required), divide this by 
the distance between the two points (calculated using the distance formula), and multiplied by the desired 
velocity, so that m gives us the amount to move in each dimension each frame. The only remaining calculation 
is the number of frames this movement will take: distance/velocity frames. Note that again velocity is 
measured here in units/frame, not units/second, for simplicity. 2.1.4 Grabbing the Air The grabbingtheair 
technique usesthe metaphorof literally grabbingtheworld aroundyou (usuallyempty space), and pulling yourself 
through it using hand gestures [Mapes and Moshell 1995]. This is similar to pulling yourself along a 
rope, except that the rope exists everywhere, and can take you in anydirection. The grabbing the air 
technique has manypotential uses in videogames including climbingbuildings or mountains, swimming, and 
.ying. Toimplementthe one-handedversionofthistechnique(thetwo-handedversioncangetcomplexif rotation andworldscalingisalso 
supported),whentheinitialbuttonpressis detected,wesimplyobtaintheposition ofthehandintheworld coordinate 
system.Then,everyframeuntilthebuttonis released,getanewhand position, subtract it from the old one, and 
move the objects in the world by this amount. Alternately, you can leave the world .xed, and translate 
the viewpoint by the opposite vector. Before exiting the callback, be sure to update the old hand position 
for use on the next frame. Note it is tempting to implement this technique simply by attaching the world 
to the hand, but this will have the undesirable effect of also rotating the world when the hand rotates, 
which can be quite disorienting. You can also do simple constrained motion simply by ignoring one or 
more of the components of the hand position (e.g. only consider x and z to move at a constant height). 
 2.2 Selection 3D selection is the process of accessing one or more objects in a 3D virtual world. Note 
that selection and manipulation are intimately related, and that several of the techniques described 
here can also be used for manipulation. There are several common issues for the implementation of selection 
techniques. One of the most basic is how to indicate that the selection event should take place (e.g. 
you are touching the desired object,nowyouwanttopickitup). Thisis usuallydoneviaabutton press, gesture, 
orvoice command,but it might also be done automatically if the system can infer the users intent. One 
also has to have ef.cient algorithms for object intersections for manyof these techniques. Well discuss 
a couple of possibilities. The feedback given to the user regarding which object is about to be selected 
is also very important. Manyof the techniques require an avatar (virtual representation) for the users 
hand. Finally, considerkeepingalistofobjectsthatare selectable, sothata selection techniquedoesnothavetotest 
every object in the world, increasing ef.ciency. Four common selection techniques include the virtual 
hand, ray-casting, occlusion, and arm extension. 2.2.1 The Virtual Hand The most common selection technique 
is the simple virtual hand, which does real-world selection via direct touchingof virtual objects.Inthe 
absenceofhaptic feedback,thisisdoneby intersectingthe virtual hand (which is at the same location as 
the physical hand) with a virtual object. The virtual hand has great potential in manydifferent videogame 
genres. Examples include selection of sports equipment, direct selectionofguns,ammo,andhealthpacksin.rstpersonshootergames,asahandof 
God in real-time strategygames, and interfacing with puzzlesin action/adventuregames. Implementing this 
technique is simple, provided you have a good intersection/collision algorithm. Often, intersections 
are only performed with axis-aligned bounding boxes or bounding spheres rather than with the actual geometry 
of the objects. 2.2.2 Ray-Casting Another common selection technique is ray-casting. This technique 
uses the metaphor of a laser pointer an in.nite ray extending from the virtual hand [Mine 1995]. The 
.rst object intersected along the ray is eligible for selection. This technique is ef.cient, based on 
experimental results, and only requires the user tovary2degreesof freedom (pitchandyawofthe wrist) rather 
thanthe3DOFs requiredbythe simple virtual hand and other location-based techniques. Ideally, ray-casting 
could be used whenever special powersarerequiredinthegameorwhentheplayerhastheabilitytoselectobjectsata 
distance. There are many ways to implement ray-casting. Abrute-force approach would calculate the parametric 
equation of the ray, based on the hands position and orientation. First, as in the pointing technique 
for travel, .nd the world coordinate system equivalent of the vector (0, 0, -1). This is the direction 
of the ray. If the hands position is represented by (xh,yh,zh), and the direction vector is (xd,yd,zd), 
then the parametric equations are given by x(t)= xh + xdt (1) y(t)= yh + ydt (2) z(t)= zh + zdt. (3) 
Only intersections with t> 0 should be considered, since we do not want to count intersections behind 
the hand. It is important to determine whether the actual geometry has been intersected, so .rst testing 
the intersection with the bounding box will result in manycases being trivially rejected. Another method 
might be more ef.cient. In this method, instead of looking at the hand orientation in the world coordinate 
system, we consider the selectable objects to be in the hands coordinate system, by transforming their 
vertices or their bounding boxes. This might seem quite inef.cient, because there is only one hand, while 
there are many polygons in the world. However, we assume we have limited the objects by using a selectable 
objects list. Thus, the intersection test we will describe is much more ef.cient. Once we have transformed 
the vertices or bounding boxes, we drop the z coordinate of each vertex. This maps the 3D polygon onto 
a 2D plane (the xy plane in the hand coordinate system). Since the ray is (0, 0, -1) in this coordinate 
system, we can see that in this 2D plane, the ray will intersect the polygon if and only if the point 
(0, 0) is in the polygon. We can easily determine this with an algorithm that counts the number of times 
the edges of the 2D polygon cross the positive x-axis. If there are an odd number of crossings, the origin 
is inside, if even, the origin is outside. 2.2.3 OcclusionTechniques Occlusion techniques (also called 
image plane techniques) work in the plane of the image; object are selected by covering it with the virtual 
hand so that it is occluded from your point of view [Pierce et al. 1997]. Geometrically, this means that 
a ray is emanating from your eye, going through your .nger, and then intersecting an object. Occlusion 
techniques could be used for object selection at a distance but instead of using a laser pointer metaphor 
that ray casting affords, players could simply touch distant objects to select them. These techniques 
can be implemented in the same ways as the ray-casting technique, since it is also using a ray. If you 
are doing the brute-force ray intersection algorithm, you can simply de.ne the rays direction by subtracting 
the .nger position from the eye position. However, if you are using the 2nd algorithm, you require an 
object to de.ne the rays coordinate system. This can be done in two steps. First, create an empty object, 
and place it at the hand position, aligned with the world coordinate system. Next, determine how to rotate 
this object/coordinate system so that it is aligned with the ray direction. The angle can be determined 
using the positions of the eye and hand, and some simple trigonometry. In 3D, two rotations must be done 
in general to align the new objects coordinate system with the ray. 2.2.4 Arm-Extension The arm-extension 
(e.g., Go-Go) techniqueis based on the simple virtual hand,butit introducesa nonlinear mapping between 
the physical hand and the virtual hand, so that the user s reach is greatly ex Figure 6: The nonlinear 
mapping function used in the Go-Go selection technique. To implement Go-Go, we .rst need the concept 
of the position of the users body. This is needed because we stretch our hands out from the center of 
our body, not from our head (which is usually the position that is tracked). We can implement this using 
an inferred torso position, which is de.ned as a constant offset in the negativeydirection from the head.Atracker 
could alsobe placed on the users torso. Before rendering each frame, we get the physical hand position 
in the world coordinate system, and then calculate its distance from the torso object using the distance 
formula. The virtual hand distance can then be obtained by applying the function shown in the graph in 
Figure 6. d2.3 (starting at D) is a useful function in many environments, but the exponent used depends 
on the size of the environment and the desired accuracyof selection at a distance. Once the distance 
at which to place the virtual hand is known, we need to determine its position. The most common implementation 
is tokeep the virtual hand on the ray extending from the torso and going through the physical hand. Therefore, 
if we get a vector between thesetwo points, normalizeit, multiplyitbythe distance, thenadd thisvectortothe 
torso point,we obtain the position of the virtual hand. Finally, we can use the virtual hand technique 
for object selection.  2.3 Manipulation As we noted earlier, manipulation is connected with selection, 
because an object must be selected before it can be manipulated. Thus, one important issue for anymanipulation 
technique is how well it integrates with the chosen selection technique. Manytechniques, as we have said, 
do both: e.g. simple virtual hand, ray-casting, and go-go. Another issue is that when an object is being 
manipulated, you should take care to disable the selection technique and the feedback you give the user 
for selection. If this is not done, then serious problems can occur if, forexample, the user tries to 
release the currently selected objectbut the system also interprets this as trying to select a new object. 
Finally, thinking about what happens when the object is released is important. Does it remain at its 
last position, possibly .oating in space? Does it snaptoagrid?Doesitfallviagravityuntilit contactssomethingsolid?The 
application requirementswill determine this choice. Three common manipulation techniques include HOMER, 
Scaled-World Grab, and World-in-Miniature. 2.3.1 HOMER The Hand-Centered Object Manipulation Extending 
Ray-Casting (HOMER) technique uses ray-casting for selection and then moves the virtual hand to the object 
for hand-centered manipulation [Bowman and Hodges 1997]. The depth of the object is based on a linear 
mapping. The initial torso-physical hand distanceis mapped onto the initial torso-object distance, so 
that moving thephysical hand twice asfar away also moves the object twice asfaraway. Also, moving thephysical 
hand all theway back to the torso moves the object all the way to the users torso as well. Like Go-Go, 
HOMER requires a torso position, because you want to keep the virtual hand on the ray betweenthe usersbody(torso)andthephysicalhand.TheproblemhereisthatHOMERmovesthe 
virtual hand from the physical hand position to the object upon selection, and it is not guaranteed that 
the torso, physicalhand,andobjectwillalllineupatthistime. Therefore,we calculatewherethe virtualhandwould 
be if it were on this ray initially, then calculate the offset to the position of the virtual object, 
and maintain this offset throughout manipulation. When an object is selected via ray-casting, .rst detach 
the virtual hand from the hand tracker. This is due tothefact thatifit remained attachedbutthe virtual 
hand modelismovedaway fromthephysical hand location, a rotation of the physical hand will cause a rotation 
and translation of the virtual hand. Next, move the virtual hand in the world coordinate system to the 
position of the selected object, and attach the object to the virtual hand in the scene graph (again, 
without moving the object in the world coordinate system). To implement the linear depth mapping, we 
need to know the initial distance between the torso and the physical hand dh, and between the torso and 
the selected object do. The ratio do/dh will be the scaling factor. For each frame, we need to set the 
position and orientation of the virtual hand. The selected object is attached to the virtual hand, so 
it will follow along. Setting the orientation is relatively easy. Simply copy the transformation matrix 
for the hand tracker to the virtual hand, so that their orientation matches.To set the position, we need 
to know the correct depth and the correct direction. The depth is found by applying the linear mapping 
to the current physical hand depth. The physical hand distance is simply the distance betweenit and the 
torso, and we multiply thisby the scalefactor do/dh to get the virtual hand distance. We then obtain 
a normalized vector between the physical hand and the torso, multiply this vector by the virtual hand 
distance, and add the result to the torso position to obtain the virtual hand position. 2.3.2 Scaled-World 
Grab The scaled-world grab technique (see Figure 7) is often used with occlusion selection. The idea 
is that since you are selecting the object in the image plane, you can use the ambiguity of that single 
image to do some magic. When the selection is made, the user is scaled up (or the world is scaled down) 
so that the virtual hand is actually touching the object that it is occluding. If the user does not move 
(and the graphics are not stereo), there is no perceptual difference between the images before and after 
the scaling [Mine et al. 1997]. However, when the user starts to move the object and/or his head, he 
realizes that he is now a giant (or that the world is tiny) and he can manipulate the object directly, 
just like the simple virtual hand. Figure 7: An illustration of the scaled-world grab technique. To 
implement scaled-world grab, correct actions must be performed at the time of selection and release. 
Nothing special needs to be done in between, because the object is simply attached to the virtual hand, 
as in the simple virtual hand technique. At the time of selection, scale the user by the ratio (distance 
from eye to object/distance from eye to hand). This scaling needs to take place with the eye as the .xed 
point, so that the eye does not move, and should be uniform in all three dimensions. Finally, attach 
the virtual object to the virtual hand. At the time of release, the opposite actions are done in reverse. 
Re-attach the objecttotheworld,and scalethe user uniformlybythe reciprocalofthe scalingfactor,againusingthe 
eye as a .xed point. 2.3.3 World-in-Miniature Figure 8: An example of a WIM. The world-in-miniature (WIM) 
technique uses a small dollhouse version of the world to allow the user to do indirect manipulation of 
the objects in the environment (see Figure 8). Each of the objects in the WIM are selectable using the 
simple virtual hand technique, and moving these objects causes the full-scale objectsintheworldtomoveina 
correspondingway [Stoakleyetal. 1995]. TheWIM can alsobe usedfor navigation by including a representation 
of the user, in a way similar to the map-based travel technique, but including the 3rd dimension [Pausch 
et al. 1995]. To implement the WIM technique, .rst create the WIM. Consider this a room with a table 
object in it. The WIM is represented as a scaled down version of the room, and is attached to the virtual 
hand. The table object does not need to be scaled, because it will inherit the scaling from its parent 
(the WIM room). Thus, the table object can simply be copied within the scene graph. When an object in 
the WIM is selected using the simple virtual hand technique, .rst match this object to the corresponding 
full-scale object. Keeping a list of pointers to these objects is an ef.cient way to do this step. The 
miniature object is attached to the virtual hand, just as in the simple virtual hand technique. While 
the miniature object is being manipulated, simply copyits position matrix (in its local coordinate system, 
relative to its parent, the WIM) to the position matrix of the full-scale object. Since we want the full-scale 
objecttohavethe same positioninthe full-scaleworld coordinate systemasthe miniature object does in the 
scaled-down WIM coordinate system, this is all that is necessary to move the full-scale object correctly. 
 2.4 System Control System control provides a mechanism for users to issue a command to either change 
the mode of interaction or the system state. In order to issue the command, the user has to select an 
item from a set. System control is a wide-ranging topic, and there are manydifferent techniques to choose 
from such as the use of graphical menus,voice commands, gestures, and tool selectors.For the most part, 
these techniques are notdif.cultto implement,sincetheymostlyinvolve selection.Forexample, virtualmenuitemsmightbe 
selectedusing ray-casting.Forallofthe techniques,good visual feedbackis required, sincethe user needs 
toknownotonlywhatheis selecting,butwhatwillhappenwhenhe selectsit.Inthis section,webrie.y highlight some 
of the more common system control techniques. 2.4.1 Graphical Menus Figure 9: TheVirtualTricorder: anexampleofagraphical 
menu withdevice-centered placement. Graphical menus can be seen as the 3D equivalent of 2D menus. Placement 
in.uences the access of the menu (correct placement cangiveastrong spatial referencefor retrieval),andtheeffectsof 
possible occlu sionofthe .eldof attention. The paperby Feineretal.isan important sourcefor placement 
issues [Feiner et al. 1993]. The authors divided placement into surround-.xed, world-.xed and display-.xed 
windows. The subdivision of placement can, however, be made more subtle. World-.xed and surround-.xed 
windows, the term Feiner et al. use to describe menus, can be subdivided into menus which are either 
freely placed into the world, or connected to an object. Display-.xed windows can be renamed, and made 
more precise, by referring to their actual reference frame: the body. Body-centered menus, either head 
referenced or body-referenced, can supply a strong spatial reference frame. One particularly interesting 
possibleeffectof body-centered menusiseyes-offusage,in which users can perform system control without 
having to look at the menu itself. The last reference frame is the group of device-centered menus. Device-centered 
placement provides the user with a physical reference frame (see Figure 9) [Wloka and Green.eld 1995]. 
Agood example is the placement of menus on a responsive workbench, where menus are often placed at the 
border of the display device. We can subdivide graphical menus into hand-oriented menus, converted 2D 
menus, and 3D widgets. One can identify two major groups of hand-oriented menus. 1DOF menus are menus 
which use a circular object on which several items are placed. After initialization, the user can rotate 
his/her hand along one axis until the desired item on the circular object falls within a selection basket. 
User performance is highly dependent on hand and wrist physical movement and the primary rotation axis 
should be carefully chosen. 1DOF menus have been made in several forms, including the ring menu, sundials, 
spiral menus (a spiral formed ring menu), and a rotary tool chooser. The second group of hand-oriented 
menus are hand-held-widgets, in which menus are stored at a body-relative position. The second group 
is the most often applied group of system control interfaces: converted 2D widgets. These widgets basically 
function the same as in desktop environments, although one often has to deal with more DOFs when selecting 
an item in a 2D widget. Popular examples are pull-down menus, pop-up menus, .ying widgets, toolbars and 
sliders. Figure 10: Anexampleofa3Dwidgetusedto scaleageometric object. The .nal groupof graphical menusisthe 
groupknownas3D widgets [Conneretal. 1992].Ina3Dworld, widgets often mean moving system control functionality 
into the world or onto objects (see Figure 10). This matches closely with the de.nition of widgets given 
by Conner et al., widgets are the combination of geometry and behavior . This can also be thought of 
as moving the functionality of a menu onto anobject. A very importantissuewhenusingwidgetsis placement. 
3Dwidgetsdifferfromthepreviously discussed menu techniques (1DOF and converted 2D menus) in the way 
the available functions are mapped: most often, the functions are co-located near an object, thereby 
formingahighly context-sensitive menu. 2.4.2 Voice Commands Voiceinputallowsthe initialization, selectionandissuingofacommand. 
Sometimes, anotherinput stream (likeabuttonpress)ora speci.cvoice commandisusedtoallowthe actualactivationofvoiceinputfor 
system control.Theuseofvoiceinputasasystem control techniquecanbeverypowerful:itis hands-free and natural. 
Still, continuousvoiceinputistiring,andcannotbeusedineveryenvironment. Furthermore, the voice recognition 
engine often has a limited vocabulary. In addition, the user .rst needs to learn the voice commands before 
they can be applied. Problems often occur when applications are more complex, and the complete set of 
voice commands can not be remembered. The structural organization of voice commands is invisible to the 
user: often no visual representation is coupled to the voice command in order to see the available commands. 
In order to prevent mode errors, it is often very important to supply the user with some kind of feedback 
after she has issued a command. This can be achieved by voice output, or by the generation of certain 
sounds. A very interesting way of supporting the user when interacting with voice and invisible menu 
structures can be foundin telecommunication: usinga telephoneto access information often posesthe same 
problemsto the user as using voice commands in a virtual environment. 2.4.3 Gestures andPostures Figure 
11: Auser interacting with a dataset for visualizing a .ow .eld around a space shuttle. The user simultaneously 
manipulates the streamlines with his left hand and the shuttle with his right hand while viewing the 
data in stereo. The user asked for these tools using speechinput. When using gestural interaction, we 
apply a hand-as-tool metaphor: the hand literally becomes a tool. When applying gestural interaction, 
the gesture is both the initialization and the issuing of a command, just as in voice input. When talking 
about gestural interaction, we refer, in this case, to gestures and postures, not to gestural input with 
pen-and-tablet or similar metaphors. There is a signi.cant difference between gestures and postures: 
postures are static movements (like pinching), whereas gestures include a change of position and/or orientation 
of the hand. A good example of gestures is the usage of sign language. Gestural interaction can be a 
very powerful system control technique. In fact, gestures are relatively limitless when it comes to their 
potential uses in videogames. Gestures can be use to communicate with otherplayers,tocastspellsinaroleplayinggame,callpitchesorgivesignsina 
baseballgame,andissues combination attacksinactiongames.However,oneproblemwithgestural interactionisthattheuserneeds 
to learn all the gestures. Since the user can normally not remember more than about7gestures (due to 
the limited capacityoftheworking memory),inexperienced users canhave signi.cant problems with gestural 
interaction, especially when the application is more complex and requires a larger amount of gestures. 
Users often do not have the luxury of referring to a graphical menu when using gestural interaction -the 
structure underneath the available gestures is completely invisible. In order to make gestural interaction 
easiertouseforalessadvanceduser,strongfeedback,likevisualcuesafter initiationofacommand,might be needed. 
An example of application that used a gestural system control technique is MSVT [LaViola 2000]. This 
application combined gesturesandvoiceinputto createamultimodal interfaceforexploratory scienti.c visualization 
(see Figure 11). 2.4.4 Tools We can identify two different kinds of tools, namely physical tools and 
virtual tools. Physical tools are context-sensitive input devices, which are often referred to as props. 
Aprop is a real-world object which is duplicatedin the virtualworld.Aphysical tool mightbe space multiplexed 
(the tool only performs one function)ortimemultiplexed,whenthetoolperformsmultiple functionsovertime(likeanormaldesktop 
mouse). One accesses a physical tool by simply reaching for it, or by changing the mode on the input 
device itself. Figure 12: An example of a virtual toolbelt. The user looks down to invoke the belt and 
can grab tools and use them in the virtual environment. Virtual tools are tools which can be best exempli.ed 
with a toolbelt (see Figure 12). Users wear a virtual toolbeltaroundthewaist,fromwhichtheusercan accessspeci.c 
functionsbygrabbingat particularplaces on belt, as in the real world. Sometimes, functions on a toolbelt 
are accessed via the same principles as used with graphical menus, where one should look at the menu 
itself. The structure of tools is often not complex: as stated before, physical tools are either dedicated 
devices for one function, or one can access several (but not many) functions with one tool. Sometimes, 
a physical tool is the display medium for a One example of a commonly used physically-based tool is the 
pen and tablet. Users hold a large plastic tableton whicha (traditional)2D interfaceis displayedinthe 
virtualworld(see Figure13). Users are able to use graphical menu techniques and can move objects with 
a stylus within a window on the tablet. The tablet supplies the user with strong physical cues with respect 
to the placement of the menu, and allows increased performanceduetofaster selectionof menu items[Bowmanetal. 
1998]. For the implementation of this technique, the most crucial thing is the registration (correspondence) 
between the physical and virtual pens and tablets. The tablets, especially, must be the same size and 
shape sothattheedgeofthephysicaltablet,whichtheusercanfeel, correspondstotheedgeofthevirtualtablet as 
well. In order to make tracking easy, the origin of the tablet model should be located at the point where 
thetrackeris attachedtothephysicaltablet,sothat rotationsworkproperly.Evenwithcare,itsdif.cultto do these 
things exactly right, so a .nal tip is to include controls within the program to tweak the positions 
and/or orientations of the virtual pen and tablet, so that they can be into registration if there s a 
problem. Another useful function to implement is the ability for the system to report (for example when 
a callback function is called) the position of the stylus tip in the tablet coordinate system, rather 
than in the world or user coordinate systems. This can be used for things likethe map-based travel technique 
described earlier.  3 3D Interfaces with 2D and 3D Cameras One of the most active research areas relative 
to human/computer interaction has been in processing realtime video input. The no wires, no batteries 
aspect of video-as-input approaches holds many practical advantages, especially for consumer applications 
[Intel 1999]. In addition, the richness of information visual sensingoffers and the naturalfamiliarity 
usershave with such information make visual tracking an attractive solution for manyapplications. Just 
as with computer graphics, video imaging involves perspective projection. The simpli.ed, often-used model 
for a camera involves light from a 3D scene being projected onto a 2D imaging plane, much the way computer 
graphics are projected onto a 2D display. However, recovering 3D information from a 2D image is impossible 
in the general case and complex even in highly constrained situations. Generally, it requires making 
strong assumptions about the scene or having a priori models of objects in the scene. 3.1 EyeToy Figure 
14: EyeToy. The PlayStation EyeToylaunched in 2003. Though essentially a webcam, it was designed speci.cally 
to enable playerstomove their bodiesto interact with PlayStation2games. Thus,itsdefault frameratewas 
60 frames/sec, twiceasfastas other webcams. Also,ithada56degree diagonal .eldofview,toallow completeimagingoftheuppertorsoand 
outstretchedarmswhenstandingata reasonable distance(6to8 feet). 3.1.1 Motion Detection The .rst EyeToygames 
useda magic mirror paradigmin which videoof the playerwasoverlayed with computer graphics generatedby 
thegame. The player interacted with thegame graphicsby moving his body.To accomplish this interaction, 
thesegames relied almostexclusively upon motion detection. The most straight-forward implementation of 
motion detection involves comparing the current video frame with the previous video frame to generate 
a difference image. This difference between the frames is assumed to be caused by player motion. The 
difference image can be further processed to create a binary motion mask, often by performing a simple 
thresholding step. Many documented techniques exist for computing the difference image, and various techniques 
also exist for creating a motion mask from a difference image. The EyeToy: Play game Wishi-Washi clearly 
demonstrates this technology, as the entire display is covered by virtual dirt and the player must wipe 
the dirt away. Pixels contained in the motion mask are removed from the virtual dirt mask, using the 
simple boolean logic: DirtMask(i +1) = DirtMask(i)&#38;!MotionMask(i). (4) Othergamesevaluatethe motion 
maskonlyin particularregions,to decideif player motionis occurring ina particular areaofthe screen thathas 
some signi.canceforthegame. Forexample, this canbe used to have the player touch .xed targets, such as 
the four corners of the screen. By integrating (adding) the motion mask in a region over a period of 
time, it is possible to see if continual motion is occurring in an area. Such areas are sometimes called 
rubbybuttons because the player feels as if they are rubbing the screen area to activate. This technique 
is especially useful for system control because the integration is much more reliable than motion detection 
for a single video frame (see Figure 15). Figure 15: Motion detection for menu navigation in EyeToy: 
Play. ManyEyeToygames perform dynamic collision detection between virtualgame objects and the motion 
mask (see Figure 16). Note that although both the player and the virtualgame objects may move in 3D, 
this collision detection is a 2D operation because the motion mask is a 2D representation of the player 
motion. Thus, dynamic collision detection is essentially the same operation described above, except the 
motion mask region that is evaluated is dynamically changed to match the 2D projection of the virtual 
game object. Because the motion mask collision detection is 2D, thegame interactions achievable using 
motion detection have a limited 2D feeling. 3.1.2 ColorTracking Color trackingis another interaction 
techniquethatwasexploredusingEyeToyand PlayStation2. Players could interact with a 3D scene by moving 
known brightly colored objects that were visually tracked. Figure17showsa demonstration presentedat 
SIGGRAPH2000EmergingTechnologies[Marks2000]; as the player moves the bright green toyhe is holding, a 
virtual sword object moves in direct response. At SIGGRAPH 2001, two more demonstrations were shown based 
on color tracking of colored foam spheres [Marks et al. 2001]. Color tracking can be broken into two 
main steps: segmentation and pose recovery. Segmentation consists of labeling every video pixel that 
corresponds to the object being tracked. General shape segmentation for arbitrary objectsisadif.cultand 
computationallyexpensive problem.However,by designingthe tracking object to have distinctive, solid, 
saturated colors, the object segmentation process can be simpli.ed to basic colorsegmentation.By choosingextremely 
saturated colors,thelikelihoodof these colors occurring intheenvironmentis reduced. Colorsegmentationcanbe 
accomplishedinasinglepassthroughtheimage using chrominance banding/thresholding. Figure18showsthe resultsofsegmentationfora 
mace object consisting of a large orange ball mounted on a blue stick. By using only chrominance and 
not luminance, the segmentation process can be more robust to variation caused by scene lighting. Pose 
recovery consists of converting 2D image data into 3D object pose (position and/or orientation). Pose 
recovery canbe accomplishedrobustlyfor certain shapesofknownphysical dimensionsby measuringthe statistical 
properties of the shape s 2D projection. In this manner, for a sphere the 3D position can be recovered 
(but no orientation), and for a cylinder, the 3D position and a portion of the orientation can be recovered. 
Multiple objects can be also be combined for complete 3D pose recovery, though occlusion issues arise. 
By computing statistical properties using the entire segmentation region rather than using edges/features, 
extent, or other linear measurements, the pose measurement can be less quantized; this is especially 
important given EyeToy s low-resolution video. When color tracking works properly, it enables true 3D 
interaction that can lead to manyinteresting new entertainment experiences such as those shown at SIGGRAPH 
2000 and 2001. However, primarily due to the variability of scene lighting and background, it is dif.cult 
to make robust even with brightly colored toyobjects. Later, we describe how the design of the PlayStation 
Move motion controller addressed these issues. Figure 18: Color thresholding for object segmentation. 
The object is a large sphere mounted on a stick.  3.2 PlayStation Eye The PlayStation Eye is a USB peripheral 
device designed to be used with PlayStation 3. Just as EyeToy was createdtomatchthe capabilitiesofPS2, 
PlayStationEyewas created speci.callyforPS3.Itsprimary intended useisfor interactive entertainment similartoEyeToygames,buttakentoanewlevelonPS3. 
Secondary uses include video chat, video movie capture, and still-image snapshots. Just as with EyeToy, 
the manufacturing cost of the device was required to be low for successful deployment, so PlayStation 
Eye s video capabilitywas designedto maximize utilitygiven cost constraints. Its resolutionis four times 
that of EyeToy, its low-light sensitivity is signi.cantly improved, and it transfers uncompressed video 
data to avoid compression artifacts that could compromise video processing. Besides improving upon the 
video sensing capability of EyeToy, PlayStation Eye was also designed to enablenewinteractiveexperiencesbyadding2newfeatures.The.rst 
featureisalensthatcanbemanually switched to give either a 56 degrees or 75 degrees diagonal .eld of view. 
The .rst setting (similar to EyeToy) is appropriate for imaging a player s upper torso, while the wider 
view of the second setting allows for imaging a much larger workspace that can include the entire body 
of two players. The second feature is a small-baseline linear 4-microphone array that enables spatial 
audio .ltering so that voiceinput applications can achieve good signal-to-noise without the encumbrance 
of a headset or hand-held microphone. PlayStation 3 game applications using PlayStation Eye are in active 
development today. In addition, the PlayStation Eye is an essential part of the PlayStation Move motion 
control system described later. There are manycomputer vision techniques that are possible given the 
combination PlayStation Eye and PlayStation 3; the following sections highlight two techniques that have 
shown promise for 3D spatial interaction. 3.2.1 FaceTracking Face trackingfor controllinga3Dgame characterwas 
demonstratedusingEyeToyand PlayStation2at SIGGRAPH 2003 [Marks et al. 2003]. The player could lean left 
or right, and jump or duck, and the virtualgame character (riding ona futuristichover board)would perform 
correspondinggame actions. The player also held colored objects to contol the character s arms independently 
using color tracking. ThegameEyeToy:Antigrav similarlyusedface trackingto controlthe actionsofa virtual 
character,and used motion detection to control the character s arms. A different use offace tracking 
forgame interaction was .rst shown publicly on stage at the D.I.C.E. videogame summitin 2005. The positionofthefacewas 
usedtomovethe playerina 1st-persongame, allowingthe playerto duck behindcoverand peek around cornersorovercover. 
This natural interface forgamesisvery similarto DesktopVR or FishTankVR becauseofthe 1st-person renderingofthe 
game scene. The resulting motion parallaxprovidesa compelling3Deffect,famously demonstratedina 2007 video 
created by JohnnyChung Lee. Thoughface trackingwas possibleusingEyeToyand PlayStation2,its viabilityforgamesismuch 
greater using PlayStation Eye and PlayStation 3. The improved video quality of PlayStation Eye allows 
for more reliable tracking, and the increased processing capability of PlayStation 3 easily accommodates 
face tracking and enables more advanced techniques such real-timeface detection andface recognition. 
In addition, since the PlayStation Eye is part of the PlayStation Move system, the uses offace tracking 
described above are ideal to combine with motion control. For example, face tracking can be used to provide 
a player more complete control of a virtual character or to allow the player to view the virtual scenefromadifferent 
perspective whileusingthe motion controllersto manipulategame objects. 3.2.2 MarkerTracking Another 
interesting technique for 3D spatial interaction that has been used forgames is marker tracking. Thoughvarious 
manyformsof marker trackingexist,a common practiceisto use cards with distinctive markings that make 
detection and 3D pose tracking possible. Two famous systems for doing this are CyberCode [Rekimoto andAyatsuka 
2000], developedby the Sony Computer Science Laboratory, and ARToolKit [Kato and Billinghurst 1999], 
originally developed by Hirokazu Kato of the Nara Institute of Science andTechnology. These technologies 
canbe used for real-time tracking from video, allowinga virtual object to be rendered as if it was rigidly 
attached to the card. The .rst title for PlayStation Eye was The Eye of Judgment, which used the CyberCode 
technology to render virtual monsters in 3D onto playing cards. A special mode of the game allowed players 
to manipulate a card in 3D in front of the camera, and motion detection was used to allow players to 
poke the monstersto demonstrate their attack animations.In Europe,the PlayStationEyegame EyePet recently 
launched witha Magic Card includedin thegame box. The specially marked cardis usedby the player to interact 
with a virtual pet in variety of ways. Though markers are generally designed to be black and white so 
as to have high contrast, tracking robustness is still an issue in poorly lit environments. Also, since 
the markers are often printed onto 2D planes, the tracking quality degrades as the marker plane moves 
away from parallel to the camera imaging plane. Finally, tracking also degrades with 3D distance as the 
marker becomes less visually distinct.  3.3 3D Cameras Though a standard 2D camera can be used to extract 
3D spatial data for special known objects, the same cannotbedoneforarbitrary scenesorfortrackingpeople.Toaddressthis,manycompanieshavesoughtto 
create3D cameras, also known as depth cameras orZ cameras. Generally, such cameras provide not only colorforeverypixel,butalso 
distance,or some parameter directly relatedto distance,suchas disparity. Knowing the distance for every 
pixel, or Z-sensing, provides two major bene.ts. First, it provides a very powerful segmentation mechanism 
for distinguishing between the foreground and the background. Second, Z-sensing provides 3D information 
for unstructured scenes. Ultimately, this is the real power of Z-sensing, because it is an ideal complement 
to other computer vision methods. While motion and color detection are powerful attention mechanisms, 
and pattern matching is a powerful correspondence mechanism, Z-sensing provides a powerful segmentation 
mechanism and 3D spatial information. This combination enables manynewpossibilities, especially for augmented 
reality and real-time motion capture applications. 3.3.1 Technology Overview Manytechniques exist for 
obtaining 3D information in a scene. The most well known is computer stereo vision or stereopsis, which 
involves .nding correspondences in images from two or more offset cameras and then using triangulation 
to obtain depth. In practice, using computer stereo vision for Z-sensing is dif.cult because .nding correspondences 
at every image location for arbitrary scenes has manyissues, including the aperture problem, parallax 
occlusion, and camera variation. Recently, several companies have created Z-sensing cameras that use 
active infrared illumination and infrared camera sensors. One such approach is based on the same triangulation 
concept as computer stereo vision,butit uses just one camera and anoffset infrared light that projectsa 
special pattern (see Figure 21). To calculate depth, the camera data is corresponded to the known projected 
pattern, which is carefully designed tofacilitate this process. Figure 21: 3D pixel data can be computed 
from triangulation using one camera and patterned illumination emitted from a .xed baseline. Other active 
illumination Z-sensing cameras use time-of-.ight techniques instead of triangulation to measure depth. 
Because the speedof lightisveryfast, the high-speed electronics requiredhave only recently become available 
for such devices. Two very different time-of-.ight approaches are: 1) pulsed light with active electronic 
shuttering, and 2) modulated light with phase detection. The .rst technique involves pulsing a light 
source and actively controlling an electronic shutter to truncate the amount of the re.ected pulse that 
is captured (see Figure 22). The further away the re.ection occurs, the less of the pulse that will have 
traveled back to the camera. To compensate for scene re.ectance, the shuttered image must be normalizedby 
an unshuttered image. By choosing the pulse length and shutter timing, the near andfar limits for the 
Z-sensing can be set. This approach relies heavily upon the ability to precisely control the electronic 
shutter and light pulse. The second technique involves modulating the light source at a known frequency. 
Depending on the distance, the re.ected light will arrive back at the camera at a different phase compared 
to the emitter. This phase shift is measured to compute depth using a special sensor, and the modulation 
frequency can beadjustedto controltherangeofthe camera.Notethatthephase wraps around beyondonewavelength, 
which introduces an ambiguity, so the modulation frequencymust be chosen to avoid this. One major bene.t 
of active illumination approaches is that theydo not rely on scene illumination for Zsensing,sodepthcanbe 
measuredevenincomplete darkness.Onedrawbackof currentactiveillumination Z-sensing camerasisthattheRGBandZarenot 
perfectly corresponded.Thisis becausetheyuse separate cameras for the RGB andZ sensing, thereby introducing 
some parallax. Several sensor companies are investigating the possibility of RGB-IR sensors to help address 
this issue and potentially lower costs by requiring only a single sensor and lens. 3.3.2 Foreground/Background 
Separation Segmentation is one of the most dif.cult problems in traditional monocular computer vision. 
Separating the foreground from the background in a scene is a speci.c use of segmentation that is important 
to applications such as movie special effects and video compression. For movies, segmentation is typically 
solved with chromakeying (blue screen) and requiresa controlled backdrop. Z-sensing providesavery similar 
segmentation capabilitybut does not requirea controlled backdrop (see Figure 23). Figure 23: Segmentation 
of the foreground from the background based on 3D pixel data. Asimple approach for using Z-sensing to 
separate foreground and background is to simply threshold the Inferring 3D information with a standard 
2D camera is possible in scenes for which a model is known (suchasEyeToysphereandcylinder color trackingforPS2),but 
Z-sensingprovides3D informationfor completely unknown objects. This greatly extends the possibilities 
of augmented reality for merging virtual worlds with the real world. Because Z-sensing provides per-pixel 
depth information, the video image effectively becomes a Z-sprite that can be easily composited with 
computer graphics constructs using a standard z-buffer. The visual occlusion cues this z-compositing 
creates provides a much more compelling augmented reality experience than a simple graphical overlay 
(see Figure 24). Besides occlusion, Z-sensing also can enable other visual cues for augmented reality. 
Surfaces inferred from theZdata can be used as both shadow blockers and shadow receivers, allowing virtual 
objects to cast shadows on the scene, and to allow players to shadow virtual objects. In addition, these 
surfaces allow dynamic virtual 3D light sources to realistically re-light the real-world scene. By enabling 
these visual cues, Z-sensing helps bring the quality level of real-time augmented reality a few steps 
closer to movie special effects. 3.3.4 Real-time Motion Capture One exciting use of spatial interfaces 
for games is using real-time motion capture to control a virtual character. This can be done by computing 
a dynamic virtual skeleton model that represents the player s motion, though this is dif.cult because 
human body has manydegrees of freedom. Z-sensing helps to make the computation of this model more tractable 
by enabling reliable foreground/background separation for segmentingoutthepixelsthat correspondtotheplayer.Also,theper-pixel3Ddatacanbeusedfora.tting 
Figure 25: Askeleton model is .t to data from a 3D camera to enable real-time motion capture. Figure 
26: Data from 3D camera used to implement a 3D mouse. Real-time motion capture can be used for other 
applications beyond controlling a virtual character. Since the positions of the head and hands are known, 
manyof the applications described for color tracking and face trackingarealso possible.Forexample,the 
locationofthehandcanbeusedasmuchlikea3D mouse (see Figure 26). In addition, the relative3D positioning 
of body parts can also be used for spatial interfaces. For the mouse application, the player s hand can 
be ignored unless it is a certain distance in front of his torso. This effectively lets the player choose 
when to address the application, thus solving the alwayson problem of camera interfaces, similar to 
how push-to-talk solves the issue for voice input. Other applicationssuchas3D multi-touchare enabledbyusingthepositiondatafortwohands 
simultaneously.   4 Working with the Nintendo Wiimote As with the EyeToyand PlayStation Eye, the NintendoWii 
Remote(Wiimote)is anotherexampleofa spatially convenient device [Wingrave et al. 2010]. Spatially convenient 
devices involve three important components: Spatial data. The device provides 3D input data, be it partial, 
error-prone, or conditional.  Functionality. The device packs a range of useful sensors, emitters, and 
interface implements.  Commodity design. The device is inexpensive, durable, easily con.gurable, and 
robust.  Regarding spatial data, traditional 3D hardware trackers present information in 6 degrees of 
freedom (DOF)inatrackedspacewithrelativelygoodprecision.In contrast,aWiimotepresentsthreeaxesofacceleration 
datain no particular frameof reference, with intermittent optical sensing. (TheWii MotionPlus gyroscope 
attachment can add three axes of orientation change, or angular velocity.) Although this means theWiimote 
s spatialdata doesn t directlymaptoa real-world positions,thedevicecanbeemployedeffectively under constrained 
use [Shirai et al. 2007; Shiratori and Hodgins 2008]. Regarding functionality, traditional3D hardware 
might come withafewbuttons. TheWiimote incorporates severalbuttons, some inagamepad and trigger con.guration, 
and hasa speaker, programmable LEDs, anda rumbledevice. Regarding commodity design, 3D hardware can require 
extensive installations and environment instrumentationandcanbedif.culttoworkwith.TheWiimoteiseasytosetup,turnon,and 
maintain.Overall, theWiimote incorporates many useful input and output features in an inexpensive, consumer-oriented, 
easy-to-replace, and easy to repurchase package. This letsgame developers, researchers, and homebrew 
engineers use and modify it to best serve their needs. 4.1 Wiimote Basics TousetheWiimotein3DUIs,youneedtoknow 
about connecting theWiimote to the system,  its frames of reference (FORs),  the sensor bar connection 
(SBC),  dealing with accelerometer and gyroscope data, and  other design considerations. For technical 
details and specs, see theWiiBrewWeb site (www.wiibrew.org). 4.1.1 Connecting the Wiimote TheWiimote 
connectstoaWiigame consoleor computer wirelessly through Bluetooth. Whenyou press both the1and2buttons 
simultaneously or press the red Syncbutton in the battery case, theWiimote s LEDs blink, indicating it 
s in discovery mode. Make sure your computer has a Bluetooth adapter and proper drivers. Currently, each 
OS and library handles the connection process differently. PCs. Here are the basic steps (for extra assistance, 
seek online help such as at www.brianpeek.com): 1. Open BluetoothDevicesin the ControlPanel. 2. Under 
theDevices tab, click the Addbutton. 3. PuttheWiimote into discovery mode (continuously reenter this 
mode during this processasit times out). 4. SelectNext, selecttheWiimoteto connectto,and selectNextagain. 
 5. Don tuse a passkey, and select Next, then Finish.  Ifafailure occurs,repeatthis processuntilthePC 
connectstotheWiimote. Asimple startforaPCto retrieve data fromaWiimoteistheWiimoteLib(www.wiimotelib.org). 
Macs. The setup needs to run only once, and future Wiimote connections are simpler. To set up the Wiimote, 
run the Bluetooth Setup Assistant utility and follow these steps: 1. Select the AnyDevice option. 2. 
Put theWiimote into discovery mode. 3. Selectthedevice (NintendoRVL-CNT-01)andsetthepasskeyoptionsto 
Donotuseapasskey. 4. Press Continue until the Mac is connected; then quit the Setup Assistant.  To 
connectin the future,in the Bluetooth sectionof System Preferences, select theWiimote entry and set it 
to connect. Then, put theWiimote into discovery mode, and theWiimote will connect. The OS can keep some 
applications from connecting to theWiimote. In those cases, disconnect theWiimote and let the application 
connect to theWiimote. Figure 27: TheWiimote, with labels indicating theWiimote s coordinate system. 
Multiple coordinate systems and partial spatial data make theWiimote dif.cult to design for. 4.1.2 Frames 
of Reference Figure27showstheWiimote sFOR.Thex,yandzaxesarelabeled,alongwiththe rotationabouteach axis: 
pitch, roll, andyaw, respectively. A second FORis the Earth s, important because theWiimote s accelerometers 
detect the Earth s gravity. A third FOR is theWiimote s relationship to the sensor bar. Threeexamples 
clarifyhowtheseFOR interact.Tobegin,auseris considered holdingaWiimote naturally when+zisupinboththeWiimote 
sandthe Earth sFORandtheWiimote sfrontpointsawayfromthe user and toward a sensor bar, which is usually 
on top of the display. In the .rst example, the user moves theWiimotetowardthe sensorbar. This resultsin 
acceleration reportedinthe y-axisof boththe Earthand Wiimote FORs and the sensor bar reporting decreased 
distance between theWiimote and it. In the second example,the user rotatestheWiimotedowntopointtowardthe 
earth(a90 pitch). Whenthe useragain movestheWiimote directlytowardthe sensorbarin frontofhimorher,theWiimote 
reports acceleration in its z-axis. However, the Earth s FOR has acceleration in its y-axis because the 
90 downward pitch didn tchange the Earth s FOR. The sensor bar has no FOR because as theWiimote rotatesaway 
from the sensor bar, theWiimote loses contact with it. The thirdexample has the same con.guration as 
the second example,but with the sensor bar on the ground directly below theWiimote, at the user s feet. 
When the user repeats that forward motion, the sensor bar reports theWiimote movingupin the sensor bar 
s z-axis, whereasthe EarthandWiimote data arethe sameasinthepreviousexample. Theseexamplesshow that eachFOR 
captures importantanddifferent information.The following sectionsexplainthisin more detail. 4.1.3 The 
Sensor Bar Connection The SBC is one of the Wiimote s two primary spatial sensors. It occurs when the 
Wiimote s infrared optical camera points at a sensor bar and sees the infrared (IR) light emitted by 
its LEDs. A sensor bar has LEDs on each side (see Figure 28), with a known width between them. This produces 
IR blobs that theWiimotetracksandreportsinxandycoordinates,alongwiththeblob swidthinpixels.Toimprove 
tracking distance(theWiimotecan senseblobsupto16feetaway),the sensorbarLEDsarespreadina slightarc,withtheouterLEDsangledoutandtheinnerLEDsangledin. 
Actually,anyIR sourcewillwork, such as custom IR emitters, candles, or multiple sensor bars (provided 
you have the means to differentiate between the sensor bars). WhentheWiimoteis pointedatthe sensorbar,itpicksuptwo 
points PL =(xL,yL) and PR =(xR,yR) from the LED arrays. The midpoint between these PL and PR can easily 
be calculated and used as a 2D cursor or pointer on the display. In addition, if theWiimote is rotated 
about theY-axis, we can calculate the roll of the device with respect to the X-axis using () v roll = 
arccos x  (5) .v. where x = (1, 0) and v = PL - PR. Combining information from the sensor bar and the 
Wiimote s optical sensor also make it possible to determine howfar theWiimote is from the sensor bar 
using triangulation. The distance d between the sensor bar andWiimoteis calculated using w/2 d = (6) 
tan(./2) m  wimg w = (7) mimg v mimg =(xL - xR)2 +(yL - yR)2 (8) where . is the optical sensor s viewing 
angle, m is the distance between the sensor bar s left and right LEDs, wimg is the width of the image 
taken from the optical sensor, and mimg is the distance between PL and PR taken from the optical sensor 
s image. Note that ., wimg, and m are all constants. This calculation only works when the Wiimote device 
is pointing directly at the sensor bar (orthogonally). When the Wiimote is off-axis from the sensor bar, 
more information is needed to .nd depth. We can utilize the relative sizes of the points on the optical 
sensor s image to calculate depth in this case. To .ndd in this case, we compute the distances corresponding 
to the the left and right points on the optical sensor s image wL/2 dL = (9) tan(./2) wR/2 dR = (10) 
tan(./2) using wimg  diamLED wL = (11) diamL wimg  diamLED wR = (12) diamR where diamLED is the diameter 
of the actual LED marker from the sensor bar and diamL and diamR are the diameters of the points found 
on the optical sensor s image. With dL and dR, we can then calculate Wiimote s distance to the sensor 
bar as v d = dL 2 +(m/2)2 - 2dL(m/2)2 cos(.) (13) where d22 - d2 cos(.)= LmR . (14) 2mdL Note that with 
d, dL, andm wecanalso.ndtheangularpositionoftheWiimotewithrespecttothe sensor bar, calculated as () d2m2 
- d2 a = arccos L . (15) mdL 4.1.4 3-Axis Accelerometer The secondWiimote input is the device s 3-axis 
accelerometer. The accelerometer reports acceleration datainthedevice sx,yandz directions,expressed convenientlying 
s. Thisis commonof manydevices employing 3-axis accelerometers such as the cell phones like the iPhone, 
laptops and camcorders. With this information,theWiimoteisableto sensemotion, reportingvaluesthatareablendof 
accelerations exerted by the user and gravity. As the gravity vector is constantly oriented towards the 
Earth (or (0, 0, 1) inEarth sFOR),thegravityvectorcanbeusedtodiscoverpartoftheWiimote s orientationintermsof 
earth s frame of reference using () pitch = arctan az (16) ay () roll = arctan az . (17) ax Unfortunately, 
determiningyawinthe Earth sFORisn tpossible becausethe Earth sgravityvectoraligns with its z-axis. Another 
unfortunate issue is that determining the actual acceleration of theWiimote is problematic owing to the 
confounding gravity vector. To determine the actual acceleration, one of the following must take place: 
 theWiimote must be under no acceleration other than gravity so that you can accurately measure the gravity 
vector (in which case, you already know the actual acceleration is zero),  you must make assumptions 
about theWiimote s orientation, thus allowing room for errors, or  you must determine the orientation 
by other means, such as by the SBC or a gyroscope (we discuss this in more detail later).  The implicationsfor 
orientation trackingbythe accelerometers are thattheWiimote s orientationisonly certain whenitis under 
no acceleration. For this reason, manyWiigames require that users either hold theWiimotesteadyforashortperiodoftime 
beforeusingitinagametrialorhaveitpointedatthe screen and orient by the SBC. 4.1.5 Wii MotionPlus This 
attachmentusestwo gyroscopestoreportangularvelocityalongallthreeaxes(one dual-axisgyrofor xandyanda single-axisgyroforz). 
Mechanical gyroscopeswould typicallybetoolargeandexpensive foraWiimote. So,it uses MEMS (microelectromechanical 
system) gyroscopes, which operate usinga vibrating structure, are inexpensive, use little power, and 
arefairly accurate. A MotionPlus-augmented Wiimote provides information on changes to theWiimote s orientation, 
alleviating manyof the device s data limitations.Withthis, Nintendois attemptingtoimprovethe orientation 
accuracyofthedevice. The MotionPlusisn tyetfullyreverse engineered.It reports orientation changesintwo 
granularities,fast and slow, withfast being roughly four times the rate per bit. The gyroscope manufacturer 
reports that the two gyroscopeshavea lineargainbut thatthedifferent gyroscopes reportvaluesintwodifferent 
scales, so there s no single scalingfactor. Additionally, temperature and pressure changes can impact 
this scale factor and change the value associated with zero orientation change. Merging the acceleration 
and gyroscopic data isn tsimple; both sensors haveaccuracyand drift errors that, albeit small, amount 
to large errorsover short time periods. When using the SBC, you can compensate for these accumulating 
errors by providing an absolute orientation and position. Researchers have improved orientationby merging 
accelerometer and gyroscopic databut didn ttesta system under translational motion [Luinge et al. 1999]. 
Other research has shown that you can combine accelerometers and gyroscopes for accurate position and 
orientation tracking [Williamson and Andrews 2001]. In addition, researchers have successfully used Kalman 
.lters to merge accelerometer and gyroscopic data [Azuma and Bishop 1994;Williamson et al. 2010]  4.2 
Design Considerations BecauseoftheWiimote smanylimitations,youmusttakeinto accounttwodesign considerations. 
4.2.1 Waggling Motions Cheating motions(orlessfatiguingand comfortablemovements, dependingonyour perspective)area 
sideeffectofWiimote input limitations. Game designers might intend forgamesto encourageexercise, breaking 
the stereotype of the lazy videogame player. However, theWiimote s inability to detect actual position 
change lets users make only small or limited waggling motions, which theWiimote interprets as fullmovement. 
The resultis boxinggames playedby tappingtheWiimote, tennisgames played with wrist .icks,and manygames 
winnableby simplymovingthedevice randomly. Although thisis stillfun forgamers,it limitstheWiimote s utilityfor 
3DUIsandexerciseand healthgaming, unlessyouemploy better hardware and data-interpretation methods. 4.2.2 
Compensationby Story Whenever possible, the easiest means of compensating for input hardware limitations 
is through the use of story. By use of story, we mean that by careful manipulation of the users tasks 
and their goals, the shortcomingsofthe hardware canbeavoided. Thisis commoninWiigaming,in which accuracyis 
secondtoenjoymentand playability.Asa .rstexample, considerthe inherent driftinWiimotes.You can compensate 
for the drift by requiring the user to return to a known position from which you can assume theWiimote 
s orientation,oryoucan createanSBCby requiringthe usertopointatan on-screenbutton to begin a task. Asa 
secondexample, story can engage users, instructing them to freeze their hand ata known orientation. From 
this orientation assumption, the gravity vector can be assumed and actual acceleration calculated. Severalexamples 
come fromgames. TheWe Cheergame instructs playersto hold theWiimote asif they reholding pom-poms,WarioWaretellsplayershowtoholdtheWiimoteforeachminigame,andWii 
Sports Boxing makes players raise their hands when preparing for a .ght. Researchers commonly guide participants 
by story, instructing them to enter start positions before a trial. For example, in studies of 3DUI selection 
techniques, researchers often have users select an object to begin a trial.  4.3 Interpreting Wiimote 
Data HerewereviewtwobasictypesofWiimotedata interpretation,integrationand recognition through heuristics. 
Note we also discuss traditional gesture recognition in the last section of these course notes. 4.3.1 
Integration Because acceleration is the rate of velocity change, which is the rate of positional change, 
you can theoretically integratetheWiimote acceleration datato .ndvelocityandreintegrateitto .ndtheWiimote 
s actual position change. You should also be able to integrate the gyroscope s angular velocity to achieve 
absolute orientation. However,this approach has three signi.cant limitations [Giansanti et al. 2003]. 
First, acceleration jitter can lead to signi.cant positional deviations during a calculation. So, you 
can ignore accelerations below a threshold because they re most likely the result of jitter. Alternatively, 
a smoothing function can help reduce jitter. Second, you must remove the gravityvector from the reported 
acceleration before computingvelocity.IftheSBCora gyroscopeisavailable,you can possibly infertheWiimote 
s orientation and realize the gravity vector as a 1-g downward force; otherwise, you must determine the 
gravity vector on the .y. One way to compute the gravity vector is by watching the derivative of the 
acceleration (or jerk data). When this is close to zero (that is, no acceleration change is computed) 
and the reported acceleration magnitudeis closeto1g, thereby eliminating casesof user-induced constant 
acceleration,youcan assumetheWiimoteis reportingonlythegravityvector. Subtractingthisvectorfrom future 
accelerations results in the acceleration being directly attributable to user action, assuming theWiimote 
maintains its orientation. Third, orientation must be accurate; even slight errors produce large positional 
errorsaftermovement.Forexample,onemeteroftravelaftera.ve-degree orientationchange(wellwithin thevarianceofthehand) 
resultsinan errorofnearly9 cm.Alarger30-degree orientationchange results in an error of 50 cm. Additionally, 
unaccounted-for orientation changes can give very wrong results. For example, raisingtheWiimoteafoot,invertingit,andloweringittoits 
original locationwill resultinno actual positional change,butas computed will resultinatwo-footupwardmovementintheWiimote 
s FOR.You can address this onlyby using the SBC ora gyroscope, because theWiimote s accelerometers can 
teasily providea gravityvector while theWiimoteis moving, as we discussed earlier. Inwork witha locomotion 
interfacefor American football[Williamsonetal. 2010], double integrationlet users controlthe applicationusingaWiimoteandhave 
theirbodymovements maneuverthe quarterback. To achievethis,aWiimotewas attachedtothe centerofthe user 
s chest, whichwas closetothebody s center of mass. Although this improved the reported acceleration data, 
it broke the SBC. TheWiimote acceleration data was then passed through an exponential smoothing .lter: 
acurrent = aai + (1 - a)ai-1 (18) where a =0.9.An alternate approachisto usea Kalman .lter,but this requires 
more computation[LaViola 2003]. Finally the double-integration step was performed on the smoothed acceleration 
data. Owing to these three steps, theWiimotewas responsive, and the user seemed to move relatively accurately 
for that application. To further evaluate the control s accuracy, tests were performed in which the user 
moved from a starting locationandthenback. Thesetestsshowedlittle errorinpositionovershorttimeperiods(5to10seconds), 
but only when users maneuveredin an unnaturally upright and stifffashion. 4.3.2 Recognition through 
Heuristics TheWiimote providesa raw data stream, and you can use heuristics to interpret and classify 
the data. Whether you use heuristics on their own or as features for gesture recognition (which we discuss 
later), their higher-level meaning is more useful for 3DUI design and implementation. In this section, 
we discuss several heuristic-based approachesfor interpretingWiimotedatausedintwo prototypegame applications. 
OneManBandusedaWiimoteto simulatethemovements necessaryto controltherhythmandpitchof several musical 
instruments [Bott et al. 2009]. Careful attention to theWiimote data enabled seamless transitions between 
all musical instruments. RealDance explored spatial 3D interaction for dance-based gamingand instruction 
[Charbonneauetal. 2009].By wearingWiimotesonthe wristsand ankles, players followed an on-screen avatar 
s choreographyand had their movements evaluated on the basis of correctness and timing. Poses and underway 
intervals. A pose is a length of time during which the Wiimote isn t changing position. Poses canbe useful 
for identifying held positionsin dance, duringgames, or possiblyevenin yoga.AnunderwayintervalisalengthoftimeduringwhichtheWiimoteismovingbutnot 
accelerating. Underway intervals can help identify smooth movements and differentiate between, say, strumming 
on a guitar and beating on a drum. Because neither poses nor underway intervals have an acceleration 
component, you can t differentiate them by accelerometer data alone. To differentiate the two, an SBC 
can provide an FOR to identify whether theWiimote hasvelocity. Alternatively,you can use context, trackingWiimote 
accelerationsover time togauge whether the device is moving or stopped. This approach can be error prone,but 
you can successfully use it until you reestablish the SBC. Poses and underway intervals have three components. 
First, the time span is the duration in which the user maintains a pose or an underway interval. Second, 
the gravity vector s orientation helps verify that the userisholdingtheWiimoteatthe intended orientation.Of 
course, unlessyouuseanSBCora gyroscope, theWiimote syawwon tbe reliably comparable. Third,theallowedvarianceisthe 
thresholdvalueforthe amountofWiimote acceleration allowedin the heuristic before rejecting the pose or 
underway interval. In RealDance, poses were important for recognizing certain dance movements. For a 
pose, the user was supposed to stand still in a speci.c posture beginning at time t0 and lasting until 
t0 + N, where N is a speci.ed number of beats. So, a player s score could be represented as the percentage 
of the time interval during which the user successfully maintained the correct posture. Impulse motions. 
An impulse motionis characterizedbya rapid changein acceleration, easily measured bytheWiimote s accelerometers. 
Agoodexampleisa tennisorgolfclubswingin whichtheWiimote motion accelerates through an arc or a punching 
motion, which contains a unidirectional acceleration. An impulse motion has two components, which designers 
can tune for their use. First, the time span of the impulse motion speci.esthe windowover whichthe impulseis 
occurring. Shorter time spans increase the interaction speed,but larger time spans are more easily separable 
from background jitter. The second component is the maximum magnitude reached. This is the acceleration 
bound that must be reached during thetimespanin orderfortheWiimoteto recognizethe impulse motion. You 
can also characterize impulse motions by their direction. The acceleration into a punch is basically 
a straight impulse motion, a tennis swing has an angular acceleration component, and a golf swing has 
both angular acceleration and even increasing acceleration during the follow-through when the elbow bends. 
All three of these impulse motions, however, are indistinguishable to theWiimote, which doesn t easily 
sense these orientation changes.Forexample, the punch has an accelerationvector alonga single axis, as 
does the tennis swing as it roughly changes its orientation as the swing progresses. You can differentiate 
the motions onlyby using an SBC ora gyroscope orby assuming that theWiimote orientation doesn t change. 
RealDance used impulse motionsto identify punches.Apunchwas characterizedbya rapid deceleration occurringwhenthearmwasfullyextended.Inarhythmgame,this 
instantshouldlineupwithastrongbeat in the music. An impulse motion was scored by considering a one-beat 
interval centered on the expected beat.FortheWiimote correspondingtotherelevantlimb,thetimesampleinthetimespan 
corresponding tothe maximal accelerationintheWiimote s longitudinalaxiswas selected.Ifthis maximal acceleration 
was below a threshold, no punch occurred, and the score was zero. Otherwise, the score was computed from 
the distance to the expected beat. If the gesture involved multiple limbs, the maximal acceleration valuehadtobe 
greaterthanthe thresholdforallWiimotesinvolved. Theaverageofallindividuallimb scores served as the gesture 
s overall score. Impact events. An impacteventisan immediate halttotheWiimoteduetoa collision, characterized 
by an easily identi.able accelerationbursting across all three dimensions. Examplesof thisevent include 
the user tapping theWiimote ona table ora droppedWiimote hitting the .oor. To identify an impact event, 
compute the change in acceleration (jerk) vectors for each pair of adjacent time samples. Here, tk corresponds 
to the largest magnitude of jerk: tk = argmax .at - at-1.. (19) T If the magnitudeis larger thana thresholdvalue, 
an impact has occurred. RealDance used impact motions to identify stomps. If the interval surroundingadance 
movehadamaximal jerkvalue less thanathreshold, no impact occurred, and the score was zero. Otherwise, 
the score was calculated the same way as for an impulse motion. One Man Band also used impact events 
to identify when a Nintendo Nunchuk controller andWiimote collided, whichishow users played handcymbals. 
Modal differentiation. You can use easily recognized modes in an interface to differentiate between functionality. 
Forexample,abutton s semantics can change as theWiimote changes orientation, or the Wiimote s pitch might 
differentiate between a system s states. Although modes can lead to errors when users are unawareof them, 
constant user action suchas holdingabuttonor pose can leadto quasimodal states that are easily understood 
and useful. This is important because theWiimote has several attachments, such as the Nunchuk, and multiplebuttons 
that you can use to create quasimodes. In One Man Band, the multi-instrument musical interface (MIMI) 
differentiated between .ve different instruments by implementingmodaldifferencesbasedontheWiimote s orientation.Figure29showsfourofthese.Ifthe 
userheldtheWiimoteonitssideandtotheleft,asifplayingaguitar,the application interpretedimpulse motionsas 
strumming motions.Ifthe userheldtheWiimotetotheleft,asif playinga violin,the application interpreted 
the impulse motions as violin sounds. To achieve this, the MIMI s modal-differentiation approach used 
a normalization step on the accelerometer data to identify the most prominent orientation: a anorm = 
(20) .a. followed by two exponential smoothing functions (see Equation 18). The .rst function, with an 
a =0.1, removed jitter and identi.ed drumming and strumming motions. The second function, with an a = 
0.5, removed jitter and identi.ed short, sharp gestures such as violin strokes. The MIMI also used the 
Nunchuk s thumbstick to differentiate between the bass and guitar. Figure 29: One Man Band differentiated 
between multipleWiimotegestures using mostly simple modal differentiationsfor(a)drums,(b)guitar,(c) violin,and(d)theremin.Totheplayer,changing 
instruments onlyrequired orientingtheWiimoteto matchhowan instrument wouldbe played.  4.4 Designingfor 
Universal3D Interaction Although theWiimote has limitations for universal 3D interaction tasks, no single 
limitation completely prevents its use in a 3DUI. On the basis of this, we ve developed the following 
guidelines. 4.4.1 Selection This task-choosing one or more objects-is a useful starting point for manipulation 
tasks or for interacting withbuttons or other widgets.Two basicegocentric metaphorsexist for selection.Virtual 
hand techniques involve reaching to grab objects, with a correspondence between the virtual hand and 
the user s hand. These techniques are useful for differentiating between occluding objects and near andfar 
objects during selection.Virtual pointer techniquesinvolve de.ningavector along which the user selects 
intersecting or nearly intersecting objects. These techniques require less arm movement and can befaster 
than virtualhand techniques. Because virtual-hand techniques require depth data, theWiimote s ability 
to specify depth relates directly to itseffectiveness. Thisis critical becauseWiimote depth datais error 
prone,even under optimal conditions when an SBC exists. Unfortunately, SBCs aren t always maintained 
because users often hold a Wiimote in a power grip pointing up (like a hammer). So, pointing at the sensor 
bar requires effort and effectively operates as a virtual-pointing technique anyway. 3DUI techniques 
can help you design around these limitations. Originally, virtual-pointer selection employed reeling 
to reel objects forward or backward along the selection vector.4 Therefore, you can also effectivelyusereelingtoreelthe 
virtualhandalongits orientationvector. Discretebuttonsor incorporating the user s reaching (that is, 
extending and pulling in the arm control reeling) can achieve this. You could even steer the virtual 
hand around the sceneby mapping theWiimote soffsetvector froma prede.ned positiontothe virtualhand svelocityvector.Inthisway,Wiimotedepth 
errorsonly minimallyimpactthe virtual hand s control. Both reeling and steering limit the jitter effect 
because velocity jitter is smoother for control than positional jitter.For virtual-pointer techniques, 
theWiimote alone can tde.nea pointing vector without an SBC or a gyroscope. This is because you can tdetermine 
yaw from accelerometer data alone.Fortunately, although typicallyit sdif.cultto assumeanSBCexists,a selectiontaskby 
de.nition has the user pointing at something on the screen. So, the SBC assumption holds in a natural 
way. An exception to this would be head-mounted displays (HMDs), in which the display is attached to 
the user and not the environment. However, most commodity setups don t use HMDs.You can use gyroscopes 
to generateayawvector,but their inherent drift requires constant realignment, whichagain requiresan SBC. 
Another option enablestheWiimote aloneto specifya partial pointingvector.Yawis vitalto specifying the 
selectionvector.However,iftheuser orientsandholdstheWiimotesuchthatrollandnotyawislost (that is, pointing 
it up or down), virtual pointing becomes possible with yaw and pitch. This is because roll about the 
selection vector is lost because the selection vector is a ray (although manipulation after selection 
might be affected). To achieve this partial pointing vector, you can use story to affect the user s Wiimote 
grip. In summary: Virtual-hand techniques can have dif.culties because specifying depth is error-prone. 
Improving their depth-specifying capabilities can improve their effectiveness.  Virtual pointing can 
be effective when theWiimote remains pointed at the sensor bar or you can cajole the user into holding 
theWiimoteina different orientation.  Unless objects occlude or exist at manydepths, you should use 
virtual pointing.  4.4.2 Manipulation This task changes a selected object s position and orientation. 
It commonly occurs after a successful selection, implying that the object is being manipulated as if 
it were attached to a ray or virtual hand. Both attachment types rely on the speci.cation of accurate 
depth information, which is dif.cult with an SBC.Doublyintegratedpositionisapossibility,but orientationchangesgreatlyimpactits 
accuracy,unless assisted by an SBC or a gyroscope. In either case, problems will occur that require compensating 
design. Manipulation control can be considered in terms of its position and orientation components. For 
position manipulation of objects attached to a ray, reeling is applicable. For position manipulation 
of objects attached to a virtual hand, the steerable virtual hand is applicable. Both manipulations 
are nonisomorphic and resultinalossof naturalness.For controllingjust orientation manipulation,aWiimoteis 
adequate,but most manipulationtaskspairpositionand orientation manipulation.AWiimotewitha gyroscopecansuf.ciently 
manipulate orientation.However,orientingtheWiimotecanlosetheSBC,whichisvitalforposition manipulation.Toavoid 
losing the SBC, you can use nonisomorphic rotations.Arate-based approach can mapWiimote orientations 
to angular rotations. Or, you can use alternate rotational mappings-such as those for mapping 2D mouse 
movements to 3D orientation manipulation-to map smaller orientation changes to larger ones. In summary: 
 Isomorphic manipulation is problematic, especially for orientation speci.cation.  Manynonisomorphic 
techniques hold great potential for both position and orientation speci.cation.  4.4.3 Travel Thistaskphysicallychangestheuser 
spositionand orientation.Youcanachievetravelintwomainways. The .rst is by declarative means-that is, 
selecting a position to travel to. The second is by directional means-that is, indicating a velocity 
vector to travel along. Whereas you can achieve declarative travel by using a selection technique, directional 
travel requires the problematic speci.cation of a vector to travel along.So,travelin3DenvironmentsbyWiimoteis 
limitedinways similarto selectionand manipulation usingaWiimote. Both typesof travel require 1. selecting 
position and/or orientation, 2. selecting velocity or acceleration, and 3. indicating when to start, 
continue, and end travel.  Position and orientation selection can employvirtual-hand techniques.Velocity 
and acceleration selection can be a static value chosen by you or dynamically by the application or the 
user. Examples of the latter includehavingthe user sreachortheWiimote spitch indicatevelocity. Indicatingwhento 
start, continue, andendtravelis typicallyachievedby discreteeventssuchaspushingabutton,butyoucanalsousevoice 
commandsor clearly de.nable modaldifferentiations(forexample,extendingtheWiimotebeginstravel). Youcanproduceatravelvectorinseveralways. 
AlthoughtheWiimotehasenoughbuttonsto controltravel ina3D space, thiswouldbe limitingandwould feel unnatural.However,the 
discretebuttons could also produce precise, constrained movements if the task requires. Otherwise, the 
typical means of specifying atravelvectoristopointinthe directionoftravel.WithWiimotes,thisis hinderedbythe 
inabilityto determine yaw by accelerometers alone. Even with an SBC, there s a limitation of 45 degrees. 
However, aswe mentionedbefore,a gyroscope enablestheWiimotetoreportyaw information.Pairingonewithan SBCto 
deal with gyroscope drift will createafairlyrobust travelvector. Alternatively,a positionoffset oftheWiimotefroma 
prede.nedpointcan indicatea3Dvector usefulfortravel.Forexample,the user canselectanarbitrarypointbypressingandholdingabutton. 
Anydisplacementfromthatpointcanbe detected by the SBC or as impulse motions. This creates a vector that 
you can map to a velocity. This design leaves theWiimote s orientation informationavailable. Several 
compensating design alternatives exist for when an SBC isn t available. For many3D environments, travel 
is constrained to the environment s 2D surface, so only a 2D vector is required. In this case, youcouldeasilymaptheWiimote 
orientation s2Drollandpitchtoa2Dtravelvector. Alternatively,you could use story to compel the user to 
hold theWiimote upright for two possible uses: making roll and pitch appear as yaw and thrust and  
a virtual analog joystick.  You can extend these 2D vector speci.cation designs to 3D travel. To travel 
off the 2D plane, you can mapWiimotebuttonsor raisingandloweringtheWiimotetoa magnitudevector. Raisingandlowering 
are identi.ableby impulse motions.However, thisis limitedbytheWiimote s inabilityto detect rotation andpitch 
whilemoving, unlessthe orientationisheld constant,a gyroscopeis attached,oranSBCis used. OrientationscouldeasilybreaktheSBC,butthese 
casestendtoberareowingtothe constrainingrange of comfortable wrist movements. Although these compensating 
designs are suf.cient for travel, theylimit the ability to control orientation during travel. In summary: 
 Witha gyroscope and an SBC,a well-con.gured travel technique can practically eliminate theWiimote s 
limitations for travel tasks.  If travelis required only alonga2D plane, theWiimote alone canbe suf.cient. 
  4.4.4 System Control and Symbolic Input Thesetasksinject command-basedand discreteeventsintoan application.In2Dinterfaces,thekeyboard 
and WIMP (windows, icons, menus, pointing devices) metaphor are the dominant means of generating this 
input. However, neither is optimally suited for theWiimote. First, typing can be dif.cult while holding 
aWiimote. Second,3D interfaceslet usersmove,so users aren t necessarily within reachofakeyboard. Third, 
mouse emulationwithaWiimoteis possible,butevenwith .lteringto removejitter,the emulation lacks the precision 
users can achieve by manipulating a mouse with their .ngertips. If you use WIMP interfaces withWiimotes, 
larger on-screen elements shouldbe used. You can improve yourWiimote system control and symbolic inputin 
four additional ways. First, nonisomorphic mapping of theWiimote s orientation and position can result 
in more precise control of the Wiimote s cursor. Alternatively, using theWiimote s position to control 
its cursor can improve accuracy but can quickly leadtofatigue. Second, identifyinga small setofWiimote 
gestures can leadtoa simple vocabulary for system control. Keeping the gesture set small improves user 
recall and improves gesture recognition. Third, theWiimote can provide feedbackin the formof LEDs and 
sound. Incorporating better feedback with on-screen widgets could improve user interaction. Finally, 
low-frequency movements tend to indicate narrowing-in ona target during selection. Adaptivelyvarying 
theWiimote s sensitivity can improve the user s accuracyyet still allow large movements. DesigningaWiimotekeyboardisalsoa 
possibility. Symbolicinputofan alphabetwiththeWiimotehas often reliedon reproducingavirtual qwertykeyboard. 
Although thesekeyboards arefamiliarto users,they aren t optimally designed for all uses. Alternatives 
include using rotation to select symbols or devising keyboardlayoutsbettersuitedtotheWiimote. Chorded 
interactioncanalsoimprovesymbolicinput.Such interaction relies on the product of separate measures to 
create manyusable symbols. For example, you could differentiate the Wiimote s orientation into four states: 
forward, back, left, and right. With just twobuttons(1and2), which produce threebutton-press states(1,2,and1&#38;2), 
chorded interaction can produce 12 distinct symbols (through four orientations and the three states). 
Users ability to recall all these chorded combinations is another matter. Overall, system-control and 
symbolic-input tasks for the Wiimote are similar to those for existing 3D hardware. Theyrequire special 
attention to the task and the equipment ergonomics. So, existing design guidelines are directly applicable, 
and a few become especially important: Understand the wrist s limitations for comfortable rotation. 
 TheWiimotecanbepronetojitter, especiallyasthe distancefromthe screen increases.  Chorded interactioncanleadtomanysymbols,but 
without assistance, whether mnemonicsor visual representations, recall quickly becomes the limitingfactor. 
  RealNav(seeFigure30)isa reality-based locomotionstudydirectly applicabletovideogameinterfaces; speci.cally, 
locomotion control of the quarterback in American football [Williamson et al. 2010]. Focusing on American 
football drives requirements and ecologically grounds the interface tasks of: running down the .eld, 
maneuvering in a small area, and evasive gestures such as spinning, jumping, and the juke. The locomotion 
interfaceis constructedbyexploringdata interpretation methodsontwo commodity hardware con.gurations. 
The choices represent a comparison between hardware available to video game designers, trading off traditional 
3D interface data for greater hardware availability. Con.guration one matches traditional 3D interface 
data, with a commodity head tracker and legaccelerometers for running in place. Con.guration two uses 
a spatially convenient device with a single accelerometer and infrared camera. Data interpretation methods 
on con.guration two use two elementary approaches and a third hybrid approach, making use of the disparate 
and intermittent input data combined with a Kalman .lter. Methods incorporating gyroscopic data are used 
to further improve the interpretation. 4.5.2 One Man Band OneManBand(seeFigure31)isa prototypevideogamefor 
musicalexpressionthatusesnovel3Dspatial interaction techniques using accelerometer-based motion controllers 
[Bott et al. 2009]. One Man Band Real Dance(see Figure32)isagame prototype thatisexploring more natural, 
fullbody interfacesfor dancinggames [Charbonneau et al. 2009]. In thegame, users wear fourWiimotes attached 
to their wrists and ankles usingvelcro strips. This provides an untetheredexperience, meaning that the 
user does not need to stand in one place or position. Another goal of Real Dance is to explore how to 
increase the number of The visual interface 4.5.4 WoW Navigation Figure 33: Auser playingWorldofWarcraft 
using body-based controlsto navigate. Figure 34: Auser wears the sensor barinWiisoccer. WorldofWarcrafthasa 
complex interfaceandworkisbeingdoneto determinehow body-based interaction can be used to complement 
and reduce this complexity [Silva and Bowman 2009]. The idea is to using body based controls to of.oadkeyboard 
and mouse navigation, helping players to concentrate on other tasks (see Figure 33). In this con.guration, 
the user wears a modi.ed sensor bar and mounts a Wiimote on the ceiling, using the device as a camera. 
Navigation is based on a leaning metaphor. Starting fromaneutralbody position,a small amountofforwardor 
backwardmovementmakesthe characterwalk, andleaningfartherforwardmakesthe characterrun(ratecontrol).Thereisadeadzonesurroundingthe 
neutral point in which the character stands still. Leaning to the side rotates the character, with the 
amount of rotation proportional to the distance the player leaned (position control). Note that a foot 
pedal is used to activate and deactivate movement. Preliminary experimentation has shown that body-based 
interaction in addition tokeyboard and mouse can help players perform more tasks at the same time and 
can be especially attractive and helpful to new players. 4.5.5 Wiisoccer The focus of this project, 
developed at Brown University under the direction of Chad Jenkins, is on exergaming,an importantand 
up-and-coming research areathatexamineshowtobuildeffectiveexercisebasedgames.Wiisoccerisagamethatusesnatural 
locomotiontomoveplayersona soccer.eld.Thekey innovationisthattheIR sensorbaris attachedtothe usersleg(seeFigure34)andaWiimoteisusedasa 
camera and placed on the side of the player. TheWiimote detects the players running motion as well as 
kicks and passes.  5 3D Spatial Interaction with the PlayStation Move Figure 35: The PlayStation Move 
motion controller. The PlayStationMoveisa new controller for PlayStation3that will becomeavailable autumnof 
2010. Itis not meant asa replacement for the standard DualShock3 controller,but rather as an alternative 
that will enable newgameexperiences. The PlayStationMove design addresses manyof the issues discovered 
during EyeToydevelopment, combining the advantages of a camera-based interface with those of motion sensingandbuttons. 
The following sections describethe PlayStationMove systemandhowit canbe used toprovide3D inputtogames. 
5.1 PlayStation Move Characteristics The PlayStation Move system consists of a PlayStation Eye, one to 
four PlayStation Move motion controllers, and a PS3 SDK. It combines the advantages of camera tracking 
and motion sensing with the reliability and versatility of buttons. The wireless controller is used with 
one hand, and it has several digitalbuttonsonthefrontanda long-throwanalogTbuttonontheback. Internally,ithasseveralMEMS 
inertial sensors, including a three-axis gyroscope and three-axis accelerometer. But the most distinctive 
featureofthe controlleristhe 44mm-diameter sphereonthetop which housesaRGBLED thatgames cansettoanycolor.Thespherecolorcanbevariedto 
enhancegameplay,buttheprimary purposeofthe sphere is to enable reliable recovery of the controller 3D 
position using color tracking with the PlayStation Eye. Figure 36: Very early PlayStation Move prototype, 
based on a modi.ed SIXAXIS controller. The illuminatedspheredesignsolvesmanyofthecolortrackingissuesexperiencedwithEyeToy. 
Because the sphere generates its own light for the camera to see, scene lighting is much less of an issue. 
Tracking works perfectly in a completely dark room, and for scenes with highly varied lighting, the generated 
light mitigatesthevariability.Also,thelightcolorcanbeadjustedto ensureitisvisuallyuniquewithrespectto 
the rest of the scene. Finally, the choice of the sphere shape makes the color tracking invariant to 
rotation; this simpli.es position recovery and improves position precision by allowing a strong model 
to be .t to the projection. Though the PlayStation Move hardware and the algorithms for combining the 
disparate sensor data are quite complex, the SDK provides a simple interface for PS3 developers. The 
high-level state for each controller state can be queried at anytime, and consists of the following information: 
 3D position  3D orientation (as a quaternion)  3D velocity  3D angular velocity  3D acceleration 
 3D angular acceleration  Buttons status  Tracking status (e.g. visible)  5.2 PlayStationMoveTechnology 
understanding of each step. 5.2.1 Image Analysis Figure 37: Visualizing the 2D pixel intensity distribution 
of the PlayStation Move sphere is useful for debugging the model .tting stage. This size and locationin 
the image are used asa starting point for .tting the shape model.To achieve the sub-pixel precision necessary 
for acceptable 3D position recovery, the shape model is .t to the original image RGB data rather than 
only the segmented pixels and accounts for camera lens blur. It is well-known that the 2D perspective 
projection of a sphere is an ellipse [Shivaram and Seetharaman 1998], though manytracking systems introduce 
signi.cant error by approximating the projection as a circle. In theory, .ttingsuchamodeltotheimagedatais 
straightforward,butinpracticemanyissuesarise. Motionofthe sphere causes motion blur in the image proportional 
to the exposure time. Motion also causes the sphere to appear warped due to rolling shutter effects; 
like most low-cost CMOS webcams, PlayStation Eye uses a rolling electronic shutter. This meansevery lineinthe 
videois imagedata slightlydifferent time, which leads to stretching or shortening for vertical motions 
and shear for horizontal motions. Subtle artifacts can also be introduced depending on the method used 
to convert the raw sensor data to RGB data. Like most low-cost cameras,the PlayStationEye sensor usesaBayer 
patternsuchthatonlyR,G,orBis actually measured at every pixel, and RGB must be inferred from surrounding 
pixels (a process known as Bayerpattern demosaicking, for which manyapproaches exist [Gunturk et al. 
2005]). Another major concern is partial occlusion, which generally causes large errors in a global .t. 
To address this, a 2D ray-casting approach is used to identify areas of partial occlusion and remove 
them from the .t. Because the size of the sphere and the focal length of the PlayStation Eye are known, 
the 3D position of the sphere relative to the camera can be computed directly from the 2D ellipse parameters. 
The output of the image analysis stage is a timestamp and a measurement of the 3D camera-relative position 
for each sphere, updated at the framerate of the camera (typically 60Hz). 5.2.2 Sensor Fusion The results 
of the image analysis are combined with the inertial sensor data using a modi.ed unscented Kalman .lter. 
The details of this powerful state estimation technique are beyond the scope of these notes, but there 
are many excellent explanations available [Crassidis and Markley 2003][Julier and Uhlmann 1997][WanandVanDer 
Merwe 2002]. Thoughthe sensorsall contributetothe .nal stateina complex manner, each has a fundamental 
contribution that is necessary for the complete state computation. For example, the camera tracking provides 
an absolute measure of the 3D position. When the controller is not moving, the accelerometer provides 
the direction of gravity, which gives an absolute measure of the pitch and roll angles. In addition, 
when the orientation is known, gravity can be subtracted from the accelerometer data to recover the controller 
acceleration. The acceleration is part of the state, and it can also be used to reduce noise in the 3D 
position and to derive the 3D velocity. The gyroscope data is also crucial because it directly provides 
angular velocity. When integrated, this provides a responsive measure of 3D rotation (relative orientation) 
and can be used to derive angular acceleration. The remaining unknown, absoluteyaw,is the most tricky,butit 
canbe inferredby comparing the motion direction computed from body-relative inertial measurements to 
the motion direction computed from camera-relative image measurements. Again, though theoretically straightforward, 
in practice there are manyissues for sensor fusion. Because each controller, the PlayStation Eye, and 
the PS3 are driven by independent clocks, each piece of sensor data must be carefully timestamped so 
that it can be combined with other sensor data properly. Inertial sensor biases must be computed dynamically 
as they vary, most often due to changes in temperature. When the sphere is temporarily occluded or beyond 
the camera view, the state must be updated despite the lack of direct position measurements. And wireless 
data dropouts, though infrequent, must be hidden by extrapolating the state based on previous inertial 
data.  5.3 Spatial Input vs. Buttons Despite the complexity of the underlying technology, the PlayStation 
Move SDK makes accessing the high-level state straightforward. At anyinstant in time, the 3D state elements 
represent a complete spatial description of the controller, and thebutton status captures whichbuttons 
are being pressed. Developers arefreeto decidehowthis spatialandbuttoninputwillbeusedto interfaceto theirgame. 
How to interpret the controller state data is an importantgame design decision. As we learned when developingEyeToy, 
thoughitis possibletomake some greatexperiences withoutbuttons,for other great experiencesbuttons are 
both necessary and desirable. Spatial datais appropriate for some actions,but dif.cult to map to others. 
Alternatively,buttons can be abstracted to trigger nearly anyaction,but this abstraction can causea disconnect 
between the player and thegame. Spatial interactioniseffective for actions such as manipulation, selection, 
pointing, or movement. Also, spatial interaction is applicable when multiple degrees of freedom must 
be modi.ed simultaneously. An early study into the design issues for spatial input can be found in [Hinckley 
et al. 1994]. Buttons are most effective when an action is binary, or when it needs to occur often. Buttons 
are also ideal for time-sensitive actions, since very little time is required for the actual player action. 
 Providinga combinationof spatialinputandbuttonswasakeydesign strategyfor PlayStationMove,asit providesthemost 
interaction possibilities.TheanalogTbuttonin particularwas designedto complement the spatial input. Though 
manymight initially dismiss thisbutton as justa nice-feeling trigger, the longthrow design allowsa playerto 
easily controlthe analoglevelby alteringthe positionofthebutton(not the pressure, which is much harder 
to control effectively). This feature of the analogTbutton makes it ideal as a modi.er for spatial actions, 
such as controlling the line thickness while drawing or the rate of a variable-speed drill. It offers 
great affordance for grasping actions in particular, because squeezing theT button is directly analogous 
to grasping. 5.4 1-to-1 Data Mapping vs. Other Mappings The spatial input for the PlayStation Move available 
to a game is the complete 3D pose and motion of the controller, and thegame must decide how best to use 
this state. The controller pose may be used without modi.cation to achievethe special case of 1-to-1 
mapping of controller movement to movement of a virtual object. Alternatively, the 3D state can be mapped 
in a less direct manner, ranging from a simple geometric transformation of the state to an abstracted 
interpretation of the data (such as gesture recognition). 5.4.1 1-to-1 Bene.ts and Challenges True 1-to-1 
mappingofthe controller statetoavirtual object createsavery realisticand intuitiveinterface. The player 
feels very connected to the virtual experience, because of the close correspondence between his own physical 
motion and the observed motion of a game object. Since the absolute position and orientation are measured, 
not only does the player s motion matter,but also where he is performing the action also matters.Astrong 
senseof control results as players quickly realize their actions are mimicked perfectly,and little trainingorexplanationis 
neededin most cases. This manifestsasavery .rst-person experience, with the player feeling as if he is 
directly performing an action himself rather than just issuing a command that it should occur. An even 
stronger .rst-person feeling arises for applications that use a true 1-to-1 mapping to make the controller 
behave as a pointing device, such as a gun or a .ashlight. This is implemented by using the full 3D controller 
pose and the display size/location to calculate the intersection of a ray cast from the controller with 
the display. The result feels as if the display is a window to a virtual world, and the controller can 
point from the real-world through this window and into the virtual world. Similarly, if the 1-to-1 mapping 
is used to position the virtual vantage point by updating the display view transform, the rendering is 
as if the virtual world is being imaged from the point of view of the controller. This is much likethe 
DeskTopVReffectforface tracking described earlier,except that becausethe controlleris heldin the player 
s hand,itis asif the virtualworldis being see througha camcorder that the playeris holding. Several challenges 
can arise when using 1-to-1 mapping, mostly related to a discrepancy between the constraints that exist 
in the real world or virtual worlds. For example, in the real-world, there is a limit to the volume over 
which the controller can be tracked, which therefore limits the virtual coverage space. For somegames, 
this canbe addressedbykeeping theexperience on rails so that the coverage space automaticallymovesinthe 
virtual spaceas dictatedbythegame. To achieve more free-formmovement about the virtual space, the spatial 
navigation techniques described earlier could be applied by using a clutch totodisengagethe1-to-1mapping 
temporarily[Hinckleyetal.1994]orbyusingasecondmotion controller. Alternatively, a 1-to-1 mapped motion 
controller in one hand can be used in conjunction with a more traditionalgame controllerin the other 
(such as the DualShock, or the PlayStationMove navigation controller). Conversely,inthe virtualworld, 
sometimesthereare obstaclessuchthata1-to-1mappingofthe controlled virtual objectwould resultinan impossiblegame 
situationfor certain real-world posesofthe controller. Thegame cannot controltheplayer s actionstoavoidsuch 
situations, soitmust handlethem. Simple collision detection and response may suf.ce if the obstacles 
are much lighter than the controlled virtual object,but generally other techniques must be used. One 
option is to simply break the 1-to-1 mapping temporarilyand restrictthegame object motion, though thatmay 
createa disconnectinthe playerexperience. Another option is to ghost thegame object and allow it complete 
freedom in the virtual space, though that may break the desiredgame behavior. Finally, a combination 
approach canbe used, such that when an impossible situation arises, thegame objectis temporarily disconnectedbuta 
ghost versionis drawn until the situation is reconciled. Finally,ifthe controlled virtual objectistobe 
heldbya virtualgame character, there canbea mismatch in the range of motion achievable for the character 
vs. the player.For simple humanoidgame characters, thiscanbe addressedby calibratingtheplayersizeto computean 
appropriatescalingfactortomapthe playermotiontothe character.Formoreexotic characters,itmaynotbepossibletousea1-to-1mapping 
directly; instead, a 1-to-1 mapping could be used to calculate a target goal, and a .tting function could 
be used to compute the character state that comes closest to achieving the goal. Asimilar effect may 
be achieved by applying forces to move the character generated by virtual springs between the target 
goal location and the current object location. Note that though such a technique used the 1-to-1 mapping 
for computation, the resulting player experience may not be perceived as 1-to-1, and an alternative mapping 
that is not 1-to-1 might be equally effective. 5.4.2 OtherPossible Data Mappings There are an endless 
number of other possible data mappings for the PlayStation Move controller state. The analog, multidimensional 
nature of the data makes it ideal for mapping to other parameterizations. Simple direct mappings using 
a subset of the pose are often useful, such as mapping the controller XY position to a 2D cursor position 
on the screen, or perhaps the controller yaw and pitch to the same 2D cursor position.Partsofthe controllerposecanalsobemappedto 
non-spatial parameterssuchascoloror a character s smile. Nonlinear mapping functions can be used to further 
extend the mapping possibilities; for example, trigonometric functions are useful for creating a periodic 
mapping, while exponential and logarithmic functions are commonly used to increase either the range or 
the precision as desired. Otherinputdatacanbemappedtogame parametersin additiontothe absoluteposedata.Therelativepose 
with respect to a temporary reference can be used, or more generally the differential pose between two 
motion controllers. This data can be distilled down to a single value, like the magnitude of the distance 
betweentwo controllers,toprovideasimple control parameter.The controllervelocitiesand accelerations canalsobemappedtogame 
parameters,withthe magnitudeand directionbeing particularly useful.For example, the speed at which the 
controller is moved can be used to control the intensity of a magic spell, or the direction of acceleration 
can be used to aim an attack. The possibilities for mapping spatial data are limitless,but unlike1-to-1, 
other mappings may not be obvious or intuitive. Because of this, immediate graphical feedback is important, 
and extra work may be neededbythegameto educatethe playeronhowto usethe control scheme. Thisis especially 
important for highly abstracted mappings such as gestural input, which can be useful for triggering and 
in.uencing actions that are not possible or practical with direct mapping (e.g. doing a back .ip).  
 6 3D Gesture RecognitionTechniques Recognizing 3D gestures has had a long history in the virtual reality 
and 3D user interface communities. Themotion controllerswehave discussedthusfarcanallbeusedtodetect3Dgesturesfordifferentvideo 
game applications. We divide gesture recognition into heuristic-based and machine-learning approaches. 
With heuristic-based approaches, the designer manually identi.es the heuristics that classify a particular 
gesture and differentiate it from other gestures that the system recognizes. Although this manual approach 
can be time consuming and tedious, especially as the set of recognized gestures grows, it s also highly 
understandable. Understandability is useful when new gestures are added to the gesture set and must be 
differentiated from past gestures. Simple heuristics such as the ones we ve discussed in the section 
onWiimotes can be effective, especially when composed into more complex heuristics. With machine learning 
approaches, gestures canbe recognizedby .nding useful featuresin the input data stream. These features 
de.ne a feature vector that is used as part of a machine learning algorithm. Typically, a set of gestures 
are collected to train the machine learning algorithm, which is used to match the feature vector to the 
set of possible gestures, differentiating between them. Inthis section,wewillexaminetwocommonlyused machine 
learning algorithms,alinear classi.erandan algorithm using the AdaBoost framework, and conduct experiments 
to examine their accuracy[Hoffman et al. 2010]. There are, of course, a variety of different machine 
learning algorithms that could be used to recognize 3D gesture and the reader is encouraged to examine 
[Duda et al. 2001]. We make use of the Wiimotetoevaluate these algorithms,but data fromanyofthedeviceswehave 
discussedin this course could be used. The only difference would be in how the features are calculated. 
6.1 Linear Classi.er The linear classi.er is based on Rubine s gesture recognition algorithm [Rubine 
1991]. Given a feature vector f, associated with a given 3D gesture g in a gesture alphabet C, a linear 
evaluation function is derived over the features. The linear evaluation function is given as F . gc = 
wc0 + wcifi (21) i=1 where 0 = c<C, F is the number of features, and wci are the weights for each feature 
associated with each gesture in C. The classi.cation of the correct gesture g is the c that maximizes 
gc. Training of this classi.er is done by .nding the weights wci from the gesture samples. First, a feature 
vector mean fc is calculated using Ec-1 . 1 fci = fcei (22)Ec e=0 where fcei is the ith feature of the 
eth example of gesture c and 0 = e<Ec where Ec is the number of training samples for gesture c. The sample 
covariance matrix for gesture c is Ec-1 .  Lcij =(fcei - fci)(fcej - f cj). (23) e=0 The Lcij are averaged 
to create a common covariance matrix C.-1 Lcij Lij = c=0 C.-1 . (24) -C + Ec c=0 The inversion of Lij 
then lets us calculate the appropriate weights for the linear evaluation functions, F .  wcj = (L-1)ijfci 
(25) i=1 and F . wc0 = - 1 wcifci (26) 2 i=1 where 1 = j = F . 6.2 AdaBoost Classi.er The AdaBoost 
algorithm is based on LaViola spairwise AdaBoost classi.er [LaViola and Zeleznik 2007]. AdaBoost [Schapire 
1999] takes a series of weak or base classi.ers and calls them repeatedly in a series ofroundsontrainingdatatogeneratea 
sequenceofweakhypotheses.Eachweakhypothesishasaweight associated withit thatis updated after each round, 
basedonits performanceonthe training set.Aseparate setofweightsareusedtobiasthetrainingsetsothatthe importanceof 
incorrectly classi.edexamplesare increased. Thus, the weak learners can focus on them in successive rounds. 
Alinear combination of the weakhypothesesandtheirweightsareusedtomakeastronghypothesisfor classi.cation. 
More formally,for each unique 3D gesture pair,our algorithm takes as input training set (x1,y1), ..., 
(xm,ym), where each xi, represents a feature vector containing J features. Each yi labels xi using label 
set Y = {-1, 1}, and m is the total number of training samples. Since we are using a pairwise approach, 
our algorithmneedstotrainalluniquepairsof gestures.Foreachuniquepair,the AdaBoostalgorithmiscalled on 
a set of weak learners, one for each feature discussed in Section 6.3. We chose this approach because 
we found, based on empirical observation, that our features can discriminate between different 3D gesture 
pairs effectively. We wanted the features to be the weak learners rather than having the weak learners 
act on the features themselves. Thus, each weak learner Cj uses the jth element in the xi training samples, 
which is noted by xi(j) for 1 = j = J. 6.2.1 Weak Learner Formulation We use weak learners that employa 
simple weighted distance metric, breaking (x1,y1), ..., (xm,ym) into two parts corresponding to the training 
samples for each gesture in the gesture pair. Assuming the training samples are consecutive for each 
gesture, we separate (x1,y1), ..., (xm,ym) into (x1,y1), ..., (xn,yn) and (xn+1,yn+1), ..., (xm,ym) and 
de.ne D1(i) for i =1, ..., n and D2(i) for i = n +1, ..., m to be training weights for each gesture. 
Note that in our formulation, D1 and D2 are the training weights calculated in the AdaBoost algorithm 
(see Section 6.2.2). For each weak learnerCj in each feature vector xi(j) in the training set, the weighted 
averages are then calculated as . n k=1 xk(j)D1(k) j1 = . n (27) l=1 D1(l) and . m xk(j)D2(k)k=n+1 j2 
= . m . (28) D2(l) l=n+1 Theseaveragesareusedto generatetheweakhypothesesusedinthe AdaBoost training 
algorithm. Ifa given feature value for a candidate gesture is closer to j1, the candidate is labeled 
as a 1, otherwise the candidate is labeled as a -1. If the feature value is an equal distance away from 
j1 and j2, we simply choose to label the gesture as a 1. Note that it is possible for the results of 
a particular weak classi.er to obtainlessthan50% accuracy.Ifthis occurstheweaklearnerisreversedsothatthe.rstgesturereceivesa 
-1 and second gesture receivesa 1. This reversal lets us use the weak learner s output to the fullestextent. 
 6.2.2 AdaBoost Algorithm For each roundt =1, ..., T *J where T isthe numberof iterationsoverthe J weak 
learners, the algorithm generatesa weakhypothesis ht : X . {-1, 1} from weak learner Cj and the training 
weights Dt(i) where j = mod(t - 1,J)+1 and i =1, ..., m. This formulation lets us iterate over the J 
weak learners and still conform to the AdaBoost framework [Schapire 1999]. Indeed, the AdaBoost formulation 
allows us to select weak classi.ers from differentfamilies at different iterations. 1 Initially, Dt(i) 
are set equally to m , where m is the number of training examples for the gesture pair. However, with 
each iteration the training weights of incorrectly classi.ed examples are increased so the weak learners 
can focus on them. The strengthofa weakhypothesisis measuredby its error . .t = Pri~Dt .Dt(i). (29) [ht(xi(j)) 
= yi]= i:ht(xi(j)).=yi Givena weakhypothesis, the algorithm measures its importance using the parameter 
 () 11 - .t at = ln . (30) 2 .t Withat, the distribution Dt is updated using the rule Dt(i) exp(-atyiht(xi(j))) 
Dt+1(i)= (31) Zt where Zt isa normalizationfactor ensuring that Dt+1 is a probability distribution. This 
rule increases the weight of samples misclassi.ed by ht so that subsequent weak learners will focus on 
more dif.cult samples. Once the algorithm has gone through T * J rounds,a .nalhypothesis () T *J . H(x)= 
sgn atht(x)(32) t=1 is used to classify gestures where at is the weight of the weak learner from round 
t, and ht is the weak hypothesis from roundt. If H(x) is positive, the new gesture is labeled with the 
.rst gesture in the pair and if H(x) is negative it is labeled with the second gesture in the pair. These 
stronghypotheses are computed for each pairwise recognizer with the labels and stronghypothesis scores 
tabulated.To combine the results from each stronghypothesis we use the approach suggestedby Friedman 
[Friedman 1996]; the correct classi.cation for the new gesture is simply the one that wins the most pairwise 
comparisons.If thereisatie, thentheraw scores fromthe stronghypotheses are usedand the one of greatest 
absolute value breaks the tie.  6.3 Feature Set The features used in the linear and AdaBoost classi.er 
on Rubine s feature set [Rubine 1991]. Since Rubine sfeatures were designedfor2Dgesturesusingthe mouseor 
stylus,theywereextendedtoworkfor 3D gestures. TheWiimote andWii MotionPlus sense linear acceleration 
and angularvelocity respectively. Although we could have derived acceleration and angular velocity-speci.c 
features for this study, we chose to make an underlying assumption to treat the acceleration and angular 
velocity data as position information in 3D space. This assumption made it easy to adapt Rubine s feature 
set to the 3D domain and the derivative information from theWiimote andWii MotionPlus. The .rst featureintheset, 
quanti.esthe total durationofthe gesturein milliseconds, followedby features for the maximum, minimum, 
mean and median values of x,y and z. Next we analyzed the coordinates in 2D space by using the sine and 
cosine of the starting angle in the XY and the sine of the starting angle in the XZ plane as features. 
Then the feature set included features with the sine and cosine of the angle from the .rst to last points 
in the XY and the sine of the angle from the .rst to last points in the XZ plane. After that, features 
for the total angle traversed in the XY and XZ planes, plus the absolute value and squared value of that 
angle, completed the features set which analyzes a planar surface. Finally, the length of the diagonalofthe 
boundingvolume,the Euclidian distance betweenthe .rstandlast points,the total distance traveledby the 
gesture and the maximum acceleration squared ful.ll Rubine s list. Initially,this featuresetwasusedforboththeWiimoteandtheWii 
MotionPlus1, totaling58 featuresused in the two classi.ers. However, after some initial pilot runs, a 
singular common covariance matrices was formingwiththe linear classi.er.Asingular commoncovariance matrix 
causethe matrixinverse neededto 1TheWii MotionPlus used maximum angularvelocity squared insteadof maximum 
acceleration squared. .nd the weights for the linearevaluation functions impossible. These singular matrices 
were formed when tryingtousethe featuresetwiththeWii MotionPlus attachment. Becauseofthis problem,the 
MotionPlus featuresetwas culledtouseonlythe minimumand maximumx,y,andzvalues,the meanx,y,andz, values,andthemedianx,y,andzvalues.Thus,29 
featureswereusedwhenrunningtheexperimentswith theWiimote and41 features were used when runningexperiments 
with theWiimote coupled with theWii MotionPlus. 6.4 Gesture Set The majorityoftheworkon3D gesture recognitionwith 
accelerometerand gyroscope-basedinputdevices containexperimentsusingonlyfourto10unique gestures[Kratzetal.2007;Rehmetal.2008;Schl 
 omer et al. 2008]. In order to better understand how manygestures these types of devices can accurately 
recognize and how manytraining samples would be needed to do so, we chose to more than double that set 
and use25 gestures. The gestures, depicted graphicallyin Figure39, are performedby holdingtheWiimote 
in various orientations. For a majority of the gestures, the orientation and rotation are the same for 
both left and right handed users. The onlyexceptions are the gesturesTennis Swing, Golf Swing,Parry, 
and Lasso. The gesturesTennis Swing and Golf Swing are performed on either the left or right side of 
the user, corresponding to the hand that holds theWiimote.For theParry gesture,a left-handed person will 
movetheWiimotetowardstheright,whilea right-handedpersonwillmovetheWiimotetowardstheleft. Finally, the 
Lasso gesture requires a left-and right-handed user to rotate in opposite directions, clockwise versus 
counterclockwise, respectively. Thisgesturesetwasdevelopedbyexaminingexistingvideogamesthatmakeuseof3Dspatial 
interaction, speci.cally fromthe NintendoWiigaming console.Weexaminedavarietyofdifferentgames from different 
genres including sportsgames, .rst person shooters, .ghtinggames, and cookinggames. Although thegesturesetisgamespeci.c,itisgeneralenoughtoworkinawidevarietyof 
virtualandaugmented reality applications. Initially, we focused on simple movements such as line to the 
right and line to the left which could be applied to various actions. From there, slightly more complex 
gestures were added which involved closed .gures such as the square, triangle and circle. With this group, 
we add user variations in velocity, duration and distance traversed. Finally the last set of maneuvers 
allowed for more freedom in body movement in an effort to help disambiguate gestures during feature analysis. 
Examples of these gestures include golf swing, whip and lasso. 6.5 3D Gesture Data Collection Machine 
learning algorithms such as the linear and AdaBoost classi.ers need data to train on. In order for the 
algorithms to learn and then recognize gestures, a gesture database was created to use in both training 
and testing.We recruited17 participants(4 femaleand13male)fromtheUniversityof Central Florida,of which 
four men were left handed, to provide the gesture samples. Each participant had experience using the 
NintendoWiigaming console,withtwo participantsdoingsoonaweekly basis. Several steps were taken to ensure 
that participants provided good gestures samples. First, they were givena briefoverviewoftheWiimote gesture 
data collection interfaceanda demonstrationofhowto interact with the application. After the demonstration, 
participants were presented with an introduction screen asking them to choose what hand they would hold 
the Wiimote with to perform the gestures. This decision also told the application to present either left 
or right-handed gesture demonstration videos. Next, an orientation window (see Figure 40) was displayed 
to help participants learn how to perform the gestures before data collection began. Each of the 25 gestures 
are randomly listed on the left hand side of thewindowto reducetheordereffectduetofatigue.Participantsmovedthroughthelistof 
gesturesusing theupanddownbuttonsontheWiimote.Atanytime, participantscouldviewavideo demonstratinghow 
toperforma gestureby pressingtheWiimote srightbutton. These videos demonstratedhowtoholdthe Wiimote in 
the proper orientation in addition to showing howto actually perform the motion of the gesture. Before 
the gesture collection experiment began, an orientation phase required participants to perform and commit 
one samplefor each gesture. This reducedthe ordereffectduetothe learning curve.To performa gesture, participants 
pressedandhelddowntheWiimote s B buttontomakeagesture. After participants released the B button, theycould 
either commit the gesturebypressing theWiimote s A button or redo the gestureby pressingand holdingthe 
B buttonand performingthe gestureagain. This feature enabled participants to redo gesture samples they 
were not happywith. Once a gesture sample is committed to the database, the system pauses for two seconds, 
preventing the start of an additional sample. This delay between samplesallows participantsto resettheWiimote 
positionforthenext gesture sample.In addition, thedelaypreventstheuserfromtryingtogamethesystembyquickly 
performingthesamegesturewith little to no regard of matching the previous samples of that gesture.  
After participants created one sample for each of the 25 gestures, they entered data collection mode. 
The only difference between orientation mode and data collection mode is the amount of samples which 
need to be committed. When participants commit a training sample for a particular gesture, a counter 
is decremented and displayed. Amessage saying the collection process for a given gesture is complete 
is shown after20 sampleshavebeen enteredfora gesture. This message indicatestothe participantthatthey 
can move on to providing data for a remaining gesture. A total of 20 samples for each gesture, or 500 
samplesinall,isrequiredforafullusertrainingset.Intotal,8,500gesturesampleswere collected2.Each data collection 
session lasted from 30-45 minutes. 6.6 3D Gesture Recognition Experiments With the 3D gesture database, 
we were able to run a series of experiments to examine both learning algorithms in terms of user dependent 
and user independent recognition. The primary metric analyzed in our experiments is gesture classi.cation 
accuracy, while having the over-arching goal of maximizing the number of gestures correctly recognized 
at varying degrees of training the machine learning algorithms. 2The gesture database can be downloaded 
at http://www.eecs.ucf.edu/isuelab/downloads.php. For the user dependent recognition experiments, we 
used a subset of samples for each gesture from a single user to train the machine learning algorithms 
and the remaining samples to test the recognizers. This approach is equivalent to having a user provide 
samples to the recognizer up front, so the algorithm can be tailored to that particular user. For each 
user dependent test, two categories of experiments were created: classi.cation over all 25 gestures and 
.nding the maximum recognition accuracy rate over as manygesturesas possible. Both categories were then 
broken into threeexperimentsproviding5,10,or15 training samples per gesture with the remaining samples 
used for testing accuracy. Each experiment was executed on all four of the classi.ers mentioned earlier. 
The experiment set was conducted on each of the 17 participant s data. The resultsoftryingto recognizeall25 
gesturesineachexperimentareshowninFigure41.We analyzed this data using a 3 way repeated measures ANOVA 
and found signi.cance for the number of training samples(F2,15 = 17.47,p < 0.01), the classi.cation algorithm(F1,16 
= 119.42,p < 0.01), and the use of theWii MotionPlus data(F1,16 =8.23,p < 0.05). To further analyze the 
data, pairwise t tests were run. The most notable observation is the high level of accuracy across all 
experiments. In particular, the Figure 42: The average recognition results for the user dependent experiments 
over a varying number of gestures.Thegoalofthisexperimentwastorecognize,withhigh accuracy,asmanygesturesas 
possible with different levels of training. The results are grouped by the number of training samples 
(5, 10, or 15) provided for each gesture within an experiment. A single user dependent experiment utilized 
two classi.ers (linear and AdaBoost), each executing with either theWiimote orWiimote+MotionPlus input 
device producing four accuracy values. Comparison Test Statistic PValue Linear5 -Ada5 t16 = 7.17 p < 
0.01 LinearMP5 -AdaMP5 t16 = 8.36 p < 0.01 Linear10 -Ada10 t16 = 6.48 p < 0.01 LinearMP10 -AdaMP10 t16 
= 6.54 p < 0.01 Linear15 -Ada15 t16 = 7.69 p < 0.01 LinearMP15 -AdaMP15 t16 = 7.16 p < 0.01 Table 1: 
Results from a set of t-tests showing signi.cance differences between the linear classi.er and AdaBoost 
indicating the linear classi.er outperforms AdaBoost in our test cases. Note that under the comparison 
column,MP stands for whether theWii MotionPlus was used and the numberrepresents how many samples were 
used for training the algorithms. linear classi.ergaveamean accuracy valueof93.6%usingonly5trainingsamplesforeachgestureand 
98.5% using 15 training samples per gesture(t16 = -5.78,p < 0.01). Furthermore, the linear classi.er 
outperformed AdaBoostinevery situationbyat least3%(seeTable1forthe statistical results). Another important 
result shownin Figure41is the roleof theWii MotionPlusin both recognition algorithms. The Wii MotionPlus 
signi.cantly increased the recognition accuracyfor the linear classi.er to 95.5% using5 training samples(t16 
= -2.81,p < 0.05)per gesture and 99.2% using 15 training samples per gesture (t16 = -2.54,p < 0.05).For 
AdaBoost, the changein accuracy was negligible. While these accuracylevels are good for some applications, 
it is important to know which gestures would need to be removed from the classi.cation set in order to 
improve recognition results. To begin this examination, the set of gestures frequently recognized incorrectly 
from the previous three experiment categories were removed.Within this set we noticedafew gestures with 
similar attributes such asTennis Swing and Golf Swing or Square and Triangle. These gesture pairs involve 
similar movement which caused the classi.ers to misidentify each type as the other. Once the poorly classi.ed 
gestures were extractedthe accuracyresultseasily increasedtoabove98%.Wesystematicallyaddedeachoftheremoved 
gestures back into the gesture set ona casebycase basis, leaving one gesture from each similar pairing 
out in order to improve the recognition on the other included gesture from each pair. The results are 
shown in Figure42.Aswiththepreviousexperiment,werana3 way repeated measuresANOVA aswellasttests and found 
signi.cant results for the training sample/number of gestures pairs(F2,15 =5.01,p < 0.05), the classi.cation 
algorithm(F1,16 = 48.55,p < 0.01), and the use of theWii MotionPlus data(F1,16 = 9.69,p < 0.01). In theexperiment 
using5training samples, the gesturesForward, Golf Swing, Spike,Triangle and Line to Left were removed, 
thereby producing over 93.5% accuracy for AdaBoost and over 96.3% for the linear classi.er. Once the 
number of training samples was increased to 10, the set of removed gestures included only Spike and Triangle. 
This experiment yielded higher accuracy with the linear classi.er (t16 = -1.90,p =0.075)and AdaBoost(t16 
= -1.07,p =0.3),but these results werenot signi.cant. Finally, since the recognition rates for 15 training 
samples were already greater than 95%, no gestures were removed during this last test. These results 
show that recognition accuracy rates as high as 97% can be achieved for 20 gestures using only 5 samples 
per gesture and 98% for 23 gestures using 10 samples per gesture with theWiimote coupled with theWii 
MotionPlus attachment. Infact, for the linear classi.er, the recognizer obtained signi.cantly higher 
accuracyusing theWii MotionPlus attachment in the5training sample/20 gesture case(t16 = -2.32,p < 0.05)and 
the 10 training sample/23 gesture case (t16 = -2.18,p < 0.05). 6.6.2 User Independent Recognition Results 
Figure 43: The average recognition results for the user independent experiments over a varying number 
ofgestures.Thegoalofthisexperimentwastorecognize,withhigh accuracy,asmanygesturesaspossible when given 
different levels of training. The results are grouped by the number of user training samples (100,200, 
or 300) pergesture used for training within anexperiment. These numbers are analogous to using5,10,and15 
users datafortraining.Asingle user independentexperiment utilizedtwo classi.ers (linear and AdaBoost), 
each executing with either the Wiimote or Wiimote + MotionPlus input device producing four accuracy values. 
For the user independent recognition experiments, a subset of the 17 user study gesture data .les was 
used for training. From the remaining .les, recognition is performed and reported on a per user basis. 
This approach is equivalent to having the recognizers trained on a gesture database and having new users 
simply walk up and use the system. The bene.t of this approach is there is no pre-de.ned training needed 
for each user at a potential cost in recognition accuracy. We ran three tests in this experiment, using 
data from 5, 10, and 15 users from our gesture database for training with the remaining user data for 
testing. As in the user dependent study, each scenario was executed on all four of the classi.ers mentioned 
earlier. Note that for the independent recognition results, we did not perform anystatistical analysis 
on the data because we did not have an equal number of samples for each condition and the sample size 
for the 15 user case as two small (only two test samples). An approach to dealing with this issue is 
to perform cross validation on the data so proper statistical comparisons can be made. The results, shown 
in Figure 43, shows that the linear classi.er outperforms AdaBoost by at least 3% and using theWii MotionPlus 
attachment improved recognition for only the linear classi.er. However, unlike in the user dependent 
tests, theWii MotionPlus slightly hindered AdaBoost accuracy. After removing the poorly classi.ed gestures, 
we followed the reintroduction method we used for the user dependent tests. The linear classi.er was 
able to recognize a total of 9, 10, and 13 gestures with mean accuracy values of 95.6%, 97.6%, and 98.3% 
respectively using theWiimote coupled with theWii MotionPlus attachment. The gestures enabled for the5 
user training set includedForward, Stop, Open Door,Parry, Chop, Circle, LinetoRight,LineUpandStab. Whenusingthe10 
user trainingset,theTwisterandSquare gestures were added while maintaining slightly higher accuracylevels. 
On the other hand, the gesture Open Door was removed because the newly introduced training data increased 
confusion among samples of that type. Finally, for the 15 user training set, the gestures Open Door (again), 
In.nity, Zorro and Line Down were addedbut theTwister gesturewas removed. 6.7 Discussion From theseexperiments, 
we can see that253D gestures canbe recognized using theWiimote coupled withtheWii MotionPlus attachmentatover99% 
accuracyinthe user dependent caseusing15 training samples per gesture. This result signi.cantly improves 
upon the results in the existing literature in terms ofthetotalnumberofgesturesthatcanbe accuratelyrecognizedusingaspatiallyconvenientinputdevice. 
There is, of course, a tradeoffbetween accuracyand the amount of time needed to enter training samples 
in the user dependent case. 15 training samples per gesture might be too time consuming for a particular 
application. As an alternative, the results for5training samples per gesture only showsa small accuracy 
degradation.For the user independent case, we can see the accuracyimproves and the numberof gestures 
that can reliably be recognized also increases as the number of training samples increases. Although 
the overall recognition accuracyand the number of gestures was higher in the dependent case (as expected), 
the independent recognizer still provides strong accuracyresults. Comparing the two classi.ers in the 
experiments shows the linear classi.er consistently outperforms the AdaBoost classi.er. This is somewhat 
counterintuitive given the AdaBoost classi.er is a more sophisticated technique. However, as we discussed 
in Section 6.3, the linear classi.er can suffer from the singular matrix problem which can limit its 
utility when new features that could improve accuracy are added. With theWii MotionPlus, we also tried 
to adjust for gyroscopic drift in data. Our attempt at calibration involved setting theWiimote coupled 
with the MotionPlusdevice ona table with thebuttonsdown for approximately10 seconds, followedby storingasnapshotoftheroll,pitchandyawvalues. 
That snapshot was then used as a point of reference for future collection points. However, using those 
offsets caused decreased accuracy in AdaBoost and singular matrices in the linear classi.er leading to 
the use of raw gyroscope data instead.  Acknowledgements A special thanks to Doug Bowman, Ernst Kruijff, 
Ivan Poupyrev, ChadWingrave, and members of the Interactive Systems and User Experience Lab for assisting 
in the development of the material for this course. References AZUMA,R., AND BISHOP,G. 1994. Improving 
static and dynamic registrationin an optical see-through hmd. In SIGGRAPH 94: Proceedings of the 21st 
annual conference on Computer graphics and interactive techniques,ACM,NewYork,NY, USA, 197 204. BOTT, 
J., CROWLEY, J., AND LAVIOLA, J. 2009. Exploring 3d gestural interfaces for music creation in videogames. 
In Proceedings of TheFourth International Conference on theFoundations of Digital Games 2009, 18 25. 
BOWMAN,D.A., AND HODGES,L.F. 1997.Anevaluationof techniquesfor grabbingand manipulating remote objects 
in immersive virtual environments. In SI3D 97: Proceedings of the 1997 symposium on Interactive 3D graphics,ACM,NewYork,NY, 
USA, 35 ff. BOWMAN,D.A.,KOLLER,D., AND HODGES,L.F. 1997.Travelin immersive virtualenvironments: An evaluation 
of viewpoint motion control techniques. In Proceedings of theVirtual Reality Annual International Symposium, 
45 52. BOWMAN,D.A.,WINEMAN,J.,HODGES,L.F., AND ALLISON,D. 1998. Designing animal habitats within an immersive 
ve. IEEE Comput. Graph. Appl. 18, 5, 9 13. BOWMAN,D.A.,KRUIJFF,E.,LAVIOLA,J.J., AND POUPYREV,I. 2004. 
3D User Interfaces: Theory and Practice. AddisonWesleyLongman Publishing Co., Inc., Redwood City, CA, 
USA. CHARBONNEAU, E., MILLER, A., WINGRAVE, C., AND LAVIOLA, JR., J. J. 2009. Understanding visual interfacesforthenext 
generationof dance-basedrhythm videogames.In Sandbox 09: Proceedingsof the 2009ACM SIGGRAPH Symposium 
onVideo Games,ACM Press, 119 126. CONNER, B. D., SNIBBE, S. S., HERNDON, K. P., ROBBINS, D. C., ZELEZNIK, 
R. C., AND VAN DAM, A. 1992. Three-dimensional widgets. In SI3D 92: Proceedings of the 1992 symposium 
on Interactive 3D graphics,ACM,NewYork,NY, USA, 183 188. CRASSIDIS, J. L., AND MARKLEY, F. L. 2003. Unscented 
.ltering for spacecraft attitude estimation. Journal of Guidance, Control, and Dynamics 26, 4, 536 542. 
DUDA,R.O.,HART,P.E., AND STORK,D.G. 2001. Pattern Classi.cation. JohnWileyand Sons. FEINER, S., MACINTYRE, 
B., HAUPT, M., AND SOLOMON, E. 1993. Windows on the world: 2d windows for 3d augmented reality. In UIST 
93: Proceedings of the 6th annualACM symposium on User interface software and technology,ACM,NewYork,NY, 
USA, 145 155. FRIEDMAN,J.H. 1996. Another approachto polychotomous classi.cation. Tech. rep., Departmentof 
Statistics, Stanford University. GIANSANTI,D.,MACELLARI,V., AND MACCIONI,G. 2003.Isit feasibleto reconstructbodysegment 
3-d position and orientation using accelerometric data. IEEETrans. Biomed.Eng50, 2003. GUNTURK,B.K.,GLOTZBACH,J.,ALTUNBASAK,Y., 
AND SCHAFER,R.W. 2005. Demosaicking: color .lter array interpolation. IEEE Signal processing magazine 
22, 44 54. HINCKLEY, K., PAUSCH, R., GOBLE, J. C., AND KASSELL, N. F. 1994. A surveyof design issues 
in spatial input. In Proc.ACM UIST 94 Symposium on User Interface Software&#38;Technology,ACM, 213 222. 
HOFFMAN,M.,VARCHOLIK,P., AND LAVIOLA,J. 2010. Breakingthe statusquo: Improving3d gesture recognition 
with spatially convenient input devices. In IEEEVirtual Reality 2010, IEEE Press, 59 66. INTEL. 1999.Video 
as input(vai) white paper.Tech. rep., Intel Architecture Labs. JULIER,S., AND UHLMANN,J. 1997.A newextensionofthe 
kalman .lterto nonlinear systems.In Int. Symp. Aerospace/Defense Sensing, Simul. and Controls. KATO,H., 
AND BILLINGHURST,M. 1999. Marker tracking and hmd calibration fora video-based augmented reality conferencing 
system. In IWAR99:Proceedingsofthe2nd IEEEandACM International Workshop onAugmented Reality, 85. KATZOURIN,M.,IGNATOFF,D.,QUIRK,L.,LAVIOLA,J., 
AND JENKINS,O.C. 2006. Swordplay: Innovatinggamedevelopment through vr. IEEE Computer Graphics and Applications 
26, 6, 15 19. KRATZ,L.,SMITH,M., AND LEE,F.J. 2007.Wiizards:3d gesture recognitionforgameplay input. 
In Future Play 07: Proceedings of the 2007 conference on Future Play,ACM, NewYork, NY, USA, 209 212. 
LAVIOLA,J.J., AND ZELEZNIK,R.C. 2007.Apractical approach for writer-dependent symbol recognition usingawriter-independent 
symbol recognizer. IEEETransactions onPattern Analysis and Machine Intelligence 29, 11, 1917 1926. LAVIOLA,J. 
2000. Msvt:Avirtual reality-based multimodal scienti.c visualization tool. InProceedings of the ThirdIASTED 
International Conference on Computer Graphics and Imaging, 1 7. LAVIOLA, J. J. 2003. Double exponential 
smoothing: an alternative to kalman .lter-based predictive tracking. In EGVE 03:Proceedingsofthe workshoponVirtualenvironments2003,ACM, 
NewYork, NY, USA, 199 206. LEE,J. 2008. Hacking the nintendo wii remote. Pervasive Computing, IEEE7,3(July-Sept.), 
39 45. LUINGE, H., VELTINK, P., AND BATEN, C. 1999. Estimating orientation with gyroscopes and accelerometers. 
Technology and Health Care7, 6, 455 459. MAPES,D., AND MOSHELL,M. 1995.Atwo-handed interfacefor object 
manipulationin virtualenvironments. Presence:Teleoper.Virtual Environ.4, 4, 403 416. MARKS,R.,SCOVILL,T., 
AND MICHAUD-WIDEMAN,C. 2001. Enhanced reality:A new frontier for computer entertainment. In SIGGRAPH 
2001: ConferenceAbstracts and Applications,ACM, 117. MARKS, R., DESHPANDE, R., KOKKEVIS, V., LARSEN, 
E., MICHAUD-WIDEMAN, C., AND SARGAI-SON,S. 2003. Real-time motion capture for interactive entertainment. 
In SIGGRAPH 2003: Emerging Technologies,ACM. MARKS,R. 2000. Medieval chamber. In SIGGRAPH 2000: EmergingTechnologies,ACM, 
etech58. MINE, M. R., BROOKS, JR., F. P., AND SEQUIN, C. H. 1997. Moving objects in space: exploiting 
proprioception in virtual-environment interaction. In SIGGRAPH 97: Proceedings of the 24th annual conference 
on Computer graphics and interactive techniques,ACM Press/Addison-WesleyPublishing Co.,NewYork,NY, USA, 
19 26. MINE,M. 1995.Virtual environment interaction techniques.Tech. rep., UNC Chapel HillCS Dept. PAUSCH,R.,BURNETTE,T.,BROCKWAY,D., 
AND WEIBLEN,M.E. 1995.Navigationand locomotion in virtual worlds via .ight into hand-held miniatures. 
In SIGGRAPH 95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques,ACM,NewYork,NY,USA, 
399 400. PIERCE, J. S., FORSBERG, A. S., CONWAY, M. J., HONG, S., ZELEZNIK, R. C., AND MINE, M.R. 1997. 
Image plane interaction techniques in 3d immersive environments. In SI3D 97: Proceedings of the 1997 
symposium on Interactive 3D graphics,ACM,NewYork,NY, USA, 39 ff. POUPYREV,I.,BILLINGHURST,M.,WEGHORST,S., 
AND ICHIKAWA,T. 1996. The go-go interaction technique: non-linear mapping for direct manipulation in 
vr. In UIST 96: Proceedings of the 9th annual ACM symposium on User interface software and technology,ACM,NewYork,NY, 
USA, 79 80. REHM,M.,BEE,N., AND ANDRE,E. 2008.Wave like anegyptian: accelerometer based gesture recognition 
for culture speci.c interactions. In BCS-HCI 08: Proceedings of the 22nd British HCI Group Annual Conference 
on HCI 2008, British Computer Society, Swinton, UK, UK, 13 22. REKIMOTO,J., AND AYATSUKA,Y. 2000. Cybercode: 
designing augmented reality environments with visual tags. In DARE 2000:Proceedingsofthe2000ACM Conferenceon 
DesigningAugmented Reality Environments,ACM, 1 10. RUBINE,D. 1991. Specifying gesturesbyexample. In SIGGRAPH 
91: Proceedings of the 18th annual conference on Computer graphics and interactive techniques,ACM,NewYork,NY, 
USA, 329 337. SCHAPIRE,R.E. 1999.Abrief introductionto boosting.InIJCAI 99: Proceedings of the Sixteenth 
InternationalJoint Conference on Arti.cial Intelligence, Morgan Kaufmann Publishers Inc., San Francisco, 
CA, USA, 1401 1406. SCHL Gesture recognition with a wii OMER, T., POPPINGA, B., HENZE, N., AND BOLL, 
S. 2008. controller. In TEI 08: Proceedings of the 2nd international conference onTangible and embedded 
interaction,ACM,NewYork,NY, USA, 11 14. SHIRAI,A.,GESLIN,E., AND RICHIR,S. 2007.Wiimedia: motion analysis 
methods and applications usinga consumer videogame controller. In Sandbox 07: Proceedings of the 2007ACM 
SIGGRAPH symposium onVideo games,ACM,NewYork,NY, USA, 133 140. SHIRATORI, T., AND HODGINS, J. K. 2008. 
Accelerometer-based user interfaces for the control of a physically simulated character. ACMTrans. Graph. 
27, 5, 1 9. SHIVARAM, G., AND SEETHARAMAN, G. 1998. A new technique for .nding the optical center of 
cameras. In ICIP 98: Proceedings of 1998 International Conference on Image Processing, vol. 2, 167 171. 
SILVA,M.G., AND BOWMAN,D.A. 2009. Body-based interaction for desktopgames. In CHI EA 09: Proceedings 
of the 27th international conference extended abstracts on Human factors in computing systems,ACM,NewYork,NY, 
USA, 4249 4254. STOAKLEY,R.,CONWAY,M.J., AND PAUSCH,R. 1995.Virtual reality ona wim: interactiveworldsin 
miniature. In CHI 95: Proceedings of the SIGCHI conference on Human factors in computing systems, ACM 
Press/Addison-WesleyPublishing Co.,NewYork,NY, USA, 265 272. WAN, E. A., AND VAN DER MERWE, R. 2002. 
The unscented kalman .lter for nonlinear estimation. 153 158. WILLIAMSON,R., AND ANDREWS,B. 2001. Detecting 
absolutehumankneeangleandangularvelocity using accelerometers and rate gyroscopes. Medical and Biological 
Engineering and Computing 39, 3, 294 302. WILLIAMSON,B.,WINGRAVE,C., AND LAVIOLA,J. 2010. Realnav: Exploring 
natural user interfaces for locomotionin videogames. In Proceedings of IEEE Symposium on 3D User Interfaces 
2010, IEEE Computer Society,3 10. WINGRAVE, C., WILLIAMSON, B., VARCHOLIK, P., ROSE, J., MILLER, A., 
CHARBONNEAU, E., BOTT, J., AND LAVIOLA, J. 2010. Wii remote and beyond: Using spatially convenient devices 
for 3duis. IEEE Computer Graphics and Applications 30, 2, 71 85. WLOKA, M. M., AND GREENFIELD, E. 1995. 
The virtual tricorder: a uniform interface for virtual reality. In UIST 95: Proceedings of the 8th annualACM 
symposium on User interface and software technology,ACM,NewYork,NY, USA, 39 40. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837104</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>73</pages>
		<display_no>3</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Biomedical applications]]></title>
		<subtitle><![CDATA[what you need to know]]></subtitle>
		<page_from>1</page_from>
		<page_to>73</page_to>
		<doi_number>10.1145/1837101.1837104</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837104</url>
		<abstract>
			<par><![CDATA[<p>The goal of this course is to introduce you to the wealth of challenges and problems that exist in the biomedical and biology domains -- specifically, how knowledge of the computer graphics areas of shape, pattern, and animation could be used to address these problems. We'll start with a general background on where these problems come from, then move on to two specific examples.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264992</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837105</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>106</pages>
		<display_no>4</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Build your own 3D display]]></title>
		<page_from>1</page_from>
		<page_to>106</page_to>
		<doi_number>10.1145/1837101.1837105</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837105</url>
		<abstract>
			<par><![CDATA[<p>Commercial stereoscopic displays have reemerged in the consumer market within the last few years. Today, film studios routinely produce live-action and animated 3D content for theatrical release. While primarily enabled by the widespread adoption of digital projection, allowing accurate view synchronization, the underlying 3D display technologies have changed little in the last few decades. Theatrical systems rely on <i>stereoscopic display</i>: projecting a unique left/right image pair that is separated by various filters placed within a pair of glasses worn by each viewer. In contrast, several LCD manufactures are introducing <i>automultiscopic displays</i>: allowing view-dependent imagery to be perceived without the aide of special glasses. Given these developments, 3D display is poised for another resurgence.</p> <p>This course is a hands-on introduction to 3D display and provides attendees with the mathematics, software, and practical details necessary to build their own low-cost stereoscopic displays. An example-driven approach is used throughout, with each new concept illustrated using a practical 3D display implemented with off-the-shelf parts. First, glasses-bound stereoscopic displays are explained. Detailed plans are provided for attendees to construct their own LCD shutter glasses. Next, unencumbered automultiscopic displays are explained, including step-by-step directions to construct lenticular and parallax barrier designs using modified LCDs. All the necessary software, including algorithms for rendering and calibration, is provided for each example, allowing attendees to quickly construct 3D displays for their own educational, amusement, and research purposes.</p> <p>The course concludes by describing various methods for capturing, rendering, and viewing various multi-view imagery sources. Stereoscopic OpenGL support is reviewed, as well as methods for ray-tracing multiview imagery with POV-Ray. Techniques for capturing "live-action" light fields are also outlined. Finally, recent developments are summarized, inspiring attendees to evolve the capabilities of their self-built 3D displays.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264993</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264994</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Build Your Own 3D Display  SIGGRAPH 2010 Course Noutes Sunday, July 25, 2010 Matthew Hirsch Douglas 
Lanman MIT Media Lab Brown University mhirsch@media.mit.edu dlanman@brown.edu  Abstract Commercial stereoscopic 
displays have reemerged in the consumer market within the last few years. Today, .lm studios routinely 
produce live-action and animated 3D content for theatrical release. While primarily enabled by the widespread 
adoption of digital projection, allowing accurate view synchronization, the underlying 3D display technologies 
have changed little in the last few decades. Theatrical systems rely on stereoscopic display: projecting 
a unique left/right image pair that is separated by various .lters placed within a pair of glasses worn 
by each viewer. In contrast, several LCD manufactures are introducing automultiscopic displays: allowing 
viewdependent imagery to be perceived without the aide of special glasses. Given these developments, 
3D display is poised for another resurgence. This course is a hands-on introduction to 3D display and 
provides attendees with the mathematics, software, and practical details necessary to build their own 
low-cost stereoscopic displays. An example-driven approach is used throughout, with each new concept 
illustrated using a practical 3D display implemented with off-the-shelf parts. First, glasses-bound 
stereoscopic displays are explained. Detailed plans are provided for attendees to construct their own 
LCD shutter glasses. Next, unencumbered automultiscopic displays are explained, including step-by-step 
directions to construct lenticular and parallax barrier designs using modi.ed LCDs. All the necessary 
software, including algorithms for rendering and calibration, is provided for each example, allowing 
attendees to quickly construct 3D displays for their own educational, amusement, and research purposes. 
The course concludes by describing various methods for capturing, rendering, and viewing various multi-view 
imagery sources. Stereoscopic OpenGL support is reviewed, as well as methods for ray-tracing multiview 
imagery with POV-Ray. Techniques for capturing live-action light .elds are also outlined. Finally, recent 
developments are summarized, inspiring attendees to evolve the capabilities of their self-built 3D displays. 
 Prerequisites Attendees should have hobbyist-level experience with do-it-yourself electronic projects. 
While executables are provided for beginners, attendees with prior knowledge of Matlab, C/C++, and OpenGL 
will be able to directly examine and modify the provided source code. i Speaker Biographies Matthew 
Hirsch MIT Media Lab mhirsch@media.mit.edu http://web.media.mit.edu/ mhirsch Matthew Hirsch is a Ph.D. 
student at the MIT Media Lab. His research focuses on imaging devices that enable new understanding and 
interaction scenarios. He works with Henry Holtzman and Ramesh Raskar in the Information Ecology and 
Camera Culture groups, respectively. Matthew graduated from Tufts University in 2004 with a B.S. in Computer 
Engineering. He worked as an Imaging Engineer at Analogic Corp. from 2004 to 2007, where he designed 
threat detection algorithms for computed tomography security scanners. Douglas Lanman Brown University 
dlanman@brown.edu http://mesh.brown.edu/dlanman Douglas Lanman is a .fth-year Ph.D. student at Brown 
University. His research is focused on computational photography, particularly in the use of active illumination 
for 3D reconstruction and light .eld capture and display. He received a B.S. in Applied Physics with 
Honors from Caltech in 2002 and a M.S. in Electrical Engineering from Brown University in 2006. Prior 
to joining Brown, he was an Assistant Research Staff Member at MIT Lincoln Laboratory from 2002 to 2005. 
Douglas has worked as an intern at Intel, Los Alamos National Laboratory, INRIA Rh one-Alpes, Mitsubishi 
Electric Research Laboratories (MERL), and the MIT Media Lab. He previously presented the Build Your 
Own 3D Scanner course at SIGGRAPH 2009 and SIGGRAPH Asia 2009. ii  Course Outline 10 minutes: Introduction: 
History and Physiology Matthew Hirsch This part will overview the course, review historical developments, 
and describe the physiology of depth perception. Random dot stereograms will be introduced to illustrate 
binocular disparity cues. Perceptual concerns, such as the hyperfocal distance and accommodation, will 
be described before the comprehensive review of display technologies in the following part. 15 minutes: 
Representation and Display Douglas Lanman This part will .rst describe how light .elds model the set 
of rays emitted by 3D displays. Afterward, a 3D display taxonomy will be presented, emphasizing the division 
between glasses-bound stereoscopic displays and unencumbered automultiscopic displays. 20 minutes: 
Glasses-bound Stereoscopic Displays Matthew Hirsch This part will describe stereoscopic display methods 
that require the use of special eyewear, including: color .lters (anaglyph images)  linear/circular 
polarizers (including dual-projectors)  LCD shutter glasses (temporal multiplexing)  head-mounted displays 
 Detailed plans, including a parts list and circuit schematics, will be provided so attendees can build 
their own LCD shutter glasses. 20 minutes: Unencumbered Automultiscopic Displays Douglas Lanman This 
part will describe automultiscopic display methods that do not require special eyewear, including: integral 
imaging with lenticular and .y s eye arrays  static and dynamic parallax barriers  iii The projects 
in this part involve modifying off-the-shelf LCDs to implement various 3D displays. Practical construction 
details and calibration and rendering software will be provided for each device. The procedure to construct 
a static parallax barrier, using a single LCD and a printed mask, will be described .rst. Next, an improved 
dynamic parallax barrier will be described. This project will require disassembling an LCD so that the 
spatial light modulator can be used separately from the backlight. Attendees will learn how to construct 
a dual-stacked LCD, capable of producing left/right image pairs for viewing with linearly-polarized glasses. 
Finally, an improved dynamic parallax barrier, where the barrier is shifted over time, will be built 
using the dual-stacked LCD. 10 minutes: Source Material: Rendering and Capture Matthew Hirsch This part 
will describe how to generate multi-view content (i.e., light .elds) for use with the displays constructed 
in prior parts. Topics to be covered, include: light .eld capture (e.g., translated cameras and stereo 
rigs)  stereoscopic support in OpenGL  light .eld rendering (e.g., modi.cation to POV-Ray)  commercial 
and open source materials (e.g., 3D Blu-ray)  Software details, such as rendering parallax barrier base 
images, will be covered in earlier parts; this part will emphasize general methods for acquiring multi-view 
imagery. 10 minutes: Emerging Technology Douglas Lanman This part will review recent commercial and 
academic methods, including those from prior SIGGRAPH and SIGGRAPH Asia proceedings. 5 minutes: Q &#38; 
A Matthew Hirsch and Douglas Lanman Attendees will be invited to view the prototype hardware. iv  Introduction: 
History and Physiology R R epresent t at i ti on and D i d Di spl l ay  Glassesbound Stereoscopic Displays 
 Glasses--bound Stereoscopic Displays  Unencumbered A utomultiscopic Displays  Unencumbered Automultiscopic 
Displays  Source Material: Rendering and Capture  Emer g in g Technolo gy  gg g y  Conclusion 
and Q &#38; A    Goals of the Course  Understand how the human visual system (HVS) perceives depth 
i n t he world ( and how  (HVS) perceives depth in the world (and how to trick it)  Understand light 
field representation and application to 3D display   Be able to use and modify offthe--shelfBe able 
to use and modify off--theshelf hardware t o perform optics e xperiments and hardware to perform optics 
experiments and create 3D display devices  Introduction: History and Physiology Ph Ph ysi i o ll og 
y  History  Representation and Display  Glassesbound Stereoscopic Displays Glasses--bound Stereoscopic 
Displays  Unencumbered Automultiscopic Displays S M i l R d i d C  S ource M a t er i a l : R en 
d er i ng an d C ap t ure  Emerging Technology Conclusion and Q &#38; A  Conclusion and Q &#38; 
A  Monocular Cues  Motion Parallax  Motion Parallax  Perspective  Relative Size  A tmos p 
pheric Effects  Occlusion Texture Texture  Shading/Lighting    Also sometimes called psychological 
cues, these effects can be captured by a monocular camera and perceived by a viewer of a traditional 
screen.  These effects are due to Another Monocular Cue physical functions of the eye, and not interpretation 
fth of the scene Accommodation  Accommodation   Binocular Cues  Stereopsis  Convergence  Convergence 
  The HVS can ignore conflicting or missing depth c ues  depth cues  Understand depth in 2D (monocular) 
video  Perceive shape in noise   magiceye.com Ponzo Illusion: &#38;#169; Walt Anthony 2006  Ponzo 
Illusion Illusion W W e percei i ve d ept h th w ith out b i t bi nocu ll ar cues d i t h  Depth perception 
chan g ges perception of line length  Random Dot Stereograms   Random Dot Stereograms Bela Julesz 
  All presented displays do this P i bl l d t f ti ti  P oss ibl y l ea d s t o f a ti gue over ti 
me    Introduction: History and Physiology Ph i Ph ys i o ll ogy  History  Representation and 
Display  Glassesbound Stereoscopic Displays Glasses--bound Stereoscopic Displays  Unencumbered Automultiscopic 
Displays S M i l R d i d C  S ource M a t er i a l : R en d er i ng an d C ap t ure  Emerging Technology 
 Conclusion and Q &#38; A  1838 Wheatstone Stereoscopes  1848 Brewseter  1848 Brewseter  1881 
Popularized by Oliver Wendell Holmes   1853 Earliest Anaglyph photographs  Conclusion and Q &#38; 
A  http://courses.ncssm.edu/gallery/collections/toys/html/exhibit01.htm 1939-Today Equivalent  1908 
Lippmann, Integral Imaging, Lenticular Printing  1908 Lippmann, Integral Imaging, Lenticular  1934 
Polarizing Glasses ( 2 synchronized p rojectors)  1934 Polarizing Glasses (2 synchronized projectors) 
  1950s Anaglyph and polarizing glasses popular to counter rise of television  counter rise of television 
  Introduction: History and Physiology  R t t i d D i l  R epresen t a ti on an d Di sp l a y 
 Glassesbound Stereoscopic Displays Glasses--bound Stereoscopic Displays  Unencumbered A utomultiscopic 
Displays  Unencumbered Automultiscopic Displays  Source Material: Rendering and Capture  Emer g 
in g Technolo gy  gg g y  Conclusion and Q &#38; A   Two Plane Parameterization Position-Angle 
Parameterization Light Fields [Levoy and Hanrahan 1996, Gortler et al. 1996] Radiance along any direction 
at every point in 3D space  Light fields are 4D (e.g., two plane vs. position-angle parameterization) 
  Light Fields [Levoy and Hanrahan 1996, Gortler et al. 1996]  Radiance along any direction at every 
point in 3D space  For a planar 3D display, the emitted light field is a 4D function  Parameterized 
using absolute or relative two plane parameterization   viewer moves right   viewer moves right 
 b b  Example: Parallax Barriers (u,s) s parallax barrier u LCD Light Fields [Levoy and Hanrahan 
1996 Gortler et al 1996] LightFields[LevoyandHanrahan 1996, Gortleretal. 1996] Radiance along any direction 
at every point in 3D space  For a planar 3D display,y the emitted ligght field is a 4D function pp, 
 Parameterized using absolute or relative two plane parameterization  Represents multiple views projected 
by parallax barrier displays   Light Fields [Levoy and Hanrahan 1996 Gortler et al 1996] Light Fields 
[Levoy and Hanrahan 1996, Gortler et al. 1996]  Radiance along any direction at every point in 3D space 
 For a ggeneral pplanar 3D dispplay,y, the emitted ligght field is a 4D function  Light field is parameterized 
using intersection with two planes  Represents multiple views projected by volumetric displays   Immersive 
(blocks direct-viewing of real world) Head-mounted (eyepiece-objective and microdisplay) See-through 
(superimposes synthetic images onto real world) (superimposes synthetic images onto real world) Glasses-bound 
Stereoscopic  Spatially-multiplexed (field-concurrent) (color filters, polarizers, autostereograms, 
etc.) Multiplexed (t(stereo paiir with same di display surfface))ith l Temporally-multiplexed (field-sequential) 
(LCD shutter glasses) Parallax Barriers Parallax Barriers (uniform array of 1D slits or 2D pinhole arrays) 
Parallax-based (2D display with light-directing elements) Integral Imaging (lenticular sheets or fly 
s eye lenslet arrays) Multi planar Multi-planar (time-sequential projection onto swept surfaces) Unencumbered 
Volumetric (directly illuminate points within a volume) Automultiscopic Transparent Substrates (intersecting 
laser beams, fog layers, etc.) St ti Static (holographic films) Holographic (reconstructs wavefront using 
2D element) Dynamic(holovideo) Taxonomy adapted from Hong Hua  Immersive (blocks direct-viewing of 
real world) Head-mounted (eyepiece-objective and microdisplay) Glasses-bound Stereoscopic Multiplexed 
(stereo pair with same display surface)   Immersive (blocks direct-viewing of real world) Head-mounted 
(eyepiece-objective and microdisplay) See-through (superimposes synthetic images onto real world) Glasses-bound 
Stereoscopic Multiplexed (stereo pair with same display surface)  (eyepiece-objective and microdisplay) 
 See-through (superimposes synthetic images onto real world) Glasses-bound Stereoscopic Sppatiallyy-multipplexed 
((field-concurrent)) (color filters, polarizers, autostereograms, etc.) Multiplexed (stereo pair with 
same display surface)   Immersive (blocks direct-viewing of real world) Head-mounted (eyepiece-objective 
and microdisplay) See-through (superimposes synthetic images onto real world) Glasses-bound Stereoscopic 
 Sppatiallyy-multipplexed ((field-concurrent)) (color filters, polarizers, autostereograms, etc.) Multiplexed 
(stereo pair with same display surface) Temporally-multiplexed (field-sequential) (LCD shutter glasses) 
 (1920x1080, 1x8 views) Parallax Barriers (uniform array of 1D slits or 2D pinhole arrays) Parallax-based 
 (2D display with light-directing elements) Unencumbered Volumetric (directly illuminate points within 
a volume) Automultiscopic Holographic (reconstructs wavefront using 2D element)  Alioscopy 3DHD 42'' 
(1920x1200, 1x8 views) (1920x1200, 1x8 views) Parallax Barriers (uniform array of 1D slits or 2D pinhole 
arrays) Parallax-based (2D display with light-directing elements) Integral Imaging (lenticular sheets 
or fly s eye lenslet arrays) Unencumbered Volumetric (directly illuminate points within a volume) Automultiscopic 
Holographic (reconstructs wavefront using 2D element)  Parallax Barriers (uniform array of 1D slits 
or 2D pinhole arrays) Parallax-based (2D display with light-directing elements) Integral Imaging (lenticular 
sheets or fly s eye lenslet arrays)  Multi-planar (time-sequential projection onto swept surfaces) Unencumbered 
Volumetric (directly illuminate points within a volume) Automultiscopic Holographic (reconstructs wavefront 
using 2D element)  Parallax Barriers (uniform array of 1D slits or 2D pinhole arrays) Parallax-based 
 (2D display with light-directing elements) Integral Imaging (lenticular sheets or fly s eye lenslet 
arrays)  Multi-planar (time-sequential projection onto swept surfaces) Unencumbered Volumetric (directly 
illuminate points within a volume) Automultiscopic Transparent Substrates (intersecting laser beams, 
fog layers, etc.) Holographic (reconstructs wavefront using 2D element)  capture reconstruction Parallax 
Barriers (uniform array of 1D slits or 2D pinhole arrays) Parallax-based (2D display with light-directing 
elements) Integral Imaging (lenticular sheets or fly s eye lenslet arrays)  Multi-planar (time-sequential 
projection onto swept surfaces) Unencumbered Volumetric (directly illuminate points within a volume) 
Automultiscopic Transparent Substrates (intersecting laser beams, fog layers, etc.) Static (holographic 
films) Holographic (reconstructs wavefront using 2D element)  Tay et al. MIT Media Lab Spatial Imaging 
Group [Nature 2008] [Holovideo 1989 present]present] [Nature, 2008] [Holovideo, 1989 Parallax Barriers 
(uniform array of 1D slits or 2D pinhole arrays) Parallax-based (2D display with light-directing elements) 
Integral Imaging (lenticular sheets or fly s eye lenslet arrays)  Multi-planar (time-sequential projection 
onto swept surfaces) Unencumbered Volumetric (directly illuminate points within a volume) Automultiscopic 
Transparent Substrates (intersecting laser beams, fog layers, etc.) Static (holographic films) Holographic 
(reconstructs wavefront using 2D element) Dynamic(holovideo)  Introduction: History and Physiology 
R R epresent t at i ti on and D i d Di spl l ay  Glassesbound Stereoscopic Displays  Glasses--bound 
Stereoscopic Displays Unencumbered A utomultiscopic Displays  Unencumbered Automultiscopic Displays 
  Source Material: Rendering and Capture  Emer g in g Technolo gy  gg g y  Conclusion and Q &#38; 
A  Goal: Provide different images to each eye by modifying t he scene at each e ye  by modifying the 
scene at each eye  Methods: Forms of glasses   Active  Shutters  LCD Screen   Passive Passive 
 Polarizers Color F ilters Color Filters    Introduction: History and Physiology R R epresent t 
at i ti on and D i d Di spl l ay  Glassesbound Stereoscopic Displays  Glasses--bound Stereoscopic Displays 
 Overview of Common G lasses Designs  Overview of Common Glasses Designs  BYO Shutter Glasses  
 Unencumbered Automultiscopic Displays S M i l R d i d C  S ource M a t er i a l : R en d er i ng an 
d C ap t ure  Emerging Technology Conclusion and Q &#38; A  Conclusion and Q &#38; A  Use passive 
optical filters to modify an image produced by another device produced by another device  Cheap glasses, 
expensive display device  11--to-N where N is largeto-N where N is large Theaters ( Movie/Omni) Theaters 
(Movie/Omni)   Anaglyph (usually red/blue) glasses an early type  Separate i mages in wavelength 
w ith notch filters  Separate images in wavelength with notch filters   Unpleasant color e ffect 
 Unpleasant color effect    Apollo17: VIP Site Anaglyph Apollo17: VIP Site Anaglyph Credit: Gene Cernan, 
Apollo 17, NASA Anaglyph by Erik van Meijgaarden  Comb Filters: lots of notch filters across the  
Solve the unpleasant colored image effect C b d i t h l h i t  C an b e use d w ith norma l w hit e 
screens  Basic configuration     Poincure sphere representation representation  Polarization 
mapped to volume of a unit s phere volume of a unit sphere  Linear states at the equator  Circular 
states at the poles  Orthogonal states Orthogonal states diametrically opposite  ( x , y , z ) point 
in sphere are 3 normalized Stokes parameters  ( xyz ) point in sphere a re 3 http://www.hpl.hp.com/hpjournal/95feb/feb95a4b.pdf 
 Circular allows viewer to rotate head  RealD currently use circular polarizers   Real--D currently 
use circular polarizers Linear polarization: Circular polarization: Orientation Independent of dependant 
orientation  A linear polarizer is a set A circular polarizer is a of conductors that short linear polarizer 
followed by out the electric field in one a quarter wave plate direction  Use electronically controlled 
spatial light modulators t o m odify or produce i mages of a  modulators to modify or produce images 
of a scene  Expensive glasses, cheap (or no) accompanying display  Popular for personal Popular 
for personal viewing  Simplest idea: put screens right in front of the e ye  the eye  Optics to 
make the screens visible  Original immersive VR headgear Original immersive VR headgear  Expensive 
and obscure other vision     Single LCD pixel in front of each eye   eye   LCD N N ormal l 
anti ti -para-parallll e ll con fif i gura tii on  http://www.liquidcrystaltechnologies.com/tech_support/Pi_Cell.html 
  Introduction: History and Physiology R R epresent t at i ti on and D i d Di spl l ay  Glassesbound 
Stereoscopic Displays  Glasses--bound Stereoscopic Displays Overview of Common G lasses D esigns Overview 
of Common Glasses Designs  BYO Shutter Glasses  Unencumbered Automultiscopic Displays S M i l R d i 
d C  S ource M a t er i a l : R en d er i ng an d C ap t ure  Emerging Technology Conclusion and 
Q &#38; A  Components Overview  Obt Obt ai i n ii ng par t s  LCD Shutters  Controller board design 
  Conclusion and Q &#38; A  Fb i F a b r i ca tii ng thh e b b oard d  Makin g g the g glasses  
Syncing to a source  Electronics Mi l l l  Mi crocon t ro ll er, componen t s f f or power supp l 
y  LCD Shutters  Liquid Crystal Technologies Frame Frame   Acrylic   Electronics  Digikey 
Parts List: $8.36 Parts List: $8.36 Part Description Digikey Part Number Qty Cost Microcontroller PIC16F506-I/SL-ND 
1 0.90 N-Channel MOSFET FDS6630ACT-ND 1 0.68 5v Regulator ZLDO1117G50DICT-ND 1 0.62 2.4v Zener Diode 
FLZ2V4ACT-ND 1 0.33 100uH Inductor PCD2119CT-ND 1 0.86 1uF Capacitor 493-2129-1-ND 1 0.45 40V Schottky 
Diode BAS40-FDICT-ND 1 0.60 10K 1210 Resistor 541-10KVCT-ND 1 0.261 100K 1210 Resistor 541-100KVCT-ND 
2 0.522 SwitchSwitch SW262CT NDSW262CT-ND 11 0900.90 220 0805 Resistor RHM220ARCT-ND 1 0.034 1K 0805 
Resistor 541-1.0KACT-ND 1 0.077 .1uF 0603 Capacitorp 490-3167-1-ND 3 0.032 Header (male) A34253-40-ND 
1 2.09  Electronics  Mouser Parts List: $4.24  Total Cost  Mouser: $4.24  Digikey: $8.36 Digikey: 
$8.36  LCT (next slide): $25 Total: $37 60 + s hipping + t ime Total: $37 . 60 + shipping + time  
  Part Description Mouser Part Number Qty Cost HV508LG LCD Driver 689-HV508LG-G 1 4.24  LCD Shutters 
 Liquid Crystal Technologies  @   sales @ liquidcrystaltechnologies.com to order sample kit  $25 
per sample kit (two shutters)    Frames Can be made from your favorite material  Lasercut acrylic 
 acrylic  Foamcore  Template to be provided  Boost supply generate    Boost supply 35v supply 
for L CD shutter 35v supply for LCD shutter  Current through inductor generates high voltage when switched 
  Charge stored on capacitor  Code provided on our site G G ood f f ree programmer:ree programmer: 
usbpicprog  usbpicprog  gEDA tools:  David Carr s milling machine  makeyourbot.org  Uses a laser 
printer to protect copper from acid (etching)  acid (etching)  http://www.riccibitti.com/pcb/pcb.htm 
       Solder parts   VGA vsync pin connectorVGA vsyncpin connector TTL l l l  TTL l eve l 
si i gna l  Connect pin directly to onboard sync connector   IR tower for nvidia or other glasses 
IR tower for nvidia Will require some extra hardware a ttached to sync Will require some extra hardware 
attached to sync connector  D t d t t lik f i l  D oes no t d e t ec t sense, lik e pro f ess i ona 
l glassesglasses Push button to swap left and right  Push button to swap left and right eyes  Video 
 Video through lens of glasses, demonstrates right/left c hannel selectivity right/left channel selectivity 
  Introduction: History and Physiology R R epresent t at i ti on and D i d Di spl l ay  Glassesbound 
Stereoscopic Displays Glasses--bound Stereoscopic Displays Unencumbered Automultiscopic D isplays Unencumbered 
Automultiscopic Displays  Integral Imaging with Fly s Eye Lenslet Arrays  Static and Dynamic Parallax 
Barriers S M i l R d i d C   S ource M a t er i a l : R en d er i ng an d C ap t ure  Emerging Technology 
 Conclusion and Q &#38; A  Conclusion and Q &#38; A  Parts List [$70-$250, but only $70 if an existing 
LCD monitor is used] [1] LCD monitor (e.g, ViewSonic VX2433wm 24") [$180]  [1] Paper vellum diffuser 
(e.g., ClearPrint 11"x17x17" Vellum Pad) [$15] [1] Paper vellum diffuser (e.g., ClearPrint 11 Vellum 
Pad) [$15]  [1] Lenticular or Fly s Eye Lenslet Array (e.g., Fresnel Technologies #300) [$55] (see: 
http://www.fresneltech.com/)  Additional mounting hardware (tape, clamps, framing, etc.)   Input Image 
(Rectangular Lattice) Resampled Image (Hexagonal Lattice) Converting between rectangular and hexagonal 
sampling lattices Use included MATLAB functions rect2hex.m and hex2rect.m  Allows rectanggularlyy-samppled 
imagges to be dispplayyed usingg an hexaggonal lenslet array (e.g., Fresnel Technologies #300)    
Introduction: History and Physiology R R epresent t at i ti on and D i d Di spl l ay  Glassesbound 
Stereoscopic Displays Glasses--bound Stereoscopic Displays Unencumbered Automultiscopic D isplays Unencumbered 
Automultiscopic Displays  Integral Imaging with Fly s Eye Lenslet Arrays  Static and Dynamic Parallax 
Barriers  S M i l R d i d C  S ource M a t er i a l : R en d er i ng an d C ap t ure  Emerging Technology 
 Conclusion and Q &#38; A  Conclusion and Q &#38; A  Dual layer Barrier based 3D Display Dual-layer 
Barrier-based 3D Display 1D case is pictured, but can be extended trivially to 2D screens  Index two 
pplanes as i and k ((i.e.,, absolute two pplane pparameterization))  View-dependent display by blocking 
undesired rays with k-plane barrier  Emits ray (i,k) by multiplying intensities at i-plane and k-plane 
   ti dR EEvalluating ththe FFrontt and Rear LCD P LCD Patttterns    viewwer movves up    
  Parts List [$25-$400, but only $25 with existing monitor and laser printer] [1] LCD monitor (e.g, 
ViewSonic VX2433wm 24") [$180]  [1] Paper vellum diffuser (e.g., ClearPrint 11"x17x17" Vellum Pad) [$15] 
 [1] Paper vellum diffuser (e.g., ClearPrint 11 Vellum Pad) [$15]  [1] Printed parallax barrier (e.g., 
emulsion-based transparency) [$10-$200] (see: http://www.pageworks.com/transparencymasks.html/)  Additional 
mounting hardware (plastic spacer, clamps, framing, etc.)   Parts List [$515, possibly $515+$250n, 
where n = # of broken LCDs] [2] High-speed LCD monitor (e.g, ViewSonic VX2265wm FuHzion 22") [$250] 
 [1] Paper vellum diffuser (e.g., ClearPrint 11"x17x17" Vellum Pad) [$15] [1] Paper vellum diffuser 
(e.g., ClearPrint 11 Vellum Pad) [$15]  Additional mounting hardware (plastic spacer, clamps, framing, 
etc.)             Dual-Stacked LCD Construction a)a) Rear LCD panel and backlight (unmodified) 
Rear LCD panel and backlight (unmodified) b) Plastic spacer c) Front LCD panel (modified) d) Replacement 
linear polarizing sheet  Course Outline  Introduction: History and Physiology R R epresent t at i 
ti on and D i d Di spl l ay  Glassesbound Stereoscopic Displays Glasses--bound Stereoscopic Displays 
 Unencumbered A utomultiscopic Displays  Unencumbered Automultiscopic Displays  Source Material: 
Rendering and Capture  Emer g in g Technolo gy  gg g y  Conclusion and Q &#38; A  Introduction: 
History and Physiology   R R epresent t at i ti on and D i d Di spl l ay  Glassesbound Stereoscopic 
Displays Glasses--bound Stereoscopic Displays  Unencumbered A utomultiscopic Displays  Unencumbered 
Automultiscopic Displays  Source Material: Rendering and Capture  Emer g in g Technolo gy  gg g 
y  Conclusion and Q &#38; A   Immersive (blocks direct-viewing of real world) Head-mounted (eyepiece-objective 
and microdisplay) See-through (superimposes synthetic images onto real world) Glasses-bound Stereoscopic 
 Sppatiallyy-multipplexed ((field-concurrent)) (color filters, polarizers, autostereograms, etc.) Multiplexed 
(stereo pair with same display surface) Temporally-multiplexed (field-sequential) (LCD shutter glasses) 
 Parallax Barriers (uniform array of 1D slits or 2D pinhole arrays) Parallax-based (2D display with 
light-directing elements) Integral Imaging (lenticular sheets or fly s eye lenslet arrays)  Multi-planar 
(time-sequential projection onto swept surfaces) Unencumbered Volumetric (directly illuminate points 
within a volume) Automultiscopic Transparent Substrates (intersecting laser beams, fog layers, etc.) 
Static (holographic films) Holographic (reconstructs wavefront using 2D element) Dynamic(holovideo) 
 Time-shifted Parallax Barriers  Varying-width Parallax Barriers K. Perlin, S. Paxia, J. Kollin. An 
Autostereoscopic Display. ACM SIGGRAPH, 2000 K. Perlin, C. Poultney, J. Kollin, D. Kristjansson, S. Paxia. 
Recent Advances in the NYU Autostereoscopic Display. Proceedings of the SPIE, 2001  D. J. Sandin et 
al. The Varrier Autostereoscopic Virtual Reality Display. ACM SIGGRAPH, 2005 T. Peterka et al. Dynallax: 
Solid State Dynamic Parallax Barrier Autostereoscopic VR Display. IEEE Virtual Reality, 2007  M. Zwicker, 
W. Matusik, F. Durand, H. Pfister. Antialiasing for Automultiscopic 3D Displays. EGSR, 2006 A. Jones, 
I. McDowall, H. Yamada, M. Bolas, P. Debevec. Rendering for an Interactive 360 Light Field Display. 
ACM SIGGRAPH, 2007    Introduction: History and Physiology R R epresent t at i ti on and D i d Di 
spl l ay  Glassesbound Stereoscopic Displays Glasses--bound Stereoscopic Displays  Unencumbered A 
utomultiscopic Displays  Unencumbered Automultiscopic Displays  Source Material: Rendering and Capture 
  Emer g in g Technolo gy  gg g y  Conclusion and Q &#38; A  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837106</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>160</pages>
		<display_no>5</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Gazing at games]]></title>
		<subtitle><![CDATA[using eye tracking to control virtual characters]]></subtitle>
		<page_from>1</page_from>
		<page_to>160</page_to>
		<doi_number>10.1145/1837101.1837106</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837106</url>
		<abstract>
			<par><![CDATA[<p>Welcome to the course: Gazing at Games: Using Eye Tracking to Control Virtual Characters. I will start with a short introduction of the course which will give you an idea of its aims and structure. I will also talk a bit about my background and research interests and motivate why I think this work is important.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264995</person_id>
				<author_profile_id><![CDATA[81100357979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Veronica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sundstedt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Blekinge Institute of Technology, Sweden and Trinity College Dublin, Ireland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[O'Donovan, J., Ward, J., Hodgins, S., and Sundstedt, V. 2009. Rabbit Run: Gaze and Voice Based Game Interaction. In Eurographics Ireland Workshop, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400972</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wilcox, T., Evans, M., Pearce, C., Pollard, N., and Sundstedt, V. 2008. Gaze and Voice Based Game Interaction: The Revenge of the Killer Penguins. In SIGGRAPH '08: ACM SIGGRAPH 2008 posters, ACM, New York, NY, USA, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sundstedt, V., 2009. Interacting with Virtual Characters (invited talk), Intel Visual Computing Research Conference, Saarbr&#252;cken.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667491</ref_obj_id>
				<ref_obj_pid>1667488</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Isokoski, P., Joos, M., Spakov, O., and Martin, B. 2009. Gaze controlled games. Universal Access in the Information Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531361</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[McDonnell R., Larkin M., Hernandez B., Rudomin I., and O'Sullivan C., Eye-catching Crowds: Saliency based selective variation, ACM Transactions on Graphics (SIGGRAPH 2009), 28, (3), 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[MacEvoy B. The Visual Field. http://www.handprint.com, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Snowden R., Thompson P., and Troscianko T., Basic Vision: an introduction to visual perception. Oxford University Press, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer - Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Noton D. and Stark L. W., Scanpath in Saccadic Eye Movements While Viewing and Recognizing Patterns. Vision Research, 11:929--942, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Snowden R., Thompson P., and Troscianko T., Basic Vision: an introduction to visual perception. Oxford University Press, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Vilis T., The Physiology of the Senses Transformations for Perception and Action. Course Notes, University of Western Ontario, Canada, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Rensink R. A., Visual Attention. In L. Nadel, editor, Encyclopedia of Cognitive Science, pages 509--515. Nature Publishing Group, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1090150</ref_obj_id>
				<ref_obj_pid>1090122</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sundstedt V., Debattista K., Longhurst P., Chalmers A., Troscianko T., Visual Attention for Efficient High-Fidelity Graphics, Spring Conference on Computer Graphics, May 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>297870</ref_obj_id>
				<ref_obj_pid>297843</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Itti L., Koch C., and Niebur E., A Model of Saliency-Based Visual Attention for Rapid Scene Analysis. IEEE Trans. Pattern Anal. Mach. Intell., 20(11):1254--1259, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Yarbus A. L., Eye Movements and Vision. Plenum Press, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., A Breadth-First Survey of Eye Tracking Applications, Behavior Research Methods, Instruments, and Computers, Nov;34(4):455--70, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Duchowski, A. T., Eye Tracking Techniques for Perceptually Adaptive Graphics, ACM SIGGRAPH, EUROGRAPHICS Campfire, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., A Breadth - First Survey of Eye Tracking Applications, Behavior Research Methods, Instruments, and Computers, Nov;34(4):455 -- 70, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer - Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Luebke D., Hallen B., Newfield D., and Watson B., Perceptually Driven Simplification Using Gaze-Directed Rendering. Tech. Rep. CS-2000-04, University of Virginia, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Aaltonen A., Introduction to Eye Tracking, Tampere University Computer Human Interaction Group, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer - Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667491</ref_obj_id>
				<ref_obj_pid>1667488</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Isokoski, P., Joos, M., Spakov, O., and Martin, B. 2009. Gaze controlled games. Universal Access in the Information Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Nilsson T., A Tobii Technology Introduction and Presentation, And How Tobii Eye Tracking Could be used in advertising, at Beyond AdAsia2007, Jeju Island, Korea, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Poole, A. and Ball, L. J., Eye Tracking in Human-Computer Interaction and Usability Research: Current Status and Future Prospects. In Ghaoui, Claude (Ed.), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Rayner, K., and Pollatsek, A., The psychology of reading. Englewood Cliffs, NJ: Prentice Hall, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer - Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[R&#228;ih&#228; K - J., New Interaction Techniques. Course Notes, TAUCHI -- Tampere Unit for Computer - Human Interaction, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>640601</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Goldberg, H. J. and Wichansky, A. M., Eye tracking in usability evaluation: A practitioner's guide. In J. Hy&#246;n&#228;, R. Radach, &amp; H. Deubel (Eds.), The mind's eye: Cognitive and applied aspects of eye movement research (pp. 493--516). Amsterdam: Elsevier, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[http://www.cogain.org/wiki/Eye_Trackers]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Poole, A., Tips for Using Eyetrackers in HCI Experiments, Lancaster University, Lecture, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>633452</ref_obj_id>
				<ref_obj_pid>633292</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Schnipke S. K. and Todd M. W., Trials and tribulations of using an eye - tracking system, CHI '00: CHI '00 extended abstracts on Human factors in computing systems, pp 273 -- 274, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97246</ref_obj_id>
				<ref_obj_pid>97243</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Jacob R. J. K., What you look at is what you get: eye movement-based interaction techniques, CHI '90: Proceedings of the SIGCHI conference on Human factors in computing systems, 11--18, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>968388</ref_obj_id>
				<ref_obj_pid>968363</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Rothkopf C. A. and Pelz J. B., Head movement estimation for wearable eye tracker. In ETRA '04: Proceedings of the 2004 symposium on Eye tracking research &amp; applications, pages 123--130, New York, NY, USA, ACM Press, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97245</ref_obj_id>
				<ref_obj_pid>97243</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[I. Starker and R. Bolt, A Gaze-Responsive Self-Disclosing Display," in CHI '90: Proceedings of the SIGCHI conference on Human factors in computing systems, (New York, NY, USA), pp. 3{10, ACM, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[http://www.speechtechmag.com/articles/readarticle.aspx?articleid=29432]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[O'Donovan, J. 2009. Gaze and Voice Based Game Interaction. University of Dublin, Trinity College. Master of Computer Science in Interactive Entertainment Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Q. Mehdi, X. Zeng, and N. Gough, An interactive speech interface for virtual characters in dynamic environments," 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1178849</ref_obj_id>
				<ref_obj_pid>1178823</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[El-Nasr, M. S., and Yan, S. 2006. Visual attention in 3d video games. In Proc. of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology, 22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Kenny, A., Koesling, H., Delaney, D., Mcloone, S., and Ward, T. 2005. A preliminary investigation into eye gaze data in a first person shooter game. In 19th European Conference on Modelling and Simulation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Peters, R. J., and Itti, L. 2007. Beyond bottom-up: Incorporating task-dependent influences into a computational model of spatial attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1.8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1279923</ref_obj_id>
				<ref_obj_pid>1279920</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Peters, R. J., and Itti, L. 2008. Applying computational tools to predict gaze direction in interactive visual environments. ACM Transactions on Applied Perception 5, 2, 22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Sasse, D. (2008). A Framework for Psychophysiological Data Acquisition in Digital Games. Master's Thesis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Sennersten, C. (2004). Eye movements in an Action Game Tutorial. Student Paper. Department of Cognitive Science. Lund University, Sweden.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Sennersten, C., Alfredseon, J., Castor, M., Hedstr&#246;m, J., Lindhal, B, Lindley, C., and Svensson, E. (2007) Verification of an Experimental Platform Integrating a Tobii Eyetracking System with the HiFi Game Engine. Command and Control Systems, Methodology Report, FOI-R--2227-SE, ISSN 1650--1942, FOI Devence Research Agency, February 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Stellmach, S. (2007). A Psychophysiological Logging System for a Digital Game Modification. Technical Bachelor's Report.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[V. Sundstedt, E. Stavrakis, M. Wimmer, E. Reinhard, APGV '08 - The 5th Symposium on Applied Perception in Graphics and Visualization, Los Angeles, California, Aug 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667491</ref_obj_id>
				<ref_obj_pid>1667488</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Isokoski, P., Joos, M., Spakov, O., and Martin, B. 2009. Gaze controlled games. Universal Access in the Information Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[O'Donovan, J. 2009. Gaze and Voice Based Game Interaction. University of Dublin, Trinity College. Master of Computer Science in Interactive Entertainment Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Leyba, J. and Malcolm, J. (2004) Eye Tracking as an Aiming Device in a Computer Game. Course work (CPSC 412/612 Eye Tracking Methodology and Applications by A. Duchowski), Clemson University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>303053</ref_obj_id>
				<ref_obj_pid>302979</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[S. Zhai, C. Morimoto, and S. Ihde, Manual and gaze input cascaded (magic) pointing," in CHI '99: Proceedings of the SIGCHI conference on Human factors in computing systems, (New York, NY, USA), pp. 246--253, ACM, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[J&#246;nsson E.: If Looks Could Kill - An Evaluation of EyeTracking in Computer Games. Master's thesis, KTH Royal Institute of Technology, Sweden, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Kenny A., Koesling H., Delaney D., Mcloone S., Ward T.: A Preliminary Investigation into Eye Gaze Data in a First Person Shooter Game. In Proceedings of the 19th European Conference on Modelling and Simulation (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[J&#246;nsson E.: If Looks Could Kill - An Evaluation of EyeTracking in Computer Games. Master's thesis, KTH Royal Institute of Technology, Sweden, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1178847</ref_obj_id>
				<ref_obj_pid>1178823</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Smith, J., Graham, T. C. N.: Use of eye movements for video game control. In: Proc. ACM SIGCHI international conference on Advances in computer entertainment technology (ACE 2006). ACM, New York (2006)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Isokoski P., Martin B.: Eye Tracker Input in First Person Shooter Games. In Proceedings of the 2nd COGAIN Annual Conference on Communication by Gaze Interaction: Gazing into the Future (2006), pp. 78--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Isokoski P., Hyrskykari A., Kotkaluoto S., Martin B.: Gamepad and Eye Tracker Input in FPS Games: data for the first 50 min. In Proceedings of COGAIN (2007), pp. 10--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Dorr, M., B&#246;hme, M., Martinetz, T., Brath, E.: Gaze beats mouse: a case study. In: Proceedings of COGAIN, pp. 16--19 (2007)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667491</ref_obj_id>
				<ref_obj_pid>1667488</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Isokoski, P., Joos, M., Spakov, O., and Martin, B. 2009. Gaze controlled games. Universal Access in the Information Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1344523</ref_obj_id>
				<ref_obj_pid>1344471</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[H. O. Istance, R Bates, A. Hyrskykari and S. Vickers (2008) Snap Clutch, a Moded Approach to Solving the Midas Touch Problem. <i>Proceedings of the 2008 symposium on Eye tracking research &amp; applications ETRA '08</i>, ACM Press, Savannah, March 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Vickers, S., Istance, H., Hyrskykari, A. Ali, N., and Bates, R. (2008). Keeping an Eye on the Game: Eye Gaze Interaction with Massively Multiplayer Online Games and Virtual Communities for Motor Impaired Users. Proceedings of the 7th International Conference on Disability, Virtual Reality and Associated Technologies; ICDVRAT 2008, Maia, Portugal, 8th--10th September 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[http://www.cogain.org/wiki/Snap_Clutch]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Castellina E., Corno F.: Multimodal Gaze Interaction in 3D Virtual Environments. In Proceedings of the 4th COGAIN Annual Conference on Communication by Gaze Interaction, Environment and Mobility Control by Gaze (2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Hillaire S., Lecuyer A., Cozot R., and Casiez, G., Using an Eye - Tracking System to Improve Depth - of - Field Blur Effects and Camera Motions in Virtual Environments, Proceedings of IEEE Virtual Reality (VR) Reno, Nevada, USA, pp. 47 -- 51, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[S&#233;bastien Hillaire, Gaspard Breton, Nizar Ouarti, R&#233;mi Cozot, Anatole L&#233;cuyer, Using a Visual Attention Model to Improve Gaze Tracking Systems in Interactive 3D Applications Accepted for publication in Computer Graphics Forum, 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1358820</ref_obj_id>
				<ref_obj_pid>1358628</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Ekman, I. M., Poikola, A. W., and M&#228;k&#228;r&#228;inen, M. K. (2008) Invisible eni: using gaze and pupil size to control a game. In CHI '08 Extended Abstracts on Human Factors in Computing Systems, CHI '08. ACM, New York, NY, 3135--3140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1344501</ref_obj_id>
				<ref_obj_pid>1344471</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Ekman, I., Poikola, A., M&#228;k&#228;r&#228;inen, M., Takala, T., and H&#228;m&#228;l&#228;inen, P. (2008) Voluntary pupil size change as control in eyes only interaction. In Proceedings of the 2008 Symposium on Eye Tracking Research &amp; Applications - ETRA '08. ACM, New York, NY, 115--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Howell Istance, Aulikki Hyrskykari, Stephen Vickers' and Thiago Chaves, For Your Eyes Only: Controlling 3D Online Games by Eye-Gaze, Lecture Notes in Computer Science, Springer Berlin / Heidelberg, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[O'Donovan, J., Ward, J., Hodgins, S., and Sundstedt, V. 2009. Rabbit Run: Gaze and Voice Based Game Interaction. In Eurographics Ireland Workshop, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[O'Donovan, J. 2009. Gaze and Voice Based Game Interaction. University of Dublin, Trinity College. Master of Computer Science in Interactive Entertainment Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400972</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Wilcox, T., Evans, M., Pearce, C., Pollard, N., and Sundstedt, V. 2008. Gaze and Voice Based Game Interaction: The Revenge of the Killer Penguins. In SIGGRAPH '08: ACM SIGGRAPH 2008 posters, ACM, New York, NY, USA, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400972</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Wilcox, T., Evans, M., Pearce, C., Pollard, N., and Sundstedt, V. 2008. Gaze and Voice Based Game Interaction: The Revenge of the Killer Penguins. In SIGGRAPH '08: ACM SIGGRAPH 2008 posters, ACM, New York, NY, USA, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[J. O'Donovan, J. Ward, S. Hodgins, and V. Sundstedt, "Rabbit Run: Gaze and Voice Based Game Interaction," in Eurographics Ireland Workshop (to appear), Dec 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Castellina E., Corno F.: Multimodal Gaze Interaction in 3D Virtual Environments. In Proceedings of the 4th COGAIN Annual Conference on Communication by Gaze Interaction, Environment and Mobility Control by Gaze (2008).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Kenny A., Koesling H., Delaney D., Mcloone S., Ward T.: A Preliminary Investigation into Eye Gaze Data in a First Person Shooter Game. In Proceedings of the 19th European Conference on Modelling and Simulation (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[J&#246;nsson E.: If Looks Could Kill - An Evaluation of EyeTracking in Computer Games. Master's thesis, KTH Royal Institute of Technology, Sweden, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[J. O'Donovan, J. Ward, S. Hodgins, and V. Sundstedt, "Rabbit Run: Gaze and Voice Based Game Interaction," in Eurographics Ireland Workshop (to appear), Dec 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Bulling, A., Roggen, D., and Tr&#246;ster, G. (2008) EyeMote - Towards Context-Aware Gaming Using Eye Movements Recorded from Wearable Electrooculography. In the Second International Conference on Fun and Games, LNCS 5294. pp 33--45. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Charlotte Sennersten and Craig Lindley (2008) Evaluation of Real-time Eye Gaze Logging by a 3D Game Engine. In Proc. 12th IMEKO TC1 &amp; TC7 Joint Symposium on Man Science &amp; Measurement, Annecy, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Lee, Eui-Chul and Park, Kang-Ryoung (2005) <i>The KIPS transactions. Part B, Volume b12, Issue 4</i>, Korea Information Processing Society (August 2005, ISSN 1598-284x), pp. 465--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Gowases, T. (2007) Gaze vs. Mouse: An evaluation of user experience and planning in problem solving games. Master's thesis May 2, 2007. Department of Computer Science, University of Joensuu, Finland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Gowases, T., Bednarik, R., and Tukiainen, M. (2008) Gaze vs. Mouse in Games: The Effects on User Experience. In Proceedings of the International Conference on Computers in Education, ICCE 2008, pp. 773--777.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028647</ref_obj_id>
				<ref_obj_pid>1028630</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Hornof, A., Cavender, A., &amp; Hoselton, R. (2004). EyeDraw: A System for Drawing Pictures with Eye Movements. Proceedings of the ACM SIGACCESS Conference on Computers and Accessibility.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667491</ref_obj_id>
				<ref_obj_pid>1667488</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Isokoski, P., Joos, M., Spakov, O., and Martin, B. 2009. Gaze controlled games. Universal Access in the Information Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Spakov, O. (2005). EyeChess: the tutoring game with visual attentive interface. Alternative Access: Feelings and Games 2005, Department of Computer Sciences, University of Tampere, Finland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[http://www.scss.tcd.ie/Veronica.Sundstedt/gazingatgames2010.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Slide 1   SIGGRAPH 2010 The People Behind the Pixels  Slide 2 SIGGRAPH 2010Gazing at Games: Using 
Eye Tracking to Control Virtual CharactersVeronica SundstedtBlekinge Institute of Technology, SwedenTrinity 
College Dublin, IrelandWednesday, 28 July, 3:45 PM 5:15 PM  ACM SIGGRAPH 2010 Course Gazing at Games: 
Using Eye Tracking to Control Virtual Characters. Instructor: Veronica Sundstedt, Blekinge Institute 
of Technology, Sweden &#38; Trinity College Dublin, Ireland. Wednesday, 28 July, 3:45 PM 5:15 PM, Los 
Angeles Convention Center.  Slide 3 Introduction Welcome Course aims Course structure Who am I? Motivation 
 Welcome to the course: Gazing at Games: Using Eye Tracking to Control Virtual Characters. I will start 
with a short introduction of the course which will give you an idea of its aims and structure. I will 
also talk a bit about my background and research interests and motivate why I think this work is important. 
  Slide 4 Course aims Introduce the attendees to eye tracking, attention, and technologies for tracking 
gaze Present work done in gaze-controlled gamesand discuss issues relating to eye tracking Describe two 
case studiesin which gaze and voice have been used to control virtual characters and their behaviour 
Audience discussionaround novel multi-modal interaction techniques for games  This course is for people 
who are interested in incorporating eye tracking in games and virtual environments. The attendees will 
be given an introduction to attention, eye movements, and different eye tracking technologies. Some previous 
work in the field of gaze in gaming will be summarized, as well as future ideas to create richer interaction 
and attention-aware behavior algorithms for characters and crowds in virtual environments. The lessons 
learned in the case studies will be presented and issues relating to incorporating eye tracking in games 
will be discussed. I would also like to encourage audience discussion about using novel multi-modal interaction 
techniques for games. There are no particular prerequisites for this course.  Slide 5 Course structure 
Introduction (5 min) Eye tracking (15 min) Related work (25 min) Case studies (30 min) Future work (5 
min) Q&#38;A (10 min) Finish (5:15PM)Images from Wilcox et al. (2008) and O Donovan et al. (2009).  
Here you can see the course structure. After this introduction I will talk about attention, eye movements, 
and different eye tracking technologies. This will be followed by a presentation of some of the key related 
work in the field of using gaze as an input device in computer games. After this I will present two of 
our case studies which use gaze and voice recognition in combination to control virtual characters and 
to interact with computer games. The first case study The Revenge of the Killer Penguins is a third 
person adventure puzzle game using a combination of non intrusive eye tracking technology and voice recognition 
for novel game features [Wilcox et al. 2008]. The second case study Rabbit Run is a first person maze 
game which was created to compare gaze and voice input with traditional techniques, such as mouse and 
keyboard [O Donovan et al. 2009]. The lessons learned in these case studies will be presented and issues 
relating to incorporating eye tracking in games will be discussed. After this I will present some ideas 
for how the research can be developed further to create richer interaction for characters and crowds 
in virtual environments. This will be followed by a 10 min Q&#38;A session in which I encourage audience 
discussion around the topic. This is a short course which will end at 5:15 PM. O DONOVAN, J., WARD, 
J., HODGINS, S., AND SUNDSTEDT, V. 2009. Rabbit Run: Gaze and Voice Based Game Interaction. In Eurographics 
Ireland Workshop, December. WILCOX, T., EVANS, M., PEARCE, C., POLLARD, N., AND SUNDSTEDT, V. 2008. 
Gaze and Voice Based Game Interaction: The Revenge of the Killer Penguins. In SIGGRAPH 08: ACM SIGGRAPH 
2008 posters, ACM, New York, NY, USA, 1 1.  Slide 6 Who am I?GV2, Trinity College Dublinhttp://gv2.cs.tcd.ie/Veronica 
Sundstedt -veronica.sundstedt@bth.seBlekinge Institute of Technologyhttp://www.bth.se/eng  Veronica 
Sundstedt was recently appointed as a Lecturer in Computer Graphics at Blekinge Institute of Technology, 
Karlskrona, Sweden. Prior to this she was working as a Lecturer in the Graphics, Vision, and Visualisation 
Group in the School of Computer Science and Statistics at Trinity College Dublin, Ireland. Previously, 
she was a Postdoctoral Research Associate in the Department of Computer Science at the University of 
Bristol and the University of Bath, UK. She holds a PhD in Computer Graphics from the University of Bristol 
and a M.Sc. in Media Technology from the University of Linkping, Sweden. Her research interests are 
in graphics and perception, in particular perceptually-based rendering techniques, multi-modal interaction, 
experimental validation, and eye tracking techniques.  Slide 7 Motivation Mice, keyboards, specific 
game controllers Recently alternative input modalities have emerged as a means of interacting with computer 
games Motion sensing, gesture recognition, pointing, sound  Traditional input devices include mice, 
keyboards, and specific game controllers. Recent innovations in the video game industry include alternative 
input modalities for games, such as motion sensing, gesture recognition, pointing, and sound. We believe 
that alternative input modalities can be used in novel ways to enhance gameplay, for example by using 
them to allow virtual humans to interact with objects, navigate through environments, and interact with 
other characters and crowds [Sundstedt 2009]. Our aim is to create an enhanced, more immersive user experience. 
Alternative means of interaction in games are especially important for disabled users for whom traditional 
techniques, such as mouse and keyboard, may not be feasible. SUNDSTEDT, V., 2009. Interacting with Virtual 
Characters (invited talk), Intel Visual Computing Research Conference, Saarbrcken.  Slide 8 ExamplesEyeToy 
camera, PlayStation 2.Author: Dave PapePlayStation Eye, PlayStation 3.Author: Ryoichi Tanaka -wiggling.net 
 The EyeToy for PlayStation 2 for example is a colour digital camera (like a web camera). The camera 
allows players to interact with the game using motion, colour detection, and sound via its microphone. 
To process the images (edge detection, colour tracking, and face mapping) taken by the camera the technology 
uses vision and gesture recognition. The idea with the EyeToy was to have a more natural user interface. 
The camera needs to be used in a room which is well-lit in order to process the input from the player. 
The PlayStation Eye for PlayStation 3 is a successor of the EyeToy camera. The PlayStation Eye makes 
use of a microphone array which allows for tracking multi-dimensional voice locations, echo cancellation, 
and to suppress background noise.  Slide 9 ExamplesWii remote, Nintendo.Author: Greyson OrlandoProject 
Natal, Xbox360, Microsoft.Author: Jake Metcalf  The Wiimote has motion sensing capability allowing the 
user to interact on screen via gesture recognition and pointing through the use of accelerometer and 
optical sensor technology. Project Natal for Xbox360 by Microsoft allows the player to have a completely 
controller free gaming experience. The add-on equipment allows the user to interact with the game using 
gestures, spoken commands, and by presenting objects or images.  Slide 10 ExamplesX50 eye tracker, 
Tobii.Author: Veronica SundstedtT-series eye tracker, Tobii.Author: TobiiEyeLink II eye trackerAuthor: 
McDonnell et al. [2009]  Recent innovations in the video game industry include alternative input modalities 
for games to provide an enhanced, more immersive user experience. Eye tracking input has recently been 
explored as an input modality in games [Isokoski et al. 2009]. Using eye tracking in computer graphics 
and interactive techniques user studies is a relatively new phenomenon. Nowadays eye tracking technology 
has advanced and it is possible to obtain both cheaper, easier to use, faster, and more accurate eye 
tracking systems [Duchowski 2003]. As eye trackers become more accurate, cheaper, and less intrusive 
to the user, the technology could well be integrated into the next generation of games. It is important 
therefore to ascertain its viability as an input modality and explore how it can be used to enhance the 
gamer experience. Next I will be talking a little bit more about what eye tracking is and how we can 
use it as input to games Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 
2003. ISOKOSKI, P., JOOS, M., SPAKOV, O., AND MARTIN, B. 2009. Gaze controlled games. Universal Access 
in the Information Society. McDonnell R., Larkin M., Hernandez B., Rudomin I., and O'Sullivan C., Eye-catching 
Crowds: Saliency based selective variation, ACM Transactions on Graphics (SIGGRAPH 2009), 28, (3), 2009. 
  Slide 11 Eye Tracking What do we see? What is eye tracking? Measuring eye movements Eye tracking 
technology  I will now talk a little bit about attention, eye movements, and different eye tracking 
technologies.   Slide 12 What do we see?Schematic representation of the human visual field. Adapted 
from MacEvoy [2007].MacEvoy B. The Visual Field. http://www.handprint.com, 2007.  The information in 
the environment that reaches our eyes is much greater than our brain can process. Humans use selective 
visual attention to extract relevant information. Visual acuity relates to the resolution limit of the 
eye and our ability to resolve fine details [Snowden et al. 2006]. Due to the uneven distribution of 
photoreceptor cells in the retina humans have higher visual acuity in the fovea. The figure shows a schematic 
representation of the human visual field. The foveal vision has an area of approximately 2. To reposition 
the image onto this area, the human visual system uses five basis types of eye movements. MacEvoy B. 
The Visual Field. http://www.handprint.com, 2007. Snowden R., Thompson P., and Troscianko T., Basic 
Vision: an introduction to visual perception. Oxford University Press, 2006.  Slide 13 Eye movements 
Five basic types Saccades Smooth pursuits Vergence Vestibular ocular reflex (VOR) Optokinetic reflex 
(OKR) Fixations 200-300 ms, 90% of viewing time Scan paths Trajectory between fixation points  As described 
in the previous slide, the highest visual acuity is obtained in the foveal region. To reposition the 
image onto this area, the human visual system uses five basis types of eye movements: saccades, smooth 
pursuits, vergence, vestibular ocular reflex, and optokinetic reflex [Duchowski 2003]. The eye movements 
are controlled by six muscles, which allow the eye to move within six degrees of freedom. The five types 
of eye movements are summarised below based on descriptions by Vilis [2006] and Duchowski [2003]: Saccades 
are fast and ballistic eye movements used to reposition the fovea. Ballistic means that when the saccade 
has begun the final destination cannot be changed. These movements are both voluntary and reflexive and 
last from 10-100 ms. There is virtually no visual information obtained during a saccade. Smooth pursuits 
are movements used when tracking a visually moving target. Although it is dependent on the range and 
the speed of the target, the eyes are normally capable of matching its velocity. Vergence movements 
are used for depth perception to focus the pair of eyes over a distant target. Vestibular ocular reflex 
(VOR) movements are used to fixate the eyes on an object even if the head rotates. It works even if the 
eyes are closed. Optokinetic reflex (OKR) movements are used to account for the motion of the visual 
field. They produce a sense of self-motion which can be experienced when sitting in a stationary train 
and the opposite train starts to move. Between eye movements fixations occur, which often last for about 
200-300 ms and are rarely shorter than 100 ms [Snowden et al. 2006]. Approximately 90% of viewing time 
is spent on fixations [Duchowski 2003]. During a fixation the image is held approximately still on the 
retina; the eyes are never completely still, but they always jitter using small movements called tremors 
or drifts [Snowden et al. 2006]. The trajectory between fixation points is usually called a scanpath 
[Noton and Stark 1971]. The image shows a scan path from an observer looking at the Lucy statue. The 
circles indicate fixation points where a larger radius represents longer fixation duration. Rendering 
of the Lucy scene (Image courtesy of Diego Gutierrez and Oscar Anson). The model of Lucy was created 
by the Stanford University Computer Graphics Laboratory. The eye tracking was done using the Tobii x50 
eye tracker. Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003. 
 Noton D. and Stark L. W., Scanpath in Saccadic Eye Movements While Viewing and Recognizing Patterns. 
Vision Research, 11:929 942, 1971. Snowden R., Thompson P., and Troscianko T., Basic Vision: an introduction 
to visual perception. Oxford University Press, 2006. Vilis T., The Physiology of the Senses Transformations 
for Perception and Action. Course Notes, University of Western Ontario, Canada, 2006.  Slide 14 Visual 
attention Combination of two main processes Bottom-up and top-down visual search Models of visual attention.Task 
maps, saliency maps, importance maps etc.  The information in the environment that reaches our eyes 
is much greater than our brain can process. Selective visual attention is a complex action consisting 
of conscious and subconscious processes in the brain, which are used to extract relevant information 
in a quick and efficient manner [Rensink 2003]. There are two general visual attention processes, termed 
bottom- up and top-down, which determine where humans locate their attention. A lot of recent work has 
focused on trying to model these processes separate or in combination. This includes models (or maps) 
that try to predict what will attract attention, including task maps aiming to model what the user will 
look at while performing a task or saliency maps, which tries to model what will automatically attract 
attention in a scene in the case where no task is present. Models of human visual attention have been 
used in computer graphics to guide rendering resources for example in ray tracing. For example brighter 
regions in an importance map will receive more samples [Sundstedt et al. 2005]. Rensink R. A., Visual 
Attention. In L. Nadel, editor, Encyclopedia of Cognitive Science, pages 509 515. Nature Publishing Group, 
2003. Sundstedt V., Debattista K., Longhurst P., Chalmers A., Troscianko T., Visual Attention for Efficient 
High-Fidelity Graphics, Spring Conference on Computer Graphics, May 2005.  Slide 15 Bottom-up processingCorridor 
scene: (left) high quality and (right) saliency map using the algorithm by Itti et al. (1998).  In bottom-up 
processing the visual stimulus capture attention automatically without volitional control [Itti et al. 
1998]. Low-level, bottom-up features which influence visual attention include contrast, size, shape, 
colour, brightness, orientation, edges, and motion. Examples of maps from the corridor scene. Frame 1 
(left) and saliency map (right). The saliency map is generated using the algorithm by Itti et al. [1998]. 
 Itti L., Koch C., and Niebur E., A Model of Saliency-Based Visual Attention for Rapid Scene Analysis. 
IEEE Trans. Pattern Anal. Mach. Intell., 20(11):1254 1259, 1998.  Slide 16 Top-down processingImages 
used with permission from Springer Science and Business Media.  In contrast, top-down processes focus 
on the observer s goal; they depend on the task. The top-down approach was studied by Yarbus who asked 
an observer to look at a picture while their eye movements were recorded. Yarbus [1967] study showed 
that the scanpath was influenced by the question being asked of the observer while studying the picture. 
The figure shows seven scanpaths from one observer: (1) freeviewing, (2) estimate the material, circumstances 
of the family, (3) estimate their age, (4) guess what the family did before the unexpected visitor arrived, 
(5) remember their clothes, (6) remember the position of the people and the objects, and (7) estimate 
how long the unexpected visitor had been away. Yarbus A. L., Eye Movements and Vision. Plenum Press, 
1967.  Slide 17 What is eye tracking? Eye tracking allows us to determine where an observers gaze 
is focused at a given timeExample eye movements from an observer free-viewing and performing a counting 
task.  Eye-tracking is a process that records eye movements allowing us to determine where an observer 
s gaze is fixed at a given time. The point being focused upon on a screen is called a gaze point. Eye 
tracking allow us to capture the gaze of an observer. The direction of gaze indicates where humans focus 
their attention. Eye-tracking techniques make it possible to capture the scan path of an observer. In 
this way we can gain insight into what the observer looked at, what they might have perceived, and what 
drew their attention [Duchowski 2003]. We can also study how tasks affect gaze behaviour as seen in the 
figure above. The left image shows an observer free-viewing the Kalabsha temple scene and the image to 
the right shows the eye movements of the same observer performing the task of counting the stones in 
the courtyard. There are several application areas for using eye tracking [Duchowski 2002]. Eye tracking 
has previously extensively been used in psychology, neuroscience, human factors, human computer interaction, 
in particular in the evaluation of web design and advertising to find out what people look at and how 
sites can be made more efficient for specific tasks etc. Duchowski A.T. , A Breadth-First Survey of 
Eye Tracking Applications, , Behavior Research Methods, Instruments, and Computers, Nov;34(4):455-70, 
2002. Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003.  Slide 
18 Eye tracking applicationsEye tracking systemsInteractiveDiagnosticSelectiveGaze-contingentScreen-basedModel-basedThe 
figure below is adapted from Duchowski [2001].  There are different types of eye tracking applications 
shown in the hierarchy. Broadly eye tracking systems can be divided into two categories: interactive 
and diagnostic systems. The figure is adapted from Duchowski [2001]. In interactive systems the users 
gaze is used to interact with the application. The users gaze can be used as an alternative input device. 
Early work in perceptually adaptive graphics mainly falls into gaze-contingent where parts of the virtual 
environment are modified based on the gaze of the observer [Luebke et al. 2000]. Gaze-contingent techniques 
are also divided into screen-based and model-based techniques. Screen-based manipulate frame buffer before 
display. Model-based techniques reduce resolution by modifying geometry prior to rendering [Duchowski 
2001]. Today I will be talking about selective applications where the users gaze is used to interact 
with and control virtual characters. Duchowski, A. T., Eye Tracking Techniques for Perceptually Adaptive 
Graphics, ACM SIGGRAPH, EUROGRAPHICS Campfire, 2001. Duchowski A.T. , A Breadth-First Survey of Eye 
Tracking Applications, , Behavior Research Methods, Instruments, and Computers, Nov;34(4):455-70, 2002. 
 Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003. Luebke D., Hallen 
B., Newfield D., and Watson B., Perceptually Driven Simplification Using Gaze-Directed Rendering. Tech. 
Rep. CS-2000-04, University of Virginia, 2000.  Slide 19 Eye tracking technology Video-based Head-mounted 
Portable Wearable Remote Electronic Skin electrodes Mechanical LensesX50 eye tracker, Tobii.Author: Veronica 
SundstedtAccuracy: 0.5-0.7 degreesSpatial resolution: 0.35 degreesTemporal resolution: 50 Hz  Many different 
types of eye tracking systems have been developed since it was first used in reading research about 100 
years ago [Rayner and Pollatsek 1989, Nilsson 2009]. There are two general techniques for studying eye 
movements: (1) by measuring the position of the eye relative to the head, or (2) by measuring the orientation 
of the eye in space [Duchowski 2003]. The most common system for the second technique is the video-based 
corneal reflection eye-tracker [Duchowski 2003]. Video-based eye tracking systems can be head-mounted, 
portable, or wearable. There are also electronic methods in which skin electrodes are used around the 
eyes to measure the potential differences in the eye or mechanical methods which uses contact lenses 
with a metal coil around the edge [Aaltonen 2005]. These are very intrusive techniques. Most eye trackers 
today use video images of the eye to determine where a person is looking, i.e. their point of regard 
[Poole and Ball 2004]. There are several important terms used in eye tracking. The accuracy of the eye 
tracker shows the expected difference in degrees of visual angle between true eye position and mean computed 
eye position during a fixation [Aaltonen 2005]. The accuracy is usually 0.5-1O.. The spatial resolution 
of the eye tracker shows the smallest change in eye position that can be measured and the temporal resolution 
or sampling rate is the number of recorded eye positions per second [Aaltonen 2005]. Many eye trackers 
has a sampling rate of about 60-120Hz. Isokoski et al. [2009] highlights that it is much more important 
to have rapid and fluid use of eye control in fast-paced games and that it can be lower in turn-based 
games. Aaltonen A., Introduction to Eye Tracking, Tampere University Computer Human Interaction Group, 
2005. Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 2003. ISOKOSKI, 
P., JOOS, M., SPAKOV, O., AND MARTIN, B. 2009. Gaze controlled games. Universal Access in the Information 
Society. Nilsson T., A Tobii Technology Introduction and Presentation, And How Tobii Eye Tracking Could 
be used in advertising, at Beyond AdAsia2007, Jeju Island, Korea, 2009. Poole, A. and Ball, L. J., Eye 
Tracking in Human-Computer Interaction and Usability Research: Current Status and Future Prospects. In 
Ghaoui, Claude (Ed.), 2004. Rayner, K., and Pollatsek, A., The psychology of reading. Englewood Cliffs, 
NJ: Prentice Hall, 1989. Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 
2006.  Slide 20 Video-based tracking(Left): Purkinje images and (right) relative positions of pupil 
and first Purkinje images as seen by the eye-tracker s camera during calibration. Adapted from Duchowski 
(2003) and Rih (2005).  In video-based eye-trackers, the light source reflection on the cornea (caused 
by infra-red light) is measured relative to the location of the pupil s centre [Duchowski 2003]. These 
two points are used as reference to compensate for head movements [Duchowski 2003]. The corneal reflections 
are also known as Purkinje reflections or Purkinje images [Duchowski 2003]. Due to the properties of 
the eye four Purkinje reflections normally appear [Rih 2005]. The first reflection is at the front 
of the cornea, the second at the rear of the cornea, the third at the front of the lens, and the fourth 
at the rear of the lens. Video-based eye-trackers normally locate the first Purkinje image [Duchowski 
2003]. For example, the Tobii x50 eye-tracker uses near infra-red light-emitting diodes (NIR-LEDs) and 
a high-resolution camera with a large field of view [Tobii 2006]. The NIR-LEDs and the camera are used 
to generate the Purkinje images of the eyes and to capture images of the observer. Eye tracking software 
consists of image processing algorithms that extract important features, such as the eyes and the Purkinje 
images generated by the NIR-LEDs. It can also calculate a three-dimensional position in space of where 
the eyes were located to determine the gaze point at a given time. The image shows Purkinje images (left) 
and (right) relative positions of pupil and first Purkinje images as seen by the eye-tracker s camera 
during calibration. Duchowski A. T., Eye Tracking Methodology: Theory and Practice, Springer-Verlag, 
2003. Rih K-J., New Interaction Techniques. Course Notes, TAUCHI Tampere Unit for Computer-Human 
Interaction, 2005. Tobii. UserManual: Tobii Eye Tracker, ClearView analysis software, February, 2006. 
Slide 21 Calibration Grid point calibration Calibration issues Using a chin rest Re-calibrate between 
trialsClearView calibration, Tobii.Author: Veronica Sundstedt  Before starting a recording the video-based 
eye trackers need to be fine-tuned to the participant in a calibration process [Poole and Ball 2004]. 
Calibration of the eye-tracker is achieved by measuring the gaze of the observer at specific grid points 
(usually 5, 9, or 16) [Duchowski 2003, Goldberg and Wichansky 2003]. The Purkinje images then appear 
as a small dot close to the pupil. The calibration process can be incorporated within the game so that 
the player is being calibrated before the game starts. Duchowski A. T., Eye Tracking Methodology: Theory 
and Practice, Springer-Verlag, 2003. Goldberg, H. J. and Wichansky, A. M., Eye tracking in usability 
evaluation: A practitioner s guide. In J. Hyn, R. Radach, &#38; H. Deubel (Eds.), The mind's eye: Cognitive 
and applied aspects of eye movement research (pp. 493-516). Amsterdam: Elsevier, 2003.   Slide 22 
Eye trackers http://www.cogain.org/wiki/Eye_Trackers Eye trackers for interactive applications Eye trackers 
for research on eye movements Open source, low cost, and freeware solutions  For further information 
about different kinds of eye trackers the COGAIN (Communication by Gaze Interaction) has a very good 
summarised list of different trackers and extensive information about their target users, technical specifications, 
and features. For gaming it is important to have non-intrusive eye trackers that can deal with head movements. 
In general more accurate results can be obtained if the users head is fixed. Another limitation with 
eye trackers is that they can only track over a specific area. http://www.cogain.org/wiki/Eye_Trackers 
  Slide 23 Recording issues Physiological reasons Pupil Eyelid External reasons Eyeglasses ContactsSetup 
with the Tobii x50 tracker.Author: Veronica Sundstedt  In eye tracking there are several issues that 
can affect the gaze measurements [Schnipke and Todd 2000]. These can broadly be categorised into two 
areas: (1) physiological reasons, and (2) external reasons. First, eye trackers are known to have some 
issues calibrating various lenses (bi-focal/tri-focal, super-dense, hard), lazy eye, large and small 
pupils, if there is a low contrast between pupil and eye white, and if the eyelid covers part of the 
pupil [Poole 2005]. Another reason is that the pupil does not reflect enough light. Participants wearing 
eyeglasses or hard contacts can also be two external reasons why recording can be difficult [Schnipke 
and Todd 2000]. There are also other issues that can affect recording. For example if the participant 
is moving their head significantly this could cause a delay until the eye tracker is capturing the eye 
again and it can also be a reason for calibration loss [Schnipke and Todd 2000]. Poole, A. , Tips for 
Using Eyetrackers in HCI Experiments, Lancaster University, Lecture, 2005. Schnipke S. K. and Todd M. 
W., Trials and tribulations of using an eye-tracking system, CHI '00: CHI '00 extended abstracts on Human 
factors in computing systems, pp 273-274, 2000.  Slide 24 Raw dataType Description Timestamp Timestamp 
in milliseconds for when the gaze data was collected Gaze PointX Left Horizontal screen position of the 
gaze point for the left eyeGaze PointY Left Vertical screen position of the gaze point for the left eyeCamXLeft 
Horizontal location of the left pupil in the camera imageCamYLeft Vertical location of the left pupil 
in the camera imageDistanceLeft Distance from the eye tracker to the left eyePupilLeft Size of the pupil 
(left eye) in mmValidityLeft Validity of the gaze data (e.g. 0 = good tracking quality)Source [Tobii 
2006].  Eye-trackers normally produce a large amount of raw data since humans perform several saccades 
per second. This raw data contains many parameters (for the left and right eye), including gaze point 
(x,y), pupil location in the camera image (x,y), distance from camera to eye, pupil size, time stamp 
in ms, and a frame number [Tobii 2006]. The raw data needs to be filtered and reduced before it is analysed. 
In this process it is common to identify fixations and saccades [Rothkopf and Pelz 2004]. Blinks can 
also be identified. This process allows us to get a better idea of the users intentions [Jacob 1990]. 
Eye tracking can be used for both offline analysis and real time interaction. Jacob R. J. K., What you 
look at is what you get: eye movement-based interaction techniques, CHI '90: Proceedings of the SIGCHI 
conference on Human factors in computing systems, 11-18, 1990. Rothkopf C. A. and Pelz J. B., Head movement 
estimation for wearable eye tracker. In ETRA 04: Proceedings of the 2004 symposium on Eye tracking research 
&#38; applications, pages 123 130, New York, NY, USA, ACM Press, 2004. Tobii. UserManual: Tobii Eye 
Tracker, ClearView analysis software, February, 2006.  Slide 25 Midas touch Everywhere one looks, 
another command is activated; the viewer cannot look anywhere without issuing another command [Jacob 
1990] Mouse click Dwell timeKing Midas with his daughter.Author: Walter Crane (Illustrator)  However, 
gaze based interaction is not without its issues. It tends to suffer from the Midas touch problem. This 
is where everywhere one looks, another command is activated; the viewer cannot look anywhere without 
issuing a command [Jacob 1990]. Midas is popularly remembered for his ability to turn everything he touched 
into gold: the Midas touch. It is not ideal to have the player look at something and then issue a command 
all the time in the game. Ideally the gaming interface should allow the user to look around the game 
and interact with objects and characters they choose to interact with. Jacob [1990] was one of the first 
to look into feasibility of gaze based selective interaction. He first identified Midas Touch and suggested 
dwell time to overcome it. To combat this problem gaze is often used in conjunction with another input 
mechanism such as a mouse click or dwell time; the object will be marked when the user looks at it and 
after a certain delay it will be chosen.  Slide 26 Background and related work Seminal work Voice 
recognition in gaming Attention in gaming Gaze in gaming  I will not talk about some of the previous 
work done in the field.  Slide 27 Seminal work Starker and Bolt [1990] Gaze responsive self closing 
display Introduced one of the first systems that had real-time eye tracking and intentionally constructed 
storytellingI. Starker and R. Bolt, A Gaze-Responsive Self-Disclosing Display," in CHI '90.  Starker 
and Bolt [1990] introduced one of the first systems with real-time eye tracking and intentionally constructed 
storytelling. A rotating planet was displayed with various features including volcanoes, staircases, 
and flowers. When the gaze of the observer was focused on these objects for a duration, the system provided 
the user with more information regarding the object using synthesized speech. Although this was not a 
game as such, games of today allow the player to investigate the surroundings and interact with the environment 
and other characters. I. Starker and R. Bolt, A Gaze-Responsive Self-Disclosing Display," in CHI '90: 
Proceedings of the SIGCHI conference on Human factors in computing systems, (New York, NY, USA), pp. 
3{10, ACM, 1990.  Slide 28 Voice recognition in gamingOriginal summary by Speech Technologies Make 
Video Games Complete [2005]. Recreated from O Donovan [2009].  A few games, such as Tom Clancy s EndWar 
Ubisoft, use voice recognition allowing the player to input voice commands to the game. In this game, 
voice control is used to issue orders to troops for example. The following table shows some different 
games that make use of voice recognition in gaming. There are two different categories of speech recognition 
technologies, speaker dependent and speaker independent [Mehdi et al. 2004]. Speaker-dependent requires 
each user to go through a process of training the engine to recognise his/her voice. Speech independent 
recognition avoids this by training with a collection of speakers in the development phase [O Donovan 
2009]. Our case study Rabbit Run presented later is using a speaker independent technology. Instead of 
using a mouse click or dwell time in combination with gaze input our work has explored if voice recognition 
can be used instead. Our work has investigated if the combination of gaze and voice as an input modality 
offers a hands-free solution to the Midas Touch problem. I will talk more about this in a bit. http://www.speechtechmag.com/articles/readarticle.aspx?articleid=29432 
 O DONOVAN, J. 2009. Gaze and Voice Based Game Interaction. University of Dublin, Trinity College. Master 
of Computer Science in Interactive Entertainment Technology. Q. Mehdi, X. Zeng, and N. Gough, An interactive 
speech interface for virtual characters in dynamic environments," 2004.  Slide 29 Attention in gaming 
The avatar of the player can direct attention to important objects or approaching enemies by looking 
at them This can aid the player in puzzle solvingThe Legend of Zelda: The Wind WakerTM, Nintendo  Several 
computer games have exploited the concept of visual attention in gaming. For example, in The Legend of 
Zelda: The Wind Waker , Nintendo, the avatar of the player can direct attention to important objects 
or approaching enemies by looking at them. This can aid the player in puzzle solving.  Slide 30 Attention 
in gaming Sennersten [2004] action game tutorial Kenny et al. [2005] fps games, focus in center El-Nasr 
and Yan [2006] top-down &#38; bottom-up Sennersten et al. [2007] eye tracker + HiFi engine Peters and 
Itti [2007,2008] task/saliency Stellmach [2007], Sasse [2008] psychophysiological logging, data acquisition 
Sundstedt et al. [2008] active vs. passive  Sennersten [2004] studied eye movements in an action game 
tutorial (Counterstrike v. 1.0). The SMI iView&#38;#169; eye-tracker was used to gather information about 
the players eye movements. Recent studies suggest that in adventure games, fixation behavior can follow 
both bottom-up and top-down processes [El-Nasr and Yan 2006]. Visual stimuli are reported to be more 
relevant when located near objects that fit players' top-down visual search goals. In first-person shooter 
games, gaze tends to be more focused on the center of the screen than in adventure games [Kenny et al. 
2005; El-Nasr and Yan 2006]. Sennersten et al. [2007] have also performed a verification of an experimental 
platform integrating a Tobii eye tracking system with the HiFi game engine. In an experiment involving 
active video game play, nine low-level heuristics were compared to gaze behavior collected using eye 
tracking [Peters and Itti 2008]. This study showed that these heuristics performed above chance, and 
that motion alone was the best predictor. This was followed by flicker and full saliency (color, intensity, 
orientation, flicker, and motion). Nonetheless, these results can be improved further by incorporating 
a measure of task relevance, which could be obtained by training a neural network on eye tracking data 
matched to specific image features [Peters and Itti 2007]. Stellmach [2007] developed a psychophysiological 
logging framework, which allows psychophysiological data can be correlated with in-game data in real 
time. The framework is also capable of logging viewed game objects via an eye tracker integration, which 
can inform us of how game elements affect our attention. Sundstedt et al. [2008] performed a psychophysical 
study of fixation behaviour in a computer game. Their study particularly looked at the differences in 
eye movements for active and passive game play. EL-NASR, M. S., AND YAN, S. 2006. Visual attention in 
3d video games. In Proc. of the 2006 ACM SIGCHI International Conference on Advances in Computer Entertainment 
Technology, 22. KENNY, A., KOESLING, H., DELANEY, D., MCLOONE, S., AND WARD, T. 2005. A preliminary 
investigation into eye gaze data in a first person shooter game. In 19th European Conference on Modelling 
and Simulation. PETERS, R. J., AND ITTI, L. 2007. Beyond bottom-up: Incorporating task-dependent influences 
into a computational model of spatial attention. In Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition, 1.8. PETERS, R. J., AND ITTI, L. 2008. Applying computational tools to predict 
gaze direction in interactive visual environments. ACM Transactions on Applied Perception 5, 2, 22. 
Sasse, D. (2008). A Framework for Psychophysiological Data Acquisition in Digital Games. Master's Thesis. 
 Sennersten, C. (2004). Eye movements in an Action Game Tutorial. Student Paper. Department of Cognitive 
Science. Lund University, Sweden. Sennersten, C., Alfredseon, J., Castor, M., Hedstrm, J., Lindhal, 
B, Lindley, C., and Svensson, E. (2007) Verification of an Experimental Platform Integrating a Tobii 
Eyetracking System with the HiFi Game Engine. Command and Control Systems, Methodology Report, FOI-R--2227-SE, 
ISSN 1650-1942, FOI Devence Research Agency, February 2007. Stellmach, S. (2007). A Psychophysiological 
Logging System for a Digital Game Modification. Technical Bachelor's Report. V. Sundstedt, E. Stavrakis, 
M. Wimmer, E. Reinhard, APGV 08 - The 5th Symposium on Applied Perception in Graphics and Visualization, 
Los Angeles, California, Aug 2008.  Slide 31 Gaze in gaming resources COGAIN Communication by Gaze 
Interaction http://www.cogain.org/links/gaze-controlled-games Survey paper Isokoski P., Joos, M., Spakov, 
O., &#38; Martin, B. (2009). Gaze Controlled Games. Universal Access in the Information Society 8(4). 
Springer  In the last few years there has been an increasing amount of work done in the field of gaze 
in gaming. Although this work is still in its early stages. It is still not common to use eye tracking 
in gaming and there are not many games that support eye tracking technology [Isokoski et al. 2009]. For 
an extensive survey on gaze controlled games please see the survey paper by Isokoski et al. [2009]. Some 
previous work is also summarised in the M.Sc. Thesis of O Donovan [2009]. When looking at eye tracking 
as a means of computer game interaction it is important to look at different genres and the challenges 
they present. Isokoski et al. [2009] discuss this in some detail. In this course I will mentioned a few 
projects that have used gaze in games. I will particularly focus on 3D games. For more information on 
previous projects using gaze in virtual environments please see: http://www.cogain.org/wiki/Gaze-Controlled_Games 
 ISOKOSKI, P., JOOS, M., SPAKOV, O., AND MARTIN, B. 2009. Gaze controlled games. Universal Access in 
the Information Society. O DONOVAN, J. 2009. Gaze and Voice Based Game Interaction. University of Dublin, 
Trinity College. Master of Computer Science in Interactive Entertainment Technology.  Slide 32 Leyba 
and Malcolm [2004] Mouse vs. gaze to aim Remove 25 balls with random velocity vectors MAGIC pointing 
was used Hypothesis: gaze less accurate but task could be performed faster Mouse was more accurate and 
faster  Leyba and Malcolm [2004] used eye tracking as an aiming device in a computer game. They compared 
mouse and gaze for aiming. They believed that although the accuracy would be less with gaze, the speed 
to remove 25 balls with random velocity vectors would be faster. The aim of the game was to remove 25 
balls by clicking on them using mouse. To overcome the Midas touch problem a conservative form Manual 
and Gaze Input Cascaded (MAGIC) [Zhai et al. 1999] pointing was used. This kind of pointing warps the 
cursor to the general point of regard. The user can then make small adjustments to be directly on target 
with a mouse. Leyba and Malcolm [2004] adapted method used the gaze point as the cursor position on screen 
when the mouse was clicked. They found that as expected mouse input was more accurate. Gaze also proved 
to be slower but the authors say this is most likely due to problems with calibration and the players 
tending to click the mouse repeatedly rather than aim accurately when using the mouse as input. Leyba, 
J. and Malcolm, J. (2004) Eye Tracking as an Aiming Device in a Computer Game. Course work (CPSC 412/612 
Eye Tracking Methodology and Applications by A.Duchowski), Clemson University. S. Zhai, C. Morimoto, 
and S. Ihde, Manual and gaze input cascaded (magic) pointing," in CHI '99: Proceedings of the SIGCHI 
conference on Human factors in computing systems, (New York, NY, USA), pp. 246-253, ACM, 1999.  Slide 
33 Jnsson [2005] Gaze input in two open source games Sacrifice (Shoot-em-up) Half-Life (FPS) Midas 
Touch resolved using mouse click Half Life decoupled aiming from camera movement Higher scores using 
gaze in Sacrifice Eye control was more fun  Jnsson [2005] used gaze input in two open source games, 
the FPS game Half Life and the Shoot-em-up Sacrifice. Open source versions of these games were adapted 
to accept gaze input from a Tobii eye tracker. Two demos were created using Sacrifice, one were aim was 
controlled by mouse and one by gaze. In Half Life three demos were created, one were the weapon sight 
and field of view were controlled by mouse, one were weapon sight and field of view were controlled by 
gaze and one where weapon sight was controlled by eyes and field of view with mouse. Participants achieved 
higher scores with eye control when playing Sacrifice than without it. Jnsson also reported that playing 
with gaze was more perceived as more fun. Jnsson also experimented with another shoot-em-up game with 
moving targets. It was reported that players tend to track the target which resulted in that their shots 
were missing the target and landing behind it. JNSSON E.: If Looks Could Kill - An Evaluation of EyeTracking 
in Computer Games. Master s thesis, KTH Royal Institute of Technology, Sweden, 2005.  Slide 34 Kenny 
et al. [2005] Playing a FPS while recording gaze data Players spend 82% of game time looking in the near 
centre of the screen Inner 400 x 300 rectangle of a 800 x 600 screen  Kenny et al. [2005] conducted 
a study in which they looked at player s fixations after playing a first person shooter (FPS) game. Their 
results showed that players spend 82% of the game time looking in the near centre of the screen, the 
inner 400 x 300 rectangle of a 800 x 600 screen. They FPS game was created using the Torque Game Engine 
and they used the SR Research EyeLink2 eye tracker to record the eye movements. Our work presented later 
in one of the case studies Rabbit Run is inspired by their result. KENNY A., KOESLING H., DELANEY D., 
MCLOONE S., WARD T.: A Preliminary Investigation into Eye Gaze Data in a First Person Shooter Game. In 
Proceedings of the 19th European Conference on Modelling and Simulation (2005).  Slide 35 Smith and 
Graham [2006] Gaze control for three games Quake 2 (FPS) orientation control Neverwinter Nights (roleplay) 
avatar moves by pointing Lunar Command (action/arcade) moving objects targeted through pointing Performance 
and subjective data when playing using mouse and eye control Eye-based input can alter the gaming experience 
and make the virtual environment feel more immersive  Smith and Graham [2006] used the Tobii 1750 eye 
tracker as a control device for video games. They examined how it could be used within different game 
genres, such as in Quake 2 (FPS), Neverwinter Nights (roleplay), and in Lunar Command (action/arcade). 
In Quake 2 eye movements were used to control the orientation. In Neverwinter Nights they study eye tracking 
as a means of interaction with characters. Finally in Lunar Command eye tracking was used to target moving 
objects through pointing. The participants played all three games using both means of control. Users 
performed significantly better with the mouse in Lunar Command but no significant performance difference 
was found for Quake 2 or Neverwinter Nights. One of their main results indicates that using gaze control 
can increase the immersion of a video game. Smith and Graham also found that players tend to fire behind 
missiles using eye control which agrees with the result by Jnsson [2005]. JNSSON E.: If Looks Could 
Kill - An Evaluation of EyeTracking in Computer Games. Master s thesis, KTH Royal Institute of Technology, 
Sweden, 2005. Smith, J., Graham, T.C.N.: Use of eye movements for video game control. In: Proc. ACM 
SIGCHI international conference on Advances in computer entertainment technology (ACE 2006). ACM, New 
York (2006)  Slide 36 Isokoski and Martin [2006] Created a FPS and compared three input devices Keyboard 
/ mouse / gaze (to aim) Keyboard / mouse Xbox 360 controller Eye tracker competitive with the gamepad 
Mouse/keyboard more efficient than the others  Isokoski and Martin [2006] developed a FPS style game 
and compared the score obtained using different input techniques. They compared the score obtained when 
using keyboard/mouse/gaze, only keyboard/mouse, and with an Xbox360 controller. The game used gaze for 
aim, mouse to control the camera and the keyboard to move the player around the scene. They report that 
using eye tracking does not improve performance in comparison to the mouse/keyboard condition, but it 
is competitive with the Xbox 360 controller. They only evaulated the system with one player (one of the 
authors). ISOKOSKI P., MARTIN B.: Eye Tracker Input in First Person Shooter Games. In Proceedings of 
the 2nd COGAIN Annual Conference on Communication by Gaze Interaction: Gazing into the Future (2006), 
pp. 78 81.  Slide 37 Isokoski et al. [2007] Same game as in 2006 Full gamepad control Moving with 
gamepad / aiming with eyes Steering and aiming with eyes Velocity control and shooting controlled with 
gamepad Increasing eye control did not affect the performance (target hit), but they fire more shots 
 Isokoski et al. [2007] presents some additional results using the same FPS game as presented in their 
previous work. They used the same game as in 2006 with three different input techniques. The first one 
was to have full gamepad control. In the second one they moved with the gamepad and aimed with the eyes. 
Finally in the third condition they were both steering and aiming with their eyes. Their results showed 
that increasing eye control did not affect the performance in terms of target hits but they did fire 
more shots. ISOKOSKI P., HYRSKYKARI A., KOTKALUOTO S., MARTIN B.: Gamepad and Eye Tracker Input in FPS 
Games: data for the first 50 min. In Proceedings of COGAIN (2007), pp. 10 15.  Slide 38 Dorr et al. 
[2007] Mouse vs. gaze in a paddle game 20 students playing each other in pairs Each player used both 
controls The eye tracker had a statistically significant advantage over mouse controlScreenshot from 
Lbreakout2.  Isokoski et al. [2009] reports that eye tracking is particularly useful in paddle games 
such as PONG style games. Dorr et al. [2007] investigated if the performance by participants controlling 
a paddle using gaze or mouse in a modified version of LBreakout2, published under the GNU General Public 
License (GPL), would differ. The results showed that the eye tracker had a statistically significant 
advantage over mouse control. When playing with gaze the ball was released after 5 seconds to avoid a 
needed mouse click. They used the SensoMotoric Instruments iViewX Hi-Speed tracker running at 240 Hz 
[Dorr et al. 2007]. They also report that they have played it successfully with a 50 Hz SMI RED-X remote 
tracker, which in which they did not need to fix the head of the player. Dorr, M., Bhme, M., Martinetz, 
T., Brath, E.: Gaze beats mouse: a case study. In: Proceedings of COGAIN, pp. 16 19 (2007) ISOKOSKI, 
P., JOOS, M., SPAKOV, O., AND MARTIN, B. 2009. Gaze controlled games. Universal Access in the Information 
Society.  Slide 39 Istance et al. [2008] Snap Clutch software tool Solution for the Midas Touch problem 
Using gaze data to generate keyboard and mouse events Program responds with these as input World of Warcraft 
Second Life  Istance et al. [2008] have developed a software tool called Snap Clutch. This is an application 
which uses gaze data to generate normal key and mouse events. These events can then be used to use applications 
such as World of Warcraft and Second Life. The application makes different gaze interaction techniques 
available to the user and allow them to switch between these in an efficient manner [COGAIN wiki]. Eye 
trackers that support Snap Clutch include Tobii and the ITU GazeTracker [COGAIN wiki]. This tool has 
also been used in Vickers et al. [2008]. H.O. Istance, R Bates, A. Hyrskykari and S. Vickers (2008) 
Snap Clutch, a Moded Approach to Solving the Midas Touch Problem. Proceedings of the 2008 symposium on 
Eye tracking research &#38; applications ETRA '08, ACM Press, Savannah, March 2008. Vickers, S., Istance, 
H., Hyrskykari, A. Ali, N., and Bates, R. (2008). Keeping an Eye on the Game: Eye Gaze Interaction with 
Massively Multiplayer Online Games and Virtual Communities for Motor Impaired Users. Proceedings of the 
7th International Conference on Disability, Virtual Reality and Associated Technologies; ICDVRAT 2008, 
Maia, Portugal, 8th-10th September 2008 http://www.cogain.org/wiki/Snap_Clutch  Slide 40 Castellina 
and Corno [2008] Created some simple 3D game scenarios to test multimodal input Used Virtual Keys to 
move forward/backwards, to look left/right. Activated by dwell time Gaze input was as accurate as other 
forms, but not as fast due to use of dwell time  Castellina and Corno [2008] created some simple 3D 
game scenarios to test multimodal input. They used semitransparent buttons (or Virtual Keys ) to rotate 
the camera and move the avatar. As can be seen in the figure these were used to move forward/backwards 
and to look left/right. These were activated by dwell time. In their studies gaze input was found to 
be as accurate but since dwell time was used they mention that it was not as fast as mouse/keyboard interaction. 
 CASTELLINA E., CORNO F.: Multimodal Gaze Interaction in 3D Virtual Environments. In Proceedings of the 
4th COGAIN Annual Conference on Communication by Gaze Interaction, Environment and Mobility Control by 
Gaze (2008).  Slide 41 Hillaire et al. [2008]Image courtesy: Hillaire et al. 2008 Quake 3 Arena screenshot 
courtesy of Id SoftwareHillaire et al. [2008] -Using an Eye-Tracking System to Improve Depth-of-Field 
Blur Effects and Camera Motions in Virtual Environments.  Hillaire et al. [2008] developed an algorithm 
to simulate depth-of-field blur for first-person navigation in virtual environments. Using an eye-tracking 
system, they analysed users focus point during navigation in order to set the parameters of the algorithm. 
Using this focus point they propose rendering techniques which aim to improve the users sensations. The 
results achieved suggest that the blur effects could improve the sense of realism experienced by players. 
Hillaire et al. [2010] has also used a model of visual attention to improve gaze tracking systems in 
interactive 3D applications. Hillaire S., Lecuyer A., Cozot R., and Casiez, G., Using an Eye-Tracking 
System to Improve Depth-of-Field Blur Effects and Camera Motions in Virtual Environments, Proceedings 
of IEEE Virtual Reality (VR) Reno, Nevada, USA, pp. 47-51, 2008. Sbastien Hillaire, Gaspard Breton, 
Nizar Ouarti, Rmi Cozot, Anatole Lcuyer, Using a Visual Attention Model to Improve Gaze Tracking Systems 
in Interactive 3D Applications Accepted for publication in Computer Graphics Forum, 2010  Slide 42 
 Ekman et al. [2008] Invisible Eni Gaze, blinking, pupil sizeto affect game state Pupil size affected 
by physical activation, strong emotional experiences, and cognitive effort Player controls game by the 
use of willpower Gaze direction move with response to gaze Gaze blinking disappear into a puff of smoke 
Pupil size open magic flowers (cognitive and emotional effort )  Ekman et al. [2008] developed a game 
which only uses the eyes as input. They make use of gaze, blinking, and pupil size to affect game state. 
They report that the pupil size is affected by physical activation, strong emotional experiences, and 
cognitive effort. They discuss limitations of using pupil size as an input modality. Pupil size is increased 
when the player is engaged in the game interaction. They use this to model magic powers. Ekman, I. M., 
Poikola, A. W., and Mkrinen, M. K. (2008) Invisible eni: using gaze and pupil size to control a game. 
In CHI '08 Extended Abstracts on Human Factors in Computing Systems, CHI '08. ACM, New York, NY, 3135-3140. 
Ekman, I., Poikola, A., Mkrinen, M., Takala, T., and Hmlinen, P. (2008) Voluntary pupil size change 
as control in eyes only interaction. In Proceedings of the 2008 Symposium on Eye Tracking Research &#38; 
Applications - ETRA '08. ACM, New York, NY, 115-118.  Slide 43 Istance et al. [2009] Software device 
using gaze input for emulating mouse and keyboard events Controlling on-line games (World of Warcraft) 
Feasible to carry out tasks at a beginners skill level using gaze alone Usability issues for three tasks 
Implications of only using gaze as input  Istance et al. [2009] reports on the development of a software 
device using gaze input in different modes for emulating mouse and keyboard events when interacting in 
on-line games. They explored how gaze could be used to control the game World of Warcraft using this 
device. They report that it is possible to perform tasks at a beginner s level using gaze alone. Howell 
Istance, Aulikki Hyrskykari, Stephen Vickers, and Thiago Chaves, For Your Eyes Only: Controlling 3D Online 
Games by Eye-Gaze, Lecture Notes in Computer Science, Springer Berlin / Heidelberg, 2009.  Slide 44 
 Previous work summary Studies have looked at various aspects Different input devices, game genres, duration 
of play, eye trackers, game engines Difficult to assess the significance of the results Performance vs. 
immersion More research is needed  Studies have looked at various aspects Different input devices, game 
genres, duration of play, eye trackers, game engines Difficult to assess the significance of the results 
Performance vs. immersion More research is needed  Slide 45 Case studies Case study I The Revenge 
of the Killer Penguins Case study II Rabbit Run  I will present two game case studies in which gaze 
and voice in combination were used to control virtual characters and their behaviour. The first game 
is described further in Wilcox et al. [2008] and the second in O Donovan [2009] and O Donovan et al. 
[2009]. O DONOVAN, J., WARD, J., HODGINS, S., AND SUNDSTEDT, V. 2009. Rabbit Run: Gaze and Voice Based 
Game Interaction. In Eurographics Ireland Workshop, December. O DONOVAN, J. 2009. Gaze and Voice Based 
Game Interaction. University of Dublin, Trinity College. Master of Computer Science in Interactive Entertainment 
Technology. WILCOX, T., EVANS, M., PEARCE, C., POLLARD, N., AND SUNDSTEDT, V. 2008. Gaze and Voice Based 
Game Interaction: The Revenge of the Killer Penguins. In SIGGRAPH 08: ACM SIGGRAPH 2008 posters, ACM, 
New York, NY, USA, 1 1.  Slide 46 Gaze and voice in gaming Can the Midas touch problem be overcome 
by combining voice recognition with gaze? Hands-free method of game interaction Positive implications 
for disabled users Novel game features Gaze inspired algorithms exploit the perception of the observer 
and in doing so enhancing their experience  Given that gaze and voice are entirely hands-free it can 
also present a real opportunity for disabled users to interact fully with computer games. I will now 
talk a little about two of our case studies which have explored if the Midas touch problem can be overcome 
by combining voice recognition with gaze to achieve a completely hands-free method of game interaction. 
We have also investigated different ways of using gaze and voice to develop novel game features, some 
of which will be presented here.  Slide 47 The revenge of the killer penguinsImages from Wilcox et 
al. (2008).  Our first case study, The Revenge of the Killer Penguins, is a third person adventure puzzle 
game using a combination of non intrusive eye tracking technology and voice recognition for novel game 
features [Wilcox et al. 2008]. The first game consists of one main 3rd person perspective adventure puzzle 
game and two first person sub-games, a catapult challenge and a staring competition, which use the eye 
tracker functionality in contrasting ways. I will talk a little bit more about the different features 
available in these games. WILCOX, T., EVANS, M., PEARCE, C., POLLARD, N., AND SUNDSTEDT, V. 2008. Gaze 
and Voice Based Game Interaction: The Revenge of the Killer Penguins. In SIGGRAPH 08: ACM SIGGRAPH 2008 
posters, ACM, New York, NY, USA, 1 1.  Slide 48 Design tools Game framework in C++ and using Ogre3D 
Driver objects for the tracker and speech input TET Components API, Tobii Eye Tracker SDK Microsoft Speech 
SDKX50 eye tracker, Tobii.Author: Veronica Sundstedt  The game framework was written in C++ and implemented 
using Ogre3D. This framework included driver objects for the eye tracker and speech input, which involved 
implementing the TET Components API provided by the Tobii Eye Tracker SDK and the Microsoft Speech SDK 
respectively.   Slide 49 Inspiration Monkey Island 1, The Secret of Monkey Island, Lucasarts  As 
inspiration for the game we chose the bright and colourful style of the Monkey Island series by Lucasarts. 
These games are point and click style type games which we thought would suit eye tracking and voice commands 
well. For example in these games the player uses the mouse to point at actions to do and also the objects 
to interact with. Mouse clicks are also used to walk longer distances in the game environment.  Slide 
50 Catapult and staringImages from Wilcox et al. (2008).  To play the catapult game, the user can simply 
look at the target and voluntary blink to fire a projectile towards the object under the crosshair. Voice 
commands could be used to initiate the power bar increments and launch the projectile as an alternative 
option.  Slide 51 SmartText Text is displayed on the screen until it has been detected that the player 
has read itImage from Wilcox et al. (2008).  Another feature is SmartText in which dialogue and text 
are displayed on the screen until it has been detected that the player has read it. Other features include 
detecting voluntary blinks and winks to be used as controls. The idea behind SmartText is that people 
read at a massive range of different speeds which makes it a problem attaching a time limit to how long 
a text should be displayed for. The benefits of SmartText are that a player can look away from the screen, 
be a slow reader or fast and impatient and the text will display long enough for them to read it and 
no longer, guaranteeing they get the information.  Slide 52 Controls Look, pick up, walk, speak, use 
Example: Look at a pot (highlighted in yellow) while issuing a voice commandImage from Wilcox et al. 
(2008).  There are two different modes of control in the main game. The user can select objects by looking 
at them and perform look, pickup, walk, speak, use and other commands by vocalizing the respective words. 
For example, the figure shows how the behaviour of a virtual character can be controlled by looking at 
a pot (highlighted in yellow) and issuing a voice command. Alternatively, they can perform each command 
by blinking and winking at objects. The main menu or inventory overlays will also be displayed upon command. 
  Slide 53 Lessons learned Selection lag Calibration issues Snap cursor to objects nearby Smoothing 
filter for cursor Details Design objects to be more distinct  A trend identified in testing was that 
users would look towards a new target whilst attempting to apply a vocal command on a previously selected 
object. By implementing a lag in selecting items, we were able to solve this issue without impeding the 
user s natural motions by providing the user with enough time to voice any command. This selection lagging 
has therefore contributed to the natural aspect of input system. Our experience with the system was that 
sometimes the calibration could become poor resulting in users not being able to select objects easily. 
By designing levels with careful attention to camera positions, size and the locations of selectable 
objects we were able to mitigate this problem. Building scenes for the purposes of eye selection required 
careful positioning of cameras and making selectable objects larger and spaced apart. A smoothing filter 
was also used to compensate for the fact that the eye is never static resulting in an unstable cursor. 
A small focus group was used during development to investigate how gaze and voice can benefit from each 
other. This also helped us identify issues that emerged from this combination. We found that when designing 
a point-and-click style game for use with the eye tracker, aesthetics becomes ergonomics as scenes and 
selectable elements must be laid out and scaled for optimal usage; buttons must be bigger and targets 
must be carefully positioned in order to produce an effective system.  Slide 54 Rabbit RunImages from 
O Donovan et al. (2009).  This case study is the result of the M.Sc. (Interactive Entertainment Technology) 
undertaken by my student Jonathan O Donovan. The purpose of the project was to create a novel game evaluation 
framework that could be controlled by input from gaze and voice as well as mouse and keyboard. This framework 
was evaluated both using quantitative measures and subjective responses from participant user trials. 
This is the first evaluation study performed comparing gaze and voice against mouse and keyboard input 
[O Donovan et al. 2009]. The main result indicates that, although game performance was significantly 
worse, participants reported a higher level of immersion when playing using gaze and voice. J. O Donovan, 
J.Ward, S. Hodgins, and V. Sundstedt, Rabbit Run: Gaze and Voice Based Game Interaction, in Eurographics 
Ireland Workshop (to appear), Dec 2009.  Slide 55 Game concept Easy to understand Player must escape 
a rabbit warren maze Goal: navigate to the exit in the shortest possible time Challenging to bare relevance 
to a real game Inclusion of common gaming tasks Navigating Collecting coins Shooting rabbits Selecting 
objectsImages from O Donovan et al. (2009).  The game needed to be relatively simple so users could 
understand it quickly and finish within a reasonable time frame. It also needed to include common gaming 
tasks, such as navigation and object selection. The premise decided upon was Rabbit Run. The player is 
trapped in a rabbit warren, inhabited by evil rabbits, from which he/she must escape. The main objective 
is to navigate through the warren maze and find the exit in the shortest time possible. To earn extra 
points coins distributed throughout the maze could be collected. In order to pass by the evil rabbits 
they needed to be shot. Once the exit was reached the game ended. A map would also be provided (upon 
a key press or voice command depending on the input) in order to assist players finding their way through 
the maze. Purpose built for evaluation tests in a controlled manner Storage of relevant game data 
Was decided to put a time limit on game  Slide 56 Design tools Tobii T60 eye tracker Integrated monitor 
Tobii SDK COM objects, gaze data retrieval, calibration Voice Recognition Microsoft Speech API Game Development 
.NET allows for communication with COM objects XNA (First use of XNA with gaze based interaction)  The 
game evaluation framework was developed using Microsoft XNA. The framework allowed the game to be controlled 
by mouse/keyboard (MK), gaze/voice (GV), mouse/voice (MV), and gaze/keyboard (GK). Ultimately only MK 
and GV were tested in the user evaluation. A menu system was also provided controllable by MK and GV. 
All relevant game data such as shots fired and coins collected were stored in XML format once a game 
trial was completed. Calibration and processing of gaze data from the Tobii T60 eye tracker was accomplished 
using the Tobii SDK. Upon start of the application all previous calibration data was cleared and a new 
calibration was initiated. Once calibrated real-time gaze data was relayed to the application. The gaze 
data includes the area being looked at on screen by both the left and right eyes and the distance of 
both eyes from the screen. This information was averaged to give the most likely POR. Microsoft Speech 
SDK 5.1 was used to implement voice recognition. A few problems were encountered with the voice recognition 
in an early user test. More intuitive voice commands were not always recognised so instead more distinct 
voice commands were chosen. For example, instead of saying Map to bring up the game map, Maze had to 
be used instead. When selecting menu items Select also proved to be inconsistent so the command Option 
was used instead.  Slide 57 Game controls Mouse/Keyboard Arrow keys used to move player Change game 
view using mouse Gaze/Voice Voice commands to move player Gaze Camera used to change game view  Navigation 
for MK in the game environment was implemented using the arrow keys. Players could move forwards, backwards, 
left, or right relative to the direction the camera was pointing. This updated the position at a walking 
pace. If players wanted to increase their speed to a run pace they needed to hold down the shift key. 
Navigation with GV was achieved using three commands Walk , Run and Stop . When the Walk command was 
issued the camera proceeded to move, at a walking pace, in the direction the camera was facing until 
it encountered an obstacle, such as a wall, a rabbit, or the Stop command was issued. The Run command 
worked in a similar way except at a faster pace. Mouse/Keyboard Arrow keys used to move player Change 
game view using mouse  Gaze/Voice input was done in the following way Voice commands to move player 
 So Walk / Run to move player Continuous movement required so users would not have to keep repeating 
voice commands Stop to halt movement Gaze Camera used to change game view which I will go into now 
on the next slide   Slide 58 Gaze camera Ideas of Kenny et al. and Castellina and Corno Virtual Keys 
to change camera view Alpha-blended buttons Outer area of viewing spacing to minimise impact on gameplay 
Recall 82% of game time looking in the near centre of the screen Only displayed when activated by gaze 
Decouples aiming from camera movementImages from O Donovan et al. (2009).  Given that the game was implemented 
in the first person perspective, the cameras in the game played a dual role of showing the game play 
and acting as the player s avatar. The position of the camera was checked for collisions with the surrounding 
game objects. The camera implementation differed between the GV and MK input types. The MK camera works 
like most FPS cameras where the target is at the centre of the screen and the mouse movement is used 
to shift the camera in a given direction. For targeting rabbits to shoot a cross hairs was rendered at 
the current position of the mouse, which is almost always at the centre of the screen. The GV camera 
builds upon the idea of Castellina and Corno [2008]. They used semitransparent buttons to rotate the 
camera and move the avatar. As shown by Kenny et al. [2005] the vast majority of game time in a FPS games 
is spent looking in the inner 400 x 300 rectangle of a 800 x 600 resolution screen. Our implementation 
is based on this idea by using the outer rectangle of the screen to place the semitransparent gaze activated 
buttons. The inner rectangle could be left alone to allow normal game interaction. By placing the buttons 
in this outer area it was hoped that they would not interfere with game play. The buttons would not be 
displayed unless activated, by looking in that area of the screen, to avoid distracting the player. The 
purpose of the buttons was to rotate the camera in a given direction. By looking at the left and right 
part of the screen the camera would shift left and right respectively. The original idea had been to 
place the buttons in such a way as to form an eight sided star. CASTELLINA E., CORNO F.: Multimodal 
Gaze Interaction in 3D Virtual Environments. In Proceedings of the 4th COGAIN Annual Conference on Communication 
by Gaze Interaction, Environment and Mobility Control by Gaze (2008). KENNY A., KOESLING H., DELANEY 
D., MCLOONE S., WARD T.: A Preliminary Investigation into Eye Gaze Data in a First Person Shooter Game. 
In Proceedings of the 19th European Conference on Modelling and Simulation (2005).  Slide 59 Gaze camera 
Building on idea of Castellina and Corno Created two gaze activated semi-transparent buttons Allow camera 
to move left or right Such buttons should have minimum impact since most game play occurs in near centreImages 
from O Donovan et al. (2009).  An early user test showed that the star gaze camera buttons were difficult 
to use. The camera rotated when the user did not want it to, causing it to spin in a disorientating manner. 
After feedback this was simplified to only use the left and right arrows. Original eight-sided star gaze 
camera is shown on the left. This was abandoned in favour of a simpler gaze camera with only left and 
right arrows. The buttons act as a visual aid to the player indicating the direction the camera was shifting. 
For targeting rabbits a cross hairs was displayed using the current POR as screen coordinates. This separated 
the targeting from the camera view much in the same way as Jnsson [2005] did in her Half Life demo. 
 JNSSON E.: If Looks Could Kill - An Evaluation of EyeTracking in Computer Games. Master s thesis, KTH 
Royal Institute of Technology, Sweden, 2005.  Slide 60 Gaze map Helps the player find the way in the 
environment Mouse Map Generator Displays where the player has been in the game Gaze Map Generator Displays 
what the player has seen in the gameImages from O Donovan et al. (2009).  A map was provided in order 
to assist the players navigate the warren. The map was generated on the fly showing the places where 
the player had been in the warren. There was a subtle difference between its implementation using MK 
versus GV. For MK the map was only updated with the coordinates of where the player currently was. So 
if the player travelled to a new location that location would be revealed on the map. A novel game feature 
was used for GV input. In this case locations were revealed based on where the player had looked. So 
if the player looked at a particular location that area would be shown on the map without requiring the 
player to physically move to those positions. This novel game feature could be useful in puzzle based 
games where the player needs to memorise parts of the virtual environment or objects seen.  Slide 61 
 User evaluation 8 participants involved in the trial Participants given same explanatory material To 
ensure instructions were similar for each participant Quantitative data from game storage while game 
was played Subjective data from questionnaires were collected after each trial  A user evaluation was 
performed to evaluate how suitable GV is as a means of video game control compared to MK. This meant 
that each participant would need to play the game using both modes of interaction. One of the main objectives 
of the user study was to gather both quantitative measures and subjective comments. Because the game 
could be controllable by MK as well as GV it facilitated direct comparisons between the two methods of 
interaction. In addition to saved game data questionnaires were given to the participants to ascertain 
subjective data, such as how effective GV was perceived to be and how immersive or entertaining the experience 
was. Participants were sought by recruiting volunteering postgraduates and staff in the college. Ten 
participants (8 men and 2 women, age range 22-33) with normal or corrected to normal vision were recruited 
for the user evaluation. As the eye tracker occasionally loses calibration during a trial, especially 
for participants wearing glasses, trials which clearly produced unreliable data were removed. In total 
eight users successfully completed their trials. The participants had a variety of experience with computer 
games. Each participant played the game using both GV and MK. Half the participants played the GV game 
first, while the other half played the MK first. The participants were first asked to play a demo version 
of the game using the selected interaction method. They were allowed to play the demo version of the 
game for as long as they wanted.   Slide 62 Equipment and setup Tobii binocular T60 eye tracker 60cm 
away from the user 17 inch TFT, 1280 x 1024 pixels 0.5 degrees accuracy, 60Hz Stand-alone unit No restraints 
on the user Sound proof video conference room To negate any possible ill effects of background noiseT-series 
eye tracker, Tobii.Author: Tobii  The Tobii binocular T60 eye tracker was placed at a distance of 60 
cm away from the user. The T60 is a portable stand-alone unit, which puts no restraints on the user. 
The freedom of head movement is 44 x 22 x 30 cm. The eye-tracker is built into a 17 inch TFT, 1280 x 
1024 pixels monitor. The eye-tracker has an accuracy of 0.5 degrees and has a data rate of 60 Hz. This 
degree of accuracy corresponds to an error of about 0.5 cm between the measured and actual gaze point 
(at a 60 cm distance between the user and the screen). An adjustable chair was provided to allow participants 
to make themselves comfortable and place themselves at the correct distance from the monitor. The user 
trial took place in a sound proof video conferencing room. This was to avoid any interference background 
noise might have on voice recognition. The hardware setup consisted of a laptop running the application 
while connected to the T60 eye tracker via an Ethernet connection. A keyboard, a mouse, and a microphone 
headset were also connected to the host laptop to allow for keyboard, mouse and voice input. The lighting 
was dimmed throughout the experiment. A calibration was carried out for each participant, prior to each 
trial, to ensure that the collected data would be reliable.  Slide 63 Procedure Consent form and background 
questionnaire Age, profession, gaming habits Instruction pamphlet Demo version Questionnaires  Upon 
arrival each participant were asked to fill in a consent form and answer a short background questionnaire 
about age, profession, and their gaming habits. After this the participants read a sheet of instructions 
on the procedure of the particular game they would play. It was decided that an instruction pamphlet 
would be used to inform all users in a consistent way. The participants were first asked to play a demo 
version of the game using the selected interaction method. They were allowed to play the demo version 
of the game for as long as they wanted. They were also encouraged to ask any questions they might have 
at this stage. Once satisfied with the controls participants were asked to complete the full user trial 
version of the game. Immediately after playing each trial, participants were asked to answer a second 
questionnaire. This questionnaire aimed to get the participants subjective opinions on that particular 
input. After the second and final game was played (and the post-trial questionnaire was completed) a 
third and final questionnaire was given to the participants. This final questionnaire aimed to compare 
the two different games played by the participant to gauge which one they preferred. An open question 
at the end invited comments from the participants.  Slide 64 Maze designImages from O Donovan et al. 
(2009).  The game was designed to bear relevance to a real game, while being controlled enough to allow 
for analysis. Each game needed to be played twice in order to yield comparable results. It was decided 
that the exact same layout would be used in each trial by only swapping the start and exit points. The 
different start and end points would eliminate the possibility of skewed results from learning. The exact 
same number of coins and rabbits were used, distributed in the same positions in each setup.  Slide 
65 Results Statistics from quantitative measures Values in bold indicate no significant difference 
 Table shows quantitative measures recorded while participants played both versions of the game Mouse/Keyboard 
performs better on most* although there was no significant difference for Rabbits shot Shooting accuracy 
 Map references  *I ll go into the reasons for performance difference in a moment  Slide 66 Results 
 Statistics for each subjective measure. Values in bold indicate no significant differences Note the 
mean score is higher for gaze/voice than mouse/keyboard in the immersive ratings The second table shows 
the subjective data (gathered using questionnaires) Again mouse/keyboard was deemed easier than gaze/voice 
on most measures  Although no significant difference was given between the following measures: Easy 
of shooting rabbits Game enjoyment Map usefulness  Significantly however participants deemed that 
gaze/voice was more immersive  For more information regarding the results and statistical analysis 
we refer the reader to [O Donovan et al. 2009]. J. O Donovan, J.Ward, S. Hodgins, and V. Sundstedt, 
Rabbit Run: Gaze and Voice Based Game Interaction, in Eurographics Ireland Workshop (to appear), Dec 
2009.  Slide 67 Results The main result indicates that, although game performance was significantly 
worse, participants reported a higher level of immersion when playing using gaze and voice Menu and game 
navigation, control and precision, coin collection, effort, difficulty Shooting rabbits, map usefulness 
 Immersive Gaze/voice was ranked more immersive. Game enjoyment No statistically significant difference 
between rankings Which was more enjoyable? 75% of participants selected gaze/voice as more enjoyable 
to play. Biggest issue encountered was the collision response It was decided to stop the motion of 
the player should they collide with a wall This was worked ok for mouse keyboard since participants 
simply adjusted with the arrow keys However for voice control this was awkward since participants had 
to Adjust the gaze camera Then reissue voice commands  Game allowed hands free control obviously 
significant for disabled users for which regular means of input (such as mouse/keyboard) may not be feasible 
  Although adapting open source games reduces the implementation time required, these games were originally 
created with mouse and keyboard in mind. So the adapted game is restricted to using gaze as a replacement 
for the mouse rather than an input device in its own right, with the gaze input acting only as a mouse 
emulator. When a game is developed from scratch it should be free of this restriction. Improve collision 
response to allow for smoother voice/gaze control Slide 68 Future directionsGaze Voice Games: Our study 
suffered from mouse emulation Future work could exploit the use of gaze and voice in the real world to 
create novel and exciting ways to interact with video games The game experience itself could be measured 
using a game experience questionnaire  In terms of future research into gaze voice games one of the 
lessons from this study is that of mouse emulation: By this I mean that the gaze acts as a mouse/mouse 
emulator. When a game is developed from scratch it should be free of this restriction. However perhaps 
the goal of comparing gaze and voice directly against mouse and keyboard unwittingly manifested this 
idea on the project.  Future work in this area could look at how we use gaze and voice in the real 
world to create some novel/exciting ways to interact with video games. For example hide-and-seek. 
 To help avoid this I would suggest that measurement of such a game would be conducted in terms of a 
game experience questionnaire rather than attempting to directly comparing mouse/keyboard VERSUS gaze/voice 
   Slide 69 Future directionsHead Tracking &#38; Gaze Input: Investigate how Natural Point s TrackIRsystem 
could be used in conjunction with gaze data to reduce camera motion issues  Some users experienced issues 
when rotating the camera using Virtual Keys  Ways around this could include: Blinking etc could be 
used to help the interface OR Using Head Tracking device such as Natural Point s TrackIR head direction 
to control the look direction in-game  Head movements seemed to be a suitable interaction method for 
controlling the field of view but poor for aiming a weapon.  Slide 70 Future work Discussion of future 
research and directions  Future interaction with computer games is likely to be made using novel multimodal 
interaction techniques. We believe that alternative input modalities can be used in novel ways to enhance 
gameplay. I will now discuss some ideas for interesting future research.  Slide 71 Crowd behavior 
Crowd behaviours based on the player Assassin s CreedTM, Ubisoft Advanced crowd interaction Crowd interacts 
with the player Social rulesMetropolis Project, GV2  Computer games have also started to provide more 
advanced crowd interaction, such as in Assassins Creed in which the crowd interacts with the player (based 
on its position for example). I believe this is an interesting area of research, combining gaze input 
to interact with crowds. For example you could allow virtual humans to interact with objects, navigate 
through environments, and interact with other characters (enemies) and crowds. Another area worth investigating 
is how the animation and AI of game characters could be adapted to react to the player s gaze and voice. 
More socially realistic scenarios, based on social rules, could be created if game characters responded 
to the voice and gaze of the player.  Slide 72 Memory algorithms This idea could be extended further 
to equip virtual characters with novel memory algorithms constructed based on eye movement information 
Hints based on what has been seen / not seen Abilities to remember which objects to use (inventory), 
where to go (map)  By gathering information about player eye movements we can store information on what 
the player has seen in a game. Maybe this idea could be used to equip virtual characters with novel memory 
algorithms.   Slide 73 Player state Players interest What are they looking at? How can this information 
be used to create novel game features? Pupil size Engaged, scared, stressed Skin sensors  Player s interest 
What are they looking at? How can this information be used to create novel game features? Pupil size 
Engaged, scared, stressed Skin sensors   Slide 74 Other issues Sounds Low cost solutions Calibration 
Accessibility  Explore possible effects of background noise on voice recognition. User evaluation held 
in sound proof room. No game sounds were used. There is still a great need for cheap, accurate, and 
novel solutions which could be used for mass-market products. These low cost solutions need to be accurate, 
non-intrusive, and deal with different gaming setups and game genres. Alternative means of interaction 
in games are especially important for disabled users for whom traditional techniques, such as mouse and 
keyboard, may not be feasible. More work is needed in how to make computer games content more accessible 
to all users.  Slide 75 Further resources COGAIN: Communication by Gaze Interaction http://www.cogain.org/ 
ETRA 2010: Eye Tracking Research and Applications http://etra.cs.uta.fi/ APGV 2010: Symposium on Applied 
Perception in Graphics and Visualisation http://www.apgv.org/ TAP: ACM Transactions on Applied Perception 
http://tap.acm.org/  Here are some relevant conferences and journals for more related work. These are 
some additional references for related work: Bulling, A., Roggen, D., and Trster, G. (2008) EyeMote 
- Towards Context-Aware Gaming Using Eye Movements Recorded from Wearable Electrooculography. In the 
Second International Conference on Fun and Games, LNCS 5294. pp 33-45. Springer. Charlotte Sennersten 
and Craig Lindley (2008) Evaluation of Real-time Eye Gaze Logging by a 3D Game Engine. In Proc. 12th 
IMEKO TC1 &#38; TC7 Joint Symposium on Man Science &#38; Measurement, Annecy, 2008. Lee, Eui-Chul and 
Park, Kang-Ryoung (2005) The KIPS transactions. Part B, Volume b12, Issue 4, Korea Information Processing 
Society (August 2005, ISSN 1598-284x), pp. 465-472. 2D Games: EyeDraw: A System for Drawing Pictures 
with Eye Movements [Hornof et al. 2004] EyeChess: the tutoring game with visual attentive interface [Spakov 
2005] Gaze vs. Mouse: An evaluation of user experience and planning in problem solving games [Gowases 
2007] Eyegaze communication system by LC Technologies [Isokoski et al. 2009] Paddle games (PONG style), 
Scour Four, Mahjong MyTobii system by Tobii Technology Minesweeper, Gobblet Chess, tic-tac-toe, and Lines 
 Gowases, T. (2007) Gaze vs. Mouse: An evaluation of user experience and planning in problem solving 
games. Master s thesis May 2, 2007. Department of Computer Science, University of Joensuu, Finland. 
Gowases, T., Bednarik, R., and Tukiainen, M. (2008) Gaze vs. Mouse in Games: The Effects on User Experience. 
In Proceedings of the International Conference on Computers in Education, ICCE 2008, pp. 773-777. Hornof, 
A., Cavender, A., &#38; Hoselton, R. (2004). EyeDraw: A System for Drawing Pictures with Eye Movements. 
Proceedings of the ACM SIGACCESS Conference on Computers and Accessibility. ISOKOSKI, P., JOOS, M., 
SPAKOV, O., AND MARTIN, B. 2009. Gaze controlled games. Universal Access in the Information Society. 
 Spakov, O. (2005). EyeChess: the tutoring game with visual attentive interface. Alternative Access: 
Feelings and Games 2005, Department of Computer Sciences, University of Tampere, Finland.  Slide 76 
 Acknowledgements I would like to thank Jonathan O Donovan, Tom Wilcox, Mike Evans, Chris Pearce, Nick 
Pollard, Diego Gutierrez, Oscar Anson, Erik Reinhard, Marina Bloj, Mary Whitton, Carol O Sullivan, and 
Trinity College (GV2) for their collaboration on the research and ideas presented in this course Jon 
Ward and Scott Hodgins, Acuity ETS Ltd. for the loan of the Tobii T60 eye tracker University of Bristol, 
Blekinge Institute of Technology Stephen Spencer   Slide 77 Thank you -Questions and answersImages 
from Wilcox et al. (2008) and O Donovan et al. (2009).Contact:Veronica SundstedtBTHveronica.sundstedt@bth.sehttp://www.scss.tcd.ie/Veronica.Sundstedt/gazingatgames2010.html 
 Thank you for attending the course: Gazing at Games: Using Eye Tracking to Control Virtual Characters. 
I would now be happy to answer any questions you might have. I would also like to encourage any discussion 
around novel multi-modal interaction techniques for games, in particular of using gaze as input to control 
games. If you would like any further information please do not hesitate to contact me on: Veronica Sundstedt, 
Blekinge Institute of Technology, Karlskrona, Sweden (email: veronica.sundstedt@bth.se). The course material 
can also be found online at: http://www.scss.tcd.ie/Veronica.Sundstedt/gazingatgames2010.html  Slide 
78   SIGGRAPH 2010 The People Behind the Pixels   10Logo_A.jpg s10logoWeb450.jpg SIGGRAPH 2010 
 Gazing at Games: Using Eye Tracking to Control Virtual Characters Veronica Sundstedt Blekinge Institute 
of Technology, Sweden Trinity College Dublin, Ireland Wednesday, 28 July, 3:45 PM 5:15 PM s10logoWeb450.jpg 
Introduction Welcome Course aims Course structure Who am I? Motivation   Course aims Introduce 
the attendees to eye tracking, attention, and technologies for tracking gaze Present work done in gaze-controlled 
gamesand discuss issues relating to eye tracking Describe two case studiesin which gaze and voice have 
been used to control virtual characters and their behaviour Audience discussionaround novel multi-modal 
interaction techniques for games   s10logoWeb450.jpg Course structure Introduction (5 min) Eye tracking 
(15 min) Related work (25 min) Case studies (30 min) Future work (5 min) Q&#38;A (10 min) Finish 
(5:15PM)  representative_image.jpg Images from Wilcox et al. (2008) and O Donovan et al. (2009).  
Who am I? BTH-logga.gif v.JPG pixel_logo.png GV2, Trinity College Dublin http://gv2.cs.tcd.ie/ Veronica 
Sundstedt -veronica.sundstedt@bth.se Blekinge Institute of Technology http://www.bth.se/eng  Motivation 
 Mice, keyboards, specific game controllers  Recently alternative input modalities have emerged as a 
means of interacting with computer games Motion sensing, gesture recognition, pointing, sound   Examples 
 PlayStation_Eye.jpg EyeToy_camera_1.jpg EyeToy camera, PlayStation 2. Author: Dave Pape PlayStation 
Eye, PlayStation 3.Author: Ryoichi Tanaka wiggling.net Examples Wii_Remote_Image.jpg Wii remote, Nintendo.Author: 
Greyson Orlando Project_Natal_Camera.jpg Project Natal, Xbox360, Microsoft. Author: Jake Metcalf  Examples 
Tobii_T60_T120_Eye_Tracker_Perspective.jpg X50 eye tracker, Tobii. Author: Veronica Sundstedt T-series 
eye tracker, Tobii.Author: Tobii EyeLink II eye tracker Author: McDonnell et al. [2009]  s10logoWeb450.jpg 
Eye Tracking What do we see? What is eye tracking? Measuring eye movements Eye tracking technology 
  What do we see?  Schematic representation of the human visual field. Adapted from MacEvoy [2007]. 
 MacEvoy B. The Visual Field. http://www.handprint.com, 2007. Eye movements Five basic types Saccades 
 Smooth pursuits Vergence Vestibular ocular reflex (VOR) Optokinetic reflex (OKR)  Fixations 200-300 
ms, 90% of viewing time  Scan paths Trajectory between fixation points   lucy.bmp Visual attention 
Combination of two main processes Bottom-up and top-down visual search Models of visual attention 
.Task maps, saliency maps, importance maps etc.   task2 saliency2 added 1_1 Bottom-up processing 
Corridor scene: (left) high quality and (right) saliency map using the algorithm by Itti et al. (1998). 
 Top-down processing  Images used with permission from Springer Science and Business Media.  What is 
eye tracking? Eye tracking allows us to determine where an observers gaze is focused at a given time 
   Example eye movements from an observer free-viewing and performing a counting task.  Eye tracking 
applications  Eye tracking systems Interactive Diagnostic Selective Gaze-contingent Screen-based Model-based 
   The figure below is adapted from Duchowski [2001].  Eye tracking technology Video-based Head-mounted 
 Portable Wearable Remote  Electronic Skin electrodes  Mechanical Lenses   X50 eye tracker, 
Tobii.Author: Veronica SundstedtAccuracy: 0.5-0.7 degreesSpatial resolution: 0.35 degreesTemporal resolution: 
50 Hz Video-based tracking (Left): Purkinje images and (right) relative positions of pupil and first 
Purkinje images as seen by the eye-tracker s camera during calibration. Adapted from Duchowski (2003) 
and Rih (2005). Calibration Grid point calibration Calibration issues Using a chin rest Re-calibrate 
between trials  calibration.jpg ClearView calibration, Tobii. Author: Veronica Sundstedt  Eye trackers 
http://www.cogain.org/wiki/Eye_Trackers Eye trackers for interactive applications Eye trackers for 
research on eye movements Open source, low cost, and freeware solutions   s10logoWeb450.jpg Recording 
issues Physiological reasons Pupil Eyelid  External reasons Eyeglasses Contacts   UNIBRISsetup 
Setup with the Tobii x50 tracker.Author: Veronica Sundstedt Raw data Type Description Timestamp Timestamp 
in milliseconds for when the gaze data was collected Gaze PointX Left Horizontal screen position of 
the gaze point for the left eye Gaze PointY Left Vertical screen position of the gaze point for the 
left eye CamXLeft Horizontal location of the left pupil in the camera image CamYLeft Vertical location 
of the left pupil in the camera image DistanceLeft Distance from the eye tracker to the left eye PupilLeft 
Size of the pupil (left eye) in mm ValidityLeft Validity of the gaze data (e.g. 0 = good tracking quality) 
 Source [Tobii 2006].  Midas touch Everywhere one looks, another command is activated; the viewer cannot 
look anywhere without issuing another command [Jacob 1990] Mouse click Dwell time   403px-Midas_gold2.jpg 
King Midas with his daughter.Author: Walter Crane (Illustrator) s10logoWeb450.jpg Background and related 
work Seminal work Voice recognition in gaming Attention in gaming Gaze in gaming   Seminal work 
Starker and Bolt [1990] Gaze responsive self closing display Introduced one of the first systems that 
had real-time eye tracking and intentionally constructed storytelling   I. Starker and R. Bolt, A Gaze-Responsive 
Self-Disclosing Display," in CHI '90. Voice recognition in gaming  Original summary by Speech Technologies 
Make Video Games Complete [2005]. Recreated from O Donovan [2009]. Attention in gaming The avatar of 
the player can direct attention to important objects or approaching enemies by looking at them This 
can aid the player in puzzle solving  The Legend of Zelda: The Wind WakerTM, Nintendo Attention in 
gaming Sennersten [2004] action game tutorial Kenny et al. [2005] fps games, focus in center El-Nasr 
and Yan [2006] top-down &#38; bottom-up Sennersten et al. [2007] eye tracker + HiFi engine Peters and 
Itti [2007,2008] task/saliency Stellmach [2007], Sasse [2008] psychophysiological logging, data acquisition 
 Sundstedt et al. [2008] active vs. passive   Gaze in gaming resources COGAIN Communication by Gaze 
Interaction http://www.cogain.org/links/gaze-controlled-games  Survey paper Isokoski P., Joos, M., 
Spakov, O., &#38; Martin, B. (2009). Gaze Controlled Games. Universal Access in the Information Society 
8(4). Springer   Leyba and Malcolm [2004] Mouse vs. gaze to aim Remove 25 balls with random velocity 
vectors MAGIC pointing was used Hypothesis: gaze less accurate but task could be performed faster 
Mouse was more accurate and faster   Jnsson [2005] Gaze input in two open source games Sacrifice 
(Shoot-em-up) Half-Life (FPS)  Midas Touch resolved using mouse click Half Life decoupled aiming from 
camera movement Higher scores using gaze in Sacrifice Eye control was more fun   Kenny et al. [2005] 
Playing a FPS while recording gaze data Players spend 82% of game time looking in the near centre of 
the screen Inner 400 x 300 rectangle of a 800 x 600 screen   Smith and Graham [2006] Gaze control 
for three games Quake 2 (FPS) orientation control Neverwinter Nights (roleplay) avatar moves by pointing 
 Lunar Command (action/arcade) moving objects targeted through pointing  Performance and subjective 
data when playing using mouse and eye control Eye-based input can alter the gaming experience and make 
the virtual environment feel more immersive   Isokoski and Martin [2006] Created a FPS and compared 
three input devices Keyboard / mouse / gaze (to aim) Keyboard / mouse Xbox 360 controller  Eye tracker 
competitive with the gamepad Mouse/keyboard more efficient than the others   Isokoski et al. [2007] 
Same game as in 2006 Full gamepad control Moving with gamepad / aiming with eyes Steering and aiming 
with eyes Velocity control and shooting controlled with gamepad  Increasing eye control did not affect 
the performance (target hit), but they fire more shots   Dorr et al. [2007] Mouse vs. gaze in a paddle 
game 20 students playing each other in pairs Each player used both controls  The eye tracker had a 
statistically significant advantage over mouse control  breakout2.jpg Screenshot from Lbreakout2. 
Istance et al. [2008] Snap Clutch software tool Solution for the Midas Touch problem Using gaze data 
to generate keyboard and mouse events Program responds with these as input World of Warcraft Second 
Life   Castellina and Corno [2008] Created some simple 3D game scenarios to test multimodal input 
 Used Virtual Keys to move forward/backwards, to look left/right. Activated by dwell time  Gaze input 
was as accurate as other forms, but not as fast due to use of dwell time   Hillaire et al. [2008] q3.jpg 
Image courtesy: Hillaire et al. 2008 Quake 3 Arena screenshot courtesy of Id Software Hillaire et al. 
[2008] -Using an Eye-Tracking System to Improve Depth-of-Field Blur Effects and Camera Motions in Virtual 
Environments.  Ekman et al. [2008] Invisible Eni Gaze, blinking, pupil sizeto affect game state Pupil 
size affected by physical activation, strong emotional experiences, and cognitive effort Player controls 
game by the use of willpower Gaze direction move with response to gaze Gaze blinking disappear into 
a puff of smoke Pupil size open magic flowers (cognitive and emotional effort )    Istance et al. 
[2009] Software device using gaze input for emulating mouse and keyboard events Controlling on-line 
games (World of Warcraft) Feasible to carry out tasks at a beginners skill level using gaze alone Usability 
issues for three tasks Implications of only using gaze as input   Previous work summary Studies have 
looked at various aspects Different input devices, game genres, duration of play, eye trackers, game 
engines  Difficult to assess the significance of the results Performance vs. immersion  More research 
is needed   s10logoWeb450.jpg Case studies Case study I The Revenge of the Killer Penguins  Case 
study II Rabbit Run   Gaze and voice in gaming Can the Midas touch problem be overcome by combining 
voice recognition with gaze? Hands-free method of game interaction Positive implications for disabled 
users Novel game features  Gaze inspired algorithms exploit the perception of the observer and in doing 
so enhancing their experience   The revenge of the killer penguins Images from Wilcox et al. (2008). 
 Design tools Game framework in C++ and using Ogre3D Driver objects for the tracker and speech input 
 TET Components API, Tobii Eye Tracker SDK Microsoft Speech SDK   X50 eye tracker, Tobii. Author: 
Veronica Sundstedt  Inspiration Monkey Island 1, The Secret of Monkey Island, Lucasarts   Catapult 
and staring staringBox.png catapult.png Images from Wilcox et al. (2008). SmartText Text is displayed 
on the screen until it has been detected that the player has read it  D:\VERONICAS PAPERS\Papers\COGAIN2007\COGAIN2007\images\ss\labSmartCursor.png 
Image from Wilcox et al. (2008). Controls Look, pick up, walk, speak, use Example: Look at a pot (highlighted 
in yellow) while issuing a voice command  indoorHighlightLook.png Image from Wilcox et al. (2008). 
 Lessons learned Selection lag Calibration issues Snap cursor to objects nearby  Smoothing filter 
for cursor Details Design objects to be more distinct   Rabbit Run Bunny2 Images from O Donovan 
et al. (2009).  Game concept Easy to understand Player must escape a rabbit warren maze Goal: navigate 
to the exit in the shortest possible time Challenging to bare relevance to a real game Inclusion of 
common gaming tasks Navigating Collecting coins Shooting rabbits Selecting objects   Bunny2 Images 
from O Donovan et al. (2009).  Design tools Tobii T60 eye tracker Integrated monitor  Tobii SDK COM 
objects, gaze data retrieval, calibration  Voice Recognition Microsoft Speech API  Game Development 
 .NET allows for communication with COM objects XNA (First use of XNA with gaze based interaction) 
  Game controls Mouse/Keyboard Arrow keys used to move player Change game view using mouse  Gaze/Voice 
 Voice commands to move player Gaze Camera used to change game view   Gaze camera Ideas of Kenny 
et al. and Castellina and Corno Virtual Keys to change camera view Alpha-blended buttons Outer area 
of viewing spacing to minimise impact on gameplay Recall 82% of game time looking in the near centre 
of the screen  Only displayed when activated by gaze Decouples aiming from camera movement   Gaze 
Cam2 Images from O Donovan et al. (2009). Gaze camera Building on idea of Castellina and Corno Created 
two gaze activated semi-transparent buttons Allow camera to move left or right  Such buttons should 
have minimum impact since most game play occurs in near centre   Images from O Donovan et al. (2009). 
 s10logoWeb450.jpg Gaze map Helps the player find the way in the environment Mouse Map Generator Displays 
where the player has been in the game  Gaze Map Generator Displays what the player has seen in the 
game   Map Images from O Donovan et al. (2009).  User evaluation 8 participants involved in the trial 
 Participants given same explanatory material To ensure instructions were similar for each participant 
 Quantitative data from game storage while game was played Subjective data from questionnaires were 
collected after each trial   Equipment and setup Tobii binocular T60 eye tracker 60cm away from the 
user 17 inch TFT, 1280 x 1024 pixels 0.5 degrees accuracy, 60Hz  Stand-alone unit No restraints on 
the user  Sound proof video conference room To negate any possible ill effects of background noise 
  T60 T-series eye tracker, Tobii.Author: Tobii Procedure Consent form and background questionnaire 
 Age, profession, gaming habits  Instruction pamphlet Demo version Questionnaires   Maze design 
 Images from O Donovan et al. (2009). Results Statistics from quantitative measures Values in bold 
indicate no significant difference   Results  Results The main result indicates that, although game 
performance was significantly worse, participants reported a higher level of immersion when playing using 
gaze and voice Menu and game navigation, control and precision, coin collection, effort, difficulty 
 Shooting rabbits, map usefulness   Future directions Gaze Voice Games: Our study suffered from mouse 
emulation Future work could exploit the use of gaze and voice in the real world to create novel and 
exciting ways to interact with video games The game experience itself could be measured using a game 
experience questionnaire   Future directions Head Tracking &#38; Gaze Input: Investigate how Natural 
Point s TrackIRsystem could be used in conjunction with gaze data to reduce camera motion issues   
Future work Discussion of future research and directions   s10logoWeb450.jpg Crowd behavior Crowd behaviours 
based on the player Assassin s CreedTM, Ubisoft Advanced crowd interaction Crowd interacts with the 
player  Social rules   ScreenShot2.jpg Metropolis Project, GV2 Memory algorithms This idea could 
be extended further to equip virtual characters with novel memory algorithms constructed based on eye 
movement information Hints based on what has been seen / not seen Abilities to remember which objects 
to use (inventory), where to go (map)   Player state Players interest What are they looking at? 
How can this information be used to create novel game features?  Pupil size Engaged, scared, stressed 
 Skin sensors   Other issues Sounds Low cost solutions Calibration Accessibility   s10logoWeb450.jpg 
Further resources COGAIN: Communication by Gaze Interaction http://www.cogain.org/  ETRA 2010: Eye 
Tracking Research and Applications http://etra.cs.uta.fi/  APGV 2010: Symposium on Applied Perception 
in Graphics and Visualisation http://www.apgv.org/  TAP: ACM Transactions on Applied Perception http://tap.acm.org/ 
   s10logoWeb450.jpg Acknowledgements I would like to thank Jonathan O Donovan, Tom Wilcox, Mike Evans, 
Chris Pearce, Nick Pollard, Diego Gutierrez, Oscar Anson, Erik Reinhard, Marina Bloj, Mary Whitton, Carol 
O Sullivan, and Trinity College (GV2) for their collaboration on the research and ideas presented in 
this course Jon Ward and Scott Hodgins, Acuity ETS Ltd. for the loan of the Tobii T60 eye tracker University 
of Bristol, Blekinge Institute of Technology Stephen Spencer   Thank you -Questions and answers representative_image.jpg 
Images from Wilcox et al. (2008) and O Donovan et al. (2009). Contact:Veronica SundstedtBTHveronica.sundstedt@bth.se 
http://www.scss.tcd.ie/Veronica.Sundstedt/gazingatgames2010.html s10logoWeb450.jpg 10Logo_A.jpg 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837107</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>76</pages>
		<display_no>6</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Image statistics]]></title>
		<subtitle><![CDATA[from data collection to applications in graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>76</page_to>
		<doi_number>10.1145/1837101.1837107</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837107</url>
		<abstract>
			<par><![CDATA[<p>Natural images exhibit statistical regularities that differentiate them from random collections of pixels. Moreover, the human visual system appears to have evolved to exploit such statistical regularities. As computer graphics is concerned with producing imagery for observation by humans, it would be prudent to understand which statistical regularities occur in nature, so they can be emulated by image synthesis methods. In this course we introduce all aspects of natural image statistics, ranging from data collection to analysis and finally their applications in computer graphics, computational photography, visualization and image processing.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational photography]]></kw>
			<kw><![CDATA[natural image statistics]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264996</person_id>
				<author_profile_id><![CDATA[81100331006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264997</person_id>
				<author_profile_id><![CDATA[81464672320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tania]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pouli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2264998</person_id>
				<author_profile_id><![CDATA[81405595701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cunningham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brandenburg Technical University, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baddeley, R. J., and Hancock, P. J. B. 1991. A statistical analysis of natural images matches psychophysically derived orientation tuning curves. <i>Proc. Roy. Soc. Lond. B 246</i>, 219--223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Balboa, R., and Grzywacz, N. 2000. Occlusions and their relationship with the distribution of contrasts in natural images. <i>Vision Research</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Balboa, R. M., Tyler, C. W., and Grzywacz, N. M. 2001. Occlusions contribute to scaling in natural images. <i>Vision Research 41</i>, 7, 955--964.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bell, A. J., and Sejnowski, T. J. 1996. Edges are the 'independent components' of natural scenes. <i>Advances in Neural Information Processing Systems 9</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bell, A. J., and Sejnowski, T. J. 1997. The independent components of natural scenes are edge filters. <i>Vision Research 37</i>, 3327--3338.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>712686</ref_obj_id>
				<ref_obj_pid>647010</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bischof, H., and Leonardis, A. 2000. Recognizing objects by their appearance using eigenimages. <i>LECTURE NOTES IN COMPUTER SCIENCE</i>, 245--265.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Brady, M., and Legge, G. 2009. Camera calibration for natural image studies and vision research. <i>Journal of the Optical Society of America A 26</i>, 1, 30--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Burton, G. J., and Moorhead, I. R. 1987. Color and spatial structure in natural scenes. <i>Applied Optics 26</i>, 1 (January), 157--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Chiao, C.-C., Cronin, T. W., and Osorio, D. 2000. Color signals in natural scenes: characteristics of reflectance spectra and the effects of natural illuminants. <i>J. Opt. Soc. Am. A 17</i>, 2 (February), 218--224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Chiao, C.-C., Osorio, D., Vorobyev, M., and Cronin, T. W. 2000. Characterization of natural illuminants in forests and the use of digital video data to reconstruct illuminant spectra. <i>J. Opt. Soc. Am. A 17</i>, 10 (October), 1713--1721.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Dong, D. W., and Atick, J. J. 1995. Statistics of natural time-varying images. <i>Network: Computation in Neural Systems 6</i>, 3, 345--358.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Dong, D. W., and Atick, J. J. 1995. Temporal decorrelation: A theory of lagged and nonlagged responses in the lateral geniculate nucleus. <i>Network: Computation in Neural Systems 6</i>, 2, 159--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Dror, R., Leung, T., Adelson, E., and Willsky, A. 2001. Statistics of real-world illumination. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141956</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Fergus, R., Singh, B., Hertzmann, A., Roweis, S., and Freeman, W. 2006. Removing camera shake from a single photograph. <i>SIGGRAPH '06: SIGGRAPH 2006 Papers</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Field, D. J., and Brady, N. 1997. Visual sensitivity, blur and the sources of variability in the amplitude spectra of natural scenes. <i>Vision Research 37</i>, 23, 3367--3383.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Field, D. J. 1987. Relations between the statistics of natural images and the response properties of cortical cells. <i>J. Opt. Soc. Am. A 4</i>, 12, 2379--2394.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Field, D. J. 1993. Scale-invariance and self-similar 'wavelet' transforms: An analysis of natural scenes and mammalian visual systems. In <i>Wavelets, fractals and Fourier transforms</i>, M. Farge, J. C. R. Hunt, and J. C. Vassilicos, Eds. Clarendon Press, Oxford, 151--193.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Field, D. 1999. Wavelets, vision and the statistics of natural scenes. <i>Philosophical Transactions: Mathematical</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628552</ref_obj_id>
				<ref_obj_pid>628308</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Fleet, D. J., and Jepson, A. D. 1993. Stability of phase information. <i>IEEE Trans. on PAMI 15</i>, 12, 1253--1268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Hancock, P. J. B., Baddeley, R. J., and Smith, L. S. 1992. The principle components of natural images. <i>Network 3</i>, 61--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628576</ref_obj_id>
				<ref_obj_pid>628310</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Healey, G., and Kondepudy, R. 1994. Radiometric ccd camera calibration and noise estimation. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 16</i>, 3, 267--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Huang, J., and Mumford, D. 1999. Statistics of natural images and models. In <i>Proc. CVPR 99</i>, 541--547.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Huang, J., Lee, A., and Mumford, D. 2000. Statistics of range images. <i>Computer Vision and Pattern Recognition</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Hurri, J., Hyv&#228;rinen, A., and Oja, E. 1997. Wavelets and natural image statistics. In <i>Proc. of 10th Scandinavian Conference on Image Analysis</i>, 13--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Kim, S., and Pollefeys, M. 2004. Radiometric alignment of image sequences. <i>Computer Vision and Pattern Recognition</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288167</ref_obj_id>
				<ref_obj_pid>288126</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Kindlmann, G., and Durkin, J. W. 1998. Semi-automatic generation of transfer functions for direct volume rendering. In <i>VVS '98: Proceedings of the 1998 IEEE symposium on Volume visualization</i>, ACM, New York, NY, USA, 79--86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2286572</ref_obj_id>
				<ref_obj_pid>2286439</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[
Levin, A., and Weiss, Y. 2007. User assisted separation of reflections from a single image using a sparsity prior. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Levin, A. 2007. Blind motion deblurring using image statistics. <i>Advances in Neural Information Processing Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Mann, S., and Picard, R. 1995. On being 'undigital' with digital cameras: Extending dynamic range by combining differently exposed pictures. <i>Proceedings of IS&T 48th annual conference</i>, 422--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Mitsunaga, T., and Nayar, S. 1999. Radiometric self calibration. <i>Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on. 1</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[van Hateren, H., and van der Schaaf, A. 1998. Independent component filters of natural images compared with simple cells in primary visual cortex. <i>Proc. R. Soc. Lond. B 265</i>, 359--366.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[van Hateren, H., and Ruderman, D. L. 1998. Independent component analysis of natural image sequences yields spatiotemporal filters similar to simple cells in primary visual cortex. <i>Proc. R. Soc. Lond. B 265</i>, 2315--2320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Olshausen, B., and Field, D. 1996. Wavelet-like receptive fields emerge from a network that learns sparse codes for natural images. <i>Nature</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Olshausen, B., and Field, D. 1996. Natural image statistics and efficient coding. <i>Network: Computation in Neural Systems, 7: 333--339</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Ostiak, P. Implementation of hdr panorama stitching algorithm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[P&#225;rraga, C. A., Brelstaff, G., Troscianko, T., and Moorehead, I. R. 1998. Color and luminance information in natural scenes. <i>J. Opt. Soc. Am. A 15</i>, 3, 563--569.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>314292</ref_obj_id>
				<ref_obj_pid>314286</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Pass, G., and Zabih, R. 1999. Comparing images using joint histograms. <i>Multimedia Systems 7</i>, 3, 234--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Portilla, J., and Simoncelli, E. P. 2000. Image denoising via adjustment of wavelet coefficient magnitude correlation. In <i>Proc. 7th IEEE Int'l Conf. on Image Processing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>363108</ref_obj_id>
				<ref_obj_pid>363101</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Portilla, J., and Simoncelli, E. P. 2000. A parametric texture model based on joint statistics of complex wavelet coefficients. <i>Int'l Journal of Computer Vision 40</i>, 1 (December), 49--71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Pouli, T., Cunningham, D., and Reinhard, E. 2010. Image statistics and their applications in computer graphics. In <i>Eurographics State of the Art Report (STAR)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1215599</ref_obj_id>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Preim, B., and Bartz, D. 2007. <i>Visualization in Medicine: Theory, Algorithms, and Applications (The Morgan Kaufmann Series in Computer Graphics)</i>. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Rainville, S. J. M., and Kingdom, F. A. A. 1999. Spatial-scale contribution to the detection of mirror symmetry in fractal noise. <i>J. Opt. Soc. Am. A 16</i>, 9 (September), 2112--2123.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Reinagel, P., and Zadow, A. M. 1999. Natural scene statistics at the centre of gaze. <i>Network: Comput. Neural Syst. 10</i>, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618848</ref_obj_id>
				<ref_obj_pid>616072</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Ashikhmin, M., Gooch, B., and Shirley, P. 2001. Color transfer between images. <i>IEEE Computer Graphics and Applications 21</i> (September/October), 34--41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Shirley, P., and Troscianko, T. 2001. Natural image statistics for computer graphics. Tech. Rep. UUCS-01-002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1012568</ref_obj_id>
				<ref_obj_pid>1012551</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Shirley, P., Ashikhmin, M., and Troscianko, T. 2004. Second order image statistics in computer graphics. <i>Proceedings of the 1st Symposium on Applied perception in graphics and visualization</i>, 99--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D., and Bialek, W. 1994. Statistics of natural images: Scaling in the woods. <i>Physical Review Letters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D. L., and Bialek, W. 1994. Statistics of natural images: Scaling in the woods. <i>Physical Review Letters 73</i>, 6, 814--817.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D. L., Cronin, T. W., and Chiao, C. 1998. Statistics of cone responses to natural images: Implications for visual coding. <i>Journal of the Optical Society of America A 15</i>, 8, 2036--2045.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D. L. 1994. The statistics of natural images. <i>Network: Computation in Neural Systems</i>, 5, 517--548.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D. L. 1997. Origins of scaling in natural images. <i>Vision Research 37</i>, 3385--3398.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Ruderman, D. L. 1997. The statistics of natural images. <i>Network: Computation in Neural Systems 5</i>, 4, 517--548.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Van Der Schaaf, A. 1998. <i>Natural image statistics and visual processing</i>. PhD thesis, Rijksuniversiteit Groningen, The Netherlands.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Shah, S., and Aggarwal, J. 1996. Intrinsic parameter calibration procedure for a (high-distortion) fish-eye lens camera with distortion model and accuracy estimation*. <i>Pattern Recognition 29</i>, 11, 1775--1788.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360672</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Shan, Q., Jia, J., and Agarwala, A. 2008. High-quality motion deblurring from a single image. <i>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Shih, S., Hung, Y., and Lin, W. 1995. When should we consider lens distortion in camera calibration. <i>Pattern Recognition 28</i>, 3, 447--461.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Simoncelli, E. P., and Portilla, J. 1998. Texture characterization via joint statistics of wavelet coefficient magnitudes. In <i>Proc. 5th Int'l Conf. on Image Processing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Simoncelli, E. P. 1997. Statistical models for images: Compression, restoration and synthesis. In <i>31st Asilomar Conference on Signals, Systems and Computers</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Simoncelli, E. P. 1999. Bayesian denoising of visual images in the wavelet domain. In <i>Bayesian Inference in Wavelet Based Models</i>, Springer-Verlag, New York, P. M&#252;ller and B. Vidakovic, Eds., vol. 141 of <i>Lecture Notes in Statistics</i>, 291--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Simoncelli, E. P. 1999. Modelling the joint statistics of images in the wavelet domain. In <i>Proc. SPIE 44th Anual Meeting</i>, vol. 3813, 188--195.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Thomson, M. G. A. 1999. Higher-order structure in natural scenes. <i>Journal of the Optical Society of America A 16</i>, 7, 1549--1553.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Thomson, M. G. A. 1999. Visual coding and the phase structure of natural scenes. <i>Network: Computation in Neural Systems 10</i>, 2, 123--132.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Thomson, M. G. A. 2001. Beats, kurtosis and visual coding. <i>Network: Computation in Neural Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Tolhurst, D. J., Tadmor, Y., and Chiao, T. 1992. Amplitude spectra of natural images. <i>Ophthalmic and Physiological Optics 12</i>, 229--232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Tolhurt, D. J., and Tadmor, Y. 1997. Discrimination of changes in the slopes of the amplitude spectra of natural images: Band-limited contrast and psychometric functions. <i>Perception 26</i>, 8, 1011--1025.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Torralba, A., and Oliva, A. 2003. Statistics of natural image categories. <i>Network: Computation in Neural Systems 14</i>, 391--412.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[van der Schaaf, A., and Hateren, J. V. 1996. Modelling the power spectra of natural images: statistics and information. <i>Vision Research 36</i>, 17, 2759--2770.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[von der Twer, T., and Macleod, D. 2001. Optimal nonlinear codes for the perception of natural colours. <i>Network: Computation in Neural Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Wainwright, M. J., Simoncelli, E. P., and Willsky, A. S. 2000. Random cascades of gaussian scale mixtures and their use in modelling natural images with application to denoising. In <i>Proc. 7th IEEE Int'l Conf. on Image Processing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Webster, M. A., and Miyahara, E. 1997. Contrast adaptation and the spatial structure of natural images. <i>Journal of the Optical Society of America A 14</i>, 9 (September), 2355--2366.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Willmore, B., and Tolhurst, D. 2001. Characterizing the sparseness of neural codes. <i>Network: Computation in Neural Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Willson, R. 1994. Modeling and calibration of automated zoom lenses.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Zetzsche, C., and Rhrbein, F. 2001. Nonlinear and extra-classical receptive field properties and the statistics of natural scenes. <i>Network: Computation in Neural Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>685553</ref_obj_id>
				<ref_obj_pid>646257</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Ziegaus, C., and Lang, E. W. 1997. Statistics of natural and urban images. In <i>Proc. 7&#60;sup&#62;th&#60;/sup&#62; International Conference on Artificial Neural Networks</i>, Springer-Verlag, Berlin, vol. 1327 of <i>Lecture Notes in Computer Science</i>, 219--224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Ziegaus, C., and Lang, E. W. 1998. Statistical invariances in artificial, natural and urban images. <i>Z. Naturforsch 53a</i>, 1009--1021.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>148073</ref_obj_id>
				<ref_obj_pid>148069</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[J J Atick and N A Redlich. What does the retina know about natural scenes? <i>Neural Computation</i>, 4:196--210, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[R J Baddeley and P J B Hancock. A statistical analysis of natural images matches psychophysically derived orientation tuning curves. <i>Proceedings of the Royal Society of London B</i>, 246:219--223, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Anthony J Bell and Terrence J Sejnowski. Edges are the 'independent components' of natural scenes. <i>Advances in Neural Information Processing Systems</i>, 9, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Anthony J Bell and Terrence J Sejnowski. The independent components of natural scenes are edge filters. <i>Vision Research</i>, 37:3327--3338, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[T Bossomaier and A W Snyder. Why spatial frequency processing in the visual cortex? <i>Vision Research</i>, 26:1307--1309, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[G J Burton and Ian R Moorhead. Color and spatial structure in natural scenes. <i>Applied Optics</i>, 26(1):157--170, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[J Cohen. Dependency of the spectral reflectance curves of the munsell color chips. <i>Psychonomic Science</i>, 1:369--370, 1964.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>329559</ref_obj_id>
				<ref_obj_pid>329556</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Mark E Crovella and Murad S Taqqu. Estimating the heavy tail index from scaling properties. <i>Methodology and Computing in Applied Probability</i>, 1(1):55--79, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Dawei W Dong and Joseph J Atick. Statistics of natural time-varying images. <i>Network: Computation in Neural Systems</i>, 6(3):345--358, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[David J Field. Relations between the statistics of natural images and the response properties of cortical cells. <i>Journal of the Optical Society of America A</i>, 4(12):2379--2394, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[David J Field. Scale-invariance and self-similar 'wavelet' transforms: An analysis of natural scenes and mammalian visual systems. In M Farge, J C R Hunt, and J C Vassilicos, editors, <i>Wavelets, fractals and Fourier transforms</i>, pages 151--193. Clarendon Press, Oxford, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[David J Field and Nuala Brady. Visual sensitivity, blur and the sources of variability in the amplitude spectra of natural scenes. <i>Vision Research</i>, 37(23):3367--3383, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Peter J B Hancock, Roland J Baddeley, and Leslie S Smith. The principle components of natural images. <i>Network</i>, 3:61--70, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Frederic J Harris. On the use of windows for harmonic analysis with the discrete fourier transform. <i>Proceedings of the IEEE</i>, 66(1):51--84, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[H van Hateren and A van der Schaaf. Independent component filters of natural images compared with simple cells in primary visual cortex. <i>Proceedings of the Royal Society of London B</i>, 265:359--366, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Bruce M Hill. A simple general approach to inference about the tail of a distribution. <i>The Annals of Statistics</i>, 3(5):1163--1174, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Jarmo Hurri, Aapo Hyv&#228;rinen, and Erkki Oja. Wavelets and natural image statistics. In <i>Proceedings of the</i> 10&#60;sup&#62;
<i>th</i>
&#60;/sup&#62; <i>Scandinavian Conference on Image Analysis</i>, pages 13--18, June 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Aapo Hyv&#228;rinen. Survey on independent components analysis. <i>Neural Computing Surveys</i>, 2:94--128, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Deane B Judd, David L MacAdam, and G&#252;nther Wyszecki. Spectral distribution of typical light as a function of correlated color temperature. <i>Journal of the Optical Society of America</i>, 54(8):1031--1040, 1964.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Michael S Langer. Large-scale failures of <i>f</i>
&#60;sup&#62;&minus;&alpha;&#60;/sup&#62; scaling in natural image spectra. <i>Journal of the Optical Society of America A</i>, 17(1):28--33, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Hsien-Che Lee. Internet color imaging. In <i>Proceedings of the SPIE</i>, volume 3080, pages 122--135, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1036783</ref_obj_id>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Hsien-Che Lee. <i>Introduction to color imaging science</i>. Cambridge University Press, Cambridge, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Laurence T Maloney. Evaluation of linear models of surface spectral reflectance with small number of parameters. <i>Journal of the Optical Society of America A</i>, 3:1673--1683, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Laurence T Maloney and Brian A Wandell. Color constancy: A method for recovering surface spectral reflectance. <i>Journal of the Optical Society of America A</i>, 3(1):29--33, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Chrysostomos L Nikias and Athina P Petropulu. <i>Higher-order spectra analysis</i>. Signal Processing Series. Prentice Hall, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[B A Olshausen and D J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. <i>Nature</i>, 381:607--609, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[C A P&#225;rraga, G Brelstaff, T Troscianko, and I R Moorhead. Color and luminance information in natural scenes. <i>Journal of the Optical Society of America A</i>, 15(3):563--569, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[C A P&#225;rraga, T Troscianko, and D J Tolhurst. The human visual system is optimised for processing the spatial information in natural visual images. <i>Current Biology</i>, 10(1):35--38, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[F H G Pitt and E W H Selwyn. Colour of outdoor photographic objects. <i>The Photographic Journal</i>, 78:115--121, 1938.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>363108</ref_obj_id>
				<ref_obj_pid>363101</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Javier Portilla and Eero P Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. <i>International Journal of Computer Vision</i>, 40(1):49--71, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[St&#233;phane J M Rainville and Frederick A A Kingdom. Spatial-scale contribution to the detection of mirror symmetry in fractal noise. <i>Journal of the Optical Society of America A</i>, 16(9):2112--2123, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[D L Ruderman and W Bialek. Statistics of natural images: Scaling in the woods. <i>Physical Review Letters</i>, 73(6):814--817, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Daniel L Ruderman. The statistics of natural images. <i>Network: Computation in Neural Systems</i>, 5(4):517--548, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[A van der Schaaf. <i>Natural image statistics and visual processing</i>. PhD thesis, Rijksuniversiteit Groningen, The Netherlands, March 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Eero P Simoncelli. Bayesian denoising of visual images in the wavelet domain. In P M&#252;ller and B Vidakovic, editors, <i>Bayesian Inference in Wavelet Based Models</i>, volume 141 of <i>Lecture Notes in Statistics</i>, pages 291--308, New York, 1999. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Eero P Simoncelli. Modelling the joint statistics of images in the wavelet domain. In <i>Proceedings of the 44th SPIE Anual Meeting</i>, volume 3813, pages 188--195, July 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Eero P Simoncelli and Javier Portilla. Texture characterization via joint statistics of wavelet coefficient magnitudes. In <i>Proceedings of the 5&#60;sup&#62;th&#60;/sup&#62; International Conference on Image Processing</i>, October 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Mitchell G A Thomson. Higher-order structure in natural scenes. <i>Journal of the Optical Society of America A</i>, 16(7):1549--1553, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Mitchell G A Thomson. Visual coding and the phase structure of natural scenes. <i>Network: Computation in Neural Systems</i>, 10(2):123--132, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[D J Tolhurst, Y Tadmor, and T Chiao. Amplitude spectra of natural images. <i>Ophthalmic and Physiological Optics</i>, 12:229--232, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>685553</ref_obj_id>
				<ref_obj_pid>646257</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Christian Ziegaus and Elmar W Lang. Statistics of natural and urban images. In <i>Proceedings of the 7&#60;sup&#62;th&#60;/sup&#62; International Conference on Artificial Neural Networks</i>, volume 1327, pages 219--224, Berlin, 1997. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Christian Ziegaus and Elmar W Lang. Statistical invariances in artificial, natural and urban images. <i>Zeitschrift f&#252;r Naturforschung A</i>, 53a(12):1009--1021, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Statistics: From Data Collection to Applications in Graphics Erik Reinhard* Tania Pouli Douglas 
Cunningham University of Bristol, UK University of Bristol, UK Brandenburg Technical University, Germany 
 Figure 1: The left image can easily be recognised as a natural scene. The image on the right however 
appears as random noise despite both images consisting of the same pixels. Abstract Natural images exhibit 
statistical regularities that differentiate them from random collections of pixels. Moreover, the human 
visual system appears to have evolved to exploit such statistical regularities. As computer graphics 
is concerned with producing imagery for observation by humans, it would be prudent to understand which 
statistical regularities occur in nature, so they can be emulated by image synthesis methods. In this 
course we introduce all aspects of natural image statistics, ranging from data collection to analysis 
and .nally their applications in computer graphics, computational photography, visualization and image 
processing. CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation Display Algorithms; Keywords: 
Natural Image Statistics, Computational Photography, Rendering 1 Outline The .eld of natural image statistics 
studies images and their statistical regularities. The human visual system has evolved in natural environments, 
i.e. those without man-made structures. Human vision, it is argued, has therefore evolved to be particularly 
good at observing natural scenes, as opposed to random images [van der Schaaf 1998]. As an example, the 
left image in Figure 1 shows a scene that is easy to interpret. Scrambling the pixels produces the image 
on the right, which does not have any structure that can be recognised. However, the number of random 
images is many orders of magnitude larger than the number of natural images. Thus, natural images form 
a very small subset of all possible images, and contain structure to which the human visual system is 
optimised. By studying the input to the human visual system, it is thought that the human visual system 
can be understood better. This has led to a rich literature on various aspects of natural image statistics 
[Pouli et al. 2010]. *e-mail: reinhard@cs.bris.ac.uk The typical process by which natural image statistics 
are computed, is to pass each image of an ensemble through a particular transform. The transforms typically 
employed tend to correspond to a component of human vision. It is, for instance, possible to compute 
the power spectra of each image, and from these compute the average power spectral slope for the whole 
set. This course reviews this and many other transforms and statistics. Thus, the reason for this course 
is that, aside from learning about human vision and the implications for perception, some of these statistical 
regularities have transferred to computer graphics: several applications now successfully employ statistics 
of natural images to help solve engineering problems. As an example, deblurring algorithms rely on 
optimisation involving priors that help push candidate images towards visually plausible solutions. Of 
course, visually plausible solutions are those that appear to humans in some sense natural. Image priors 
have therefore been designed with the aid of speci.c knowledge of natural image statistics, in particular 
regarding the average distribution of image gradients. We think that the use of natural image statistics 
has so far shown great promise, with interesting applications making use of some of the available statistical 
regularities. We also see excellent scope for further future developments. We hope to educate the graphics 
community and make researchers and practicioners in this .eld aware of the exciting possibilities of 
natural image statistics. To this end, we will review the current state of natural image statistics, 
including data collection and calibration techniques and the different types of transforms and statistics 
encountered. We will thoroughly discuss speci.c areas of application within computer graphics, computational 
photography, visualization and image processing with the aid of speci.c examples and .nally, we will 
draw conclusions and hypothesize about how the use of natural image statistics can be extended in the 
future. 2 Target Audience The target audience for this course includes researchers and practitioners 
in computer graphics, computer vision, as well as image processing who are interesting in understanding 
how to compute and use image statistics in the context of image processing, or in any algorithm aiming 
at producing natural images. More specifically, the course should be useful to computational photography 
and image processing researchers interested in incorporating information about human vision and natural 
environments in their work. 3 Topics The course will incorporate all aspects of natural image statistics, 
from the capture of image ensembles, their analysis, currently known statistical regularities, as well 
as applications that make use of such statistics. Introduction We will introduce image statistics in 
the context of human vision research, explain why such .ndings are useful in computer graphics and related 
disciplines and will categorise different types of statistics. Data collection and calibration Here, 
we will focus on the process of collecting the image ensembles and calibrating them in order to remove 
artefacts due to lens and camera inaccuracies [Willson 1994; Shih et al. 1995; Shah and Aggarwal 1996; 
Brady and Legge 2009]. The areas covered will include radiometric calibration, geometric and chromatic 
lens distortions and noise. Image capture will also be discussed covering areas such as high dynamic 
range capture [Healey and Kondepudy 1994; Mann and Picard 1995; Mitsunaga and Nayar 1999; Kim and Pollefeys 
2004] and panorama stitching [Ostiak ]. Finally, the different types of scenes that can be used for 
image statistics will be presented. Although image statistics work has traditionally focused on natural 
images [Ruderman 1997b; van der Schaaf 1998; Huang and Mumford 1999], urban and arti.cial scenes have 
also been explored [Ziegaus and Lang 1997; Ziegaus and Lang 1998] while Dror et al. [Dror et al. 2001] 
have been the .rst to look at the statistics of high dynamic range, spherical environment maps. First 
order statistics First order statistics look at single pixels in the image and do not take into account 
relations between neighbours. They correspond to the distribution of values within the analysed data 
set and its properties, making them easy to both compute and interpret [van der Schaaf 1998; Huang and 
Mumford 1999]. Typical examples include histogram moments such as the mean, standard deviation, skew 
and kurtosis. Second order statistics Second order statistics capture relations and regularities over 
pairs of pixels in the image. The power spectra of images have been extensively analysed with most work 
focusing on the slope resulting from the frequencypower relation over sets of natural images [Burton 
and Moorhead 1987; Tolhurst et al. 1992; Ruderman and Bialek 1994b; Ruderman and Bialek 1994a; van der 
Schaaf and Hateren 1996; Field and Brady 1997; Ruderman 1997a; Tolhurt and Tadmor 1997; Webster and Miyahara 
1997; Rainville and Kingdom 1999; Reinhard et al. 2001b; Reinhard et al. 2004]. Power spectra are computed 
in Fourier space, but relate to the auto-correlation function in image space, thus effectively providing 
information on pairs of pixels. The power spectrum, when averaged over large numbers of natural images, 
tends to show a behaviour which can be well modelled with P = f-2 , where P is the power as function 
of frequency f . Similar results were found for the temporal aspects in video [Dong and Atick 1995a; 
Dong and Atick 1995b]. The 1/f2 slope of power spectra constitutes perhaps the strongest result from 
the .eld of natural image statistics, and has several interesting consequences for both human vision, 
as well as applications in visual engineering disciplines such as graphics and image processing. Image 
gradients and their distributions have also been extensively analysed for natural image sets [Ruderman 
1994; Huang and Mumford 1999], range images [Huang et al. 2000], volume visualization [Kindlmann and 
Durkin 1998] and high dynamic range environment maps [Dror et al. 2001]. Further, rich literature exists 
on gradients in the logarithmic domain as they represent contrast [Ruderman 1994; Reinagel and Zadow 
1999]. The general .nding is that gradient histograms show a very sharp peak at zero, falling off sharply 
away from zero, leading to long tails where higher gradients are located. This behaviour can be attributed, 
at least in part, to the existence of edges in scenes [Balboa and Grzywacz 2000; Balboa et al. 2001]. 
In volume visualization, in contrast, where the intensity values usually represent the density of tissue 
or other material present at that a given location, the gradient histograms tend to show peaks at the 
interface between materials or objects [Preim and Bartz 2007]. Although most studies assume that image 
statistics are stationary, i.e., they do not change signi.cantly over the area of an images, in individual 
images this assumption does not tend to hold. This means that in individual images, the distribution 
of contrasts may lead to observers altering how they focus their attention. This effect has been studied 
by means of eye tracking [Reinagel and Zadow 1999]. Higher order statistics To capture higher level information 
about image structure, features and positions, statistics that measure relationships between more than 
two pixels can be used. Wavelets have been extensively studied [Field 1993; Hurri et al. 1997; Simoncelli 
and Portilla 1998; Simoncelli 1999b; Huang and Mumford 1999] as well as their relation to cortical cells 
[Field 1987; Field 1999]. Additionally, sparse coding of natural scenes has been investigated in the 
context of human vision [Olshausen and Field 1996b; Olshausen and Field 1996a; Field 1999; Willmore 
and Tolhurst 2001], aiming to derive a set of ef.cient basis functions that would better represent the 
response properties of neurons. Principal [Baddeley and Hancock 1991; Hancock et al. 1992; Zetzsche and 
Rhrbein 2001] and Independent [Bell and Sejnowski 1996; Bell and Sejnowski 1997; van Hateren and van 
der Schaaf 1998; van Hateren and Ruderman 1998; Zetzsche and Rhrbein 2001] Component Analysis (PCA and 
ICA) have also been applied to image ensembles with interesting results, decomposing images to a set 
of independent components that closely resemble the responses of cortical cells. This has provided the 
insight that the human visual system appears to decorrelate signals as much as possible. Although second 
order statistics can be derived from the amplitude spectrum of Fourier transformed images, it is generally 
thought that much of the image structure is encoded in the phase spectrum. This has led to the analysis 
phase statistics. To eliminate the effect of second order correlations, it is possible to whiten the 
images .rst, a process whereby the slope of the amplitude spectrum of each image is made zero. Higher 
order phase analysis has essentially led to the discovery of distributions with high kurtosis [Fleet 
and Jepson 1993; Thomson 1999b; Thomson 1999a; Thomson 2001]. Colour statistics Statistical regularities 
have also been found when studying distributions of colours [Burton and Moorhead 1987; Parraga et al. 
1998; Chiao et al. 2000a; Chiao et al. 2000b; von der Twer and Macleod 2001]. A particularly interesting 
and useful insight was obtained with a set of spectral images, which were subsequently converted to cone 
space, and subjected to Principal Components Analysis [Ruderman et al. 1998]. This rotates the data so 
that the three resulting colour axes maximally decorrelate the data. The surprising result is that the 
axes that were found correspond to colour opponency. As a result, the human visual system appears to 
decorrelate colour data when it computes colour opponent channels. Existing graphics applications Statistical 
regularities of images have been exploited by a number of computer graphics and vision applications. 
Wavelets have been used extensively for a variety of purposes from compression and restoration [Simoncelli 
1997], to denoising [Simoncelli 1999a; Portilla and Simoncelli 2000a; Wainwright et al. 2000] and texture 
synthesis [Portilla and Simoncelli 2000b]. Image processing algorithms such as deblurring [Fergus et 
al. 2006; Levin 2007; Shan et al. 2008] and separation of re.ections [Levin and Weiss 2007] have used 
models of gradient distributions in natural images in order to achieve more plausible results. Finally, 
a variety of statistics have been utilised in scene classi.cation and comparisons, from joint histograms 
[Pass and Zabih 1999], to eigenimages [Bischof and Leonardis 2000] and spectral signatures [Torralba 
and Oliva 2003]. Colour statistics in the form of colour opponent channels have been used extensively, 
for instance in the computation of colour differences. More recently, colour transfer between images 
has speci.cally used the fact that colour opponent channels are decorrelated for natural images [Reinhard 
et al. 2001a]. Further, colour statistics are used routines in white balancing, mostly by making the 
grey-world assumption, i.e. the average re.ectance of all objects in a scene is grey. 4 Syllabus 1. 
Introduction -Erik Reinhard (5 minutes) 2. Data Collection and Calibration -Tania Pouli (15 minutes) 
 3. First Order Statistics -Tania Pouli (15 minutes) 4. Second Order Statistics -Douglas Cunningham 
(25 minutes) 5. Higher Order Statistics -Douglas Cunningham (15 minutes) 6. Colour Statistics -Erik 
Reinhard (15 minutes) 7. Conclusions/Summary -Erik Reinhard (5 minutes) 8. Q&#38;A -All (10 minutes) 
  5 Author Biographies Erik Reinhard Department of Computer Science, University of Bristol, UK Email: 
reinhard@cs.bris.ac.uk Erik Reinhard received his Ph.D. in Computer Science from the University of Bristol 
in 2000, having worked on his Ph.D. for three years at Delft University of Technology, prior to a further 
three years in Bristol. Following a post-doctoral position at the University of Utah (2000-2002) and 
assistant professor at the University of Central Florida (2002-2005), he returned to Bristol as a lecturer 
in January 2006. Erik founded the prestigious ACM Transactions on Applied Perception, and has been Editor-in-Chief 
since its inception in 2003, until early 2009. He is also guest editor for a special issue on HDRI for 
Elsevier s Journal of Visual Communication and Image Representation (2007). He is lead author of High 
Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting . He was program chair for Eurographics 
PGV 2002 and ACM APGV 2006, He is also program co-chair for the short papers program of the Eurographics 
main conference in 2008. He is invited speaker for the Society for Information Display s main conference 
(2007) and keynote speaker for Eurographics 2010. His work on natural image statistics include an assessment 
of such statistics for use in computer graphics, as well as an image statistics based algorithm for the 
transfer of colour between two images. Tania Pouli Department of Computer Science, University of Bristol, 
UK Email: pouli@cs.bris.ac.uk Tania Pouli is a PhD student at the University of Bristol. She received 
a B.Sc. in Computer Software Theory from the University of Bath in 2003 and has been a member of the 
Interaction &#38; Graphics group in Bristol since then. Her current research focuses on statistical analysis 
of high dynamic range images and novel image based editing applications using image statistics as priors. 
Douglas W. Cunningham Brandenburg Technical University Cottbus, Cottbus, Germany Email: douglas.cunningham@tuebingen.mpg.de 
Douglas Cunningham is guest professor for Graphical Systems at the Brandenburg Technical University 
Cottbus. His current research focuses on the conceptual and practical integration of computer graphics 
and perception research, with particular emphasis on facial animation and recognition as well as sensorimotor 
adaptation. Douglas studied Psychology, receiving a B.S. from the University of Washington, a M.S from 
the University of Georgia, and in 1997 a Ph.D. from Temple University. In 2007 he received his habilitation 
in Cognition Science from the University of T ubingen (in Germany). He worked as a senior research scientist 
for Logicon Technical Services, the Max Planck Institute for Biological Cybernetics, and the University 
of T ubingen.  References BADDELEY, R. J., AND HANCOCK, P. J. B. 1991. A statistical analysis of 
natural images matches psychophysically derived orientation tuning curves. Proc. Roy. Soc. Lond. B 246, 
219 223. BALBOA, R., AND GRZYWACZ, N. 2000. Occlusions and their relationship with the distribution of 
contrasts in natural images. Vision Research. BALBOA, R. M., TYLER, C. W., AND GRZYWACZ, N. M. 2001. 
Occlusions contribute to scaling in natural images. Vision Research 41, 7, 955 964. BELL, A. J., AND 
SEJNOWSKI, T. J. 1996. Edges are the independent components of natural scenes. Advances in Neural Information 
Processing Systems 9. BELL, A. J., AND SEJNOWSKI, T. J. 1997. The independent components of natural scenes 
are edge .lters. Vision Research 37, 3327 3338. BISCHOF, H., AND LEONARDIS, A. 2000. Recognizing objects 
by their appearance using eigenimages. LECTURE NOTES IN COMPUTER SCIENCE, 245 265. BRADY, M., AND LEGGE, 
G. 2009. Camera calibration for natural image studies and vision research. Journal of the Optical Society 
of America A 26, 1, 30 42. BURTON, G. J., AND MOORHEAD, I. R. 1987. Color and spatial structure in natural 
scenes. Applied Optics 26, 1 (January), 157 170. CHIAO, C.-C., CRONIN, T. W., AND OSORIO, D. 2000. Color 
signals in natural scenes: characteristics of re.ectance spectra and the effects of natural illuminants. 
J. Opt. Soc. Am. A 17,2 (February), 218 224. CHIAO, C.-C., OSORIO, D., VOROBYEV, M., AND CRONIN, T. W. 
2000. Characterization of natural illuminants in forests and the use of digital video data to reconstruct 
illuminant spectra. J. Opt. Soc. Am. A 17, 10 (October), 1713 1721. DONG, D. W., AND ATICK, J. J. 1995. 
Statistics of natural timevarying images. Network: Computation in Neural Systems 6, 3, 345 358. DONG, 
D. W., AND ATICK, J. J. 1995. Temporal decorrelation: A theory of lagged and nonlagged responses in the 
lateral geniculate nucleus. Network: Computation in Neural Systems 6, 2, 159 178. DROR, R., LEUNG, T., 
ADELSON, E., AND WILLSKY, A. 2001. Statistics of real-world illumination. Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition 2. FERGUS, R., SINGH, B., HERTZMANN, A., ROWEIS, S., AND FREEMAN, 
W. 2006. Removing camera shake from a single photograph. SIGGRAPH 06: SIGGRAPH 2006 Papers. FIELD, D. 
J., AND BRADY, N. 1997. Visual sensitivity, blur and the sources of variability in the amplitude spectra 
of natural scenes. Vision Research 37, 23, 3367 3383. FIELD, D. J. 1987. Relations between the statistics 
of natural images and the response properties of cortical cells. J. Opt. Soc. Am. A4, 12, 2379 2394. 
FIELD, D. J. 1993. Scale-invariance and self-similar wavelet transforms: An analysis of natural scenes 
and mammalian visual systems. In Wavelets, fractals and Fourier transforms, M. Farge, J. C. R. Hunt, 
and J. C. Vassilicos, Eds. Clarendon Press, Oxford, 151 193. FIELD, D. 1999. Wavelets, vision and the 
statistics of natural scenes. Philosophical Transactions: Mathematical. FLEET, D. J., AND JEPSON, A. 
D. 1993. Stability of phase information. IEEE Trans. on PAMI 15, 12, 1253 1268. HANCOCK, P. J. B., BADDELEY, 
R. J., AND SMITH, L. S. 1992. The principle components of natural images. Network 3, 61 70. HEALEY, G., 
AND KONDEPUDY, R. 1994. Radiometric ccd camera calibration and noise estimation. IEEE Transactions on 
Pattern Analysis and Machine Intelligence 16, 3, 267 276. HUANG, J., AND MUMFORD, D. 1999. Statistics 
of natural images and models. In Proc. CVPR 99, 541 547. HUANG, J., LEE, A., AND MUMFORD, D. 2000. Statistics 
of range images. Computer Vision and Pattern Recognition. HURRI, J., HYV  ARINEN, A., AND OJA, E. 1997. 
Wavelets and natural image statistics. In Proc. of 10th Scandinavian Conference on Image Analysis, 13 
18. KIM, S., AND POLLEFEYS, M. 2004. Radiometric alignment of image sequences. Computer Vision and Pattern 
Recognition. KINDLMANN, G., AND DURKIN, J. W. 1998. Semi-automatic generation of transfer functions for 
direct volume rendering. In VVS 98: Proceedings of the 1998 IEEE symposium on Volume visualization, ACM, 
New York, NY, USA, 79 86. LEVIN, A., AND WEISS, Y. 2007. User assisted separation of re.ections from 
a single image using a sparsity prior. IEEE Transactions on Pattern Analysis and Machine Intelligence. 
LEVIN, A. 2007. Blind motion deblurring using image statistics. Advances in Neural Information Processing 
Systems. MANN, S., AND PICARD, R. 1995. On being undigital with digital cameras: Extending dynamic range 
by combining differently exposed pictures. Proceedings of IS&#38;T 48th annual conference, 422 428. MITSUNAGA, 
T., AND NAYAR, S. 1999. Radiometric self calibration. Computer Vision and Pattern Recognition, 1999. 
IEEE Computer Society Conference on. 1. VAN HATEREN, H., AND VAN DER SCHAAF, A. 1998. Independent component 
.lters of natural images compared with simple cells in primary visual cortex. Proc. R. Soc. Lond. B 265, 
359 366. VAN HATEREN, H., AND RUDERMAN, D. L. 1998. Independent component analysis of natural image sequences 
yields spatiotemporal .lters similar to simple cells in primary visual cortex. Proc. R. Soc. Lond. B 
265, 2315 2320. OLSHAUSEN, B., AND FIELD, D. 1996. Wavelet-like receptive .elds emerge from a network 
that learns sparse codes for natural images. Nature. OLSHAUSEN, B., AND FIELD, D. 1996. Natural image 
statistics and ef.cient coding. Network: Computation in Neural Systems, 7: 333 339. OSTIAK, P. Implementation 
of hdr panorama stitching algorithm. PARRAGA, C. A., BRELSTAFF, G., TROSCIANKO, T., AND MOOREHEAD, I. 
R. 1998. Color and luminance information in natural scenes. J. Opt. Soc. Am. A 15, 3, 563 569. PASS, 
G., AND ZABIH, R. 1999. Comparing images using joint histograms. Multimedia Systems 7, 3, 234 240. PORTILLA, 
J., AND SIMONCELLI, E. P. 2000. Image denoising via adjustment of wavelet coef.cient magnitude correlation. 
In Proc. 7th IEEE Int l Conf. on Image Processing. PORTILLA, J., AND SIMONCELLI, E. P. 2000. A parametric 
texture model based on joint statistics of complex wavelet coef.cients. Int l Journal of Computer Vision 
40, 1 (December), 49 71. POULI, T., CUNNINGHAM, D., AND REINHARD, E. 2010. Image statistics and their 
applications in computer graphics. In Eurographics State of the Art Report (STAR). PREIM, B., AND BARTZ, 
D. 2007. Visualization in Medicine: Theory, Algorithms, and Applications (The Morgan Kaufmann Series 
in Computer Graphics). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. RAINVILLE, S. J. M., 
AND KINGDOM, F. A. A. 1999. Spatialscale contribution to the detection of mirror symmetry in fractal 
noise. J. Opt. Soc. Am. A 16, 9 (September), 2112 2123. REINAGEL, P., AND ZADOW, A. M. 1999. Natural 
scene statistics at the centre of gaze. Network: Comput. Neural Syst. 10, 1 10. REINHARD, E., ASHIKHMIN, 
M., GOOCH, B., AND SHIRLEY, P. 2001. Color transfer between images. IEEE Computer Graphics and Applications 
21 (September/October), 34 41. REINHARD, E., SHIRLEY, P., AND TROSCIANKO, T. 2001. Natural image statistics 
for computer graphics. Tech. Rep. UUCS01-002. REINHARD, E., SHIRLEY, P., ASHIKHMIN, M., AND TROS-CIANKO, 
T. 2004. Second order image statistics in computer graphics. Proceedings of the 1st Symposium on Applied 
perception in graphics and visualization, 99 106. RUDERMAN, D., AND BIALEK, W. 1994. Statistics of natural 
images: Scaling in the woods. Physical Review Letters. RUDERMAN, D. L., AND BIALEK, W. 1994. Statistics 
of natural images: Scaling in the woods. Physical Review Letters 73, 6, 814 817. RUDERMAN, D. L., CRONIN, 
T. W., AND CHIAO, C. 1998. Statistics of cone responses to natural images: Implications for visual coding. 
Journal of the Optical Society of America A 15, 8, 2036 2045. RUDERMAN, D. L. 1994. The statistics of 
natural images. Network: Computation in Neural Systems, 5, 517 548. RUDERMAN, D. L. 1997. Origins of 
scaling in natural images. Vision Research 37, 3385 3398. RUDERMAN, D. L. 1997. The statistics of natural 
images. Network: Computation in Neural Systems 5, 4, 517 548. VAN DER SCHAAF, A. 1998. Natural image 
statistics and visual processing. PhD thesis, Rijksuniversiteit Groningen, The Netherlands. SHAH, S., 
AND AGGARWAL, J. 1996. Intrinsic parameter calibration procedure for a (high-distortion) .sh-eye lens 
camera with distortion model and accuracy estimation*. Pattern Recognition 29, 11, 1775 1788. SHAN, Q., 
JIA, J., AND AGARWALA, A. 2008. High-quality motion deblurring from a single image. SIGGRAPH 08: ACM 
SIGGRAPH 2008 papers. SHIH, S., HUNG, Y., AND LIN, W. 1995. When should we consider lens distortion 
in camera calibration. Pattern Recognition 28, 3, 447 461. SIMONCELLI, E. P., AND PORTILLA, J. 1998. 
Texture characterization via joint statistics of wavelet coef.cient magnitudes. In Proc. 5th Int l Conf. 
on Image Processing. SIMONCELLI, E. P. 1997. Statistical models for images: Compression, restoration 
and synthesis. In 31st Asilomar Conference on Signals, Systems and Computers. SIMONCELLI, E. P. 1999. 
Bayesian denoising of visual images in the wavelet domain. In Bayesian Inference in Wavelet Based Models, 
Springer-Verlag, New York, P. M uller and B. Vidakovic, Eds., vol. 141 of Lecture Notes in Statistics, 
291 308. SIMONCELLI, E. P. 1999. Modelling the joint statistics of images in the wavelet domain. In 
Proc. SPIE 44th Anual Meeting, vol. 3813, 188 195. THOMSON, M. G. A. 1999. Higher-order structure in 
natural scenes. Journal of the Optical Society of America A 16, 7, 1549 1553. THOMSON, M. G. A. 1999. 
Visual coding and the phase structure of natural scenes. Network: Computation in Neural Systems 10, 2, 
123 132. THOMSON, M. G. A. 2001. Beats, kurtosis and visual coding. Network: Computation in Neural Systems. 
TOLHURST, D. J., TADMOR, Y., AND CHIAO, T. 1992. Amplitude spectra of natural images. Ophthalmic and 
Physiological Optics 12, 229 232. TOLHURT, D. J., AND TADMOR, Y. 1997. Discrimination of changes in the 
slopes of the amplitude spectra of natural images: Band-limited contrast and psychometric functions. 
Perception 26, 8, 1011 1025. TORRALBA, A., AND OLIVA, A. 2003. Statistics of natural image categories. 
Network: Computation in Neural Systems 14, 391 412. VAN DER SCHAAF, A., AND HATEREN, J. V. 1996. Modelling 
the power spectra of natural images: statistics and information. Vision Research 36, 17, 2759 2770. VON 
DER TWER, T., AND MACLEOD, D. 2001. Optimal nonlinear codes for the perception of natural colours. Network: 
Computation in Neural Systems. WAINWRIGHT, M. J., SIMONCELLI, E. P., AND WILLSKY, A. S. 2000. Random 
cascades of gaussian scale mixtures and their use in modelling natural images with application to denoising. 
In Proc. 7th IEEE Int l Conf. on Image Processing. WEBSTER, M. A., AND MIYAHARA, E. 1997. Contrast adaptation 
and the spatial structure of natural images. Journal of the Optical Society of America A 14, 9 (September), 
2355 2366. WILLMORE, B., AND TOLHURST, D. 2001. Characterizing the sparseness of neural codes. Network: 
Computation in Neural Systems. WILLSON, R. 1994. Modeling and calibration of automated zoom lenses. ZETZSCHE, 
C., AND RHRBEIN, F. 2001. Nonlinear and extraclassical receptive .eld properties and the statistics 
of natural scenes. Network: Computation in Neural Systems. ZIEGAUS, C., AND LANG, E. W. 1997. Statistics 
of natural and urban images. In Proc. 7th International Conference on Arti.cial Neural Networks, Springer-Verlag, 
Berlin, vol. 1327 of Lecture Notes in Computer Science, 219 224. ZIEGAUS, C., AND LANG, E. W. 1998. Statistical 
invariances in arti.cial, natural and urban images. Z. Naturforsch 53a, 1009 1021.  The statistics of 
natural images have attracted the attention of researchers in a variety of .elds and have been used as 
a means to better understand the human visual system and its processes. A number of algorithms in computer 
graphics, vision and image processing take advantage of such statistical .ndings to create visually more 
plausible results. With this report we aim to review the state of the art in image statistics and discuss 
existing and potential applications within computer graphics and related areas.  The .eld of natural 
image statistics studies images and their statistical regularities. The human visual system has evolved 
in natural environments, i.e. those without manmade structures. Human vision, it is argued, has therefore 
evolved to be particularly good at observing natural scenes... ...as opposed to random images. The 
number of random images is many orders of magnitude larger than the number of natural images. Thus, natural 
images form a very small subset of all possible images, and contain structure to which the human visual 
system is optimized. By studying the input to the human visual system, it is thought that the human visual 
system can be understood better. The typical process by which natural image statistics are computed, 
is to pass each image of an ensemble through a particular transform. The transforms typically employed 
tend to correspond to a component of human vision. It is, for instance, possible to compute the power 
spectra of each image, and from these compute the average power spectral slope for the whole set. This 
course reviews this and many other transforms and statistics.  Statistics in Graphics Thus, the reason 
for this course is that, aside from learning about human vision and the implications for perception, 
some of these statistical regularities have transferred to computer graphics: several applications now 
successfully employ statistics of natural images to help solve engineering problems. As an example, deblurring 
algorithms rely on optimization involving priors that help push candidate images towards visually plausible 
solutions. Of course, visually plausible solutions are those that appear to humans in some sense natural. 
Image priors have therefore been designed with the aid of speci.c knowledge of natural image statistics, 
in particular regarding the average distribution of image gradients. We think that the use of natural 
image statistics has so far shown great promise, with interesting applications making use of some of 
the available statistical regularities. We also see excellent scope for further future developments. 
 For the rest of this course, we will classify statistics depending on the number of pixels considered. 
First order statistics look at individual pixels, thus ignoring spatial relations. Second order statistics 
consider pairs of pixels while higher order statistics refer to any transform involving more than two 
pixels.  The statistical analysis of natural images typically involves the collection of large ensembles. 
Various image data sets are already available but in order to create new ensembles, several points need 
to be considered. One might need to create new ensembles if the intended application involves a speci.c 
class of images, or if the study is to understand human vision under speci.c viewing conditions. Advances 
in camera technology have made the capture of higher resolution and higher dynamic range imagery possible. 
Each such type of image presents its own advantages in terms of information and its own speci.c issues 
regarding capturing and calibration. The type of image appropriate for each study largely depends on 
the type of information studied. For most purposes, low dynamic range (LDR) imagery is suf.cient, although 
HDR images can capture a wider range of luminance values, making them more appropriate for scenes of 
more extreme lighting (such as night scenes where arti.cial light sources are much brighter than the 
rest of the scene). Panoramas or wider angle images can capture a larger part of the .eld of view, or 
even the full scene in the case of spherical panoramas. The type of scene to be captured is another 
important consideration, and again the answer depends on the purpose of the study. Typically, studies 
coming from the area of human vision tend to focus on natural images (i.e. those without manmade structures). 
If on the other hand the purpose of the analysis is to derive a scene categorization algorithm then different 
scene types may be appropriate. As for the number of images necessary, although generally larger data 
sets afford more accurate statistical .ndings, in the case of image statistics, the trade-off between 
number of images and data collection time often needs to be resolved. The number of images appropriate 
for a particular study largely depends on its aims. Here, some example numbers from existing studies 
are given. When measuring the statistical regularities of an image ensemble, some care is necessary 
to ensure that irregularities and artifacts due to the camera, lens or shooting conditions do not affect 
the results. To avoid such an effect, the properties of the equipment used need to be accounted for. 
Moreover, to ensure the images are calibrated and therefore pixel values between them are directly comparable, 
properties of the scene may need to be measured. Lens Aberrations Chromatic Monochromatic Longitudinal 
Spherical Lateral Coma Purple fringing Astigmatism Petzval .eld curvature The lens is responsible for 
focusing the light coming from the scene to an image plane (which may be a sensor in digital cameras 
or the .lm). For many applications, it is suf.cient to model the camera as a simple pinhole whereby no 
lens is present and the aperture through which the light enters is a point. To model the various effects 
of an optical system to the image however, more complex lenses need to be considered. Imperfections in 
the design or construction of such lenses can cause them to violate the assumption of Gaussian optics. 
These deviations are known as aberrations and can be broadly categorized as chromatic, where the optical 
system behaves differently for different wavelengths, and monochromatic, where the distortions in the 
image are apparent in all channels. Longitudinal (axial) Lateral Lens elements can have wavelength-dependent 
indices of refraction, resulting in different colors focusing in slightly different places. This causes 
longitudinal or axial chromatic aberration, which appears as chromatic image blur. Typically this form 
of aberration is managed optically, using a pair of thin lenses with different refractive indices, known 
as achromatic doublets. Such systems can reduce longitudinal aberration effects but not always completely 
remove them. Another type of chromatic aberration is lateral chromatic aberration, which is the result 
of different wavelengths hitting the image plane at slightly different positions. This is more commonly 
known as color fringing and can be reduced digitally. This can typically be achieved by realigning the 
color channels of the image which can compensate for the effect of the color shifting to some extent, 
or for a more accurate reconstruction, a more complete characterization of the optical system can be 
utilized. One particular case of chromatic aberration speci.c to digital cameras, especially ones using 
a charge coupled device sensor (CCD), is known as purple fringing. This effect is caused by a combination 
of factors that operate in addition to the lens aberrations described earlier. Highlights in the scene 
that are overly bright can cause some of the CCD quantum-wells to .ood, creating the visual effect of 
blooming. Additionally, a false color effect can be created by the demosaicing process. A number of digital 
solutions are available for correcting the effects of chromatic aberrations in images with various levels 
of accuracy. DxO Optics handles chromatic aberrations by taking into account the characteristics of the 
lens where possible. A similar approach is also found in PTLens. Adobe Photoshop (version CS4 tested) 
provides simple sliders to account for red/cyan and blue/yellow shifting. A wider range of artifacts 
in images appears as the result of monochromatic aberrations. These include blurring aberrations and 
geometric distortions. Blurring aberrations can be further divided into spherical aberration, coma, astigmatism 
and Petzval .eld curvature. Spherical lenses are commonly used in optical systems as they are relatively 
easy to manufacture, but their shape is not ideal for the formation of a sharp image. Spherical aberrations 
are the consequence of light rays further from the lens axis (marginal) focusing at a different position 
than rays passing through the middle of the lens. When marginal rays focus nearer the lens than the principal 
rays, the effect is known as positive or undercorrected spherical aberration, while a marginal focus 
further away from the lens than the paraxial focus causes negative or overcorrected spherical aberration. 
Comatic aberrations cause objects further from the optical axis to appear distorted. Positive coma occurs 
when marginal rays focus further away from the optical axis than principal rays, while negative coma 
is the opposite effect, namely the marginal rays focusing closer to the optical axis. A third type of 
blurring aberration is known as astigmatism and is the result of off-axis rays of different orientations 
focusing at slightly different positions. The meridional plane for an off-axis object point is de.ned 
by that point and the optical axis. The plane orthogonal to the meridional plane is known as the saggital 
plane. Rays along these two planes focus at different positions on the optical axis. As such, if the 
lens is focused such that the meridional image is sharp, blur will occur in the saggital direction and 
vice versa. A .nal aberration considered here is a consequence of the .eld curvature. The image and object 
planes are generally considered to be planar. However, in the case of spherical lenses they are in fact 
spherical, which is known as Petzval .eld curvature. Regions of the image further from the optical axis 
increasingly deviate from the planar assumption. This is more evident when a planar image plane is used 
(such as a sensor) in which case the image is less sharp towards the periphery. The effect of blurring 
aberrations can be reduced in post-processing by sharpening the image or during the image capture by 
using smaller aperture settings and thus only allowing rays closer to the optical axis to enter. Optical 
solutions do exist and generally involve the use of lens doublets or carefully designed lenses. These 
solutions however, complicate the design of the optical system and are as a result found in more expensive 
lenses. A different class of monochromatic aberrations consists of radial distortions. These cause straight 
lines in the scene to appear curved in the resulting image and can be corrected digitally either manually 
or using information about the characteristics of the lens that was used. Radial distortions are the 
result of deviations from a linear projection model and are more pronounced for wider angle lenses. Although 
interactions between the various lens elements of a complex optical system can produce some less well 
de.ned distortions, the two main types considered here are barrel and pincushion distortions. Barrel 
distortion is the result of decreasing lateral magni.cation and it causes straight lines to curve away 
from the image center. Pincushion distortion occurs in lenses with increasing lateral magni.cation and 
causes straight lines to curve towards the image center. Radial distortions can be adequately estimated 
as a fourth degree polynomial. Several methods have been proposed in the literature for estimating the 
distortion parameters of that polynomial. In the simplest case, by photographing a test scene containing 
straight lines, the distortion parameters can be adjusted until the lines in the resulting image are 
also straight. Various software packages offer solutions that reduce or remove the effect of radial distortions. 
Options such as DxO Optics or PTLens use lens-speci.c information while other solutions exist that require 
more manual input, such as for instance Hugin or the Lens Correction module in Adobe Photoshop. Noise 
can be a major cause of image degradation and can be de.ned as any random variation in brightness or 
color in the image caused by the sensor, circuitry or other parts of the camera. Noise can have various 
sources, each requiring different treatment. Generally, noise can be illumination-independent, in which 
case the amount of light hitting the .lm or sensor does not have an effect on the amount of noise, or 
illumination-dependent if there is a correlation between that type of noise and the illumination in the 
scene.  Fixed pattern noise: any noise component that survives frame averaging  DSNU - capture and 
subtract image with no illumination (lens cap on)  PRNU - capture .at, constantly illuminated scenes 
 Fabrication errors can cause different pixels of a sensor array to have different responses to constant 
illumination. This is known as .xed pattern noise and appears in two forms. Dark signal non-uniformity 
(DSNU) is the .xed offset from the average across each pixel of the image sensor, occurring even when 
no illumination is present. Photo response non-uniformity (PRNU) is the variation in the responsivity 
of pixels under constant illumination.   Noise  Dark current: Depends on temperature, time, exposure... 
 Photon Shot noise: Due to random .uctuations in number of photons hitting the sensor Both are more 
apparent in low light conditions. Even if no light reaches the sensor, electrons can still reach the 
quantum-wells, generating dark current shot noise. The severity of this type of noise is affected by 
temperature and consequently can be reduced by cooling the sensor. The number of photons collected by 
each individual quantum-well of an image sensor is not constant for a given intensity. Due to this randomness, 
a given pixel can receive more or fewer photons than the average. The distribution of photons arriving 
at the sensor can be modeled by Poisson statistics (mean  equals variance s2). As such, if the intensity 
of the source increases, so will the variance in the signal. Most of the noise sources discussed do not 
depend on illumination levels in the scenes. As such, in darker scenes, the signal-to-noise ratio becomes 
smaller, effectively amplifying the effect of noise in the image. Although dark current and .xed pattern 
noise can be minimized though the use of appropriate calibration, other noise sources (including photon 
shot noise) cause less predictable patterns. Refer to the work by Healey and Kondepudy for a detailed 
account on noise estimation and camera calibration. HEALEY G., KONDEPUDY R.: Radiometric ccd camera calibration 
and noise estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence 16, 3 (1994) To 
map pixel values to real-world radiometric quantities, the response function of the camera, which is 
the mapping between sensor illuminances to pixel values, needs to be known. Although this information 
is not generally available, the non-linearities for a particular camera can be recovered through the 
use of calibrated input. The relationship between scene radiance and pixel values can be recovered in 
two ways and the process is known as camera characterization. Several approaches exist for .nding the 
transformation between the measured XYZ values and the captured RGB values. Look-up tables store the 
data of such measured/captured pairs. If a large enough number of samples is collected so that the camera 
gamut is densely populated, then XYZ values corresponding to RGB samples not in the look-up table can 
be estimated using three-dimensional interpolation. Alternatively, if the number of samples available 
is not suf.cient for interpolation, a function can be .tted to the available data. This is however much 
more computationally expensive.   As discussed, cameras are not linear light meters. Although these 
non-linearities do make more detail in the image visible (much like gamma correction or the sRGB curve), 
there are cases where a linear relation between pixel values and luminance values is required. One such 
example is merging exposures into an HDR image.  If the accurate representation of the scene illumination 
is necessary, a single exposure may not be suf.cient to cover the full dynamic range in a scene. A series 
of exposures can be captured, which can then be merged into a single high dynamic range image. Each individual 
exposure follows the non-linearities of the camera. As such, before they can be merged into an HDR image, 
the different exposures need to be linearized using the response curve of the camera. The techniques 
described previously can be used to recover this response curve. Alternatively, the sequence of differently 
exposed images can be used directly to recover the response curve. A commonly used method is proposed 
by Debevec and Malik (1997). By exploiting a property of both physical .lm and digital sensors known 
as reciprocity (meaning that halving the irradiance hitting the sensor E and simultaneously doubling 
the exposure time .t will result in the same pixel values p), a linear system is derived. The solution 
to that is the inverse function g-1 to the camera response curve. Once the individual exposures are linearized, 
a set of weights can be used to de.ne their contribution for the .nal HDR image. Several weighting functions 
have been proposed in the literature. Mann and Picard merge the exposures using certainty functions computed 
from the derivatives of the response curves for each differently exposed image. A simpler approach is 
used in where a hat function sup- pressing values that are likely to be under- or over-exposed is applied. 
Finally, inspired by signal theory, Mitsunaga and Nayar propose a weighting function that takes into 
account the signal-to-noise ratio. Despite their differences, any of these approaches will yield satisfactory 
results. In terms of software, many packages are capable of merging exposures into an HDR image. Photoshop 
offers a 'Merge to HDR' function since its CS2 version that allows some manual control over the image 
alignment. Other packages include HDRSoft's Photomatix and HDRShop, both of which offer batch processing 
functionality. Finally, a free option that is only available for Apple computers is Greg Ward's Photosphere,which 
is also an excellent viewer for HDR image collections.  The simplest regularities in images that can 
be explored through statistical analysis are properties of single pixels. The distribution of pixel values 
in images is typically represented with histograms. First order statistics do not capture relationships 
between pixels and as such are not suitable for studying spatial regularities within images. These two 
images show an example of two very different images that would result in identical .rst order statistics. 
The right image is constructed by randomly permuting the pixels of the left image, yet it appears unnatural. 
Statistical analysis of the distribution of pixel intensities can however lead to interesting insights. 
 Statistical moments are commonly employed to quantitatively describe the shape of a distribution. The 
kth moment of a distribution can be computed as shown where c can be any constant. Generally, if c = 
0 then the above equation computes the raw moments of the distribution, while setting c =  gives us 
the central moments (i.e. centered at the mean). The .rst raw moment is the mean  of the distribution 
and the second is the variance s2. The meaning of further moments is less straight forward but the skewness 
S and kurtosis . of a distribution relate to the third and fourth moments respectively. More speci.cally, 
the skewness and the kurtosis are de.ned as shown, where m3 and m4 are the third and fourth central moments 
respectively. Huang and Mumford analyzed more than 4000 greyscale images of natural scenes (taken from 
the database created by J. H. van Hateren) by computing central moments of the log histograms. The value 
of the skewness shows that the distribution is not symmetrical, which can be attributed, at least partly, 
to the presence of sky in many of the images, resulting in a bias towards higher intensities. In addition 
to that, the values of both the skewness and the kurtosis show that the distribution is non-Gaussian. 
  A less skewed distribution was found by Brady and Field (2000) in their analysis of 46 logarithmically 
transformed natural images, while the linear images resulted in a distribution skewed towards darker 
values. Although no exact values were provided for the moments of the distributions, the resulting histograms 
for both cases are shown here. As can be seen from the above examples, although in both cases natural 
images were used and analyzed, the results vary. Generally, the distribution of log intensities for natural 
images does not deviate far from symmetric. Results do depend however on the choice of images.  One 
particular case of interest was explored by Dror et al. (2001) where high dynamic range spherical panoramas 
were analyzed. Two separate datasets were used consisting of 95 images taken from the Teller database 
(http://city.lcs.mit.edu/data) and 9 from Debevec's Light Probe Image Gallery (http://www.debevec.org/Probes/). 
The resulting log intensity histograms for the two datasets are shown here.  The shape of a histogram 
can also be associated with particular qualities of the depicted surface. Motoyoshi et al. studied the 
perceived surface qualities of various materials and analyzed their relation to the associated histograms. 
They found that materials that are perceived as glossier tend to have a positively skewed distribution 
while matte surfaces result in a negatively skewed distribution. The correlation found between the surface 
properties and the skewness of the histograms is shown here. As the diffuse re.ectance of the surface 
increases, the lightness rating as perceived by the observers also increases while the corresponding 
histogram skewness decreases(left). An increase in specular intensity also results in an increased rating 
of glossiness as well as higher skewness value (right). MOTOYOSHI I., NISHIDA S., SHARAN L., ADELSON 
E. H.: Image statistics and the perception of surface qualities. Nature 447, 7141 (2007) Despite their 
simplicity, .rst order statistics have now found several applications in image processing. Studies have 
shown correlations between .rst order statistical regularities in images and properties of the illuminant 
which has proved useful in areas such as white balancing. Moreover, transferring statistical moments 
between images in appropriate color spaces has been demonstrated in what is now known as color transfer. 
These color-related application will be discussed in more detail later in the course. First order statistics 
can also be computed on a single image basis. By manipulating the distribution of values of a single 
image, a variety of effects can be achieved. For instance, the contrast of an image that only covers 
a small portion of the available range of intensities can be increased by adjusting the pixel values 
such that the full range of intensities is more equally represented. This is known as histogram equalization. 
 A more general version of histogram equalization is known as histogram matching and it allows the histogram 
of a source image to be matched to that of a given target. First, the cumulative histograms C of the 
source and target are computed. The inverse function C-1 is then computed, which acts as a reverse lookup 
on the histogram, returning the bin index corresponding to a given count. An example of this technique 
applied on a source-target pair of color images is shown here together with the corresponding histograms 
for the 3 channels. Due to their limitations to single pixel properties, .rst order statistics are the 
simplest to compute and interpret as already discussed. To further explore the relations between pixels, 
their spatial con.gurations and their associations with aspects of human vision more complex transforms 
are necessary. Second and higher order statistics can be used to provide the additional information and 
they will be discussed in the following parts of this course. It is important though to note that the 
statistical tools presented in this part of the course can be used directly on the intensities of an 
image, leading to .rst order statistical results, or on distributions resulting from other transforms, 
leading to 2nd or higher order statistical .ndings.  While the .rst order statistics of an image provide 
considerable information, their focus on individual pixels restricts them to the unstructured aspects 
of the image equivalent to an investigation of individual rays of light. As James J. Gibson (The ecological 
approach to visual perception. 1979) famously pointed out, since the interaction of light rays with an 
object systematic alters light ac- cording to the surface properties of the object, the pattern of change 
across a set of light rays which he referred to as the structured optic array provides direct, reliable 
information about the object. this higher-order structure is of central interest to the human visual 
system. It should not be surprising, then, that considerable attention has been directed towards investigating 
second and even higher order statistics. Natural images consist of two main features: large homogeneous 
surfaces where there is little variation in intensities and large gradients between such surfaces that 
correspond to edges. Second order statistics capture the relations between pairs of pixels, allowing 
us to study these features in images.  Perhaps the simplest way to examine information contained in 
more than a single pixel, is to look at the relationship between pairs of pixels. This is a discrete 
approximation of the .rst derivative or gradient of the intensity function of an image and represents 
the rate of change. There are three different discrete approximations: forward/backward difference, central 
difference, and the sbel operator. The most straightforward is the forward difference method. Here, 
the gradient at a pixel (i,j) is calculated from the difference between it and the next pixel forwards 
as shown, where I(i,j) is the luminance for pixel i, j, Dx is the horizontal gradient and Dy the vertical 
gradient. An obvious variant on this, referred to as the the backward difference method, is to use the 
previous pixel (e.g., I(i-1,j) rather than the next one. Ib both cases, it is common to calculate the 
mean gradient magnitude at a given location from the horizontal and vertical components although one 
may choose to keep the vertical and horizontal gradients separate A slightly more robust method is to 
calculate the central differences by considering the pixels before and after the pixel in question. As 
a larger neighborhood is considered, this approach results in a better approximation.  Ii,j+1 Ii-1,j 
Ii,j  Ii-1,j Ii,j Ii+1,j Ii,j-1  An even larger neighborhood can be taken into account by using the 
Sbel operator which also looks at the diagonal pixels. This is nearly isotropic but is more computationally 
expensive than the simpler approaches. To compute the gradient distribution for an image ensemble, the 
individual distributions for each image need to be computed .rst and then averaged. One may choose to 
consider horizontal and vertical gradients separately or average them. It has been repeatedly found 
that the gradient histogram has a very sharp peak at zero and then falls off very rapidly (with long 
tails at the higher gradients). The distribution can be modeled as e-xa with a < 1. The reason for the 
speci.c shape of the gradient distribution seems to be that images contain many, large surfaces which 
tend to be smooth and somewhat homogenous (so that the gradients tend to be near zero) along with a few 
high contrast edges (which will yield very high gradients) . An object often .nds itself over a consistent 
background, which means that the transition from the object's surface to the background will be similar 
all along the object's silhouette. This is re.ected in the symmetry of the gradient distribution around 
the central peak. Images contain information at different spatial scales, from the large to very small. 
As is well known from Fourier analysis, sharp edges, can be described by a weighted sum of sinusoids, 
with higher frequencies being weighted less. In other words, natural images contains information at many 
spatial frequencies. An examination of the relative power of the different spatial frequencies reveals 
several interesting trends, which are so prominent, that most work on image statistics provides an analysis 
of the power spectrum.  The power spectrum S(u,v) of an N x N image is given by: S(u,v) = |F (u, v)|2 
/ N2 where F is the Fourier transform of the image, and (u,v) are pixel indices in Fourier space. Two-dimensional 
frequencies can also be represented with polar coordinates (f,.), using u = f cos(.) and v = f sin(.). 
 While the sprectra of individual images varies considerably (which may play a role in the rapid detection 
of certain scenes or objects), when the averaging over a suf.ciently large number of images (and across 
orientation), a clear pattern arises: the lower frequencies contain the most power, with power decreasing 
as a function of frequency. In fact, on a log-log scale, amplitude as function of frequency lies approximately 
on a straight line. That is, the averaged spectrum tends to follow a power law, which can be well modeled 
with P = 1/ f, where P is the power as function of frequency f , and  is spectral slope.  A number 
of studies have reported values for the spectral slope for different image ensembles. While the particulars 
of the image ensembles vary considerably, they tend to focus on natural scenes (which usually means simply 
eliminating carpentered environments). The average spectral slope varies from 1.8 to 2.4, with most values 
clustering around 2.0. One of the most prominent facets of such a power law is that these images tend 
to be scale invariant. That is, they share features at every scale, which is also a hallmark of fractal 
images. In practice, this means that we should be able to zoom in and out of an image, or travel through 
a natural scene, and expect that the statistics will remain roughly constant. It is also worth noting 
that, according to the the Wiener-Kintchine theorem, the power spectrum and the auto-correlation function 
form a Fourier transform pair. This means that the spectral slope of image ensembles can be interpreted 
as describing relations between pairs of pixels. Intuitively, the means that since since the surface 
of a given object tends to be rather homogenous, it is expected that neighboring pixels will be similar 
and that the farther apart two pixels are, the less similar they will be. There is also some emerging 
evidence that the slope of the power spectrum is distinct for different scenes or objects. For example, 
Huang and Mumford examined 216 images which had been painstakingly segmented into pixels representing 
11 different categories and found that although the image ensemble had an average slope of 1.96, there 
were systematic differences in the slopes across categories. Speci.cally, the slopes were 2.3, 1.8, 1.4, 
and 1.0 for man-made, vegetation, road, and sky pixels, respectively. Likewise, Webster and Miyahara 
analyzed the power spectra and rms-contrast of 48 natural scenes. They found signi.cant differences in 
both spectral slope and contrast across the three scene types (2.15, 2.23, and 2.4 for the forest, close-up, 
and distant meadow scenes, respectively) HUANG J., MUMFORD D.: Image Statistics for the British Aerospace 
Segmented Database. Tech. rep., Division of Applied Math, Brown Univeristy, 1999  TORRALBA A., OLIVA 
A.: Statistics of natural image categories. Network: Computation in Neural Systems 14 (2003)  WEBSTER 
M., MIYAHARA E.: Contrast adaptation and the spatial structure of natural images. Journal of the Optical 
Society of America A 14 (1997) Furthermore, when the power spectra are not averaged over all orientations, 
it is clear that there is some variation as a function of angle: natural images tend to concentrate their 
power in horizontal and vertical angles. For example, in examining over 6000 man-made and 6000 natural 
scenes, Torralba and Oliva found that the slope varied as a function of both scene type and orientation 
(with slopes of 1.98, 2.02, and 2.22 for horizontal, oblique, and vertical angles in natural scenes and 
1.83, 2.37, and 2.07 for man-made scenes). Thus, the spectral slope may be be useful in object or scene 
discrimination. It is critical to mention that if two images have similar spectral slopes, then swap- 
ping their power spectra will not affect recognition as long as phase information is preserved.  BADDELEY 
R. J., ABBOTT L. F., MICHAEL C. A., BOOTH C. A., SENGPIEL F., FREEMAN T., WAKEMAN E. A., ROLLS E. T.: 
Responses of neurons in primary and inferior temporal visual cortices to natural scenes. Proceedings 
of the Royal Society B: Biological Sciences 264, 1389 (1997)  OLIVA A., TORRALBA A.: Modeling the shape 
of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision 
42, 3 (2001)  RUDERMAN D. L.: Origins of scaling in natural images. Vision Research 37 (1997)  SWITKES 
E., MAYER M. J., SLOAN J. A.: Spatial frequency analysis of the visual environment: Anisotropy and the 
carpentered environment hypothesis. Vision Research 18, 10 (1978)  VAN HATEREN J., VAN DER SCHAAF A.: 
Independent component .lters of natural images compared with simple cells in primary visual cortex. Proceedings 
of the Royal Society of London B 265 (1998)  TORRALBA A., OLIVA A.: Statistics of natural image categories. 
Network: Computation in Neural Systems 14 (2003)  BILLOCK V. A., CUNNINGHAM D. W., TSOU B. H.: What 
visual discrimination of fractal textures can tell us about discrimination of camou.aged targets. In 
Proceedings of Human Factors Issues: In Combat Identi.cation and in Image Analysis Using Unmanned Aerial 
Vehicles (2008)  TOLHURST D. J., TADMOR Y., CHIAO T.: Amplitude spectra of natural images. Ophthalmic 
and Physiological Optics 12 (1992)  TADMOR Y., TOLHURST D. J.: Both the phase and the amplitude spectrum 
may determine the appearance of natural images. Vision Research 33, 1 (1993)     When taking a photograph 
of a scene, it is not uncommon that either the camera or an object in the scene moves. The longer the 
aperture is open, the more likely this is to be the case. As a result, all or part of the image will 
be blurred. A number of approaches for sharpening an image have been proposed. One type of approach, 
blind motion deconvolution, essentially treats the photograph as the result of a convolution between 
an unknown sharp image and an equally unknown blurring kernel. The goal is to estimate the blur kernel 
so that it can be deconvolved with the blurred image to yield the sharp image. Naturally this is an underspeci.ed 
problem, so additional constraints are needed and recently, a number of researchers have employed natural 
image statistics to provide them.  Applications: Blind motion Deconvolution Constraint: Real world 
image must follow power law (Caron et al, 2002; Jalobeanu et al, 2002; Neelamani et al, 2004) Constraint: 
Estimate Blur by optimizing to match real gradient distributions (Fergus et al, 2006) For example, Caron 
et al. assume that the sharp image's power spectra follows a power law distribution, while Jalobeanu 
et al. and Neelamani et al. use an interaction between power spectra and wavelets. Fergus et al. estimate 
the blur kernel by optimizing for the gradient distributions using priors derived from natural images. 
All of these approaches assume camera motion. That is, that there is a single blur kernel for the entire 
image ARON J. N., NAMAZI N. M., ROLLINS C. J.: Non-iterative blind data restoration by use of an extracted 
.lter function. Applied Optics 41, 32 (2002)  JALOBEANU A., BLANC-FRAUD L., ZERUBIA J.: Estimation 
of blur and noise parameters in remote sensing. In In Proc. of Int. Conference on Acoustics, Speech and 
Signal Processing (2002)  NEELAMANI R., CHOI H., BARANIUK R.: ForWaRD: Fourier-wavelet regularized deconvolution 
for ill-conditioned systems. Signal Processing, IEEE Transactions on 52, 2 (2004)  FERGUS R., SINGH 
B., HERTZMANN A., ROWEIS S., FREEMAN W.: Removing camera shake from a single photo- graph. ACM Transactions 
on Graphics (TOG) 25 (2006)   Applications: Blind motion Deconvolution In an alternate approach, Levin 
use the gradient structure to .nd those pixels that are blurred and segment them from the rest of the 
image. Additionally, rather than use statistics of sharp images, of the blurring process. Speci.cally, 
they model how the the gradient distribution changes as a function of blurring to discovering the blurring 
kernel for a given image. One primary features of motion blurring is the attenuation of higher frequencies. 
This shows in the slope of the power spectra (by increasing ) as well as in gradient histogram (e.g., 
in particular by removing the longer tails at the higher gradients). Levin attempts to recover the blur 
kernel by applying different blurs to the image to .nd the one that produces a gradient histogram that 
matches the blurred one. This requires a non-blurred image or image region. Using a completely different 
image tends not to produce reliable results (due to differences in the underlying gradient histogram). 
Since much terrestrial motion tends to be horizontal, the majority of the effects of motion blurring 
are also horizontal. Thus, the vertical gradients can under some circumstances be used to calculate the 
blur of the horizontal components. LEVIN A.: Blind motion deblurring using image statistics. Advances 
in Neural Information Processing Systems (2007) One image processing application area where image statistics 
have been used successfully is known as image inpainting. In order to remove an unwanted object from 
an image, the texture that would otherwise be behind that object need to be re-created, a task that can 
be particularly dif.cult if the holes are large. Among the various approaches, several have used image 
statistics to .nd optimal replacements for the missing texture and structure, usually by assuming that 
a patch similar to the missing portion can be found somewhere in the remaining image. For example, Hirany 
&#38; Totsuka, use a combination of spectral and spatial information to .nd the replacement patch. Levin 
et al. select the missing section based on the gradients at the boundary region as well as to maximize 
the match the global gradient histogram. Shen et al. .ll in the missing region by completing the gradient 
maps and then reconstructing the image by solving a Poisson equation.  HIRANI A. N., TOTSUKA T.: Combining 
frequency and spatial domain information for fast interactive image noise removal. In SIGGRAPH '96 (1996) 
 LEVIN A., ZOMET A., WEISS Y.: Learning how to inpaint from global image statistics. In Proceedings 
of the Ninth IEEE International Conference on Computer Vision (2003)  SHEN J., JIN X., ZHOU C., WANG 
C. C. L.: Gradient based image completion by solving poisson equation. Computers and Graphics (2005) 
    Whereas second order statistics take into account pairs of pixels through various transformations 
(in particular gradients and the amplitude spectrum), it is possible to .nd statistical regularities 
in larger groups of pixels using suitable image manipulations. The statistics that we are looking at 
in this section are listed on this slide. It can be argued that although statistical regularities are 
present in power spectra of natural images, much of the perceptually relevant information is encoded 
in phase spectra. In this example, we have swapped the phase spectra of two images. As can be seen, 
much of the image structure has swapped. Thus, the image structure is largely encoded into the phase 
spectrum. Second order statistics such as the auto-correlation function (and therefore power spectra 
in Fourier space) and variance are insensitive to signal phase. They therefore do not adequately measure 
image structure. To gain access to phase information without polluting the results with .rst and second 
order information, we can whiten the images .rst. This amounts to adjusting the spectral slope to become 
.at. The auto-correlation function will therefore be zero everywhere, except at the origin. THOMSON 
M. G. A.: Visual coding and the phase structure of natural scenes. Network: Computation in Neural Systems 
10 (1999)  THOMSON M. G. A.: Beats, kurtosis and visual coding. Network: Computation in Neural Systems 
12 (2001)  After whitening, we can compute .rst order statistics on resulting images  Can be seen 
as computing statistics on edges  Results show positive skew and high kurtosis,   i.e. long tails 
 Nonetheless, phase spectra remain somewhat of a mystery By removing the second moment from consideration, 
it is now possible to compute skewness and kurtosis on the whitened signal. The whitened skew and whitened 
kurtosis are thus a measure of variations in the phase spectra. The results of applying this procedure 
to a set of natural images, leads to the conclusion that the whitened images are almost always positively 
skewed and are always positively kurtosed. In contrast, if the phase spectrum is randomized on the same 
images, the whitened skewness and kurtosis are close to zero. While positive skewness and kurtosis of 
phase spectra points in the direction of the presence of statistical regularities in the phase spectrum, 
these results are relatively weak and do not easily explain aspects of human vision. Furthermore, they 
do not appear to have found employ in any graphics related applications that we are aware of.  Phase 
spectra are computed over entire images  What about spatially localized analysis?  Wavelets do this 
  They are also selective to speci.c orientations and scales  We have seen that second order statistics, 
and in particular the amplitude spectrum of image ensembles, show striking statistical regularities. 
The amplitude spectrum can be analyzed for different orientations, but as it is computed in Fourier space, 
it provides information of whole images rather than regions of images. In this section we study transforms 
that allow us to analyze images in local regions of space, as well as for different orientations and 
spatial frequencies. Such models and transforms started with Gabor who introduced the notion of time-frequency 
representations. It is noted that such systems are in some sense similar to how images are analyzed by 
portions of the human visual system. There are several transforms that allow us to analyze images in 
local regions of space, as well as for different orientations and spatial frequencies. Such models and 
transforms started with Gabor who introduced the notion of time-frequency representations. Many wavelet 
transforms are self-similar, consisting of spatially localized bandpass basis functions which vary only 
by a dilation, translation or rotation. They have found many direct applications in image analysis and 
image processing.   Both phase and amplitude can be measured and correlated in a wavelet decomposition 
  Surprising result: natural images are scaleinvariant in both phase and amplitude  Field has shown 
that both phase and amplitude can be measured and correlated across scales and location. Although the 
amplitude spectrum has shown scale invariance, the phase structure revealed by Fourier space analysis 
remains somewhat of a mystery. However, by analyzing wavelets, which are simultaneously localized both 
in space and frequency, it can be shown that natural images are scale in variant in both phase and amplitude. 
 This means that the locations of edges are correlated across scales and orientation  At higher frequencies, 
neighbors in frequency become more highly correlated  At higher frequencies, neighbors in space become 
less correlated  Phases in natural images contain alignment that extends over greater ranges at higher 
frequencies  This means that in local image regions, the location of edges are correlated across scales. 
At higher frequencies neighbors in frequency become more highly correlated, whereas neighbors in space 
become less correlated. In other words, phases in natural images contain alignment that extend over greater 
ranges at higher frequencies. Phase structures tend to change somewhat in orientation as well as position 
when moving across scales. Such behavior is well modeled with wavelets. It is therefore possible that 
this may help explain why mammalian visual system use wavelet-like structures in the visual cortex. It 
may well be that this is why wavelets are useful in a range of applications, as outlined above. It was 
found that the distributions of histograms of wavelet coef.cients are highly non-Gaussian, showing in 
particular long tails, i.e. having high kurtosis. In particular, these histogram distributions can be 
modeled by a generalized Laplacian. If the bandwidths of the wavelet .lters are chosen to be around 
1-2 octaves, that the kurtosis of the histograms of wavelet transforms is maximal. This means that under 
these conditions the resulting transform is maximally sparse. In clusters of neurons, sparsity is an 
important feature. It means that much of the variability in the input signal is explained by a small 
number of neurons. Such neuronal activity is metabolically ef.cient, minimizes wiring length, and increases 
capacity in associative memory  An example application of wavelet statistics is image denoising, where 
a Bayesian approach is employed to remove noise. Assuming that the wavelet coef.cients of natural images 
follow a generalized Laplacian distribution, as outlined above, Gaussian image noise translates into 
Gaussian pollution of wavelet coef.cients. SIMONCELLI E. P.: Bayesian denoising of visual images in 
the wavelet domain. In Bayesian Inference in Wavelet Based Models, Mller P., Vidakovic B., (Eds.). Springer-Verlag, 
New York, (1999)  PORTILLA J., SIMONCELLI E. P.: Image denoising via adjustment of wavelet coef.cient 
magnitude correlation. In Proceedings of the 7th International Conference on Image Process- ing (2000) 
  An application area where wavelets have found extensive use is object detection. An important problem 
in that area is .nd- ing an effective and compact way of characterizing objects in images. Some object 
categories, such as faces, show surprisingly little variation in terms of overall structure and color, 
other types of objects however can be more complex. To overcome this issue, wavelets, and in particular 
Haar wavelets, can be used to provide a standardized template for characterizing objects in images. Using 
a training scheme, a small subset of Haar wavelets can be derived that can appropriately describe the 
structure of a particular class of images, such as for instance faces or pedestrians. This set of basis 
functions then forms the classi.er that can be used to detect the object in question in new image. More 
recently, a real-time face detection algorithm was proposed by Viola and Jones where features are applied 
on the image in a cascade fashion, ensuring that more complex classi.ers are only used on instances that 
have been already accepted by simpler ones. Although the features used in this work are similar to Haar 
wavelets, some are more complex, as shown here. Image-based retrieval using the wavelet decomposition 
of each image has also been demonstrated by Jacobs et al. An image querying metric is developed using 
truncated, quantized versions of the wavelet decompositions of the images. This forms a compact signature 
for each image that is fast to compute and captures suf.cient amount of information to allow searching. 
 OREN M., PAPAGEORGIOU C., SINHA P., OSUNA E., POGGIO T.: Pedestrian detection using wavelet templates. 
In IEEE Conference in Computer Vision and Pattern Recognition, (1997)  PAPAGEORGIOU C. P., OREN M., 
POGGIO T.: A general framework for object detection. In Sixth International Conference on Computer Vision, 
(1998)  VIOLA P., JONES M.: Robust real-time object detection. International Journal of Computer Vision 
(2001).  JACOBS C. E., FINKELSTEIN A., SALESIN D. H.: Fast multiresolution image querying. In SIGGRAPH 
'95 (1995) One of the main properties of natural images is that pixel values are correlated. Nearby 
pixels tend to have similar values while these correlations tend to be less strong as the distance between 
pixels increases. One way to analyze these correlations is by decomposing the image data to a set of 
components that are as decorrelated as possible. Each of these components will then capture some particular 
mode of variation within the data.  HANCOCK P., BADDELEY R. J., SMITH L.: The principal components of 
natural images. Network: Computation in Neural Systems 3, 1 (1992)   Principal component analysis (PCA 
 also known as the Karhunen-Love transform) is a solution commonly employed to that end. By computing 
the eigenvectors and corresponding eigenvalues of the covariance matrix of the image collection, a set 
of orthogonal, decorrelated components can be derived. PCA can be used to reduce the dimensionality 
of a given set. This observation has been used in object and in particular face recognition applications 
successfully. The face recognition method known as Eigenfaces is one example. The database of face images 
is decomposed into a set of eigenvectors and eigenvalues, each of which corresponds to some dimension 
along which faces vary. Faces tend to have a lot of common characteristics in terms of structure, making 
them an ideal subject for dimensionality reduction. TURK M., PENTLAND A.: Eigenfaces for recognition. 
Journal of cognitive neuroscience 3, 1 (1991)  TURK M., PENTLAND A.: Face recognition using eigenfaces. 
In Proceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition (1991)   In order 
to match input faces to samples in the database, the new face can be projected to the computed "face 
space". It then becomes a simple matter of computing the distance between this new point and the database 
to decide whether the input is in fact a face. Recognition can then take place by .nding the points closest 
to the input. While Principal Components Analysis measures covariances of pixels separated by varying 
distances, the components that are found constitute a linear transform. This means that although the 
data may be decorrelated, there is no guarantee that the resulting components are in any way independent. 
In particular, only if the input data happens to have a Gaussian distribution, then the resulting principal 
components are both decorrelated and independent. As we have seen, many of the statistics of natural 
images that are currently known point in the direction of high kurtosis, i.e. they are highly non-Gaussian. 
This has given rise to the use of Independent Components Analysis (ICA), which is a technique to separate 
a multivariate signal into a set of components that will sum to the original signal under the assumption 
that the input is non-Gaussian. Like PCA, ICA is a special case of blind source separation, which is 
a general collection of techniques that try to separate a set of signals from a mixed signal without 
prior knowledge of the source signals or the mixing process. ICA algorithms tend to require a set of 
preprocessing steps to make the problem more tractable. In particular, the data need to be centred and 
whitened. Also, data reduction is often applied. The whitening can be achieved by running the PCA algorithm 
.rst. By keeping only the .rst n components, data reduction is achieved. It ensures that only those components 
are used that are meaningful to the problem being solved. Moreover, it speeds-up the computations required 
to determine independent components. Several algorithms are known, including infomax (Bell &#38; Sejnowski), 
extended infomax (Lee et al.), fastICA (Hyvrinen), Jade (Cardoso), kernel ICA (Bach &#38; Jordan) and 
RADICAL (Learned-Miller &#38; Fisher). Although the implementations vary, their goals are the same: represent 
multivariate data as a sum of components in such a way that the components reveal the underlying structure 
that gave rise to the observed signal. COMON P.: Independent component analysis: A new concept. Signal 
Processing 36, 3 (1994)  HYVRINEN A., KARHUNEN J., OJA E.: Independent Component Analysis. John Wiley 
&#38; Sons, (2001)  ACHARYYA R.: A New Approach for Blind Source Separation of Convolutive Sources: 
Wavelet Based Separation Using Shrinkage Function. (2008)  BELL A. J., SEJNOWSKI T. J.: An information- 
maximization approach to blind separation and blind deconvolution. Neural Computation 7, 6 (1995)  LEE 
T.-W., GIROLAMI M., SEJNOWSKI T. J.: Independent component analysis using an extended infomax algorithm 
for mixed sub-gaussian and super-gaussian sources. Neural Computation 11, 2 (1999)  HYVRINEN A.: Fast 
and robust .xed-point algorithms for independent component analysis. IEEE Transactions on Neural Networks 
10, 3 (1999)  CARDOSO J. F.: High-order contrasts for independent component analysis. Neural Computation 
11, 1 (1999)  BACH F. R., JORDAN M. I.: Kernel independent component analysis. Journal of Machine Learning 
Research 3 (2002)  LEARNED-MILLER E. G., FISHER III J. W.: ICA us- ing spacings estimates of entropy. 
Journal of Machine Learning Research 4 (2003)  One could ask what are the independent components in 
the context of natural images. It could be argued that these are the objects that comprise a scene. It 
is exactly these that provide the structure that gives rise to the pixel array that is analyzed by computer 
algorithms as well as by the human visual system. Both PCA and ICA, however, are generative models which 
are based on linear superposition. Thus, they cannot take into account translation and rotation of objects. 
However, it has been found that small image patches of natural image ensembles can be analyzed using 
ICA, revealing structures that resemble those found in the human visual system. In particular, this technique 
yields Gabor-like patches elongated structures that are localized in space, orientation and frequency. 
Their size, aspect ratio, spatial frequency bandwidth, receptive .eld length and orientation tuning bandwidth 
are similar to those measured in cortical cells. These results lend credence to the argument that the 
human visual system appears to represent natural images with independent variables, each having a highly 
kurtotic distribution that leads to a metabolically ef.cient sparse coding. OLSHAUSEN B. A., FIELD D. 
J.: Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research 37 (1997) 
 BELL A. J., SEJNOWSKI T. J.: The .independent components' of natural scenes are edge .lters. Vision 
Research 37 (1997)  VAN HATEREN J., VAN DER SCHAAF A.: Independent component .lters of natural images 
compared with simple cells in primary visual cortex. Proceedings of the Royal Society of London B 265 
(1998)    Images are formed as light that is transduced to electric signals. This happens both in 
the retina as well as in electronic image sensors. Before that, light is emitted and usually re.ected 
several times off different surfaces before it reaches a sensor. This process is modeled by the rendering 
equation. This equation models the behavior of light at a surface: light Li impinging on a surface point 
from all directions O is re.ected into an outgoing direction of interest, weighted by the bi-directional 
re.ectance distribution function (BRDF) fr. The amount of light Lo going into that direction is then 
the sum of the re.ected light and the light emitted by the surface point (which is zero unless the surface 
is a light source). Here, we have speci.cally made all relevant terms dependent on wavelength . to indicate 
that this behavior happens at all wavelengths, including all visible wavelengths. This means that Lo(.) 
can be seen as a spectral distribution. Under photopic lighting conditions, the human visual system contains 
three cone types, each with a different peak sensitivity. Thus, each cone type integrates the incident 
light according to a different weight function. A spectral distribution therefore gives rise to a triple 
of numbers (L, M, S) which represent the signal that is passed on from the photoreceptors to the remainder 
of the human visual system. Such triples are called tristimulus values. An important implication of 
this behavior is that different spectral distributions can integrate to the same tristimulus values. 
Such spectral distributions are then necessarily perceived to be identical, which is known as metamerism. 
It is the reason that we are able to build display devices with three primaries that are not the same 
as the responsivities of the cones: tristimulus values in one color space can be converted to tristimulus 
values in another color space, and although they do not represent the same spectral distributions, through 
metamerism they lead to the same percept. Further, rather than study spectral distributions, we can limit 
ourselves to the study of tristimulus values. Humans tend to effortlessly discount the illumination 
of their environments. In other words, the color of the light sources in a particular viewing environment 
are to a large extent ignored, so that human vision is fairly good at detecting surface colors. This 
phenomenon is known as color constancy. Computational models that aim to achieve the same are known as 
white balancing algorithm. They intend to remove color casts from images due to illumination. As light 
is re.ected off surfaces according to the rendering equation, reconstructing either surface re.ectance 
and/or illumination is an under-constrained problem. This means that white balancing can only be achieved 
by means of statistical assumptions. In color spaces such as the aforementioned LMS space, equal values 
of the three components denote achromatic colors. One way to achieve such neutral colors is to start 
with an equal energy spectrum, i.e. a spectral distribution which has the same value Lo for each wavelength 
.. This could happen if a scene is illuminated by an equal-energy light source, a source that emits the 
same energy at all wavelengths. If the BRDF then also re.ects all wavelengths equally, a neutral color 
would be achieved. In practice, a scene is illuminated by only one or at most a few light sources, with 
an emission spectrum that is off- white. The BRDFs in a scene are much more variable. How- ever, a surprising 
.nding is that when a collection of BRDFs are averaged, the result ends up being a distribution function 
that is close to grey. This is known as the grey-world assumption. REINHARD E., KHAN E. A., AKYZ A. 
O., JOHNSON G. M.: Color Imaging: Fundamentals and Applications. A K Peters, Wellesley, 2008 The implication 
of this .nding is that if we were to average the colors of all pixels in an image, and the grey-world 
assumption holds, the average tristimulus value is a good indicator of the color of the light source. 
Of course, when averaged over natural image ensembles, the grey world assumption does tend to hold. In 
single images this assumption may or may not hold. In particular, if the surface colors in the scene 
are biased towards some speci.c saturated color, the average re.ectance will not tend toward grey.  
 The grey world assumption is often used to aid white balancing. After all, if we know the color of 
the illuminant, then we can correct all pixels by simply dividing all pixels by the average image value. 
The grey-world assumption is a statistical argument that is necessary to perform white balancing on images 
in the absence of further information about the illuminant, given that white balancing is by itself an 
underconstrained problem. Note that we have glossed over the fact that this procedure is best applied 
in a perceptual color space, such as LMS, thereby mimicking chromatic adaptation processes that occur 
in the human visual system. If an image is given in a different color space, most likely the sRGB space, 
the image should .rst be converted to LMS. The approximation of the illuminant can be improved by excluding 
the most saturated pixels from the estimation. Alternatively, the image can be subjected to further statistical 
analysis to determine if the color distribution is due to colored surfaces or due to a colored light 
source. The image can be .rst converted to the CIELAB color opponent space. Ignoring the lightest and 
darkest pixels as they do not contribute to a reliable estimate, the remaining pixels are used to computed 
a two- dimensional histogram F (a,b) on the two chromatic channels a and b. In each channel, the chromatic 
distributions are modeled using the mean and variances of the histogram projections onto the a and b 
axes. In CIELAB neutral colors lie around the (a,b) = (0,0) point. To assess the strength of the cast, 
we need to measure how far the histogram lies from this point. If the spread of the histogram is small, 
and lies far away from the origin, the image is likely to be dominated by strong re.ectances rather than 
illumination. FUNT B., BARNARD K., MARTIN L.: Is machine colour constancy good enough? In Proceedings 
of the 5th European Conference on Computer Vision (1998)  ADAMS J., PARULSKI K., SPAULDING K.: Color 
processing in digital cameras. IEEE Micro 18, 6 (1998),  GASPARINI F., SCHETTINI R.: Color balancing 
of digital photos using simple image statistics. Pattern Recognition 37 (2004)  It was found that while 
grey-world algorithms work well on some images, alternate solutions such as the white-patch algorithm 
(Land) perform better on texture-rich images. The white-patch algorithm assumes that the lightest pixels 
in an image depict a surface with neutral re.ectance, so that its color represents the illuminant. Both 
the grey-world and white patch algorithms are special instances of the Minkowski norm. LAND E.: The 
retinex theory of color vision. Scienti.c American 237, 6 (1977)  FINLAYSON G., TREZZI E.: Shades of 
gray and colour constancy. In Proceedings of the Twelfth Color Imaging Conference (2004) A further generalized 
assumption can be made about images, which is that the average difference between two pixels evaluates 
to grey. This is known as the grey-edge assumption.  VAN DE WEIJER J., GEVERS T.: Color constancy based 
on the grey-edge hypothesis. In Proceedings of the International Conference on Image Processing (2005) 
 GIJSENIJ A., GEVERS T.: Color constancy using natural image statistics. In IEEE Computer Society Conference 
on Computer Vision and Pattern Recognition (2007)   Having noted that many color constancy and white 
balancing algorithms exist, with none of them universally applicable, Gijsenij and Gevers use natural 
image statistics to classify an image, and then use this classi.cation to select the most appropriate 
white balancing algorithm for the image at hand. In particular, they use the .nding that the distribution 
of edge responses in an image can be modeled by means of a Weibull distribution. It is now possible to 
.t Weibull parameters to the derivatives of a large number of images, and correlate their values with 
the white balancing algorithm that performs best for each image. The parameter space tends to form clusters 
where a speci.c algorithm tends to produce the most accurate result. This means that the Weibull distribution 
can be used to select the most appropriate white balancing algorithm for each image individually. GIJSENIJ 
A., GEVERS T.: Color constancy using natural image statistics. In IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition (2007)  GEUSEBROEK J., SMEULDERS A.: A six-stimulus theory for 
stochastic texture. International Journal of Computer Vision 62, 1-2 (2005)  If the average surface 
re.ectance is grey, and light sources tend towards white, then in the LMS color space and similar RGB-like 
spaces, values in one color channel tend to be good predictors of values in the other two color channels. 
In other words, if we .nd a high value for a red pixel, then chances are that the green and blue values 
are high as well. This means that the color channels in such color spaces are highly correlated. An 
interesting experiment was conducted whereby a set of spectral images was converted to log LMS space, 
before being subjected to Principal Components Analysis (PCA) (Ruderman et al.). The logarithm was taken 
as a data-conditioning measure. Applied in this manner, PCA rotates the axes to the point where they 
are maximally decorrelated. The resulting color space is termed L a where L is a light- ness channel, 
and a and  are color opponent channels. By starting in LMS cone space, the rotation yields a new color 
space which surprisingly corresponds closely to the color opponent space found in the ganglion cells 
(these are the cells that transport the visual signal from the retina through the optic nerve to the 
lateral geniculate nucleus, thought to be a relay station in the brain). Color opponent spaces are characterized 
by a single achromatic channel, typically encoding luminance or light- ness, and two chromatic axes which 
can be thought of as spanning red-green and yellow-blue (although the axes do not precisely correspond 
to these perceptual hues). The chromatic axes can have positive and negative values; a positive value 
can for instance denote the degree of redness, whereas a negative value in the same channel would denote 
the degree of greenness. A consequence of color opponency is that we are not able to simultaneously perceive 
an object to have green and red hues. Although we may describe objects as reddish-blue or yellowishgreen, 
we never describe them as reddish-green. The same holds for yellow and blue. RUDERMAN D. L., CRONIN T. 
W., CHIAO C.: Statistics of cone responses to natural images: Implications for visual coding. Journal 
of the Optical Society of America A 15, 8 (1998)  Statistical Decorrelation For 2000 random samples 
drawn from each of the 3 images shown earlier, their distribution is plotted in the next two slides. 
The point clouds form more or less diagonal lines in RGB space when pairs of channels are plotted against 
each other, showing that the three color channels in RGB space are almost completely correlated for these 
images. This is not the case for the same pixels plotted in L a space. Correlations in RGB/LMS   
              Color Opponent Space           La color space decorrelated - values 
of pixels in one channel do not predict the values in another L - luminance a, - opponent channels 
The color opponency of the La space is demonstrated here, where the image is decomposed into its separate 
channels. The image representing the a channel has the  channel reset to 0 and vice versa. We have retained 
the luminance variation here for the purpose of visualization. The image showing the luminance channel 
only was created by setting both the a and  channels to zero. The fact that natural images can be transformed 
to a decorrelated color space, which coincides with a color decomposition occurring in the human visual 
system, points to a beautiful adaptation of human vision to its natural input. It also points to several 
applications that are possible due to the fact that opponent spaces are decorrelated. In particular, 
we highlight color transfer, an algorithm that attempts to transfer the look and feel of one image to 
another on the basis of its color content. Due to the three-dimensional nature of color spaces, this 
is in the general sense a complicated three-dimensional problem. However, by converting to a color opponent 
space, the data in each channel will be decorrelated from the other channels. As a .rst example, the 
histogram projection onto the a and b axes yields meaningful results as the CIELAB space used here is 
an example of a color opponent space.  Perhaps one of the most direct applications of the use of decorrelated 
color spaces is color transfer between images. Earlier, we mentioned that the shape of the distribution 
could be matched between two images, a process known as histogram matching. Although this is a simple 
way to transfer the color palette between two images, the results are often less than ideal.  Many different 
techniques exist that aim to transfer some statistical properties of one image to another. One of the 
simplest approaches was proposed by Reinhard et al. where the mean and standard deviations of the three 
channels are matched. To make this process straightforward, images can be converted to a decorrelated 
color space, such as the La space proposed by Ruderman. In this space, properties of all the pixels 
can be computed separately in each of the three axis. Doing this for both a source and a target image 
yields a set of factors and terms that can be used to transform the pixel data in one image to match 
the appearance of the other image. E REINHARD, M ASHIKHMIN, B GOOCH, P SHIRLEY: Color transfer between 
images. IEEE Computer Graphics and Applications (2001) Of course using a decorrelated color space such 
as Lab cannot guarantee that every image will in fact be perfectly decorrelated. To rectify that, statistical 
analysis similar to what Ruderman did can be performed on the actual input images to derive image-speci.c 
color spaces, or the color palette transfer can be handled as a 3D problem instead of 3 independent 1D 
problems.  ABADPOUR A., KASAEI A.: A fast and ef.cient fuzzy color transfer method. Proceedings of the 
IEEE Symposium on Signal Processing and Information Technology (2004)  GRUNDLAND M., DODGSON N.A.: Color 
histogram speci.cation by histogram warping. Proceedings of SPIE (2005)  NEUMANN L., NEUMANN A.: Color 
style transfer techniques using hue, lightness and saturation histogram matching. Computational Aesthetics 
2005 (2005) p  PITIE F., KOKARAM R., DAHYOT R.: N-dimensional probability density function transfer 
and its application to colour transfer. International Conference on Computer Vision (ICCV'05), Beijing 
(2005)  SENANAYAKE C.R., ALEXANDER D.C.: Colour Transfer by Feature Based Histogram Registration. British 
Machine Vision Conference (2007)  XIAO X., MA L.: Color transfer in correlated color space. Proceedings 
of the 2006 ACM international conference on Virtual reality continuum and its applications (2006) pp. 
309  POULI T,, REINHARD E.: Progressive Histogram Reshaping for Creative Color Transfer and Tone Reproduction, 
NPAR (2010)        The following section is an extract from the book Color Imaging: Fundamentals 
and Applications by Erik Reinhard, Erum Arif Khan, Ahmet O. guz Akyuz and Garrett M. Johnson, published 
by AK Peters in 2008. 1  1 Natural Image Statistics Images of natural scenes tend to exhibit statistical 
regularities, which are important for the understanding of the human visual system that has evolved to 
interpret such natural images. In addition, these properties may prove valuable in image-processing applications. 
If all greyscale images that consist of 1000  1000 pixels are considered, then the space that these 
images occupy would be a million-dimensional space. Most of these images never appear in our world: natural 
images form a sparse subset of all possible images. In recent years, the statistics of natural images 
have been studied to understand how properties of natural images in.uence the human visual system. In 
order to assess invariance in natural images, usually a large set of calibrated images are collected 
into an ensemble and statistics are computed on these ensembles, rather than on individual images. Natural 
image statistics can be characterized by their order. In particular, .rst-order, second-order, and higher-order 
statistics are distinguished [34]: First-order statistics treat each pixel independently, so that, for 
example, the distribution of intensities encountered in natural images can be estimated. Second-order 
statistics measure the dependencies between pairs of pixels, which are usually expressed in terms of 
the power spectrum (see Section 1.2). Higher-order statistics are used to extract properties of natural 
scenes which can not be modeled using .rstand second-order statistics. These include, for example, lines 
and edges. Each of these categories is discussed in some detail in the following sections. 1.1 First-Order 
Statistics Several simple regularities have been established regarding the statistical distribution of 
pixel values; the gray-world assumption is often quoted in this respect. Averaging all pixel values in 
a photograph tends to yield a color that is closely related to the blackbody temperature of the dominant 
illuminant [29]. The integrated color tends to cluster around a blackbody temperature of 4800 K. For 
outdoor scenes, the average image color remains relatively constant from 30 minutes after dawn until 
within 30 minutes of dusk. The correlated color temperature then increases, giving an overall bluer color.1 
The average image color tends to be distributed around the color of the dominant scene illuminant according 
to a Gaussian distribution, if measured in log space [21, 22]. The eigenvectors of the covariance matrix 
of this distribution follow the spectral responsivities of the capture device, which for a typical color 
negative .lm are [22]: L = 1 v 3 (log(R) + log(G) + log(B)) , (1.1a) s = 1 v 2 (log(R) -log(B)) , (1.1b) 
t = 1 v 6 (log(R) -2 log(G) + log(B)) . (1.1c) For a large image database, the standard deviations in 
these three channels are found to be 0.273, 0.065, and 0.030. Note that this space is similar to the 
La color space. Although that space is derived from LMS rather than RGB, both operate in log space, 
and both have the same channel weights. The coef.cients applied to 1Despite the red colors observed at 
sunrise and sunset, most of the environment remains illuminated by a blue skylight, which increases in 
importance as the sun loses its strength. 2 each of the channels are identical as well, with the only 
difference being that the role of the green and blue channels has been swapped.  1.2 Second-Order Statistics 
The most remarkable and salient natural image statistic that has been discovered so far is that the slope 
of the power spectrum tends to be close to 2. The power spectrum of an M  M image is computed as 2 |F(u,v)| 
 S(u,v)= , (1.2) M2 where F is the Fourier transform of the image. By representing the two-dimensional 
frequencies u and v in polar coordinates (u = f cos f and v = f sin f) and averaging over all directions 
f and all images in the image ensemble, it is found that on log-log scale amplitude as a function of 
frequency, f lies approximately on a straight line [6, 10, 32 34]. This means that spectral power as 
a function of spatial frequency behaves according to a power law function. Moreover, .tting a line through 
the data points yields a slope a of approximately 2 for natural images. Although this spectral slope 
varies subtly between different studies [6, 9, 11, 32, 40], and with the exception of low frequencies 
[20], it appears to be extremely robust against distortions and transformations. It is therefore concluded 
that this spectral behavior is a consequence of the images themselves, rather than of particular methods 
of camera calibration or exact computation of the spectral slope. We denote this behavior by AA S(f) 
. = , (1.3) fa f2-. where A is a constant determining overall image contrast, a is the spectral slope, 
and . is its deviation from 2. However, the exact value of the spectral slope depends somewhat on the 
type of scenes that make up the ensembles. Most interest of the natural image statistics community is 
in scenes containing mostly trees and shrubs. Some studies show that the spectral slope for scenes containing 
man-made objects is slightly different [41, 42]. Even in natural scenes, the statistics vary, dependent 
on what is predominantly in the images. The second-order statistics for sky are, for example, very different 
from those of trees. One of the ways in which this becomes apparent is when the power spectra are not 
circularly averaged, but when the log average power is plotted against angle. For natural image ensembles, 
all angles show more or less straight power spectra, although most power is concentrated along horizontal 
and vertical directions [33, 34] (see also Figure 3). The horizon and the presence of tree trunks are 
said to be factors, although this behavior also occurs in man-made environments. The power spectrum is 
related to the auto-correlation function through the Wiener-Khintchine theorem, which states that the 
auto-correlation function and the power spectrum form a Fourier transform pair [25]. Hence, power spectral 
behavior can be equivalently understood in terms of correlations between pairs of pixel intensities. 
A related image statistic is contrast, normally de.ned as the standard deviation of all pixel intensities 
divided by the mean intensity (s/). This measure can either be computed directly from the image data, 
or it can be derived from the power spectrum through Parceval s theorem [34]: s2 . = S(u,v). (1.4) 2 
(u,v) This particular contrast computation can be modi.ed to compute contrast in different frequency 
bands. Frequency-conscious variance can then be thresholded, yielding a measure which can detect blur 
[12]. This is useful as lack of contrast can also be caused by the absence of suf.cient detail in a sharp 
image. 3 The above second-order statistics are usually collected for luminance images only, as luminance 
is believed to carry the greatest amount of information. However, chrominance channels are shown to 
exhibit similar spectral behavior [27], and therefore, all subsequent qualitative arguments are expected 
to be true for color as well. The fact that the power spectral behavior of natural images, when plotted 
in log-log space, yields a straight line with a slope of around -2 is particularly important, since recent 
unrelated studies have found that in image interpretation tasks, the human visual system performs best 
when the images conform to the above second-order statistic. In one such study, images of a car and a 
bull were morphed into each other, with varying distances between the images in the sequence [28]. Different 
sequences were generated with modi.ed spectral slopes. The minimum distance at which participants could 
still distinguish consecutive images in each morph sequence was then measured. This distance was found 
to be smallest when the spectral slope of the images in the morph sequence was close to 2. Deviation 
of the spectral slope in either direction resulted in a deteriorated performance to distinguish between 
morphed images. In a different study, the effect of spectral slope on the detection of mirror symmetry 
in images was assessed [31]. Here, white noise patterns with varying degrees of vertical symmetry were 
created and subsequently .ltered to alter the spectral slope. An experiment, in which participants had 
to detect if symmetry was present, revealed that performance was optimal for images with a spectral slope 
of 2. These studies con.rm the hypothesis that the HVS is tuned to natural images. In fact, the process 
of whitening (i.e., .attening the power spectrum to produce a slope a of 0) is consistent with psychophysical 
measurements [1], which indicates that the HVS expects to see images with a 1/f2 power spectrum. 1.3 
Power Spectra In this section, we show how the power spectrum of an ensemble of images can be established. 
We used 133 images from the van Hateren database of natural images [15]. The images in this ensemble 
are available as luminance images. Example images are shown in Figure 1. For this image ensemble, the 
power spectrum was computed and the spectral slope was estimated. The power spectrum computation proceeds 
as follows.2 A window of 512  512 pixels was cut out of the middle of the image upon which further processing 
was applied. Then, the weighted mean intensity was subtracted to avoid leakage from the DC-component 
of the image, where is de.ned as L(x,y)w(x,y) (x,y) = . . (1.5) (x,y) w(x,y) Next, the images were 
pre.ltered to avoid boundary effects. This is accomplished by applying a circular Kaiser-Bessel window 
function (with parameter a =2) to the image [14]: x2 +y2 I0 pa 1.0 - (N/2)2 . N w(x,y)= , 0 = x2 + y2 
= . (1.6) I0(pa)2 Here, I0 is the modi.ed zero-order Bessel function of the .rst kind and N is the window 
size (512 pixels). In addition, this weight function was normalized by letting w(x,y)2 =1. (1.7) (x,y) 
 2This calculation closely follows the method described in [34]. 4  Figure 1: Example images drawn from 
the van Hateren database [15]. Images courtesy of Hans van Hateren. This windowing function was chosen 
for its near-optimal trade-off between side-lobe level, main-lobe width, and computability [14]. The 
resulting images were then compressed using the Fourier transform: L(x,y) - 2pi(ux+vy) F(u,v)= e. (1.8) 
 (x,y) Finally, the power spectrum was computed as per (1.2) and the resulting data points plotted. 
Although frequencies up to 256 cycles per image are computed, only the 127 lowest frequencies were used 
to estimate the spectral slope. Higher frequencies may suffer from aliasing, noise, and low modulation 
transfer [34]. The estimation of the spectral slope was performed by .tting a straight line through the 
logarithm of these data points as a function of the logarithm of 1/f. This method was chosen over other 
slope estimation techniques, such as the Hill estimator [16] and the scaling method [8], to maintain 
compatibility with [34]. In addition, the number of data points (127 frequencies) is insuf.cient for 
the scaling method, which requires at least 1,000 data points to yield reasonably reliable estimates. 
With this method, second-order statistics were extracted. The 1.87 spectral slope reported for the van 
Hateren database was con.rmed (we found 1.88 for our subset of 133 images). The deviations from this 
value for the arti.cial image ensemble are depicted in Figure 2. The angular distribution of power tends 
to show peaks near horizontal and vertical angles (Figure 3). Finally, the distribution of spectral slopes 
for the 133 images in this ensemble is shown in Figure 4. 5 log10Power 0 Powervs.spatialfrequency 
-1 Slope-1.88 -2  -9 012 Spatialfrequency(log10cycles/image) Figure 2: Spectral slope for the 
image ensemble. The double lines indicate 2 standard deviations for each ensemble. Averagepowerperdirection 
log10averagepower -4.5 -5.0 -5.5 -6.0 0 90 180 Orientation(deg) 270 360 Figure 3: Log average power 
as function of angle.  1.4 Higher-Order Statistics One of the disadvantages of using amplitude information 
in the frequency domain is that phase information is completely discarded, thus ignoring the position 
of edges and objects. For this reason, higher-order statistics have been applied to natural image ensembles. 
The simplest global nth-order statistics that may capture phase structure are the third and fourth moments, 
commonly referred to as skew and kurtosis [25]: 3} E{x s = , (1.9a) E{x2}3/2 4} E{x k = -3. (1.9b) 
E{x2}2 These dimensionless measures are, by de.nition, zero for pure Gaussian distributions. Skew can 
be interpreted as an indicator of the difference between the mean and the median of a data set. Kurtosis 
is based on the size of a distribution s tails relative to a Gaussian distribution. A positive value, 
associated with long tails in the distribution of intensities, is usually associated with natural image 
ensembles. This is for example evident when plotting log-contrast histograms, which plot the probability 
of a particular contrast value appearing. These plots are typically non-Gaussian with positive kurtosis 
[33]. Thomson [39] has pointed out that for kurtosis to be meaningful for natural image analysis, the 
data should 6 0.15 0.10 0.05 0.00 1/f-exponent (a) Figure 4: A histogram of spectral slopes, derived 
from an image ensemble of 133 images taken from the van Hateren database. be decorrelated, or whitened, 
prior to any computation of higher-order statistics. This can be accomplished by .attening the power 
spectrum, which ensures that second-order statistics do not also capture higher-order statistics. Thus, 
skew and kurtosis become measures of variations in the phase spectra. Regularities are found when these 
measures are applied to the pixel histogram of an image ensemble [38,39]. This appears to indicate that 
the HVS may exploit higher-order phase structure after whitening the image. Understanding the phase structure 
is therefore an important avenue of research in the .eld of natural image statistics. However, it appears 
that so far it is not yet possible to attach meaning to the exact appearance of higher-order statistics. 
 1.5 Gradient Field Statistics Rather than study the statistical properties of pixels, we can also look 
at derived quantities, such as gradients. The gradient .eld of an image is a vector-valued quantity .L, 
de.ned as: .L =(L(x +1,y) -L(x,y),L(x,y + 1) -L(x,y)) . (1.10) Natural images typically have strong discontinuities 
in otherwise relatively smooth regions. This means that large image areas have only small gradients, 
with larger gradients happening more sparsely. This means that the components of the gradient .eld will 
show a characteristic distribution, which can be observed by computing its histogram. The gradient distribution 
of a set of four example images, shown in Figure 5, is plotted in Figure 6. The images used here were 
captured in high dynamic range (see the following chapter), and therefore contain much larger gradients 
than encountered in conventional 8-bit images. In addition, these images are linear by construction, 
having a gamma of 1.0. Conventional images are likely to have gamma correction applied to them, and this 
may affect the distribution of gradients in images. 1.6 Response Properties of Cortical Cells One of 
the reasons to study the statistics of natural images is to understand how the HVS codes these images. 
Because natural images are not completely random, they incorporate a signi.cant amount of redundancy. 
The HVS may represent such scenes using a sparse set of active elements. These elements will then be 
largely independent. Here, it is assumed that an image can be represented as a linear superposition of 
basis functions. Ef.ciently encoding images now involves .nding a set of basis functions that spans 
image space and ensures that the coef.cients across an image ensemble are statistically as independent 
as possible [26]. The resulting basis functions (or .lters) can then be compared to the response properties 
of cortical cells in order to explain 7 fraction  Figure 5: Example images used for computing gradient 
distributions, which are shown in Figure 6. In reading order: Abbaye de Beauport, Paimpol, France (July, 
2007), Combourg, France (July, 2007), Foug` eres, France (June, 2007), Kermario, Carnac, France (July, 
2007). Gradient ( 104) Figure 6: The probability distribution of gradients for the four images depicted 
in Figure 5. 8 the early stages of human visual processing. This has resulted in a number of different 
representations for basis functions, including principal components analysis (PCA) [2, 13], independent 
components analysis (ICA) [3, 4, 15], Gabor basis functions [10, 11], and wavelets [11, 17, 26]. The 
PCA algorithm .nds a set of basis functions that maximally decorrelates pairs of coef.cients; this is 
achieved by computing the eigenvalues of the covariance matrix (between pairs of pixel intensities). 
The corresponding eigenvectors represent a set of orthogonal coef.cients, whereby the eigenvector with 
the largest associated eigenvalue accounts for the largest part of the covariance. By using only the 
.rst few coef.cients, it is possible to encode an image with a large reduction in free parameters [10]. 
If stationarity is assumed across all images in an ensemble and between all regions in the images, i.e., 
the image statistics are the same everywhere, then PCA produces coef.cients that are similar to the Fourier 
transform [5]. Indeed, PCA is strictly a second-order method assuming Gaussian signals. Unfortunately, 
decorrelation does not guarantee independence. Also, intensities in natural images do not necessarily 
have a Gaussian distribution, and therefore PCA yields basis functions that do not capture higherorder 
structure very well [26]. In particular, the basis functions tend to be orientation and frequency sensitive, 
but with global extent. This is in contrast to cells in the human primary cortex which are spatially 
localized (as well as being localized in frequency and orientation). In contrast to PCA, independent 
components analysis constitutes a transformation resulting in basis functions that are non-orthogonal, 
localized in space, and selective for frequency and orientation. They aim to extract higher-order information 
from images [3, 4]. ICA .nds basis functions that are as independent as possible [18]. To avoid second-order 
statistics from in.uencing the result of ICA, the data is usually .rst decorrelated (also called whitened), 
for example using a PCA algorithm. Filters can then be found that produce extrema of the kurtosis [34]. 
A kurtotic amplitude distribution is produced by cortical simple cells, leading to sparse coding. Hence, 
ICA is believed to be a better model than PCA for the output of simple cortical cells. The receptive 
.elds of simple cells in the mammalian striate cortex are localized in space, oriented, and bandpass. 
They are therefore similar to the basis functions of wavelet transforms [11, 26]. For natural images, 
strong correlations between wavelet coef.cients at neighboring spatial locations, orientations, and scales, 
have been shown using conditional histograms of the coef.cients log magnitudes [36]. These results were 
successfully used to synthesize textures [30,37] and to denoise images [35]. 1.7 Dimensionality of Re.ectance 
and Daylight Spectra The re.ectance spectra of large numbers of objects have been analyzed for the purpose 
of determining their dimensionality. We have so far assumed that a spectral distribution is sampled at 
regular intervals, for instance from 380 to 800 with a 5 or 10 nm spacing. This leads to a representation 
of each spectrum by 43 or so numbers. Values in between sample points can be approximated, for instance, 
by linear interpolation. However, it is also possible to use higher-order basis functions and, thereby, 
perhaps reduce the number of samples. The question then becomes which basis functions should be used, 
and how many are required. It has been found that the eigenvalues of the covariance matrix decay very 
fast, leading to only a few principal components that account for most of the variance [7]. Similar results 
have been found for daylight spectra [19], suggesting that spectral distributions can be represented 
in a low-dimensional space, for instance having no more than six or seven dimensions [23, 24]. 9  References 
 [1] J J Atick and N A Redlich. What does the retina know about natural scenes? Neural Computation, 4:196 
210, 1992. [2] R J Baddeley and P J B Hancock. A statistical analysis of natural images matches psychophysically 
derived orientation tuning curves. Proceedings of the Royal Society of London B, 246:219 223, 1991. [3] 
Anthony J Bell and Terrence J Sejnowski. Edges are the independent components of natural scenes. Advances 
in Neural Information Processing Systems, 9, 1996. [4] Anthony J Bell and Terrence J Sejnowski. The independent 
components of natural scenes are edge .lters. Vision Research, 37:3327 3338, 1997. [5] T Bossomaier and 
A W Snyder. Why spatial frequency processing in the visual cortex? Vision Research, 26:1307 1309, 1986. 
[6] G J Burton and Ian R Moorhead. Color and spatial structure in natural scenes. Applied Optics, 26(1):157 
170, 1987. [7] J Cohen. Dependency of the spectral re.ectance curves of the munsell color chips. Psychonomic 
Science, 1:369 370, 1964. [8] Mark E Crovella and Murad S Taqqu. Estimating the heavy tail index from 
scaling properties. Methodology and Computing in Applied Probability, 1(1):55 79, 1999. [9] Dawei W 
Dong and Joseph J Atick. Statistics of natural time-varying images. Network: Computation in Neural Systems, 
6(3):345 358, 1995. [10] David J Field. Relations between the statistics of natural images and the response 
properties of cortical cells. Journal of the Optical Society of America A, 4(12):2379 2394, 1987. [11] 
David J Field. Scale-invariance and self-similar wavelet transforms: An analysis of natural scenes and 
mammalian visual systems. In M Farge, J C R Hunt, and J C Vassilicos, editors, Wavelets, fractals and 
Fourier transforms, pages 151 193. Clarendon Press, Oxford, 1993. [12] David J Field and Nuala Brady. 
Visual sensitivity, blur and the sources of variability in the amplitude spectra of natural scenes. Vision 
Research, 37(23):3367 3383, 1997. [13] Peter J B Hancock, Roland J Baddeley, and Leslie S Smith. The 
principle components of natural images. Network, 3:61 70, 1992. [14] Frederic J Harris. On the use of 
windows for harmonic analysis with the discrete fourier transform. Proceedings of the IEEE, 66(1):51 
84, 1978. [15] H van Hateren and A van der Schaaf. Independent component .lters of natural images compared 
with simple cells in primary visual cortex. Proceedings of the Royal Society of London B, 265:359 366, 
1998. [16] Bruce M Hill. A simple general approach to inference about the tail of a distribution. The 
Annals of Statistics, 3(5):1163 1174, 1975. [17] Jarmo Hurri, Aapo Hyv arinen, and Erkki Oja. Wavelets 
and natural image statistics. In Proceedings of the 10th Scandinavian Conference on Image Analysis, pages 
13 18, June 1997. 10 [18] Aapo Hyvarinen. Survey on independent components analysis. Neural Computing 
Surveys, 2:94 128, 1999. [19] Deane B Judd, David L MacAdam, and Gunther Wyszecki. Spectral distribution 
of typical light as a function of correlated color temperature. Journal of the Optical Society of America, 
54(8):1031 1040, 1964. [20] Michael S Langer. Large-scale failures of f-a scaling in natural image spectra. 
Journal of the Optical Society of America A, 17(1):28 33, 2000. [21] Hsien-Che Lee. Internet color imaging. 
In Proceedings of the SPIE, volume 3080, pages 122 135, 2000. [22] Hsien-Che Lee. Introduction to color 
imaging science. Cambridge University Press, Cambridge, 2005. [23] Laurence T Maloney. Evaluation of 
linear models of surface spectral re.ectance with small number of parameters. Journal of the Optical 
Society of America A, 3:1673 1683, 1986. [24] Laurence T Maloney and Brian A Wandell. Color constancy: 
A method for recovering surface spectral re.ectance. Journal of the Optical Society of America A, 3(1):29 
33, 1986. [25] Chrysostomos L Nikias and Athina P Petropulu. Higher-order spectra analysis. Signal Processing 
Series. Prentice Hall, 1993. [26] B A Olshausen and D J Field. Emergence of simple-cell receptive .eld 
properties by learning a sparse code for natural images. Nature, 381:607 609, 1996. [27] CAParraga, 
G Brelstaff, T Troscianko, and I R Moorhead. Color and luminance information in natural scenes. Journal 
of the Optical Society of America A, 15(3):563 569, 1998. [28] C A Parraga, T Troscianko, and D J Tolhurst. 
The human visual system is optimised for processing the spatial information in natural visual images. 
Current Biology, 10(1):35 38, 2000. [29] F H G Pitt and E W H Selwyn. Colour of outdoor photographic 
objects. The Photographic Journal, 78:115 121, 1938. [30] Javier Portilla and Eero P Simoncelli. A parametric 
texture model based on joint statistics of complex wavelet coef.cients. International Journal of Computer 
Vision, 40(1):49 71, 2000. [31] Stephane J M Rainville and Frederick A A Kingdom. Spatial-scale contribution 
to the detection of mirror symmetry in fractal noise. Journal of the Optical Society of America A, 16(9):2112 
2123, 1999. [32] D L Ruderman and W Bialek. Statistics of natural images: Scaling in the woods. Physical 
Review Letters, 73(6):814 817, 1994. [33] Daniel L Ruderman. The statistics of natural images. Network: 
Computation in Neural Systems, 5(4):517 548, 1997. [34] A van der Schaaf. Natural image statistics and 
visual processing. PhD thesis, Rijksuniversiteit Groningen, The Netherlands, March 1998. [35] Eero P 
Simoncelli. Bayesian denoising of visual images in the wavelet domain. In P Muller and B Vidakovic, 
editors, Bayesian Inference in Wavelet Based Models, volume 141 of Lecture Notes in Statistics, pages 
291 308, New York, 1999. Springer-Verlag. 11 [36] Eero P Simoncelli. Modelling the joint statistics 
of images in the wavelet domain. In Proceedings of the 44th SPIE Anual Meeting, volume 3813, pages 188 
195, July 1999. [37] Eero P Simoncelli and Javier Portilla. Texture characterization via joint statistics 
of wavelet coef.cient magnitudes. In Proceedings of the 5th International Conference on Image Processing, 
October 1998. [38] Mitchell G A Thomson. Higher-order structure in natural scenes. Journal of the Optical 
Society of America A, 16(7):1549 1553, 1999. [39] Mitchell G A Thomson. Visual coding and the phase structure 
of natural scenes. Network: Computation in Neural Systems, 10(2):123 132, 1999. [40] D J Tolhurst, Y 
Tadmor, and T Chiao. Amplitude spectra of natural images. Ophthalmic and Physiological Optics, 12:229 
232, 1992. [41] Christian Ziegaus and Elmar W Lang. Statistics of natural and urban images. In Proceedings 
of the 7th International Conference on Arti.cial Neural Networks, volume 1327, pages 219 224, Berlin, 
1997. Springer-Verlag. [42] Christian Ziegaus and Elmar W Lang. Statistical invariances in arti.cial, 
natural and urban images. Zeitschrift fur Naturforschung A, 53a(12):1009 1021, 1998. 12  Index Auto-correlation 
function, 3 Average image color, 2 Blur detection, 3 Color average image color, 2 correlated color temperature, 
2 Contrast natural image statistics, 3 Correlated color temperature outdoor environment, 2 Ensemble, 
2 Gray-world assumption, 2 ICA, 9 Image ensemble, 2 Independent components analysis (ICA), 9 Kaiser-Bessel 
window, 4 Kurtosis, 6, 9 Natural image statistics, 2 1/f failure, 3 1/f statistic, 3 and cortical cells, 
9 chrominance, 4 contrast, 3 correlated color temperature, 2 dimensionality of spectra, 9 .rst-order, 
2 higher-order, 2, 6 image ensemble, 2 kurtosis, 6 power spectrum, 3 second-order, 2 skew, 6 stationarity, 
9 whitening, 4 Parceval s theorem, 3 PCA, 9 Power spectrum, 3 Principal components analysis (PCA), 9 
Re.ectance dimensionality of spectra, 9 Skew, 6 Whitening, 4, 9 Wiener-Khintchine theorem, 3 13  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837108</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>159</pages>
		<display_no>7</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Perceptually-motivated graphics, visualization and 3D displays]]></title>
		<page_from>1</page_from>
		<page_to>159</page_to>
		<doi_number>10.1145/1837101.1837108</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837108</url>
		<abstract>
			<par><![CDATA[<p>This course presents timely, relevant examples on how researchers have leveraged perceptual information for optimization of rendering algorithms, to better guide design and presentation in (3D stereoscopic) display media, and for improved visualization of complex or large data sets. Each presentation will provide references and short overviews of cutting-edge current research pertaining to that area. We will ensure that the most up-to-date research examples are presented by sourcing information from recent perception and graphics conferences and journals such as ACM Transactions on Perception, paying particular attention work presented at the 2010 Symposium on Applied Perception in Graphics and Visualization.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human information processing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010216.10010217</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence->Cognitive science</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2264999</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2265000</person_id>
				<author_profile_id><![CDATA[81100567689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Crete]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2265001</person_id>
				<author_profile_id><![CDATA[81100462179]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2265002</person_id>
				<author_profile_id><![CDATA[81100203190]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Healey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. D. Adelstein, T. G. Lee, and S. R. Ellis. Head tracking latency in virtual environments: Psychophysics and a model. <i>In Proc. of the 47th Annual Human Factors and Ergonomics Society meeting</i>, pages 2083--2087, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320306</ref_obj_id>
				<ref_obj_pid>2319012</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[St&#233;phane Albin, Gilles Rougeron, Bernard P&#233;roche, and Alain Tr&#233;meau. Quality image metrics for synthetic images based on perceptual color differences. <i>IEEE Transactions on Image Processing</i>, 11(9):961--971, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835823</ref_obj_id>
				<ref_obj_pid>580521</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Robert S. Allison, Laurence R. Harris, Michael Jenkin, Urszula Jasiobedzka, and James E. Zacher. Tolerance of temporal delay in virtual environments. In <i>VR '01: Proceedings of the Virtual Reality 2001 Conference (VR'01)</i>, page 247, Washington, DC, USA, 2001. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Ariely. Seeing sets: Representation by statistical properties. <i>Psychological Science</i>, 12(2):157--162, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ronald Azuma and Gary Bishop. Improving static and dynamic registration in an optical see-through hmd. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 197--204, New York, NY, USA, 1994. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. H. Bailey and B. G. Witmer. Learning and transfer of spatial knowledge in a virtual environment. <i>Proc. of the Human Factors and Ergonomics Society 38th Annual Meeting</i>, pages 1158--1162, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dirk Bartz, Douglas Cunningham, Jan Fischer, and Christian Wallraven. Star state of the art report the role of perception for computer graphics, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[James P. Bliss, Philip D. Tidwell, and Michael A. Guest. The effectiveness of virtual reality for administering spatial navigation training to firefighters. <i>Presence</i>, 6(1):73--86, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280924</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mark R. Bolin and Gary W. Meyer. A perceptually based adaptive sampling algorithm. <i>Computer Graphics</i>, 32(Annual Conference Series):299--309, August 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218497</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. R. Bolin and G. W. Meyer. A frequency based ray tracer. In <i>ACM SIGGRAPH '95 Conference Proceedings</i>, pages 409--418, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[K. R. Brandt, J. M. Gardiner, and C. N. Macrae. The distinctiveness effect in forenames: The role of subjective experiences and recognition memory. <i>British Journal of Psychology</i>, pages 269--280, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[K. R. Brandt, C. N. Macrae, A. M. Schloerscheidt, and A. B. Milne. Do i know you? target typicality and recollective experience. <i>Memory</i>, 11(1):89--100, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W. Brewer and J. Treyens. Role of schemata in memory for places. <i>Cognitive Psychology</i>, 13:207--230, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. E. Broadbent and M. H. P. Broadbent. From detection to identification: Response to multiple targets in rapid serial visual presentation. <i>Perception and Psychophysics</i>, 42(4):105--113, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[T. C. Callaghan. Dimensional interaction of hue and brightness in preattentive field segregation. <i>Perception &amp; Psychophysics</i>, 36(1):25--34, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T. C. Callaghan. Interference and domination in texture segregation: Hue, geometric form, and line orientation. <i>Perception &amp; Psychophysics</i>, 46(4):299--311, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[T. C. Callaghan. Interference and dominance in texture segregation. In D. Brogan, editor, <i>Visual Search</i>, pages 81--87. Taylor &amp; Francis, New York, New York, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Cataliotti and A. Gilchrist. Local and global processes in lightness perception. In <i>Perception and Psychophysics</i>, volume 57(2), pages 125--135. Perception, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882443</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[K. Cater, A. Chalmers, and G. Ward. Detail to attention: exploiting visual tasks for selective rendering. In <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 270--280, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Heinrich Conference Chair-B&#252;lthoff and Tom Conference Chair- Troscianko. Apgv '05: Proceedings of the 2nd symposium on applied perception in graphics and visualization. <i>A Coro na, Spain</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Roland W. Conference Chair-Fleming and Sunghee Conference Chair- Kim. Apgv '06: Proceedings of the 3rd symposium on applied perception in graphics and visualization. <i>Boston, Massachusetts</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Victoria Conference Chair-Interrante, Ann Conference Chair-McNamara, Heinrich Program Chair-Bulthoff, and Holly Program Chair-Rushmeier. Apgv '04: Proceedings of the 1st symposium on applied perception in graphics and visualization. <i>Los Angeles, California</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Christian Conference Chair-Wallraven and Veronica Conference Chair-Sundstedt. Apgv '07: Proceedings of the 4th symposium on applied perception in graphics and visualization. <i>Tubingen, Germany</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[et AL. Conway, M. A. Changes in memory awareness during learning: The acquisition of knowledge by psychology undergraduates. <i>Journal of Experimental Psychology: General</i>, 126(4):393 413, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197783</ref_obj_id>
				<ref_obj_pid>197765</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Scott Daly. The visible differences predictor: an algorithm for the assessment of image fidelity. In <i>Digital images and human vision</i>, pages 179--206, Cambridge, MA, USA, 1993. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Kate Devlin, Alan Chalmers, Alexander Wilkie, and Werner Purgathofer. Star: Tone reproduction and physically based spectral rendering. In Dieter Fellner and Roberto Scopignio, editors, <i>State of the Art Reports, Eurographics 2002</i>, pages 101--123, Vienna, September 2002. The Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[S. A. Dewhurst, S. J. Holmes, K. R. Brandt, and G. M. Dean. Measuring the speed of the conscious components of recognition memory: Remembering is faster than knowing. <i>Consciousness and Cognition</i>, 15:147--162, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835733</ref_obj_id>
				<ref_obj_pid>554230</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Huong Q. Dinh, Neff Walker, Chang Song, Akira Kobayashi, and Larry F. Hodges. Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments. In <i>VR '99: Proceedings of the IEEE Virtual Reality</i>, page 222, Washington, DC, USA, 1999. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383885</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[
George Drettakis, Nicolas Bonneel, Carsten Dachsbacher, Sylvain Lefebvre, Michael Schwarz, and Isabelle Viaud-Delmon. An interactive perceptual rendering pipeline using contrast and spatial masking. In <i>Rendering Techniques (Proceedings of the Eurographics Symposium on Rendering)</i>. Eurographics, June 2007.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. R. Ellis, K. Mania, B. D. Adelstein, and M. I. Hill. Generalizability of latency detection in a variety of virtual environments. <i>In Proc. of the 48th Annual Human Factors and Ergonomics Society meeting</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[S. R. Ellis, M. J. Young, S. M. Ehrlich, and B. D. Adelstein. Discrimination of changes of rendering latency during voluntary hand movement. <i>In Proc. of the 43th Annual Human Factors and Ergonomics Society meeting</i>, pages 1182--1186, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[S. R. Ellis, A. Wolfram, and B. D. Adelstein. Large amplitude three-dimensional tracking in augmented environments: a human performance trade-off between system latency and update rate. <i>In Proc. of the 46th Annual Human Factors and Ergonomics Society meeting</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Stephen R. Ellis, F. Breant, Brian M. Menges, Richard H. Jacoby, and Bernard D. Adelstein. Operator interaction with visual objects: Effect of system latency. <i>HCI (2)</i>, pages 973--976, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>741581</ref_obj_id>
				<ref_obj_pid>647944</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Stephen R. Ellis, Mark J. Young, Bernard D. Adelstein, and Sheryl M. Ehrlich. Discrimination of changes in latency during head movement. <i>HCI (2)</i>, pages 1129--1133, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Land. M. F. Motion and vision: why animals move their eyes. <i>Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioural Physiology</i>, 185(4):341--352, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Jean-Philippe Farrugia and Bernard P&#233;roche. A progressive rendering algorithm using an adaptive perceptually based image metric. <i>Comput. Graph. Forum</i>, 23(3):605--614, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[J. Ferwerda. Hi-fi rendering. <i>ACM Siggraph Eurographics campfire on perceptually adaptive graphics</i>. http://isg.cs.tcd.ie/campfire/jimferwerda2.html, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1227136</ref_obj_id>
				<ref_obj_pid>1227134</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Philip W. Fink, Patrick S. Foo, and William H. Warren. Obstacle avoidance during walking in real and virtual environments. <i>ACM Trans. Appl. Percept.</i>, 4(1):2, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[R. Flannery, K. A. Walles. How does schema theory apply to real versus virtual memories? <i>Cyberspychology and Behavior</i>, 6(2):151--159, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1227137</ref_obj_id>
				<ref_obj_pid>1227134</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Harald Frenz, Markus Lappe, Marina Kolesnik, and Thomas B&#252;hrmann. Estimation of travel distance from visual motion in virtual environments. <i>ACM Trans. Appl. Percept.</i>, 4(1):3, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[J. M. Gardiner and A. Richardson-klavehn. Remembering and knowing. <i>In: Tulving, E. and Craik, F. I. M., eds. Handbook of Memory</i>, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[S. Gibson and R. J. Hubbold. Efficient hierarchical refinement and clustering for radiosity in complex environments. <i>Computer Graphics Forum</i>, 15(5):297--310, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[A. Gilchrist, S. Delman, and A. Jacobsen. The classification and integration of edges as critical to the perception of reflectance and illumination. <i>Perception and Psychophysics</i>, 33(5):425--436, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[A. L. Gilchrist. The perception of surface blacks and whites. <i>Scientific American</i>, 240(3):88--97, March 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Alan Gilchrist and Alan Jacobsen. Perception of lightness and illumination in a world of one reflectance. <i>Perception</i>, 13:5--19, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280888</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Baining Guo. Progressive radiance evaluation using directional coherence maps. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, pages 255--266, New York, NY, USA, 1998. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[J&#246;rg Haber, Karol Myszkowski, Hitoshi Yamauchi, and Hans-Peter Seidel. Perceptually guided corrective splatting. <i>Computer Graphics Forum</i>, 20(3), 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288232</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Christopher G. Healey and James T. Enns. Building perceptual textures to visualize multidimensional datasets. In <i>Proceedings Visualization '98</i>, pages 111--118, Research Triangle Park, North Carolina, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614428</ref_obj_id>
				<ref_obj_pid>614274</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Christopher G. Healey and James T. Enns. Large datasets at a glance: Combining textures and colors in scientific visualization. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 5(2):145--167, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732107</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[David Hedley, Adam Worrall, and Derek Paddon. Selective culling of discontinuity lines. In Julie Dorsey and Philipp Slusallek, editors, <i>Rendering Techniques '97 (Proceedings of the Eighth Eurographics Workshop on Rendering)</i>, pages 69--80. Springer Wien, 1997. ISBN 3-211-83001-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[R. Held, A. Efsathiouand, and M Greene. Adaptation to displaced and delayed visual feedback from the hand. <i>Journal of Experimental Psychology</i>, 72(6):887--891, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[L. Huang and H. Pashler. A boolean map theory of visual attention. <i>Psychological Review</i>, 114(3):599--631, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[L. Huang, A. Treisman, and H. Pashler. Characterizing the limits of human visual awareness. <i>Science</i>, 317:823--825, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>297870</ref_obj_id>
				<ref_obj_pid>297843</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. <i>IEEE Trans. Pattern Anal. Mach. Intell.</i>, 20(11):1254--1259, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[B. Jul&#233;sz. <i>Foundations of Cyclopean Perception</i>. University of Chicago Press, Chicago, Illinois, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[B. Jul&#233;sz. Experiments in the visual perception of texture. <i>Scientific American</i>, 232:34--43, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[B. Jul&#233;sz. A theory of preattentive texture discrimination based on first-order statistics of textons. <i>Biological Cybernetics</i>, 41:131--138, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[B. Jul&#233;sz, E. N. Gilbert, and L. A. Shepp. Inability of humans to discriminate between visual textures that agree in second-order statistics---revisited. <i>Perception</i>, 2:391--405, 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[B. Jul&#233;sz, E. N. Gilbert, and J. D. Victor. Visual discrimination of textures with identical third-order statistics. <i>Biological Cybernetics</i>, 31:137--140, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[J. Y. Jung, B. D. Adelstein, and S. R. Ellis. Discriminability of prediction artifacts in a time delayed virtual environment. <i>In Proc. of the 44th Annual Human Factors and Ergonomics Society meeting</i>, pages 499--502, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Grzegorz Krawczyk, Karol Myszkowski, and Hans-Peter Seidel. Light-ness perception in tone reproduction for high dynamic range images. In <i>The European Association for Computer Graphics 26th Annual Conference EUROGRAPHICS 2005</i>, volume 24 of <i>Computer Graphics Forum</i>, pages xx--xx, Dublin, Ireland, 2005. Blackwell.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Grzegorz Krawczyk, Karol Myszkowski, and Hans-Peter Seidel. Computational model of lightness perception in high dynamic range imaging. In Bernice E. Rogowitz, Thrasyvoulos N. Pappas, and Scott J. Daly, editors, <i>Human Vision and Electronic Imaging XI, IS&T SPIE's 18th Annual Symposium on Electronic Imaging (2006)</i>, volume xxxx, pages xxx--xxx, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Gregory Ward Larson, Holly Rushmeier, and Christine Piatko. A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 3(4):291--306, October 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641635</ref_obj_id>
				<ref_obj_pid>641633</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[William B. Lathrop and Mary K. Kaiser. Perceived orientation in physical and virtual environments: changes in perceived orientation as a function of idiothetic information available. <i>Presence: Teleoper. Virtual Environ.</i>, 11(1):19--32, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272593</ref_obj_id>
				<ref_obj_pid>1272582</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Guillaume Lavou&#233;. A roughness measure for 3d mesh visual masking. In <i>APGV '07: Proceedings of the 4th symposium on Applied perception in graphics and visualization</i>, pages 57--60, New York, NY, USA, 2007. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[A. Liu, S. Tharp, L. Lai, French, and L. Stark. Some of what one needs to know about using head-mounted displays to improve teleoperator performance. <i>IEEE Transactions on Robotics and Automation</i>, 9(5):638--648, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[G. Liu, E. Austen, K. Booth, B. Fischer, M. Rempel, and J. Enns. Multiple object tracking is based on scene, not retinal, coordinates. <i>Journal of Experimental Psychology: Human Perception and Performance</i>, 31(2):235--247, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[G. R. Loftus and N. H. Mackworth. Cognitive determinants of fixation location during picture viewing. <i>Educational Psychology: Human Perception and Performance</i>, 4(4):565--572, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[A. Mack and I. Rock. <i>Inattentional Blindness</i>. MIT Press, Menlo Park, California, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[G. W. Maconcie and L. C. Loschky. Human performance with a gaze linked multi-resolutional display. <i>In Proceedings of the Advanced Displays and Interactive Displays First Annual Symposium 1997</i>, pages 25--34, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1160392</ref_obj_id>
				<ref_obj_pid>1160382</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[K. Mania, A. Robinson, and K. Brandt. The effect of memory schemata on object recognition in virtual environments. <i>Presence, Teleoperators and Virtual Environments</i>, 14(5):606--615, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>942443</ref_obj_id>
				<ref_obj_pid>942439</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[K. Mania, T. Troscianko, R. Hawkes, and A. Chalmers. Fidelity metrics for virtual environment simulations based on human judgments of spatial memory awareness states. <i>Presence, Teleoperators and Virtual Environments</i>, 12(3):296--310, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1012559</ref_obj_id>
				<ref_obj_pid>1012551</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Katerina Mania, Bernard D. Adelstein, Stephen R. Ellis, and Michael I. Hill. Perceptual sensitivity to head tracking latency in virtual environments with varying degrees of scene complexity. In <i>APGV '04: Proceedings of the 1st Symposium on Applied perception in graphics and visualization</i>, pages 39--47, New York, NY, USA, 2004. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1670673</ref_obj_id>
				<ref_obj_pid>1670671</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Katerina Mania, Shahrul Badariah, Matthew Coxon, and Phil Watten. Cognitive transfer of spatial awareness states from immersive virtual environments to reality. <i>ACM Trans. Appl. Percept.</i>, 7(2):1--14, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1128755</ref_obj_id>
				<ref_obj_pid>1128595</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Katerina Mania, Dave Wooldridge, Matthew Coxon, and Andrew Robinson. The effect of visual and interaction fidelity on spatial cognition in immersive virtual environments. <i>IEEE Trans. Vis. Comput. Graph.</i>, 12(3):396--404, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080418</ref_obj_id>
				<ref_obj_pid>1080402</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Rafal Mantiuk, Karol Myszkowski, and Hans-Peter Seidel. A perceptual framework for contrast processing of high dynamic range images. In <i>APGV '05: Proceedings of the 2nd symposium on Applied perception in graphics and visualization</i>, pages 87--94, New York, NY, USA, 2005. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[G Marmitt and A. Duchowski. Modeling visual attention in vr: measuring the accuracy of predicted scanpaths. <i>Proceedings of Eurographics 2002, Short Presentations</i>, pages 217--226, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246862</ref_obj_id>
				<ref_obj_pid>1246860</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Jeffrey McCandless, Stephen R. Ellis, and Bernard D. Adelstein. Localization of a time-delayed, monocular virtual object superimposed on a real environment. <i>Presence</i>, 9(1):15--24, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[B. H. McCormick, T. A. DeFanti, and M. D. Brown. Visualization in scientific computing. <i>Computer Graphics</i>, 21(6):1--14, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[A. McNamara. Evaluating image quality metrics vs. human evaluation. ACM SIGGRAPH 2000 Sketches Program, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Ann McNamara. Visual perception in realistic image synthesis. <i>Comput. Graph. Forum</i>, 20(4):211--224, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Ann McNamara. Visual perception in realistic image synthesis. <i>Comput. Graph. Forum</i>, 20(4):211--224, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835964</ref_obj_id>
				<ref_obj_pid>832289</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Michael Meehan, Sharif Razzaque, Mary C. Whitton, and Frederick P. Brooks, Jr. Effect of latency on presence in stressful virtual environments. In <i>VR '03: Proceedings of the IEEE Virtual Reality 2003</i>, page 141, Washington, DC, USA, 2003. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[G. W. Meyer and A. Liu. Color spatial acuity control of a screen subdivision image synthesis algorithm. <i>Human Vision, Visual Processing, and Digital Display</i>, 1666(3):387--399, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Don P. Mitchell. Generating antialiased images at low sampling densities. <i>Computer Graphics</i>, 21(4):65--72, July 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272594</ref_obj_id>
				<ref_obj_pid>1272582</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Yann Morvan and Carol O'Sullivan. A perceptual approach to trimming unstructured lumigraphs. In <i>APGV '07: Proceedings of the 4th symposium on Applied perception in graphics and visualization</i>, pages 61--68, New York, NY, USA, 2007. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1857895</ref_obj_id>
				<ref_obj_pid>1857893</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[
N. Mourkoussis, F. Rivera, T. Troscianko, T. Dixon, R. Hawkes, and K. Mania. Quantifying fidelity for virtual environment simulations employing memory schema assumptions. <i>ACM Transactions on Applied Perception</i>, 2010.
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[K. Myszkowski. The visible differences predictor: Applications to global illumination problems. In G. Drettakis and N. Max, editors, <i>Rendering Techniques '98 (Proceedings of Eurographics Rendering Workshop '98)</i>, pages 233--236, New York, NY, 1998. Springer Wien.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>259243</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[K. Myszkowski, A. B. Khodulev, and E. A. Kopylov. Validating global illumination algorithms and software. <i>In Visual Proceedings, Technical Sketch at ACM Siggraph 1997</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[U. Neisser. The control of information pickup in selective looking. In A. D. Pick, editor, <i>Perception and its Development: A Tribute to Eleanor J. Gibson</i>, pages 201--219. Lawrence Erlbaum and Associates, Hilsdale, New Jersey, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[K. Nemire, R. H. Jacoby, and S. R. Ellis. Simulation fidelity of a virtual environment display. <i>Human Factors</i>, 36(1):1994, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Carol O'Sullivan, Sarah Howlett, Yann Morvan, Rachel McDonnell, and Keith O'Conor. Perceptually Adaptive Graphics. In Christophe Schlick and Werner Purgathofer, editors, <i>STAR-Proceedings of Eurographics 2004</i>, number STAR-6 in State of the Art Reports, pages 141--164. INRIA and the Eurographics Association, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[A. J. Parkin, J. M. Gardiner, and R. Rosser. Functional aspects of recollective experience in face recognition. <i>Consciousness and Cognition</i>, 4(4):387--398, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[S. N. Pattanaik, J. A. Ferwerda, D. A. Greenberg, and M. D. Fairchild. A multiscale model of adaptation and spatial vision for realistic imaging. In <i>Computer Graphics (ACM SIGGRAPH '98 Proceedings)</i>, pages 287--298, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Bobby Program Chair-Bodenheimer, Carol Program Chair-O'Sullivan, Katerina Conference Chair-Mania, and Bernhard Conference Chair-Riecke. Apgv '09: Proceedings of the 6th symposium on applied perception in graphics and visualization. <i>Chania, Crete, Greece</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Sarah Program Chair-Creem-Regehr and Karol Program Chair-Myszkowski. Apgv '08: Proceedings of the 5th symposium on applied perception in graphics and visualization. <i>Los Angeles, California</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[P. Rademacher, J. Lengyel, E. Cutrell, and T. Whitted. Measuring the perception of visual realism in images, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360659</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Ganesh Ramanarayanan, Kavita Bala, and James A. Ferwerda. Perception of complex aggregates. In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</i>, pages 1--10, New York, NY, USA, 2008. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276472</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Ganesh Ramanarayanan, James Ferwerda, Bruce Walter, and Kavita Bala. Visual equivalence: towards a new standard for image fidelity. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, page 76, New York, NY, USA, 2007. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311543</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Mahesh Ramasubramanian, Sumanta N. Pattanaik, and Donald P. Greenberg. A perceptually based physical error metric for realistic image synthesis. In <i>SIGGRAPH '99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pages 73--82, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[J. E. Raymond, K. L. Shapiro, and K. M. Arnell. Temporary suppression of visual processing in an RSVP task: An attentional blink? <i>Journal of Experimental Psychology: Human Perception &amp; Performance</i>, 18(3):849--860, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311569</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Matthew J. P. Regan, Gavin S. P. Miller, Steven M. Rubin, and Chris Kogelnik. A real-time low-latency hardware light-field renderer. In <i>SIGGRAPH '99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pages 287--290, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566575</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Erik Reinhard, Michael Stark, Peter Shirley, and James Ferwerda. Photographic tone reproduction for digital images. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, pages 267--276, New York, NY, USA, 2002. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[R. A. Rensink, J. K. O'Regan, and J. J. Clark. To see or not to see: The need for attention to perceive changes in scenes. <i>Psychological Science</i>, 8:368--373, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Ronald A. Rensink. Seeing, sensing, and scrutinizing. <i>Vision Research</i>, 40(10--12):1469--1487, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618711</ref_obj_id>
				<ref_obj_pid>616063</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[James C. Rodger and Roger A. Browse. Choosing rendering parameters for effective communication of 3d shape. <i>IEEE Computer Graphics and Applications</i>, 20(2):20--28, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Thomas B. Sheridan. Remote manipulative control with transmission delay. <i>IEEE Transactions on Human Factors in Electronics</i>, 4(1):25--29, 1963.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>128956</ref_obj_id>
				<ref_obj_pid>128947</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Thomas B. Sheridan. Musings on telepresence and virtual presence. <i>Presence</i>, 1(1):120--125, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[D. J. Simons and R. A. Rensink. Change blindness: Past, present, and future. <i>Trends in Cognitive Science</i>, 9(1):16--20, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Daniel J. Simons. Current approaches to change blindness. <i>Visual Cognition</i>, 7(1/2/3):1--15, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[P. H. Smith and J. Van Rosendale. Data and visualization corridors report on the 1998 CVD workshop series (sponsored by DOE and NSF). Technical Report CACR-164, Center for Advanced Computing Research, California Institute of Technology, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015795</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[William A. Stokes, James A. Ferwerda, Bruce J. Walter, and Donald P. Greenberg. Perceptual illumination compone high quality global illumination rendering. <i>ACM Transactions on Graphics</i>, 23(3):742--749, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1394288</ref_obj_id>
				<ref_obj_pid>1394281</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Veronica Sundstedt, Efstathios Stavrakis, Michael Wimmer, and Erik Reinhard. A psychophysical study of fixation behavior in a computer game. In <i>APGV '08: Proceedings of the 5th symposium on Applied perception in graphics and visualization</i>, pages 43--50, New York, NY, USA, 2008. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[J. J. Thomas and K. A. Cook. <i>Illuminating the Path: Research and Development Agenda for Visual Analytics</i>. IEEE Press, Piscataway, New Jersey, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5091</ref_obj_id>
				<ref_obj_pid>5088</ref_obj_pid>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[A. Treisman. Preattentive processing in vision. <i>Computer Vision, Graphics, and Image Processing</i>, 31:156--177, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[A. Treisman. Search, similarity, and integration of features between and within dimensions. <i>Journal of Experimental Psychology: Human Perception &amp; Performance</i>, 17(3):652--676, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[A. Treisman and S. Gormican. Feature analysis in early vision: Evidence from search asymmetries. <i>Psychological Review</i>, 95(1):15--48, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[E. Tulving. <i>Elements of Episodic Memory</i>. Oxford: Oxford Science Publications, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[Jack Tumblin and Holly E. Rushmeier. Tone Reproduction for Realistic Images. <i>IEEE Computer Graphics and Applications</i>, 13(6):42--48, November 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836073</ref_obj_id>
				<ref_obj_pid>523977</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[S. Uno and M. Slater. The sensitivity of presence to collision response. In <i>VRAIS '97: Proceedings of the 1997 Virtual Reality Annual International Symposium (VRAIS '97)</i>, page 95, Washington, DC, USA, 1997. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[C. J. van den Branden Lambrecht. <i>Perceptual models and architectures for video coding applications</i>. PhD thesis, Ecole Polytechnique Federal de Lausanne, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246754</ref_obj_id>
				<ref_obj_pid>1246749</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[David Waller, Earl Hunt, and David Knapp. The transfer of spatial knowledge in virtual environment training. <i>Presence: Teleoper. Virtual Environ.</i>, 7(2):129--143, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927407</ref_obj_id>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[B. J. Walter. <i>Density estimation techniques for global illumination</i>. Ph.D. thesis, Cornell University, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147161</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[Leonard Wanger. The effect of shadow quality on the perception of spatial relationships in computer generated imagery. In <i>I3D '92: Proceedings of the 1992 symposium on Interactive 3D graphics</i>, pages 39--42, New York, NY, USA, 1992. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>983611</ref_obj_id>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[Colin Ware. <i>Information Visualization: Perception for Design, 2nd Edition</i>. Morgan Kaufmann Publishers, Inc., San Francisco, California, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[Benjamin Watson, Neff Walker, Larry F. Hodges, and Martin Reddy. An evaluation of level of detail degradation in head-mounted display peripheries. <i>Presence</i>, 6(6):630--637, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835966</ref_obj_id>
				<ref_obj_pid>832289</ref_obj_pid>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[Benjamin Watson, Neff Walker, Peter Woytiuk, and William Ribarsky. Maintaining usability during 3d placement despite delay. In <i>VR '03: Proceedings of the IEEE Virtual Reality 2003</i>, page 133, Washington, DC, USA, 2003. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[R. B. Welch, T. T. Blackmon, A. Liu, B. A. Mellers, and L. W. Stark. The effects of pictorial realism, delay of visual feedback and observer interactivity on the subjective sense of presence. <i>Presence, Teleoperators and Virtual Environments</i>, 5(3):263--273, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1265961</ref_obj_id>
				<ref_obj_pid>1265957</ref_obj_pid>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[Betsy Williams, Gayathri Narasimham, Claire Westerman, John Rieser, and Bobby Bodenheimer. Functional similarities in spatial representations between real and virtual environments. <i>ACM Trans. Appl. Percept.</i>, 4(2):12, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[Jeremy M. Wolfe. Guided Search 2.0: A revised model of visual search. <i>Psychonomic Bulletin &amp; Review</i>, 1(2):202--238, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[Jeremy M. Wolfe and Kyle R. Cave. Deploying visual attention: The Guided Search model. In T. Troscianko and A. Blake, editors, <i>AI and the Eye</i>, pages 79--103. John Wiley &amp; Sons, Inc., Chichester, United Kingdom, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[Jeremy M. Wolfe, Kyle R. Cave, and Susan L. Franzel. Guided Search: An alternative to the feature integration model for visual search. <i>Journal of Experimental Psychology: Human Perception &amp; Performance</i>, 15(3):419--433, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1048097</ref_obj_id>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[P Zimmons. <i>The Influence of Lighting Quality on Presence and task Performance in Virtual Environment, Unpublished PhD thesis</i>. PhD thesis, University of North Carolina, ftp://ftp.cs.unc.edu/pub/publications/techreports/04-017.pdf, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1621011</ref_obj_id>
				<ref_obj_pid>1620993</ref_obj_pid>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[Alexandros Zotos, Katerina Mania, and Nicholaos Mourkoussis. A schema-based selective rendering framework. In <i>APGV '09: Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization</i>, pages 85--92, New York, NY, USA, 2009. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[Aks, D. J., and Enns, J. T. Visual search for size is influenced by a background texture gradient. <i>Journal of Experimental Psychology: Perception and Performance 22</i>, 6 (1996), 1467--1481.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[Banks, W. P., and Prinzmetal, W. Configurational effects in visual information processing. <i>Perception &amp; Psychophysics 19</i> (1976), 361--367.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[Bauer, B., Jolicoeur, P., and Cowan, W. B. Visual search for colour targets that are or are not linearly-separable from distractors. <i>Vision Research 36</i> (1996), 1439--1446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[Bauer, B., Jolicoeur, P., and Cowan, W. B. The linear separability effect in color visual search: Ruling out the additive color hypothesis. <i>Perception &amp; Psychophysics 60</i>, 6 (1998), 1083--1093.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833862</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[Bergman, L. D., Rogowitz, B. E., and Treinish, L. A. A rule-based tool for assisting colormap selection. In <i>Proceedings Visualization '95</i> (Atlanta, Georgia, 1995), pp. 118--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[Bruckner, L. A. On Chernoff faces. In <i>Graphical Representation of Multivariate Data</i>, P. C. C. Wang, Ed. Academic Press, New York, New York, 1978, pp. 93--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[Callaghan, T. C. Dimensional interaction of hue and brightness in preattentive field segregation. <i>Perception &amp; Psychophysics 36</i>, 1 (1984), 25--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[Callaghan, T. C. Interference and domination in texture segregation: Hue, geometric form, and line orientation. <i>Perception &amp; Psychophysics 46</i>, 4 (1989), 299--311.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[Callaghan, T. C. Interference and dominance in texture segregation. In <i>Visual Search</i>, D. Brogan, Ed. Taylor &amp; Francis, New York, New York, 1990, pp. 81--87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[Chernoff, H. The use of faces to represent points in kdimensional space graphically. <i>Journal of the American Statistical Association 68</i>, 342 (1973), 361--367.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[CIE. <i>CIE Publication No. 15, Supplement Number 2 (E-1.3.1): Official Recommendations on Uniform Color Spaces, Color-Difference Equations, and Metric Color Terms</i>. Commission Internationale de L'&#200;clairge, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[Coren, S., and Hakstian, A. R. Color vision screening without the use of technical equipment: Scale development and cross-validation. <i>Perception &amp; Psychophysics 43</i> (1988), 115--120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[Cutting, J. E., and Millard, R. T. Three gradients and the perception of flat and curved surfaces. <i>Journal of Experimental Psychology: General 113</i>, 2 (1984), 198--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[D'Zmura, M. Color in visual search. <i>Vision Research 31</i>, 6 (1991), 951--966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[Egeth, H. E., and Yantis, S. Visual attention: Control, representation, and time course. <i>Annual Review of Psychology 48</i> (1997), 269--297.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[Foley, J., and Ribarsky, W. Next-generation data visualization tools. In <i>Scientific Visualization: Advances and Challenges</i>, L. Rosenblum, Ed. Academic Press, San Diego, California, 1994, pp. 103--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D., van Dam, A., Feiner, S. K., and Hughes, J. F. <i>Computer Graphics: Principles and Practice</i>. Addison-Wesley Publishing Company, Reading, Massachusetts, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[Glassner, A. S. <i>Principles of Digital Image Synthesis</i>. Morgan Kaufmann Publishers, Inc., San Francisco, California, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[Grinstein, G., Pickett, R., and Williams, M. EXVIX: An exploratory data visualization environment. In <i>Proceedings Graphics Interface '89</i> (London, Canada, 1989), pp. 254--261.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[Hallett, P. E. Segregation of mesh-derived textures evaluated by resistance to added disorder. <i>Vision Research 32</i>, 10 (1992), 1899--1911.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[Haralick, R. M., Shanmugam, K., and Dinstein, I. Textural features for image classification. <i>IEEE Transactions on System, Man, and Cybernetics SMC-3</i>, 6 (1973), 610--621.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245597</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G. Choosing effective colours for data visualization. In <i>Proceedings Visualization '96</i> (San Francisco, California, 1996), pp. 263--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G. Building a perceptual visualisation architecture. <i>Behaviour and Information Technology (in press)</i> (1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>217855</ref_obj_id>
				<ref_obj_pid>217853</ref_obj_pid>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G., Booth, K. S., and Enns, J. T. Real-time multivariate data visualization using preattentive processing. <i>ACM Transactions on Modeling and Computer Simulation 5</i>, 3 (1995), 190--221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288232</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G., and Enns, J. T. Building perceptual textures to visualize multidimensional datasets. In <i>Proceedings Visualization '98</i> (Research Triangle Park, North Carolina, 1998), pp. 111--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258796</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[Interrante, V. Illustrating surface shape in volume data via principle direction-driven 3d line integral convolution. In <i>SIGGRAPH 97 Conference Proceedings</i> (Los Angeles, California, 1997), T. Whitted, Ed., pp. 109--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B. Textons, the elements of texture perception, and their interactions. <i>Nature 290</i> (1981), 91--97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B. A theory of preattentive texture discrimination based on first-order statistics of textons. <i>Biological Cybernetics 41</i> (1981), 131--138.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B. A brief outline of the texton theory of human vision. <i>Trends in Neuroscience 7</i>, 2 (1984), 41--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B., and Bergen, J. R. Textons, the fundamental elements in preattentive vision and perception of textures. <i>The Bell System Technical Journal 62</i>, 6 (1983), 1619--1645.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[Kawai, M., Uchikawa, K., and Ujike, H. Influence of color category on visual search. In <i>Annual Meeting of the Association for Research in Vision and Ophthalmology</i> (Fort Lauderdale, Florida, 1995), p. #2991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288234</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Laidlaw, D. H., Ahrens, E. T., Kremers, D., Avalos, M. J., Jacobs, R. E., and Readhead, C. Visualizing diffusion tensor images of the mouse spinal cord. In <i>Proceedings Visualization '98</i> (Research Triangle Park, North Carolina, 1998), pp. 127--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949634</ref_obj_id>
				<ref_obj_pid>949607</ref_obj_pid>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[Levkowitz, H. Color icons: Merging color and texture perception for integrated visualization of multiple parameters. In <i>Proceedings Visualization '91</i> (San Diego, California, 1991), pp. 164--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617723</ref_obj_id>
				<ref_obj_pid>616021</ref_obj_pid>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[Levkowitz, H., and Herman, G. T. Color scales for image data. <i>IEEE Computer Graphics &amp; Applications 12</i>, 1 (1992), 72--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[Liu, F., and Picard, R. W. Periodicity, directionality, and randomness: Wold features for perceptual pattern recognition. In <i>Proceedings 12th International Conference on Pattern Recognition</i> (Jerusalem, Israel, 1994), pp. 1--5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[Mack, A., and Rock, I. <i>Inattentional Blindness</i>. MIT Press, Menlo Park, California, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[Malik, J., and Perona, P. Preattentive texture discrimination with early vision mechanisms. <i>Journal of the Optical Society of America A 7</i>, 5 (1990), 923--932.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[Meier, B. J. Painterly rendering for animation. In <i>SIGGRAPH 96 Conference Proceedings</i> (New Orleans, Louisiana, 1996), H. Rushmeier, Ed., pp. 477--484.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>167590</ref_obj_id>
				<ref_obj_pid>167580</ref_obj_pid>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Rao, A. R., and Lohse, G. L. Identifying high level features of texture perception. <i>CVGIP: Graphics Models and Image Processing 55</i>, 3 (1993), 218--233.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949887</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[Rao, A. R., and Lohse, G. L. Towards a texture naming system: Identifying relevant dimensions of texture. In <i>Proceedings Visualization '93</i> (San Jose, California, 1993), pp. 220--227.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>167602</ref_obj_id>
				<ref_obj_pid>167592</ref_obj_pid>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[Reed, T. R., and Hans Du Buf, J. M. A review of recent texture segmentation and feature extraction techniques. <i>CVGIP: Image Understanding 57</i>, 3 (1993), 359--372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[Rensink, R. A., O'Regan, J. K., and Clark, J. J. To see or not to see: The need for attention to perceive changes in scenes. <i>Psychological Science 8</i> (1997), 368--373.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91436</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[Rheingans, P., and Tebbs, B. A tool for dynamic explorations of color mappings. <i>Computer Graphics 24</i>, 2 (1990), 145--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617433</ref_obj_id>
				<ref_obj_pid>616001</ref_obj_pid>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[Robertson, P. K. Visualizing color gamuts: A user interface for the effective use of perceptual color spaces in data displays. <i>IEEE Computer Graphics &amp; Applications 8</i>, 5 (1988), 50--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949889</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[Rogowitz, B. E., and Treinish, L. A. An architecture for rule-based visualization. In <i>Proceedings Visualization '93</i> (San Jose, California, 1993), pp. 236--243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Wong, M. T., Hughes, J. F., and Salesin, D. H. Orientable textures for image-based pen-and-ink illustration. In <i>SIGGRAPH 97 Conference Proceedings</i> (Los Angeles, California, 1997), T. Whitted, Ed., pp. 401--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801128</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>181</ref_seq_no>
				<ref_text><![CDATA[Schweitzer, D. Artificial texturing: An aid to surface visualization. <i>Computer Graphics (SIGGRAPH 83 Conference Proceedings) 17</i>, 3 (1983), 23--29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>182</ref_seq_no>
				<ref_text><![CDATA[Simon, D. J., and Levin, D. T. Change blindness. <i>Trends in Cognitive Science 1</i> (1997), 261--267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>183</ref_seq_no>
				<ref_text><![CDATA[Snowden, R. J. Texture segregation and visual search: A comparison of the effects of random variations along irrelevant dimensions. <i>Journal of Experimental Psychology: Human Perception and Performance 24</i>, 5 (1998), 1354--1367.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>184</ref_seq_no>
				<ref_text><![CDATA[Tamura, H., Mori, S., and Yamawaki, T. Textural features corresponding to visual perception. <i>IEEE Transactions on Systems, Man, and Cybernetics SMC-8</i>, 6 (1978), 460--473.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5091</ref_obj_id>
				<ref_obj_pid>5088</ref_obj_pid>
				<ref_seq_no>185</ref_seq_no>
				<ref_text><![CDATA[Triesman, A. Preattentive processing in vision. <i>Computer Vision, Graphics and Image Processing 31</i> (1985), 156--177.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>186</ref_seq_no>
				<ref_text><![CDATA[Triesman, A. Search, similarity, and integration of features between and within dimensions. <i>Journal of Experimental Psychology: Human Perception &amp; Performance 17</i>, 3 (1991), 652--676.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>187</ref_seq_no>
				<ref_text><![CDATA[Triesman, A., and Gormican, S. Feature analysis in early vision: Evidence from search asymmetries. <i>Psychological Review 95</i>, 1 (1988), 15--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237285</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>188</ref_seq_no>
				<ref_text><![CDATA[Turk, G., and Banks, D. Image-guided streamline placement. In <i>SIGGRAPH 96 Conference Proceedings</i> (New Orleans, Louisiana, 1996), H. Rushmeier, Ed., pp. 453--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617432</ref_obj_id>
				<ref_obj_pid>616001</ref_obj_pid>
				<ref_seq_no>189</ref_seq_no>
				<ref_text><![CDATA[Ware, C. Color sequences for univariate maps: Theory, experiments, and principles. <i>IEEE Computer Graphics &amp; Applications 8</i>, 5 (1988), 41--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>46357</ref_obj_id>
				<ref_obj_pid>46356</ref_obj_pid>
				<ref_seq_no>190</ref_seq_no>
				<ref_text><![CDATA[Ware, C., and Beatty, J. C. Using colour dimensions to display data dimensions. <i>Human Factors 30</i>, 2 (1988), 127--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>200974</ref_obj_id>
				<ref_obj_pid>200972</ref_obj_pid>
				<ref_seq_no>191</ref_seq_no>
				<ref_text><![CDATA[Ware, C., and Knight, W. Using visual texture for information display. <i>ACM Transactions on Graphics 14</i>, 1 (1995), 3--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>192</ref_seq_no>
				<ref_text><![CDATA[Wolfe, J. M. Guided Search 2.0: A revised model of visual search. <i>Psychonomic Bulletin &amp; Review 1, 2</i> (1994), 202--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>193</ref_seq_no>
				<ref_text><![CDATA[Wolfe, J. M., Yu, K. P., Stewart, M. I., Shorter, A. D., Friedman-Hill, S. R., and Cave, K. R. Limitations on the parallel guidance of visual search: Color x color and orientation x orientation conjunctions. <i>Journal of Experimental Psychology: Human Perception &amp; Performance 16</i>, 4 (1990), 879--892.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>194</ref_seq_no>
				<ref_text><![CDATA[Wyszecki, G., and Stiles, W. S. <i>Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition</i>. John Wiley &amp; Sons, Inc., New York, New York, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>195</ref_seq_no>
				<ref_text><![CDATA[Aks, D. J. and Enns, J. T. 1996. Visual search for size is influenced by a background texture gradient. <i>J. Experiment. Psych.: Human Percept. Perf. 22</i>, 6, 1467--1481.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833862</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>196</ref_seq_no>
				<ref_text><![CDATA[Bergman, L. D., Rogowitz, B. E., and Treinish, L. A. 1995. A rule-based tool for assisting colormap selection. In <i>Proceedings of Visualization '95</i> (Atlanta, Ga.). 118--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>197</ref_seq_no>
				<ref_text><![CDATA[Birren, F. 1969. <i>Munsell: A Grammar of Color</i>. Van Nostrand Reinhold Company, New York, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>198</ref_seq_no>
				<ref_text><![CDATA[Brown, R. 1978. Impressionist technique: Pissarro's optical mixture. In <i>Impressionism in Perspective</i>, B. E. White, Ed. Prentice-Hall, Inc., Englewood Cliffs, N. J., 114--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>199</ref_seq_no>
				<ref_text><![CDATA[Callaghan, T. C. 1990. Interference and dominance in texture segregation. In <i>Visual Search</i>, D. Brogan, Ed. Taylor &amp; Francis, New York, 81--87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>200</ref_seq_no>
				<ref_text><![CDATA[Chevreul, M. E. 1967. <i>The Principles of Harmony and Contrast of Colors and Their Applications to the Arts</i>. Reinhold Publishing Corporation, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>201</ref_seq_no>
				<ref_text><![CDATA[CIE. 1978. <i>CIE Publication No. 15, Supplement Number 2 (E-1.3.1, 1971): Official Recommendations on Uniform Color Spaces, Color-Difference Equations, and Metric Color Terms</i>. Commission Internationale de L'&#200;clairge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>202</ref_seq_no>
				<ref_text><![CDATA[Coren, S., Ward, L. M., and Enns, J. T. 2003. <i>Sensation and Perception (6th Edition)</i>. Wiley, New York, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>203</ref_seq_no>
				<ref_text><![CDATA[Curtis, C. J., Anderson, S. E., Seims, J. E., Fleischer, K. W., and Salesin, D. H. 1997. Computer-generated watercolor. In <i>SIGGRAPH 97 Conference Proceedings</i> (Los Angeles, Calif.). T. Whitted, Ed. ACM, New York, 421--430.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>204</ref_seq_no>
				<ref_text><![CDATA[Cutting, J. E. and Millard, R. T. 1984. Three gradients and the perception of flat and curved surfaces. <i>J. Experiment. Psych.: General 113</i>, 2, 198--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375241</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>205</ref_seq_no>
				<ref_text><![CDATA[Ebert, D. and Rheingans, P. 2000. Volume illustration: Non-photorealistic rendering of volume models. In <i>Proceedings of Visualization 2000</i> (San Francisco, Calif.). 195--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>206</ref_seq_no>
				<ref_text><![CDATA[Egeth, H. E. and Yantis, S. 1997. Visual attention: Control, representation, and time course. <i>Ann. Rev. Psychol. 48</i>, 269--297.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192223</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>207</ref_seq_no>
				<ref_text><![CDATA[Finkelstein, A. and Salesin, D. H. 1994. Multiresolution curves. In <i>SIGGRAPH 94 Conference Proceedings</i> (Orlando, Fla.). A. S. Glassner, Ed. ACM, New York, 261--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508545</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>208</ref_seq_no>
				<ref_text><![CDATA[Gooch, B., Coombe, G., and Shirley, P. 2002. Artistic vision: Painterly rendering using computer vision techniques. In <i>Proceedings of the NPAR 2002 Symposium on Non-Photorealistic Animation and Rendering</i> (Annecy, France). 83--90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>209</ref_seq_no>
				<ref_text><![CDATA[Gooch, B. and Gooch, A. 2001. <i>Non-Photorealistic Rendering</i>. A K Peters, Ltd., Natick, Mass.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>210</ref_seq_no>
				<ref_text><![CDATA[Grinstein, G., Pickett, R., and Williams, M. 1989. EXVIS: An exploratory data visualization environment. In <i>Proceedings of Graphics Interface '89</i> (London, Ont., Canada). 254--261.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>211</ref_seq_no>
				<ref_text><![CDATA[Haberli, P. 1990. Paint by numbers: Abstract image representations. <i>Comput. Graph. (SIGGRAPH 90 Conference Proceedings) 24</i>, 4, 207--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>212</ref_seq_no>
				<ref_text><![CDATA[Haberli, P. and Segal, M. 1993. Texture mapping as a fundamental drawing primative. In <i>Proceedings of the 4th Eurographics Workshop on Rendering</i> (Paris, France). M. Cohen, C. Puech, and F. Sillion, Eds. 259--266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>213</ref_seq_no>
				<ref_text><![CDATA[Haralick, R. M., Shanmugam, K., and Dinstein, I. 1973. Textural features for image classiffication. <i>IEEE Trans. Syst., Man, and Cybernet. SMC-3</i>, 6, 610--621.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245597</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>214</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G. 1996. Choosing effective colours for data visualization. In <i>Proceedings of Visualization '96</i> (San Francisco, Calif.). 263--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>230563</ref_obj_id>
				<ref_obj_pid>230562</ref_obj_pid>
				<ref_seq_no>215</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G., Booth, K. S., and Enns, J. T. 1996. High-speed visual estimation using preattentive processing. <i>ACM Trans. Computer-Hum. Interact. 3</i>, 2, 107--135.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288232</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>216</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G. and Enns, J. T. 1998. Building perceptual textures to visualize multidimensional datasets. In <i>Proceedings of Visualization '98</i> (Research Triangle Park, N. C.). 111--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614428</ref_obj_id>
				<ref_obj_pid>614274</ref_obj_pid>
				<ref_seq_no>217</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G. and Enns, J. T. 1999. Large datasets at a glance: Combining textures and colors in scientic visualization. <i>IEEE Trans. Visual. Comput. Graph. 5</i>, 2, 145--167.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>218</ref_seq_no>
				<ref_text><![CDATA[Hering, E. 1964. <i>Outlines of a Theory of Light Sense</i>. Harvard University Press, Cambridge, Mass.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>219</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In <i>SIGGRAPH 98 Conference Proceedings</i> (Orlando, Fla.). M. Cohen, Ed. ACM, New York, 453--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508546</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>220</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 2002. Fast texture maps. In <i>Proceedings of the NPAR 2002 Symposium on Non-Photorealistic Animation and Rendering</i> (Annecy, France). 91--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>221</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. 2001. Image analogies. In <i>SIGGRAPH 2001 Conference Proceedings</i> (Los Angeles, Calif.). E. Fiume, Ed. ACM, New York, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192186</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>222</ref_seq_no>
				<ref_text><![CDATA[Hsu, S. C. and Lee, I. H. H. 1994. Drawing and animation using skeletal strokes. In <i>SIGGRAPH 94 Conference Proceedings</i> (Orlando, Fla.). A. Glassner, Ed. ACM, New York, 109--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618772</ref_obj_id>
				<ref_obj_pid>616067</ref_obj_pid>
				<ref_seq_no>223</ref_seq_no>
				<ref_text><![CDATA[Interrante, V. 2000. Harnessing natural textures for multivariate visualization. <i>IEEE Comput. Graph. Applic. 20</i>, 6, 6--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>224</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B. 1975. Experiments in the visual perception of texture. <i>Scient. Amer. 232</i>, 34--43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>225</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B. 1984. A brief outline of the texton theory of human vision. <i>Trends Neurosci. 7</i>, 2, 41--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>226</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B., Gilbert, E. N., and Shepp, L. A. 1973. Inability of humans to discriminate between visual textures that agree in second-order statistics---revisited. <i>Perception 2</i>, 391--405.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>227</ref_seq_no>
				<ref_text><![CDATA[Jul&#233;sz, B., Gilbert, E. N., and Victor, J. D. 1978. Visual discrimination of textures with identical third-order statistics. <i>Biologic. Cybernet. 31</i>, 137--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319429</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>228</ref_seq_no>
				<ref_text><![CDATA[Kirby, R. M., Marmanis, H., and Laidlaw, D. H. 1999. Visualizing multivalued data from 2D incompressible flows using concepts from painting. In <i>Proceedings of Visualization '99</i> (San Francisco, Calif.). 333--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618804</ref_obj_id>
				<ref_obj_pid>616069</ref_obj_pid>
				<ref_seq_no>229</ref_seq_no>
				<ref_text><![CDATA[Laidlaw, D. H. 2001. Loose, artistic "textures" for visualization. <i>IEEE Comput. Graph. Applic. 21</i>, 2, 6--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288234</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>230</ref_seq_no>
				<ref_text><![CDATA[Laidlaw, D. H., Ahrens, E. T., Kremers, D., Avalos, M. J., Jacobs, R. E., and Readhead, C. 1998. Visualizing diffusion tensor images of the mouse spinal cord. In <i>Proceedings of Visualization '98</i> (Research Triangle Park, N. C.). 127--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808605</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>231</ref_seq_no>
				<ref_text><![CDATA[Lewis, J.-P. 1984. Texture synthesis for digital painting. <i>Comput. Graph. (SIGGRAPH 84 Proceedings) 18</i>, 3, 245--252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>232</ref_seq_no>
				<ref_text><![CDATA[Litwinowicz, P. 1997. Processing images and video for an impressionist effect. In <i>SIGGRAPH 97 Conference Proceedings</i> (Los Angeles, Calif.). T. Whitted, Ed. ACM, New York, 407--414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>233</ref_seq_no>
				<ref_text><![CDATA[Liu, G., Healey, C. G., and Enns, J. T. 2003. Target detection and localization in visual search: A dual systems perspective. <i>Percept. Psychophys. 65</i>, 5, 678--694.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602131</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>234</ref_seq_no>
				<ref_text><![CDATA[Lu, A., Morris, C. J., Ebert, D. S., Rheingans, P., and Hansen, C. 2002. Non-photorealistic volume rendering using stippling techniques. In <i>Proceedings of Visualization 2002</i> (Boston, Mass.). 211--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>235</ref_seq_no>
				<ref_text><![CDATA[MacEachren, A. M. 1995. <i>How Maps Work</i>. Guilford Publications, Inc., New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>236</ref_seq_no>
				<ref_text><![CDATA[Mack, A. and Rock, I. 1998. <i>Inattentional Blindness</i>. MIT Press, Menlo Park, Calif.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>237</ref_seq_no>
				<ref_text><![CDATA[McCormick, B. H., DeFanti, T. A., and Brown, M. D. 1987. Visualization in scientific computing. <i>Comput. Graph. 21</i>, 6, 1--14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>238</ref_seq_no>
				<ref_text><![CDATA[Meier, B. J. 1996. Painterly rendering for animation. In <i>SIGGRAPH 96 Conference Proceedings</i>, (New Orleans, La.) H. Rushmeier, Ed. ACM, New York, 477--484.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>239</ref_seq_no>
				<ref_text><![CDATA[Munsell, A. H. 1905. <i>A Color Notation</i>. Geo. H. Ellis Co., Boston, Mass.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>240</ref_seq_no>
				<ref_text><![CDATA[Pomerantz, J. and Pristach, E. A. 1989. Emergent features, attention, and perceptual glue in visual form perception. <i>J. Experiment. Psych.: Human Percept. Perf. 15</i>, 4, 635--649.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>241</ref_seq_no>
				<ref_text><![CDATA[Posner, M. I. and Raichle, M. E. 1994. Images of mind. Scientific American Library.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>242</ref_seq_no>
				<ref_text><![CDATA[Ramachandran, V. S. and Hirstein, W. 1999. The science of art: A neurological theory of aesthetic experience. <i>J. of Conscious. Stud. 6</i>, 6--7, 15--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>167590</ref_obj_id>
				<ref_obj_pid>167580</ref_obj_pid>
				<ref_seq_no>243</ref_seq_no>
				<ref_text><![CDATA[Rao, A. R. and Lohse, G. L. 1993a. Identifying high level features of texture perception. <i>CVGIP: Graph. Models Image Process. 55</i>, 3, 218--233.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949887</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>244</ref_seq_no>
				<ref_text><![CDATA[Rao, A. R. and Lohse, G. L. 1993b. Towards a texture naming system: Identifying relevant dimensions of texture. In <i>Proceedings of Visualization '93</i> (San Jose, Calif.). 220--227.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>245</ref_seq_no>
				<ref_text><![CDATA[Rensink, R. A. 2000. Seeing, sensing, and scrutinizing. <i>Vision Res. 40</i>, 10--12, 1469--1487.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614497</ref_obj_id>
				<ref_obj_pid>614283</ref_obj_pid>
				<ref_seq_no>246</ref_seq_no>
				<ref_text><![CDATA[Rheingans, P. and Ebert, D. 2001. Volume illustration: Nonphotorealistic rendering of volume models. <i>IEEE Trans. Vis. Comput. Graph. 7</i>, 3, 253--264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91436</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>247</ref_seq_no>
				<ref_text><![CDATA[Rheingans, P. and Tebbs, B. 1990. A tool for dynamic explorations of color mappings. <i>Comput. Graph. 24</i>, 2, 145--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>949889</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>248</ref_seq_no>
				<ref_text><![CDATA[Rogowitz, B. E. and Treinish, L. A. 1993. An architecture for rule-based visualization. In <i>Proceedings of Visualization '93</i> (San Jose, Calif.). 236--243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>249</ref_seq_no>
				<ref_text><![CDATA[Rood. O. N. 1879. <i>Modern Chromatics, with Applications to Art and Industry</i>. Appleton, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617897</ref_obj_id>
				<ref_obj_pid>616031</ref_obj_pid>
				<ref_seq_no>250</ref_seq_no>
				<ref_text><![CDATA[Rosenblum, L. J. 1994. Research issues in scientific visualization. <i>IEEE Comput. Graph. Applic. 14</i>, 2, 61--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237286</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>251</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Anderson, C., Lischinski, D., and Salesin, D. H. 1996. Scale-dependent reproduction of pen-and-ink illustrations. In <i>SIGGRAPH 96 Conference Proceedings</i> (New Orleans, La.). H. Rushmeier, Ed. ACM, New York, 461--468.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>252</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Anderson, S. E., Barzel, R., and Salesin, D. H. 1994. Interactive pen-and-ink illustrations. In <i>SIGGRAPH 94 Conference Proceedings</i> (Orlando, Fla.). A. S. Glassner, Ed. ACM, New York, 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>253</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Wong, M. T., Hughes, J. F., and Salesin, D. H. 1997. Orientable textures for image-based pen-and-ink illustration. In <i>SIGGRAPH 97 Conference Proceedings</i> (Los Angeles, Calif.). T. Whitted, Ed. ACM, New York, 401--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>254</ref_seq_no>
				<ref_text><![CDATA[Schapiro, M. 1997. <i>Impressionism: Reflections and Perceptions</i>. George Brazillier, Inc., New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312145</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>255</ref_seq_no>
				<ref_text><![CDATA[Shiraishi, M. and Yamaguchi, Y. 1999. Image moment-based stroke placement. In <i>SIGGRAPH 99 Sketches &amp; Applications</i> (Los Angeles, Calif.). R. Kidd, Ed. ACM, New York, 247.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>256</ref_seq_no>
				<ref_text><![CDATA[Simons, D. J. 2000. Current approaches to change blindness. <i>Vis. Cognit</i>. 7, 1/2/3, 1--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>257</ref_seq_no>
				<ref_text><![CDATA[Slocum, T. A. 1998. <i>Thematic Cartography and Visualization</i>. Prentice-Hall, Inc., Upper Saddle River, N. J.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>258</ref_seq_no>
				<ref_text><![CDATA[Smith, P. H. and Van Rosendale, J. 1998. Data and visualization corridors report on the 1998 CVD workshop series (sponsored by DOE and NSF). Tech. Rep. CACR-164, Center for Advanced Computing Research, California Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>259</ref_seq_no>
				<ref_text><![CDATA[Snowden, R. J. 1998. Texture segregation and visual search: A comparison of the effects of random variations along irrelevant dimensions. <i>J. Experiment. Psych.: Human Percept. Perf. 24</i>, 5, 1354--1367.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>260</ref_seq_no>
				<ref_text><![CDATA[Sousa, M. C. and Buchanan, J. W. 1999a. Computer-generated graphite pencil rendering of 3d polygon models. <i>Comput. Graph. Forum (Proceedings Eurographics '99) 18</i>, 3, 195--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>261</ref_seq_no>
				<ref_text><![CDATA[Sousa, M. C. and Buchanan, J. W. 1999b. Computer-generated pencil drawings. In <i>Proceedings SKIGRAPH '99</i> (Banff, Canada).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>262</ref_seq_no>
				<ref_text><![CDATA[Sousa, M. C. and Buchanan, J. W. 2000. Observational models of graphite pencil materials. <i>Comput. Graph. Forum 19</i>, 1, 27--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15911</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>263</ref_seq_no>
				<ref_text><![CDATA[Strassmann, S. 1986. Hairy brushes. <i>Comput. Graph. (SIGGRAPH 86 Proceedings) 20</i>, 4, 185--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>544522</ref_obj_id>
				<ref_seq_no>264</ref_seq_no>
				<ref_text><![CDATA[Strothotte, T. and Schlechtweg, S. 2002. <i>Non-Photorealistic Computer Graphics: Modeling, Rendering and Animation</i>. Morgan Kaufmann, Inc., San Francisco, Calif.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>259274</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>265</ref_seq_no>
				<ref_text><![CDATA[Takagi, S. and Fujishiro, I. 1997. Microscopic structural modeling of colored pencil drawings. In <i>SIGGRAPH 97 Sketches &amp; Applications</i> (Los Angeles, Calif.). D. S. Ebert, Ed. ACM, New York, 187.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312375</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>266</ref_seq_no>
				<ref_text><![CDATA[Takagi, S., Fujishiro, I., and Nakajima, M. 1999. Volumetric modeling of artistic techniques in colored pencil drawing. In <i>SIGGRAPH 99 Sketches &amp; Applications</i> (Los Angeles, Calif.). R. Kidd, Ed. ACM, New York, 283.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>267</ref_seq_no>
				<ref_text><![CDATA[Tamura, H., Mori, S., and Yamawaki, T. 1978. Textural features corresponding to visual perception. <i>IEEE Trans. Sys., Man, and Cybernet. SMC-8</i>, 6, 460--473.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5091</ref_obj_id>
				<ref_obj_pid>5088</ref_obj_pid>
				<ref_seq_no>268</ref_seq_no>
				<ref_text><![CDATA[Triesman, A. 1985. Preattentive processing in vision. <i>Comput. Vis. Graph. Image Process. 31</i>, 156--177.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>269</ref_seq_no>
				<ref_text><![CDATA[Triesman, A. 1991. Search, similarity, and integration of features between and within dimensions. <i>J. Experiment. Psych.: Human Percept. Perf. 17</i>, 3, 652--676.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>270</ref_seq_no>
				<ref_text><![CDATA[Triesman, A. and Gormican, S. 1988. Feature analysis in early vision: Evidence from search asymmetries. <i>Psychol. Rev. 95</i>, 1, 15--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>33404</ref_obj_id>
				<ref_seq_no>271</ref_seq_no>
				<ref_text><![CDATA[Tufte, E. R. 1983. <i>The Visual Display of Quantitative Information</i>. Graphics Press, Cheshire, Conn.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78223</ref_obj_id>
				<ref_seq_no>272</ref_seq_no>
				<ref_text><![CDATA[Tufte, E. R. 1990. <i>Envisioning Information</i>. Graphics Press, Cheshire, Conn.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248468</ref_obj_id>
				<ref_seq_no>273</ref_seq_no>
				<ref_text><![CDATA[Tufte, E. R. 1997. <i>Visual Explanations: Images and Quantities, Evidence and Narrative</i>. Graphics Press, Cheshire, Conn.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>274</ref_seq_no>
				<ref_text><![CDATA[Venturi, L. 1978. Impressionist style. In <i>Impressionism in Perspective</i>, B. E. White, Ed. Prentice-Hall, Inc., Englewood Cliffs, N. J., 105--113.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617432</ref_obj_id>
				<ref_obj_pid>616001</ref_obj_pid>
				<ref_seq_no>275</ref_seq_no>
				<ref_text><![CDATA[Ware, C. 1988. Color sequences for univariate maps: Theory, experiments, and principles. <i>IEEE Comput Graph. Applic. 8</i>, 5, 41--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>329554</ref_obj_id>
				<ref_seq_no>276</ref_seq_no>
				<ref_text><![CDATA[Ware, C. 2000. <i>Information Visualization: Perception for Design</i>. Morgan-Kaufmann, San Francisco, Calif.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>200974</ref_obj_id>
				<ref_obj_pid>200972</ref_obj_pid>
				<ref_seq_no>277</ref_seq_no>
				<ref_text><![CDATA[Ware, C. and Knight, W. 1995. Using visual texture for information display. <i>ACM Trans. Graph. 14</i>, 1, 3--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>278</ref_seq_no>
				<ref_text><![CDATA[Weigle, C., Emigh, W. G., Liu, G., Taylor, R. M., Enns, J. T., and Healey, C. G. 2000. Oriented texture slivers: A technique for local value estimation of multiple scalar fields. In <i>Proceedings of Graphics Interface 2000</i> (Montr&#233;al, Quebec, Canada). 163--170.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>279</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G. and Salesin, D. H. 1994. Computer-generated pen-and-ink illustration. In <i>SIGGRAPH 94 Conference Proceedings</i> (Orlando, Fla.). A. Glassner, Ed. ACM, New York, 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>280</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G. and Salesin, D. H. 1996. Rendering free-form surfaces in pen-and-ink. In <i>SIGGRAPH 96 Conference Proceedings</i> (New Orleans, La.). H. Rushmeier, Ed. ACM, New York, 469--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>281</ref_seq_no>
				<ref_text><![CDATA[Wolfe, J. M. 1994. Guided Search 2.0: A revised model of visual search. <i>Psycho. Bull. Rev. 1</i>, 2, 202--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>282</ref_seq_no>
				<ref_text><![CDATA[Wolfe, J. M., Klempen, N., and Dahlen, K. 2000. Post attentive vision. <i>J. Experiment. Psych.: Human Percept. Perf. 26</i>, 2, 693--716.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>283</ref_seq_no>
				<ref_text><![CDATA[Zeki, S. 1999. <i>Inner Vision</i>. Oxford University Press, Oxford, U. K.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>284</ref_seq_no>
				<ref_text><![CDATA[Baltissen, R., and Ostermann, B. 1998. Are the dimiensions underlying aesthetic and affective judgment the same? <i>Empirical Studies of the Arts 16</i>, 2, 97--113.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>285</ref_seq_no>
				<ref_text><![CDATA[Barber, C., Dobkin, D., and Huhdanpaa, H. 1993. The quickhull algorithm for convex hull. Tech. Rep. TR GCG53, The Geometry Center, University of Minnesota, Minneapolis, MN.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>235821</ref_obj_id>
				<ref_obj_pid>235815</ref_obj_pid>
				<ref_seq_no>286</ref_seq_no>
				<ref_text><![CDATA[Barber, C., Dobkin, D., and Huhdanpaa, H. 1996. The quickhull algorithm for convex hulls. <i>ACM Trans. Math. Softw. 22</i>, 4, 469--483.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>287</ref_seq_no>
				<ref_text><![CDATA[Berlyne, D. 1971. <i>Aesthetics and Psychobiology</i>. Appleton-Century-Crofts, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>288</ref_seq_no>
				<ref_text><![CDATA[Birkhoff, G. 1932. <i>Aesthetic Measure</i>. Harvard University Press, Cambridge, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>289</ref_seq_no>
				<ref_text><![CDATA[Brown, Kevin, Q. 1979. Voronoi diagrams from convex hulls. <i>Information Processing Letters 9</i>, 5, 223--228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>290</ref_seq_no>
				<ref_text><![CDATA[Davis, R. 1936. An evaluation and test of birkhoff's aesthetic measure and formula. <i>Journal of General Psychology 15</i>, 231--240.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>291</ref_seq_no>
				<ref_text><![CDATA[Garland, M., and Heckbert, P. S. 1997. Surface simplification using quadric error metrics. In <i>SIGGRAPH 97 Conference Proceedings</i>, T. Whitted, Ed., 209--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288280</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>292</ref_seq_no>
				<ref_text><![CDATA[Garland, M., and Heckbert, P. S. 1998. Simplifying surfaces with color and texture using quadric error metrics. In <i>Proceedings Visualization '98</i>, 263--269.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>293</ref_seq_no>
				<ref_text><![CDATA[Garland, M. 1999. <i>The Design, Use, and Required Facilities of an Interactive Visual Computer Simulation Language to Explore Production Planning Problems</i>. Ph.D. thesis, Carnegie Mellon University, Pittsburg, PA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602121</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>294</ref_seq_no>
				<ref_text><![CDATA[Grigoryan, G., and Rheingans, P. 2002. Probabilistic surfaces: Point based primitives to show uncertainty. In <i>Proceedings Visualization 2002</i>, 147--153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>295</ref_seq_no>
				<ref_text><![CDATA[Haeberli, P. 1990. Paint by numbers: Abstract image representations. <i>Computer Graphics (SIGGRAPH 90 Conference Proceedings) 24</i>, 4, 207--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>966135</ref_obj_id>
				<ref_obj_pid>966131</ref_obj_pid>
				<ref_seq_no>296</ref_seq_no>
				<ref_text><![CDATA[Healey, C. G., Tateosian, L., Enns, J. T., and Remple, M. 2004. Perceptually based brush strokes for nonphotorealistic visualization. <i>ACM Trans. Graph. 23</i>, 1, 64--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280951</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>297</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In <i>SIGGRAPH 98 Conference Proceedings</i>, M. Cohen, Ed., 453--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508546</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>298</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 2002. Fast texture maps. In <i>Proceedings NPAR 2002 Symposium on Non-Photorealistic Animation and Rendering</i>, 91--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>986005</ref_obj_id>
				<ref_obj_pid>985921</ref_obj_pid>
				<ref_seq_no>299</ref_seq_no>
				<ref_text><![CDATA[Holman, D., Vertegaal, R., Sohn, C., and Cheng, D. 2004. Attentive display: paintings as attentive user interfaces. In <i>CHI '04: Extended abstracts of the 2004 conference on Human factors and computing systems</i>, ACM Press, 1127--1130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192186</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>300</ref_seq_no>
				<ref_text><![CDATA[Hsu, S. C., and Lee, I. H. H. 1994. Drawing and animation using skeletal strokes. In <i>SIGGRAPH 94 Conference Proceedings</i>, A. Glassner, Ed., 109--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319429</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>301</ref_seq_no>
				<ref_text><![CDATA[Kirby, R. M., Marmanis, H., and Laidlaw, D. H. 1999. Visualizing multivalued data from 2D incompressible flows using concepts from painting. In <i>Proceedings Visualization '99</i>, 333--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288234</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>302</ref_seq_no>
				<ref_text><![CDATA[Laidlaw, D. H., Ahrens, E. T., Kremers, D., Avalos, M. J., Jacobs, R. E., and Readhead, C. 1998. Visualizing diffusion tensor images of the mouse spinal cord. In <i>Proceedings Visualization '98</i>, 127--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808605</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>303</ref_seq_no>
				<ref_text><![CDATA[Lewis, J.-P. 1984. Texture synthesis for digital painting. <i>Computer Graphics (SIGGRAPH 84 Proceedings) 18</i>, 3, 245--252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258893</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>304</ref_seq_no>
				<ref_text><![CDATA[Litwinowicz, P. 1997. Processing images and video for an impressionist effect. In <i>SIGGRAPH 97 Conference Proceedings</i>, T. Whitted, Ed., 407--414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602131</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>305</ref_seq_no>
				<ref_text><![CDATA[Lu, A., Morris, C. J., Ebert, D. S., Rheingans, P., and Hansen, C. 2002. Non-photorealistic volume rendering using stippling techniques. In <i>Proceedings Visualization 2002</i>, 211--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237288</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>306</ref_seq_no>
				<ref_text><![CDATA[Meier, B. J. 1996. Painterly rendering for animation. In <i>SIGGRAPH 96 Conference Proceedings</i>, H. Rushmeier, Ed., 477--484.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>108682</ref_obj_id>
				<ref_obj_pid>108681</ref_obj_pid>
				<ref_seq_no>307</ref_seq_no>
				<ref_text><![CDATA[Pham, B. 1991. Expressive brush strokes. <i>Computer Vision, Graphics and Image Processing 53</i>, 1, 1--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>308</ref_seq_no>
				<ref_text><![CDATA[Ramachandran, V. 2000. The science of art: How the brain responds to beauty. In <i>Understanding wisdom: Sources, science, and society.</i>, W. S. Brown, Ed. Templeton Foundation Press., Philadelphia, PA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614497</ref_obj_id>
				<ref_obj_pid>614283</ref_obj_pid>
				<ref_seq_no>309</ref_seq_no>
				<ref_text><![CDATA[Rheingans, P., and Ebert, D. 2001. Volume illustration: Nonphotorealistic rendering of volume models. <i>IEEE Transactions on Visualization and Computer Graphics 7</i>, 3, 253--264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>310</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Anderson, S. E., Barzel, R., and Salesin, D. H. 1994. Interactive pen-and-ink illustrations. In <i>SIGGRAPH 94 Conference Proceedings</i>, A. S. Glassner, Ed., 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258890</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>311</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Wong, M. T., Hughes, J. F., and Salesin, D. H. 1997. Orientable textures for image-based pen-and-ink illustration. In <i>SIGGRAPH 97 Conference Proceedings</i>, T. Whitted, Ed., 401--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508537</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>312</ref_seq_no>
				<ref_text><![CDATA[Secord, A. 2002. Weighted voronoi stippling. In <i>Proceedings NPAR 2002 Symposium on Non-Photorealistic Animation and Rendering</i>, 37--43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340923</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>313</ref_seq_no>
				<ref_text><![CDATA[Shiraishi, M., and Yamaguchi, Y. 2000. An algorithm for automatic painterly rendering based on local source image approximation. In <i>Proceedings NPAR 2000 Symposium on Non-Photorealistic Animation and Rendering</i>, 53--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826635</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>314</ref_seq_no>
				<ref_text><![CDATA[Stompel, A., Lum, E., and Ma, K.-L. 2002. Feature-enhanced visualization of multidimensional, multivariate volume data using non-photorealistic rendering techniques. In <i>10th Pacific Conference on Computer Graphics and Applications</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15911</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>315</ref_seq_no>
				<ref_text><![CDATA[Strassmann, S. 1986. Hairy brushes. <i>Computer Graphics (SIGGRAPH 86 Proceedings) 20</i>, 4, 185--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>316</ref_seq_no>
				<ref_text><![CDATA[Tateosian, L. G. 2002. <i>Nonphorealistic visualization of multidimensional datasets</i>. Master's thesis, North Carolina State University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601688</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>317</ref_seq_no>
				<ref_text><![CDATA[Walter, J. D., and Healey, C. G. 2001. Attribute preserving dataset simplification. In <i>VIS '01: Proceedings of the conference on Visualization '01</i>, IEEE Computer Society, 113--120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987674</ref_obj_id>
				<ref_obj_pid>987657</ref_obj_pid>
				<ref_seq_no>318</ref_seq_no>
				<ref_text><![CDATA[Wilson, B., and Ma, K.-L. 2004. Rendering complexity in computer-generated pen-and-ink illustrations. In <i>NPAR '04: Proceedings of the 3rd international symposium on Nonphotorealistic animation and rendering</i>, ACM Press, 129--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>319</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. H. 1994. Computer-generated pen-and-ink illustration. In <i>SIGGRAPH 94 Conference Proceedings</i>, A. Glassner, Ed., 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>320</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. H. 1996. Rendering parametric surfaces in pen-and-ink. In <i>SIGGRAPH 96 Conference Proceedings</i>, H. Rushmeier, Ed., 469--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>321</ref_seq_no>
				<ref_text><![CDATA[Wooding, D. 2002. Eye movements of large populations: Ii. deriving regions of interest, coverage, and similarity using fixation maps. <i>Behavior Research Methods, Instruments, and Computers 34</i>, 4, 518--528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perceptually-Motivated Graphics, Visualization and 3D Displays Ann McNamara* Department of Visualization 
Texas A&#38;M University Katerina Mania Department of Electronic &#38; Computer Engineering Technical 
University of Crete Marty Banks Visual Space Perception Laboratory University of California, Berkeley 
 Christopher Healey Department of Computer Science North Carolina State University May 18, 2010 *ann@viz.tamu.edu 
mania@ced.tuc.gr martybanks@berkeley.edu healey@csc.ncsu.edu Abstract This coursepresentstimely, relevantexamples 
onhow researchershaveleveraged perceptualinformationfor optimization ofrendering algorithms, tobetterguide 
designandpresentationin(3D stereoscopic) display media, andforimproved visualization of complex or large 
data sets. Each presentation will provide references and short overviewsof cutting-edge current researchpertaining 
tothat area. We will ensure that the most up-to-date research examples are presented by sourcing information 
from recent perception and graphics conferences and journals suchas ACM Transactions on Perception, paying 
particular attention workpresented atthe2010Symposium onAppliedPerceptioninGraphics and Visualization. 
 About the Lecturers Ann McNamara Department of Visualization Texas A&#38;M University 3137 TAMU College 
Station, TX 77843-3137 +1-979-845-4715 ann@viz.tamu.edu http://www.viz.tamu.edu/people/ann AnnMcNamara 
receivedher undergraduate andgraduatedegreesfrom theUniversity of Bristol, UK. Anns research focuses 
on the advancement of computer graphics and scienti.c visualization through novel approaches for optimizing 
an individuals experience when creating, viewing and interacting with virtual spaces. She investigates 
new ways to exploit knowledge of human visual perception to produce high quality computer graphics and 
animations more e.ciently. Shejoined thefaculty of the newlyformedDepartment ofVisualization at TexasA&#38;M 
University in 2008, where she is currently an assistantprofessor. Ann serves on several IPCs including 
APGV. Katerina Mania Department of Electronic and Computer Engineering Technical University of Crete 
University Campus Kounoupidiana Chania Crete Greece +30 28210 37222 k.mania@ced.tuc.gr http://www.music.tuc.gr/kmania 
KaterinaMania completed aB.Sc. inMathematics,UniversityofCrete,Greece, an M.Sc./Ph.D in Computer Science, 
University of Bristol, UK, funded by HP Labs. She worked at HP Labs as a researcher before serving on 
Faculty in the DepartmentofInformatics,University ofSussex. Katerinaspenthersabbatical at NASA Ames Research 
Centre (Advanced Displays and Spatial Perception Laboratory) in 2003. She is currently an Assistant Professor 
with tenure at the Technical University of Crete, Greece. Katerina is the program co-chair for APGV 2010 
and Associate Editor of ACM Transactions on Applied Perception and Presence Teleoperators and Virtual 
Environments. Marty Banks Marty Banks Lab University of California, Berkeley Vision Science, 360 Minor 
Hall Berkeley, CA 94720-2020 +1-510-642-7679 martybanks@berkeley.edu http://bankslab.berkeley.edu/ MartinS.Banks 
receivedhisBachelorsdegree atOccidentalCollege(1970). AfteroneyearinGermany teaching,he entered thegraduateprograminPsychology 
atUCSanDiego. He receivedaMastersdegreeinExperimentalPsychology (1973). Banks then transferred to the 
graduate program at the University of Minnesota wherehe receivedhis PhD. inDevelopmentalPsychology(1976). 
He was Assistant &#38; Associate Professor of Psychology at the University of Texas atAustin(1976-1985).He 
moved toUCBerkeleySchool ofOptometry in1985 where his is now a Full Professor of Optometry and Vision 
Science. Christopher Healey Department of Computer Science North Carolina State University 890 Oval Drive 
#8206 +1 919.513.8112 healey@csc.ncsu.edu http://www.csc.ncsu.edu/faculty/healey Christopher G. Healey 
received a B.Math from the University of Waterloo in Waterloo, Canada, and an M.Sc. and Ph.D. from the 
University of British Columbia in Vancouver, Canada. Following a postdoctoral fellowship at the University 
of California at Berkeley, he joined the Department of Computer Science at North Carolina State University, 
where he is currently an Associate Professor. His research interests include visualization, graphics, 
visual perception, and areas of applied mathematics, databases, arti.cial intelligence, and aesthetics 
related to visual analysis and data management. 4 Course Overview 5 minutes: Welcome and Introductions 
Ann McNamara Welcome, overview of course&#38; motivationfor attending. SpeakerIntroductions 40 minutes: 
Perceptually Motivated 3D Displays &#38; Depth Perception Martin Banks An overview ofDepthPerception 
andimportantphenomenon whenpresenting information on 3D Displays 40 minutes: Perceptually Motivated Visualization 
Chris Healey A look at Visual Attention, Visual Memory, and its Role in Visualization. 15 minutes: Break 
30 minutes: Perceptually Motivated Rendering Ann McNamara Overview of how knowledge from perceptual research 
feeds into optimized rendering algorithms. 30 minutes: Perceptually Motivated Simulation and Virtual 
Environments Katerina Mania Perceptually-basedOptimizations&#38;FidelityMetricsforSimulationTechnology 
30 minutes:Leading-edge research and APGV 2010 Katerina Mania &#38; Martin Banks A summary of cutting 
edge perceptual research selected from APGV 2010. 10 minutes:A look to the future Ann McNamara &#38; 
Katerina Mania Discussion of trends for APGV 2010 10 minutes: Conclusion, Questions &#38; Answers All 
Wrap up, review, questions and discussion. Contents 1 Introduction 1 1.1 Motivation .............................. 
1 1.2 CourseOverview ........................... 1 1.3 FocusAreas.............................. 1 
1.3.1 Perceptually Motivated 3D Displays &#38; Depth Perception 1 1.3.2 Perceptually Motivated Visualization 
. . . . . . . . . . . . 2 1.3.3 Exploitation of the limitations of the HVS to reduce rendering times.......................... 
2 1.3.4 Explorationofincorporatingperceptual and cognitiveaspectstoVirtualEnvironments(VEs). . . . . 
. . . . . . . 2 1.4 Summary ............................... 2 2 Perceptually Motivated 3D Displays &#38; 
Depth Perception 3 2.1 Introduction.............................. 3 3 Perceptually Motivated Visualization 
37 3.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.2 Visual Attention 
and Preattentive Processing . . . . . . . . . . . 37 3.3 TheoriesofPreattentiveProcessing . . . . . . 
. . . . . . . . . . . 39 3.3.1 FeatureIntegration. . . . . . . . . . . . . . . . . . . . . . 39 3.3.2 
Textons ............................ 39 3.3.3 GuidedSearch ........................ 40 3.3.4 BooleanMaps. 
. . . . . . . . . . . . . . . . . . . . . . . . 41 3.3.5 EnsembleCoding . . . . . . . . . . . . . . . 
. . . . . . . . 42 3.3.6 FeatureHierarchies. . . . . . . . . . . . . . . . . . . . . . 42 3.4 VisualMemory 
............................ 43 3.4.1 ChangeBlindness. . . . . . . . . . . . . . . . . . . . . . . 43 
3.4.2 InattentionalBlindness ................... 43 3.4.3 AttentionalBlink . . . . . . . . . . . . . 
. . . . . . . . . . 45 3.5 Conclusions .............................. 45 4 Perceptually Motivated Rendering 
46 4.1 Visual Perception in Realistic Image Synthesis . . . . . . . . . . 46 4.1.1 VisualPerception. 
. . . . . . . . . . . . . . . . . . . . . . 46 4.1.2 TheHumanVisualSystem ................. 46 4.1.3 
Contrast . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.1.4 Constancy . . . . . . . . . 
. . . . . . . . . . . . . . . . . . 49 4.1.5 HumanVisualPerception .................. 49 4.1.6 LightnessPerception. 
. . . . . . . . . . . . . . . . . . . . 51 4.2 Perceptually drivenrendering .................... 52 
4.3 Conclusion .............................. 57 6 5 Perceptually Motivated Simulation and Virtual Environments 
93 5.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 5.2 Perceptually-based 
Selective Rendering . . . . . . . . . . . . . . . 93 5.3 Behavioral Fidelity of Simulations based on 
Space Memory . . . 98 5.4 Investigating Perceptual Sensitivity to Head Tracking Latency. . 102 6 Trends 
from the 7th annual ACM/Eurographics Symposium on Applied Perception in Graphics and Visualization 2010 
107 6.1 APGVProceedingsandResources . . . . . . . . . . . . . . . . . 107   1 Introduction 1.1 Motivation 
The (re) introduction of 3D cinema, advent of a.ordable stereoscopic display technology, and seamlessintegration 
of real-world scenes with computergraphics fuels our continuing ability to create and display stunning 
realistic imagery. With the arrival of new technology, algorithms and display methods comes the realization 
thatgains canbe madeby tailoring output to theintended audience; humans. Human beings have an amazingly 
complex perceptual systems, which have the ability to quickly capture and process vast amounts of complex 
data. With all its capabilityhowever, the Human VisualSystem(HVS) has some surprisingnuances andlimitations 
that canbe exploited to thebene.t of numerous graphics applications. This new tutorial will provide insight 
into those aspects of the HVS and other perceptual systems that can serve as both a guide and yard-stick 
to further the development and evaluation of computer graphics imagery and presentations. The literature 
on perception provides a rich source of knowledge that can be applied to the realm of computergraphicsfor 
immediate and direct bene.t, generating images that not only exhibit higher quality, but useless time 
and resources toprocess. In addition,knowledge of theHVS serves as a guide on how best to present the 
images to ful.ll the application at hand. 1.2 Course Overview We will present timely, relevant examples 
on how researchers have leveraged perceptualinformationfor optimization ofrendering algorithms, tobetterguide 
designandpresentationin(3D stereoscopic) display media, andforimproved visualization of complex orlargedata 
sets. Each section willprovide references and short overviews of cutting-edge current researchpertainingto 
that area. We will ensurethat the most up-to-date research examples arepresentedby sourcing informationfrom 
recentperception andgraphics conferences andjournals such asACMTransactions onPerception,payingparticular 
attention workpresented at the 2010 Symposium on Applied Perception in Graphics and Visualization. 1.3 
Focus Areas We will focus on four key areas in which perceptual knowledge has been successfully interleaved 
with computer graphics. 1.3.1 Perceptually Motivated 3D Displays &#38; Depth Perception 3D stereoscopic 
displays are being used in a wide range of .elds. To understand how better to present information on 
such displays, a comprehensive understanding of depth perception is necessary. This area will focus on 
depth perception and applications of such toimagepresentation. 1 1.3.2 Perceptually Motivated Visualization 
Discussion of recent research pertaining to psychophysics and application to scienti.c and information 
visualization. A closer look at visual attention and visual memory will provide the framework for steering 
perceptually informed visualizations. 1.3.3 Exploitation of the limitations of the HVS to reduce rendering 
times while improving resulting image quality. This includes real-time and non-real time graphics, image 
quality metrics and high dynamic range imagery. 1.3.4 Exploration of incorporating perceptual and cognitive 
aspects to Virtual Environments (VEs). Such principles could be applied to selective real-time rendering 
algorithms, positive transfer of training as well as to optimizations for latency degradations and predictive 
tracking.  1.4 Summary In summary, this course represents a whirlwind tour of insights into how the 
eye and brain capture and process visual information through our perceptual systems, and how we can use 
those insights to further advance many areas in computer graphics. 2 Martin S. Banks  2 Perceptually 
Motivated 3D Displays &#38; Depth Perception 2.1 Introduction The human visual system has evolved in 
an environment with constrained relationships between objects and retinal images. That relationship 
is often altered in stereoscopicdisplays, so itisimportant to understand the situationsin which the alteration 
is insigni.cant and the situations in which it causes undesirable perceptual or ergonomic e.ects. This 
section will review the current literature on visual perception and human ergonomics in the context of 
the viewing of stereo displays. The literature shows that stereo displays can be associated with viewer 
fatigue/discomfort, reduced visual performance, and distorted 3D perception. Thissectionwill alsodiscusswaysto 
minimizetheseadverseviewer e.ects. 3 Martin S. Banks What should we know about Human Depth Perception 
in Constructing 3D Displays?  Excitement about Stereo Display Applications Entertainment stereo cinema 
video games Medical Imaging diagnosis surgery instruction Scientific Visualization geology molecular 
biology Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? 
 Problems with Using Stereo Displays Technical Issues Developpging content  Sufficient resolution 
over time: temporal aliasing  Sufficient separation between two eyes images: ghosting  User Issues 
 Percepptual distortions due to incorrect viewing position gp  Vergence-accommodation conflict: distortion, 
fatigue  Treatment of blur  Problems with Using Stereo Displays Technical Issues Developpging content 
 Sufficient resolution over time: temporal aliasing  Sufficient separation between two eyes images: 
ghosting  User Issues Percepptual distortions due to incorrect viewing position gp  Vergence-accommodation 
conflict: distortion, fatigue  Treatment of blur  Martin S. Banks What should we know about Human Depth 
Perception in Constructing 3D Displays? Viewing Pictures Almost never view pictures Almost never view 
pictures from correct position. Retinal image thus specifies different scene than depicted. Do people 
compensate, and if so how? so, how?  Martin S. Banks What should we know about Human Depth Perception 
in Constructing 3D Displays? Experimental Task Stimulus: simulated 3D ovoid with variable aspect ratio. 
Task: adjust ovoid until appears spherical. Vary monitor slant Smto assess compensation for oblique 
viewing positions.  Sm If compensate, will set ovoid to sphere on screen (ellipse on retina). Observation 
Point  Vishwanath, Girshick, &#38; Banks (2005), Nature Neuroscience. Predictions  Martin S. Banks 
What should we know about Human Depth Perception in Constructing 3D Displays? Predictions  Martin 
S. Banks What should we know about Human Depth Perception in Constructing 3D Displays?  Compensation 
for Incorrect Viewing Position Pictures not useful unless percepts are robust to changes in ie ing position 
 viewing position.  People compensate for oblique viewing position when viewing 2d pictures.  Two theories 
of compensation: pictorial &#38; surface. Data clearly favor surface compensation.  Two versions of 
surface method: global &#38; local. Data clearly favor local slant.  Martin S. Banks What should we 
know about Human Depth Perception in Constructing 3D Displays? 2D Pictures vs Stereo Pictures  Two 
eyes presented same Two eyes presented different imagge imagges  Binocular disparities specify Binocular 
disparities specify orientation &#38; distance of picture orientation &#38; distance of picture surface; 
hence useful for surface and layout of picture compensation contents; hence not useful for  compensation 
 Stereo Pictures For most applications, viewers will not be at correct position.  Retinal disparities 
thus specify a different layout than depicted.  Do people comppensate? pp  Is correct seating position 
for a 3D movie more important than for 2D movie?  Martin S. Banks What should we know about Human Depth 
Perception in Constructing 3D Displays?  Stereo Picture Geometry  Predictions Hingge Setting (deg) 
 120 90 60 30 30 0 Viewing Angle (deg) Invariance: hinge settings are 90 for all viewing angles and 
base slants Retinal disparity: hinge settings vary signifiicantly with viewing angle &#38; base slant 
angle &#38; base slant Martin S. Banks What should we know about Human Depth Perception in Constructing 
3D Displays?  Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? 
  Where to Sit with Stereo Cinema?  Martin S. Banks What should we know about Human Depth Perception 
in Constructing 3D Displays? Problems with Using Stereo Displays Technical Issues Developpging content 
 Sufficient resolution over time: temporal aliasing  Sufficient separation between two eyes images: 
ghosting  User Issues Percepptual distortions due to incorrect viewing position gp  Vergence-accommodation 
conflict: distortion, fatigue  Treatment of blur  Martin S. Banks What should we know about Human 
Depth Perception in Constructing 3D Displays? Consequences of Vergence-accommodation Conflicts Does 
accommodation affect 3d shape perception?  Is vergence-accommodation conflict the cause of discomfort 
&#38; fatigue in viewing stereo displays?  Martin S. Banks What should we know about Human Depth Perception 
in Constructing 3D Displays? Displays with Nearly Correct Focus Cues Two multi-focal displays we ve 
developed: 11.FiFixedd-viiewpoiint, vollumetriic di displlay wi h ith miirror system &#38; 3 f &#38; 
3 focall planes (Akeley, Watt, Girshick, &#38; Banks, SIGGRAPH2004). 2.Fixed-viewpoint, volumetric display 
with switchable lens &#38; 4 focal planes (Love, Hoffman, Kirby, Hands, Gao, &#38;Banks,Optics Express, 
2009) Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? 
 Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? Vergence-accommodation 
Conflict &#38; Perceived Shape For estimating 3d shape, horizontal disparities must be scaled by an 
estimate of fixation distance  Information about fixation distance is available from vergenceAND accommodation 
  Watt, Ernst, Akeley, &#38; Banks (2005), Journal of Vision Martin S. Banks What should we know about 
Human Depth Perception in Constructing 3D Displays? Vergence-accommodation Conflict &#38; Perceived Shape 
 Vergenceand focal  Vergenceand focal distance affect estimated distance used to scale disparity  
Thus focal distance affects perceived 3d shape  shape Watt, Ernst, Akeley, &#38; Banks (2005), Journal 
of Vision  Problems with Using Stereo Displays Technical Issues Developpging content  Sufficient resolution 
over time: temporal aliasing  Sufficient separation between two eyes images: ghosting  User Issues 
 Percepptual distortions due to incorrect viewing position gp  Vergence-accommodation conflict: distortion, 
fatigue  Treatment of blur  Martin S. Banks What should we know about Human Depth Perception in Constructing 
3D Displays?  Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? 
 Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays?  Problems 
with Using Stereo Displays Technical Issues Developpging content Sufficient resolution over time: temporal 
aliasing  Sufficient separation between two eyes images: ghosting  User Issues Perceptual distortions 
due to incorrect viewing position  Vergence-accommodation conflict: distortion, fatigue  Treatment 
of blur Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? 
 Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays?  Martin 
S. Banks What should we know about Human Depth Perception in Constructing 3D Displays?  Martin S. Banks 
What should we know about Human Depth Perception in Constructing 3D Displays? Distance Information from 
Blur  z0= As0(1 - d1) c1   Solve for absolute distance (z0) given blur, aperture, &#38; relative distance 
(d) Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? Estimating 
Relative Distance from Perspective  Grid lines placed on image to determine vanishing points  Estimate 
local slant from linear perspective  Calculate relative distances  Martin S. Banks What should we 
know about Human Depth Perception in Constructing 3D Displays?  Probabilistic Model  By combining information 
from blur &#38; perspective, can estimate absolute distance &#38; therefore absolute size Held, Cooper, 
O Brien, &#38; Banks, TOG, 2009 Estimating Absolute Distance  Martin S. Banks What should we know 
about Human Depth Perception in Constructing 3D Displays? Estimating Absolute Distance    Martin 
S. Banks What should we know about Human Depth Perception in Constructing 3D Displays?  Martin S. Banks 
What should we know about Human Depth Perception in Constructing 3D Displays? Psychophysical Experiment 
7 scenes from GoogleEarth Each scene rendered 4 ways: no blur, blur consistent with distance, blur &#38; 
distance gradients aligned, blur &#38; distance gradients orthogonal 5 blur magnitudes Nave subjects 
viewed each image monocularly for 3 sec  Reported distance from marked building in image center to the 
camera that produced the image  camera that produced the image 7 repetitions, random order Martin S. 
Banks What should we know about Human Depth Perception in Constructing 3D Displays? Disparity Geometry 
 Martin S. Banks What should we know about Human Depth Perception in Constructing 3D Displays? Blur Geometry 
 c1= |A(s0/z0)(1-(z0/z1))| where c1= |A(s0/z0)(1-(1/d))| where Geometries of Disparity &#38; Blur 
Comparing disparity &#38; blur: Martin S. Banks What should we know about Human Depth Perception in Constructing 
3D Displays? Summary Various user issues associated with stereo displays Comppensation for incorrect 
viewing position is much less gp complete with stereo pictures than with conventional pictures Blur &#38; 
accommodation matter to depth perception Accommodation affects disparity scaling and hence perceived 
3d shape Vergence-accommodation conflicts cause visual discomfort/fatigue Blur in combination with other 
pictorial cues can be effective cue to absolute distance Blur and disparity have similar geometries and 
may provide complimentary depth information in natural viewing Martin S. Banks What should we know about 
Human Depth Perception in Constructing 3D Displays?   Depth-weighted Blending  Depth-weighted blending 
along lines of sight  Weights dependent on  dioptric distances to planes Akeley, Watt, Girshick, &#38; 
Banks (2004), SIGGRAPH. Martin S. Banks What should we know about Human Depth Perception in Constructing 
3D Displays? Switchable Lens &#38; Multi-plane Display Birefringent material has ordinary &#38; extra 
ordinary ordinary &#38; extra-ordinary refractive indices depending on polarization Calcite lens has 
two focal lengths Fo&#38;Fe Variation in power effected by polarization modulator (FLC) (FLC) Can stack 
lenses; 2N states Love, Hoffman, Hands, Kirby,Gao, &#38;Banks, Optics Express, 2009 Christopher Healey 
3 Perceptually Motivated Visualization 3.1 Introduction A fundamental goal of visualization is to produce 
images of data that support visual analysis, exploration and discovery, and identifying novel insights. 
An important consideration during visualization design is the role of human visual perception[79,111,114,125]. 
Howwe see detailsinanimagecandirectly impact a user s e.ciency and e.ectiveness. This article surveys 
research on attentionand visualperception, with aspeci.cfocusonresultsthathavedirect relevance to visualization 
and visual analytics. We discuss theories of low-level visual perception, then show how these .ndings 
form a foundation for more recent work on visual memory and visual attention. 3.2 Visual Attention and 
Preattentive Processing For many years vision researchers have been investigating how the human visual 
system analyzes images. An important initial result was the discovery of a limited set of visual properties 
that are detected very rapidly by low-level and fast-acting visual processes. These properties were initially 
called preattentive, since their detection seemed to precede focused attention. We now know that attention 
plays a critical role in what we see, even at this early stage of vision. The term preattentive continues 
to be used, however, since it conveys an intuitive notion of the speed and ease with which theseproperties 
areidenti.ed. Typically, tasks thatcanbeperformed onlarge multi-elementdisplaysinless than200 250 milliseconds(msec) 
are consideredpreattentive. Eye movements take at least 200 msec to initiate, and random locations of 
the elements in the display ensure that attention cannot be prefocused on any particular location, yetviewers 
report that these tasks canbe completed with verylittle e.ort. This suggeststhatcertaininformationinthedisplayis 
seen inparallelbylow-level visual processes. A simple example of a preattentive task is the detection 
of a red circle in a group ofblue circles(Fig.1). Thetarget objecthas a visualproperty redthat the blue 
distractor objects do not. A viewer can tell at a glance whether the targetispresent orabsent. Herethe 
visual systemidenti.esthetargetthrough a di.erence in hue, speci.cally, a red target in a sea of blue 
distractors. Hue is not the only visualfeature thatispreattentive. For example, viewers canjust as easily 
.nd a red circle in a background of red squares. Here, the visual system identi.esthetargetthrough adi.erencein 
curvature(orform). A unique visualproperty inthetarget a redhue or a curvedform allows it to pop out 
of a display. A conjunction target made up of a combination of non-unique features normally cannot be 
detected preattentively. For example, consider combiningthe twobackgrounds and searchingfor a red circlein 
a sea of bluecirclesandred squares.Thered circletargetismadeup oftwofeatures: red (a) (b) (c) (d) 
(e) (f)  Figure1: Targetdetection:(a) huetarget red circle absent;(b) targetpresent; (c)shapetarget 
red circle absent;(d) targetpresent;(e) conjunctiontargetred circlepresent;(f) target absent and circular. 
Oneof thesefeaturesispresentin each of thedistractorobjects red squares and blue circles. The visual 
system has no unique visual property to search for when trying to locate the target. A search for red 
items always returns true because there are red squares in each display. Similarly, a search for circular 
items always sees blue circles. Numerous studies have shown that a conjunction target cannot be detected 
preattentively. Viewers must perform a time-consuming serial search through the display to con.rm its 
presence or absence. If low-level visual processes can be harnessed during visualization, it can draw 
attention to areas of potential interest in a display. This cannot be accomplished in an ad-hoc fashion, 
however. The visual features assigned to different data attributes the data-feature mapping must take 
advantage of the strengths of our visual system, must be well-suited to the analysis needs of theviewer,andmustnotproducevisualinterferencee.ects(e.g., 
conjunction search) that could mask information. 3.3 Theories of Preattentive Processing Anumber of 
theorieshavebeenproposedto explainhowpreattentiveprocessing occurs within the visual system: feature 
integration, textons, guided search, and boolean maps. We provide an overview of these theories, then 
discuss brie.y feature hierarchies, which describes situations where the visual system favors certain 
visual features over others, and ensemble coding, which shows that viewers can generate summaries of 
the distribution of visual features in a scene, even when they are unable to locate individual elements 
based those same features. 3.3.1 Feature Integration Anne Treisman was one of the original researchers 
to document the area of preattentive processing [117, 115, 116]. In order to explain the phenomena, Treismanproposed 
a modellow-levelhuman vision made up of a set offeature maps and a master map of locations. Each feature 
map registers activity for a speci.c visual feature. Treisman suggested a manageable number of feature 
maps, including one for each of the opponent colors, as well as separate maps for orientation, shape, 
and texture. When the visual system .rst sees an image, all the features are encoded in parallel into 
their respective maps. A viewer can access aparticular map to checkfor activity, andperhapstodeterminethe 
amount of activity. The individual feature maps give no information about location, spatial arrangement, 
or relationships to activityin other maps, however. 3.3.2 Textons Bela Julesz was also instrumental 
in expanding our understanding of what we see in an image. Jullesz initially focused on statistical 
analysis of texture patterns[55,56,57,58,59]. Hisgoal wastodeterminewhether variationsin a particular 
order statistic were detected by the low-level visual system, for example contrast a .rst-order statistic 
orientation and regularity a secondorder statistic and curvature a third-order statistic. Based on these 
.ndings, Julesz suggested that the early visual system detects a group of features called textons, which 
fall into three general categories: 1. Elongatedblobs linesegments,rectangles,orellipses withspeci.cproperties 
of hue, orientation, width, and so on. 2. Terminators ends of line segments.  (a) (b) (c) Figure2: 
Textons: (a,b) two textons A and B that appeardi.erentin isolation, but have the same size, number of 
terminators, and join points; (c) a target group of B-textons is di.cult to detect in a background of 
A-textons when random rotation is applied 3. Crossings of line segments Julesz believed that only a 
di.erence in textons or in their density could be detectedpreattentively(Fig.2). Nopositionalinformationaboutneighboring 
textons is available without focused attention. Like Treisman, Julesz suggested that preattentive processing 
occurs in parallel and focused attention occurs in serial. 3.3.3 Guided Search Morerecently,JeremyWolfehasproposedatheorythathecalls 
guided search. He hypothesized that an activation map based on both bottom-up and topdown information 
is constructed during visual search. Attention is drawn to peaks in the activation map that represent 
areas in the image with the largest combination ofbottom-up and top-downin.uence[131,132,130]. As with 
Treisman, Wolfe believes early vision divides an image into individual feature maps. In his theory, 
there is one map for each feature type a color map, an orientation map, and so on. Within each map a 
feature is .ltered into multiple categories. Bottom-up activationfollowsfeature categorization. It measures 
how di.erent an element is from its neighbors. Top-down activation is a user-driven attempt to .nd items 
with a speci.c property or set of properties. The activation map is a combination of bottom-up and top-down 
activity. Hills in the activation map mark regions that generate relatively large amount of bottom-up 
or top-down in.uence, but without providing information about the source of a hill. A subject s attention 
is drawn from hill to hill in order of decreasing activation.  (b) (c) (d) Figure 3: Conjunction search 
with boolean maps: (a b) blue horizontal target, select blue objects,thensearch withinforahorizontal 
target,presentin(a), absentin(b);(c d) red vertical target,select red objects,thensearch within for a 
vertical target, absentin(c),presentin(d) 3.3.4 Boolean Maps A more recent model of low-level vision 
has been presented by Huang et al. [52, 53]. This theory carefully divides visual search into two parts: 
selection and access. Selection involves choosing a set of objects from a scene. Access determines what 
properties of the selected objects a viewer can apprehend. Although both operations are implicitly present 
in previous theories, they are often described as a whole and not as separate steps. Huang et al. suggest 
that the visual system can divide a scene into exactly twoparts:selected elementsand excluded elements. 
Thisisthe booleanmap that underlies their theory. The visualsystem can then access certainproperties 
of the selected elements in the map. Once a boolean map is created, two properties are available to 
a viewer: the label for any feature in the map, and the spatiallocation of the selected elements(Fig.3). 
Boolean maps canbe created in two ways. First, a viewer can specify a single value of an individual feature 
to select all objects that contain that feature. Second, union or intersection can be applied to two 
existing maps. In either case, only the resultis retained, since evidence suggests that a viewer can 
only hold and access one boolean map at a time. Viewers can chain these operations together to search 
for targets in a fairly complex scene. 3.3.5 Ensemble Coding Existing characterizationsofpreattentive 
visionhavefocused onhowlowlevelvisual processes can be used to guide attention to speci.c location or 
object in alargerscene. An equally important characteristicoflow-level visualprocesses is their ability 
to generate a quick summary of how simple visual features are distributedacrossthe .eld of view. Theabilityofhumanstoregisterarapid 
and in-parallel summary of a scene in terms of its simple features was .rst reported byAriely[4]. Hedemonstratedthat 
observers could extract the average size of a large number ofdotsfrom onlya singleglimpse at adisplay. 
Yet, when observers were tested on the same displays and asked to indicate whether a single dot of a 
given size was present, they were unable to do so. This suggests that there is a preattentive mechanism 
that records summary statistics of visual features without retaining information about the constituent 
elements that generated the summary. This ability to rapidly identify scene-based averages may o.er important 
advantages in certain visualization environments. For example, given a stream of real-timedata, ensemble 
coding would allow viewersto observethe stream at a highframe rate,yetstillidentifyindividualframes withinterestingdistributions 
of visualfeatures(i.e. attributevalues). Ensemblecoding would alsobecritical for anysituation where viewers 
want to estimate the amount of aparticulardata attribute in a display. These capabilities were hinted 
at in a paper by Healey et al., but without the bene.t of ensemble coding as a possible explanation. 
 3.3.6 Feature Hierarchies One promising strategy for multidimensional visualization is to assign di.erent 
visual features to di.erent data attributes. This allows multiple data values to be shown simultaneouslyin 
a singleimage. Akeyrequirement ofthis methodis a data-feature mappingthatdoes notproduce visualinterference. 
One example of interferenceis a conjunctiontarget. Another exampleisthepresence offeature hierarchies 
that appears to exist in the visual system. For certain tasks one visualfeaturemaybe moresalient thananother. 
Researchesinpsychophysics and visualization have demonstrated a hue-shape hierarchy: the visual system 
favors color over shape[15,16,17,48,49]. Backgroundvariationsinhueinterfere with a viewer s ability to 
identify the presence of individual shapes and the spatial patterns they form. If hue is held constant 
across the display, these same shape patterns are immediately visible. The interference is asymmetric: 
random variations in shape have no e.ect on a viewer s ability to see color patterns. Similar luminance-hue 
and hue-texture hierarchies have also been identi.ed.  3.4 Visual Memory Preattentive processing asks 
in part: What visual properties draw our eyes, and therefore our focus of attention to a particular object 
in a scene? An equally interesting question is: What do we remember about an object or a scene when we 
stop attending to it and look at something else? Many viewers assume that as we look around us we are 
constructing a high-resolution, fully detailed description of what we see. Researchers in psychophysics 
have known for some time that this is not true. In fact, in many cases our memoryfordetail between glances 
at a scene is very limited. Evidence suggests that a viewer s current state of mind can play a critical 
role in determining what is seen and what is not. We present three theories that demonstrate and attempt 
to explain this phenomena: change blindness, inattentional blindness, and attentional blink. Understanding 
what we remember as we focus on di.erent parts of a visualization is critical to designing visualizations 
that encourage locating and retaining the information that is most important to the viewer. 3.4.1 Change 
Blindness New research in psychophysics has shown that an interruption in what is being seen a blink, 
an eye saccade, or a blank screen renders us blind to signi.cantchangesthat occurinthe sceneduring theinterruption[104,69,105,110]. 
This changeblindnessphenomena canbeillustrated using atask similarto one shown in comic strips for many 
years. A viewer is shown two pairs of images. A number of signi.cant di.erences exists between the images. 
Many viewers have a di.cult time seeing any di.erence and often have to be coached to look carefully 
to .nd it. Once they discover it, they realize that the di.erence was not a subtle one. Change blindness 
is not a failure to see because of limited visual acuity; rather,itisafailurebased oninappropriateattentionalguidance. 
Someparts of the eye and thebrain are clearly respondingdi.erently to the two pictures. Yet, this does 
not becomepart of our visual experience until attention is focused directly on the objects that vary. 
The presence of change blindness has important implications for visualization. The images weproduce 
are normally novelfor our viewers, soprior expectations cannot be used to guide their analyses. Instead, 
we strive to direct the eye, and therefore the mind, to areas of interest or importance within a visualization. 
This ability forms the .rst step towards enabling a viewer to abstract details that will persist over 
subsequent images. 3.4.2 Inattentional Blindness A related phenomena called inattentional blindness 
suggests that viewers fail to perceive objects or activities that occur outside of the focus of attention 
[69]. Thisphenomenaisillustratedthrough an experiment conductedbyNeisser [90,109]. His experiment superimposed 
video streams of twobasketballgames. Players wore white shirts in one stream and black shirts in the 
other. Subjects (a) (b) Figure 4: Change blindness, a major di.erence exists between the two images 
attended to one team either white orblack andignored the other. Whenever the subject s team made a pass, 
they were told to press a key. After about 30 seconds of video, a third stream was superimposed showing 
a woman walking through the scene with an open umbrella. The stream was visible for about 4 seconds, 
after which another 25 seconds of basketball video was shown. Followingthe trial, onlysix oftwenty-eight 
naive observers reported seeing the woman. When subjectsonly watched the screenanddid not countpasses,100% 
noticed the woman. Additional issues with relevance to visualization are also being investigated. Mostet 
al. arestudyingtherelationshipbetweeninattentionalblindnessand attentional capture, the ability of an 
object todraw thefocus of attention without a viewer s active participation. Researchers are also studying 
how perceptual load a.ects inattentional blindness. Finally, results suggest meaningful objects (e.g., 
a person s name or a happyface icon) may be easier to notice. 3.4.3 Attentional Blink In each of the 
previous methods for studying visual attention, the primary emphasisis onhowhuman attentionislimited 
inits abilityto represent thedetails of a scene(changeblindness) andinits ability to represent multiple 
objects at the same time(inattentionalblindness). But attentionis also severelylimited in its ability 
to process information that arrives in quick succession, even when thatinformationispresented at a singlelocationin 
space. The attentionalblink paradigm is currently the most widely used method to study the availability 
of attention across time. Its name blink derives from the .nding that when two targets are presented 
in rapid succession, the second of the two targets cannotbedetected oridenti.ed whenit appearswithin 
approximately100 500 msecfollowing the .rsttarget[14,101]. Thissuggeststhatthatattentionoperates over 
time like a window or gate, opening in response to .nding a visual item that matches its current criterion 
or template and then closing shortly thereafter to consolidate that item as a distinct object or event 
from others. The attentional blink is an index of the dwell-time needed to consolidate a rapidly presented 
visual item into visual short term memory.  3.5 Conclusions This presentation surveys past and current 
theories of low-level visual perception and visual attention. Initial workinpreattentiveprocessingidenti.edbasic 
visual features that can implicitly or explicitly capture a viewer s focus of attention. More recent 
work has extended this to study limited visual memory for change change blindness and attentional blink 
and being blind to objects that are outside the focus of attention inattentional blindness. Each of 
these phenomena have signi.cant consequences for visualization. We strive to produce images that are 
salient and memorable, and that guide attention to locations of importance within the data. Understanding 
what the visual seems sees and does not see is critical to designing e.ective visual displays. Ann McNamara 
 4 Perceptually Motivated Rendering 4.1 Visual Perception in Realistic Image Synthesis Realismis often 
aprimarygoalin computergraphicsimagery. We strive to create images that are perceptually indistinguishable 
from an actual scene. Rendering systems can now closely approximate the physical distribution of light 
in an environment. However, physical accuracy does not guarantee that the displayed images will have 
an authentic visual appearance. In recent years the emphasis in realistic image synthesis has begun to 
shift from the simulation of lightin an environmenttoimagesthatlook asreal asthephysical environment 
theyportray. In other wordsthe computerimage shouldbe not onlyphysically correctbut alsoperceptually 
equivalenttothe sceneit represents. Thisimplies aspects of the Human Visual System (HVS) must be considered 
if realism is required. Visual perception is employed in many di.erent guises in graphics to achieve 
authenticity[92,7]. Certain aspects oftheHVS mustbe considered to identify the perceptual e.ects that 
a realistic rendering system must achieve in order to e.ectively reproduce a similar visual response 
to a real scene. This section outlines the main characteristics of the HVS and the manner in which knowledge 
about visual perception is increasingly appearing in state-of-the-art realistic image synthesis. Perception 
driven rendering algorithms are described, which focus on embedding models of the HVS directly into global 
illumination computations in order to improve their e.ciency. 4.1.1 Visual Perception Perception is the 
process by which humans, and other organisms, interpret and organize sensation in order to understand 
their surrounding environment. Sensation refers to the immediate, relatively unprocessed result of stimulation 
of sensory receptors. Perception, on the other hand, is used to describe the ultimate experience and 
interpretation of the world and usuallyinvolvesfurther processing of sensory input. Sensory organs translate 
physical energy from the environmentinto electricalimpulsesprocessedby thebrain. In the case ofvision 
light,in theform of electromagnetic radiation, activates receptor cellsin the eye triggering signals 
to thebrain. These signals are not understood aspure energy, rather, perception allows them to be interpreted 
as objects, events, people and situations. 4.1.2 The Human Visual System Vision is a complex process 
that requires numerous components of the human eye andbrainto worktogether. Visionisde.ned asthe abilityto 
see thefeatures of objects we look at, such as color, shape, size, details, depth, and contrast. Visionbegins 
withlight raysbouncing o. the surface of objects. These re.ected light rays enter the eye and are transformed 
into electrical signals. Millions of signals per second leave the eye via the optic nerve and travel 
to the visual area of the brain. Brain cells then decode the signals providing us with sight. The response 
of the human eye to light is a complex, still not well understood process. It is di.cult to quantify 
due to the high level of interaction between the visual system and complex brain functions. A sketch 
of the anatomical components of the human eye is shown in Figure 5 The main structures are the iris, 
lens, pupil, cornea, retina, vitreous humor, optic disk and optic nerve. Figure 5: Cross section of 
the human eye The path of light through the visual system begins at the pupil, is focused by the lens, 
then passes onto the retina, which covers the back surface of the eye. The retina is a mesh of photoreceptors, 
which receive light and pass the stimulus on to the brain. The internal structure of the human eye, a 
sphere, typically 12mm in radius, is enclosed by a protective membrane, the sclera. At the front of the 
sclera lies the cornea, a protruding opening, and an optical system comprising the lens and ciliary muscles 
which change the shape of the lensproviding variablefocus. Light enterstheeyethough thelensandproceeds 
through the vitreoushumor, a transparent substance, to the rear wall of the eye, the retina. The retinahasphotoreceptors 
coupled to nerve cells, whichintercept incoming photons and output neural signals. These signals are 
transmitted to the brain through the optic nerve, connected to the retina at the optic disk or papilla, 
more commonly known as the blind spot. The retina is composed of two major classes of receptor cells 
known as rods and cones. The rods are extremely sensitive to light and provide achromatic vision at low 
(scotopic) levels of illumination. The cones are less sensitive than the rods but provide colorvisionathigh(photopic) 
levelsofillumination. Aschematicdrawing of rod and cone cells is shown in Figure 5. Cones are nerve cells 
that are sensitive to light, detail, and color. Millions of cone cells are packed into the macula, aiding 
it in providing the visual detail needed to scan the letters on an eye chart, see a street sign, or read 
the wordsin a newspaper. Rods aredesignedfor night vision. They alsoprovideperipheral vision,but theydo 
not see as acutely as cones. Rods areinsensitive to color. When apersonpassesfrom abrightlylit place 
to one that is dimly illuminated, such as entering a movie theatre during theday, theinterior seems verydark. 
After some minutes thisimpressionpasses and vision becomes more distinct. In this period of adaptation 
to the dark, the eye becomes almost entirely dependent on the rods for vision, which operate best at 
very low light levels. Since the rods do not distinguish color, vision in dim light is almost colorless. 
Cones provide both luminance and color vision in daylight. They contain threedi.erentpigments, which 
respond either toblue, red, orgreen wavelengths of light. A person who is missing one or more of the 
pigments is said to be color-blind and has di.culty distinguishing between certain colors, such as red 
fromgreen. Thesephotoreceptorcells areconnected to each otherand theganglion cells which transmit signals 
to and from the optic nerve. Connections are achieved via two layers, the .rst and second synaptic layers. 
The interconnections between the rods and cones are mainly horizontal links, indicating a preferentialprocessing 
of signalsinthehorizontalplane. Normal daytime vision, where the cones predominate visual processing, 
is termedphotopic, whereaslowlightlevelswheretherodsareprincipally responsible for perception is termed 
scotopic vision. When both rods and cones are equally involved then vision is termed mesopic. Visual 
acuity is the ability of the Human Visual System (HVS) to resolve detail in an image. The human eye is 
less sensitive to gradual and sudden changes in brightness in the image plane but has higher sensitivity 
to intermediate changes. Acuity decreases with increase in distance. Visual acuity can be measured using 
a Snellen Chart, a standardized chart of symbols and letters. Visual .eld indicates the ability of each 
eye to perceive objects to the side of the central area of vision. A normal .eld of vision is 180 degrees 
. 4.1.3 Contrast Contrast is de.ned as: lmax - lmin lmax = lmax + lmin (1) where lmax and lmin are the 
maximum and minimum luminance. Human brightness sensitivity is logarithmic, so it follows that for the 
same perception, higher brightness requires higher contrast. Apparent brightness is dependent on background 
brightness. This phenomenon, termed simultaneous contrast, is illustrated in Figure 6. Despite the fact 
that all centre squares are the samebrightness,they areperceived asdi.erentduetothedi.erentbackground 
brightness. Depth Perception is the ability to see the world in three dimensions and to perceive distance. 
Images projected onto the retina are two-dimensional, and from these .at images vivid three dimensional 
worlds are constructed. Binocular Disparity and monocular cues provide information for depth perception. 
Binoculardisparity isthedi.erencebetweentheimagesprojected ontotheleft and right eye. The brain integrates 
these two images into a single three dimensionalimage to allowdepth anddistanceperception. Monocular 
cues are cues to depth that are e.ective when viewed with only one eye, including interposition, Figure6: 
SimultaneousContrast. Despite thefact thatall centre squares arethe samebrightness,they areperceived 
asdi.erentduetothedi.erentbackground brightness. atmospheric perspective, texture gradient, linear perspective, 
size cues, height cues and motion parallax. 4.1.4 Constancy PerceptualConstancyis aphenomenon whichenables 
the sameperception of an object despite changes in the actual pattern of light falling on the retina. 
Psychologistshaveidenti.ed a number ofperceptual constanciesincludinglightness constancy, color constancy, 
size constancy and shape constancy. LightnessConstancy: The term lightness constancy describes the ability 
of the visual system to perceive surface lightness correctly despite changes in the level of illumination. 
ColorConstancy: Closelyrelatedtolightness constancy, thisis the ability of the HVS to perceive the correct 
color of an object despite changes in illumination. Shape Constancy: Objects are perceived as having 
the same shape regardless of changes in their orientation. -example with cube, from front and side Size 
Constancy: This is the tendency to perceive objects as staying the same size despite changes in viewing 
distance. 4.1.5 Human Visual Perception A number ofpsychophysical experimental studies havedemonstrated 
many features ofhow the HVS works. However,problems arise when trying togeneralize these results for 
use in computer graphics. This is because, often, experiments are conducted underlimited laboratory conditions 
and are typicallydesigned to explore a single dimension of the HVS. As described earlier, the HVS comprises 
complex mechanisms, which rather than working independently, often features work together, and therefore 
it makes sense to examine the HVS as a whole. Instead of reusing information from previous psychophysical 
experiments, new experiments are needed. Some examples will support this. Figure 2.6: When a black and 
white patterned top shown on the left is rotated at 5-10 revolutions per second, colored rings can be 
seen. The light intensity distribution of the rotating pattern as a function of time is shown on the 
right. Spatiotemporal interactions between antagonistic, spectrally opponent color mechanisms account 
for this phenomenon. A Benhams disk is a .at disc, half of which is black and the other half has three 
sets of lines like the grooves on a record but more spaced out, Figure 7. When the disk is spun a human 
observer sees red, yellow and green rings, despite the fact that there are no colors in the pattern. 
The curves on the right of the pattern begin to explain what happens. Each curve plots the temporal light 
intensity distribution at the di.erent radii from the centre, created when the topis spun. These changinglightpatternsproduce 
spatiotemporal interaction in the HVS that unbalance antagonistic, spectrallyopponent mechanisms to 
create the appearance of colored rings. This illusion demonstrates that, although it may be convenient 
to model the HVS in terms of unidimensional responses to motion, pattern and color, human percepts are 
in fact the product of complex multidimensional response. Figure 7: When a black and white patterned 
top shown on the left is rotated at 5-10 revolutions per second, colored rings can be seen. The light 
intensity distribution of the rotating pattern as a function of time is shown on the right. Spatiotemporal 
interactions between antagonistic, spectrally opponent color mechanisms account for this phenomenon. 
A second example, Figure 8, shows the panels in checkerboard block on the left and a .at pattern on the 
right, which have the same re.ectance, but di.erencesin their three-dimensionalorganization means they 
areperceiveddifferently. Thetwopanels marked withXshavethe same re.ectance,but onthe block they appear 
to have di.erent re.ectance under di.erent levels of illumination. Conversely, the two panels marked 
with Os have di.erent re.ectance values but on the block appear to be the same color due to the di.erent 
illumination conditions. This demonstrates the complexity of interactions between apparent re.ectance, 
apparent illumination and apparent shape that can dramatically a.ect human perception. Figure 8: Interaction 
between apparent re.ection, apparent illumination and apparent three-dimensional shape. Corresponding 
panels in the two patterns have the same physical re.ectance. Di.erences in the perceived spatial organization 
of the patterns produces di.ering interpretations in terms of lightness (apparent re.ectance)andbrightness(apparentillumination). 
 4.1.6 Lightness Perception Gilchrist[43,44,18]justi.ed the systematic study oflightness error as an 
understanding of the HVS. He found that there are always errors when judging lightness, and these errors 
are not random,but systematic. Thepattern of these systematic errorsthereforeprovide a signature of the 
visual system. Hede.nes alightness error as anydi.erence between the actual re.ectance of a target surface 
and the re.ectance of the matching chip selectedfrom aMunsell chart.The taskde.nedfor thepsychophysical 
experimentsdescribed laterin this thesisinvolves asking human observers to match the re.ectance of real 
world objects to a Munsell chart, which gives a measure of errors in lightness matching. The observeristhen 
asked to matchthe re.ectance of simulated objects(in a computer generated rendition of the real world) 
to the same Munsell chart. This gives a measure of lightness errors with respect to the computer image. 
There arelimitations on theHVS, sothere willbe errors(systematic errors) inboth cases. For the rendered 
image to be deemed a faithful representation, both sets of lightness errors should be close to each other. 
Gilchrist(1977)[45] showed thattheperceptionof thedegreeoflightnessof asurfacepatch(i.e. whetheritiswhite,gray 
orblack) isgreatly a.ectedby the perceived distance and orientation of the surface in question, as well 
as the perceived illumination falling on the surface -where the latter was experimentally manipulated 
through a variety of cues such as occlusion, or perspective. Perceptionof thelightnessofpatchesvaryingin 
re.ectancemay thusbe a suitable candidate for the choice of visual task. It is simple to perform, and 
it is known that lightness constancydepends on the successfulperception of lighting and the 3D structure 
of a scene, for exampleFigure9When viewedin isolation, the patches on the top left hand corner appear 
to be of di.erent luminance. However, when examined in the context of the entire scene, it can be seen 
that the patches have been cut from the edge of the stairwell, and are perceived as an edge where the 
entire stairwell has the same luminance. Lightness has been applied when developing Tone Mapping techniques 
for High Dynamic Range Imagery[61, 62]. Figure 9: Importance of depth perception for lightness constancy. 
 4.2 Perceptually driven rendering Recent years have seen an increase in the application of visual perception 
to computer graphics. As mentioned earlier, in certain applications it is important that computer images 
should not only be physically correct but also perceptually equivalent to the scene it is intended to 
represent. Realism implies computational expense, and research is beginning to emerge to investigate 
how knowledge of the human visual system can be used to cut corners and minimize rendering timesbyguiding 
algorithms to compute only whatis necessary to satisfy the observer.Perceptionbasedimagequality metrics, 
which canbe used to evaluate, validate and compareimageryhavebeenpresented[99, 97,80,76,86]. Evenfor 
realisticimage synthesisthere maybelittlepoint spending time or resources to compute detailin animage 
that would not bedetected by ahuman observer. By eliminating any computation spent on calculating image 
features which lie below the threshold of visibility, rendering times can be shortened leading to more 
e.cient processing. Because the chief objective of physically basedrenderingis realism,incorporating 
models ofHVSbehaviorinto rendering algorithms can improve performance, as well as improving the quality 
of the imageryproduced. Soby taking advantageof thelimitations of thehuman eye, just enough detail to 
satisfy the observer can be computed without sacri.cing image quality. Several attempts have been made 
to develop image synthesis algorithms that detect threshold visual di.erences and direct the algorithm 
to work on those parts of an image that are in most need of re.nement. Raytracing produces an image by 
computing samples of radiance, one for eachpixelin theimageplane. Producingan anti-aliasedimageisdi.cult 
unless veryhigh samplingdensities are used. Mitchell[85] realized thatdeciding where to do extra sampling 
can be guided by knowledge of how the eye perceives noise as a function of contrast and color. Studies 
have shown that the eye ismost sensitiveto noiseinintermediatefrequencies[123]. Whilefrequencies of upto60 
cyclesperdegree(cpd) canbe visible,the maximum responseto noise is at approximately 4.5 cpd, so sampling 
in regions with frequency above this threshold can be minimized, without a.ecting the visual quality 
of the image. Mitchell begins by sampling the entire image at low frequency then uses an adaptive sample 
strategy on the image according to the frequency content. This results in a non uniform sampling of the 
image, which enables aliasing noise tobe channelledintohighfrequencies where artifacts areless conspicuous. 
However, non-uniform sampling alonedoesnt eliminate aliasing,just changesits characteristicsto makeitless 
noticeable. Mitchellapplies twolevels of sampling. To decide whether the high sampling density should 
be invoked the variance of samples couldbe used[89],but thisis apoor measure of visualperception of local 
variation. InsteadMitchell chooses to use contrast to model the non-linear response of the eye to rapid 
variations in light intensity: As each sampleconsists of threeseparateintensitiesfor red,greenandblue, 
three separate contrasts can be computed for each of them. These three contrasts aretested against separatethresholds,0.4,0.3 
and0.6for red,green and blue respectively, and super-sampling is done if any one exceeds the threshold. 
The contrast metric is then used to determine when the high sampling density should be invoked. This 
test is most sensitive to green in accordance with the human eyes response to noise as a function of 
color. Multi stage .lters are then used to reconstruct the non-uniform samples into a digital image. 
Although this idea has the beginnings of a perceptual approach, it is at most a crude approximation to 
the HVS. Only two levels of sampling are used and it doesnt account for visual masking 1 . The HVS exhibits 
di.erent spatial acuities in response to di.erent colors. Evidence exists that color spatial acuityis 
less than monochrome spatial acuity. Exploiting thispoor color spatial acuity of theHVS,Meyer andLiu[84] 
developed an adaptiveimage synthesis algorithm which uses an opponentsprocessing model of color vision[61] 
comprising chromatic and achromatic color channels. Using aPainter andSloan[121] adaptive subdivision, 
ak-D 2 tree representation 3 of the image is generated. Areas of the image containing high frequency 
1 The presence of high spatial frequency in an image can mask the presence of other high frequency information 
2 A KD Tree is a data structure that is used in computer science during orthogonal range searching information 
are stored at the lower levels of the tree. They then modi.ed a screen subdivision raytracer to limit 
the depth to which the k-D tree must be descended to compute the chromatic color channels. The limit 
is determined bypsychophysical resultsdescribing the color spatialfrequency. They achieved a modest saving 
in computational e.ort and showed, using a psychophysical experiment, that decreasing the number of rays 
used to produce the chromatic channelshadless of an e.ect onimagequality than reducing the number of 
rays used to create the achromatic channels. This was the .rst work to attempt to minimizethe computationofcolorcalculations,asopposedtojustdecreasing 
costly object intersection calculations. Bolin andMeyer[9] took afrequencybased approachto raytracing, 
which uses a simple vision model, making it possible for them to control how rays are cast in a scene. 
Their algorithm accounts for the contrast sensitivity, spatial frequency and maskingproperties of the 
HVS. The contrast sensitivity response of the eye is non-linear. So, when deciding where rays should 
be cast, the algorithm deems a luminance di.erence at lowintensity to be ofgreater importance than the 
same luminance di.erence at high intensity. The spatial response of the HVS is known to be less for patterns 
of pure color than for patterns that include luminance di.erences. This means that it is possible to 
cast fewer rays into regions with color spatial variations than are cast in regions with spatial frequency 
variations in luminance. Finally, it is known that the presence of high spatial frequency can mask the 
presence of other high frequency information(masking). When usedin conjunction with aMonteCarlo raytracer,more 
rays are spawned when low frequency terms are being determined than when high frequency terms are being 
found. Using this strategy, the artifacts that are most visible in the scene can be eliminated from the 
image .rst, then noise can be channelled into areas of the image where artifacts are less conspicuous. 
This techniqueis animprovementonMitchells methodbecause the vision model employed accounts for contrast 
sensitivity, spatial frequency and masking. Despite the simplicity of the vision models used in these 
approaches, the results arepromising, especially astheydemonstratethefeasibility of embedding HVS models 
into the rendering systems to produce more economical systems without forfeiting image quality. Fueled 
by the notion that more sophisticated models of theHVS wouldyield evengreaterspeedup, several researchersbegan 
to introduce more complex models of the HVS into their global illumination computations. Myszkowski[88] 
applied a more sophisticated vision modelto steer computation of a Monte Carlo based raytracer. Aiming 
to take maximum advantage of the limitations of the HVS, his model included threshold sensitivity, spatial 
frequency sensitivity and contrast masking. A perceptual error metric is built into the renderingengine 
allowing adaptive allocation of computation e.ortinto areas where errors remain above perceivable thresholds 
and allowing computation to be halted in all other areas (i.e. those areas where errors are below the 
perceivable threshold and thus not visible to a human observer). This perceptual error metric takes 
the form of Dalys [25] Visible Di.erence Predictor (VDP). Bolin and Meyer [10] devised a similar scheme, 
also using a sophisticated vision model,in an attempt to make use of allHVSlimitations. Theyintegrated 
a simpli.ed versionof theSarno.VisibleDiscriminationModel(VDM) into an image synthesis algorithm to detect 
threshold visible di.erences and, based on those di.erences direct subsequent computational e.ort to 
regions of the image in most need of re.nement. The VDM takes two images, speci.ed in CIE XYZ color space, 
asinput. Outputofthe modelis aJustNoticeableDi.erence(JND) map. One JND corresponds to a 75% probability 
that an observer viewing thetwoimageswoulddetect adi.erence[81]. They usethe upper andlower boundimagesfrom 
the computation results atintermediate stages and used the predictor to get an error estimate for that 
stage. Drettakis et al introduced a perceptual rendering pipeline which takes into account visual masking 
due to contrast and spatial frequency[29]. Scenes are split into layers to account for inter-object masking. 
Using a perceptually driven level of detail algorithm the layers are then usedto choose an appropriatelevel 
ofdetailfor each objectbased onpredicted contrast and and spatial masking. Asubsequent user study showed 
that their algorithmic choices corresponded well withperceiveddi.erencesin the images. Masking has also 
been used in geometric modeling, Lavoue et. al. [65] introduced the notion of roughness for a 3D mesh. 
Roughness gives a measure of geometric noise on the surface, based on this noise masking can be invoked 
to hide geometric distortions. Applying a complex vision model at each consecutive time step of image 
generation requires repeated evaluation of the embedded vision model. The VDP can be expensive to process 
due to the multi-scale spatial processing involved in some of its components. This means that in some 
cases the cost of recomputing the vision model may cancel the savings gained by employing the perceptual 
error metric to speed up the rendering algorithm. To combat this, Ramasubramanian[100]introduced a metric 
thathandlesluminance-dependent processing and spatially-dependent processing independently, allowing 
the expensivespatially-dependent componenttobeprecomputed. Ramasubramanian developed a physical error 
metric that predicts the perceptual threshold for detecting artifacts in the image. This metric is then 
used to predict the sensitivity of the HVS to noise in the indirect lighting component. This enables 
a reduction in the number of samples needed in areas of an image with high frequencytexturepatterns,geometricdetails, 
anddirectlighting variations,giving a signi.cant speedup in computation. Using validated image models 
that predict image .delity, programmers can work towards achieving greater e.ciencies in the knowledge 
that resulting images will still be faithful visual representations. Also in situations where time or 
resources are limited and .delity must be traded o. against performance, perceptually based error metrics 
could be used to provide insights into where computation could be economized with least visual impact. 
In additiontoToneMapping Operators(TMOs) being usefulfor rendering calculated luminance to the screen 
[119, 63, 26, 103], they are also useful for givinga measure of theperceptibledi.erencebetween twoluminances 
at agiven level of adaptation. This function can then be used toguide algorithms, such as discontinuity 
meshing, where thereis a need todetermine whether someprocess would be noticeable or not to the end user. 
Gibson andHubbold[42] haveusedfeatures of thethreshold sensitivity displayed by the HVS to accelerate 
the computation of radiosity solutions. A perceptuallybased measurecontrolsthegenerationof viewindependent 
radiosity solutions. Thisis achievedwith an apriori estimate of real-world adaptation luminance, anduses 
aTMO to transformluminance values todisplay colors and is then used as a numerical measure of their perceived 
di.erence. The model stopspatch re.nement oncethedi.erencebetween successivelevels of elements becomesperceptually 
unnoticeable. Theperceivedimportance of anypotential shadow falling across a surface can be determined, 
this can be used to control the number of rays cast during visibility computations. Finally, they use 
perceptual knowledge to optimize the element mesh for faster interactive display and save memory during 
computations. This technique was used on the adaptive element re.nement, shadow detection, and mesh 
optimization portions of the radiosity algorithm. Discontinuity meshing is an established technique used 
to model shadows in radiosity meshes. It is computationally expensive, but produces meshes which are 
far more accurate and which also contain fewer elements. Hedley et al. [50] used aperceptuallyinformed 
error metric to optimize adaptive mesh subdivision for radiosity solutions,thegoalbeing todevelop scalablediscontinuity 
meshing methods by considering visual perception. Meshes were minimized by discarding discontinuities 
which had a negligible perceptible e.ect on a mesh. They demonstrated that a perception-based approach 
results in a greater reduction in mesh complexity, without introducing more visual artifacts than a purely 
radiometrically-based approach. Farrugia andPeroche[36] used aperceptual metricfordiscontinuity re.nementtodevelop 
aprogressiveradianceevaluationbased onthe work ofGuo et. al. [46]. Guo used an iterative process to construct 
an irregular subdivision of the image in blocks which refer to smooth regions, or discontinuous regions 
to build aDirectionalCoherenceMap(DCM).Asthe algorithmproceedsthe current DCM dictates where new samples 
are taken. A contrast based perceptual heuristic based on contrast over samples corresponding to the 
corners of each block is used. Farrugia and Peroch extend this by applying a visual di.erences predictor 
based on Pattanaik et al.s Multiscale Model of Adaptation and SpatialVisiontoclassify theirsubdivisioncells[94]. 
Tospeed up computationthey apply the metric over each cellpair using a statistical approachbased onAlbin 
et. al. [2]. Recognizing that the illumination on a surface can be split into separable components,which 
canbeindividually computed,Stokeset. al[112]introduced a new approach which applied a perceptual metric 
on each component. After a suite of psychophysical experiments to probe various global illumination scenariosthey 
determinedlimited contributionoflightpathinteractionand .tted a mathematical model used to guide rendering 
based on the metric predicted relativeimportance of each component as afunction of visible surface materials. 
Ramanarayanan et. al. noticed that when viewing an aggregate, observers attend less to individual objects 
and focus more on overall properties such as numerosity, variety, and arrangement. They also noted that 
rendering and modeling costs increase with aggregate complexity, exactly when observers are attendinglesstoindividual 
objects. Theypresented new aggregateperception metrics to simplify scenes by substituting geometrically 
simpler aggregates for more complex ones without changing appearance[98]. Ramanarayanan[99] workedtowarddeveloping 
aperceptual metricsbased onhigherorderaspectsof visual coding andintroduced theterm Visual equivalence 
. Images are visually equivalent if they convey the same impressions of scene appearance, even if they 
are visibly di.erent. They conducted a series of psychophysical experiments to investigate how object 
geometry, material, and illumination interact in.uence appearance. In their paper they characterized 
conditions under which two classes of transformations on illumination maps (blurring and warping) yield 
images that are visually equivalent to reference solutions, and from this developed a metric to predict 
visual equivalence. 4.3 Conclusion Usingvalidatedimagemodelsthatpredictimage .delity,programmerscanwork 
toward achievinggreatere.cienciesintheknowledgethat resultingimageswill stillbefaithful visual representations. 
Alsoin situations where time or resources are limited and .delity must be traded o. against performance, 
perceptually based error metrics could be used to provide insights into where computation could be economized 
with least visual impact. Some of the applications of visualperceptionin computergraphicswere explored. 
For many applications computer imagery should not only be physically correct but also perceptually equivalent 
to the scene it represents. Knowledge of he HVS can be employed to greatly bene.t the synthesis of realistic 
images at various stages ofproduction. Globalillumination computations are costly in terms of computation. 
There is a great deal of potential to improve the e.ciency of such algorithms by focusing computation 
on the features of a scene which are more conspicuous to the human observer. Those features that are 
belowperceptual visibility thresholdshavenoimpactonthe .nal solution,and therefore can be omitted from 
the computation, increasing e.ciency without causing any perceivable di.erence to the .nal image. Perceptual 
metrics involving advanced HVS models can be used to determine the visible di.erences between apair 
ofimages. These metrics canthenbe used to compareand evaluate image quality. They can also be used within 
the rendering framework to steer computationinto regions of animage which arein most need of re.nement, 
and tohalt computation whendi.erencesin successiveiterations of the solution become imperceptible. Katerina 
Mania  5 Perceptually Motivated Simulation and Virtual Environments 5.1 Introduction Computer graphics 
algorithms have for long dealt with simulation of physics: simulation of the geometry of a real-world 
space, simulation of the light propagation in a real environment and simulation of motor actions with 
appropriate tracking. Perception principles have subsequently been incorporated into rendering algorithms[82], 
in order to save rendering computation, mainlyfollowing thegenericideaof donotrenderwhatwecannotsee [77,54,68]. 
However, with Virtual Environment (VE) simulator technologies aiming at simulating real-world task situations, 
the research community is challenged to produce a much more complex system which is perceptually optimized. 
We do not necessarily require accurate simulation of physics to induce reality. Much less detail is 
often adequate[126,37,72,73]. 5.2 Perceptually-based Selective Rendering Perception principles have 
been incorporated into rendering algorithms in orderto optimize renderingcomputation andproducephotorealisticimagesfrom 
a human rather than a machinepoint of view. In order to economize on rendering computation, selective 
rendering guides high level of detail to speci.c regions of a synthetic scene and lower quality to the 
remaining scene, without compromising the level of information transmitted. Scene regions that have 
been rendered in low and high quality can be combined to form one complete scene. Such decisions are 
guided by predictive attention modeling, gaze or task-based information. In order to economize on rendering 
computation, previous research dealing with interactive synthetic scenes has been focused on either rendering 
in high quality the2-3degreesfoveal regionof vision and withlessdetail theperiphery of visionbased ongazeinformation[70], 
or rendering inhighquality thefoveal area based on a-priory knowledge of the viewers task focus [19, 
113]. Gazedependent rendering encounters di.culties of maintaining display updates free of visual artifacts 
after a fast ( 4ms) eye saccade. Such processes are quite computationallydemanding,however,if the speedgaze-to-renderingissueis 
resolved, task performance results are indistinguishable to a fully .edged, high resolution real-time 
environment. It has also been proposed to assign selective highquality renderingin the visual angle of 
the fovea(2o) centered on the users taskfocus[19,113]. This approach,however, cannotbe appliedwhen thereis 
no overt task to be conducted. Moreover, there is no acceptable model of comparing or predicting task-relevant 
saccades. Following a di.erent approach, Haber et al. [47]suggested renderingtheinformative areas of 
a scenein varyingquality based on saliency models. Such models aim to predict the visual features that 
involuntarilyattract visual attention such as object edges, sudden color changes or movements. It was 
proposed that the most noticeable areas as derived from saliency modeling shouldbe renderedinhigherquality. 
Bottom-up visual attention models are not showntopredict attention regions successfully[77]. Correlation 
between actual human and computationally-derived scan-paths was found to be much lower than predicted 
when carrying out a real-world task such as making a cup of tea [35]. Moreover, we have no generally 
accepted model of comparing scan paths. A comprehensive approach shouldbe task andgaze-independent, simulating 
cognitive processes rather than predicting attention employing bottom-up processes such as saliency 
models. A recent selective rendering approach exploits existing research on memory schemata which could 
ultimately guide selective rendering based on spatial cognition processes. Schemata are knowledge structures 
based on the notion that an individuals prior experience will in.uence how he or she perceives, comprehends 
and remembers new information. When participants are exposed to a large amount of information in a scene, 
cognitive psychologists have suggested that schemata are used to guide the search forinformationin memory[13]. 
Ageneralpremisederivedfromthisresearch is that information which is not related to the schema being used 
in retrieval will be harder to recall than information which is schema related. In terms of real world 
scenes, schemata represent the general meaning of a scene such as o.ce,theatreetc. Schematain.uence memory 
of the objectsinagivencontext according to their association with the schema in place. When being exposed 
to a synthetic environment, similar information should be transmitted between the simulated scene and 
the real-world scene, both depicting a speci.c schema. This would, in due course, indicate which objects 
or areas in a synthetic scene couldbe renderedinlowerquality without a.ecting information uptakebut at 
the sametime reducing computational complexity[87]. Flannery and Walles [39] investigated how schema 
theories apply to real versus virtual memories. Participants wereinstructed to explore either a virtual 
or a similar real environment for 20 seconds, without prior knowledge that their memory of the space 
would be subsequently assessed. Participants then completed a recognition task. Recognition scores revealed 
that participants had better recognition for consistent objects, but were more con.dent for the recognition 
of the inconsistent objects. Previous work[71,72],included apreliminaryinvestigation of the e.ect of 
objecttype(consistentvs. inconsistent) and shadows(.at-shaded scenevs. radiosity scene) on object memory 
recognition in a VE. The computer graphics simulation wasdisplayed on aHeadMountedDisplay(HMD) utilizing 
stereo imagery and head tracking. Thirty-six participants across three conditions of varied renderingquality 
of the same space were exposed to the computergraphics environment and completed a memory recognition 
task. The high-quality and mid-quality conditions included a pre-computed radiosity simulation of an 
academics o.ce(with80% and40% radiosityiterations computed respectively). The low-quality condition consisted 
of a .at-shaded version of the same o.ce. Results revealedthatschema consistent elements ofthe scene 
were morelikelyto be recognized than inconsistent information. Overall, higher con.dence ratings were 
assigned to consistent rather than inconsistent items. Total object recognitionwasbetterforthe sceneincluding 
rough shadows(mid-quality condition) compared to the .at-shaded scene. The presence of accurate shadow 
information, though, did not a.ect recognition of consistent or inconsistent objects, therefore lower 
quality of rendering was adequate for better memory recognition of consistent objects. This study was 
limited to the investigation of subtle shadow variations. Another experimentalstudy employed a more extreme 
set of rendering types: wireframewith added color,andfull radiosity[87](Figure3). Theproportion ofinconsistent/consistentobjects 
was varied, and object recognition tests ensured that all objects were easily recognizedin all conditions. 
The results showed a signi.cantinteractionbetween renderingtype, object type, and consistency ratio. 
This suggests that inconsistent objects are onlypreferentially remembered if the scene looks normal or 
if there are many such objects in an abnormal scene such asin the wireframe condition. It was also shown 
that memoryperformanceisbetterfortheinconsistent objectsinthe radiosity rendering condition compared 
to the wireframe condition. We conclude that memory for objects can be used to assess the degree to which 
the context of a VE appears close to expectations. Despite contradictory results in literature as detailed 
above, it seems that perceptual information can be complemented by involuntary knowledge based onpast 
experience. Experimental studiesin syntheticsceneshaverevealed that consistent objects which are expected 
to be found in a scene can be rendered inlowerquality without a.ecting information uptaketaking advantage 
of such expectations, whereas inconsistent items which are salient would require a high level of renderingdetail 
in orderfor them to beperceptually acknowledged[71]. Therefore, by exploiting schema theory, it is possible 
to reduce computational complexity, producing scenes from a cognitive point of view without a.ecting 
information uptake and resulting in an entirely novel and interdisciplinary approach which is gaze, 
task and saliency-model independent. A novel selective rendering system has been presented that exploits 
schema theory by identifying theperceptualimportanceofsceneregions[134]. Objectsthathavebeen rendered 
in low and high quality are incorporated in a scene based on schema expectations. The rendered quality 
of these objects will change in real time, dependent on user navigation andinteraction[87]. High level 
visual cognition is that which takes place late in the HVS, that is parietal and temporal cortex and 
into the frontal lobes when decisions based onvisualinformationneed tobe made. Lowlevel visual cognitionisthat 
which takes place in the occipital lobe, early on in the visual processing stream, e.g. the visual signalis 
receivedinthe retinae, andinitiallypassed through theLateral Geniculate Nucleus to the occipital lobe 
at the back. Thus, these higher decision processes are una.ected by changes in the experiments, whilst 
normal (visual) cognition occurs as long as the scene is realistic. Taken together, the results ofpreviousstudiesinvestigating 
the e.ect of schemas on object recognition suggestthathigh-level visual cognitionisgenerally una.ectedby 
ubiquitous Figure10: Task-based rendering[19]. Figure11: Gaze-based rendering[70].  Figure12: Schema 
memory experimental studies[87]. Figure13: SelectiveRendererArchitecture[134]. graphics manipulations 
such as polygon count and depth of shadow rendering; normal cognition operates as long as the scenes 
look acceptably realistic. On the other hand, when the overall realism of the scene is greatly reduced, 
such as in the wireframe condition, then visual cognition becomes abnormal. Specifically, e.ects that 
distinguish schema-consistent from schema-inconsistent objects changebecause the whole scene nowlooksincongruent, 
andwehave shown that this e.ect is not due to a failure of basic recognition. Thus, a recipe for anyone 
wishing to use such displaysin studies of visual cognitionis to construct environments which look acceptably 
realistic in terms of polygon count but need not be of very high quality. This relates to the kinds of 
high-level visual cognitive e.ects we have studied here, such as object congruence. Lower-level e.ects, 
such asrecognition,canbedissociatedfromthesehigh-level e.ects(but make their presence felt when the scene 
is further degraded, e.g. when color is removedfromthe wireframescenes). Thus,high-levelprocessesneed 
somewhat greater realism than low-level ones. 5.3 Behavioral Fidelity of Simulations based on Space 
Memory The entertainment world appears to consider highly realistic visual quality one of the keys to 
success, with cinematic quality graphics claimed for the next generation of gaming consoles. On the other 
hand, when interactive immersive Virtual Environments (VEs) are implemented for training rather than 
entertainment purposes, visual quality might not be as signi.cant. If the training is to be e.ective, 
the skills acquired must transfer into the real world at appropriate levels of performance. A VE with 
maximum visual and interaction .delity would resultin a transfer ofinformation equivalent to real worldtraining 
sincetheenvironmentswouldbeindistinguishable[73]. Visual .delity refersto the degree to which visual 
features in the VE conform to visual features in the equivalent real environment [122, 106]. Functional 
realism refers to the communication of similar information in the real and virtual world rather than 
the aesthetics orphysics,inthe sensethat users areable makethe samejudgments andperformthe sametasks 
asinthe real world[37,91]. It is tempting to replicate the real world as accurately as possible in order 
toprovideequivalent experiences[67]. Whilst arguablyideal,itis notyet computationally feasible for this 
to occur. Trade-o.s between visual/interaction .delity and computational complexity shouldbe applied 
to asimulationsystem withoutdetracting fromitstraining e.ectiveness[72,124]. Thereis,therefore, a call 
for e.cient techniques assessing the .delity of a VE and determine its relationship with performance 
in order to economize on rendering computation without compromising thelevel ofinformationtransmitted(functional 
realism) [37]. The utility ofVirtualEnvironment(VE) technologiesfortraining systems such as .ight simulators 
is predicated upon the accuracy of the spatial representation formed in the VE. Spatial memory tasks, 
therefore, are often incorporated in benchmarking processes when assessing the .delity of a VE simulation 
for training. Spatial awareness is signi.cant for human performance e.ciency of such tasks as it is dependent 
on spatial knowledge of an environment [64, 28, 129]. A central research issue, therefore, is how an 
interactive synthetic scene is cognitively encoded and how recognition and memory of such worldstransfertoreal 
world conditions[72,1,38]. Previousresearchhasexamined the variables that communicate transfer of spatial 
knowledge acquired in a simulation environment,in the real-world anddiscuss theform anddevelopment of 
spatial awareness in VE training compared to either real-world training or training with maps, photographs 
and blueprints [8, 6]. The suitability of VE systems as e.ective training mediums was examined and was 
concluded to be as e.ective as map or blueprint training. Con.gurational knowledge acquisition based 
on estimation of absolute distances and directions between known points couldyield training e.ects similartotraining 
withphotographsand real world training[8]. Furthermore,estimationof traveldistancefromoptic .owis subject 
to scaling when compared to static intervals in the environment, irrespective of additionaldepth cues[40]. 
Past research often aimstoidentify the minimum system characteristics relevant to rendering computations 
and interaction interfaces that would yield the maximum performance on a task or the greatest sense 
of presence. For example, search objects rendered in global or ambient illumination have been shown to 
take signi.cantly longer to identify than those rendered through alocalillumination model[133]. Whatif 
the visual .delity of a system should be assessed across a range of applications and tasks? Could we 
interrogate the human cognitive systems that are activated when training within VE scenes of varied visual 
or interaction .delity in order to identify whether such responses are transferable to the real-world 
task situation simulated? Which simulation characteristics should we optimizein order to match the capabilities 
of the VE system to the requirements of these cognitive systems? Becauseof the wide-rangeofVE applicationsanddi.erencesinparticipants 
across their backgrounds, abilities and method of processing information, an understanding of how spatial 
knowledge is acquired within a VE, complementing spatial memory performance per se, is signi.cant. Common 
strategies may be revealed across a range of applications and tasks. Recent research focuses upon the 
e.ect of renderingquality(.at-shaded vs radiosity) on object-location recognition memory andits associated 
awareness states while spatialknowledge istransferredfroma synthetictraining environmentinto areal-world 
situation. The mainpremise ofthis workisthataccuracy ofperformanceper seis animperfect re.ection of 
the cognitive activity that underlies performance on memory tasks [74]. The framework to be presented 
has been drawn from traditional memory research adjustedtoform an experimentalprocedure[118,11,27]. Accurate 
recognition memory can be supported by: a speci.c recollection of a mentalimage orprior experience(remembering); 
reliance on ageneral sense ofknowing withlittle or no recollection of the source of this sense(knowing); 
guesses. Gardiner andRichardson-Klavehn[41] explained the remembering as personal experiences of the 
past that are recreated mentally. Meanwhile knowing refers to other experiences of the past but without 
the sense of reliving it mentally. The work of Tulving [118] .rst suggested that remembering and knowing 
were measurable constructs. Through a series of experiments, Tulving [118]reported that participants 
.nd it easy to distinguish between experiences of remembering and knowing when self-reporting their experiences. 
The sense of knowing has since been further divided into two related concepts. The correct answer maybejustknown 
without the associated recollection of contextual detail associated with remembering or the answer may 
feel more familiar than a uninformedguess,but cannotbe considered asbeing known(familiar). According 
to this theoretical framework derived from memory psychology, measures of the accuracy of memory can 
be complemented by self-report of states of awareness such as remember, know, familiar and guess during 
recognition[24]. Previous studieshaveinvestigated the relationshipbetween recognition memory and simulation 
environments of varied visual and interaction .delity. Such work by the authors of this paper revealed 
varied distribution of awareness states whilst overall accuracy remained the same across experimental 
conditions suggesting that measurement of awareness states acts as a useful additional measure to supplement 
the information provided by accuracy [75, 72]. A di.erent study employing the same methodology aims to 
interrogate the mentalprocesses associated with obtaining spatialknowledgeduring exposureto a simulated 
scene while transferring suchknowledgein the real-world scene simulated[74]. An object-memory task wasperformedinthe 
simulated real-world environment immediately after VE training and a retention test was conducted one 
week after the VE exposure. The virtual scene was rendered with one of twolevelsof visual .delity(.atshaded 
vs. radiosity rendering) anddisplayed on a stereoHeadMountedDisplay(HMD). The experimental scene consisted 
of a room depicting an academics o.ce. Central to this work is identifying whether high .delity or low 
.delity scenes are associated with stronger visually induced recollections represented by self-report 
of the remember awareness state. A secondary, exploratory goal is to investigate the e.ect of schemas 
on memory recognition post VE exposure. Memory recognition studies in synthetic scenes have demonstrated 
that low interaction .delity interfaces such as the mouse compared to head tracking as well as low visual 
.delity scenes provoked a higher proportion of visually-induced recollections associated with the remember 
awareness state, while there was no e.ect of condition upon memory recognitionperformance[75, 72]. Broadly, 
desirablein.uences on recognition memory and the associated cognitive states may be ultimately identi.ed 
and generalized to aid speci.c applications. It could be true, for instance, that for .ight simulation 
applications it is crucial for trainees to refer to mental images associated with instruments as opposed 
to recollections that are con.dent but not accompanied by mental imagery when training is transferred 
into a real-world .ight situation. The study presented, therefore, explores the e.ectof traininginimmersive 
environments of varied visual .delity on the distribution of memory awareness states measured in a real-world 
task[74]. Thefactthatithasbeen shownthatinterfaces oflow interaction or visual .delity induce a higher 
number of recollections based on mental imagery when compared with systems of high visual or interaction 
.delity, mayrelate to attentional resourcesdirected to systems that vary strongly from the real-world. 
The results demonstrated that participants who trained inthelow .delity simulationreported alargerproportionof 
correctremember responses while conducting the memory recognition task in the real-world situation compared 
to participants trained in the high .delity simulation. These resultswereconsistent withprevious .ndingsthat 
associated alargerproportion of correct remember responses with low visual and interaction .delity simulations 
[75, 72]. The results observed consistently in previous studies was also observed in this study despite 
the fact that participants physically performed the task in the real-world room after training in its 
simulated counterpart consisting of an ecologicallyplausibletraining scenario. Recent developments in 
psychological research have shown that distinctive information or experiences generate more awareness 
states associated with remembering. For example, participants who are shown typical and distinctive 
faces are more likely to recognize the distinctive faces in a later memory test with an accompanying 
experience of remembering[12]. Similar resultshave also beenfound using otherstimuli such asforenames[11] 
.Inthecurrentcontext, a low .delity rendered simulation could be considered as being more distinctive 
than a high .delity rendered simulation because of its variation from real. Given that these are immersive 
environments, distinctiveness in this instance wouldbejudged relativeto reality. Theless realthe environmentis,the 
more distinctive it can be considered. It would be expected that a more distinctive immersive environment, 
e.g. a low .delity one would result in more remember responsesthanalessdistinctiveimmersiveenvironment,e.g. 
ahigh.delity one. It is worth noting that distinctiveness in this sense may not only refer to visual 
distinctivenessbuttomotorresponsestotheenvironments[72]. Theimportant variable therefore appears to be 
di.erentiation relative to multiple aspects of reality, e.g. visual appearance of, and, motor responses 
within. Here, higher con.dence scores associated with the .at-shaded condition compared to con.dence 
of recollections after training in the radiosity condition further support this suggestion. Whilstthe 
relationshipbetweendistinctivenessand memory mayproveusefulin explainingthese e.ectsitisimportant to 
consider whatcognitiveprocesses may underlie such arelationship. Previouspsychological researchhasindicated 
that rememberresponsesrequiremoreattentionalprocessinginthe .rstinstance thanthosebased onfamiliarity[93,12]. 
Atentative claim wouldthereforebe: immersive environments that are distinctive recruit more attentional 
resources. This additional attentionalprocessing maybring about a changeinparticipants subjective experiences 
of remembering when they later recall the environment. This change would therefore lead to an increase 
in the experience of remembering. Interestingly, this e.ect was not observed during the retest that 
revealed similarproportions of awareness statesdistributed acrossthe viewing conditions. It is likely 
that the .delity of the training environment only a.ects awareness states when transfer of training is 
tested immediately. As time goes by, the enhanced attentional resourcesassociated withlow .delity environmentsdonot 
in.uence the long-term memories associated with the training simulation. Moreover, it is found here that 
more correct know responses are reported after training in the high .delity rendered simulation than 
in the low .delity rendered simulation. This would suggest a shift from remember responses to know responses. 
Memories that are accompanied with a feeling of remembering forparticipantsinthelow .delitysimulationareonly 
accompanied with afeeling of knowing in the high .delity simulation. In line with suggestions made above, 
this could be explained on the basis of reduced attentional processing of these items in the high .delity 
simulation. 5.4 Investigating Perceptual Sensitivity to Head Tracking Latency VirtualEnvironment(VE) 
latencyis the timelagbetween a users actionin a Virtual Environment and the systems response to this 
action. This lag typically takes the form of a transport delay and arises from the sum of times associated 
with measurement processes of the various input devices, computation of the VE contents and interaction 
dynamics, graphics rendering, and .nite data transmission intervals between these various components. 
The VE andhumanfactorsliteraturehas established that thesedelayshave a signi.cant impact onuserperformance[33],[32] 
and userimpressionsof simulation .delity [31, 33, 60, 78, 1]. Latency negatively a.ects user performance 
in 3D object placement tasks[66, 127]. Excessive latency has long been known to hinder operator adaptation 
to other display distortions such as static displacement o.set [51]. Latency also degrades manualperformance,forcing 
users to slowdown topreserve manipulativestability, ultimately driving themto adopt amoveand waitstrategy[107] 
, [108]. Operatorcompensationfor adelay usually requiresthe ability topredict the future state of a tracked 
element. Interest has more recently been directed toward the subjective impact of systemlatency relevant 
to virtual reality simulations. Latency as well as update ratehavebeen considered asfactorsa.ecting the 
operatorssense ofpresencein theenvironment[128,120]. Inarecentstudy,lowerlatencieswereassociated with 
a higher self-reported sense of presence and a statistically higher change inheart ratefor userswhilein 
a stress-inducing(fear ofheights),photorealistic environmentinvolving walking around a narrowpit[83]. 
Since the combination of sensing, computation, rendering, and transmission delayis unavoidablein mostVE, 
tele-operation, and augmented reality applications, interest naturally is directed to how detectable 
di.ering levels of latency might be. Both the quanti.cation of perceptual sensitivity to latency and 
description of the mechanismby whichVElatencyisperceived will be essential to guide system design in 
the development of countermeasures such as predictive compensation[5, 60]. Previous research has also 
focused on the precision, stability, e.ciency and complexity of operation interaction and performance 
with latency-plagued systems[78]. Additionally,the .rst measuresofhumanoperatorsdiscriminationof the 
consequences of latency during head or hand tracked movements have been provided[34,31] Relatedinvestigationshave 
explored thehypothesis that observersdo not explicitlydetect timedelay,but ratherdetect the consequences 
of latencyi.e., they use the artifact motion of theVE scene(awayfromits normally expected spatially stablelocation) 
causedby system timelags[1]. Relevantperceptualthresholds(i.e.,JustNoticeableDi.erence orJND) wereidenti.edto 
average 8-17ms, depending on viewing condition. This psychometric quantity appeared to be invariant across 
di.erent pedestals (33, 100 and 200ms, standard stimuli). The apparentinvariance of thedetectionfunctionin[34,31,1] 
demonstratedthatthe classicWebersLawofpsychophysics(thatJNDislinearly proportional to the magnitude 
of the standard stimulus) did not hold for latency. In other words, observers of long latency VEs will 
be as sensitive to changes in latency as those who use prompter, more advanced systems. It can also be 
inferred that the same sensitivity would also apply for comparisons against zero latency pedestal. Regan 
et al.[102] found 70.7% latency thresholds averaging 15ms for a specialized non-immersing CRT display. 
By making assumptions of Gaussian psychometricfunctions and zero responsebiasfortwo-intervalforced-choicejudgments 
withbalancedpresentation order,the70.7% thresholdfrom[102] canbe equated with a JND of 18.6ms. Allison 
et al. 2001observed on the otherhand that withlarge virtual objects occupying the full Head Mounted Display 
(HMD) Field-of-View (FOV), 50% thresholds for perceived image instability (oscillopsia) were found to 
be 180320ms depending on head motion velocity [3]. This threshold indicates the latency level at which 
observers were equally likely as not to say the image was unstable and representstheir average responsebias 
orpreference. Suchresponse biases may be attributable to, among other things, the amount of observer 
trainingbeforethedata was collected andthe type ofjudgment task required. In thecaseofAllisonet. al.[3]participantsperformed 
singleintervaljudgmentsi.e., they did not compare each presentation against a standard stimulus but relied 
on their own internal notion of when an image was no longer stable. Data from citeellis99a,[31],[1] shows 
theirparticipants responsebias rangedbetween40 and70msfor atwo-intervaljudgment of whetherthe stimulus 
wasthe same as ordi.erentthanthepedestal standard. In contrast,theparticipantsin[102] were forced to 
choose which of the two stimulus intervals was actually the one with added latency, which though not 
reported, leads to a presumption of zero bias. Figure14: Experimental conditions[30],[73]. The much 
higher threshold reported by Allison et al. [3] might also be attributable to the fact that their participants 
viewed a textured virtual background (the inside surface of red and white faceted sphere) that completely 
enveloped their head and thus always occupied their entire FOV. Surrounding observers with such a geometrically 
structured environment contributes to the phenomenon of visual capture. The term visual capture implies 
that when concurrent multi-sensory spatial information is available, the observer will weight the visual 
channel more heavily in constructing a percept. It has been demonstrated that, even with very simpleVEgraphicsin 
anHMD, visuallydiscrepant information willbiasproprioceptive and vestibularfeedback of staticheadpitch 
angle[91]. Since awareness ofVEimageinstability relies on visual, vestibular, and proprioceptive information, 
the full structured background viewed in Allison et al.s study[3] may havediminished their observers 
sensitivity tolatency induced oscillopsia. Furthermore, without the inclusion of nearer objects in their 
environment,participanthead movementdoesnottriggermotionof scene contents relative to the background 
and thus does not provide cues through internal image shear. One aim of ongoing research onlatencyperception 
atNASAAmes Research Centre has been to quantify the latency that a VE system can exhibit without being 
perceptible to the user. In our prior studies, we employed very sparse environments containing only a 
single simple object such as a faceted sphere [31],[34]or ahollow-framed octahedron[1]againstan empty 
blackbackground. Synthetic environments with di.ering levels of graphical complexity with the goalof 
extending thegenerality of ourresultsforparticipant sensitivitylatency in VEs have been also employed 
by Mania et al.[73]. The focus of the [73] study is in describing observer sensitivity to latency di.erencesduringheadmovementsin 
animmersiveVE representing a real-world space(room,building, etc.)sensitivity thathas notbeen measuredinprevious 
research. On the one hand, because there could be an inherent association with howthe real worldisperceived, 
we might expect observerstobe more sensitive to the visual consequences of latency when viewing a scene 
representing what could be a real-world space rather than a sparse, simpli.ed scene with only one or 
two arti.cial objects. On the other hand, an enveloping structured scene could promote visual capture, 
thereby degrading observers sensitivity to VE latency. Duringan earlier study morefully reportedin[30], 
a simple white-red checker spheresurrounding the observer,such asthatusedin[3] and/orahollow-frame octahedron 
in front of the observer, as in [1] served as the VEs visual content. Participantswere asked to comparetwo 
sequential stimuluspresentations while moving their head in a constant pattern and report whether the 
stimuli di.ered in the visible consequences of the experimentally manipulated VE latency. Thestudypresentedhereemployed 
the sameexperimental methodology, but instead, the visual scene was a pre-computed radiosity rendering 
of two interconnected rooms that include real-world objects. Here, we also statistically compare sensitivity 
resultsderivedfromEllis et al. [30]and the studypresented in this paper. Both studies also explore whether 
relative motion shear between more than one arti.cial object in the VE could be a mechanism contributing 
to observerperception ofhead tracking latency. In summary, results from these studies conducted at NASA 
Ames Research Centre suggest that virtual environment system designers should expect observerswho are 
notburdened with any otherperformancetaskstogenerallybe able to notice di.erences in latency as low as 
15ms, regardless of the relative location of objects in the scene, the meaningfulness of the scene context 
in relation to the real world, or possibly even the degree of photorealism in their rendering. These 
results will also serve as performance guidelines to aid in the design of predictive compensation algorithms. 
APGV  6 Trends from the 7th annual ACM/Eurographics Symposium on Applied Perception in Graph ics and 
Visualization 2010 6.1 APGV Proceedings and Resources In 2001 a small group of researchers gathered in 
Snowbird for a camp.re on Graphics and Perception, there was a certain air of excitement that things 
weregetting started. Thegrowinginterestinthis area,and the realizationthat perceptionisplayinganincreasinglyimportant 
roleingraphics and visualization lead to the establishment of a symposium dedicated to perceptual research 
in graphics and visualization, called APGV -Applied Perception in Graphics and Visualization. The goal 
for this symposium is to have it serve as an inclusive forum where researchers working at the intersection 
ofperception,graphics and visualization can come together to share ideas and results. APGV hopes to provide 
a great opportunity for people not only to acquire new knowledge, but also to seek new partnerships and 
collaborations. Now in it s 7th year papers of the years have represented active interdisciplinary e.orts. 
A wide range of topics have treated . including color, shape, motion, distance judgments, virtual reality, 
and haptics, as well as application areas such as product design and medicine. Please see http://www.apgv.org 
and this years proceedings for further information,[22, 20, 21, 23, 96, 95].  References [1] B.D.Adelstein,T.G.Lee,andS.R.Ellis.Headtrackinglatencyinvirtual 
environments: Psychophysics and a model. In Proc. of the 47th Annual Human Factors and Ergonomics Society 
meeting, pages 2083 2087,2003. [2] Stephane Albin, Gilles Rougeron, Bernard Peroche, and Alain Tremeau. 
Qualityimage metricsfor syntheticimagesbased onperceptual colordifferences. IEEE Transactions on Image 
Processing, 11(9):961 971,2002. [3] Robert S. Allison, Laurence R. Harris, Michael Jenkin, Urszula Jasiobedzka, 
and James E. Zacher. Tolerance of temporal delay in virtual environments. In VR 01: Proceedings of the 
Virtual Reality 2001 Conference(VR 01),page247,Washington, DC, USA, 2001.IEEEComputer Society. [4] D. 
Ariely. Seeing sets: Representation by statistical properties. Psychological Science, 12(2):157 162, 
2001. [5] Ronald Azuma and Gary Bishop. Improving static and dynamic registrationin an optical see-throughhmd. 
In SIGGRAPH 94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques, 
pages 197 204, New York, NY, USA, 1994. ACM. [6] J.H.BaileyandB.G.Witmer.Learning andtransferof spatialknowledge 
in a virtual environment. Proc. of the Human Factors and Ergonomics Society 38th Annual Meeting, pages 
1158 1162, 1994. [7] Dirk Bartz, Douglas Cunningham, Jan Fischer, and Christian Wallraven. Star stateof 
theartreporttheroleofperceptionforcomputergraphics, 2008. [8] JamesP.Bliss,PhilipD.Tidwell,andMichaelA.Guest.Thee.ectiveness 
of virtual reality for administering spatial navigation training to .re.ghters. Presence, 6(1):73 86, 
1997. [9] Mark R. Bolin and Gary W. Meyer. A perceptually based adaptive sampling algorithm. Computer 
Graphics, 32(Annual Conference Series):299 309, August 1998. [10] M.R. Bolin and G.W. Meyer. A frequency 
based ray tracer. In ACM SIGGRAPH 95 Conference Proceedings, pages 409 418, 1995. [11] K.R.Brandt,J.M.Gardiner,andC.N.Macrae.Thedistinctivenesse.ect 
in forenames: The role of subjective experiences and recognition memory. British Journal of Psychology, 
pages 269 280, 2006. [12] K. R. Brandt, C.N. Macrae, A.M. Schloerscheidt, and A. B. Milne. Do i know 
you? target typicality and recollective experience. Memory, 11(1):89 100, 2003. [13] W. Brewer and J. 
Treyens. Role of schemata in memory for places. Cognitive Psychology, 13:207 230, 1981. [14] D.E.BroadbentandM.H.P.Broadbent.Fromdetectiontoidenti.cation: 
Responseto multiple targetsin rapidserial visualpresentation. Perception and Psychophysics, 42(4):105 
113, 1987. [15] T. C. Callaghan. Dimensional interaction of hue and brightness in preattentive .eld 
segregation. Perception &#38; Psychophysics, 36(1):25 34, 1984. [16] T. C. Callaghan. Interference and 
domination in texture segregation: Hue, geometric form, and line orientation. Perception &#38; Psychophysics, 
46(4):299 311,1989. [17] T. C. Callaghan. Interference and dominance in texture segregation. In D. Brogan, 
editor, Visual Search, pages 81 87. Taylor &#38; Francis, New York, New York, 1990.  [18] J. Cataliotti 
and A. Gilchrist. Local and global processes in lightness perception. In Perception and Psychophysics, 
volume 57(2), pages 125 135. Perception, 1995. [19] K. Cater, A. Chalmers, and G. Ward. Detail to attention: 
exploiting visual tasks for selective rendering. In EGRW 03: Proceedings of the 14th Eurographics workshop 
on Rendering, pages 270 280, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association. 
[20] Heinrich Conference Chair-Bultho. and Tom Conference Chair-Troscianko. Apgv 05: Proceedings of 
the 2nd symposium on applied perception in graphics and visualization. A Coro na, Spain, 2005. [21] Roland 
W. Conference Chair-Fleming and Sunghee Conference ChairKim.Apgv 06:Proceedingsof the3rd symposiumonappliedperception 
in graphics and visualization. Boston, Massachusetts, 2006. [22] Victoria Conference Chair-Interrante, 
Ann Conference Chair-McNamara, Heinrich Program Chair-Bultho., and Holly Program Chair-Rushmeier. Apgv 
04: Proceedings of the 1st symposium on applied perception in graphics and visualization. Los Angeles, 
California, 2004. [23] Christian Conference Chair-Wallraven and Veronica Conference Chair-Sundstedt. 
Apgv 07: Proceedings of the 4th symposium on applied perception in graphics and visualization. Tubingen, 
Germany, 2007. [24] et AL. Conway, M. A. Changes in memory awareness during learning: The acquisition 
of knowledge by psychology undergraduates. Journal of Experimental Psychology : General, 126(4):393 413, 
1997. [25] ScottDaly. Thevisibledi.erencespredictor: analgorithmfortheassessment ofimage .delity.In 
Digital images andhuman vision,pages179 206, Cambridge, MA, USA, 1993. MIT Press. [26] Kate Devlin, Alan 
Chalmers, Alexander Wilkie, and Werner Purgathofer. Star: Tone reproduction and physically based spectral 
rendering. In Dieter Fellner and Roberto Scopignio, editors, State of the Art Reports, Eurographics 
2002, pages 101 123, Vienna, September 2002. The Eurographics Association. [27] S. A. Dewhurst, S. J. 
Holmes, K. R. Brandt, and G. M. Dean. Measuring the speed of the conscious components of recognition 
memory: Remembering is faster than knowing. Consciousness and Cognition, 15:147 162, 2006. [28] HuongQ.Dinh,Ne.Walker,ChangSong,AkiraKobayashi,andLarryF. 
Hodges. Evaluatingtheimportance ofmulti-sensoryinput on memory and the sense of presence in virtual environments. 
In VR 99: Proceedings of the IEEE Virtual Reality, page 222, Washington, DC, USA, 1999. IEEE Computer 
Society. [29] George Drettakis, Nicolas Bonneel, Carsten Dachsbacher, Sylvain Lefebvre, Michael Schwarz, 
and Isabelle Viaud-Delmon. An interactiveperceptual rendering pipeline using contrast and spatial masking. 
In Rendering Techniques (Proceedings of the Eurographics Symposium on Rendering). Eurographics, June 
2007. [30] S. R. Ellis, K. Mania, B. D. Adelstein, and M. I. Hill. Generalizability of latency detection 
in a variety of virtual environments. In Proc. of the 48th Annual Human Factors and Ergonomics Society 
meeting, 2004. [31] S. R. Ellis, M. J. Young, S. M. Ehrlich, and B. D. Adelstein. Discrimination of 
changes of rendering latency during voluntary hand movement. In Proc. of the43th AnnualHumanFactors and 
Ergonomics Society meeting, pages 1182 1186, 1999. [32] S.R. Ellis, A. Wolfram, and B.D. Adelstein. Large 
amplitude threedimensional tracking in augmented environments: a human performance trade-o. between 
system latency and update rate. In Proc. of the 46th Annual Human Factors and Ergonomics Society meeting, 
2002. [33] Stephen R. Ellis, F. Breant, Brian M. Menges, Richard H. Jacoby, and Bernard D. Adelstein. 
Operator interaction with visual objects: E.ect of system latency. HCI(2), pages 973 976, 1997. [34] 
Stephen R. Ellis, Mark J. Young, Bernard D. Adelstein, and Sheryl M. Ehrlich. Discrimination ofchangesinlatencyduringhead 
movement. HCI (2), pages 1129 1133, 1999. [35] Land. M. F. Motion and vision: why animals move their 
eyes. Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioural Physiology, 
185(4):341 352,1999. [36] Jean-Philippe Farrugia and Bernard Peroche. A progressive rendering algorithm 
using an adaptive perceptually based image metric. Comput. Graph. Forum, 23(3):605 614,2004. [37] J. 
Ferwerda. Hi-. rendering. ACM Siggraph Eurographics camp.re on perceptually adaptive graphics. http://isg.cs.tcd.ie/camp.re/jimferwerda2.html, 
2001. [38] Philip W. Fink, Patrick S. Foo, and William H. Warren. Obstacle avoidance during walking 
in real and virtual environments. ACM Trans. Appl. Percept., 4(1):2, 2007. [39] R. Flannery, K.A. Walles. 
How does schema theory apply to real versus virtual memories? Cyberspychology and Behavior, 6(2):151 
159, 2003. [40] Harald Frenz, Markus Lappe, Marina Kolesnik, and Thomas Buhrmann. Estimation of travel 
distance from visual motion in virtual environments. ACM Trans. Appl. Percept., 4(1):3, 2007. [41] J. 
M. Gardiner and A. Richardson-klavehn. Remembering and knowing. In: Tulving, E. and Craik, F. I. M., 
eds.Handbook of Memory, 1992. [42] S. Gibson and R.J. Hubbold. E.cient hierarchical re.nement and clustering 
for radiosityin complex environments. Computer Graphics Forum, 15(5):297 310,1996. [43] A. Gilchrist, 
S. Delman, and A. Jacobsen. The classi.cation and integration of edges as critical to the perception 
of re.ectance and illumination. Perception and Psychophysics, 33(5):425 436,1983. [44] A. L. Gilchrist. 
The perception of surface blacks and whites. Scienti.c American, 240(3):88 97, March 1979. [45] AlanGilchristandAlanJacobsen.Perceptionoflightnessandillumination 
in a world of one re.ectance. Perception, 13:5 19, 1984. [46] Baining Guo. Progressive radiance evaluation 
using directional coherence maps. In SIGGRAPH 98: Proceedings of the 25th annual conference on Computer 
graphics and interactive techniques, pages 255 266, New York, NY, USA, 1998. ACM. [47] Jorg Haber, Karol 
Myszkowski, Hitoshi Yamauchi, and Hans-Peter Seidel.Perceptuallyguided correctivesplatting. Computer 
Graphics Forum, 20(3), 2001. [48] Christopher G. Healey and James T. Enns. Building perceptual textures 
to visualize multidimensional datasets. In Proceedings Visualization 98, pages 111 118, Research Triangle 
Park, North Carolina, 1998. [49] Christopher G. Healey and James T. Enns. Large datasets at a glance: 
Combining textures and colors in scienti.c visualization. IEEE Transactions on Visualization and Computer 
Graphics, 5(2):145 167, 1999. [50] David Hedley, Adam Worrall, and Derek Paddon. Selective culling of 
discontinuity lines. In Julie Dorsey and Philipp Slusallek, editors, RenderingTechniques 97(ProceedingsoftheEighthEurographicsWorkshop 
on Rendering), pages 69 80. Springer Wien, 1997. ISBN 3-211-83001-4. [51] R.Held,A.Efsathiouand,andMGreene.Adaptationtodisplacedanddelayed 
visualfeedbackfrom thehand. Journal of Experimental Psychology, 72(6):887 891,1966. [52] L. Huang and 
H. Pashler. A boolean map theory of visual attention. Psychological Review, 114(3):599 631,2007. [53] 
L. Huang, A. Treisman, and H. Pashler. Characterizing the limits of human visual awareness. Science, 
317:823 825,2007. [54] LaurentItti,ChristofKoch, andErnstNiebur.A model of saliency-based visual attentionfor 
rapid scene analysis. IEEETrans.PatternAnal.Mach. Intell., 20(11):1254 1259,1998. [55] B. Julesz. Foundations 
of Cyclopean Perception. University of Chicago Press, Chicago, Illinois, 1971. [56] B. Julesz. Experiments 
in the visual perception of texture. Scienti.c American, 232:34 43, 1975. [57] B. Julesz. A theory of 
preattentive texture discrimination based on .rstorder statistics of textons. Biological Cybernetics, 
41:131 138,1981. [58] B. Julesz, E. N. Gilbert, and L. A. Shepp. Inability of humans to discriminate 
between visual textures that agree in second-order statistics revisited. Perception, 2:391 405, 1973. 
[59] B.Julesz,E.N.Gilbert,andJ.D.Victor.Visualdiscriminationoftextures with identical third-order statistics. 
Biological Cybernetics, 31:137 140, 1978. [60] J. Y. Jung, B. D. Adelstein, and S. R. Ellis. Discriminability 
of prediction artifacts in a time delayed virtual environment. In Proc. of the 44th Annual Human Factors 
and Ergonomics Society meeting, pages 499 502, 2000. [61] Grzegorz Krawczyk, Karol Myszkowski, and Hans-Peter 
Seidel. Lightness perception in tone reproduction for high dynamic range images. In The European Association 
for Computer Graphics 26th Annual Conference EUROGRAPHICS 2005, volume 24 of Computer Graphics Forum, 
pages xx xx, Dublin, Ireland, 2005. Blackwell. [62] Grzegorz Krawczyk, Karol Myszkowski, and Hans-Peter 
Seidel. Computational model of lightness perception in high dynamic range imaging. In Bernice E. Rogowitz, 
Thrasyvoulos N. Pappas, and Scott J. Daly, editors, HumanVision andElectronicImagingXI,IS&#38;TSPIE 
s18thAnnual Symposium on Electronic Imaging (2006), volume xxxx, pages xxx xxx, 2006. [63] Gregory Ward 
Larson, Holly Rushmeier, and Christine Piatko. A Visibility Matching Tone Reproduction Operator for 
High Dynamic Range Scenes. IEEE Transactions on Visualization and Computer Graphics, 3(4):291 306, October 
1997. [64] WilliamB.LathropandMaryK.Kaiser.Perceived orientationinphysical and virtual environments: 
changes in perceived orientation as a function of idiothetic information available. Presence: Teleoper. 
Virtual Environ., 11(1):19 32, 2002. [65] Guillaume Lavoue. A roughness measure for 3d mesh visual masking. 
In APGV 07: Proceedings of the 4th symposium on Applied perception in graphics and visualization, pages 
57 60, New York, NY, USA, 2007. ACM. [66] A. Liu, S. Tharp, L. Lai, French, and L. Stark. Some of what 
one needs toknow about using head-mounteddisplaystoimproveteleoperatorperformance. IEEE Transactions 
on Robotics and Automation, 9(5):638 648, 1995. [67] G.Liu,E.Austen,K.Booth,B.Fischer,M.Rempel,andJ.Enns.Multiple 
objecttrackingisbased on scene, not retinal, coordinates. Journal of Experimental Psychology: Human 
Perception and Performance, 31(2):235 247, 2005. [68] G. R. Loftus and N. H. Mackworth. Cognitive determinants 
of .xation locationduringpicture viewing. Educational Psychology: Human Perception and Performance, 
4(4):565 572, 1978. [69] A. Mack and I. Rock. Inattentional Blindness. MIT Press, Menlo Park, California, 
2000. [70] G.W.MaconcieandL.C.Loschky.Humanperformancewithagazelinked multi-resolutional display. In 
Proceedings of the Advanced Displays and Interactive Displays First Annual Symposium 1997, pages 25 34, 
1997. [71] K. Mania, A. Robinson, and K. Brandt. The e.ect of memory schemata on object recognition in 
virtual environments. Presence, Teleoperators and Virtual Environments, 14(5):606 615, 2005. [72] K. 
Mania, T. Troscianko, R. Hawkes, and A. Chalmers. Fidelity metrics for virtual environmentsimulationsbased 
onhumanjudgments of spatial memory awareness states. Presence, Teleoperators and Virtual Environments, 
12(3):296 310,2003. [73] Katerina Mania, Bernard D. Adelstein, Stephen R. Ellis, and Michael I. Hill. 
Perceptual sensitivity to head tracking latency in virtual environments with varyingdegrees of scene 
complexity.In APGV 04: Proceedings of the 1st Symposium on Applied perception in graphics and visualization, 
pages 39 47, New York, NY, USA, 2004. ACM. [74] Katerina Mania, Shahrul Badariah, Matthew Coxon, and 
Phil Watten. Cognitive transfer of spatial awareness states from immersive virtual environments to reality. 
ACM Trans. Appl. Percept., 7(2):1 14, 2010. [75] Katerina Mania, Dave Wooldridge, Matthew Coxon, and 
Andrew Robinson. The e.ect of visual and interaction .delity on spatial cognition in immersive virtual 
environments. IEEE Trans. Vis. Comput. Graph., 12(3):396 404,2006. [76] Rafal Mantiuk, Karol Myszkowski, 
and Hans-Peter Seidel. A perceptual frameworkfor contrastprocessingofhighdynamic rangeimages.In APGV 
05: Proceedings of the 2nd symposium on Applied perception in graphics and visualization, pages 87 94, 
New York, NY, USA, 2005. ACM. [77] GMarmittandA.Duchowski.Modeling visual attentioninvr: measuring the 
accuracy of predicted scanpaths. Proceedings of Eurographics 2002, Short Presentations, pages 217 226, 
2002. [78] Je.rey McCandless, Stephen R. Ellis, and Bernard D. Adelstein. Localization of a time-delayed, 
monocular virtual object superimposed on a real environment. Presence, 9(1):15 24, 2000. [79] B. H. McCormick, 
T. A. DeFanti, and M. D. Brown. Visualization in scienti.c computing. Computer Graphics, 21(6):1 14, 
1987. [80] A. McNamara. Evaluating image quality metrics vs. human evaluation. ACM SIGGRAPH 2000 Sketches 
Program, 2000. [81] Ann McNamara. Visual perception in realistic image synthesis. Comput. Graph. Forum, 
20(4):211 224,2001. [82] Ann McNamara. Visual perception in realistic image synthesis. Comput. Graph. 
Forum, 20(4):211 224,2001. [83] Michael Meehan, Sharif Razzaque, Mary C. Whitton, and Frederick P. Brooks, 
Jr. E.ect of latency on presence in stressful virtual environments. In VR 03: Proceedings of the IEEE 
Virtual Reality 2003, page 141, Washington, DC, USA, 2003. IEEE Computer Society. [84] G. W. Meyer and 
A. Liu. Color spatial acuity control of a screen subdivision image synthesis algorithm. Human Vision, 
Visual Processing, and Digital Display, 1666(3):387 399,1992. [85] DonP.Mitchell.Generating antialiasedimagesatlowsamplingdensities. 
Computer Graphics, 21(4):65 72, July 1987. [86] Yann Morvan and Carol O Sullivan. A perceptual approach 
to trimming unstructuredlumigraphs.In APGV 07:Proceedingsof the4th symposium on Applied perception in 
graphics and visualization, pages 61 68, New York, NY, USA, 2007. ACM. [87] N. Mourkoussis, F. Rivera, 
T. Troscianko, T. Dixon, R. Hawkes, and K. Mania. Quantifying .delity for virtual environment simulations 
employingmemory schema assumptions. ACM Transactions on Applied Perception, 2010.  [88] K. Myszkowski. 
The visible di.erences predictor: Applications to global illumination problems. In G. Drettakis and N. 
Max, editors, Rendering Techniques 98 (Proceedings of Eurographics Rendering Workshop 98), pages 233 
236, New York, NY, 1998. Springer Wien. [89] K. Myszkowski, A. B. Khodulev, and E. A. Kopylov. Validating 
global illumination algorithms and software. In Visual Proceedings, Technical Sketch at ACM Siggraph 
1997, 1997. [90] U. Neisser. The control of information pickup in selective looking. In A. D. Pick, editor, 
Perception and its Development: A Tribute to Eleanor J. Gibson, pages 201 219. Lawrence Erlbaum and Associates, 
Hilsdale, New Jersey, 1979. [91] K. Nemire, R. H. Jacoby, and S. R. Ellis. Simulation .delity of a virtual 
environment display. Human Factors, 36(1):1994, 1994. [92] Carol O Sullivan, Sarah Howlett, Yann Morvan, 
Rachel McDonnell, and Keith O Conor. Perceptually Adaptive Graphics. In Christophe SchlickandWernerPurgathofer, 
editors, STAR-Proceedings of Eurographics 2004, number STAR-6 in State of the Art Reports, pages 141 
164. INRIA and the Eurographics Association, 2004. [93] A. J. Parkin, J. M. Gardiner, and R. Rosser. 
Functional aspects of recollective experience in face recognition. Consciousness and Cognition, 4(4):387 
398, 1995. [94] S.N.Pattanaik,J.A.Ferwerda,D.A.Greenberg,andM.D.Fairchild.A multiscale model of adaptation 
and spatial vision for realistic imaging. In ComputerGraphics(ACMSIGGRAPH 98Proceedings), pages 287 298, 
1998. [95] Bobby Program Chair-Bodenheimer, Carol Program Chair-O Sullivan, Katerina Conference Chair-Mania, 
and Bernhard Conference Chair-Riecke. Apgv 09: Proceedings of the 6th symposium on applied perception 
in graphics and visualization. Chania, Crete, Greece, 2009. [96] Sarah Program Chair-Creem-Regehr and 
Karol Program Chair-Myszkowski. Apgv 08: Proceedings of the 5th symposium on applied perception in graphics 
and visualization. Los Angeles, California, 2008. [97] P. Rademacher, J. Lengyel, E. Cutrell, and T. 
Whitted. Measuring the perception of visual realism in images, 2001. [98] Ganesh Ramanarayanan, Kavita 
Bala, and James A. Ferwerda. Perception of complex aggregates. In SIGGRAPH 08: ACM SIGGRAPH 2008 papers, 
pages 1 10, New York, NY, USA, 2008. ACM. [99] Ganesh Ramanarayanan, James Ferwerda, Bruce Walter, and 
Kavita Bala. Visual equivalence: towards a new standard for image .delity. In SIGGRAPH 07:ACMSIGGRAPH2007papers,page76,NewYork,NY, 
USA, 2007. ACM. [100] MaheshRamasubramanian,SumantaN.Pattanaik,andDonaldP.Greenberg. A perceptually 
based physical error metric for realistic image synthesis. In SIGGRAPH 99: Proceedings of the 26th annual 
conference on Computer graphics and interactive techniques, pages 73 82, New York, NY, USA, 1999. ACM 
Press/Addison-Wesley Publishing Co. [101] J. E. Raymond, K. L. Shapiro, and K. M. Arnell. Temporary suppression 
of visual processing in an RSVP task: An attentional blink? Journal of Experimental Psychology: Human 
Perception &#38; Performance, 18(3):849 860, 1992. [102] Matthew J. P. Regan, Gavin S. P. Miller, Steven 
M. Rubin, and Chris Kogelnik. A real-time low-latency hardware light-.eld renderer. In SIG-GRAPH 99: 
Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 287 
290,New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co. [103] ErikReinhard,MichaelStark,PeterShirley,andJamesFerwerda.Photographictone 
reproductionfordigitalimages. InSIGGRAPH 02: Proceedings of the 29th annual conference on Computer graphics 
and interactive techniques, pages 267 276, New York, NY, USA, 2002. ACM Press. [104] R. A. Rensink, J. 
K. O Regan, and J. J. Clark. To see or not to see: The need for attention to perceive changes in scenes. 
Psychological Science, 8:368 373, 1997. [105] Ronald A. Rensink. Seeing, sensing, and scrutinizing. Vision 
Research, 40(10-12):1469 1487,2000. [106] James C. Rodger and Roger A. Browse. Choosing rendering parameters 
for e.ective communication of 3d shape. IEEE Computer Graphics and Applications, 20(2):20 28, 2000. [107] 
Thomas B. Sheridan. Remote manipulative control with transmission delay. IEEE Transactions on Human Factors 
in Electronics, 4(1):25 29, 1963. [108] ThomasB.Sheridan.Musingsontelepresenceand virtualpresence. Presence, 
1(1):120 125, 1992. [109] D. J. Simons and R. A. Rensink. Change blindness: Past, present, and future. 
Trends in Cognitive Science, 9(1):16 20, 2005. [110] Daniel J. Simons. Current approaches to change blindness. 
Visual Cognition, 7(1/2/3):1 15,2000. [111] P. H. Smith and J. Van Rosendale. Data and visualization 
corridors report on the 1998 CVD workshop series (sponsored by DOE and NSF). Technical Report CACR-164, 
Center for Advanced Computing Research, California Institute of Technology, 1998. [112] William A. Stokes, 
James A. Ferwerda, Bruce J. Walter, and Donald P. Greenberg. Perceptual illumination compone high quality 
global illumination rendering. ACM Transactions on Graphics, 23(3):742 749,2004. [113] Veronica Sundstedt, 
Efstathios Stavrakis, Michael Wimmer, and Erik Reinhard. A psychophysical study of .xation behavior in 
a computer game. In APGV 08: Proceedings of the 5th symposium on Applied perception in graphics and 
visualization, pages 43 50, New York, NY, USA, 2008. ACM. [114] J. J. Thomas and K. A. Cook. Illuminating 
the Path: Research and Development Agenda for Visual Analytics. IEEE Press, Piscataway, New Jersey, 2005. 
[115] A.Treisman.Preattentiveprocessingin vision. Computer Vision, Graphics, and Image Processing, 31:156 
177,1985. [116] A. Treisman. Search, similarity, and integration of features between and within dimensions. 
Journal of Experimental Psychology: Human Perception &#38; Performance, 17(3):652 676, 1991. [117] A. 
Treisman and S. Gormican. Feature analysis in early vision: Evidence from search asymmetries. Psychological 
Review, 95(1):15 48, 1988. [118] E. Tulving. Elements of Episodic Memory. Oxford : Oxford Science Publications, 
1992. [119] Jack Tumblin and Holly E. Rushmeier. Tone Reproduction for Realistic Images. IEEE Computer 
Graphics and Applications, 13(6):42 48,November 1993. [120] S.UnoandM.Slater.Thesensitivity ofpresencetocollisionresponse.In 
VRAIS 97: Proceedings of the 1997 Virtual Reality Annual International Symposium (VRAIS 97), page 95, 
Washington, DC, USA, 1997. IEEE Computer Society. [121] C.J. van den Branden Lambrecht. Perceptual models 
and architectures for video coding applications. PhD thesis, Ecole Polytechnique Federal de Lausanne, 
1996. [122] DavidWaller,EarlHunt,andDavidKnapp.Thetransferofspatialknowledge in virtual environment 
training. Presence: Teleoper. Virtual Environ., 7(2):129 143, 1998. [123] B.J. Walter. Density estimation 
techniques for global illumination. Ph.D. thesis, Cornell University, 1998. [124] LeonardWanger.Thee.ect 
of shadowquality ontheperceptionof spatial relationships in computer generated imagery. In I3D 92: Proceedings 
of the 1992 symposium on Interactive 3D graphics, pages 39 42, New York, NY, USA, 1992. ACM. [125] Colin 
Ware. Information Visualization: Perception for Design, 2nd Edition. MorganKaufmann Publishers, Inc., 
SanFrancisco,California, 2004. [126] Benjamin Watson, Ne. Walker, Larry F. Hodges, and Martin Reddy. 
An evaluation of level of detail degradation in head-mounted display peripheries. Presence, 6(6):630 
637, 1997. [127] Benjamin Watson, Ne. Walker, Peter Woytiuk, and William Ribarsky. Maintaining usability 
during3dplacementdespite delay. In VR 03: Proceedings of the IEEE Virtual Reality 2003, page 133, Washington, 
DC, USA, 2003. IEEE Computer Society. [128] R. B. Welch, T. T. Blackmon, A. Liu, B. A. Mellers, and L. 
W. Stark. The e.ects of pictorial realism, delay of visual feedback and observer interactivity on the 
subjective sense of presence. Presence, Teleoperators and Virtual Environments, 5(3):263 273, 1996. [129] 
Betsy Williams, Gayathri Narasimham, Claire Westerman, John Rieser, and Bobby Bodenheimer. Functional 
similarities in spatial representations between real and virtual environments. ACM Trans. Appl. Percept., 
4(2):12, 2007. [130] Jeremy M. Wolfe. Guided Search 2.0: A revised model of visual search. Psychonomic 
Bulletin &#38; Review, 1(2):202 238, 1994. [131] Jeremy M. Wolfe and Kyle R. Cave. Deploying visual attention: 
The Guided Search model. In T. Troscianko and A. Blake, editors, AI and the Eye,pages79 103.JohnWiley&#38;Sons,Inc.,Chichester,UnitedKingdom, 
1989. [132] JeremyM.Wolfe,KyleR.Cave,andSusanL.Franzel.GuidedSearch:An alternative to the feature integration 
model for visual search. Journal of Experimental Psychology: Human Perception &#38; Performance, 15(3):419 
433, 1989. [133] P Zimmons. The In.uence of Lighting Quality on Presence and task Performance in Virtual 
Environment, Unpublished PhD thesis. PhD thesis, University of North Carolina, ftp://ftp.cs.unc.edu/pub/publications/techreports/04-017.pdf,2005. 
[134] Alexandros Zotos, Katerina Mania, and Nicholaos Mourkoussis. A schema-based selective rendering 
framework. In APGV 09: Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization, 
pages 85 92, New York, NY, USA, 2009. ACM. IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, 
VOL. 5, NO. 2, APRIL-JUNE 1999   Large Datasets at a Glance: Combining Textures and Colors in Scienti.c 
Visualization Christopher G. Healey and James T. Enns Abstract This paper presents a new method for 
using texture and color to visualize multivariate data elements arranged on an underlying height .eld. 
We combine simple texture patterns with perceptually uniform colors to increase the number of attribute 
values we can display simultaneously. Our technique builds multicolored perceptual texture elements 
(or pexels) to represent each data element. Attribute values encoded in an element are used to vary the 
appearance of its pexel. Texture and color patterns that form when the pexels are displayed can be used 
to rapidly and accurately explore the dataset. Our pexels are built by varying three separate texture 
dimensions: height, density, and regularity. Results from computer graphics, computer vision, and human 
visual psychophysics have identi.ed these dimensions as important for the formation of perceptual texture 
patterns. The pexels are colored using a selection technique that controls color distance, linear separation, 
and color category. Proper use of these criteria guarantees colors that are equally distinguishable 
from one another. We describe a set of controlled experiments that demonstrate the e.ectiveness of our 
texture dimensions and color selection criteria. We then discuss new work that studies how texture and 
color can be used simultaneously in a single display. Our results show that variations of height and 
density have no e.ect on color segmentation, but that random color patterns can interfere with texture 
segmentation. As the di.culty of the visual detection task increases, so too does the amount of color 
on texture interference increase. We conclude by demonstrating the applicability of our approach to a 
real-world problem, the tracking of typhoon conditions in Southeast Asia. Keywords Color, color category, 
experimental design, human vision, linear separation, multivariate dataset, perception, pexel, preattentive 
processing, psychophysics, scienti.c visualization, texture, typhoon I. Introduction T HIS paper investigates 
the problem of visualizing multivariate data elements arrayed across an underlying height .eld. We seek 
a .exible method of displaying effectively large and complex datasets that encode multiple data values 
at a single spatial location. Examples include visualizing geographic and environmental conditions on 
topographical maps, representing surface locations, orientations, and material properties in medical 
volumes, or displaying rigid and rotational velocities on the surface of a three-dimensional object. 
Currently, features like hue, intensity, orientation, motion, and isocontours are used to represent 
these types of datasets. We are investigating the simultaneous use of perceptual textures and colors 
for multivariate visualization. We believe an e.ective combination C. G. Healey is with the Department 
of Computer Science, North Carolina State University, Raleigh, NC 27695-7534. E-mail: healey@csc.ncsu.edu. 
J. T. Enns is with the Department of Psychology, University of British Columbia, Vancouver, British Columbia, 
Canada, V6T 1Z4. E-mail: jenns@psych.ubc.ca. of these features will increase the number of data values 
that can be shown at one time in a single display. To do this, we must .rst design methods for building 
texture and color patterns that support the rapid, accurate, and e.ective visualization of multivariate 
data elements. We use multicolored perceptual texture elements (or pexels) to represent values in our 
dataset. Our texture elements are built by varying three separate texture dimensions: height, density, 
and regularity. Density and regularity have been identi.ed in the computer vision literature as being 
important for performing texture classi.cation [39], [40], [50]. Moreover, results from psychophysics 
have shown that all three dimensions are encoded in the low-level human visual system [1], [28], [51], 
[58]. Our pexels are colored using a technique that supports rapid, accurate, and consistent color identi.cation. 
Three selection criteria are used to choose appropriate colors: color distance, linear separation, and 
named color category. All three criteria have been identi.ed as important for measuring perceived color 
di.erence [3], [4], [14], [31], [60]. One of our real-world testbeds is the visualization of simulation 
results from studies being conducted in the Department of Zoology. Researchers are designing models 
of how they believe salmon feed and move in the open ocean. These simulated salmon are placed in a set 
of known environmental conditions, then tracked to see if their behavior mirrors that of the real .sh. 
A method is needed for visualizing the simulation system. This method will be used to display both static 
(e.g., environmental conditions for a particular month and year) and dynamic results (e.g., a real-time 
display of environmental conditions as they change over time, possibly with the overlay of salmon locations 
and movement). We have approached the problems of dataset size and dimensionality by trying to exploit 
the power of the low-level human visual system. Research in computer vision and human visual psychophysics 
provides insight on how the visual system analyzes images. One of our goals is to select texture and 
color properties that will allow rapid visual exploration, while at the same time minimizing any loss 
of information due to interactions between the visual features being used. Fig. 1 shows an example of 
our technique applied to the oceanographic dataset: environmental conditions in the northern Paci.c Ocean 
are visualized using multicolored pexels. In this display, color represents open-ocean plankton density, 
height represents ocean current strength (taller for stronger), and density represents sea surface temperature 
(denser for warmer). Fig. 1 is only one frame from a much larger time-series of historical ocean conditions. 
Our choice of visual features was guided by experimental re IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER 
GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 1999 current strength gradient (height) dense plankton blooms (colour) 
 temperature gradients (density) Fig. 1. Color, height, and density used to visualize open-ocean plankton 
density, ocean current strength, and sea surface temperature, respectively; low to high plankton densities 
represented with blue, green, brown, red, and purple, stronger currents represented with taller pexels, 
and warmer temperatures represented with denser pexels sults that show how di.erent color and texture 
properties can be used in combination to represent multivariate data elements. Work described in this 
paper is an extension of earlier texture and color studies reported in [22], [23], [25]. We began our 
investigation by conducting a set of controlled experiments to measure user performance and identify 
visual interference that may occur during visualization. Individual texture and color experiments were 
run in isolation. The texture experiments studied the perceptual salience of and interference between 
the perceptual texture dimensions height, density, and regularity. The color experiments measured the 
e.ects of color distance, linear separation, and named color category on perceived color di.erence. Positive 
results from both studies led us to conduct an additional set of experiments that tested the combination 
of texture and color in a single display. Results in the literature vary in their description of this 
task: some researchers have reported that random color variation can interfere signi.cantly with a user 
s ability to see an underlying texture region [8], [9], [49], while others have found no impact on performance 
[53], [58]. Our investigation extends this earlier work on two-dimensional texture patterns into an 
environment that displays height .elds using perspective projections. To our knowledge, no one has studied 
the issue of color on texture or texture on color interference during visualization. Results from our 
experiments showed that we could design an environment in which color variations caused a small but 
statistically reliable interference e.ect during texture segmentation. The strength of this e.ect depends 
on the di.culty of the analysis task: tasks that are more di.cult are more susceptible to color interference. 
Texture variation, on the other hand, caused no interference during color segmentation. We are using 
these results to build a collection of pexels that allow a viewer to visually explore a multivariate 
dataset in a rapid, accurate, and relatively e.ortless fashion. We begin with a description of results 
from computer vision, computer graphics, and psychophysics that discuss methods for identifying and 
controlling the perceptual properties of texture and color. Next, we describe an area of human psychophysics 
concerned with modeling control of visual attention in the low-level human visual system. We discuss 
how the use of visual stimuli that control attention can lead to signi.cant advantages during visualization. 
Section4 gives anoverview ofthe experimentsweusedto build and test our perceptual texture elements. In 
Section 5, we discuss how we chose to select and test our perceptual colors. Following this, we describe 
new experiments designed to study the combined use of texture and color. Finally, we report on practical 
applications of our research in Section 7, then discuss avenues for future research in Section 8. II. 
Related Work Researchers from many di.erent areas have studied texture and color in the context of 
their work. Before we discuss our own investigations, we provide an overview of results in the literature 
that are most directly related to our interests. A. Texture The study of texture crosses many disciplines, 
including computer vision, human visual psychophysics, and computer graphics. Although each group focuses 
on separate problems (texture segmentation and classi.cation in computer vision, modeling the low-level 
human visual system in psychophysics, and information display in graphics) they each need ways to describe 
accurately the texture patterns being classi.ed, modeled, or displayed. [41] describes two HEALEY AND 
ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES AND COLORS IN SCIENTIFIC VISUALIZATION general classes 
of texture representation: statistical models that use convolution .lters and other techniques to measure 
variance, inertia, entropy, or energy, and perceptual models that identify underlying perceptual texture 
dimensions like contrast, size, regularity, and directionality. Our current texture studies focus on 
the perceptual features that make up a texture pattern. In our work we demonstrate that we can use texture 
attributes to assist in visualization, producing displays that allow users to rapidly and accurately 
explore their data by analyzing the resulting texture patterns. Di.erent methods have been used to identify 
and investigate the perceptual features inherent in a texture pattern. Bela Julesz [27], [28] has conducted 
numerous psychophysical experiments to study how a texture s .rst, second, and third-order statistics 
a.ect discrimination in the low-level visual system. This led to the texton theory [29], which proposes 
that early vision detects three types of features (or textons, as Julesz called them): elongated blobs 
with speci.c visual properties (e.g., hue, orientation, or length), ends of line segments, and crossing 
of line segments. Other psychophysical researchers have studied how visual features like color, orientation, 
and form can be used to rapidly and accurately segment collections of elements into spatially coherent 
regions [7], [8], [52], [58], [59]. Work in psychophysics has also been conducted to study how texture 
gradients are used to judge an object s shape. Cutting and Millard discuss how di.erent types of gradients 
a.ect a viewer s perception of the .atness or curvature of an underlying 3D surface [13]. Three texture 
gradients were tested: perspective, which refers to smooth changes in the horizontal width of each texture 
element, compression, which refers to changes in the height to width ratio of a texture element, and 
density, which refers to changes in the number of texture elements per unit of solid visual angle. For 
most surfaces the perspective and compression gradients decrease with distance, while the density gradient 
increases. Cutting and Millard found that viewers use perspective and density gradients almost exclusively 
to identify the relative slant of a .at surface. In contrast, the compression gradient was most important 
for judging the curvature of undulating surfaces. Later work by Aks and Enns on overcoming perspective 
foreshortening in early vision also discussed the e.ects of texture gradients on the perceived shape 
of an underlying surface [1]. Work in computer vision is also interested in how viewers segment images, 
in part to try to develop automated texture classi.cation and segmentation algorithms. Tamura et al. 
and Rao and Lohse identi.ed texture dimensions by conducting experiments that asked observers to divide 
pictures depicting di.erent types of textures (Brodatz images) into groups [39], [40], [50]. Tamura 
et al. used their results to propose methods for measuring coarseness, contrast, directionality, line-likeness, 
regularity, and roughness. Rao and Lohse used multidimensional scaling to identify the primary texture 
dimensions used by their observers to group images: regularity, directionality, and complexity. Haralick 
built grayscale spatial dependency matrices to identify features like homogeneity, contrast, and linear 
dependency [21]. These features were used to classify satellite images into categories like forest, woodlands, 
grasslands, and water. Liu and Picard used Wold features to synthesize texture patterns [35]. A Wold 
decomposition divides a 2D homogeneous pattern (e.g., a texture pattern) into three mutually orthogonal 
components with perceptual properties that roughly correspond to periodicity, directionality, and randomness. 
Malik and Perona designed computer algorithms that use orientation .ltering, nonlinear inhibition, and 
computation of the resulting texture gradient to mimic the discrimination ability of the low-level human 
visual system [37]. Researchers in computer graphics are studying methods for using texture to perform 
tasks such as representing surface shape and extent, displaying .ow patterns, identifying spatially 
coherent regions in high-dimensional data, and multivariate visualization. Schweitzer used rotated discs 
to highlight the shape and orientation of a three-dimensional surface [47]. Grinstein et al. created 
a system called EXVIS that uses stick-men icons to produce texture patterns that show spatial coherence 
in a multivariate dataset [19]. Ware and Knight used Gabor .lters to construct texture patterns; attributes 
in an underlying dataset are used to modify the orientation, size, and contrast of the Gabor elements 
during visualization [57]. Turk and Banks described an iterated method for placing streamlines to visualize 
twodimensional vector .elds [54]. Interrante displayed texture strokes to help show three-dimensional 
shape and depth on layered transparent surfaces; principal directions and curvatures are used to orient 
and advect the strokes across the surface [26]. Salisbury et al. used texturing techniques to build computer-generated 
pen-and-ink drawings that convey a realistic sense of shape, depth, and orientation [46]. Finally, Laidlaw 
described two methods for displaying a 2D di.use tensor image with seven values at each spatial location 
[32]. The .rst method used the shape of normalized ellipsoids to represent individual tensor values. 
The second used techniques from oil painting [38] to represent all seven values simultaneously via multiple 
layers of varying brush strokes. Visualization techniques like EXVIS [19] are sometimes referred to as 
glyph-based methods. Glyphs are graphical icons with visual features like shape, orientation, color, 
and size that are controlled by attributes in an underlying dataset. Glyph-based techniques range from 
representation via individual icons to the formation of texture and color patterns through the overlay 
of many thousands of glyphs. Initial work by Cherno. suggested the use of facial characteristics to 
represent information in a multivariate dataset [6], [10]. A face is used to summarize a data element; 
individual data values control features in the face like the nose, eyes, eyebrows, mouth, and jowls. 
Foley and Ribarsky have created a visualization tool called Glyphmaker that can be used to build visual 
representations of multivariate datasets in an e.ective, interactive fashion [16]. Glyphmaker uses a 
glyph editor and glyph binder to create glyphs, to arrange them spatially, and to bind attributes to 
their visual IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 1999 
properties. Levkowitz described a prototype system for combining colored squares to produce patterns 
to represent an underlying multivariate dataset [33]. Other techniques such as the normalized ellipsoids 
of Laidlaw [32], the Gabor elements of Ware [57], or even the pexels described in this paper might also 
be classi.ed as glyphs, although we prefer to think of them as texture-based visualization methods. B. 
Color As with texture, color has a rich history in the areas of computer graphics and psychophysics. 
In graphics, researchers have studied issues related to color speci.cation, color perception, and the 
selection of colors for information representation during visualization. Work in psychophysics describes 
how the human visual system mediates color perception. A number of di.erent color models have been built 
in computer graphics to try to support the unambiguous speci.cation of color [60]. These models are 
almost always three-dimensional, and are often divided into a devicedependent class, where a model represents 
the displayable colors of a given output device, and a device-independent class, where a model provides 
coordinates for colors from the visible color spectrum. Common examples of devicedependent models include 
monitor RGB and CMYK. Common examples of device-independent models include CIE XYZ, CIE LUV, and CIE 
Lab. Certain models were designed to provide additional functionality that can be used during visualization. 
For example, both CIE LUV and CIE Lab provide rough perceptual uniformity; that is, the Euclidean distance 
between a pair of colors speci.ed in these models roughly corresponds to perceived color di.erence. These 
models also provide a measure of isoluminance, since their L-axis is meant to correspond to perceived 
brightness. Previous work has also addressed the issue of constructing color scales for certain types 
of data visualization. For example, Ware and Beatty describe a simple color visualization technique 
for displaying correlation in a .vedimensional dataset [56]. Ware has also designed a method for building 
continuous color scales that control color surround e.ects [55]. The color scales use a combination 
of luminance and hue variation that allows viewers to determine the value associated with a speci.c 
color, and to identify the spatial locations of peaks and valleys (i.e., to see the shape) in a 2D distribution 
of an attribute s values. Controlling color surround ensures a small, near-constant perceptual error 
e.ect from neighboring colors across the entire range of the color scale. Robertson described user interface 
techniques for visualizing the range of colors a display device can support using perceptual color models 
[44]. Rheingans and Tebbs have built a system that allows users to interactively construct a continuous 
color scale by tracing a path through a 3D color model [43]. This technique allows users to vary how 
di.erent values of an attribute map onto the color path. Multiple colors can be used to highlight areas 
of interest within an attribute, even when those areas constitute only a small fraction of the attribute 
s full range of allowable values. Levkowitz and Herman designed a locally optimal color scale that maximizes 
the just-noticeable color di.erence between neighboring pairs of colors [34]. The result is a signi.cantly 
larger number of just-noticeably di.erent colors in their color scales, compared to standard scales 
like red-blue, rainbow, or luminance. Recent work at the IBM Thomas J. Watson Research Center has focused 
on a rule-based visualization tool [45]. Initial research addressed the need for rules that take into 
account how a user perceives visual features like hue, luminance, height, and so on. These rules are 
used to guide or restrict a user s choices during data-feature mapping. The rules use various metadata, 
for example, the visualization task being performed, the visual features being used, and the spatial 
frequency of the data being visualized. A speci.c example of one part of this system is the colormap 
selection tool from the IBM Visualization Data Explorer [5]. The selection tool uses perceptual rules 
and metadata to limit the choice of colormaps available to the user. Finally, psychophysicists have been 
working to identify properties that a.ect perceived color di.erence. Two important discoveries include 
the linear separation [3], [4], [14] and color category [31] e.ects. The linear separation theory states 
that if a target color can be separated from all the other background colors being displayed with a single 
straight line in color space, it will be easier to detect (i.e., its perceived di.erence from all the 
other colors will increase) compared to a case where it can be formed by a linear combination of the 
background colors. D Zmura s initial work on this phenomena [14] showed that a target color could be 
rapidly identi.ed in a sea of background elements uniformly colored one of two colors (e.g., an orange 
target could be rapidly identi.ed in a sea of red elements, or in a sea of yellow elements). The same 
target, however, was much more di.cult to .nd when the background elements used both colors simultaneously 
(e.g., an orange target could not be rapidly identi.ed in a sea of red and yellow elements). This second 
case is an example of a target color (orange) that is a linear combination of its background colors 
(red and yellow). The color category e.ect suggests that the perceived di.erence between a pair of colors 
increases when the two colors occupy di.erent named color regions (i.e., one lies in the blue region 
and one lies in the purple region, as opposed to both in blue or both in purple). We believe both results 
may need to be considered to guarantee perceptual uniformity during color selection. C. Combined Texture 
and Color Although texture and color have been studied extensively in isolation, much less work has 
focused on their combined use for information representation. An e.ective method of displaying color 
and texture patterns simultaneously would increase the number of attributes we can represent at one 
time. The .rst step towards supporting this goal is the determination of the amount of visual interference 
that occurs between these features during visualization. HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: 
COMBINING TEXTURES AND COLORS IN SCIENTIFIC VISUALIZATION Experiments in psychophysics have produced 
interesting but contradictory answers to this question. Callaghan reported asymmetric interference of 
color on form during texture segmentation: a random color pattern interfered with the identi.cation of 
a boundary between two groups of di.erent forms, but a random form pattern had no e.ect on identifying 
color boundaries [8], [9]. Triesman, however, showed that random variation of color had no e.ect on detecting 
the presence or absence of a target element de.ned by a di.erence in orientation (recall that directionality 
has been identi.ed as a fundamental perceptual texture dimension) [53]. Recent work by Snowden [49] 
recreated the differing results of both Callaghan and Triesman. Snowden ran a number of additional experiments 
to test the e.ects of random color and stereo depth variation on the detection of a target line element 
with a unique orientation. As with Callaghan and Triesman, results di.ered depending on the target type. 
Search for a single line element was rapid and accurate, even with random color or depth variation. 
Search for a spatial collection of targets was easy only when color and depth were .xed to a constant 
value. Random variation of color or depth produced a signi.cant reduction in detection accuracy. Snowden 
suggests that the visual system wants to try to group spatially neighboring elements with common visual 
features, even if this grouping is not helpful for the task being performed. Any random variation of 
color or depth interferes with this grouping process, thereby forcing a reduction in performance. These 
results leave unanswered the question of whether color variation will interfere with texture identi.cation 
during visualization. Moreover, work in psychophysics studied two-dimensional texture segmentation. 
Our pexels are arrayed over an underlying height .eld, then displayed in 3D using a perspective projection. 
Most of the research to date has focused on color on texture interference. Less work has been conducted 
to study how changes in texture dimensions like height, density, or regularity will a.ect the identi.cation 
of data elements with a particular target color. The question of interference in this kind of height-.eld 
environment needs to be addressed before we can recommend methods for the combined use of color and 
texture. III. Perceptual Visualization An important requirement for any visualization technique is 
a method for rapid, accurate, and e.ortless visual exploration. We address this goal by using what is 
known about the control of human visual attention as a foundation for our visualization tools. The individual 
factors that govern what is attended in a visual display can be organized along two major dimensions: 
bottom-up (or stimulus driven) versus top-down (or goal directed). Bottom-up factors in the control of 
attention include the limited set of features that psychophysicists have identi.ed as being detected 
very quickly by the human visual system, without the need for search. These features are often called 
preattentive, because their detection occurs rapidly and accurately, usually in an amount of time independent 
of the total number of elements being displayed. When applied properly, preattentive features can be 
used to perform di.erent types of exploratory analysis. Examples include searching for data elements 
with a unique visual feature, identifying the boundaries between groups of elements with common features, 
tracking groups of elements as they move in time and space, and estimating the number of elements with 
a speci.c feature. Preattentive tasks can be performed in a single glance, which corresponds to 200 milliseconds 
(ms) or less. As noted above, the time required to complete the task is independent of the number of 
data elements being displayed. Since the visual system cannot choose to refocus attention within this 
timeframe, users must complete their task using only a single glance at the image. Fig. 2 shows examples 
of both types of target search. In Fig. 2a-2d the target, a red circle, is easy to .nd. Here, the target 
contains a preattentive feature unique from the background distracters: color (red versus blue) or shape 
(circle versus square). This unique feature is used by the low-level visual system to rapidly identify 
the presence or absence of the target. Unfortunately, an intuitive combination of these results can 
lead to visual interference. Fig. 2e and 2f simulate a two-dimensional dataset where one attribute is 
encoded with color (red or blue), and the other is encoded with shape (circle or square). Although these 
features worked well in isolation, searching for a red circle target in a sea of blue circles and red 
squares is signi.cantly more di.cult. In fact, experiments have shown that search time is directly proportional 
to the number of elements in the display, suggesting that viewers are searching small subgroups of elements 
(or even individual elements themselves) to identify the target. In this example the low-level visual 
system has no unique feature to search for, since circular elements (blue circles) and red elements (red 
squares) are also present in the display. The visual system cannot integrate preattentively the presence 
of multiple visual features (circular and red) at the same spatial location. This is a very simple example 
of a situation where knowledge of preattentive vision would have allowed us to avoid displays that actively 
interfere with our analysis task. In spite of the perceptual salience of the target in Fig. 2a2d, bottom-up 
in.uences cannot be assumed to operate independently of the current goals and attentional state of the 
observer. Recent studies have demonstrated that many of the bottom-up factors only in.uence perception 
when the observer is engaged in a task in which they are expected or task-relevant (see the review by 
[15]). For example, a target de.ned as a color singleton will pop out of a display only when the observer 
is looking for targets de.ned by color. The same color singleton will not in.uence perception when observers 
are searching exclusively for luminance de.ned targets. Sometimes observers will fail completely to see 
otherwise salient targets in their visual .eld, either because they are absorbed in the performance of 
a cognitively-demanding task [36], there are a multitude of other simultaneous salient visual events 
[42], or because the salient event occurs during an eye movement or other change in viewpoint [48]. Therefore, 
the control of atten IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 
1999 (a) (b) (c) (d) (e) (f)   Fig. 2. Examples of target search: (a, b) identifying a red target 
in a sea of blue distracters is rapid and accurate, target absent in (a), present in (b); (c, d) identifying 
a red circular target in a sea of red square distracters is rapid and accurate, target present in (c), 
absent in (d); (e, f) identifying the same red circle target in a combined sea of blue circular distracters 
and red square distracters is signi.cantly more di.cult, target absent in (e), present in (f) HEALEY 
AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES AND COLORS IN SCIENTIFIC VISUALIZATION Fig. 
3. A background array of short, sparse, regular pexels; the lower and upper groups on the left contain 
irregular and random pexels, respectively; the lower and upper groups in the center contain dense and 
very dense pexels; the lower and upper groups to the right contain medium and tall pexels tion must always 
be understood as an interaction between bottom-up and top-down mechanisms. Our research is focused on 
identifying relevant results in the vision and psychophysical literature, then extending these results 
and integrating them into a visualization environment. Tools that make use of preattentive vision offer 
a number of important advantages during multivariate visualization: 1. Visual analysis is rapid, accurate, 
and relatively e.ortless since preattentive tasks can be completed in 200 ms or less. We have shown 
that tasks performed on static displays extend to a dynamic environment where data frames are shown 
one after another in a movie-like fashion [24] (i.e., tasks that can be performed on an individual display 
in 200 ms can also be performed on a sequence of displays shown at .ve frames a second). 2. The time 
required for task completion is independent of display size (to the resolution limits of the display). 
This means we can increase the number of data elements in a display with little or no increase in the 
time required to analyze the display. 3. Certain combinations of visual features cause interference 
patterns that mask information in the low-level visual system. Our experiments are designed to identify 
these situations. This means our visualization tools can be built to avoid data-feature mappings that 
might interfere with the analysis task.  Since preattentive tasks are rapid and insensitive to display 
size, we believe visualization techniques that make use of these properties will support high-speed exploratory 
analysis of large, multivariate datasets. Care must be taken, however, to ensure that we choose data-feature 
mappings that maximize the perceptual salience of all the data attributes being displayed. We are currently 
investigating the combined use of two important and commonly used visual features: texture and color. 
Previous work in our laboratory has identi.ed methods for choosing perceptual textures and colors for 
multivariate visualization. Results from vision and psychophysics on the simultaneous use of both features 
are mixed: some researchers have reported that background color patterns mask texture information, and 
vice-versa, while others claim that no interference occurs. Experiments reported in this paper are designed 
to test for colortexture interactions during visualization. A lack of interference would suggest that 
we could combine both features to simultaneously encode multiple attributes. The presence of interference, 
on the other hand, would place important limitations on the way in which visual attributes should be 
mapped onto data attributes. Visualization tools based on these .ndings would then be able to display 
textures with the appropriate mapping of data dimensions to visual attributes. IV. Pexels One of the 
main goals of multivariate visualization is to display multiple attribute values at a single spatial 
location without overwhelming the user s ability to comprehend the resulting image. Researchers in vision, 
psychophysics, and graphics have been studying how the visual system analyzes texture patterns. We wanted 
to know whether perceptual texture dimensions could be used to represent multivariate data elements during 
visualization. To this end, we designed a set of perceptual texture elements, or pexels, that support 
the variation of three separate texture dimensions: density, regularity, and height. Density and regularity 
have been identi.ed in the literature as primary texture dimensions [39], [40], [50]. Although height 
might not be considered an intrinsic textural cue , we note that height is one aspect of element size, 
and that size is an important property of a texture pattern. Results from psychophysical experiments 
have shown that di.erences in IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, 
APRIL-JUNE 1999   (a) (b) (c) Fig. 4. Three displays of pexels with di.erent regularity and a 5  
3 patch from the center of the corresponding autocorrelation graphs: (a) a completely regular display, 
resulting in sharp peaks of height 1.0 at regular intervals in the autocorrelation graph; (b) a display 
with irregularly-spaced pexels, peaks in the graph are reduced to a maximum height between 0.3 and 0.4; 
(c) a display with randomly-spaced pexels, resulting in a completely .at graph except at (0,0) and where 
underlying grid lines overlap height are detected preattentively [51], moreover, viewers properly correct 
for perspective foreshortening when they perceive that elements are being displayed in 3D [1]. We wanted 
to build three-dimensional pexels that sit up on the underlying surface. This allows for the possibility 
of applying various orientations (another important texture dimension) to a pexel. Our pexels look like 
a collection of one or more upright paper strips. Each element in the dataset is represented by a single 
pexel. The user maps attributes in their dataset to density (which controls the number of strips in a 
pexel), height, and regularity. The attribute values for a particular element can then control the appearance 
of its pexel. When all the pexels for a particular data frame are displayed, they form texture patterns 
that represent the underlying structure of the dataset. Fig. 3 shows an example of regularity, density, 
and height varied across three discrete values. Each pexel in the original array (shown in gray) is 
short, sparse, and regular. The lower and upper patches on the left of the array (shown in black) contain 
irregular and random pexels, respectively. The lower and upper patches in the middle of the array contain 
dense and very dense pexels. The lower and upper patches on the right contain medium and tall pexels. 
A. Ordering Texture Dimensions In order to use height, density, and regularity during visualization, 
we needed an ordinal ranking for each dimension. Height and density both have a natural ordering: shorter 
comes before taller, and sparser comes before denser. Although viewers can often order regularity intuitively, 
we required a more formal method for measurement. We chose to use autocorrelation to rank regularity. 
This technique measures the second-order statistic of a texture pattern. Psychophysicists have reported 
that a change in regularity produces a corresponding change in a texture s second order statistic [27], 
[28], [30]. Intuitively, autocorrelating an image shifts two copies of the image on top of one another, 
to see how closely they can be matched. If the texture is made up of a regular, repeating pattern it 
can be shifted in various ways to exactly overlap with itself. As more and more irregularity is introduced 
into the texture, the amount of overlap decreases, regardless of where we shift the copies. Consider 
two copies of an image A and B,each witha widthof N and a height of M pixels. The amount of autocorrelation 
that occurs when A is overlaid onto B at o.set (t, u)is: 1 NM .. C(t, u)= (A[x, y] - A)(B[x + t, y + 
u] - B)(1) K x=1 y=1 . . K = NM .2(A) .2(B) (2) 1 NM .. A = A[x, y] (3) NM x=1 y=1 .2(A)= 1 NM N . M 
. (A[x, y] - A)2 (4) x=1 y=1 with B and .2(B) computed in a similar fashion. Elements in A that do not 
overlap with B are wrapped to the opposite side of B (i.e., elements in A lying above the top of B wrap 
back to the bottom, elements lying below the bottom of B wrap back to the top, similarly for elements 
to the left or right of B). As a practical example, consider Fig. 4a (pexels on a regular underlying 
grid), Fig. 4b (pexels on an irregular grid), HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING 
TEXTURES AND COLORS IN SCIENTIFIC VISUALIZATION and Fig. 4c (pexels on a random grid). Irregular and 
random pexels are created by allowing each strip in the pexel to walk a random distance (up to .xed 
maximum) in a random direction from its original anchor point. Autocorrelation was computed on the orthogonal 
projection of each image. A 5  3 patch from the center of the corresponding autocorrelation graph is 
shown beneath each of the three grids. Results in the graphs mirror what we see in each display, that 
is, as randomness increases, peaks in the autocorrelation graph decrease in height. In Fig. 4a peaks 
of height 1.0 appear at regular intervals in the graph. Each peak represents a shift that places pexels 
so they exactly overlap with one another. The rate of increase towards each peak di.ers in the vertical 
and horizontal directions because the elements in the graph are rectangles (i.e., taller than they are 
wide), rather than squares. In Fig. 4b, the graph has the expected sharp peak at (0,0). It also has gentle 
peaks at shift positions that realign the grid with itself. The peaks are not as high as for the regular 
grid, because the pexels no longer align perfectly with one another. The sharp vertical and horizontal 
ridges in the graph represent positions where the underlying grid lines exactly overlap with one another 
(the grid lines showing the original position of each pexel are still regular in this image). The height 
of each gentle peak ranges between 0.3 and 0.4. Increasing randomness reduces again the height of the 
peaks in the correlation graph. In Fig. 4c, no peaks are present, apart from (0,0) and the sharp ridges 
that occur when the underlying grid lines overlap with one another. The resulting correlation values 
suggests that this image is more random than either of its predecessors. Informal tests with a variety 
of regularity patterns showed a nearperfect match between user-chosen rankings and rankings based on 
our autocorrelation technique. Autocorrelation on the perspective projections of each grid could also 
be computed. The tall peaks and .attened results would be similar to those in Fig. 4, although the density 
of their spacing would change near the top of the image due to perspective compression and foreshortening. 
B. Pexel Salience and Interference We conducted experiments to test the ability of each texture dimension 
to display e.ectively an underlying data attribute during multivariate visualization. To summarize, 
our experiments were designed to answer the following three questions: 1. Can the perceptual dimensions 
of density, regularity, and height be used to show structure in a dataset through the variation of a 
corresponding texture pattern? 2. How can we use the dataset s attributes to control the values of each 
perceptual dimension? 3. How much visual interference occurs between each of the perceptual dimensions 
when they are displayed simultaneously?  C. Experiments We designed texture displays to test the detectability 
of six di.erent target types: taller, shorter, denser, sparser, more regular, and more irregular. For 
each target type, a number of parameters were varied, including exposure duration, texture dimension 
salience, and visual interference. For example, during the taller experiment, each display showed a 20 
 15 array of pexels rotated 45. about the Xaxis. Observers were asked to determine whether the array 
contained a group of pexels that were taller than all the rest. The following conditions varied: target-background 
pairing: some displays showed a medium target in a sea of short pexels, while others showed a tall target 
in a sea of medium pexels; this allowed us to test whether some target de.ning attributes were generally 
more salient than others,  secondary texture dimension: displays contained either no background variation 
(every pexel was sparse and regular), a random variation of density across the array, or a random variation 
of regularity across the array; this allowed us to test for background interference during target search, 
 exposure duration: displays were shown for 50, 150, or 450 ms; this allowed us to test for a reduction 
in performance when exposure duration was decreased, and  target patch size: target groups were either 
2  2 pexels or 44 pexels in size; this allowed us to test for a reduction in performance for smaller 
target patches.  The heights, densities, and regularities we used were chosen through a set of pilot 
studies. Two patches were placed side-by-side, each displaying a pair of heights, densities, or regularities. 
Viewers were asked to answer whether the patches were di.erent from one another. Response times for correct 
answers were used as a measure of performance. We tested a range of values for each dimension, although 
the spatial area available for an individual pexel during our experiments limited the maximum amount 
of density and irregularity we were able to display. The .nal values we chose could be rapidly and accurately 
di.erentiated in this limited setting. The experiments that tested the other .ve target types (shorter, 
denser, sparser, regular, and irregular) were designed in a similar fashion, with one exception. Experiments 
testing regularity had only one target-background pairing: a target of regular pexels in a sea of random 
pexels (for the regular experiment), or a target of random pexels in a sea of regular pexels (for the 
irregular experiment). The pilot studies used to select values for each dimension showed that users had 
great di.culty discriminating an irregular patch from a random patch. This was due in part to the small 
spatial area available to each pexel. Our pilot studies produced experiments that tested three separate 
heights (short, medium, and tall), three separate densities (sparse, dense, and very dense) and two separate 
regularities (regular and random). Examples of two display types (taller and regular) are shown in Fig. 
5. Both displays include target pexels. Fig. 5a contains a 2  2target group of medium pexels in a sea 
of short pexels. The density of each pexel varies across the array, producing an underlying density pattern 
that is clearly visible. This display type simulates two dimensional data elements being IEEE TRANSACTIONS 
ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 1999 (a) (b)   Fig. 5. Two display 
types from the taller and regular pexel experiments: (a) a target of medium pexels in a sea of short 
pexels with a background density pattern (2  2 target group located left of center); (b) a target of 
regular pexels in a sea of irregular pexels with no background texture pattern (2  2 target group located 
3 grid steps right and 7 grid steps up from the lower-left corner of the array) visualized with height 
as the primary texture dimension and density as the secondary texture dimension. Recall that the number 
of paper strips in a pexel depends on its density. Since three of the target pexels in Fig. 5a are dense, 
they each display two strips. The remaining pexel is sparse, and therefore displays a only single strip. 
Fig. 5b contains a 2  2 target group of regular pexels in a sea of random pexels, with a no background 
texture pattern. The taller target in Fig. 5a is very easy to .nd, while the regular target in Fig. 5b 
is almost invisible. D. Results Detection accuracy data were analyzed using a multifactor analysis of 
variance (ANOVA). A complete description of our analysis and statistical .ndings is available in [22], 
[23], [25]. In summary, we found: 1. Taller target regions were identi.ed rapidly (i.e., 150 ms or less) 
with very high accuracy (approximately 93%); background density and regularity patterns produced no signi.cant 
interference. 2. Shorter, denser, and sparser targets were more di.cult to identify than taller targets, 
although good results were obtained at both 150 and 450 ms (82.3%, 94.0%, and 94.7% for shorter, denser, 
and sparser targets with no background variation at 150 ms). This was not surprising, since similar 
 results have been documented by [51] and [1] using displays of texture on a two-dimensional plane. 3. 
Background variation in non-target attributes produced small, but statistically signi.cant, interference 
e.ects. These e.ects tended to be largest when target detectability was lowest. For example, density 
and regularity interfered with searching for shorter targets; height and regularity interfered with 
searching for sparser targets; but only height interfered with the (easier to .nd) denser targets. 4. 
Irregular target regions were di.cult to identify at 150 and 450 ms, even with no secondary texture pattern 
(approximately 76%). Whether this accuracy level is su.ciently high will depend on the application. 
In contrast, regular regions were invisible under these conditions; the percentage of correct responses 
approached chance (i.e., 50%) in every condition. (a) (b)    Fig. 6. Two displays with a regular 
target, both displays should be compared with the target shown in Fig. 5b: (a) larger target, an 8  
8 target in a sea of sparse, random pexels; (b) denser background, a 22 target in a sea of dense, random 
pexels (target group located right of center) Our poor detection results for regularity were unexpected, 
particularly since vision algorithms that perform texture classi.cation use regularity as one of their 
primary decision criteria [35], [39], [40], [50]. We con.rmed that our results were not due to a di.erence 
in our de.nition of regularity; the way we produced irregular patches matches the methods described by 
[20], [28], [30], [39], [40], [50]. It may be that regularity is important for classifying di.erent 
textures, but not for the type of texture segmentation that we are performing. Informal post-experiment 
investigations showed that we could improve the salience of a reg HEALEY AND ENNS: LARGE DATASETS AT 
A GLANCE: COMBINING TEXTURES AND COLORS IN SCIENTIFIC VISUALIZATION  55.9% 68.6% 53.7% 58.5% 49.3% 
76.8%  Regularity Density Height None 80.4% 68.8% 64.1% 77.2% 93.8% 93.4% Background: Taller Shorter 
Denser Sparser Regular Random Target:  Fig. 7. A table showing the percentage of correct responses 
for each target-background pairing; target type along the horizontal axis, background type along the 
vertical axis; darker squares represent pairings with a high percentage of correct responses; blank entries 
with diagonal slashes indicate target-background pairings that do not exist ular (or irregular) patch 
by increasing its size (Fig. 6a), or by increasing the minimum pexel density to be very dense (Fig. 6b). 
However, neither of these solutions is necessarily useful. There is no way to guarantee that data values 
will cluster together to form the large spatial regions needed for regularity detection. If we constrain 
density to be very dense across the array, we lose the ability to vary density over an easily identi.able 
range. This reduces the dimensionality of our pexels to two (height and regularity), producing a situation 
that is no better than when regularity is di.cult to identify. Because of this, we normally choose to 
display an attribute with low importance using regularity. While di.erences in regularity cannot be 
detected consistently by the low-level visual system, in many cases users may be able to see the changes 
when areas of interest in the dataset are identi.ed and analyzed in a focused or attentive fashion. Fig. 
7 shows average subject performance as a table representing each target-background pair. Target type 
varies along the horizontal axis, while background type varies along the vertical axis. Darker squares 
represent targetbackground pairings with highly accurate subject performance. The number in the center 
of each square reports the percentage of correct responses averaged across all subjects. V. Perceptual 
Colors In addition to our study of pexels, we have examined methods for choosing multiple individual 
colors. These experiments were designed to select a set of n colors such that: 1. Any color can be detected 
preattentively, even in the presence of all the others. 2. The colors are equally distinguishable from 
one another; that is, every color is equally easy to identify.  We also tested for the maximum number 
of colors that can be displayed simultaneously, while still satisfying the above requirements. Background 
research suggested that we needed to consider three separate selection criteria: color distance, linear 
separation, and color category. A. Color Distance Perceptually balanced color models are often used to 
measure perceived color di.erence between pairs of colors. Examples include CIE LUV, CIE Lab, Munsell, 
and the Optical Society of America Uniform Color Space. We used CIE LUV to measure color distance. Colors 
are speci.ed * in this model using three axes: L*, u*,and v. L* encodes luminance, while u* and v* encode 
chromaticity (u* and v* correspond roughly to the red-green and blue-yellow opponent color channels). 
CIE LUV provides two important properties. First, colors with the same L* are isoluminant, that is, they 
have roughly the same perceived brightness. Second, the Euclidean distance between a pair of colors corresponds 
roughly to their perceived color di.erence. Given two colors x and y in CIE LUV space, the perceived 
di.erence measured in 6E* units is: J **)2 6E*= (6L*)2 +(6u)2 +(6v(5) xy xyxyxy Our techniques do not 
depend on CIE LUV; we could have chosen to use any perceptually balanced color model. We picked CIE LUV 
in part because it is reasonably well known, and in part because it is recommended by the Commission 
Internationale de L  Eclairage (CIE) as the appropriate model to use for CRT displays [11]. B. Linear 
Separation Results from vision and psychophysics suggest that colors that are linearly separable are 
signi.cantly easier to IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 
1999 A  blue-purple color category boundary T-BC linear separation Fig. 8. A small, isoluminant patch 
within the CIE LUV color model, showing a target color T and three background distracter colors A, B, 
and C;note that T is collinear with A and B, but can be separated by a straight line from B and C;note 
also that T, A,and C occupy the blue color region, while B occupies the purple color region distinguish 
from one another. Initial work on this prob-element colored T in a sea of background elements colored 
lem was reported in [14]. These results were subsequently A is signi.cantly more di.cult than .nding 
T in a sea of con.rmed and strengthened by [3], [4] who showed that elements colored B. Kawai et al. 
suggest this is because a perceptually balanced color model could not be used to both T and A lie within 
a color category named blue , overcome the linear separation e.ect. while B lies within a di.erent category 
named purple . As an example, consider a target color T and three back-Colors from di.erent named categories 
have a higher perground distracter colors A, B,and C shown in CIE LUV ceived color di.erence, even when 
a perceptual color model space in Fig. 8. Since the Euclidean distances TA, TB,and is used to try to 
control that di.erence. TC are equal, the perceived color di.erence between T and D. Color Selection 
Experiments A, B,and C should also be roughly equal. However, searching for a target element colored 
T in a sea of background Our .rst experiment selected colors by controlling color elements colored A 
and B is signi.cantly more di.cult than distance and linear separation, but not color category. The searching 
for T in a sea of elements colored B and C.Ex-reasons for this were twofold. First, traditional methods 
for perimental results suggest that this occurs because T is subdividing a color space into named color 
regions are tecollinear with A and B,whereas T can be separated by a dious and time-consuming to run. 
Second, we were not straight line in color space from B and C. Linear separation convinced that results 
from [31] were important for our increases perceived color di.erence, even when a perceptual color selection 
goals. If problems occurred during our inicolor model is used to try to control that di.erence. tial 
experiment, and if those problems could be addressed by controlling color category during color selection, 
this C. Color Category would both strengthen the results of [31] and highlight their Recent work reported 
by Kawai et al. showed that, dur-applicability to the general color selection task. ing their experiments, 
the named categories in which peo-We selected colors from the boundary of a maximumple place individual 
colors can a.ect perceived color di.er-radius circle embedded in our monitor s gamut. The cirence [31]. 
Colors from di.erent named categories have a cle was located on an isoluminant slice through the CIE 
larger perceived color di.erence, even when Euclidean dis-LUV color model. Previous work reported in 
[7], [9] showed tance in a perceptually balanced color model is held con-that a random variation of luminance 
can interfere with the stant. identi.cation of a boundary between two groups of di.er- Consider again 
the target color T and two background ently colored elements. Holding the perceived luminance of distracter 
colors A and B shown in CIE LUV space in each color constant guaranteed variations in performance Fig. 
8. Notice also that this region of color space has been would not be the result of a random luminance 
e.ect. Fig. 9 divided into two named color categories. As before, the shows an example of selecting .ve 
colors about the circum-Euclidean distances TA and TB are equal, yet .nding an ference of the maximum-radius 
circle inscribed within our HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES AND COLORS 
IN SCIENTIFIC VISUALIZATION monitor s gamut at L* =61.7. Since colors are located equidistant around 
the circle, every color has a constant distance d to its two nearest neighbors, and a constant distance 
l to the line that separates it from all the other colors. Y  Fig. 9. Choosing colors from the monitor 
s gamut, the boundary of the gamut at L. =61.7 represented as a quadrilateral, along with the maximum 
inscribed circle centered at (L. , u, v.)= (67.1, 13.1, -0.98), radius 70.5.E.; .ve colors chosen around 
the circle s circumference; each element has a constant color distance d with its two neighbors, and 
a constant linear separation l from the remaining (non-target) elements; the circle s circumference has 
been subdivided into ten named categories, corresponding to the ten hue names from the Munsell color 
model We split the experiment into four studies that displayed three, .ve, seven, and nine colors simultaneously. 
This allowed us to test for the maximum number of colors we could show while still supporting preattentive 
identi.cation. Displays in each study were further divided along the following conditions: target color: 
each color being displayed was tested as a target, for example, during the three-color study some observers 
searched for a red target in a sea of green and blue distracters, others search for a blue target in 
a sea of red and green distracters, and the remainder searched for a green target in a sea of red and 
blue distracters; asymmetric performance (that is, good performance for some colors and poor performance 
for others) would indicate that constant distance and separation are not su.cient to guarantee equal 
perceived color di.erence, and  display size: experiment displays contained either 17, 33, or 49 elements; 
any decrease in performance when display size increased would indicate that the search task is not preattentive. 
 At the beginning of an experiment session observers were asked to search a set of displays for an element 
with a particular target color. Observers were told that half the displays would contain an element 
with the target color, and half would not. They were then shown a sequence of experiment displays that 
contained multiple colored squares randomly located on an underlying 9  9 grid. Each display remained 
onscreen until the observer indicated via a keypress whether a square with the given target color was 
present or absent. Observers were told to answer as quickly as possible without making mistakes. E. Results 
Observers were able to detect all the color targets rapidly and accurately during both the three-color 
and .ve-color studies; the average error rate was 2.5%, while the average response times ranged from 
459 to 661 ms (response times exceeded the normal preattentive limit of 200 ms because they include the 
time required for observers to enter their responses on the keyboard). Increasing the display size had 
no signi.cant e.ect on response time. Observers had much more di.culty identifying certain colors during 
the seven-color (Fig. 10a) and nine-color studies. Response times increased and accuracy decreased during 
both studies. More importantly, the time required to detect certain colors (e.g., light green and dark 
green in the seven-color study) was directly proportional to display size. This indicates observers are 
searching serially through the display to .nd the target element. Other colors exhibited relatively 
.at response time curves. These asymmetric results suggest that controlling color distance and linear 
separation alone is not enough to guarantee a collection of equally distinguishable colors. F. Color 
Category Experiments We decided to try to determine whether named color categories could be used to explain 
the inconsistent results from our initial experiment. To do this, we needed to subdivide a color space 
(in our case, the circumference of our maximum radius circle) into named color regions. Traditional 
color naming experiments divide the color space into a .ne-grained collection of color samples. Observers 
are then asked to name each of the samples. We chose to use a simpler, faster method designed to measure 
the amount of overlap between a set of named color regions. Our technique runs in three steps: 1. The 
color space is automatically divided into ten named color regions using the Munsell color model. The 
hue axis of the Munsell model is speci.ed using the ten color names red, yellow-red, yellow, green-yellow, 
green, bluegreen, blue, purple-blue, purple, and red-purple (or R, YR, Y, GY, G, BG, B, PB, P, and RP). 
Colors are converted to Munsell space, then assigned their hue name within that space (Fig. 9). 2. Representative 
colors from each of the ten named regions are selected. We chose the color at the center of each region 
to act as the representative color for that region.  IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER 
GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 1999 Response Time (ms) Response Time (ms) Response Time (ms) 1500 
1250 1000 750 500 1500 1250 1000 750 500 1500 1250 1000 750 500  Seven-Color Experiment Category Experiment 
Display Size (Elements) (a) Display Size (Elements) (b)  Distance/Separation/Category Experiment 
Display Size (Elements) (c) Fig. 10. Graphs showing averaged subject response times for three of the 
six studies: (a) response time as a function of display size (i.e., total number of elements shown in 
each display) for each target from the seven-color study; (b) response times for each target from the 
color category study; (c) response times for each target from the combined distance-separation-category 
study 3. Observers are asked to name each of the representative colors. The amount of overlap between 
the names chosen for the representative colors for each region de.nes the amount of category overlap 
that exists between the regions. Consider Table I, which lists the percentage of observers who selected 
a particular name for six of the representative colors. For example, the table shows that representative 
colors from P and R overlap only at the pink name. Their overlap is not that strong, since neither P 
nor R are strongly classi.ed as pink. The amount of overlap is computed by multiplying the percentages 
for the common name, giving a P-R overlap of 5.2%  26.3% = 0.014. A closer correspondence of user-chosen 
names for a pair of regions results in a stronger category similarity. For example, nearly all observers 
named the representative colors from the G and GY regions as green . This resulted in an overlap of 0.973. 
Representative colors that overlap over multiple names are combined using addition, for example, YR and 
Y overlapped in both the orange and brown HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES 
AND COLORS IN SCIENTIFIC VISUALIZATION TABLE I A list of six representative colors for the color regions 
purple, red, yellow-red, yellow, green-yellow, and green, and the percentage of observers who chose a 
particular name for each representative color P R YR Y GY G  purple magenta pink red orange brown yellow 
green 86.9% 2.6% 5.2% 26.3% 71.0% 5.3% 86.8% 7.9% 2.6% 44.7% 47.4% 97.3% 100.0% names, resulting in a 
YR-Y overlap of (86.8%  2.6%) + (7.9%  44.7%) = 0.058. G. Color Category Results When we compared the 
category overlap values against results from our seven and nine-color studies, we found that the amount 
of overlap between the target color and its background distracters provided a strong indication of performance. 
Colors that worked well as targets had low category overlap with all of their distracter colors. Colors 
that worked poorly had higher overlap with one or more of their distracter colors. A measure of rank 
performance to total category overlap produced correlation values of 0.821 and 0.762 for the seven and 
nine-color studies, respectively. This suggests that our measure of category overlap is a direct predictor 
of subject performance. Low category overlap between the target color and all of its background distracters 
produces relatively rapid subject performance. High category overlap between the target color and one 
or more background distracters results in relatively slow subject performance. These results might suggest 
that color category alone can be used to choose a set of equally distinguishable colors. To test this, 
we selected seven new colors that all had low category overlap with one another, then reran the experiments. 
Results from this new set of colors were as poor as the original seven-color study (Fig. 10b). The seven 
new colors were located at the centers of their named categories, so their distances and linear separations 
varied. The colors with the longest response times had the smallest distances and separations. This suggests 
that we need to maintain at least a minimum amount of distance and separation to guarantee acceptable 
identi.cation performance. In our last experiment, we chose a .nal set of seven colors that tried to 
satisfy all three selection criteria. The categories in which the colors were located all had low overlap 
with one another. Colors were shifted within their categories to provide as large a distance and linear 
separation as possible. We also tried to maintain constant distances and linear separations for all the 
colors. Results from this .nal experiment were encouraging (Fig. 10c). Response times for each of the 
colors acting as a target were similar, with little or no e.ect from increased display size. The mean 
response error was also signi.cantly lower than during the previous two seven-color experiments. We concluded 
that up to seven isoluminant colors can be displayed simultaneously while still allowing for rapid and 
accurate identi.cation, but only if the colors satisfy proper color distance, linear separation, and 
color category guidelines. VI. Combining Texture and Color Previous work in our laboratory focused on 
selecting perceptual textures and colors in isolation. Clearly, we would like to use multicolored pexels 
during visualization. The ability to combine both features e.ectively would increase the number of attributes 
we can visualize simultaneously. Results in the literature are mixed on how this might be achieved. Some 
researchers have reported that task irrelevant variation in color has no e.ect on texture discrimination 
[51], [58], while others have found exactly this kind of interference [8], [9], [49]. Moreover, we are 
not aware of any studies that address whether there is interference from random variation in texture 
properties when discrimination is based on color. Experiments are therefore needed that examine possible 
interference e.ects in both directions, that is, e.ects of color variation on texture discrimination 
and e.ects of texture variation on color discrimination. A. Experiments In order to investigate these 
issues, we designed a new set of psychophysical experiments. Our two speci.c questions were: 1. Does 
random variation in pexel color in.uence the detection of a region of target pexels de.ned by height 
or density? 2. Does random variation in pexel height or density in.uence the detection of a region 
of target pexels de.ned by color?  We chose to ignore regularity, since it performed poorly as a target 
de.ning property during all phases of our original texture experiments [23], [25]. We chose three di.erent 
colors using our perceptual color selection technique [22], [23]. Colors were initially selected in the 
CIE LUV color space, then converted to our monitor s RGB gamut. The three colors corresponded approximately 
to red (monitor RGB = 246, 73, 50), green (monitor RGB = 49, 144, 21) and blue (monitor RGB = 82, 109, 
168). Our new experiments were constructed around a set of conditions similar to those used during the 
original texture experiments. For color targets, we varied: IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER 
GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 1999  (a) (b) (c) (d)   Fig. 11. Four displays from the combined 
color-texture experiments, printed colors may not match exactly on-screen colors used during our experiments: 
(a) a green target in a sea of blue pexels with background density variation; (b) a red target in a sea 
of green pexels with background height variation; (c) a tall target with background blue-green color 
variation; (d) a dense target with background green-red color variation target-background pairing: some 
displays contained a green target region in a sea of blue pexels, while others contained a red target 
region in a sea of green pexels (Fig. 11a and 11b); two di.erent pairings were used to increase the generality 
of the results,  secondary dimension: displays contained either no background variation (e.g., every 
pexel was sparse and short), a random variation of density across the array, or a random variation of 
height across the array; this allowed us to test for interference from two di.erent texture dimensions 
during target detection based on color,  exposure duration: displays were shown for either 50, 150, 
or 450 ms; this allowed us to see how detection accuracy was in.uenced by the exposure duration of the 
display, and  target patch size: target regions were either 22 pexels or 44 pexels in size. This allowed 
us to examine the in.uence of all the foregoing factors at both relatively di.cult (22) and easy (4 
 4) levels of target detectability. Two texture dimensions (height and density) were studied, and 
each involved two di.erent target types: taller and shorter (for height) and denser and sparser (for 
density). For each type of target, we designed an experiment that tested a similar set of conditions. 
For example, in the taller experiment we varied:  target-background pairing: half the displays contained 
a target region of medium pexels in a sea of short pexels, while the other half contained a target region 
of tall pexels in a sea of medium pexels; two di.erent pairings were used to increase the generality 
of the results,  secondary dimension: the displays contained pexels that were either a constant gray 
or that varied randomly between two colors; when color was varied, half the displays contained blue 
and green pexels, while the other half of the displays contained green and red pexels (Fig. 11c),  exposure 
duration: displays were shown for 50, 150, or 450 ms, and  target patch size: target groups were either 
2  2 pexels or 4  4 pexels in size.  Fig. 11 shows examples of four experiment displays. Fig. 11a 
and 11b contain a green target in a sea of blue pexels, and a red target in a sea of green pexels, respectively. 
Density varies in the background in Fig. 11a, while height varies in Fig. 11b. Fig. 11c contains a tall 
target with a blue-green background color pattern. Fig. 11d contains a dense target with a green-red 
background color pattern. Any background variation that is present can pass through a target. This occurs 
in Fig. 11d, where part of the target is red and part is green. Note also that, as described for Fig. 
5, the number of paper strips in an individual pexel depends on its density. The colors we used during 
our experiments were chosen in CIE LUV color space. A simple set of formulas can be used to convert from 
CIE LUV to CIE XYZ (a standard device-independent color model), and from there to our monitor s color 
gamut. To move from LUV to XYZ: HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES AND COLORS 
IN SCIENTIFIC VISUALIZATION Background: Taller Shorter Denser Sparser Color Target: Color Density Height 
None 89.1% 85.8%    93.8% 93.4%  95.5% 95.4% 93.8%  Fig. 12. A table showing the percentage of 
correct responses for each target-background pairing; target type along the horizontal axis, background 
type along the vertical axis; darker squares represent pairings with a high percentage of correct responses; 
results for taller, shorter, denser, and sparser with no background variation are from the original texture 
experiments; blank entries with diagonal slashes indicate target-background pairings that did not exist 
during the combined color-texture experiments (f L* +16 3 Y = Yw 116 X =9u'' Y (6) 4v3 Z = 'vY - 5Y - 
34uv' 'Y * where L, u',and v' are used to specify a color in CIE LUV (u' and v' are simple respeci.cations 
of u * and v *), and Yw represents the luminance of a reference white point. We then built a conversion 
matrix to map colors from CIE XYZ into our monitor s color gamut. This was done by obtaining the chromaticities 
of our monitor s red, green, and blue triads, then measuring the luminance of the monitor s maximum 
intensity red, green, and blue with a spot photometer. These values are needed to convert colors from 
a deviceindependent space (i.e., CIE XYZ) into device-dependent coordinates (i.e., our monitors RGB 
color space). All of our experiments were displayed on a Sony Trinitron monitor with CIE XYZ chromaticities 
(xr,yr)=(0.625,0.340), (xg ,yg)= (0.280,0.595), and (xb,yb)= (0.155,0.070). The luminances of maximum 
intensity red, green, and blue were Yr =5.5, Yg =16.6, Yb =2.8. This produced an XYZ to monitor RGB conversion 
matrix of: -1 0.131 0.057 0.021[ R .[ .[ X . (7) G =-0.044 0.081 0.002 Y B0.003 0.008 0.033Z For a 
complete description of how the conversion formulas are built, we refer the reader to any of [17], [18], 
[60]. Ten users participated as observers in each of the two color and four texture experiments. Each 
observer had normal or corrected acuity. Observers who completed the color experiments were also tested 
for color blindness [12]. Observers were provided with an opportunity to practice before each experiment. 
This helped them become familiar with the task and the duration of the displays. Before each testing 
session began, observers were told that half the displays would contain a target, and half would not. 
We used a Macintosh computer with an 8-bit color lookup table to run our experiments. Responses (either 
target present or target absent ) for each display an observer was shown were recorded for later analysis. 
B. Results Mean percentage target detection accuracy was the measure of performance. Observer responses 
were collected, averaged, and analyzed using multi-factor ANOVA. In summary, we found: 1. Color targets 
were detected rapidly (i.e., at 150 ms) with very high accuracy (96%). Background variation in height 
and density produced no interference e.ects in this detection task. 2. Detection accuracy for targets 
de.ned by density or height were very similar to results from our original texture experiments [23], 
[25]. When there was no background variation in color, excellent detection accuracy was obtained for 
density de.ned targets (i.e., denser and sparser targets) at 150 ms (94%). Height de.ned targets (i.e., 
taller and shorter) were detected somewhat less accurately at 150 ms (88%) but were highly detectable 
at 450 ms (93%). As we had also found previously, taller targets were generally easier to detect than 
shorter targets, and denser targets were easier than sparser targets. 3. In all four texture experiments, 
background variation in color produced a small but signi.cant interference e.ect, averaging 6% in overall 
accuracy reduction.  IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 
1999 Color: 100 90 80 70 60 50 Correct (%) Correct (%) Correct (%)  Duration (ms) (a) Height: 100 90 
80 70 60 50Duration (ms)  (b) Density: 100 90 80 70 60 50  Duration (ms) (c) Fig. 13. Graphs showing 
averaged subject results for color, height, and density trials: (a) results for color trials, horizontal 
axis plots exposure duration, vertical axis plots percentage of correct responses, each line corresponds 
to one of the three di.erent background conditions (no variation, height variation, or density variation); 
(b) results for height trials; (c) results for density trials 4. The absolute reduction in accuracy due 
to color inter-along the vertical axis. Darker squares represent targetference depended on the di.culty 
of the detection task. background pairings with highly accurate subject perfor-Speci.cally, color interfered 
more with the less visible tar-mance. The number in the center of each square reports get values (shorter 
and sparser targets yielded a mean ac-the percentage of correct responses averaged across all subcuracy 
reduction of 8%) than with the more visible targets jects. (taller and denser targets yield a mean accuracy 
reduction Target regions de.ned by a particular pexel color were of 4%). identi.ed rapidly and accurately 
in all cases. At a 150 ms Fig. 12 shows average subject performance as a table rep-exposure duration 
mean accuracy was approximately 96%. resenting each target-background pair. Target type varies The small 
increase in accuracy from shorter to longer expoalong the horizontal axis, while background type varies 
sure durations was signi.cant, F(2,36) = 41.03,p< .001. HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: 
COMBINING TEXTURES AND COLORS IN SCIENTIFIC VISUALIZATION However, variation in the background height 
or density of pexels caused no signi.cant reduction in performance (mean accuracy of 95.3% for constant 
background, 96.6% for varying height, and 96.9% for varying density; see also the graph in Fig. 13a). 
In fact, the graphs in Figure 13a report that absolute performance was slightly better for conditions 
with background variations of height or density. We suspect that geometric regularity in the texture 
pattern may produce a gestalt or con.gurational e.ect that interferes with target detection based on 
color. If so, this would be similar to previous reports in the psychophysical literature [2] showing 
inhibitory e.ects of gestalt grouping on target detection. Detection accuracy for targets de.ned by texture 
properties were very similar to results from our previous texture experiments [22], [23]. Both kinds 
of targets bene.ted from longer exposure durations (density, F(2,58) = 9.24,p < .001; height, F(2,58) 
= 10.66,p < .001), with small but signi.cant increases in accuracy with each increase in duration. With 
regard to the four kinds of targets, denser and taller target regions were easiest to identify, followed 
by sparser and shorter target regions (Fig. 13b and 13c). However, only the di.erence between taller 
versus shorter targets was statistically signi.cant, F(1,29) = 67.14,p<.001. These e.ects were not unexpected, 
since they have been reported in other psychophysical studies [1], [51]. In the target present displays, 
accuracy for shorter targets seemed to be compromised even more than usual because of occlusion: a group 
of shorter pexels was often partially occluded by a group of taller pexels placed in front of them. A 
group of taller pexels, on the other hand, tended to stand out among the shorter pexels that surrounded 
them. Sparser targets su.er from a di.erent problem: the need for a minimum amount of physical space 
to become perceptually salient. Since dense targets add information to their target region, rather than 
take information away , they were less susceptible to this problem. This asymmetry contributed to a 
signi.cant target type by region size interaction, F(1,29) = 11.14,p<.01. This was re.ected in a dramatic 
reduction in the performance gap between dense and sparse targets when 2 2and 4 4 target patches are 
compared. In displays with 22 target regions and background color variation, dense targets outperform 
sparse targets by approximately 7%. For 44 target regions, however, dense and sparse displays were nearly 
equal in accuracy (less than 1% di.erence). For targets de.ned by texture, random color variation tended 
to interfere with detection, causing accuracy to be lower for both denser and sparser targets in the 
density displays (F(1,29) = 9.12,p<.01) and by interacting with target type in the case of height (F(1,29) 
= 10.61,p < .01, see also Fig. 13b and 13c). This interaction resulted from color variation having a 
greater in.uence on accuracy for short targets (F(1,15)= 6.73,p < .03), which were generally more di.cult 
to see, than for tall targets, which were detected with uniformly high accuracy (greater than 90%). These 
results suggest that color interference can be limited when color and texture are combined, but only 
in cases where the detection task is relatively e.ortless prior to the addition of color variation. As 
can be seen in Fig. 13b and 13c, the interference e.ect of color variation tends to be greatest when 
the target detection task is most di.cult. Several other miscellaneous e.ects were worthy of note. Detection 
accuracy was generally higher on displays with a target present than when no target was present (color, 
F(1,18)= 37.32,p < .001; density, F(1,29) = 5.09,p < .04; height, F(1,29) = 6.64,p<.02). This was a small 
difference overall (an average of 4%) but it re.ected a slight bias on the part of users to guess target 
present when they were uncertain what they had seen. Large target regions (4  4) were generally easier 
to identify than small regions (2  2) (color, F(1,18)= 15.38,p< .001; density, F(1,29) = 94.24,p < .001; 
height, F(1,29) = 24.78,p < .001), due to the greater visibility associated with a larger target region. 
Taken together, these results are consistent with studies based on textures arrayed in a two-dimensional 
plane and reported in the psychophysical literature. As described by [49], we found that color produces 
a small but statistically reliable interference e.ect during texture segmentation. Moreover, we found 
color and texture form a feature hierarchy that produces asymmetric interference: color variation interferes 
with an observer s ability to see texture regions based on height or density, but variation in texture 
has no e.ect on region detection based on color. This is similar to reports by [8], [9], who reported 
asymmetric color on shape interference in a boundary detection task involving two-dimensional textures. 
VII. Practical Applications Although our theoretical results provide a solid design foundation, it is 
equally important to ensure that these results can be applied to real-world data. Our initial goal was 
a technique for visualizing multivariate data on an underlying height .eld. We decided to test our perceptual 
visualization technique by analyzing environmental conditions on a topographic map. Speci.cally, we 
visualized typhoons in the Northwest Paci.c Ocean during the summer and fall of 1997. A. Visualizing 
Typhoons The names typhoon and hurricane are regionspeci.c, and refer to the same type of weather phenomena: 
an atmospheric disturbance characterized by low pressure, thunderstorm activity, and a cyclic wind pattern. 
Storms of this type with windspeeds below 17m/s are called tropical depressions . When windspeeds exceed 
17m/s, they become tropical storms . This is also when storms are assigned a speci.c name. When windspeeds 
reach 33m/s, a storm becomes a typhoon (in the Northwest Paci.c) or a hurricane (in the Northeast Paci.c 
and North Atlantic). We combined information from a number of di.erent sources to collect the data that 
we needed. A U.S. Navy elevation dataset1 was used to obtain land elevations at 1http://grid2.cr.usgs.gov/dem/ 
IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 1999 ten minute latitude 
and longitude intervals. Land-based weather station readings collected from around the world and archived 
by the National Climatic Data Center2 provided daily measurements for eighteen separate environmental 
conditions. Finally, satellite archives made available by the Global Hydrology and Climate Center3 contained 
daily open-ocean windspeed measurements at thirty minute latitude and longitude intervals. The National 
Climatic Data Center de.ned the 1997 typhoon season to run from August 1 to October 31. Each of our 
datasets contained measurements for this time period. We chose to visualize three environmental conditions 
related to typhoons: windspeed, pressure, and precipitation. All three values were measured on a daily 
basis at each land-based weather station, but only daily windspeeds were available for open-ocean positions. 
In spite of the missing open-ocean pressure and precipitation, we were able to track storms as they 
moved across the Northwest Paci.c Ocean. When the storms made landfall the associated windspeed, sea-level 
pressure, and precipitation were provided by weather stations along their path. Based on our experimental 
results, we chose to represent windspeed, pressure, and precipitation with height, density, and color, 
respectively. Localized areas of high windspeed are obvious indicators of storm activity. We chose to 
map increasing windspeed to an increased pexel height. Although our experimental results showed statistically 
significant interference from background color variation, the absolute e.ect was very small. Taller 
and denser pexels were easily identi.ed in all other cases, suggesting there should be no changes in 
color interference due to an increase in task di.culty. Windspeed has two important boundaries: 17m/s 
(where tropical depressions become tropical storms) and 33m/s (where storms become typhoons). We mirrored 
these boundaries with height discontinuities. Pexel height increases linearly from 0-17m/s. At 17m/s, 
height approximately doubles, then continues linearly from 17-33m/s. At 33m/s another height discontinuity 
is introduced, followed by a linear increase for any windspeeds over 33m/s. Pressure is represented with 
pexel density. Since our results showed it was easier to .nd dense pexels in a sea of sparse pexels (as 
opposed to sparse in dense), an increase in pressure is mapped to a decrease in pexel density (i.e., 
very dense pexels represent the low pressure regions associated with typhoons). Three di.erent texture 
densities were used to represent three pressure ranges. Pressure readings less than 996 millibars, between 
996 and 1014 millibars, and greater than 1014 millibars produce very dense, dense, and sparse pexels, 
respectively. Precipitation is represented with color. We used our perceptual color selection technique 
to choose .ve perceptually uniform colors. Daily precipitation readings of zero, 0 0.03 inches, 0.03 
0.4 inches, 0.4 1.0 inches, and 1.0 10.71 inches were colored green, yellow, orange, red, and purple, 
respectively (each precipitation range had an equal number of entries in our typhoon dataset). Pexels 
on the open 2http://www.ncdc.noaa.gov/ol/climate/online/gsod.html 3http://ghrc.msfc.nasa.gov/ghrc/list.html 
 ocean or at weather stations where no precipitation values were reported were colored blue-green. Our 
experimental results showed no texture-on-color interference. Moreover, our color selection technique 
is designed to produce colors that are equally distinguishable from one another. Our mapping uses red 
and purple to highlight the highprecipitation areas associated with typhoon activity. We should note 
that our data-feature mapping is designed to allow viewers to rapidly and accurately identify and track 
the locations of storms and typhoons as spatial collections of tall, dense, red and purple pexels. Our 
visualization system is not meant to allow users to determine exact values of windspeed, pressure, and 
precipitation from an individual pexel. However, knowing the range of values that produce certain types 
of height, density, and color will allow a viewer to estimate the environmental conditions at a given 
spatial location. We built a simple visualization tool that maps windspeed, pressure, and precipitation 
to their corresponding height, density, and color. Our visualization tool allows a user to move forwards 
and backwards through the dataset day-by-day. One interesting result was immediately evident when we 
began our analysis: typhoon activity was not represented by high windspeed values in our open-ocean dataset. 
Typhoons normally contain severe rain and thunderstorms. The high levels of cloud-based water vapor 
produced by these storms block the satellites that are used to measure open-ocean windspeeds. The result 
is an absence of any windspeed values within a typhoon s spatial extent. Rather than appearing as a local 
region of high windspeeds, typhoons on the open-ocean are displayed as a hole , an ocean region without 
any windspeed readings (see Fig. 14b and 14d). This absence of a visual feature (i.e., ahole in the texture 
.eld) is large enough to be salient in our displays, and can be preattentively identi.ed and tracked 
over time. Therefore, users have little di.culty .nding storms and watching them as they move across 
the open ocean. When a storm makes landfall, the weather stations along the storm s path provide the 
proper windspeed, as well as pressure and precipitation. Weather stations measure windspeed directly, 
rather than using satellite images, so high levels of cloud-based water vapor cause no loss of information. 
Fig. 14 shows windspeed, pressure, and precipitation around Japan, Korea, and Taiwan during August 1997. 
Fig. 14a looks north, and displays normal summer conditions across Japan on August 7, 1997. Fig. 14b, 
looking northeast, tracks typhoon Amber (one of the region s major typhoons) approaching along an east 
to west path across the Northwest Paci.c Ocean on August 27, 1997. Fig. 14c shows typhoon Amber one day 
later as it moves through Taiwan. Weather stations within the typhoon show the expected strong winds, 
low pressure, and high levels of rainfall. These results are easily identi.ed as tall, dense, red and 
purple pexels. Compare these images to Fig. 14d and 14e, where windspeed was mapped to regularity, pressure 
to height, and precipitation to density (a mapping without color that our original texture experiments 
predict will HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES AND COLORS IN SCIENTIFIC 
VISUALIZATION (a) Typhoon Amber Typhoon Amber (b) (c) (d) (e)  Fig. 14. Typhoon conditions across 
Southeast Asia during the summer of 1997: (a) August 7, 1997, normal weather conditions over Japan; (b) 
August 27, 1997, typhoon Amber approaches the island of Taiwan from the southeast; (c) August 28, 1997, 
typhoon Amber strikes Taiwan, producing tall, dense pexels colored orange, red, and purple (representing 
high precipitation); (d, e) the same data as in (b) and (c) but with windspeed represented by regularity, 
pressure by height, and precipitation by density IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, 
VOL. 5, NO. 2, APRIL-JUNE 1999 perform poorly). Although viewers can identify areas of lower and higher 
windspeed (e.g., on the openoceanand over Taiwan), it is di.cult to identify a change in lower or higher 
windspeeds (e.g., the change in windspeed as typhoon Amber moves onshore over Taiwan). In fact, viewers 
often searched for an increase in density that represents an increase in precipitation, rather than an 
increase in irregularity; pexels over Taiwan become noticeably denser between Fig. 14d and 14e. VIII. 
Conclusions and Future Work This paper describes a method for combining perceptual textures and colors 
for multivariate data visualization. Our pexels are built by varying three perceptual texture dimensions: 
height, density, and regularity. Our perceptual colors are selected by controlling the color distance, 
linear separation, and color category of each color. Both experimental and real-world results showed 
that colored pexels can be used to rapidly, accurately, and e.ortlessly analyze large, multi-element 
displays. Care must be taken, however, to ensure that the data-feature mapping builds upon the fundamental 
workings of the low-level human visual system. An ad-hoc mapping will often introduce visual artifacts 
that actively interfere with a user s ability to perform their visual analysis task. Our initial texture 
experiments showed that taller, shorter, denser, and sparser pexels can be easily identi.ed, but that 
certain background patterns must be avoided to ensure accurate performance. During our color selection 
experiments we found that color distance, linear separation, and color category must all be considered 
to ensure a collection of equally distinguishable colors. New results on the combined use of texture 
and color showed that background color variation causes a small but statistically signi.cant interference 
e.ect during a search for targets based on height or density. The size of the e.ect is directly related 
to the di.culty of the visual analysis task; tasks that are more di.cult result in more color interference. 
Variation of height and density, on the other hand, had no e.ect on identifying color targets. These 
results are similar to reports in the psychophysical literature [8], [9], [49], although to our knowledge 
no one has studied perceptual textures and colors displayed in 3D using perspective projections. Our 
results were further validated when we applied them to real-world applications like typhoon visualization. 
Our tools were designed to satisfy .ndings from our experiments. For example, attributes were mapped 
in order of importance to height, density, and color. In cases where an subject analyzed height or density 
patterns, we tried to ensure an e.ortless search task (i.e., looking for taller or denser rather than 
shorter or sparser) to minimize any color on texture interference that might occur. One important area 
of future work is a comparison of our visualization techniques against other methods that might be used 
to represent information in our real-world applications. For example, it would be useful to test a user 
s ability to track storm activity in our visualization environment against other standard techniques 
for representing weather activity. Although we have yet to conducted these kinds of practical experiments, 
we hope to initiate them in the near future as part of our perceptual visualization studies. We are now 
working to integrate our colored pexels with other visual features. One candidate is orientation; in 
fact, our pexels were initially designed to stand up o. the underlying height .eld to support variation 
of orientation. Another visual property with signi.cant potential is apparent motion. This technique 
can be used to make individual strips in a pexel walk within their spatial extent. It may be possible 
to tie direction and speed of motion to two underlying attribute values, thereby increasing the dimensionality 
of our visualization techniques. We are designing experiments to investigate the e.ectiveness of each 
of these features for encoding information. We will also study any interactions that occur when multiple 
texture, color, orientation, and motion dimensions are displayed simultaneously. Acknowledgments We 
would like to thank the National Climatic Data Center, and Sherry Harrison and the Global Hydrology Resource 
Center for generously providing typhoon-related weather data. We would also like to thank Jeanette Lum 
for coordinating and running our experiment sessions. This research was funded in part by the National 
Science and Engineering Research Council of Canada, and by the O.ce of Naval Research (Grant N00014-96-1120) 
and the Ballistic Missile Defense Organization through the Multiuniversity Research Initiative. References 
 [1] Aks, D. J., and Enns, J. T. Visual search for size is in.uenced by a background texture gradient. 
Journal of Experimental Psychology: Perception and Performance 22, 6 (1996), 1467 1481. [2] Banks, W. 
P., and Prinzmetal, W. Con.gurational e.ects in visual information processing. Perception &#38; Psychophysics 
19 (1976), 361 367. [3] Bauer, B., Jolicoeur, P., and Cowan, W. B. Visual search for colour targets that 
are or are not linearly-separable from distractors. Vision Research 36 (1996), 1439 1446. [4] Bauer, 
B., Jolicoeur, P., and Cowan, W. B. The linear separability e.ect in color visual search: Ruling out 
the additive color hypothesis. Perception &#38; Psychophysics 60, 6 (1998), 1083 1093. [5] Bergman, 
L. D., Rogowitz, B. E., and Treinish, L. A. A rule-based tool for assisting colormap selection. In Proceedings 
Visualization 95 (Atlanta, Georgia, 1995), pp. 118 125. [6] Bruckner, L. A. On Cherno. faces. In Graphical 
Representation of Multivariate Data, P.C.C. Wang, Ed.Academic Press, New York, New York, 1978, pp. 93 
121. [7] Callaghan, T. C. Dimensional interaction of hue and brightness in preattentive .eld segregation. 
Perception &#38; Psychophysics 36, 1 (1984), 25 34. [8] Callaghan, T. C. Interference and domination 
in texture segregation: Hue, geometric form, and line orientation. Perception &#38; Psychophysics 46, 
4 (1989), 299 311. [9] Callaghan, T. C. Interference and dominance in texture segregation. In Visual 
Search, D. Brogan, Ed. Taylor &#38; Francis, New York, New York, 1990, pp. 81 87. [10] Chernoff, H. The 
use of faces to represent points in kdimensional space graphically. Journal of the American Statistical 
Association 68, 342 (1973), 361 367. [11] CIE. CIE Publication No. 15, Supplement Number 2 (E-1.3.1): 
O.cial Recommendations on Uniform Color Spaces, Color-Di.erence Equations, and Metric Color Terms. Commission 
Internationale de L ` Eclairge, 1976. HEALEY AND ENNS: LARGE DATASETS AT A GLANCE: COMBINING TEXTURES 
AND COLORS IN SCIENTIFIC VISUALIZATION [12] Coren, S., and Hakstian, A. R. Color vision screening without 
the use of technical equipment: Scale development and crossvalidation. Perception &#38; Psychophysics 
43 (1988), 115 120. [13] Cutting, J. E., and Millard, R. T. Three gradients and the perception of .at 
and curved surfaces. Journal of Experimental Psychology: General 113, 2 (1984), 198 216. [14] D Zmura, 
M. Color in visual search. Vision Research 31,6 (1991), 951 966. [15] Egeth, H. E., and Yantis, S. Visual 
attention: Control, representation, and time course. Annual Review of Psychology 48 (1997), 269 297. 
[16] Foley, J., and Ribarsky, W. Next-generation data visualization tools. In Scienti.c Visualization: 
Advances and Challenges, L. Rosenblum, Ed. Academic Press, San Diego, California, 1994, pp. 103 127. 
[17] Foley, J. D., van Dam, A., Feiner, S. K., and Hughes, J. F. Computer Graphics: Principles and Practice. 
Addison-Wesley Publishing Company, Reading, Massachusetts, 1990. [18] Glassner, A. S. Principles of Digital 
Image Synthesis. Morgan Kaufmann Publishers, Inc., San Francisco, California, 1995. [19] Grinstein, G., 
Pickett, R., and Williams, M. EXVIS: An exploratory data visualization environment. In Proceedings Graphics 
Interface 89 (London, Canada, 1989), pp. 254 261. [20] Hallett, P. E. Segregation of mesh-derived textures 
evaluated by resistance to added disorder. Vision Research 32, 10 (1992), 1899 1911. [21] Haralick, R. 
M., Shanmugam, K., and Dinstein, I. Textural features for image classi.cation. IEEE Transactions on System, 
Man, and Cybernetics SMC-3, 6 (1973), 610 621. [22] Healey, C. G. Choosing e.ective colours for data 
visualization. In Proceedings Visualization 96 (San Francisco, California, 1996), pp. 263 270. [23] 
Healey,C.G. Building a perceptual visualisation architecture. Behaviour and Information Technology (in 
press) (1998). [24] Healey, C. G., Booth, K. S., and Enns, J. T. Real-time multivariate data visualization 
using preattentive processing. ACM Transactions on Modeling and Computer Simulation 5, 3 (1995), 190 
221. [25] Healey, C. G., and Enns, J. T. Building perceptual textures to visualize multidimensional datasets. 
In Proceedings Visualization 98 (Research Triangle Park, North Carolina, 1998), pp. 111 118. [26] Interrante, 
V. Illustrating surface shape in volume data via principle direction-driven 3d line integral convolution. 
In SIG-GRAPH 97 Conference Proceedings (Los Angeles, California, 1997), T. Whitted, Ed., pp. 109 116. 
[27] Jul B.esz, Textons, the elements of texture perception, and their interactions. Nature 290 (1981), 
91 97. [28] Julesz, B. A theory of preattentive texture discrimination based on .rst-order statistics 
of textons. Biological Cybernetics 41 (1981), 131 138. [29] Julesz, B. A brief outline of the texton 
theory of human vision. Trends in Neuroscience 7, 2 (1984), 41 45. [30] JulTextons, the fundamental 
elesz, B., and Bergen, J. R. ements in preattentive vision and perception of textures. The Bell System 
Technical Journal 62, 6 (1983), 1619 1645. [31] Kawai, M., Uchikawa, K., and Ujike, H. In.uence of color 
category on visual search. In Annual Meeting of the Association for Research in Vision and Ophthalmology 
(Fort Lauderdale, Florida, 1995), p. #2991. [32] Laidlaw, D. H.,Ahrens, E. T.,Kremers,D., Avalos,M. J., 
Jacobs, R. E., and Readhead, C. Visualizing di.usion tensor images of the mouse spinal cord. In Proceedings 
Visualization 98 (Research Triangle Park, North Carolina, 1998), pp. 127 134. [33] Levkowitz, H. Color 
icons: Merging color and texture perception for integrated visualization of multiple parameters. In 
Proceedings Visualization 91 (San Diego, California, 1991), pp. 164 170. [34] Levkowitz, H., and Herman, 
G. T. Color scales for image data. IEEE Computer Graphics &#38; Applications 12, 1 (1992), 72 80. [35] 
Liu, F., and Picard, R. W. Periodicity, directionality, and randomness: Wold features for perceptual 
pattern recognition. In Proceedings 12th International Conference on Pattern Recognition (Jerusalem, 
Israel, 1994), pp. 1 5. [36] Mack, A., and Rock, I. Inattentional Blindness. MIT Press, Menlo Park, California, 
1998. [37] Malik, J., and Perona, P. Preattentive texture discrimination with early vision mechanisms. 
Journal of the Optical Society of America A 7, 5 (1990), 923 932. [38] Meier, B. J. Painterly rendering 
for animation. In SIG-GRAPH 96 Conference Proceedings (New Orleans, Louisiana, 1996), H. Rushmeier, Ed., 
pp. 477 484. [39] Rao, A. R., and Lohse, G. L. Identifying high level features of texture perception. 
CVGIP: Graphics Models and Image Processing 55, 3 (1993), 218 233. [40] Rao, A. R., and Lohse, G. L. 
Towards a texture naming system: Identifying relevant dimensions of texture. In Proceedings Visualization 
93 (San Jose, California, 1993), pp. 220 227. [41] Reed, T. R., and Hans Du Buf, J. M. A review of recent 
texture segmentation and feature extraction techniques. CVGIP: Image Understanding 57, 3 (1993), 359 
372. [42] Rensink, R. A., O Regan, J. K., and Clark, J. J. To see or not to see: The need for attention 
to perceive changes in scenes. Psychological Science 8 (1997), 368 373. [43] Rheingans, P., and Tebbs, 
B. A tool for dynamic explorations of color mappings. Computer Graphics 24, 2 (1990), 145 146. [44] Robertson, 
P. K. Visualizing color gamuts: A user interface for the e.ective use of perceptual color spaces in data 
displays. IEEE Computer Graphics &#38; Applications 8, 5 (1988), 50 64. [45] Rogowitz, B.E., and Treinish,L.A. 
An architecture for rulebased visualization. In Proceedings Visualization 93 (San Jose, California, 
1993), pp. 236 243. [46] Salisbury, M., Wong, M. T., Hughes, J. F., and Salesin, D. H. Orientable textures 
for image-based pen-and-ink illustration. In SIGGRAPH 97 Conference Proceedings (Los Angeles, California, 
1997), T. Whitted, Ed., pp. 401 406. [47] Schweitzer, D. Arti.cial texturing: An aid to surface visualization. 
Computer Graphics (SIGGRAPH 83 Conference Proceedings) 17, 3 (1983), 23 29. [48] Simon, D. J., and Levin, 
D. T. Change blindness. Trends in Cognitive Science 1 (1997), 261 267. [49] Snowden, R. J. Texture segregation 
and visual search: A comparison of the e.ects of random variations along irrelevant dimensions. Journal 
of Experimental Psychology: Human Perception and Performance 24, 5 (1998), 1354 1367. [50] Tamura, H., 
Mori, S., and Yamawaki, T. Textural features corresponding to visual perception. IEEE Transactions on 
Systems, Man, and Cybernetics SMC-8, 6 (1978), 460 473. [51] Triesman, A. Preattentive processing in 
vision. Computer Vision, Graphics and Image Processing 31 (1985), 156 177. [52] Triesman, A. Search, 
similarity, and integration of features between and within dimensions. Journal of Experimental Psychology: 
Human Perception &#38; Performance 17, 3 (1991), 652 676. [53] Triesman, A., and Gormican, S. Feature 
analysis in early vision: Evidence from search asymmetries. Psychological Review 95, 1 (1988), 15 48. 
[54] Turk, G., and Banks, D. Image-guided streamline placement. In SIGGRAPH 96 Conference Proceedings 
(New Orleans, Louisiana, 1996), H. Rushmeier, Ed., pp. 453 460. [55] Ware, C. Color sequences for univariate 
maps: Theory, experiments, and principles. IEEE Computer Graphics &#38; Applications 8, 5 (1988), 41 
49. [56] Ware, C., and Beatty, J. C. Using colour dimensions to display data dimensions. Human Factors 
30, 2 (1988), 127 142. [57] Ware, C., and Knight, W. Using visual texture for information display. ACM 
Transactions on Graphics 14, 1 (1995), 3 20. [58] Wolfe, J. M. Guided Search 2.0: A revised model of 
visual search. Psychonomic Bulletin &#38; Review 1, 2 (1994), 202 238. [59] Wolfe, J. M.,Yu, K. P.,Stewart,M. 
I., Shorter, A. D., Friedman-Hill, S. R., and Cave, K. R. Limitations on the parallel guidance of visual 
search: Color  color and orientation  orientation conjunctions. Journal of Experimental Psychology: 
Human Perception &#38; Performance 16, 4 (1990), 879 892. [60] Wyszecki, G., and Stiles, W. S. Color 
Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition.John Wiley &#38; Sons, Inc., 
New York, New York, 1982. IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 5, NO. 2, APRIL-JUNE 
1999 Christopher G. Healey received a PhD in computer science in 1996 from the University of British 
Columbia, an MSc in 1992 from the University of British Columbia, and a BMath in 1990 from the University 
of Waterloo. Following graduation he completed a two-year postdoctoral fellowship in computer graphics 
with Dr. Carlo Sequin at the University of California, Berkeley. He is currently working as an assistant 
professor in the Department of Computer Science at North Carolina State Univer sity. His dissertation 
studied methods for displaying e.ectively large, multivariate datasets during scienti.c visualization. 
This work investigated techniques for exploiting the low-level human visual system for information representation. 
His current research focuses on the use of visual features like color, texture, and apparent motion for 
visually exploring multivariate data. He is also investigating automated datafeature mapping techniques 
and data management issues in an e.ort to design a .exible, robust perceptual visualization architecture. 
 James T. Enns received a PhD in psychology from Princeton University (1984) and a BA (honours) from 
the University of Winnipeg (1980). Following graduation, he was appointed an assistant professor at 
Dalhousie University, before moving to the Unviersity of British Columbia in 1987, where he is now appointed 
as a professor in both the Department of Psychology and the Graduate Program in Neuroscience. A central 
focus of his research over the years has been the role of attention in perception. This has included 
studies of how perception and attention change with development, how the visual world is represented 
outside the focus of attention, and how attention changes the perceptions that form the basis for consciousness. 
Along with the publication of these studies in Science, Psychological Review, Perception &#38; Psychophysics, 
and The Journal of Experimental Psychology, he has edited two research volumes (The Development of Attention, 
1990; Attention, Development, &#38; Psychopathology, 1997) and coauthored two textbooks (Analysis of 
Variance, 1986; Sensation &#38; Perception, .fth edition, 1999). Perceptually-Based Brush Strokes for 
Nonphotorealistic Visualization CHRISTOPHER G. HEALEY and LAURA TATEOSIAN North Carolina State University 
and JAMES T. ENNS and MARK REMPLE The University of British Columbia An important problem in the area 
of computer graphics is the visualization of large, complex information spaces. Datasets of this type 
have grown rapidly in recent years, both in number and in size. Images of the data stored in these collections 
must support rapid and accurate exploration and analysis. This article presents a method for constructing 
visualizations that are both e.ective and aesthetic. Our approach uses techniques from master paintings 
and human perception to visualize a multidimensional dataset. Individual data elements are drawn with 
one or more brush strokes that vary their appearance to represent the element s attribute values. The 
result is a nonphotorealistic visualization of information stored in the dataset. Our research extends 
existing glyph-based and nonphotorealistic techniques by applying perceptual guidelines to build an e.ective 
representation of the underlying data. The nonphotorealistic properties the strokes employ are selected 
from studies of the history and theory of Impressionist art. We show that these properties are similar 
to visual features that are detected by the lowlevel human visual system. This correspondence allows 
us to manage the strokes to produce perceptually salient visualizations. Psychophysical experiments con.rm 
a strong relationship between the expressive power of our nonphotorealistic properties and previous .ndings 
on the use of perceptual color and texture patterns for data display. Results from these studies are 
used to produce e.ective nonphotorealistic visualizations. We conclude by applying our techniques to 
a large, multidimensional weather dataset to demonstrate their viability in a practical, real-world setting. 
Categories and Subject Descriptors: H.1.2 [Models and Principles]: User/Machine Systems human factors, 
human information processing; I.3.3 [Computer Graphics]: Picture/Image Generation display algorithms; 
I.3.6 [Computer Graphics]: Methodology and Techniques interaction techniques;J.5 [Arts and Humanities] 
.ne arts General Terms: Experimentation, Human Factors, Performance Additional Key Words and Phrases: 
Abstractionism, color, computer graphics, human vision, Impressionism, nonphotorealistic rendering, perception, 
psychophysics, scienti.c visualization, texture 1. INTRODUCTION Visualization is the conversion of collections 
of strings and numbers (datasets) into images that are used to explore, discover, validate, and analyze. 
The term scienti.c visualization originated during an NSF panel on graphics and image processing [McCormick 
et al. 1987], although the .eld had a long and rich history prior to this meeting (e.g., cartography, 
or charts and graphs [MacEachren 1995; Slocum 1998; Tufte 1983; 1990; 1997]). A number of important research 
problems were identi.ed during these initial discussions. This work was supported in part by the National 
Science Foundation (NSF) grant CISE-ACI-0083421, and by the National Sciences and Engineering Research 
Council of Canada. Authors addresses: C. G. Healey and L. Tateosian, Department of Computer Science, 
173 Venture III Suite 165A, 900 Main Campus Drive #8207, North Carolina State University, Raleigh, NC, 
27695-8207; email: healey@csc.ncsu.edu; lgtateos@unity.ncsu.edu; J. T. Enns and M. Remple, Department 
of Psychology, 2136 Main Mall, University of British Columbia, Vancouver, B. C., Canada, V6T 1Z4; email: 
jenns@psychc.ubc.ca; contactmir@hotmail.com Permission to make digital/hard copy of all or part of this 
material without fee for personal or classroom use provided that the copies are not made or distributed 
for pro.t or commercial advantage, the ACM copyright/server notice, the title of the publication, and 
its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists requires prior speci.c permission and/or 
a fee. . 2004 ACM 0730-0301/2004/0100-0001 $5.00 ACM Transactions on Graphics, Vol. 23, No. 1, January 
2004. 2  Christopher G. Healey et al. In particular, panelists emphasized the need for ways to manage 
the overwhelming amount of data being generated. This is not only an issue of the total number of sample 
points or data elements stored in a dataset (i.e., its size). Each element may also encode multiple values 
representing multiple independent data attributes (i.e., its dimensionality). The challenge is to design 
methods to represent even some of this information together in a common display, without overwhelming 
a viewer s ability to make sense of the resulting images. A follow-up report on advances in scienti.c 
visualization discussed new techniques in important application areas such as volume and .ow visualization 
[Rosenblum 1994]. At the same time, the report noted that much less progress had been made towards application-independent 
methods for managing and displaying large, multidimensional datasets. Increasing information quality 
and quantity remains an open problem; this need was again emphasized during a recent DOE/NSF meeting 
on research directions in visualization [Smith and Van Rosendale 1998]. Work in our laboratories has 
studied various issues in scienti.c visualization for much of the last ten years. A large part of this 
e.ort has focused on multidimensional visualization, the need to visualize multiple layers of overlapping 
information simultaneously in a common display. We often divide this problem into two steps: (1) The 
design of a data-feature mapping M, a function that de.nes visual features (e.g., color, texture, or 
motion) to represent the data. (2) An analysis of a viewer s ability to use the images produced by M 
to explore and analyze the data.  A multidimensional dataset D represents m attributes A =(A1,...,Am) 
recorded at n sample points ei, that is, D = {e1,...,en} and ei =(ai,1,...,ai,m),ai,j E Aj . A data-feature 
mapping M(V,<) de.nes m visual features Vj E V to use to display values for each Aj ; it also de.nes 
a corresponding <j : Aj -Vj to map the domain of Aj to the range of displayable values in Vj . An e.ective 
M must generate images that allow viewers to see e.ortlessly within their data. The need to build fundamental 
techniques that are appropriate for a wide range of visualization environments further complicates this 
problem. The guidelines used to design our M are based on the perceptual abilities of the low-level human 
visual system. Previous work has documented di.erent methods for harnessing perception during visualization 
[Bergman et al. 1995; Grinstein et al. 1989; Healey 1996; Healey et al. 1996; Healey and Enns 1999; Rheingans 
and Tebbs 1990; Rogowitz and Treinish 1993; Ware 1988; 2000; Ware and Knight 1995; Weigle et al. 2000]. 
Certain visual features are detected very quickly by the visual system [Egeth and Yantis 1997; Mack and 
Rock 1998; Pomerantz and Pristach 1989; Rensink 2000; Simons 2000; Triesman 1985; Triesman and Gormican 
1988; Wolfe 1994; Wolfe et al. 2000]; when combined properly, these same features can be used to construct 
multidimensional displays that support rapid, accurate, and e.ortless exploration and analysis. For example, 
properties of color and texture (e.g., luminance, hue, contrast, or regularity) are often used to represent 
di.erent attributes in a dataset. The way that color and texture are mapped to the data attributes is 
controlled using results from psychophysical studies of our ability to distinguish between di.erent color 
and texture patterns. The application of perception in aid of visualization has shown great promise, 
and has been explicitly cited as an important area of current and future research [Smith and Van Rosendale 
1998]. More recently, we have initiated a new set of investigations that focus on the question: Can we 
make our visualizations aesthetically pleasing? The way an image initially attracts a viewer s attention 
is di.erent from how it holds a viewer s interest over time. Cognitive scientists use the terms orienting 
and engaging to describe the distinct psychophysical processes involved in these two aspects of attention 
[Coren et al. 2003]. Display techniques that invoke these responses could be used to direct attention 
to important properties in a visualization, and to then encourage the visual system to perform a more 
detailed analysis within these areas. The idea of building artistically-motivated visualizations was 
also based on nonphotorealistic rendering algorithms in computer graphics [Curtis et al. 1997; Haberli 
1990; Hertzmann 1998; Hsu and Lee 1994; Litwinowicz 1997; Meier 1996; Strassmann 1986], and by the e.orts 
of researchers such as Interrante Perceptually-Based Brush Strokes for Nonphotorealistic Visualization 
 [Interrante 2000], Laidlaw [Kirby et al. 1999; Laidlaw 2001; Laidlaw et al. 1998], and Ebert and Rheingans 
[Ebert and Rheingans 2000; Rheingans and Ebert 2001] to extend this work to a visualization environment. 
Nonphotorealistic techniques represent a promising method to both orient and engage a viewer s attention 
within an image. Certain movements in painting (e.g., Impressionism, Abstractionism, or watercolor) are 
characterized by a set of fundamental styles [Brown 1978; Schapiro 1997; Venturi 1978]. If the basic 
brush stroke properties embodied in these styles can be identi.ed and simulated on a computer, we believe 
they can then be used to represent individual data attributes in a multidimensional dataset. Our goal 
is an image that looks like a painting, not of a real-world scene, but rather of the information contained 
in the dataset. Such a technique might initially seem di.cult to control and test. An important insight 
is that many brush stroke properties correspond closely to perceptual features that are detected by our 
visual system. In some sense this is not surprising. Artistic masters understood intuitively which properties 
of a painting would orient a viewer s gaze and engage their thoughts. We believe this overlap can act 
as a bridge between artistic styles and low-level vision, allowing us to apply our knowledge of perception 
to predict how nonphotorealistic techniques will perform in a visualization environment. In addition, 
psychophysical experiments o.er a controlled method for studying the fundamental strengths and limitations 
of a given stroke property, both in isolation and in combination with other properties shown together 
in the same display. In order to use the correspondence between painting and perception during multidimensional 
visualization, we need to show that our perceptual guidelines extend to a nonphotorealistic domain. Perceptually 
salient displays will guarantee an e.ective presentation of information. We begin this article with a 
brief overview of nonphotorealistic rendering, followed by a description of painting styles in Impressionism 
and their correspondence to perceptual features in vision. We continue with an explanation of the guidelines 
that are used to construct perceptually salient brush strokes. We next discuss a set of experiments that 
test the expressiveness of our nonphotorealistic properties to con.rm that their abilities match the 
perceptual rules on which they are based. Finally, we describe a visualization system built from our 
experimental .ndings, and demonstrate its use for exploring a collection of multidimensional weather 
datasets. 2. NONPHOTOREALISTIC RENDERING For many years researchers in the areas of modeling and rendering 
in computer graphics have studied the problem of producing photorealistic images, images of graphical 
models that are indistinguishable from photographs of an equivalent real-world scene. Advances in areas 
such as the simulation of global light transport, modeling of natural phenomena, and image-based rendering 
have made dramatic strides towards achieving this goal. At the same time, researchers have approached 
the issue of image generation from a completely di.erent direction. Although photographs are common, 
there are many other compelling methods of visual discourse, for example, oil and watercolor paintings, 
pen and ink sketches, cel animation, and line art. In certain situations, these nonphotorealistic renderings 
are often considered more e.ective, more appropriate, or even more expressive than an equivalent photograph 
[Gooch and Gooch 2001; Strothotte and Schlechtweg 2002] (see Figure 1). Di.erent methods have been suggested 
to simulate di.erent artistic styles. For example, researchers from the University of Washington presented 
a collection of techniques for generating pen-and-ink sketches. Their initial work focused on using multiresolution 
curves [Finkelstein and Salesin 1994] to build a stroke texture, a prioritized collection of simulated 
pen strokes that are drawn to create stroke patches with a speci.c texture and tone. The stroke textures 
are used to construct nonphotorealistic renderings of 3D polygonal models [Winkenbach and Salesin 1994], 
and to interactively convert greyscale reference images into penand-ink sketches [Salisbury et al. 1994]. 
Follow-on work extended the stroke textures to parametric surfaces ACM Transactions on Graphics, Vol. 
23, No. 1, January 2004. 4  Christopher G. Healey et al.  Fig. 1. A simple nonphotorealistic rendering 
of a collection of water lilies; the original image is shown in the upper-left corner [Winkenbach and 
Salesin 1996], allowed the de.nition of directional .elds to control orientation during the sketching 
of 2D reference images [Salisbury et al. 1997], and discussed ways to guarantee a constant tone that 
is independent of scale and display resolution [Salisbury et al. 1996]. Related work by Takagi et al. 
used a voxel-based simulation of the physical properties of paper and colored lead pencils to construct 
color pencil drawings [Takagi and Fujishiro 1997; Takagi et al. 1999]. Finally, Sousa and Buchanan [1999a; 
1999b; 2000] built a sophisticated simulation of graphite pencils, paper, blenders, and erasers to produce 
nonphotorealistic pencil drawings of 3D geometric models; their technique allows for the variation of 
numerous parameters such as pressure on the pencil, the shape of its tip, how the pencil is held by the 
artist, and how the pencil and paper interact. Texture synthesis techniques have been proposed by a number 
of researchers to generate nonphotorealistic results. Initial work by Lewis [1984] allowed viewers to 
interactively paint spectral properties of a texture patch in the frequency domain. Convolution and an 
inverse Fourier transform were applied to generate a randomized spatial version of the texture. This 
result was painted onto a canvas in di.erent ways using di.erent pixel copy operations. Haberli and Segal 
[1993] showed how texture mapping can be used ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 
 Perceptually-Based Brush Strokes for Nonphotorealistic Visualization  to produce a number of fundamental 
drawing primitives, including air brush and painted e.ects. More recently, Hertzmann et al. [2001] constructed 
feature analogies to automatically generate nonphotorealistic results. Texture synthesis techniques were 
applied to two images I and I. to learn how features in I map to corresponding features in I. (e.g., 
I could be a photograph and I. a painterly rendition of the photograph). Hertzmannusedthe resulting image 
analogy to automatically generate a nonphotorealistic image J. from a new source image J. The image analogy 
embodies J. with style properties similar to those seen in I. . Our interests lie mainly in nonphotorealistic 
techniques that use simulated brush strokes to produce images that look like paintings. An early example 
of this idea was proposed by Strassmann [1986]; he constructed a hairy brush , a collection of bristles 
placed along a line segment. Japanese-style sumi brush strokes were produced by applying ink to the brush, 
then moving it along a path over a simulated paper surface. Later work by Haberli [1990] allowed users 
to paint by stroking a brush over an underlying target image. The size, shape, color, location, and direction 
of brush strokes were varied to produce di.erent representations of the target. Hsu and Lee [1994] de.ned 
a reference backbone and reference thickness for a base texture, then warped these properties parametrically 
to produce line art images. This generated expressive strokes with complex paths of varying thickness. 
Litwinowicz [1997] clipped simple strokes to object boundaries in a reference image, then rendered the 
strokes as lines and texture maps with variable length, thickness, direction, and color. A stroke s properties 
were selected based on the image properties of the object it represented. Curtis et al. [1997] built 
a .uid-.ow simulation to model the interactions of brush, pigment, and paper during the painting of watercolor 
images. Their system produced subtle watercolor e.ects such as dry-brush, backruns, and color glazing. 
Shiraishi and Yamaguchi [1999] computed image moments on a target image; these values controlled the 
color, location, orientation, and size of texture-mapped brush strokes in a painterly rendering of the 
target. Hertzmann [1998] decomposed a reference image into a level-of-detail hierarchy by applying Gaussian 
kernels with increasing support. This generated a collection of reference images, each with di.erent 
amounts of blurring. The images were painted using strokes with a radius proportional to the kernel size. 
Each stroke was modeled as a spline that varied in its length, size, opacity, placement, and color jitter. 
The individual paintings were composited to produce a nonphotorealistic result. Meier [1996] addressed 
the goal of animating nonphotorealistic renderings by attaching particles to surfaces in a 3D geometric 
scene, then drawing a brush stroke with scene-controlled color, size, and direction at each particle 
position. Information stored within a particle ensured a consistent stroke appearance. This produced 
a smooth animation free of the visual artifacts that result from inconsistent variations in the appearance 
of a stroke across multiple frames. Gooch et al. [2002] segmented an image into closed regions representing 
image features; the medial axis of a region de.ned the locations and directions of brush strokes that 
were used to paint the region. Finally, Hertzmann [2002] proposed a method for simulating the appearance 
of lighting on the brush strokes in a painting. A height .eld was associated with each brush stroke, 
producing a global height map as the strokes were painted; the height map was then used to bump-map the 
painting with a Phong shading model. More recently, researchers in scienti.c visualization have started 
to investigate how techniques from nonphotorealistic rendering might be used to improve the expressiveness 
of a data display. Laidlaw extended the layered approach of Meier to visualize multidimensional data 
in a painterly fashion [Kirby et al. 1999; Laidlaw 2001; Laidlaw et al. 1998]. He varied style properties 
such as underpainting lightness and stroke size, transparency, direction, saturation, and frequency to 
display multiple layers of information in a single nonphotorealistic image. Interrante [2000] discussed 
constructing natural textures to visualize multidimensional data. Finally, Ebert and Rheingans used 
nonphotorealistic techniques such as silhouettes, sketch lines, and halos to highlight important features 
in a volumetric dataset [Ebert and Rheingans 2000; Rheingans and Ebert 2001]. More recent work applied 
stipple drawing techniques to interactively preview scienti.c and medical volumes [Lu et al. 2002]. ACM 
Transactions on Graphics, Vol. 23, No. 1, January 2004. 6  Christopher G. Healey et al. Nonphotorealistic 
rendering produces images that are expressive by making use of a wide range of painting styles. Promising 
results from scienti.c visualization show that these ideas can be extended to the problem of representing 
information. This suggests that it may be possible to construct .exible brush stroke glyphs to visualize 
multidimensional data elements. To do this properly, however, we must ensure our brush strokes will form 
nonphotorealistic visualizations that are e.ective in their ability to represent multidimensional data. 
The use of nonphotorealistic techniques also holds promise for constructing visualizations that are seen 
as aesthetic or beautiful by our viewers. Our investigations focus on understanding and controlling the 
expressive abilities of di.erent nonphotorealistic brush stroke properties during visualization. These 
properties can then be used to produce nonphotorealistic images that are both e.ective and engaging. 
3. PAINTING STYLES The fundamental properties of a nonphotorealistic image can be identi.ed in part 
by studying the styles used by artists to construct their paintings. Our investigation of nonphotorealistic 
properties is directed by two separate criteria. First, we are restricting our search to a particular 
movement in art known as Impressionism. Second, we attempt to match brush stroke characteristics from 
the Impressionist painting style with corresponding visual features that have been shown to be e.ective 
in a perceptual visualization environment. There are no technical reasons for our choice of Impressionism 
over any other movement. In fact, we expect the basic theories behind our technique will extend to other 
types of artistic presentation. For our initial work, however, we felt it was important to narrow our 
focus to a set of fundamental goals in the context of a single type of painting style. The term Impressionism 
was attached to a small group of French artists (initially including Monet, Degas, Manet, Renoir, and 
Pissaro, and later Cezanne, Sisley, and van Gogh, among others) who broke from the traditional schools 
of the time to approach painting from a new perspective. The Impressionist technique was based on a number 
of underlying principles [Brown 1978; Schapiro 1997; Venturi 1978], for example: (1) Object and environment 
interpenetrate. Outlines of objects are softened or obscured (e.g., Monet s water lilies); objects are 
bathed and interact with light; shadows are colored and movement is represented as un.nished outlines. 
 (2) Color acquires independence. There is no constant hue for an object, atmospheric conditions and 
light moderate color across its surface; objects may reduce to swatches of color. (3) Solicit a viewer 
s optics. Study the retinal system; divide tones as separate brush strokes to vitalize color rather than 
greying with overlapping strokes; harness simultaneous contrast; use models from color scientists such 
as Chevreul [1967] or Rood [1879]. (4) Minimize perspective. Perspective is shortened and distance reduced 
to turn 3D space into a 2D image. (5) Show a small section of nature. The artist is not placed in a 
privileged position relative to nature; the world is shown as a series of close-up details.  Although 
these general characteristics are perhaps less precise than we might prefer, we can still draw a number 
of important conclusions. Properties of hue, luminance, and lighting were explicitly controlled and even 
studied in a scienti.c fashion by some of the Impressionists (e.g., Seurat s use of scienti.c models 
of color [Chevreul 1967; Hering 1964; Rood 1879]). Rather than building an object-based representation, 
the artists appear to be more concerned with subdividing a painting based on the interactions of light 
with color and other surface features. Additional properties can be identi.ed by studying the paintings 
themselves. These properties often varied dramatically between individual artists, acting to de.ne their 
unique painting techniques. Examples include: ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 
 Perceptually-Based Brush Strokes for Nonphotorealistic Visualization  Path. The direction a brush stroke 
follows; van Gogh made extensive use of curved paths to de.ne boundaries and shape in his paintings; 
other artists favored simpler, straighter strokes, Length. The length of individual strokes on the canvas, 
often used to di.erentiate between contextually di.erent parts of a painting, Density. The number and 
size of strokes laid down in a .xed area of canvas, Coverage. The amount of canvas or underpainting that 
shows through the foreground strokes, Coarseness. The coarseness of the brush used to apply a stroke; 
a coarser brush causes visible bristle lines and surface roughness, and Weight. The amount of paint applied 
during each stroke; heavy strokes highlight coarseness and stroke boundaries, and produce ridges of paint 
that cause underhanging shadows when lit from the proper direction. Figure 2 shows a close-up view of 
an oil painting that demonstrates di.erent brush stroke properties such as color, path, size, and density. 
Although by no means exhaustive, this collection of features provides a good starting point for our work. 
All of the stroke properties we use are evaluated for e.ectiveness by identifying their perceptual characteristics, 
and by validating their ability to support visualization, discovery, analysis, and presentation in a 
real-world application environment. 4. PERCEPTUAL PROPERTIES Recent research in visualization has explored 
ways to apply rules of perception to produce images that are visually salient [Ware 2000]. This work 
is based in large part on psychophysical studies of the low-level human visual system. One of the most 
important lessons of the past twenty-.ve years is that human vision does not resemble the relatively 
faithful and largely passive process of modern photography [Pomerantz and Pristach 1989; Triesman 1985; 
Triesman and Gormican 1988; Wolfe 1994; Wolfe et al. 2000]. The goal of human vision is not to create 
a replica or image of the seen world in our heads. A much better metaphor for vision is that of a dynamic 
and ongoing construction project, where the products being built are short-lived models of the external 
world that are speci.cally designed for the current visually guided tasks of the viewer [Egeth and Yantis 
1997; Mack and Rock 1998; Rensink 2000; Simons 2000]. There does not appear to be any general purpose 
vision. What we see when confronted with a new scene depends as much on our goals and expectations as 
it does on the array of light that enters our eyes. Among the research .ndings responsible for this altered 
view of seeing is a greater appreciation of: (1) Detailed form and color vision is only possible for 
a tiny window of several degrees of arc surrounding the current gaze location. Seeing beyond the single 
glance therefore requires a time-consuming series of eye movements. (2) The eye movements that are needed 
to process a whole scene, such as the 180. view we often assume we have, are discrete. Many of them must 
be made in order to see the detail in a large scene, and almost no visual information is gained during 
an eye movement itself. (3) Memory for information from one glance to the next is extremely limited. 
At most, the details from only three or four objects can be monitored between glances; perception is 
often limited to only one object at a time. What we see therefore depends critically on which objects 
in a scene we are looking for and attending to. (4) Human vision is designed to capitalize on the assumption 
that the world is generally a quiet place. Only di.erences need to be registered. Objects that are very 
di.erent from their surroundings, or that change or move, draw attention to themselves because of the 
di.erence signals that emanate from these visual .eld locations.  ACM Transactions on Graphics, Vol. 
23, No. 1, January 2004. 8  Christopher G. Healey et al.  Fig. 2. A close-up view of a small section 
of an oil painting that demonstrates various stroke properties such as color, path, size, and density; 
the entire painting is shown in the upper-left corner (5) The basic visual features that can be used 
to guide attention are not large in number. They include di.erences in the .rst order properties of luminance 
and hue, and the second-order properties of orientation, texture, and motion. E.ective third-order properties 
are limited to some very simple characteristics of shape such as length, area, and convexity. The reality 
of each of these .ndings can be illustrated through the so-called change blindness which a.ects us all 
[Mack and Rock 1998; Rensink 2000; Simons 2000]. It involves a task similar to a game that has amused 
children reading the comic strips for many years. Try to .nd the di.erence between the two pictures in 
Figures 3 and 4. Many viewers have a di.cult time seeing any di.erence and often have to be coached to 
look carefully to .nd it. Once they have discovered it, they realize that the di.erence was not a subtle 
one. Change blindness is not a failure to see because of limited visual acuity; rather, it is a failure 
based on inappropriate attentional guidance. Some parts of the eye and the brain are clearly responding 
di.erently to the two pictures. Yet, this does not become part of our visual experience until attention 
is focused directly on the objects that vary. Harnessing human vision for the purposes of data visualization 
therefore requires that the images themselves be constructed so as to draw attention to their important 
parts. Since the displays being shown are typically novel, we cannot rely on the expectations that might 
accompany the viewing of a familiar scene. Rather, we must build an e.ective mapping between data values 
and visual features, so that di.erences in the features draw the eyes, and more importantly the mind, 
on their own. Attracting the viewer s gaze to a particular object or location in a scene is the .rst 
step in having the viewer form a mental representation Perceptually-Based Brush Strokes for Nonphotorealistic 
Visualization  Fig. 3. An example of change blindness, the inability to quickly identify signi.cant 
di.erences across separate views of a common scene; try to identify the di.erence between this photograph 
and the photograph shown in Figure 4 (the answer is included in footnote 1 on the next page) that may 
persist over subsequent scenes. A data-feature mapping that builds on our knowledge of perception can 
support the exploration and analysis of large amounts of data in a relatively short period of time. The 
ability to take advantage of the low-level visual system is especially attractive, since: completion 
of high-level exploration and analysis tasks (e.g., target search, boundary and region identi. cation, 
estimation, or spatial and temporal tracking) is rapid and accurate, usually requiring an exposure duration 
of 200 milliseconds or less, analysis is display size insensitive, so the time required to complete 
a task is independent of the number of elements in the display, and di.erent features can interact with 
one another to mask information; psychophysical experiments allow us to identify and avoid these visual 
interference patterns. Our most recent research has focused on the combined use of the fundamental properties 
of color (hue and luminance) and texture (size, contrast, orientation, and regularity) to encode multiple 
attributes in a single display [Healey 1996; Healey and Enns 1998; 1999]. A comparison of perceptual 
color and texture properties with painting styles from Impressionist art reveals a strong correspondence 
between the two. Reduced to perceptual elements, color and texture are the precise properties that an 
artist varies in the application of colored pigments of paint to a canvas with a brush. From this perspective, 
color and lighting in Impressionism has a direct relationship to the use of hue and luminance in perceptual 
vision. Other brush stroke properties (e.g., path, density, and length) have similar partners in perception 
(e.g., orientation, contrast, and size). This close correspondence between perceptual features and many 
of the nonphotorealistic properties we hope to apply is particularly advantageous. Since numerous controlled 
experiments on the use of perception have already been conducted, we have a large body of knowledge to 
draw from to predict how we expect our ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 10 
 Christopher G. Healey et al.  Fig. 4. An example of change blindness, the inability to quickly identify 
signi.cant di.erences across separate views of a common scene; try to identify the di.erence between 
this photograph and the photograph shown in Figure 3 (the answer is included in footnote 1 below) brush 
stroke properties to react in a multidimensional visualization environment. We applied three speci.c 
areas of research in perception and visualization to guide the use of properties of our nonphotorealistic 
brush strokes: color selection, texture selection, and feature hierarchies that cause visual interference 
and masking. 4.1 Color Selection Color is a common feature used in many visualization designs. Examples 
of simple color scales include the rainbow spectrum, red-blue or red-green ramps, and the grey-red saturation 
scale [Ware 1988]. More sophisticated techniques attempt to control the di.erence viewers perceive between 
di.erent colors, as opposed to the distance between their positions in RGB space. This improvement allows: 
perceptual balance: a unit step anywhere along the color scale produces a perceptually uniform di.erence 
in color, distinguishability: within a discrete collection of colors, every color is equally distinguishable 
from all the others (i.e., no speci.c color is easier or harder to identify), and .exibility: colors 
can be selected from any part of color space (e.g., the selection technique is not restricted to only 
greens, or only reds and blues). Color models such as CIE LUV, CIE Lab, or Munsell can be used to provide 
a rough measure of perceptual balance [Birren 1969; CIE 1978; Munsell 1905]. Within these models, Euclidean 
distance is used to estimate perceived color di.erence. More complex techniques re.ne this basic idea. 
Rheingans and Tebbs [1990] plotted a path through a perceptually balanced color model, then asked viewers 
to de.ne how attribute 1Hint: Look at the bushes immediately behind the back of the Sphinx ACM Transactions 
on Graphics, Vol. 23, No. 1, January 2004. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization 
 11 values map to positions along the path. Non-linear mappings emphasize di.erences in speci.c parts 
of an attribute s domain (e.g., in the lower end with a logarithmic mapping, or in the higher end with 
an exponential mapping). Other researchers have constructed rules to automatically select a colormap 
for a target data attribute [Bergman et al. 1995; Rogowitz and Treinish 1993]. Properties of the attribute 
such as its spatial frequency, its continuous or discrete nature, and the type of analysis to be performed 
are used to choose an appropriate color representation. Ware [1988] constructed a color scale that spirals 
up around the luminance axis to maintain a uniform simultaneous contrast error along its length. His 
solution matched or outperformed traditional color scales for metric and form identi.cation tasks. Healey 
and Enns showed that color distance, linear separation, and color category must all be controlled to 
select discrete collections of equally distinguishable colors [Healey 1996; Healey and Enns 1999]. Our 
color selection technique combines di.erent aspects of each of these methods. A single loop spiraling 
up around the L-axis (the luminance pole) is plotted near the boundary of our monitor s gamut of displayable 
colors in CIE LUV space. The path is subdivided into r named color regions (i.e., a blue region, a green 
n  colors uniformly spaced along each of the region, and so on). n colors can then be selected by choosing 
r r color regions. The result is a set of colors selected from a perceptually balanced color model, 
each with a roughly constant simultaneous contrast error, and chosen such that color distance and linear 
separation are constant within each named color region. 4.2 Texture Selection Texture is often viewed 
as a single visual feature. Like color, however, it can be decomposed into a collection of fundamental 
perceptual dimensions. Researchers in computer vision have used properties such as regularity, directionality, 
contrast, size, and coarseness to perform automatic texture segmentation and classi.cation [Haralick 
et al. 1973; Rao and Lohse 1993a; 1993b; Tamura et al. 1978]. These texture features were derived both 
from statistical analysis, and through experimental study. Results from psychophysics have shown that 
many of these properties are also detected by the low-level visual system, although not always in ways 
that are identical to computer-based algorithms [Aks and Enns 1996; Cutting and Millard 1984; Julesz 
1975; 1984; Julesz et al. 1973; 1978; Snowden 1998; Triesman 1991; Wolfe 1994]. One promising approach 
in visualization has been to use perceptual texture dimensions to represent multiple data attributes. 
Individual values of an attribute control its corresponding texture dimension. The result is a texture 
pattern that changes its visual appearance based on data in the underlying dataset. Grinstein et al. 
[1989] visualized multidimensional data with stick-man icons whose limbs encode attribute values stored 
in a data element; when the stick-men are arrayed across a display, they form texture patterns whose 
spatial groupings and boundaries identify attribute correspondence. Ware and Knight [1995] designed Gabor 
.lters that modi.ed their orientation, size, and contrast based on the values of three independent data 
attributes. Healey and Enns [1998; 1999] constructed perceptual texture elements (or pexels) that varied 
in size, density, and regularity; results showed that size and density are perceptually salient, but 
variations in regularity are much more di.cult to identify. More recent work found that orientation can 
also be used to encode information [Weigle et al. 2000]; a di.erence of 15. is su.cient to rapidly distinguish 
elements from one another. We designed brush strokes that can vary in their area, orientation, spatial 
density, and regularity (in addition to color). These texture dimensions correspond closely to the nonphotorealistic 
properties size, direction, coverage, and placement. The displays in Figures 5 and 6, used to measure 
target detection performance, show examples of each of these properties. 4.3 Feature Hierarchy A third 
issue that must be considered is visual interference. This occurs when the presence of one feature masks 
another. Although the need to measure a brush stroke s perceptual strength is not necessary in a ACM 
Transactions on Graphics, Vol. 23, No. 1, January 2004. 12  Christopher G. Healey et al.  (a) (b) (c) 
 (d) (e) (f)  Fig. 5. Examples of target detection, color targets with constant orientation (top) and 
random orientation (bottom); (a) orange target in pink strokes, constant 45. background orientation; 
(b) green target in orange strokes, constant 60. background orientation; (c) green target in orange strokes, 
constant 45. background orientation; (d) orange target in pink strokes, random 45. and 60. background 
orientation; (e) green target in orange strokes, random 30. and 45. background orientation; (f) green 
target in orange strokes, random 45. and 60. background orientation painting, this information is critical 
for e.ective visualization design. The most important attributes (as de.ned by the viewer) should be 
displayed using the most salient features. Secondary data should never be visualized in a way that masks 
the information a viewer wants to see. Certain perceptual features are ordered in a hierarchy by the 
low-level visual system. Results reported in both the psychophysical and visualization literature have 
con.rmed a luminance hue texture interference pattern. Variations in luminance interfere with a viewer 
s ability to identify the presence of individual hues and the spatial patterns they form [Callaghan 1990]. 
If luminance is held constant across the display, these same hue patterns are immediately visible. The 
interference is asymmetric: random variations in hue have Perceptually-Based Brush Strokes for Nonphotorealistic 
Visualization  13 (a) (b) (c) (d) (e) (f)  Fig. 6. Examples of target detection, orientation targets 
with constant color (top) and random color (bottom); (a) 45. target in 30. strokes, constant pink background 
color; (b) 60. target in 45. strokes, constant green background color; (c) 45. target in 30. strokes, 
constant pink background color; (d) 45. target in 30. strokes, random pink and orange background color; 
(e) 60. target in 45. strokes, random pink and orange background color; (f) 45. target in 30. strokes, 
random orange and green background color no e.ect on a viewer s ability to see luminance patterns. A 
similar hue on texture interference has also been shown to exist [Healey and Enns 1998; 1999; Snowden 
1998; Triesman 1985]; random variations in hue interfere with the identi.cation of texture patterns, 
but not vice-versa. Figure 5 shows examples of hue on orientation interference. The upper three displays 
use a constant background orientation (Figures 5(a) (c)), while the lower three vary orientation randomly 
from stroke to stroke (Figures 5(d) (f)). This has no e.ect on a viewer s ability to locate a target 
group by de.ned color; identi.cation is rapid and accurate for both sets of displays. In Figure 6 the 
mapping is reversed: background color is held constant in the upper three displays (Figures 6(a) (c)), 
and varied randomly in the lower three ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 14 
 Christopher G. Healey et al. (Figures 6(d) (f)). Locating a target group of strokes rotated counterclockwise 
from their neighbors is much harder when color varies randomly, compared to the displays where color 
is held constant. What the visual system sees initially is a random color pattern. Only with additional 
exposure time will di.erences in orientation be reported. Feature interference results suggest that luminance, 
then hue, then various texture properties should be used to display attributes in order of importance. 
Real-world evidence has con.rmed that this technique works well in practice. 4.4 Orienting Versus Engaging 
Attention We are interested in two properties of a nonphotorealistic visualization: its e.ectiveness 
and its aesthetic merit. These properties correspond to two basic aspects of human attention: orienting 
and engaging [Coren et al. 2003]. Orienting attention to a speci.c location in an image occurs when the 
location contains an abrupt transition in a visual feature that is processed by the low-level visual 
system (e.g., a high-contrast luminance edge, a brief .icker, or a motion discontinuity). This may include 
redirecting a viewer s gaze so that the foveal center of the eye is aimed at the region of interest, 
although this is not required. Visual processes can operate selectively on areas of high visual salience 
through a process called covert orienting [Posner and Raichle 1994]. Rapidly orienting a viewer s attention 
to novel or important areas in a visualization is the .rst step towards allowing the viewer to e.ciently 
discover, explore, and analyze within their data. The process of orienting is di.erent from engaging 
attention in two important ways. First, while orienting is often a momentary event based largely on the 
nature of an image, engaging re.ects the conscious intention of the viewer to search for speci.c information. 
For example, engaging is the process that allows the search for a di.cult-to-.nd target to continue, 
even when no low-level visual evidence exists to orient the visual system to the target s location. Second, 
di.erent neurological foundations are believed to control the two aspects of attention. Orienting is 
governed by the older, sub-cortical visual pathways. Engaging is determined by a network of cortical 
regions that are in close communication with the frontal lobes, the so-called central executive of the 
human brain [Posner and Raichle 1994]. Skilled visual artists are adept at exploiting these complimentary 
aspects of visual attention, even though they may do so intuitively, without understanding the underlying 
neural processes [Zeki 1999]. For example, masters of the human portrait such as Vermeer, Titian, and 
Rembrandt painted the faces of people such that the region of greatest detail and .nest spatial resolution 
was the face itself. Properties of the background and the model s clothing are often presented in shadow 
or rendered with much less resolution and contrast. This has the e.ect of drawing the viewer s eye towards 
the face, which is the center of interest in the portrait. At the same time, these artists reserved another 
small region away from the face for the most extreme contrast. This was often the collar of the model, 
a piece of jewelry, or a background surface detail. This localized region of high contrast pulls at the 
viewer s orienting system, even as the viewer tries to engage their attention on the portrait s face. 
It has recently been proposed that this interaction between orienting and engaging underlies our fascination 
with and artistic appreciation of these works [Ramachandran and Hirstein 1999; Zeki 1999]. Psychologists 
believe they may soon understand the neural substrate of this aspect of creative tension, an idea that 
is usually thought to be highly abstract. We believe that orienting and engaging are both important to 
a successful visualization. Orienting allows us to highlight important regions in an image by capturing 
the viewer s focus of attention. Engaging encourages the visual system to continue to study the details 
of an image after orienting occurs. We are pursuing nonphotorealistic visualizations as a promising 
way to build images with exactly these characteristics. Orienting occurs through the careful use of visual 
features that are rapidly detected by the low-level visual system. Engaging is achieved by constructing 
visualizations that are perceived to be beautiful or artistic by the viewer. The studies described in 
this article represent our initial steps towards investigating di.erent aspects of attention in the context 
of our nonphotorealistic visualization techniques. ACM Transactions on Graphics, Vol. 23, No. 1, January 
2004. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization  15 5. EFFECTIVENESS STUDIES 
 The .rst question we wanted to answer is whether guidelines on the use of perception in glyph-based 
visualizations will extend to our nonphotorealistic domain. We conducted a set of psychophysical experiments 
to test this hypothesis. Our experiments were designed to investigate an observer s ability to rapidly 
and accurately identify target brush strokes de.ned by a particular color or orientation [Liu et al. 
2003]. Observers were asked to determine whether a small, 3  3 group of strokes with a particular visual 
feature was present or absent in a display (e.g., a group of orange strokes, as in Figures 5(a), 5(c), 
5(d), and 5(f), or a group strokes tilted 60. in Figures 6(b) and 6(e)). Background orientation, color, 
regularity, and density varied between displays. This allowed us to test for single-glance task performance, 
and for visual interference e.ects. Since observers need at least 200 milliseconds to initiate an eye 
movement, any task performed in 200 milliseconds or less is completed based on a single glance at the 
image. In all cases, observer accuracy and response times were recorded to measure performance. The experimental 
results were then used to identify similarities and di.erences between nonphotorealistic images and existing 
perceptual visualization techniques. 5.1 Design Each experimental display contained a 22  22 array 
of simulated brush strokes (Figures 5 and 6). The color of the displays was calibrated to the monitor 
to ensure accurate reproduction. Observers were asked to determine whether a group of strokes with a 
particular target type was present or absent in each display. Displays were shown for 200 milliseconds, 
after which the screen was cleared; the system then waited for observers to enter their answer: target 
present or target absent. Observers were told to respond as quickly as possible, while still maintaining 
a high rate of accuracy. Feedback was provided after each display: a + sign if an observer s answer was 
correct, or a - sign if it was not. The displays were equally divided into two groups: one studied an 
observer s ability to identify target strokes based on color, the other studied identi.cation based on 
orientation. The appearance of the strokes in each display was varied to test for single-glance performance 
and visual interference. For example, the following experimental conditions were used to investigate 
an observer s ability to identify colored strokes: Two target-background color pairings. An orange target 
in a pink background, or a green target in an orange background; this allowed us to test for generality 
in observer performance for di.erent target background color pairings, Two background orientations. 
Constant (every stroke is oriented in the same direction, either 30. or 60.), or random (strokes are 
randomly oriented 30. and 45.,or 45. and 60.); any decrease in performance from a constant to a random 
background would indicate visual interference from orientation during the search for color targets, Three 
background densities. The size of the strokes in the display were varied to produce sparse, dense, or 
very dense patterns; this allowed us to see how changes in density a.ected target identi.cation, and 
Two background regularities. Strokes were arrayed in a regular grid pattern, or jittered randomly across 
the display; this allowed us to test for visual interference caused by spatial irregularity in the global 
texture. Our experimental conditions produced 24 di.erent color display types (two target-background 
color pairings by two background orientations by three background densities by two background regularities). 
Observers were asked to view eight variations of each display type, for a total of 192 color trials. 
For each display type, half the trials were randomly chosen to contain a group of target strokes; the 
other half did not. Examples of six color displays are shown in Figure 5. Each display contains either 
an orange target in a sea of pink strokes (Figures 5(a), 5c, 5(d), and 5(f)), or a green target in a 
sea of orange strokes (Figures 5(b) ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 16  
Christopher G. Healey et al. and 5(e)). In the upper three displays the background orientation of the 
strokes is constant (either 45. or 60.). The coverage is dense in Figure 5(a), sparse in Figure 5(b), 
and very dense in Figure 5(c). The strokes are arrayed in a regular pattern in Figure 5(a), and randomly 
jittered in Figures 5(b) and 5(c). The lower three displays are identical, except for the background 
orientation. In Figure 5(d) half the strokes were randomly selected to be oriented 45.; the other half 
are oriented 60. . In Figures 5(e) and 5(f) half the strokes are oriented 30., and half are oriented 
45. . The displays that studied orientation were designed in an identical fashion. Two target-background 
orientation pairings were tested: target strokes oriented 45. in a sea of background strokes oriented 
30.,or 60. targets in a 45. background. Two di.erent color patterns were used to search for color on 
orientation interference: constant (every stroke has the same color, either green or pink), or random 
(strokes are randomly colored green and orange, or orange and pink). Background densities and regularities 
are identical to the color displays. As before, eight variations of each display type were shown for 
a total of 192 orientation trials. Figures 6(a), 6(c), 6(d), and 6(f) show examples of 45. target strokes 
in a sea of 30. background elements. Figures 6(b) and 6(e) show a 60. target in a 45. background. The 
upper three displays have a constant background color (either pink or green). The strokes are densely 
packed and regularly positioned in Figure 6(a), sparsely packed and randomly jittered in Figure 6(b), 
and very densely packed and randomly jittered in Figure 6(c). The lower three displays are identical, 
except for the background color of the strokes. In Figures 6(d) and 6(e) half the strokes were randomly 
selected to be colored pink; the other half are colored orange. In Figure 6(f) half are colored orange, 
and half are colored green. The colors, orientations, densities, and regularities we used were chosen 
based on results from previous experiments [Healey and Enns 1998; 1999; Weigle et al. 2000]. In particular, 
the colors and orientations we selected were shown to be rapidly distinguishable from one another when 
displayed in isolation (i.e., without variations in irrelevant background dimensions). Eighteen observers 
(six males and twelve females ranging in age from 18 to 28) with normal or corrected acuity and normal 
color vision participated during our studies. The observers were undergraduate and graduate student 
volunteers, none of whom had any prior experience with scienti.c visualization. Every observer completed 
both the color and the orientation experiments within our minimum accuracy requirements of 60% or better 
for each target type. Observers were told before an experiment that half the trials would contain a target, 
and half would not. Observers completed a practice session with 24 trials before each experiment (i.e., 
color practice trials before the color experiment, and orientation practice trials before the orientation 
experiment). Observers were counterbalanced: half started with the color experiment, while the other 
half started with the orientation experiment. We used a Macintosh computer with a 24-bit color display 
to run our studies. Answers (either target present or target absent ) and response times for each trial 
an observer completed were recorded for later analysis. 5.2 Results Each observer response collected 
during our experiments was classi.ed by condition: target-background pairing, primary background type 
(either constant or random), density, regularity, and target present or absent. Trials with the same 
conditions were collapsed to produce an average accuracy a and an average response time t. We used these 
values to compute a measure of search ine.ciency for each observer in each t  ; this is a common measurement 
for situations where the direction of change in accuracy and condition e = a response time is the same 
in each experimental condition. If observer responses are perfect (i.e., a =1.0), ine.ciency e equals 
response time; as accuracy decreases, ine.ciency e increases (i.e., search ine.ciency increases both 
for longer response times and for increased error rates). Results were tested for signi.cance with a 
multifactor analysis of variance (ANOVA). We used a standard 95% con.dence interval to denote signi.cant 
variation in mean ine.ciency values. ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. Perceptually-Based 
Brush Strokes for Nonphotorealistic Visualization  17 We .rst conducted preliminary ANOVAs examining 
all possible factors, separately for accuracy data a and response time t. These analyses indicated that: 
(1) some factors were not signi.cantly related to our measures of performance (speci.cally, target presence-absence 
and target-background pairing), and (2) a and twere highly correlated. Our primary analyses were therefore 
based on the search ine.ciency measure eand the signi.cant factors of target type (color or orientation), 
primary background (constant or random), density (sparse, dense, or very dense), and regularity (regular 
or irregular). In summary, our results showed: (1) Color targets were easy to detect at our 200 millisecond 
single-glance exposure duration (mean accuracy a =91.1% and mean ine.ciency e = 811.9 over all experimental 
conditions); a random orientation pattern had no interfering e.ect on performance. (2) Orientation targets 
were easy to detect when a constant color was displayed in the background (a=71.9% and e = 1327.7 for 
constant color trials); a random background color pattern caused a signi.cant reduction in performance 
(a=67.9% and e= 1437.8 for random color trials). (3) Background density had a signi.cant e.ect on both 
color and orientation targets; denser displays produced an improvement in performance. (4) Background 
regularity had a signi.cant e.ect on both color and orientation targets; irregular displays caused a 
reduction in performance.  Color targets were easy to identify, moreover, a random variation in background 
orientation had no e.ect on performance (F(1,17) = 0.01,p< 0.94 with e = 813.7,a =91.2% for constant 
orientation, and e = 810.2,a =90.9% for random orientation). Orientation targets were easy to identify 
in a constant color background, although performance was not as good as for color targets (e= 1327.7,a=71.9%). 
A random color pattern produced a signi.cant reduction in performance (F(1,17) = 8.08,p< 0.05, with e= 
1437.8,a=67.9%). Variation in background density had a signi.cant e.ect on performance, both for color 
targets (F(2,34) = 30.84,p<0.001) and for orientation targets (F(2,34) = 7.85,p<0.01). In all cases accuracy 
and ine.ciency were best for very dense packings (e= 708.1,a=96.9% for very dense color trials; e= 1245.3,a= 
75.2% for very dense orientation trials), and worst for sparse packings (e= 953.9,a=83.0% for sparse 
color trials; e= 1511.2,a=66.0% for sparse orientation trials). Variation in background regularity also 
had a signi.cant e.ect on performance, both for color targets (F(1,17) = 5.10,p<0.04) and for orientation 
targets (F(1,17) = 24.89,p<0.001). In all cases accuracy and ine.ciency were best for regular trials 
(e= 787.5,a=92.5% for regular color trials; e= 1235.7,a= 75.6% for regular orientation trials), and worst 
for irregular trials (e= 834.7,a=89.7% for irregular color trials; e= 1523.7,a=64.1% for irregular orientation 
trials). Finally, we observed a density  regularity interaction for color trials (F(2,34) = 5.34,p< 
0.01): variations in performance were larger for harder trials. For example, the e.ect of irregularity 
was larger for sparse color trials, compared to very dense color trials; the e.ect of density was larger 
for irregular trials, compared to regular trials. The same interaction pattern was seen for the orientation 
trials, but the e.ect was only marginally signi.cant (F(2,34) = 2.93,p<0.07). 5.3 Interpretation Our 
results match previous .ndings in both the psychophysical and the visualization literature, speci.cally: 
(1) color produces better performance than orientation during target identi.cation (F(1,17)= 71.51,p< 
0.001 for our experiments), and (2) an asymmetric color on texture interference e.ect exists (random 
color patterns interfere with orientation identi.cation, but not vice-versa). Both results have been 
reported in experimental [Callaghan 1990; Snowden 1998] and real-world visualization settings [Healey 
and Enns 1998; 1999]. Our results extend the work of Healey and Enns, who found a general color on texture 
interference ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 18  Christopher G. Healey et 
al. pattern, but no corresponding texture on color e.ect [Healey and Enns 1999]. This provides positive 
evidence to support the belief that perceptual .ndings will carry to a nonphotorealistic visualization 
environment. The improvement in performance when density increased, both for color and orientation targets, 
was encouraging. An initial concern we discussed was that texture variations (e.g., orientation di.erences) 
would disappear when density increased and background color was held constant. Our results show that, 
for the types of strokes we displayed, di.erent orientations are not lost in the background, even when 
a signi.cant stroke overlap exists. This supports our goal of producing painterly images that contain 
dense stroke regions, yet at the same time allow viewers to rapidly identify variations in the underlying 
texture properties. Finally, the reduction in performance when strokes were irregularly positioned was 
intriguing. We concluded that regularity acts as a reinforcing visual cue, helping observers identify 
targets based on some other feature (e.g., color or orientation). The presence of a target patch breaks 
the regularity pattern, providing an additional visual signal that identi.es the presence of a target. 
Jittering the strokes removes this background support. In this sense, irregularity is not so much an 
interfering e.ect as it is the loss of a secondary feature that helps to highlight the presence of a 
group of target strokes. 6. NONPHOTOREALISTIC VISUALIZATION Based on the results from our experiments, 
we built a nonphotorealistic visualization system that varied brush stroke color, orientation, coverage 
(i.e., spatial density), and size to encode up to four data attributes (in addition to the two spatial 
values used to position each stroke). The presence of feature hierarchies suggest color should be used 
to represent the most important attribute, followed by texture properties. Our results further re.ne 
this to mapping color, coverage, size, and orientation in order of attribute importance (from most important 
to least important). 6.1 Painting Algorithm To produce nonphotorealistic visualizations, we must convert 
a dataset D into a nonphotorealistic image using a data-feature mapping M. We wanted to design a technique 
that was based in part on the way that artists paint on a canvas. To this end, we implemented an algorithm 
that spatially subdivides D into common regions (objects) based on attribute value, then paints each 
region independently to produce a .nished result. Our technique follows four basic steps: (1) Segment 
D into p spatially connected regions, where the attribute values ai,j for the elements in each region 
Rk are within a given tolerance Cj of one another. (2) For each region Rk containing elements e1,...,et, 
compute a region-global stroke coverage from the  .t average value 1 ai,j ,where Aj is the attribute 
represented by coverage. t i=1 (3) Paint strokes at randomly selected positions within Rk. The color, 
orientation, and size of each stroke are controlled by the attribute values of the element closest to 
the stroke s center. A stroke is accepted or rejected based on its overlap with existing strokes, and 
on its overlap with Rk. This process continues until Rk s required coverage is met. (4) After all p 
regions are painted, display the result to the viewer.  Step three represents an important di.erence 
between our nonphotorealistic technique and glyph-based visualizations. Most glyph algorithms use a one-to-one 
or one-to-many mapping to represent each data element with individual glyphs. We wanted a method that 
was more analogous to how paintings are constructed: objects in a scene are identi.ed and painted in 
turn. This is done by segmenting a dataset into spatial regions, then painting strokes within each region 
until an appropriate stroke coverage is met. In our technique strokes do not correspond to speci.c data 
elements, rather, the strokes are bound to the elements indirectly through the segments they belong to. 
ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. Perceptually-Based Brush Strokes for Nonphotorealistic 
Visualization  19 (e) (f) (g) (h) Fig. 7. Examples of di.erent segmentation algorithms applied to an 
RGB image of a golden poppy; (a, b, c, d) segments with a .xed, running average, weighted average with 
w = 1, and weighted average with w = 7 , respectively; (e, f, g, h) segments 8 overlaid on the original 
RGB image Segmentation is performed using a modi.ed region-growing algorithm. The .rst element e1 of 
a new segment Rk is selected from a list of elements that do not belong to any segment. Average attribute 
values aj = a1,j,j =1,...,m are initialized based on e1. Rk is then grown as follows: (1) Consider all 
elements in the eight-neighbor region around e1. (2) If a neighboring element ei is not part of some 
other segment, and if |aj -ai,j |.Cj =j,add ei to Rk. (3) Update aj based on ai,j , then recursively 
consider the neighbors of ei. (4) Continue until no more elements can be added to Rk.  Some care must 
be used during the updating of aj . We do not use the initial a1,j as a .xed average, for example, since 
this produces segments that are too sensitive to the selection of e1. Consider the visual example shown 
in Figure 7, where we segment a dataset of pixels with m = 3 attributes: red, green, and blue. The segment 
generated with .xed averages and e1 selected from the lower-left corner of the image is shown in grey 
in Figures 7(a) and 7(e). Because the choice of e1 produced aj that were relatively dark, the segment 
is smaller than expected. Since aj do not change as the segment is constructed, we cannot correct for 
this initial decision. Updating aj for each ei forces the averages to follow the structure of the segment 
as it grows. New attribute values ai,j must be properly weighted when they are added to aj , however. 
Consider 1  (aj + ai,j )for each newelement ei.This Figures 7(b) and 7(f), which use a simple running 
average aj = 2 places too much importance on the attribute values of ei, producing segments that are 
too large. Intuitively, a running average pushes aj too far in the direction of ei; if neighboring elements 
have similar attribute values, this signi.cantly increases the likelihood that these neighbors will be 
also accepted into the segment. The technique we implemented uses weighted averages to build data segments. 
Given elements e1,e2, ..., et, the average values at step t during segment construction are: ACM Transactions 
on Graphics, Vol. 23, No. 1, January 2004. 20  Christopher G. Healey et al.  Fig. 8. An example of 
our segmentation and brush stroke model being used to produce a nonphotorealistic rendering from an RGB 
image of a golden poppy 1 01 t-1 aj = .(wa1,j + wa2,j + + wat,j ),j =1,...,m (1) t wi-1 i=1 w is 
used to weight the contribution of each new element. When w =1, aj is a simple average of the attribute 
values within Rk.When w< 1, each additional element has a monotonically smaller e.ect on aj , allowing 
the averages to converge to near-constant values. This is particularly useful when we visualize datasets 
with smooth gradients. Specifying w< 1 allows the construction of segments that do not expand . t max 
max to .ll the entire gradient. The fraction 1/ i-1 clamps aj to lie in the range 0 ...a,where ais i=1 
wjj the largest possible value for attribute Aj . Figure 7 shows examples of two weighted average segments. 
In Figures 7(c) and 7(g) the averages are updated using w = 1. In Figures 7(d) and 7(h) the segment is 
built with w =0.875. This produces a smaller result, since elements past the .rst few contribute little 
to each aj (e.g., the tenth element at t= 10 accounts Perceptually-Based Brush Strokes for Nonphotorealistic 
Visualization  21 .10 for 0.8759/ 0.875i-1 =0.051, or approximately 5.1% of the segment average). By 
varying w,we can i=1 control the relative size of the segments we generate. Each segment Rk is painted 
by randomly placing brush strokes inside it. The percentage of Rk to be covered . t by its strokes (coverage) 
is de.ned based on 1 ai,j ,where Aj is the attribute that represents coverage. ti=1 Because the elements 
ei within Rk must have similar attribute values, a region-global coverage produces an acceptable representation 
of Aj within Rk. As each new stroke is placed, two values are computed: the overlap with existing strokes, 
and the overlap with Rk s extent. If the stroke overlap is too high, or if the segment overlap is too 
low, the stroke is rejected. The allowable stroke overlap is slowly increased to ensure that Rk s coverage 
can be met. The color, orientation, and size of each stroke are chosen using the attribute values of 
the element closest to the stroke s center. The brush strokes used in our current prototype are identical 
to the ones shown during our experiments. They are constructed with a simple texture mapping scheme. 
This technique is common in nonphotorealistic rendering (e.g., in Haberli [1990], Hertzmann [1998], Litwinowicz 
[1997], and Meier [1996]). Real painted strokes are digitally captured and converted into texture maps. 
The textures are applied to an underlying polygon to produce a collection of generic brush strokes. We 
use a small library of representative stroke textures. One of the textures is randomly selected and bound 
to a stroke when it is placed. This produces a more random, hand-generated feel to the resulting images. 
The nonphotorealistic rendering of the complete golden poppy image is shown in Figure 8. Additional examples 
of renderings and visualizations are shown in Figures 1, 9, and 10. 6.2 Practical Applications One 
of the application testbeds for our nonphotorealistic visualization technique is a collection of monthly 
environmental and weather conditions collected and recorded by the Intergovernmental Panel on Climate 
Change. This dataset contains mean monthly surface climate readings in 1 latitude and longitude steps 
for 2 the years 1961 to 1990 (e.g., readings for January averaged over the years 1961-1990, readings 
for February averaged over 1961-1990, and so on). We chose to visualize values for mean temperature, 
wind speed, pressure, and precipitation. Based on this order of importance, we built a data-feature mapping 
M that varies brush stroke color, coverage, size, and orientation. This mapping divides the concept of 
spatial density into two separate parts: size, the size of the strokes used to represent a data element 
ei,and coverage, the percentage of ei s screen space covered by its strokes. Both properties represent 
brush stroke features. Size describes the energy of strokes in a .xed region of a painting (e.g., a few 
long, broad, lazy strokes or many small, short, energetic strokes). Coverage describes the amount of 
the underlying canvas, if any, that shows through the strokes. This produced the following data-feature 
mapping M: A1 = temperature -V1 = color, <1 = dark blue for low temperature to bright pink for high temperature, 
A2 = wind speed -V2 = coverage, <2 = low coverage for weak wind speed to full coverage for strong wind 
speed, A3 = pressure -V3 = size, <3 = small strokes for low pressure to large strokes for high pressure,and 
A4 = precipitation -V4 = orientation, <4 = upright (90. rotation) for light precipitation to .at (0. 
rotation) for heavy precipitation. Figure 9 shows an example of applying M to data for February along 
the east coast of the continental United States. The top four images use a perceptual color ramp (running 
from dark blue and green for small values to bright red and pink for large values) to show the individual 
variation in temperature, pressure, wind speed, and precipitation. The result of applying M to construct 
a nonphotorealistic visualization of all four attributes is shown in the bottom image. Various color 
and texture patterns representing di.erent weather phenomena are noted on this image. Changes in temperature 
are visible as a smooth blue-green to red-pink ACM Transactions on Graphics, Vol. 23, No. 1, January 
2004. 22  Christopher G. Healey et al.  temperature pressure wind speed precipitation cold calm winds 
(blue and green) (low coverage)  light rain strong wind heavy rain pressure gradient hot (upright) (full 
coverage) (tilted) (size gradient) (pink and red) Fig. 9. Nonphotorealistic visualization of weather 
conditions for February over the eastern United States: (top row) perceptual color ramps (dark blue for 
low to bright pink for high) of mean temperature, pressure, wind speed, and precipitation in isolation; 
(bottom row) combined visualization of temperature (dark blue to bright pink for cold to hot), wind speed 
(low to high coverage for weak to strong), pressure (small to large for low to high), and precipitation 
(upright to .at for light to heavy) ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. Perceptually-Based 
Brush Strokes for Nonphotorealistic Visualization  23 (a)  (b)  Fig. 10. Weather conditions over 
the continental United States: (a) mean temperature, pressure, wind speed and precipitation (represented 
by color, size, coverage, and orientation) for January; (b) mean conditions for August ACM Transactions 
on Graphics, Vol. 23, No. 1, January 2004. 24  Christopher G. Healey et al. color variation running 
north to south over the map. Pressure gradients produce size boundaries, shown as neighboring regions 
with di.erent sized strokes (e.g., larger strokes in Florida represent higher pressure readings). Increases 
in rainfall are shown as a increasing stroke tilt running from upright (light precipitation) to .at (heavy 
precipitation). Finally, the wind s magnitude modi.es stroke coverage: weak wind speed values produce 
small numbers of strokes with a large amount of background showing through (e.g., north of the Great 
Lakes), while strong wind speed values produce larger numbers of strokes that completely .ll their corresponding 
screen space (e.g., in central Texas and Kansas). Figure 10 uses the same M to visualize weather conditions 
over the continental United States for January and August. These visualizations provide a number of interesting 
insights into historical weather conditions for this part of the world. In January (Figure 10(a)) weak 
wind speed and pressure values (shown as small, low coverage strokes) cover much of western, southeastern, 
and northeastern parts of the country. Regions of much higher pressure are shown as larger strokes in 
the center of the map. Typically heavy precipitation in the Paci.c Northwest is represented by nearly 
.at strokes. Regions of severe cold east of the Rocky Mountains near Denver and in the northern plains 
and Canadian prairies appear as patches of dark green and blue strokes. Conditions in August (Figure 
10(b)) are markedly di.erent. Most of the United States is warm with areas of intense heat, shown as 
bright pink strokes, visible in southern California, the southwest, and most of the southern states. 
Little precipitation is evident apart from Florida, where tilted strokes are displayed. Finally, wind 
speed to the west of the Rocky Mountains is much weaker than to the east; the background is clearly visible 
through the strokes in the west, while almost no background can be seen in the east. 7. VALIDATION EXPERIMENT 
 In order to further explore the capabilities of our nonphotorealistic techniques, we conducted a basic 
validation experiment designed to: (1) Test the ability of our nonphotorealistic visualization to support 
common analysis tasks on real-world data. (2) Compare our nonphotorealistic visualization with a more 
traditional display method. (3) Study whether the common method of combining displays that work well 
in isolation produces an e.ective multidimensional visualization.  Our experiment compared user performance 
in our nonphotorealistic weather visualizations with more traditional displays. The dataset we used 
for this experiment contained four data attributes A =( temperature, wind speed, wind direction, precipitation 
). Based on consultation with domain experts from the natural sciences, we decided to composite standard 
displays of the individual attributes to produce a multidimensional result. Anecdotal feedback from the 
scientists suggested that our nonphotorealistic visualizations were better than the collection of side-by-side 
displays they often employ (e.g., Figures 11(a) (c), which were captured directly from online weather 
maps), particularly when searching for combinations of weather conditions. This is not surprising, since 
a search across multiple images will produce change blindness. The low-level visual system cannot remember 
image detail beyond the local region containing the viewer s focus of attention (see the Perceptual Properties 
section for a more detailed discussion of change blindness). Because many people are already familiar 
with standard weather maps, the scientists wondered whether a combination of these displays would still 
be e.ective. Our experiment was designed to study this question, and to compare the performance of a 
combined display to our nonphotorealistic visualizations. Figure 11(d) shows the result of applying Ms(Vs, 
<s)with Vs = ( color, luminance, directed contours, semitransparent color ), <s = ( green yellow, 
dark bright, 0. 360., green red ). Certain modi.cations were needed to combine the data into 
a single image. For example, temperature, wind speed,and precipitation ACM Transactions on Graphics, 
Vol. 23, No. 1, January 2004. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization  
25 (a) (b) (c) (d) (e)  Fig. 11. Example multidimensional visualizations: (a) standard visualization 
of temperature with color (dark green for cooler to yellow for warmer); (b) standard visualization of 
precipitation with Doppler radar (green for light rainfall to red for heavy rainfall); (c) standard visualization 
of wind direction with directed contours and wind speed with color (dark blue for low winds to bright 
green for high winds); (d) a combination of three individual visualizations to form a single, multidimensional 
image; (e) a nonphotorealistic visualization with simulated paint strokes that vary their color, coverage, 
orientation, and size to visualize the same data are all represented by color in the individual displays 
(Figures 11(a) (c)); we continued to use color to represent temperature, but switched to luminance to 
represent wind speed. This variation of luminance makes areas of weaker winds appear darker (i.e., lower 
luminance), and areas of stronger winds appear lighter (i.e., higher luminance and therefore less saturated). 
We left the Doppler radar traces of precipitation intact, but made them semi-transparent to try to show 
the underlying temperature, wind speed, and wind direction. Figure 11(e) displays the same data in Figure 
11(d) as a nonphotorealistic visualization. Here, Mn is de.ned as Vn = ( color, coverage, orientation, 
size ), <n = ( dark blue bright pink, low high, 0. 360. , small large ). Since the nonphotorealistic 
technique was speci.cally designed to visualize multidimensional datasets, none of the tradeo.s used 
in Figure 11(d) were needed. Because of the coarseness of the available ACM Transactions on Graphics, 
Vol. 23, No. 1, January 2004. 26  Christopher G. Healey et al. data, we imposed a more regular structure 
on the positions of our brush strokes. Apart from this modi.cation, the painting algorithm used for Figure 
11(e) was identical to the one used for the previous datasets. Fifteen observers (computer science graduate 
students and sta. ranging in age from 18 to 41) with normal or corrected acuity and normal color vision 
participated during this experiment. The data-feature mappings Ms and Mn were explained in detail to 
the observers using a pair of visualizations di.erent from the ones shown during the experiment. Observers 
were encouraged to ask questions to ensure they understood how each attribute was being represented. 
Observers were then instructed to answer the following questions on a new pair of visualization images 
(Figures 11(d) and 11(e)): (1) In which visualization is it easiest to distinguish: temperature; precipitation; 
wind speed; wind direction? (2) Identify an area in each visualization that has: high temperature; high 
precipitation; low wind speed. (3) Identify an area in each visualization that has: high precipitation 
and low temperature; high precipitation and high wind speed. (4) Identify an area in each visualization 
where temperature changes rapidly.  The .rst question queried an observer s preferences about the representation 
techniques used for each attribute. The second question tested an observer s ability to identify values 
for three di.erent attributes. The third question tested an observer s ability to identify combinations 
of attribute values. The .nal question tested an observer s ability to identify high spatial frequency 
changes in one attribute (temperature)in the presence of a second (precipitation). As with the construction 
of the standard visualization image, these questions were selected in part through suggestions from our 
natural science colleagues. 7.1 Results Responses were recorded and tabulated for all .fteen observers. 
Chi-squared tests with a standard 95% con.dence interval were used to denote signi.cance. In summary, 
we found: (1) Observers preferred the nonphotorealistic visualization s method of representing temperature 
and wind speed. (2) Observers preferred the standard visualization s method of representing precipitation. 
 (3) Observers were better at identifying high temperature in the nonphotorealistic visualization.  
 (4) Observers were better at identifying a combination of high precipitation and high wind speed in 
the nonphotorealistic visualization. (5) Observers were better at identifying areas of rapid temperature 
change in the nonphotorealistic visualization.  Table 7.1 details observer preferences for the visualization 
that they felt made each data attribute easiest 2 to distinguish. A chi-squared test showed signi.cant 
variation within the table as a whole (.=28.8, 3 p<0.001). Chi-squared tests on each attribute identi.ed 
a signi.cant preference for the nonphotorealistic 2 visualization for temperature and wind speed (.=11.267, 
p< 0.001 in both cases), and a signi.cant 1 2 preference for the standard visualization for precipitation 
(.=8.067, p<0.01). Observers indicated that 1 it was easier to see precipitation in the standard visualization, 
since it sat on top of the other attributes. However, this made it di.cult to distinguish temperature 
and wind speed in areas of high precipitation (and thus the preference for the nonphotorealistic visualization 
s method of displaying these attributes). Although wind direction was also obscured by precipitation 
in the standard visualization, some observers felt they could infer its pattern from what they could 
see entering and exiting areas of high rainfall. Table 7.1 details observer performance for the task 
of identifying the location of high or low attribute values in the visualization. For this task, high 
was considered to be any value in the top 10% of the range shown in the visualization, and low was any 
value in the bottom 10%. Correct means an observer ACM Transactions on Graphics, Vol. 23, No. 1, January 
2004. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization  27 Table I. Combined Responses 
for the Question: In Which Visualization is it Easiest to Distinguish the Given Data Attribute? Visualization 
temperature precipitation wind speed wind direction Standard 1 13 1 5 Nonphotorealistic 14 2 14 10 Table 
II. Combined Responses for the Task: Identify an Area in Each Visualization that has the Following Attribute 
Value Visualization Response High temperature High precipitation Low wind speed correct 10 14 11 Standard 
incorrect 2 0 0 hard to tell 3 1 4 correct 15 13 13 Nonphotorealistic incorrect 0 0 0 hard to tell 0 
2 2 correctly identi.ed an area in the visualization that contained the target attribute value. Incorrect 
means an observer identi.ed an area that did not contain the target attribute value. Hard to tell means 
an observer gave no answer, but instead reported it was hard to tell where the target value was located. 
Performance for identifying high temperature was signi.cantly better in the nonphotorealistic visualization 
2 (.=6.00, p< 0.05). There was no statistical di.erence in performance for the other two attributes. 
2 Interestingly, although observers stated a preference for the way precipitation was displayed in the 
standard visualization (see Table 7.1), this did not produce any improvement in identifying regions of 
high precipitation 2 (.=0.37, p<0.90). 2 Table 7.1 details observer performance for the task of identifying 
the location of a combination of high and low attribute values in the visualization (with high and low 
de.ned as before). In both cases absolute performance was better in the nonphotorealistic visualization, 
although it was statistically signi.cant only for 2 identifying combinations of high precipitation and 
high wind speed (.=7.778, p<0.05). Observers reported 2 that it was easier to see color di.erences (i.e., 
variations in temperature) through the semi-transparent Doppler radar traces in the standard visualization, 
compared to luminance di.erences (i.e., variations in wind speed). This explained the slightly better 
absolute performance in the standard visualization for the .rst task: Identify areas of high precipitation 
and low temperature (versus the second task of identifying high precipitation and high wind speed). Table 
7.1 details observer performance for the task of identifying rapid changes in temperature.These areas 
were known to be located within areas of high precipitation, so the question was designed to test an 
observer s ability to identify sharp variations in one attribute (temperature) in the presence of a second 
(precipitation). Results showed a signi.cant performance advantage in the nonphotorealistic visualization 
2 (.=8.572, p<0.01). 1  7.2 Interpretation Although the standard visualization appeals to our familiarity 
with the weather maps we often see in dayto-day life, it was not built with methods that support rapid 
and accurate multidimensional analysis. This fact was highlighted during our experiment. Results showed 
that performance with the nonphotorealistic visualization matched or exceeded the standard visualization 
in all cases. This suggests that a method speci.cally designed for multidimensional data produces better 
visualizations than a combination of displays that work well in isolation. It also demonstrates that 
the nonphotorealistic visualizations are e.ective at representing multidimensional data in a way that 
supports real-world analysis tasks. ACM Transactions on Graphics, Vol. 23, No. 1, January 2004. 28  
Christopher G. Healey et al. Table III. Combined Responses for the Task: Identify an Area in Each Visualization 
that has the Following Combinations of Attribute Values Visualization Response High precipitation AND 
Low temperature High precipitation AND High wind speed correct 9 7 Standard incorrect 5 8 hard to tell 
1 0 correct 13 14 Nonphotorealistic incorrect 2 1 hard to tell 0 0 Table IV. Combined Responses for 
the Task: Identify an Area in Each Visualization with Rapid temperature Change Visualization Response 
rapid temperature change Standard correct 4 incorrect 11 Nonphotorealistic correct 12 incorrect 3 Given 
the foundations used to build the visualizations (rules of perception versus e.ective visualizations 
in isolation), the fact that the nonphotorealistic visualization outperformed the standard visualization 
in certain situations is not surprising. What was unexpected was that the standard visualization was 
never better than the nonphotorealistic visualization for the tasks we tested. Choosing representations 
in the standard visualization that favor some attributes (e.g., precipitation) at the expense of others 
should make these attributes highly salient. This was exactly what we observed, for example, in Tables 
7.1 and 7.1 where the presence of precipitation in the standard visualization interfered with the identi.cation 
of wind speed and temperature, respectively. Our results therefore suggest that every attribute representation 
in the nonphotorealistic visualization is at least as good as the corresponding attribute representation 
in the standard visualization. A number of issues were raised when we tried to combine the individual 
displays to produce the standard weather visualization. These included occlusion (e.g., semitransparent 
Doppler radar traces obscured underlying temperature, wind speed,and wind direction values), and links 
between visual features that caused variations in one to a.ect another (e.g., luminance variations used 
to represent wind speed lightened or darkened the colors used to represent temperature). A separate problem 
was the choice of features used in the individual displays. These choices were not always well-suited 
to the tasks the scientists said they wanted to perform. For example, the standard visualization uses 
a static colormap that assigns a .xed color to each range of temperatures. This is a common technique 
used to facilitate comparison across multiple weather maps. Unfortunately, it also results in a narrow 
range of colors when a user chooses to study a local region of interest. Our visualization scales the 
colormap to .t the range of attribute values being displayed.2 The narrow color range made it di.cult 
for users to identify speci.c temperature values in the standard visualization (both in isolation and 
in the presence of high precipitation). It may have been possible to replace the colormap to try to overcome 
some of these problems. This would not address the issues of variations in luminance to visualize wind 
speed, 2In the case of visualizing more than one map, we .rst combine temperature ranges from each map, 
then scale our colormap to cover this combined range; in this way the same colors in di.erent displays 
properly correspond to the same temperature values (e.g., see Figures 9 and 10). ACM Transactions on 
Graphics, Vol. 23, No. 1, January 2004. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization 
 29 or the occlusion that occurs in areas of high precipitation, however. Our intuition is that the 
standard visualization would continue to produce poor representations for certain tasks, and would not 
outperform the nonphotorealistic visualization, even with a more expressive colormap. Although these 
experiments visualized weather data, we are not restricted to this domain. We are currently applying 
our nonphotorealistic techniques to scienti.c simulation results in oceanography, and to the problem 
of tracking intelligent agents interacting in a simulated e-commerce auction environment. Building on 
the strengths of the low-level human visual system provides the .exibility needed to construct e.ective 
multidimensional visualizations for a wide range of problem environments. 8. CONCLUSIONS AND FUTURE WORK 
 This paper describes a method of visualization that uses painted brush strokes to represent multidimensional 
data elements. Our goal was to produce e.ective nonphotorealistic visualizations. We were motivated in 
part by nonphotorealistic rendering in computer graphics, and by the work of Laidlaw, Interrante, and 
Ebert and Rheingans to extend these techniques to a visualization environment. Our contributions to this 
work are the application of human perception during the selection of a data-feature mapping, and the 
use of controlled experiments to study the e.ectiveness of a nonphotorealistic visualization, both in 
a laboratory setting, and in a more practical, real-world context. The brush strokes we used support 
the variation of visual features that were selected based on styles from the Impressionist school of 
painting. Each attribute in a dataset is mapped to a speci.c nonphotorealistic property; attribute values 
stored in a data element can then be used to vary the visual appearance of the brush strokes. The properties 
we chose correspond closely to perceptual features detected by the lowlevel human visual system. Experimental 
results show that existing guidelines on the use of perception during visualization extend to a nonphotorealistic 
environment. This allows us to optimize the selection and application of our brush stroke properties. 
The result is a painted image whose color and texture patterns can be used to explore, analyze, verify, 
and discover information stored in a multidimensional dataset. We are optimistic that future results 
from studies of perception in visualization will also apply to our nonphotorealistic domain. In addition 
to being e.ective, our techniques try to produce visualizations that viewers perceive as engaging or 
aesthetic. Nonphotorealistic techniques that highlight important or unexpected properties can be used 
to orient a viewer s attention to speci.c areas in the image. An engaging visualization will encourage 
a more in-depth examination of these details. A number of areas for future work are now being considered. 
Experiments are currently underway to try to measure the level of artistic merit viewers attach to our 
visualizations, and to identify the basic emotional and visual composition properties of the images (e.g., 
pleasure, arousal, meaning, and complexity) that a.ect these judgments. One question of interest asks: 
Can we use these results to vary a visualization s composition in ways that improve its artistic merit? 
For example, we could try to increase the meaning of a visualization image by explaining what it represents 
and how it is used. If meaning is a predictor of artistic beauty, we would expect to see an increase 
in observers artistic merit rankings of the visualization images. Another area for investigation asks: 
How do knowledge and experience a.ect the rating scales? Our observers are, for the most part, artistic 
novices. Conducting an experiment with participants who have some type of formal training in art theory 
and art history could o.er important insights on how this knowledge a.ects appreciation of our di.erent 
image types. Results from these two questions may show that our current emotional and visual composition 
properties need to be re.ned or extended to further di.erentiate the artistic merit attached to di.erent 
images. We are evaluating new candidate properties to test during future studies. Another interesting 
suggestion is to compare the artistic merit of our nonphotorealistic visualizations with ACM Transactions 
on Graphics, Vol. 23, No. 1, January 2004. 30  Christopher G. Healey et al. traditional visualization 
techniques (e.g., multidimensional glyphs). We are now studying this possibility as a follow-on to our 
current experiments. Our brush strokes support the variation of color, orientation, coverage, and size. 
We are working to identify new nonphotorealistic properties that could be integrated into our stroke 
model. Two promising candidates we have already discussed are coarseness and weight. Other properties 
are being sought using two complementary approaches. First, we are reviewing literature on technique 
and style in Impressionist art. Second, we are looking at perceptually salient visual features that may 
correspond to new nonphotorealistic properties. Increasing the number of features we can encode e.ectively 
in each brush stroke may allow us to represent datasets with higher dimensionality. The need to display 
additional nonphotorealistic properties may exceed the abilities of our simple texture mapped stroke 
model. We are studying three techniques to overcome this limitation: (1) the creation of a larger library 
of texture mapped brush strokes that explicitly vary the properties that are not easy to modify within 
an individual texture map; (2) a model that uses spline surfaces to construct continuous representations 
of the multiple properties in a brush stroke, and (3) a model that uses a physical simulation to vary 
nonphotorealistic properties and construct visually realistic strokes. Finally, we note one other important 
advantage we can derive from the correspondence between perceptual features and nonphotorealistic properties. 
We measure the perceptual salience of a visual feature using controlled psychophysical experiments. Exactly 
the same technique is used to investigate the strengths and limitations of new nonphotorealistic features, 
both in isolation and when displayed together with other stroke properties. Just as research in perception 
helps us to identify and control nonphotorealistic features during visualization, work on new features 
may o.er insight into how the low-level visual system sees certain combinations of visual properties. 
These results could have an important impact on models of low-level human vision that are being constructed 
by researchers in the psychophysical community. ACKNOWLEDGMENTS The authors would like to thank Victoria 
Interrante, David Laidlaw, Penny Rheingans, and Theresa-Marie Rhyne for their suggestions and insights 
during discussions of this work. The authors would also like to thank Peter Rand for his suggestions 
for a standard visualization image and analysis tasks to test during our validation experiments. REFERENCES 
 Aks, D. J. and Enns, J. T. 1996. Visual search for size is in.uenced by a background texture gradient. 
J. Experiment. Psych.: Human Percept. Perf. 22, 6, 1467 1481. Bergman, L. D., Rogowitz, B. E., and Treinish, 
L. A. 1995. A rule-based tool for assisting colormap selection. In Proceedings of Visualization 95 (Atlanta, 
Ga.). 118 125. Birren, F. 1969. Munsell: A Grammar of Color. Van Nostrand Reinhold Company, New York, 
New York. Brown, R. 1978. Impressionist technique: Pissarro s optical mixture. In Impressionism in Perspective,B. 
E. White,Ed. Prentice-Hall, Inc., Englewood Cli.s, N. J., 114 121. Callaghan, T. C. 1990. Interference 
and dominance in texture segregation. In Visual Search, D. Brogan, Ed. Taylor &#38; Francis, New York, 
81 87. Chevreul, M. E. 1967. The Principles of Harmony and Contrast of Colors and Their Applications 
to the Arts. Reinhold Publishing Corporation, New York. CIE. 1978. CIE Publication No. 15, Supplement 
Number 2 (E-1.3.1, 1971): O.cial Recommendations on Uniform Color Spaces, Color-Di.erence Equations, 
and Metric Color Terms. Commission Internationale de L ` Eclairge. Coren, S., Ward,L.M., and Enns, J. 
T. 2003. Sensation and Perception (6th Edition). Wiley, New York, New York. Curtis, C. J., Anderson, 
S. E., Seims, J. E., Fleischer, K. W., and Salesin, D. H. 1997. Computer-generated watercolor. In SIGGRAPH 
97 Conference Proceedings (Los Angeles, Calif.). T. Whitted, Ed. ACM, New York, 421 430. Cutting, J. 
E. and Millard, R. T. 1984. Three gradients and the perception of .at and curved surfaces. J. Experiment. 
Psych.: General 113, 2, 198 216. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization 
 31 Ebert, D. and Rheingans, P. 2000. Volume illustration: Non-photorealistic rendering of volume models. 
In Proceedings of Visualization 2000 (San Francisco, Calif.). 195 202. Egeth, H. E. and Yantis, S. 1997. 
Visual attention: Control, representation, and time course. Ann. Rev. Psychol. 48, 269 297. Finkelstein, 
A. and Salesin, D. H. 1994. Multiresolution curves. In SIGGRAPH 94 Conference Proceedings (Orlando, Fla.). 
A. S. Glassner, Ed. ACM, New York, 261 268. Gooch, B., Coombe, G., and Shirley, P. 2002. Artistic vision: 
Painterly rendering using computer vision techniques. In Proceedings of the NPAR 2002 Symposium on Non-Photorealistic 
Animation and Rendering (Annecy, France). 83 90. Gooch, B. and Gooch, A. 2001. Non-Photorealistic Rendering. 
A K Peters, Ltd., Natick, Mass. Grinstein, G., Pickett, R., and Williams, M. 1989. EXVIS: An exploratory 
data visualization environment. In Proceedings of Graphics Interface 89 (London, Ont., Canada). 254 261. 
 Haberli, P. 1990. Paint by numbers: Abstract image representations. Comput. Graph. (SIGGRAPH 90 Conference 
Proceedings) 24, 4, 207 214. Haberli, P. and Segal, M. 1993. Texture mapping as a fundamental drawing 
primative. In Proceedings of the 4th Eurographics Workshop on Rendering (Paris, France). M. Cohen, C. 
Puech, and F. Sillion, Eds. 259 266. Haralick,R.M., Shanmugam, K., and Dinstein, I. 1973. Textural features 
for image classi.cation. IEEE Trans. Syst., Man, and Cybernet. SMC-3, 6, 610 621. Healey, C. G. 1996. 
Choosing e.ective colours for data visualization. In Proceedings of Visualization 96 (San Francisco, 
Calif.). 263 270. Healey,C.G., Booth, K. S., and Enns, J. T. 1996. High-speed visual estimation using 
preattentive processing. ACM Trans. Computer-Hum. Interact. 3, 2, 107 135. Healey, C. G. and Enns, J. 
T. 1998. Building perceptual textures to visualize multidimensional datasets. In Proceedings of Visualization 
98 (Research Triangle Park, N. C.). 111 118. Healey, C. G. and Enns, J. T. 1999. Large datasets at a 
glance: Combining textures and colors in scienti.c visualization. IEEE Trans. Visual. Comput. Graph. 
5, 2, 145 167. Hering, E. 1964. Outlines of a Theory of Light Sense. Harvard University Press, Cambridge, 
Mass. Hertzmann, A. 1998. Painterly rendering with curved brush strokes of multiple sizes. In SIGGRAPH 
98 Conference Proceedings (Orlando, Fla.). M. Cohen, Ed. ACM, New York, 453 460. Hertzmann, A. 2002. 
Fast texture maps. In Proceedings of the NPAR 2002 Symposium on Non-Photorealistic Animation and Rendering 
(Annecy, France). 91 96. Hertzmann, A., Jacobs,C.E., Oliver, N., Curless, B., and Salesin, D. H. 2001. 
Image analogies. In SIGGRAPH 2001 Conference Proceedings (Los Angeles, Calif.). E. Fiume, Ed. ACM, New 
York, 327 340. Hsu, S. C. and Lee, I. H. H. 1994. Drawing and animation using skeletal strokes. In SIGGRAPH 
94 Conference Proceedings (Orlando, Fla.). A. Glassner, Ed. ACM, New York, 109 118. Interrante, V. 2000. 
Harnessing natural textures for multivariate visualization. IEEE Comput. Graph. Applic. 20, 6, 6 11. 
Jul esz, B. 1975. Experiments in the visual perception of texture. Scient. Amer. 232, 34 43. Jul esz, 
B. 1984. A brief outline of the texton theory of human vision. Trends Neurosci. 7, 2, 41 45. Jul esz, 
B., Gilbert, E. N., and Shepp, L. A. 1973. Inability of humans to discriminate between visual textures 
that agree in second-order statistics revisited. Perception 2, 391 405. JulVisual discrimination of 
textures with identical third-order statistics. esz, B., Gilbert, E. N., and Victor, J. D. 1978. Biologic. 
Cybernet. 31, 137 140. Kirby, R. M., Marmanis, H., and Laidlaw, D. H. 1999. Visualizing multivalued data 
from 2D incompressible .ows using concepts from painting. In Proceedings of Visualization 99 (San Francisco, 
Calif.). 333 340. Laidlaw, D. H. 2001. Loose, artistic textures for visualization. IEEE Comput. Graph. 
Applic. 21, 2, 6 9. Laidlaw, D. H., Ahrens, E. T., Kremers, D., Avalos, M. J., Jacobs, R. E., and Readhead, 
C. 1998. Visualizing di.usion tensor images of the mouse spinal cord. In Proceedings of Visualization 
98 (Research Triangle Park, N. C.). 127 134. Lewis, J.-P. 1984. Texture synthesis for digital painting. 
Comput. Graph. (SIGGRAPH 84 Proceedings) 18, 3, 245 252. Litwinowicz, P. 1997. Processing images and 
video for an impressionist e.ect. In SIGGRAPH 97 Conference Proceedings (Los Angeles, Calif.). T. Whitted, 
Ed. ACM, New York, 407 414. Liu, G., Healey,C.G., and Enns, J. T. 2003. Target detection and localization 
in visual search: A dual systems perspective. Percept. Psychophys. 65, 5, 678 694. Lu, A., Morris, C. 
J., Ebert, D. S., Rheingans, P., and Hansen, C. 2002. Non-photorealistic volume rendering using stippling 
techniques. In Proceedings of Visualization 2002 (Boston, Mass.). 211 218. MacEachren, A. M. 1995. How 
Maps Work. Guilford Publications, Inc., New York. ACM Transactions on Graphics, Vol. 23, No. 1, January 
2004. 32  Christopher G. Healey et al. Mack, A. and Rock, I. 1998. Inattentional Blindness. MIT Press, 
Menlo Park, Calif. McCormick, B. H., DeFanti, T. A., and Brown, M. D. 1987. Visualization in scienti.c 
computing. Comput. Graph. 21, 6, 1 14. Meier, B. J. 1996. Painterly rendering for animation. In SIGGRAPH 
96 Conference Proceedings, (New Orleans, La.) H. Rush meier, Ed. ACM, New York, 477 484. Munsell, A. 
H. 1905. A Color Notation. Geo. H. Ellis Co., Boston, Mass. Pomerantz, J. and Pristach, E. A. 1989. Emergent 
features, attention, and perceptual glue in visual form perception. J. Experiment. Psych.: Human Percept. 
Perf. 15, 4, 635 649. Posner, M. I. and Raichle, M. E. 1994. Images of mind. Scienti.c American Library. 
Ramachandran, V. S. and Hirstein, W. 1999. The science of art: A neurological theory of aesthetic experience. 
J. of Conscious. Stud. 6, 6-7, 15 51. Rao, A. R. and Lohse, G. L. 1993a. Identifying high level features 
of texture perception. CVGIP: Graph. Models Image Process. 55, 3, 218 233. Rao, A. R. and Lohse, G. L. 
1993b. Towards a texture naming system: Identifying relevant dimensions of texture. In Proceedings of 
Visualization 93 (San Jose, Calif.). 220 227. Rensink, R. A. 2000. Seeing, sensing, and scrutinizing. 
Vision Res. 40, 10-12, 1469 1487. Rheingans, P. and Ebert, D. 2001. Volume illustration: Nonphotorealistic 
rendering of volume models. IEEE Trans. Vis. Comput. Graph. 7, 3, 253 264. Rheingans, P. and Tebbs, 
B. 1990. A tool for dynamic explorations of color mappings. Comput. Graph. 24, 2, 145 146. Rogowitz, 
B. E. and Treinish, L. A. 1993. An architecture for rule-based visualization. In Proceedings of Visualization 
93 (San Jose, Calif.). 236 243. Rood, O. N. 1879. Modern Chromatics, with Applications to Art and Industry. 
Appleton, New York. Rosenblum, L. J. 1994. Research issues in scienti.c visualization. IEEE Comput. Graph. 
Applic. 14, 2, 61 85. Salisbury, M., Anderson, C., Lischinski, D., and Salesin, D. H. 1996. Scale-dependent 
reproduction of pen-and-ink illus trations. In SIGGRAPH 96 Conference Proceedings (New Orleans, La.). 
H. Rushmeier, Ed. ACM, New York, 461 468. Salisbury, M., Anderson,S.E., Barzel, R., and Salesin, D. H. 
1994. Interactive pen-and-ink illustrations. In SIGGRAPH 94 Conference Proceedings (Orlando, Fla.). A. 
S. Glassner, Ed. ACM, New York, 101 108. Salisbury, M., Wong,M.T., Hughes, J. F., and Salesin, D. H. 
1997. Orientable textures for image-based pen-and-ink illustration. In SIGGRAPH 97 Conference Proceedings 
(Los Angeles, Calif.). T. Whitted, Ed. ACM, New York, 401 406. Schapiro, M. 1997. Impressionism: Re.ections 
and Perceptions. George Brazillier, Inc., New York. Shiraishi, M. and Yamaguchi, Y. 1999. Image moment-based 
stroke placement. In SIGGRAPH 99 Sketches &#38; Applications (Los Angeles, Calif.). R. Kidd, Ed. ACM, 
New York, 247. Simons, D. J. 2000. Current approaches to change blindness. Vis. Cognit. 7, 1/2/3, 1 
15. Slocum, T. A. 1998. Thematic Cartography and Visualization. Prentice-Hall, Inc., Upper Saddle River, 
N. J. Smith, P.H. and Van Rosendale,J. 1998. Data and visualization corridors report on the 1998 CVD 
workshop series (sponsored by DOE and NSF). Tech. Rep. CACR-164, Center for Advanced Computing Research, 
California Institute of Technology. Snowden, R. J. 1998. Texture segregation and visual search: A comparison 
of the e.ects of random variations along irrelevant dimensions. J. Experiment. Psych.: Human Percept. 
Perf. 24, 5, 1354 1367. Sousa, M. C. and Buchanan, J. W. 1999a. Computer-generated graphite pencil rendering 
of 3d polygon models. Comput. Graph. Forum (Proceedings Eurographics 99) 18, 3, 195 208. Sousa, M. C. 
and Buchanan, J. W. 1999b. Computer-generated pencil drawings. In Proceedings SKIGRAPH 99 (Ban., Canada). 
Sousa, M. C. and Buchanan, J. W. 2000. Observational models of graphite pencil materials. Comput. Graph. 
Forum 19, 1, 27 49. Strassmann, S. 1986. Hairy brushes. Comput. Graph. (SIGGRAPH 86 Proceedings) 20, 
4, 185 194. Strothotte, T. and Schlechtweg, S. 2002. Non-Photorealistic Computer Graphics: Modeling, 
Rendering and Animation. Morgan Kaufmann, Inc., San Francisco, Calif. Takagi, S. and Fujishiro, I. 1997. 
Microscopic structural modeling of colored pencil drawings. In SIGGRAPH 97 Sketches &#38; Applications 
(Los Angeles, Calif.). D. S. Ebert, Ed. ACM, New York, 187. Takagi, S., Fujishiro, I., and Nakajima, 
M. 1999. Volumetric modeling of artistic techniques in colored pencil drawing. In SIGGRAPH 99 Sketches 
&#38; Applications (Los Angeles, Calif.). R. Kidd, Ed. ACM, New York, 283. ACM Transactions on Graphics, 
Vol. 23, No. 1, January 2004. Perceptually-Based Brush Strokes for Nonphotorealistic Visualization  
33 Tamura, H., Mori, S., and Yamawaki, T. 1978. Textural features corresponding to visual perception. 
IEEE Trans. Sys., Man, and Cybernet. SMC-8, 6, 460 473. Triesman, A. 1985. Preattentive processing in 
vision. Comput. Vis. Graph. Image Process. 31, 156 177. Triesman, A. 1991. Search, similarity, and integration 
of features between and within dimensions. J. Experiment. Psych.: Human Percept. Perf. 17, 3, 652 676. 
Triesman, A. and Gormican, S. 1988. Feature analysis in early vision: Evidence from search asymmetries. 
Psychol. Rev. 95, 1, 15 48. Tufte, E. R. 1983. The Visual Display of Quantitative Information. Graphics 
Press, Cheshire, Conn. Tufte, E. R. 1990. Envisioning Information. Graphics Press, Cheshire, Conn. Tufte, 
E. R. 1997. Visual Explanations: Images and Quantities, Evidence and Narrative. Graphics Press, Cheshire, 
Conn. Venturi, L. 1978. Impressionist style. In Impressionism in Perspective, B. E. White, Ed. Prentice-Hall, 
Inc., Englewood Cli.s, N. J., 105 113. Ware, C. 1988. Color sequences for univariate maps: Theory, experiments, 
and principles. IEEE Comput Graph. Applic. 8, 5, 41 49. Ware, C. 2000. Information Visualization: Perception 
for Design. Morgan-Kaufmann, San Francisco, Calif. Ware, C. and Knight, W. 1995. Using visual texture 
for information display. ACM Trans. Graph. 14, 1, 3 20. Weigle, C., Emigh, W. G., Liu, G., Taylor, R. 
M., Enns, J. T., and Healey, C. G. 2000. Oriented texture slivers: A technique for local value estimation 
of multiple scalar .elds. In Proceedings of Graphics Interface 2000 (Montreal, Quebec, Canada). 163 
170. Winkenbach, G. and Salesin, D. H. 1994. Computer-generated pen-and-ink illustration. In SIGGRAPH 
94 Conference Proceedings (Orlando, Fla.). A. Glassner, Ed. ACM, New York, 91 100. Winkenbach, G. and 
Salesin, D. H. 1996. Rendering free-form surfaces in pen-and-ink. In SIGGRAPH 96 Conference Proceedings 
(New Orleans, La.). H. Rushmeier, Ed. ACM, New York, 469 476. Wolfe, J. M. 1994. Guided Search 2.0: A 
revised model of visual search. Psycho. Bull. Rev. 1, 2, 202 238. Wolfe, J. M., Klempen, N., and Dahlen, 
K. 2000. Post attentive vision. J. Experiment. Psych.: Human Percept. Perf. 26, 2, 693 716. Zeki, S. 
1999. Inner Vision. Oxford University Press, Oxford, U. K. Received January 2003; revised May 2003; accepted 
August 2003 ACM Transactions on Graphics, Vol. 23, No. 1, January 2004.   Engaging Viewers Through 
Nonphotorealistic Visualizations Laura G. Tateosian* Christopher G. Healey James T. Enns Department 
of Computer Science Department of Computer Science Department of Psychology North Carolina State University 
North Carolina State University University of British Columbia  Figure 1: A visual complexity style 
visualization of .ow patterns in a 2D slice through a simulated supernova collapse, using the mappings: 
.ow orientation . stroke orientation, magnitude . color (dark blue to bright pink for low to high), and 
pressure . stroke size. Abstract Research in human visual cognition suggests that beautiful images can 
engage the visual system, encouraging it to linger in certain locations in an image and absorb subtle 
details. By developing aesthetically pleasing visualizations of data, we aim to engage viewers and promote 
prolonged inspection, which can lead to new discoveries within the data. We present three new visualization 
techniques that apply painterly rendering styles to vary interpretational complexity (IC), indication 
and detail (ID), and visual complexity (VC), image properties that are important to aesthetics. Knowledge 
of human visual perception and psychophysical models of aesthetics provide the theoretical basis for 
our designs. Computational geometry and nonphotorealistic algorithms are used to preprocess the data 
and render the visualizations. We demonstrate the techniques with visualizations of real weather and 
supernova data. Keywords: nonphotorealistic rendering, visualization, aesthetics, mesh simpli.cation, 
Voronoi diagrams, NPR applications 1 Introduction Visualizations enable scientists to inspect, interpret, 
and analyze large multi-dimensional data sets. We believe effective visualizations should both orient 
and engage (attract and hold) viewer attention, directing the viewer s gaze in response to a visual 
stimulus, *e-mail: lauratateosian@yahoo.com e-mail: healey@csc.ncsu.edu and then encouraging it to linger 
at a given image location. Research on human visual perception describes how to attract attention, 
using salient visual features, such as color and texture. Less is known about how to engage viewers with 
visualizations. Recently, researchers have applied techniques from nonphotorealistic rendering in computer 
graphics to enhance and highlight important details in a visualization. We believe that nonphotorealism 
can also be used to increase the aesthetic merit of a visualization, which in turn may increase its ability 
to engage a viewer. To this end, we present three new visualization techniques designed with the goal 
of both orienting and engaging viewers. Orienting is controlled by painterly brush stroke glyphs whose 
visual properties vary to represent attributes of the underlying data. Human perception studies showed 
that the information-carrying capabilities of this painterly technique are consistent with more traditional 
glyph-based visualizations [Healey et al. 2004; Tateosian 2002]. To engage viewers, we incorporate visual 
qualities important to aesthetics into our visualizations. These visual qualities, interpretational 
complexity, indication and detail, and visual complexity, were chosen based on psychophysical models 
of aesthetics. Experiments conducted using these models provide strong evidence that varying image complexity 
can signi.cantly impact a viewer s aesthetic judgment. We apply our visualization techniques to real 
meteorological and supernova data to explore their capabilities in a real-world setting. Anecdotal feedback 
from domain experts is strongly positive, supporting the hypothesis that enhancing the artistic merit 
of a visualization can result in more effective and more productive visual analysis. The remainder of 
this paper describes our visualization techniques. Sections 2 discusses related work in artistic visualization, 
nonphotorealistic rendering, and aesthetics. Section 3 introduces the visual qualities that form the 
basis of our visualizations. Sections 4 and 5 describe the algorithms that were used to preprocess and 
render the data. Section 6 shows examples of real world applications visualized with our techniques. 
Finally, Section 7 discusses future work. 2 Related Work We reviewed previous research on aesthetics 
from the visualization, computer graphics, and human perception communities, then extended and combined 
results from each domain during the design of our aesthetic visualization algorithms. 2.1 Nonphotorealistic 
Visualization Scientists in visualization are using ideas from NPR to inspire new visualization techniques. 
Artistic techniques, such as using abstraction to eliminate unimportant distractions, and sharpening 
details to draw attention to important areas, can help to convey information more effectively. [Laidlaw 
et al. 1998; Kirby et al. 1999; Healey et al. 2004; Tateosian 2002] use painting concepts to visualize 
data. [Laidlaw et al. 1998] visualizes diseased and healthy mouse spinal cords using a tonal underpainting 
for the anatomy of the brain matter and textured elliptical brush strokes for diffusion tensor data. 
[Kirby et al. 1999] visualizes air .ow past a propeller with an underpainting, ranging from blue to yellow 
for clockwise to counter clockwise vorticity, and elliptical and arrow-shaped strokes representing additional 
.ow information. The mapping is chosen to display velocity and vorticity prominently, while still showing 
deformation of the .uid elements. [Healey et al. 2004; Tateosian 2002] create painterly visualizations 
by mapping data attributes to the visual features of rectangular brush stroke glyphs. In these techniques, 
brush strokes are painted atop an underpainting and stroke features such as shape, color, transparency, 
orientation, and texture display local data attributes. [Grigoryan and Rheingans 2002; Lu et al. 2002] 
stipple-render volumes. [Grigoryan and Rheingans 2002] uses multi-colored stippling, to show uncertainty 
on the surface of growing tumors as an intuitively uncertain, fuzzy surface of sparsely placed points. 
[Lu et al. 2002] uses pen-and-ink stippling, placing and sizing stipples to control both local and global 
tone. [Rheingans and Ebert 2001; Stompel et al. 2002] use artistic techniques like silhouetting and outline 
sketches to enhance volume visualizations. These visualization methods use artistic techniques to enhance 
their expressiveness, as do our new algorithms. One important contribution of our visualizations is 
a basis in psychological literature to vary complexity in ways speci.cally designed to improve a visualization 
s aesthetic merit. 2.2 Nonphotorealistic Rendering Techniques Existing work in NPR provides a rich vocabulary 
of artistic expression in computer graphics. We use several of the NPR concepts and algorithms described 
here. Sophisticated pen-and-ink illustration and stippling simulation systems have been developed [Winkenbach 
and Salesin 1994; Salisbury et al. 1994; Winkenbach and Salesin 1996; Salisbury et al. 1997; Secord 2002; 
Wilson and Ma 2004]. In our work, we make use of a classic pen-and-ink technique, called, indication 
and detail, that draws more detail in areas of interest and reduces detail in homogeneous regions [Winkenbach 
and Salesin 1994]. In [Winkenbach and Salesin 1994], users draw line segments on an image where high 
detail is desired. The level of detail is reduced as the distance from the line segments increases. The 
hand-drawn house in Fig. 2 shows results this system would produce if detail segments were drawn around 
the door and front windows. Painting simulation techniques are of particular interest because of our 
brush-stroke based visualization style. [Lewis 1984; Strassmann 1986; Pham 1991; Hsu and Lee 1994] present 
approaches for modeling sophisticated individual strokes. Several systems use  Figure 2: A hand-drawn 
recreation of an image from Winkenbach and Salisen s paper on computer-generated pen-and-ink drawings 
demonstrating the use of indication and detail [Winkenbach and Salesin 1994]. This technique avoids monotonous 
repetition by presenting a small amount of detail to indicate patterns that continue in the surrounding 
regions. lists of brush strokes to create Impressionist style imagery [Haeberli 1990; Meier 1996; Litwinowicz 
1997; Hertzmann 1998; Shiraishi and Yamaguchi 2000; Hertzmann 2002]. [Meier 1996] creates animated 
paintings and [Litwinowicz 1997] generates painterly videos. [Hertzmann 1998; Shiraishi and Yamaguchi 
2000] simulate paintings of source photographs. [Hertzmann 1998] layers curved B-splined strokes, increasing 
detail on each layer. [Shiraishi and Yamaguchi 2000] use image moment functions to size and orient rectangular 
strokes. [Hertzmann 2002] extends [Hertzmann 1998] with a fast paint texture method to simulate the thickness 
of paint using a height .eld and lighting, a technique we implement to create textured visualizations. 
2.3 Characterization of Beauty To choose visual properties to vary in our visualizations, we reviewed 
models of aesthetics that identify parameters that are believed to affect aesthetic judgment. In an 
early attempt to model beauty with mathematical equations, Birkhoff calculated the aesthetic measure, 
M, of objects, such as polygons, tiles, and vases as a ratio of order to complexity [Birkhoff 1932]. 
Birkhoff theorized that heightened complexity and disorder induce feelings of discomfort and so aesthetic 
measure decreases as the ratio of order to complexity decreases. Observer rankings of Birkhoff s polygons 
in a study conducted by Davis, suggest that aesthetic value is curvilinearly related to M, peaking when 
M is moderate [Davis 1936]. The same pattern is seen in Berlyne s model of aesthetic pleasure as a function 
of arousal, which plots as an inverted U [Berlyne 1971]. Viewing beautiful images generally evokes a 
feeling of pleasure. The image provides an activating stimulus and the arousal induces pleasure. Thus, 
artistic value is identi.ed with pleasure. In this model, as arousal increases, pleasure increases until 
it peaks and decreases toward indifference (zero pleasure) and displeasure (negative pleasure values). 
Structural properties, such as complexity, novelty, con.ict, expectations, ambiguity, and instability 
increase arousal; whereas, familiarity, dominance, grouping and pattern, and expectations moderate arousal. 
Arousal and pleasure axes also comprise Barrett and Russell s affective model, a tool for specifying 
a wide range of emotional states. This does not imply that aesthetic judgment is solely an emotional 
process. Factor analysis on how paintings are judged revealed that a cognitive factor, made up of complexity, 
meaningfulness, interestingness, pleasantness, and familiarity, explained a majority (51.4%) of the 
variance in judgment of paintings and an emotional factor, made up of warmth, emotionality, arousal, 
and dominance, explained 13.7% of the variance [Baltissen and Ostermann 1998]. The converse was true 
for emotional pictures (48.1% of the variance explained by the emotional scale and 14% explained by the 
cognitive scale). Viewing paintings may be largely a cognitive process, because it requires the viewer 
to identify the meaning, and emotions may arise merely as an after-effect associated with the cognitive 
process.  3 Designing Aesthetic Visualizations Birkhoff, Berlyne, and Barrett and Russell reveal that 
complexity and closely related properties such as dominance play an important role in aesthetic judgment. 
Our three nonphotorealistic visualization techniques are designed to vary these properties. Interpretational 
Complexity (IC). IC uses a layered approach to introduce complexity. Paintings are often developed in 
layers with an undercoating broadly de.ning the shapes in the image and details re.ned with subsequent 
layers. This aspect of paintings adds interpretational complexity, because the information provided by 
additional layers requires interpretation. Unlike the layering in [Tateosian 2002; Hertzmann 1998; Shiraishi 
and Yamaguchi 2000; Hertzmann 2002], we wanted our underpainting to contrast in display style and level 
of detail from the top layer, because extracting contrast reinforces attention [Ramachandran 2000]. Hence 
the underpainting, a colored canvas with sparse faint strokes, provides a lower level of detail than 
the highly detailed top level painted with wet strokes. The data to be visualized is .ltered to detect 
regions of rapid change, as described in Section 4. Then distinct strokes are laid on a second layer 
to provide detail in rapid change areas, but not elsewhere so that the underpainting is not fully covered. 
Indication and Detail (ID). ID provides focal points with high detail areas and omits monotonous details 
in homogeneous regions (Fig. 2). Tracking the eye movements of people viewing paintings has shown that 
viewers .xate on areas of high detail, such as faces [Wooding 2002; Holman et al. 2004]. The abstraction 
of detail in some areas allows other areas to be visually dominant, resolving the con.ict created by 
competing stimuli. Short-term familiarity develops when a visual element appears, disappears, and reappears 
in an image after a short hiatus. Overly repetitive visual displays may be too simplistic to be engaging. 
But variation, repeating patterns that are alike in one respect and varying in another, is a powerful 
aesthetic device [Berlyne 1971]. We can reuse the .ltering technique we developed for IC to preprocess 
the data. Then highly variable regions are drawn with high detail and clearly distinguishable brush strokes, 
and homogeneous areas are drawn with low detail and subtle strokes. Transitional strokes, something 
like the partially drawn bricks in Fig. 2, are drawn on the borders between low and high detail regions. 
In ID, stroke textures are outlined with a marker to create a congruous style, so that the detailed areas 
gradually evolve into abstracted areas, unlike IC where the layers are designed to contrast in style. 
Visual Complexity (VC). In two patterns with the same number of visual elements, the one with the most 
similarity among its elements will be considered less visually complex [Berlyne 1971]. Inspecting master 
Impressionist artist paintings reveals a great deal of variation amongst the individual brush strokes. 
There are often differences in the color and thickness of paint even within a single stroke. These visual 
properties contribute to the complexity of the painting, but in a way that differs from interpretational 
complexity. VC involves local visual variations, where IC creates global trends. Psychologists consistently 
cite complexity as one of the components affecting the judgment of aesthetics. Thus, we decided to study 
visual complexity, as well as interpretational complexity. To inject visual complexity, we introduce 
new stroke properties that correspond to observable characteristics of brush strokes in master Impressionist 
paintings. To identify these properties, we consulted an artist who has a background in Fine Arts and 
is a practicing oil and watercolor painter. We told her we were seeking salient and aesthetically pleasing 
ways to vary our basic brush stroke style and asked her to look at several Van Gogh reproductions. We 
mapped her observations of the kinds of variations she saw in these paintings to six properties: varied 
stroke contour, variegation (paint color varying within one stroke), curved strokes, embossed strokes 
(paint thickness varying within one stroke, sometimes dramatically), varied aspect ratio (some strokes 
are much longer or fatter than others), and contrast strokes (sets of dark and light strokes moving in 
different directions, juxtaposed in places). 4 Data Preprocessing (a) (b) (c) Figure 3: Processing 
Indian weather data: a) Triangulate the data element locations. b) Simplify the mesh based on data attribute 
values. c) Find the corresponding Voronoi diagram. The IC and ID techniques .rst identify areas of high 
spatial frequency within the data to be visualized. To do this we apply mesh simpli.cation followed 
by spatial analysis with Voronoi regions. Feature preserving mesh simpli.cation reduces a mesh by removing 
polygon vertices in ways that minimize the surface error. For example, a simpli.ed terrain map mesh can 
describe a plateau with just a few large polygons, whereas, many more polygons remain for craggy mountains. 
Mesh simpli.cation algorithms that handle surface properties, such as RGB triples, surface normals, and 
texture coordinates, can be used to process a multidimensional data set, with each data element s attribute 
values representing surface properties at the corresponding vertex [Walter and Healey 2001]. After simpli.cation, 
the plateaus of our example, are characterized by a sparse distribution of data elements, while in regions 
of rapid attribute value change (craggy mountains) the data elements remain dense. We used a popular 
implementation called qslim, which applies iterative vertex contraction with a quadric error metric 
to simplify meshes with associated surface properties [Garland and Heckbert 1997; Garland and Heckbert 
1998; Garland 1999]. To use qslim, we triangulated our data sets, set the surface properties to data 
attributes, and chose a desired face count. Fig. 3a shows triangulated data elements from an Indian weather 
data set. The output face count is reduced to 3% of the original face count (Fig. 3b). The data attributes, 
radiation, wet day frequency,and diurnal temperature range were set as surface properties, and used 
to determine the quadric error. To measure the denseness of the mesh vertices, we use a nearest point 
Voronoi diagram, which is a partitioning of space around a set, S,of n sites, S = p1, ..., pn,into n 
regions, such that each point inside the region containing pi is closer to pi than any of the other n-1 
sites. Since the size of the Voronoi region decreases as density increases, the Voronoi diagram of our 
reduced mesh identi.es regions of rapid change. We used an available implementation, qhull, to .nd our 
Voronoi diagrams with the results from qslim acting as input sites to qhull [Barber et al. 1993; Barber 
et al. 1996; Brown 1979]. Using our simpli.ed Indian weather example in Fig. 3b as input, the resulting 
Voronoi diagram is shown in Fig. 3c. The region sizes are correlated with the density of the input mesh. 
For example, because the input attributes, radiation, wet day frequency, and diurnal temperature range, 
vary rapidly in the Himalayas, the Voronoi regions are much smaller in these mountainous areas. 5 Visualization 
Algorithms Our three visualization techniques all create painted regions of strokes that vary in color, 
size, and orientation based on underlying data values. IC, ID, and VC share a common painting algorithm 
to lay strokes in a random manner while controlling coverage. As region Sk is painted, coverage is controlled 
by tracking the percentage, c, of coverage of the total area of Sk. A stroke is centered at a randomly 
chosen unpainted position within Sk. It is transformed and scan converted, and then retained or rejected 
based on whether there is too much overlap (when too much of a stroke falls outside Sk or overlaps previously 
laid strokes). Stroke properties such as size, orientation, and color, are determined by a data sample 
at the center of the stroke. Each sample is a vector of data attribute values at that position in the 
underlying data set, which are mapped to a vector of visual features. Stroke placement continues until 
the region s desired coverage, c, has been met. The output is a list of the accepted strokes and their 
properties. Each stroke in the list is drawn as a textured quad. Stroke textures were mostly generated 
from scanned hand-painted strokes or strokes drawn with marker. The algorithm for creating a stroke list 
with coverage Vk of region Sk is: while c<Vk do Randomly pick an unpainted position p within Sk Identify 
data element ei associated with p Set size and orientation of a new stroke, s, based on ei Center s at 
p and scan convert Compute amount of s outside of Sk, outside Compute amount of s overlapping existing 
strokes, overlap if outside or overlap are too large then Shrink s until it .ts or it cannot be shrunk 
end if if s .ts then Add to stroke list and update c end if end while 5.1 The Interpretational Complexity 
Algorithm For IC, we wanted to build two layers, each with a distinct style. Master artists often use 
an underpainting to broadly de.ne forms. The underpainting, varying in color, sometimes peeks through 
to higher detail layers. The individual strokes in the underpainting are dif.cult to discern. These patches 
of underpainting appear where there is little change or detail in the image. The Voronoi regions determine 
where the underpainting should show by partitioning the data into two sets: S0, representing low spatial 
frequency changes in the data (i.e. large Voronoi regions), and S1, representing high spatial frequency 
changes (i.e. small (a) (b) (c) Figure 4: The IC algorithm: a) Partition Voronoi regions into sets 
S0 (dark) and S1 (light) based on size. b) Paint layer S0,then S1 (Voronoi regions overlaid). c) Render 
the results. Voronoi regions). For each region, if its area exceeds a threshold percentage of the largest 
Voronoi region, it is placed in S0,otherwise it is placed in S1 (Fig. 4a). The Voronoi regions in S0 
are tiled with colored canvas texture to establish the broad color changes of the underpainting. The 
tiles are smoothly shaded, with the nearest data element s attribute values determining the color of 
each tile corner. A sparse collection of strokes are placed on the tiles by centering a stroke on each 
Voronoi vertex, and in the center of each Voronoi region (Figs. 4b and c). The strokes are textured with 
a dry brush appearance (Fig. 5), and seem faint since some of the underlying canvas shows through. Dry 
Wet Low detail High Detail Figure 5: Sample stroke textures: left) dry and wet brush strokes for the 
IC visualizations. right) strokes with increasing detail for the ID visualizations. Finally, Voronoi 
regions in S1 are painted with our painting algorithm set to produce full coverage. Each stroke is textured 
in a wet brush fashion (Fig. 5). This produces the necessary contrast with the dry brush strokes in regions 
from S0. Fig. 4 demonstrates the IC process. 5.2 The Indication and Detail Algorithm To create our stylized 
ID visualizations, we use the Voronoi regions to partition the data into three sets S0, S1,and S2, then 
paint with textured strokes like those in Fig. 5. We begin by identifying all Voronoi regions whose area 
A = threshold, and placing them in S0 (large Voronoi regions). For the remaining regions if the region 
has at least one neighbor in S0, we place it in S1 (small Voronoi regions bordering large ones), otherwise 
we place it in S2 (small Voronoi regions). Voronoi regions in S0 are drawn as smooth shaded polygons, 
with vertex colors based on the data element at their corresponding spatial position. The regions are 
then painted with one stroke at their center and one stroke at each vertex, rendered with low detail 
texture maps that have no outline (Fig. 5). (a) (b) (c) Figure 6: Generating increase in detail for 
the ID algorithm: a) Partition Voronoi regions into sets S0, S1, and S2 (dark to light). b) Paint each 
region (Voronoi regions overlaid). c) Render the results Voronoi regions in S1 and S2 are painted with 
our painting algorithm. Coverage for regions in S1 is set to V1, 0  V1 < 1.The strokes are texture 
mapped to have partial outlines (Fig. 5). Regions in S2 are painted with full coverage. Strokes in these 
regions are further partitioned based on local variations of the attribute mapped to color. First, the 
stroke is rotated based on the attribute value mapped to orientation. Next, the data is sampled 14 w 
from the stroke s center (where w represents stroke width) to lo complete outline and a center vein. 
With this design, the stroke count increases from S0 to S1 and again from S1 to S2. The addition of stroke 
outlines reinforces this gradual increase in level of detail. Fig. 6 shows an example of the ID algorithm. 
5.3 The Visual Complexity Algorithm Figure 7: Stroke properties in our VC algorithm VC varies properties 
of the individual strokes. The VC algorithm places every data element in a single region S0, then applies 
our painting algorithm with full coverage to create a list of strokes, varying the stroke properties 
as the list is created. Varied Stroke Contours. We scanned hand-painted strokes to construct our texture 
maps. This introduces more variability to the footprint of the stroke, compared to rectangular, computer 
generated strokes. For example, some of the strokes have a rounded end, while others are angular (Fig. 
7). Variegation. Strokes are drawn as three quadrilaterals q1, q,and q2, of equal length. The data is 
sampled at the center of the stroke to choose a primary stroke color C =(R,G,B), and the length l and 
width w of the stroke. C is varied randomly to produce C1 = (R1,G1,B1)and C2 =(R2,G2,B2)in a small interval 
around C s RGB values. w is also varied to generate widths w1 and w2 such that 0= w1,w2 < 18 w. q is 
then drawn centered on the sample point with length l, width w- (w1 +w2),and color C. q1 is drawn next 
to q with length l, width w1,and color C1,and q2 is drawn opposite of q1 with length l, width w2,and 
color C2. This simulates different paint colors being pushed to the margins of a stroke, as often occurs 
when the brush has residual paint from a previous dip. The randomly colored .anks provides important 
color variations. Curved Strokes. We render strokes as Bezier curves with four control points, A, B, 
C,and D.The con.guration of a curve is controlled by the data attribute mapped to orientation. We query 
the attribute values at three sample points, s0, s1,and s2, then compute the associated orientations 
to position the stroke s control points. (a) (b) (c) (d) Figure 8: Placing control points, A, B, C, 
and D: a) Get data sample, s0. b) Rotate by .0 and get samples, s1 and s2.c) Place control points, so 
that BC has orientation .0, AB has orientation .1, and CDhas orientation .2.d) If MAX{d1,d2} is large, 
draw stroke as a curve. s0 is taken at the center of the stroke to de.ne its orientation .0 (Figs. 8a). 
s0 also determines the stroke s color and size. This is consistent with how rectangular strokes are colored 
and sized. Since both the curve and s0 will lie in the convex hull of the control points, the curve will 
either pass through s0 or close to it. After the stroke is rotated by .0, samples s1 and s2 are taken 
41 l from the ends of the stroke, yielding orientations .1 and .2, respectively (Fig. 8b). The control 
points are then positioned such that that line segment BC has orientation .0, line segment AB has orientation 
.1, and line segment CDhas orientation .2 (Fig. 8c). Since Bezier curves are approximating, not interpolating, 
the curve may not pass through B and C, but it will be pulled in their direction. Curved strokes are 
only drawn where the underlying data attribute mapped to orientation varies rapidly (Fig. 8d). Embossed 
Strokes. We simulate variations in the thickness of the paint on each stroke using Hertzmann s fast paint 
texture technique [2002]. The algorithm assigns a height map to each stroke, then generates an overall 
height .eld of the image as it is painted. When the scene is lit, shadows generated by the height .eld 
simulate the undulations of paint on a canvas. This procedure is meant to avoid rendering expensive three-dimensional 
mesh models of the strokes, while producing many of the same visual results. Hertzmann s height .eld 
takes into account both the texture of each individual stroke produced by the bristles as they spread 
the paint and the overlap of paint as strokes are laid atop each other. We Figure 9: Implementation 
of Hertzmann s fast paint texture. use texture maps paired with opacity maps to calculate these values. 
We refer to the texture maps as height maps here, since the greyscale values are used as height values 
(dark to light for low to high). The contrast on wet brush stroke texture maps like those in Fig. 5 was 
increased, to provide more dramatic height differences within the height maps. We create an opacity map, 
which encodes the stroke s footprint, to correspond to each height map, by coloring all the points within 
the stroke white and setting the remaining background transparent. The opacity maps are used to compute 
a stroke overlap count. The .nal height .eld value for a given image position (x,y) is the sum of the 
stroke s height map value at (x,y) plus a weight representing the number of strokes overlapping at (x,y). 
To implement fast paint texturing, we draw our stroke list three times. Strokes are rendered differently 
each time, to collect different information about the scene. The .rst pass records the colors. The next 
two passes record the overlap count and the stroke height map information that is used to create the 
height .eld. The scene is rendered pixel by pixel, using the color buffer, the height .eld, and a Phong 
shading model to calculate each pixel s color: 1. Color Pass: Draw each stroke colored and textured with 
an opacity map and save the RGB components in the color buffer. 2. Overlap Pass: Draw each stroke white 
and textured with the same opacity maps as those used in color pass. The strokes are alpha blended with 
transparency 0 <a  1 to count the number of overlapping strokes at each pixel. Given source and destination 
blending factors, aand 1 - a, and source and destination colors, (Rs,Gs,Bs,As) and (Rd,Gd,Bd,Ad), respectively, 
we obtain:  Ri = Rsa+ Rd(1 - a) (1) Gi = Gsa+ Gd(1 - a) (2) Bi = Bsa+ Bd(1 - a) (3) Ai = a2 + Aid(1 
- a) (4) Starting with a black background sets the initial (Rd,Gd,Bd,Ad)=(0,0,0,1) at every pixel pi. 
Since each stroke s color is set to (Rs,Gs,Bs,As)= (1,1,1,a),we can compactly de.ne Ri,k, the red component 
of pixel pi where kstrokes have been drawn, as: 0, if k =0 Ri,k = (5) a+ Ri,k-1(1 - a),otherwise In 
this way, the .nal value of any of the color components of a pixel enables us to count the number of 
strokes that overlap there. We choose to save the red component of the color buffer in the overlap weight 
buffer. 3. Height Pass: Draw each stroke white and textured with the height map paired with the opacity 
map used in color and opacity passes. Each pixel pi in the resulting color buffer contains the grey value 
(i.e. the height value) of the last stroke drawn at pi. We save the red component of the color buffer 
in the height buffer. 4. Compute the height value at each pixel pi as:  h(pi)= overlap weighti + heighti 
(6) Once heights are established, we calculate per-pixel normals using the directional derivatives of 
the height .eld. 5. For each pixel pi we combine the stroke s diffuse color components in colori and 
the surface normal using the Phong model to obtain an RGB color value at pi.These value are displayed 
in the .nal rendered image (Fig. 9). Varied Stroke Aspect Ratio. This visual feature varies the ratio 
of stroke length l to stroke width w. For a majority of the strokes, l and w are selected based on the 
data attribute Ai mapped to size. Ai s value determines w, with l = cwfor some positive constant c. For 
a minority of the strokes, however, we let a different attribute Aj,j i control the aspect ratio. If 
Aj s value is below a preset = threshold t  0, w is set based on Ai s value, and l = cw with MAX(Aj 
)-t c =4. If instead aj <t + 2 , w is set to a constant value and l = c1w with c1 < 1, producing strokes 
that are wider than they are long. The paint texture reinforces the stroke direction, so that it is not 
misinterpreted as a stroke with l = 1 w. If neither c1 condition holds, wis set to a small constant 
value and l = c2wwith c2  cto yield strokes that are much longer than they are wide. Contrast Strokes. 
In Van Gogh wheat .eld paintings, burgundy strokes contrast in color and direction with a .eld of golden 
strokes. We also used the color and orientation of a minority of the strokes to contrast with the majority. 
A contrast property, either luminance, luminance and orientation, or color, is chosen. If attribute Ai 
is mapped to color and Aj is mapped to orientation, Ak is mapped to contrast (k i= j). Stroke color C 
and orientation . are = selected based on Ai and Aj. 10% of the strokes with an Ak value above a preset 
threshold are selected to act as contrast strokes. If the contrast property is luminance, the stroke 
s luminance is shifted by a constant. If the contrast property is luminance and orientation, the luminance 
is shifted and the stroke s orientation is reset to -..If the contrast property is color, color values 
in a small interval around C are assigned to the stroke. 6 Practical Applications We used two real-world 
data sets to test the practical capabilities of our visualization algorithms: .ow from a simulated supernova 
collapse, and historical weather conditions calculated for positive elevations throughout the world. 
 (a) (b) 6.1 Supernova Application A supernova is a massive explosion that occurs at the end of a star 
s lifetime. Researchers in the Physics Department at NC State University are studying these phenomena. 
Their current interests include simulating how a supernova collapses. The resulting supernova data 
sets have time steps and describe a three-dimensional volume of data. Astrophysicists often look at 
two-dimensional slices of the data at a particular time step to analyze a snapshot of the .ow. The slices 
we are visualizing contain a 500  500 regular grid of sample points composed of the attributes .x and 
.y (direction of the velocity vector), magnitude (of the velocity vector), pressure, and density. We 
created visualizations of this data using each of our new techniques. To preprocess the data, we .rst 
created a mesh by triangulating the sample points and storing magnitude, pressure,and density at each 
vertex so the simpli.ed mesh would be dense in areas where one or more of the attribute values change 
rapidly, and sparse in areas where they are relatively constant. Next we calculated the Voronoi diagram 
of the simpli.ed mesh. This Voronoi diagram was used to partition the data for the IC and ID visualizations. 
The IC visualization in Fig. 10a maps magnitude to a perceptually balanced color ramp (dark blue to bright 
pink for low to high). .x and .y determine orientation. Density is mapped to stroke size (small to large 
for low to high). pressure is not visualized, however, we know that pressure, magnitude,and density 
are relatively constant where the underpainting is visible. The bright pink area on the right represents 
a shock wave. The strokes form circular pat-To create a VC visualization, we again mapped magnitude to 
color, .x and .y to orientation, and density to stroke size. Contrast is used to reinforce magnitude 
(10% of the strokes have reduced luminance where magnitude > 20%). Pressure is mapped to aspect ratio 
(70% = pressure < 85% . wide stroke and pressure = 85% . long stroke). The resulting visualization is 
shown in Fig. 11. Wide and long strokes appear on the left where pressure is the highest. The shock wave 
has contrast strokes, reinforcing the high magnitudes that occur in this region. Dr. John Blondin, an 
astrophysicist from the physics department, provided the data and gave us anecdotal feedback on our visualizations. 
He found the use of glyphs to show multiple attributes in all three techniques assists in the investigation 
of attribute interactions. He thought visual complexity was the most appealing technique for this application, 
since the continuity of the glyphs re.ects the continuous .ow. For example, on the right side of Fig. 
11 where the shock wave impact occurs, he could see how the particles were coming together and causing 
vortices. The sense of .ow was reduced in the corresponding regions in Figs. 10a and b. His preference 
for VC is interesting because he also claimed to .nd this visualization the most aesthetically pleasing. 
6.2 Weather Application Weather data is a second convenient data source, since weather data is plentiful 
and weather attributes, such as temperature, precipitation, and wind speed, are commonly referenced 
in day-to-day settings. The data used here, collected by the International Panel on Climate Change between 
the years 1961 and 1990, consists of 30 . 12 terns where there are vortices. In Fig. 10b, the data is 
visualized year mean monthly weather conditions sampled on alatitude  . 12 longitude regular grid of 
positive elevations throughout the  world. sharp spatial boundaries. In ID visualization, stroke styles 
gradually transform across levels of detail. Figs. 13a and b and 14 show visualizations of this weather 
data using our three techniques. In Fig. 13a, average African weather conditions in the month of May 
are visualized. Here cloud cover, mean temperature, pressure,and diurnal temperature range are visualized 
with hue, luminance, size, and orientation. The underlying Voronoi diagram was created with our usual 
mesh reduction technique, with temperature, wind speed,and pressure stored at each vertex. The luminance 
is generally higher in northern Africa, indicating the higher temperature there. The canvas shows in 
interior regions that have relatively constant temperature, wind speed,and pressure. Mean January South 
American weather patterns are visualized with our indication and detail technique in Fig. 13b. Here wind 
speed, minimum temperature, wet day frequency,and precipitation are visualized with hue, luminance, 
size, and orientation. Mesh simpli.cation was performed with wet day frequency, pressure,and diurnal 
temperature range. The low detail region in the interior of Brazil shows highlight these attributes have 
low variability. In the Andes, we can see hue variations and a dark luminance, indicating variable wind 
speeds and low minimum temperatures relative to the rest of the continent. In Fig. 14, mean weather conditions 
for January in southwest Canada are visualized with our visual complexity technique. Mean temperature, 
pressure, wet day frequency, wind speed, and precipitation are visualized with color, size, orientation, 
contrast, and aspect ratio, respectively. The temperature is increasing from north to south with high 
wind speeds in south central regions where contrast strokes are drawn with reduced luminance and opposing 
orientation. Wide and long strokes appear in the southwest where precipitation is high. Our techniques 
are not limited to scienti.c data visualizations. They can also be applied to other types of input, for 
example, photographs interpreted as a regular grid of RGB data samples. Fig. 12 shows two examples of 
an input photograph of a sea turtle, rendered using the ID and VC techniques. 7 Conclusions This paper 
presents new artistic styles for visualizing large scienti.c data. The styles are designed to both orient 
viewers with the deliberate use of salient visual features and to engage viewers with aesthetic appeal. 
We designed these styles by studying psychophysical models of aesthetics and combining them with expressive 
NPR techniques. We used the new styles to visualize real world data and received encouraging feedback 
from our domain experts. In fact, our collaborators have started to use our visualizations in their presentations 
and publications. We are now conducting and analyzing controlled experiments to measure how viewers 
rate aesthetic, emotional, and visual composition properties of our NPR visualizations relative to real 
paintings by Abstractionist and Impressionist Masters. Preliminary results have been promising, particularly 
for IC and VC. This suggests that our techniques can positively impact aesthetic judgment. Given the 
apparent increase in aesthetic merit produced by our visualizations, the next step in our research will 
study whether we are increasing engagement, again through the use of controlled experiments. (a) (b) 
. luminance (dark to light for low to high), pressure . size (small to large for low to high), diurnal 
range . orientation (.at to upright for low to high). b) ID visualization for October over South American, 
wind speed . hue, minimum temperature . luminance, radiation . size, precipitation . orientation.  
 References BALTISSEN,R., AND OSTERMANN, B. 1998. Are the dimiensions underlying aesthetic and affective 
judgment the same? Empirical Studies of the Arts 16, 2, 97 113. BARBER,C., DOBKIN,D., AND HUHDANPAA, 
H. 1993. The quickhull algorithm for convex hull. Tech. Rep. TR GCG53, The Geometry Center, University 
of Minnesota, Minneapolis, MN. BARBER,C., DOBKIN,D., AND HUHDANPAA, H. 1996. The quickhull algorithm 
for convex hulls. ACM Trans. Math. Softw. 22, 4, 469 483. BERLYNE, D. 1971. Aesthetics and Psychobiology. 
AppletonCentury-Crofts, New York, NY. BIRKHOFF, G. 1932. Aesthetic Measure. Harvard University Press, 
Cambridge, MA. BROWN,KEVIN, Q. 1979. Voronoi diagrams from convex hulls. Information Processing Letters 
9, 5, 223 228. DAVIS, R. 1936. An evaluation and test of birkhoff s aesthetic measure and formula. Journal 
of General Psychology 15, 231 240. GARLAND,M., AND HECKBERT, P. S. 1997. Surface simpli.cation using 
quadric error metrics. In SIGGRAPH 97 Conference Proceedings, T. Whitted, Ed., 209 216. GARLAND,M., AND 
HECKBERT,P.S.1998.Simplifyingsurfaces with color and texture using quadric error metrics. In Proceedings 
Visualization 98, 263 269. GARLAND, M. 1999. The Design, Use, and Required Facilities of an Interactive 
Visual Computer Simulation Language to Explore Production Planning Problems. Ph.D. thesis, Carnegie Mellon 
University, Pittsburg, PA. GRIGORYAN,G., AND RHEINGANS, P. 2002. Probabilistic surfaces: Point based 
primitives to show uncertainty. In Proceed ings Visualization 2002, 147 153. HAEBERLI, P. 1990. Paint 
by numbers: Abstract image representations. Computer Graphics (SIGGRAPH 90 Conference Proceedings) 
24, 4, 207 214. HEALEY,C.G., TATEOSIAN,L., ENNS,J. T., AND REMPLE, M. 2004. Perceptually based brush 
strokes for nonphotorealistic visualization. ACM Trans. Graph. 23, 1, 64 96. HERTZMANN, A. 1998. Painterly 
rendering with curved brush strokes of multiple sizes. In SIGGRAPH 98 Conference Proceedings, M. Cohen, 
Ed., 453 460. HERTZMANN, A. 2002. Fast texture maps. In Proceedings NPAR 2002 Symposium on Non-Photorealistic 
Animation and Rendering, 91 96. HOLMAN,D., VERTEGAAL,R., SOHN,C., AND CHENG,D. 2004. Attentive display: 
paintings as attentive user interfaces. In CHI 04: Extended abstracts of the 2004 conference on Human 
factors and computing systems, ACM Press, 1127 1130. HSU,S.C., AND LEE, I. H. H. 1994. Drawing and animation 
using skeletal strokes. In SIGGRAPH 94 Conference Proceedings, A. Glassner, Ed., 109 118. KIRBY,R. M., 
MARMANIS,H., AND LAIDLAW, D. H. 1999. Visualizing multivalued data from 2D incompressible .ows using 
concepts from painting. In Proceedings Visualization 99, 333 340. LAIDLAW,D. H., AHRENS,E. T., KREMERS,D., 
AVALOS, M. J., JACOBS,R. E., AND READHEAD, C. 1998. Visualizing diffusion tensor images of the mouse 
spinal cord. In Proceedings Visualization 98, 127 134. LEWIS, J.-P. 1984. Texture synthesis for digital 
painting. Computer Graphics (SIGGRAPH 84 Proceedings) 18, 3, 245 252. LITWINOWICZ, P. 1997. Processing 
images and video for an impressionist effect. In SIGGRAPH 97 Conference Proceedings, T. Whitted, Ed., 
407 414. LU,A., MORRIS,C. J., EBERT,D. S., RHEINGANS,P., AND HANSEN, C. 2002. Non-photorealistic volume 
rendering using stippling techniques. In Proceedings Visualization 2002, 211 218. MEIER, B. J. 1996. 
Painterly rendering for animation. In SIG-GRAPH 96 Conference Proceedings, H. Rushmeier, Ed., 477 484. 
PHAM, B. 1991. Expressive brush strokes. Computer Vision, Graphics and Image Processing 53, 1, 1 6. RAMACHANDRAN, 
V. 2000. The science of art: How the brain responds to beauty. In Understanding wisdom: Sources, science, 
and society., W. S. Brown, Ed. Templeton Foundation Press., Philadelphia, PA. RHEINGANS,P., AND EBERT, 
D. 2001. Volume illustration: Nonphotorealistic rendering of volume models. IEEE Transactions on Visualization 
and Computer Graphics 7, 3, 253 264. SALISBURY,M., ANDERSON,S. E., BARZEL,R., AND SALESIN, D. H. 1994. 
Interactive pen-and-ink illustrations. In SIGGRAPH 94 Conference Proceedings, A. S. Glassner, Ed., 101 
108. SALISBURY,M., WONG,M. T., HUGHES,J.F., AND SALESIN, D. H. 1997. Orientable textures for image-based 
pen-and-ink illustration. In SIGGRAPH 97 Conference Proceedings, T. Whitted, Ed., 401 406. SECORD, A. 
2002. Weighted voronoi stippling. In Proceedings NPAR 2002 Symposium on Non-Photorealistic Animation 
and Rendering, 37 43. SHIRAISHI,M., AND YAMAGUCHI, Y. 2000. An algorithm for automatic painterly rendering 
based on local source image ap proximation. In Proceedings NPAR 2000 Symposium on Non-Photorealistic 
Animation and Rendering, 53 58. STOMPEL,A., LUM,E., AND MA, K.-L. 2002. Feature-enhanced visualization 
of multidimensional, multivariate volume data using non-photorealistic rendering techniques. In 10th 
Paci.c Conference on Computer Graphics and Applications, 1 8. STRASSMANN, S. 1986. Hairy brushes. Computer 
Graphics (SIG-GRAPH 86 Proceedings) 20, 4, 185 194. TATEOSIAN, L. G. 2002. Nonphorealistic visualization 
of multidimensional datasets. Master s thesis, North Carolina State University. WALTER,J. D., AND HEALEY, 
C. G. 2001. Attribute preserving dataset simpli.cation. In VIS 01: Proceedings of the conference on Visualization 
01, IEEE Computer Society, 113 120. WILSON,B., AND MA, K.-L. 2004. Rendering complexity in computer-generated 
pen-and-ink illustrations. In NPAR 04: Proceedings of the 3rd international symposium on Nonphotorealistic 
animation and rendering, ACM Press, 129 137. WINKENBACH,G., AND SALESIN, D. H. 1994. Computergenerated 
pen-and-ink illustration. In SIGGRAPH 94 Conference Proceedings, A. Glassner, Ed., 91 100. WINKENBACH,G., 
AND SALESIN, D. H. 1996. Rendering parametric surfaces in pen-and-ink. In SIGGRAPH 96 Conference Proceedings, 
H. Rushmeier, Ed., 469 476. WOODING, D. 2002. Eye movements of large populations: Ii. deriving regions 
of interest, coverage, and similarity using .xation maps. Behavior Research Methods, Instruments, and 
Computers 34, 4, 518 528. colori = ) overlap_weighti + heighti = h(pi) => Ni (Oi dR,Oi dG,Oi dB   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1837109</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>312</pages>
		<display_no>8</display_no>
		<article_publication_date>07-26-2010</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Spectral mesh processing]]></title>
		<page_from>1</page_from>
		<page_to>312</page_to>
		<doi_number>10.1145/1837101.1837109</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1837109</url>
		<abstract>
			<par><![CDATA[<p>Spectral mesh processing is an idea that was proposed at the beginning of the 90's, to port the "signal processing toolbox" to the setting of 3D mesh models. Recent advances in both computer horsepower and numerical software make it possible to fully implement this vision. In the more classical context of sound and image processing, Fourier analysis was a corner stone in the development of a wide spectrum of techniques, such as filtering, compression, and recognition. In this course, attendees will learn how to transfer the underlying concepts to the setting of a mesh model, how to implement the "spectral mesh processing" toolbox and use it for real applications, including filtering, shape matching, remeshing, segmentation, and parameterization.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computation of transforms (e.g., fast Fourier transform)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003717</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computation of transforms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2265003</person_id>
				<author_profile_id><![CDATA[81100154829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[L&#233;vy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2265004</person_id>
				<author_profile_id><![CDATA[81336494344]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hao]]></first_name>
				<middle_name><![CDATA[(Richard)]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AFW06} D. N. Arnold, R. S. Falk, and R. Winther. Finite element exterior calculus, homological techniques, and applications. <i>Acta Numerica 15</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Arv95} James Arvo. The Role of Functional Analysis in Global Illumination. In P. M. Hanrahan and W. Purgathofer, editors, <i>Rendering Techniques '95 (Proceedings of the Sixth Eurographics Workshop on Rendering)</i>, pages 115--126, New York, NY, 1995. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>795528</ref_obj_id>
				<ref_obj_pid>795523</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{BN03} M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. <i>Neural Computations</i>, 15(6):1373--1396, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Bra99} Ronald N. Bracewell. <i>The Fourier Transform And Its Applications</i>. McGraw-Hill, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Cip93} Barri Cipra. You can't always hear the shape of a drum. <i>What's Happening in the Mathematical Sciences</i>, 1, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{DBG+05} S. Dong, P.-T. Bremer, M. Garland, V. Pascucci, and J. C. Hart. Quadrangulating a mesh using laplacian eigenvectors. Technical report, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{DBG+06a} S. Dong, P.-T. Bremer, M. Garland, V. Pascucci, and J. C. Hart. Spectral mesh quadrangulation. <i>ACM Transactions on Graphics (SIGGRAPH 2006 special issue)</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141993</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{DBG+06b} Shen Dong, Peer-Timo Bremer, Michael Garland, Valerio Pascucci, and John C. Hart. Spectral surface quadrangulation. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, pages 1057--1066, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731315</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{dGGV08} Fernando de Goes, Siome Goldenstein, and Luiz Velho. A hierarchical segmentation of articulated bodies. <i>Computer Graphics Forum (Symposium on Geometry Processing)</i>, 27(5):1349--1356, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198666</ref_obj_id>
				<ref_obj_pid>1198555</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{DKT05} Mathieu Desbrun, Eva Kanzo, and Yiying Tong. Discrete differential forms for computational modeling. <i>Siggraph '05 course notes on Discrete Differential Geometry, Chapter 7</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{DMA02} Mathieu Desbrun, Mark Meyer, and Pierre Alliez. Intrinsic parameterizations of surface meshes. In <i>Proceedings of Eurographics</i>, pages 209--218, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{dV90} Y. Colin de Verdiere. Sur un nouvel invariant des graphes et un critere de planarite. <i>J. of Combinatorial Theory</i>, 50, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Dye06} Ramsey Dyer. Mass weights and the cot operator (personal communication). Technical report, Simon Fraser University, CA, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2286506</ref_obj_id>
				<ref_obj_pid>2286437</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{EK03} A. Elad and R. Kimmel. On bending invariant signatures for surfaces. <i>IEEE Trans. Pattern Anal. Mach. Intell.</i>, 25(10):1285--1295, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237985</ref_obj_id>
				<ref_obj_pid>237814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Fei96} J. Feidman. Computing betti numbers via combinatorial laplacians. In <i>Proc. 28th Sympos. Theory Comput.</i>, pages 386--391. ACM, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{FH04} M. S. Floater and K. Hormann. <i>Surface parameterization: a tutorial and survey</i>. Springer, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Fie73} Miroslav Fiedler. Algebraic connectivity of graphs. <i>Czech. Math. Journal</i>, 23:298--305, 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Fie75} Miroslav Fiedler. A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory. <i>Czech. Math. Journal</i>, 25:619--633, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1735623</ref_obj_id>
				<ref_obj_pid>1735603</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{GBAL} Katarzyna Gebal, Andreas Baerentzen, Henrik Aanaes, and Rasmus Larsen. Shape analysis using the auto diffusion function. <i>Computer Graphics Forum (Proc. of Symp. on Geom. Proc.)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882276</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{GGS03a} C. Gotsman, X. Gu, and A. Sheffer. Fundamentals of spherical parameterization for 3d meshes. <i>ACM Trans. Graph.</i>, 22(3):358--363, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882276</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{GGS03b} C. Gotsman, X. Gu, and A. Sheffer. Fundamentals of spherical parameterization for 3d meshes, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>830320</ref_obj_id>
				<ref_obj_pid>829510</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Got03} Craig Gotsman. On graph partitioning, spectral analysis, and digital mesh processing. In <i>Shape Modeling International</i>, pages 165--174, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{GY02} X. Gu and S.-T. Yau. Computing conformal structures of surfaces. <i>Communications in Information and Systems</i>, 2(2):121--146, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Hir03} Anil Hirani. Discrete exterior calculus. <i>PhD thesis</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{HPW06} Klaus Hildebrandt, Konrad Polthier, and Max Wardetzky. On the convergence of metric and geometric properties of polyhedral surfaces. <i>Geom Dedicata</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{HS97} D. D. Hoffman and M. Singh. Salience of visual parts. <i>Cognition</i>, 63:29--78, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{HWAG09} Qixing Huang, Martin Wicke, Bart Adams, and Leonidas J. Guibas. Shape decomposition using modal analysis. 28(2):to appear, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409100</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{HZM+08} Jin Huang, Muyang Zhang, Jin Ma, Xinguo Liu, Leif Kobbelt, and Hujun Bao. Spectral quadrangulation with orientation and alignment control. <i>ACM Transactions on Graphics (SIGGRAPH Asia conf. proc.</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{IL05} Martin Isenburg and Peter Lindstrom. Streaming meshes. In <i>IEEE Visualization</i>, page 30, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>59921</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Jai89} A. K. Jain. <i>Fundamentals of Digital Image Processing</i>. Prentice Hall, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{JNT} Dmitry Jakobson, Nikolai Nadirashvili, and John Toth. Geometric properties of eigenfunctions.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1244662</ref_obj_id>
				<ref_obj_pid>1244469</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{JZ07} Varun Jain and Hao Zhang. A spectral approach to shape-based retrieval of articulated 3D models. <i>Computer Aided Design</i>, 39:398--407, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{JZvK07} Varun Jain, Hao Zhang, and Oliver van Kaick. Non-rigid spectral correspondence of triangle meshes. <i>International Journal on Shape Modeling</i>, 2007. to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Kac66} Mark Kac. Can you hear the shape of a drum? <i>Amer. Math. Monthly</i>, 73, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344924</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{KG00a} Z. Karni and C. Gotsman. Spectral compression of mesh geometry. In <i>Proc. of ACM SIGGRAPH</i>, pages 279--286, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344924</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{KG00b} Zachi Karni and Craig Gotsman. Spectral compression of mesh geometry. In <i>SIGGRAPH</i>, pages 279--286, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344924</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{KG00c} Zachi Karni and Craig Gotsman. Spectral compression of mesh geometry. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 279--286, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882275</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{KLS03} A. Khodakovsky, N. Litke, and P. Schr&#246;der. Globally smooth parameterizations with low distortion. <i>ACM TOG (SIGGRAPH)</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Kor02} Y. Koren. On spectral graph drawing, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1756936</ref_obj_id>
				<ref_obj_pid>1756869</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Kor03} Y. Koren. On spectral graph drawing. In <i>Proc. of the International Computing and Combinatorics Conference</i>, pages 496--508, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057434</ref_obj_id>
				<ref_obj_pid>1057432</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{KSO04} Ravikrishna Kolluri, Jonathan Richard Shewchuk, and James F. O'Brien. Spectral surface reconstruction from noisy point clouds. In <i>Proc. of Eurographics Symposium on Geometry Processing</i>, pages 11--21, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>796585</ref_obj_id>
				<ref_obj_pid>795666</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{KVV00} R. Kannan, S. Vempala, and A. Vetta. On clustering - good, bad, and spectral. In <i>FOCS</i>, pages 367--377, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1136965</ref_obj_id>
				<ref_obj_pid>1136647</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Lev06} Bruno Levy. Laplace-beltrami eigenfunctions: Towards an algorithm that understands geometry. In <i>IEEE International Conference on Shape Modeling and Applications</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097816</ref_obj_id>
				<ref_obj_pid>1097115</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{LH05} Marius Leordeanu and Martial Hebert. A spectral technique for correspondence problems using pairwise constraints. In <i>International Conference of Computer Vision (ICCV)</i>, volume 2, pages 1482--1489, October 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566590</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{LPM02} Bruno Levy, Sylvain Petitjean, and Nicolas Ray Nicolas Jerome Maillot. Least squares conformal maps for automatic texture atlas generation. In ACM, editor, <i>SIGGRAPH conf. proc.</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566590</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{LPRM02} B. L&#233;vy, S. Petitjean, N. Ray, and J. Maillot. Least squares conformal maps for automatic texture atlas generation. In <i>Proc. of ACM SIGGRAPH 02</i>, pages 362--371, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1026053</ref_obj_id>
				<ref_obj_pid>1025128</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{LZ04} R. Liu and H. Zhang. Segmentation of 3D meshes through spectral clustering. In <i>Pacific Graphics</i>, pages 298--305, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{LZ07} Rong Liu and Hao Zhang. Mesh segmentation via spectral embedding and contour analysis. <i>Computer Graphics Forum (Special Issue of Eurographics 2007)</i>, 26:385--394, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{MCBH07} Diana Mateus, Fabio Cuzzolin, Edmond Boyer, and Radu Horaud. Articulated shape matching by robust alignment of embedded representations. In <i>ICCV '07 Workshop on 3D Representation for Recognition (3dRR-07)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{MDSB03} Mark Meyer, Mathieu Desbrun, Peter Schr&#246;der, and Alan H. Barr. Discrete differential-geometry operators for triangulated 2-manifolds. In Hans-Christian Hege and Konrad Polthier, editors, <i>Visualization and Mathematics III</i>, pages 35--57. Springer-Verlag, Heidelberg, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1163645</ref_obj_id>
				<ref_obj_pid>1163641</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{MIT06} Omer Meshar, Dror Irony, and Sivan Toledo. An out-of-core sparse symmetric indefinite factorization method. <i>ACM Transactions on Mathematical Software</i>, 32:445--471, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731335</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{MTAD08} Patrick Mullen, Yiying Tong, Pierre Alliez, and Mathieu Desbrun. Spectral conformal parameterization. In <i>ACM/EG Symposium of Geometry Processing</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{NJW02} A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. In <i>Neural Information Processing Systems</i>, volume 14, pages 849--856, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{OMT02} R. Ohbuchi, A. Mukaiyama, and S. Takahashi. A frequency-domain approach to watermarking 3D shapes. <i>Computer Graphics Forum</i>, 21(3):373--382, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731314</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{OSG08} Maks Ovsjanikov, Jian Sun, and Leonidas Guibas. Global intrinsic symmetries of shapes. <i>Computer Graphics Forum (Symposium on Geometry Processing)</i>, 27(5):1341--1348, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>780989</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{OTMM01} R. Ohbuchi, S. Takahashi, T. Miyazawa, and A. Mukaiyama. Watermarking 3D polygonal meshes in the mesh spectral domain. In <i>Proc. of Graphics Interface</i>, pages 9--18, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{PP93} Ulrich Pinkall and Konrad Polthier. Computing discrete minimal surfaces and their conjugates. <i>Experimental Mathematics</i>, 2(1), 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{Pra99} G. Prathap. Towards a science of fea: Patterns, predictability and proof through some case studies. <i>Current Science</i>, 77:1311--1318, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{RS00a} S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. <i>Science</i>, 290:2323--2326, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{RS00b} Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. <i>Science</i>, 290(5500):2323--2326, Dec 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1282022</ref_obj_id>
				<ref_obj_pid>1281991</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Rus07} R. M. Rustamov. Laplace-beltrami eigenfunctions for deformation invariant shape representation. In <i>Proc. of Eurographics Symposium on Geometry Processing</i>, pages 225--233, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1649644</ref_obj_id>
				<ref_obj_pid>1649575</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{RWP05a} M. Reuter, F.-E. Wolter, and N. Peinecke. Laplace-beltrami spectra as "shape-dna" of surfaces and solids. <i>CAD Journal</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1060256</ref_obj_id>
				<ref_obj_pid>1060244</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{RWP05b} Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke. Laplacespectra as fingerprints for shape matching. In <i>SPM '05: Proceedings of the 2005 ACM symposium on Solid and physical modeling</i>, pages 101--106, New York, NY, USA, 2005. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>139105</ref_obj_id>
				<ref_obj_pid>139101</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{SB92} L. S. Shapiro and J. M. Brady. Feature-based correspondence: an eigenvector approach. <i>Image and Vision Computing</i>, 10(5):283--288, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{SGD05} P. Schr&#246;der, E. Grinspun, and M. Desbrun. Discrete differential geometry: an applied introduction. In <i>SIGGRAPH Course Notes</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351611</ref_obj_id>
				<ref_obj_pid>351581</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{SM00} Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. <i>IEEE Trans. on Pattern Analysis and Machine Intelligence</i>, 22(8):888--905, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{SOG} Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably informative multi-scale signature based on heat diffusion. <i>Computer Graphics Forum (Proc. of Symp. on Geom. Proc.)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{Tau95a} G. Taubin. A signal processing approach to fair surface design. In <i>Proc. of ACM SIGGRAPH</i>, pages 351--358, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Tau95b} Gabriel Taubin. A signal processing approach to fair surface design. In <i>SIGGRAPH '95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, pages 351--358, New York, NY, USA, 1995. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{TB97} Lloyd N. Trefethen and David Bau. <i>Numerical Linear Algebra</i>. SIAM, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{TdSL00} J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. <i>Science</i>, 290:2319--2323, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015810</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{THCM04} M. Tarini, K. Hormann, P. Cignoni, and C. Montani. Polycubemaps. <i>ACM TOG (SIGGRAPH)</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{vL06} Ulrike von Luxburg. A tutorial on spectral clustering. Technical Report TR-149, Max Plank Institute for Biological Cybernetics, August 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{VM03} D. Verma and M. Meila. A comparison of spectral clustering algorithms. Technical Report UW-CSE-03-05-01, University of Washington, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{VS01} D. V. Vrani&#263; and D. Saupe. 3D shape descriptor based on 3D Fourier transform. In <i>Proc. EURASIP Conf. on Digital Signal Processing for Multimedia Communications and Services</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1297625</ref_obj_id>
				<ref_obj_pid>1297423</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{WBH+07} Max Wardetzky, Miklos Bergou, David Harmon, Denis Zorin, and Eitan Grinspun. Discrete quadratic curvature energies. <i>Computer Aided Geometric Design (CAGD)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851547</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{Wei99} Y. Weiss. Segmentation using eigenvectors: A unifying view. In <i>Proc. of the International Conference on Computer Vision</i>, pages 975--983, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{WK05} Jianhua Wu and Leif Kobbelt. Efficient spectral watermarking of large meshes with orthogonal basis functions. In <i>The Visual Computer</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281995</ref_obj_id>
				<ref_obj_pid>1281991</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{WMKG07} Max Wardetzky, Saurabh Mathur, Felix Kalberer, and Eitan Grinspun. Discrete laplace operators: No free lunch. <i>Eurographics Symposium on Geometry Processing</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{You85} F. W. Young. Multidimensional scaling. <i>Encyclopedia of Statistical Sciences</i>, 5:649--658, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614523</ref_obj_id>
				<ref_obj_pid>614286</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{ZKK02} Gil Zigelman, Ron Kimmel, and Nahum Kiryati. Texture mapping using surface flattening via multidimensional scaling. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 8(2), 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{ZL05} H. Zhang and R. Liu. Mesh segmentation via recursive and visually salient spectral cuts. In <i>Proc. of Vision, Modeling, and Visualization</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057439</ref_obj_id>
				<ref_obj_pid>1057432</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{ZSGS04} Kun Zhou, John Snyder, Baining Guo, and Heung-Yeung Shum. Iso-charts: Stretch-driven mesh parameterization using spectral analysis. In <i>Symposium on Geometry Processing</i>, pages 47--56, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{ZvKDar} Hao Zhang, Oliver van Kaick, and Ramsay Dyer. Spectral mesh processing. <i>Computer Graphics Forum</i>, 2009, to appear. http://www.cs.sfu.ca/~haoz/pubs/zhang_cgf09_spect_survey.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. A signal processing approach to fair surface design. In <i>Proc. of ACM SIGGRAPH</i>, pages 351--358, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344924</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Z. Karni and C. Gotsman. Spectral compression of mesh geometry. In <i>Proc. of ACM SIGGRAPH</i>, pages 279--286, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>59921</ref_obj_id>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[A. K. Jain. <i>Fundamentals of Digital Image Processing</i>. Prentice Hall, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[F. R. K. Chung. <i>Spectral Graph Theory</i>. AMS, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351611</ref_obj_id>
				<ref_obj_pid>351581</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. <i>IEEE Trans. on Pattern Analysis and Machine Intelligence</i>, 22(8):888--905, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Ulrike von Luxburg. A tutorial on spectral clustering. Technical Report TR-149, Max Plank Institute for Biological Cybernetics, August 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Varun Jain, Hao Zhang, and Oliver van Kaick. Non-rigid spectral correspondence of triangle meshes. <i>International Journal on Shape Modeling</i>, 13(1):101--124, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313286</ref_obj_id>
				<ref_obj_pid>1313055</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[H. Qiu and ER Hancock, <i>Clustering and embedding using commute times</i>, IEEE Transactions on Pattern Analysis and Machine Intelligence 29 (2007), no. 11, 1873--1890.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Hao Zhang, Oliver van Kaick, and Ramsay Dyer. Spectral mesh processing. <i>Computer Graphics Forum</i>, 2009, to appear. http://www.cs.sfu.ca/~haoz/pubs/zhang_cgf09_spect_survey.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Rong Liu and Hao Zhang. Mesh segmentation via spectral embedding and contour analysis. <i>Computer Graphics Forum (Special Issue of Eurographics 2007)</i>, 26:385--394, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{EY36} Eckart C., Young G.: The approximation of one matrix by another of lower rank. <i>Psychometrika 1</i> (1936), 211--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{BH03} Brand M., Huang K.: A unifying theorem for spectral embedding and clustering. In <i>Proc. of Int. Conf. on AI and Stat</i>. (Key West, Florida, 2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281995</ref_obj_id>
				<ref_obj_pid>1281991</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Max Wardetzky, Saurabh Mathur, Felix Kalberer, and Eitan Grinspun. Discrete laplace operators: No free lunch. <i>Eurographics Symposium on Geometry Processing</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{VL08} Vallet B., L&#233;vy B.: Spectral geometry processing with manifold harmonics. <i>Computer Graphics Forum (Special Issue of Eurographics) 27</i>, 2 (2008), 251--260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1282027</ref_obj_id>
				<ref_obj_pid>1281991</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Dyer, R., Zhang, H., and M&#246;ller, T. 2007. Delaunay mesh construction. In <i>Symp. Geometry Processing</i>, 271--282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281989</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{LZ06} Li J., Zhang H.: Nonobtuse remeshing and decimation. In <i>SGP</i> (2006), pp. 235--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[R. Liu, H. Zhang, A. Shamir, and D. Cohen-Or, "A Part-Aware Surface Metric for Shape Processing", <i>Eurographics</i> 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[A. Shamir, "A Survey on Mesh Segmentation Techniques", <i>Computer Graphics Forum</i> (<i>Eurographics STAR</i> 2006), 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731315</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[F. de Goes, Siome Goldenstein, and Luiz Velho, "A hierarchical Segmentation of Articulated Bodies", <i>SGP</i> 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[R. Liu and H. Zhang, "Spectral Clustering for Mesh Segmentation", <i>Pacific Graphics</i> 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[R. Liu and H. Zhang, "Mesh Segmentation via Spectral Embedding and Contour Analysis", <i>Eurographics</i> 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882369</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[S. Katz and A. Tal, "Hierarchical Mesh Segmentation via Fuzzy Clustering and Cuts", <i>SIGGRAPH</i> 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1282022</ref_obj_id>
				<ref_obj_pid>1281991</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[R. Rustomov, "Laplacian-Beltrami Eigenfunctions for Deformation Invariant Shape Representation", <i>SGP</i> 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[V. Jain, H. Zhang, O. van Kaick, "Non-Rigid Spectral Correspondence of Triangle Meshes, <i>IJSM</i> 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731314</ref_obj_id>
				<ref_obj_pid>1731309</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[M. Ovsjanikov, J. Sun, and L. Guibas, "Global Intrinsic Symmetries of Shapes", <i>SGP</i> 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1244662</ref_obj_id>
				<ref_obj_pid>1244469</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[V. Jain and H. Zhang, "A Spectral Approach to Shape-Based Retrieval of Articulated 3D Models", <i>CAD</i> 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618484</ref_obj_id>
				<ref_obj_pid>1661412</ref_obj_pid>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[K. Xu, H. Zhang, A. Tagliasacchi, L. Liu, M. Meng, L. Guo, Y. Xiong, "Partial Intrinsic Reflectional Symmetry of 3D Shapes", <i>SIGGRAPH Asia</i> 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spectral Mesh Processing SIGGRAPH 2010 Course Bruno Levy (INRIA, France) Hao (Richard) Zhang (Simon 
Fraser University, Canada)  About the instructors Bruno Levy INRIA, centre Nancy Grand Est, rue du 
Jardin Botanique, 54500 Vandoeuvre, France Email: bruno.levy@inria.fr http://alice.loria.fr/index.php/bruno-levy.html 
Bruno Levy is a researcher with INRIA. His main contribution concerns parameterization of triangulated 
surfaces (LSCM), and is now used by some 3D modeling software (including Maya, Silo, Blender, Gocad and 
Catia). He obtained his Ph.D in 1999, and was hired by INRIA in 2000. Since 2004, he has been leading 
Project ALICE -Geometry and Light, that aims at developping mathematical tools for next generation geometry 
processing. He served on the committee of ACM SPM, IEEE SMI, ACM/EG SGP, IEEE Visualization, Eurographics, 
PG, ACM Siggraph, and was program co-chair of ACM SPM in 2007 and 2008, and will be program co-chair 
of SGP 2010. He was recently awarded a Starting Grant from the European Research Council (3% acceptance, 
all disciplines of science). Hao (Richard) Zhang School of Computing Science Simon Fraser University, 
Burnaby, Canada V5A 1S6 Email: haoz@cs.sfu.ca http://www.cs.sfu.ca/ haoz Hao (Richard) Zhang is an Associate 
Professor in the School of Computing Science at Simon Fraser University, Canada, and he co-directs the 
Graphics, Usability, and Visualization (GrUVi) Lab. He received his Ph.D. from the University of Toronto 
in 2003 and M. Math. and B. Math. degrees from the University of Waterloo. His research interests include 
geometry processing, shape analysis, and computer graphics. He gave a Eurographics State-of-theart report 
on spectral methods for mesh processing and analysis in 2007 and wrote the .rst comprehensive survey 
on this topic. Recently, he has served on the program committees of Eurographics, ACM/EG SGP, ACM/SIAM 
GPM, and IEEE SMI. He was a winner of the Best Paper Award from SGP 2008. Course description Summary 
statement: In this course, you will learn the basics of Fourier analysis on meshes, how to implement 
it, and how to use it for .ltering, remeshing, matching, compressing, and segmenting meshes. Abstract: 
Spectral mesh processing is an idea that was proposed at the beginning of the 90 s, to port the signal 
processing toolbox to the setting of 3D mesh models. Recent advances in both computer horsepower and 
numerical software make it possible to fully implement this vision. In the more classical context of 
sound and image processing, Fourier analysis was a corner stone in the development of a wide spectrum 
of techniques, such as .ltering, compression, and recognition. In this course, attendees will learn 
how to transfer the underlying concepts to the setting of a mesh model, how to implement the spectral 
mesh processing toolbox and use it for real applications, including .ltering, shape matching, remeshing, 
segmentation, and parameterization. Background: Some elements of this course appeared in the Geometric 
Modeling based on Polygon Meshes course (SIGGRAPH 07, Eurographics 08), the Mesh Parameterization, Theory 
and Practice course (SIGGRAPH 07), and the Mesh Processing course (invited course, ECCV 08). Based on 
some feedback from the attendees, it seems that there is a need for a course focused on the spectral 
aspects. Whereas these previous courses just mentioned some applications of spectral methods, this course 
details the notions relevant to spectral mesh processing, from theory to implementation. Spectral methods 
for mesh processing had been presented as a Eurographics State-of-the-art report in 2007 and a subsequent 
survey was completed in 2009. Some coverages in the current notes, in particular those on applications, 
have been selected from the survey. This course, given at ACM SIGGRAPH ASIA 2009 and ACM SIGGRAPH 2010, 
starts with a more basic setup and include updates from some of the most recent developments in spectral 
mesh processing. Intended audience: Researcher in the areas of geometry processing, shape analysis, and 
computer graphics, as well as practitioners who want to learn more about how to implement these new techniques 
and what are the applications. Prerequisites: Knowledge about mesh processing, programming, mesh data 
structures, basic notions of linear algebra and signal processing. Course syllabus  (5 min) Introduction 
[Levy]  (45 min) What is so spectral? [Zhang]  Intuition and theory behind spectral methods Di.erent 
interpretations and motivating applications (55 min) Do your own Spectral Mesh processing at home [Levy] 
DEC Laplacian, numerics of spectral analysis Tutorial on implementation with open source software (15 
min) break (50 min) Applications - What can we do with it ? 1/2 [Zhang] Segmentation, shape retrieval, 
non-rigid matching, symmetry detection, ... (40 min) Applications - What can we do with it ? 2/2 [Levy] 
Quadrangulation, parameterization, . . . (15 min) Wrapup, conclusion, Q&#38;A [Zhang and Levy] Spectral 
mesh processing involves the use of eigenvalues, eigenvectors, or eigenspace projections derived from 
appropriately de.ned mesh operators to carry out desired tasks. Early work in this area can be traced 
back to the seminal paper by Taubin [Tau95a] in 1995, where spectral analysis of mesh geometry based 
on a graph Laplacian allows us to understand the low-pass .ltering approach to mesh smoothing. Over the 
past .fteen years, the list of geometry processing applications which utilize the eigenstructures of 
a variety of mesh operators in di.erent manners have been growing steadily. Most spectral methods for 
mesh processing have a basic framework in common. First, a matrix representing a discrete linear operator 
based on the topological and/or geometric structure of the input mesh is constructed, typically as a 
discretization of some continuous operator. This matrix can be seen as incorporating pairwise relations 
between mesh elements, adjacent ones or otherwise. Then an eigendecomposition of the matrix is performed, 
that is, its eigenvalues and eigenvectors are computed. Resulting structures from the decomposition are 
employed in a problem-speci.c manner to obtain a solution. We will look at the various applications of 
spectral mesh processing towards the end of the course notes (Section 5), after we provide the intuition, 
moti vation, and some theory behind the use of the spectral approach. The main motivation for developing 
spectral methods for mesh processing is the pursuit of Fourier analysis in the manifold setting, in particular, 
for meshes which are the dominant discrete representations of surfaces in the .eld of computer graphics. 
There are other desirable characteristics of the spectral approach including e.ective and information-preserving 
dimensionality reduction and its ability to reveal global and intrinsic structures in geometric data 
[ZvKDar]. These should become clear as we discuss the applications. We shall start the course notes with 
a very gentle introduction in Section 1. Instead of diving into 3D data immediately, we .rst look at 
the more classic case of processing a 2D shape represented by a contour. The motivating application is 
shape smoothing. We reduce the 2D shape processing problem to the study of 1D functions or signals specifying 
the shape s contour, naturally exposing the problem in a signal-processing framework. Our choice to start 
the coverage in the discrete setting is intended to not involve the heavy mathematical formulations 
at the start so as to better provide an intuition. We present Laplacian smoothing and show how spectral 
processing based on 1D discrete Laplace operators can perform smoothing as well as compression. The 
relationship between this type of spectral processing and the classical Fourier transform is revealed 
and a natural extension to the mesh setting is introduced. Having provided an intuition, we then instill 
more rigor into our coverage and more formally present spectral mesh processing as a means to perform 
Fourier analysis on meshes. In particular, we deepen our examination on the connection between the continuous 
and the discrete settings, focusing on the Laplace operator and its eigenfunctions. While Section 2 provides 
some theoretical background in the continuous setting (eigenfunctions of the Laplace-Beltrami operator) 
and establishes connections with other domains (machine learning and spectral graph theory). Section 
3 is concerned with ways to discretize the Laplace operator. Since spectral mesh processing in practice 
often necessitates the computation of eigenstructures of large matrices, Section 4 presents ways to make 
that process e.cient. Finally, we discuss various applications of spectral mesh processing in Section 
5. For readers who wish to obtain a more thorough coverage on the topic, we refer to the survey of Zhang 
et al. [ZvKDar]. A gentle introduction Consider the seahorse shape depicted by a closed contour, shown 
in Figure 1. The contour is represented as a sequence of 2D points (the contour vertices) that are connected 
by straight line segments (the contour segments), as illustrated by a zoomed-in view. Now suppose that 
we wish to remove the rough features over the shape of the seahorse and obtain a smoothed version, such 
as the one shown in the right of Figure 1.  Figure 1: A seahorse with rough features (left) and a smoothed 
version (right). 1.1 Laplacian smoothing A simple procedure to accomplish this is to repeatedly connect 
the midpoints of successive contour segments; we refer to this as midpoint smoothing. Figures 2(a) illustrates 
two such steps. As we can see, after two steps of midpoint smoothing, each contour vertex vi is moved 
to the midpoint of the line segment connecting the midpoints of the original contour segments adjacent 
to vi. Speci.cally, let vi-1 =(xi-1,yi-1), vi =(xi,yi), and vi+1 =(xi+1,yi+1) be three consecutive contour 
vertices. Then the new vertex v i after two steps of midpoint smoothing is given by a local averaging, 
 11 11 111 v i =(vi-1 + vi)+ (vi + vi+1)= vi-1 + vi + vi+1. (1) 22 22 424 The vector between vi and the 
midpoint of the line segment connecting the two vertices adjacent to vi, shown as a red arrow in Figure 
2(b), is called the  (a) Two steps of midpoint smoothing. (b) Laplacian smoothing. (c) 1D Laplacians 
(red). Figure 2: One step of Laplacian smoothing (b) is equivalent to two steps of midpoint smoothing 
(a). The 1D discrete Laplacian vectors (c) are in red. 1D discrete Laplacian at vi, denoted by d(vi), 
1 d(vi)= (vi-1 + vi+1) - vi. (2) 2 As we can see, after two steps of midpoint smoothing, each contour 
vertex is displaced by half of its 1D discrete Laplacian, as shown in Figure 2(c); this is referred to 
as Laplacian smoothing. The smoothed version of the seahorse in Figure 1 was obtained by applying 10 
steps of Laplacian smoothing. 1.2 Signal representation and spectral transform Let us denote the contour 
vertices by a coordinate vector V , which has n rows and two columns, where n is the number of vertices 
along the contour and the two columns correspond to the x and y components of the vertex coordinates. 
Let us denote by xi (respectively, yi) the x (respectively, y) coordinate of a vertex vi, i =1,...,n. 
For analysis purposes, let us only consider the x component of V , denoted by X; the treatment of the 
y component is similar. We treat the vector X as a discrete 1D signal. Since the contour is closed, we 
can view X as a periodic 1D signal de.ned over uniformly spaced samples along a circle, as illustrated 
in Figure 3(a). We sort the contour vertices in counterclockwise order and plot it as a conventional 
1D signal, designating an arbitrary element in X to start the indexing. Figure 3(b) shows such a plot 
for the x-coordinates of the seahorse shape (n = 401) from Figure 1. We can now express the discrete 
1D Laplacians (2) for all the vertices using an n  n matrix Figure 3: Left: The x-component of the contour 
coordinate vector V , X, can be viewed as a 1D periodic signal de.ned over uniform samples along a circle. 
Right: it is shown by a 1D plot for the seahorse contour from Figure 1.  L, called the 1D discrete Laplace 
operator, as follows: . . 11 -- 1 0 ... ... 0 22 ...... 11 ...... - 1 - 0 ... ... 0 22 . . ... . . . 
. ... . . . . ... . . d(X)= LX = X. (3) 11 -- 0 ... ... 0 1 22 11 - 0 ... ... 0 - 1 22 The new contour 
X resulting from Laplacian smoothing (1) is then given by . .. .. . 111 0 ... ... 0x 1 x1 424 ...... 
x n-1 ...... = ...... ...... ...... ...... 1 4 1 2 1 4 x 2 . . . 0 ... ... 0 x2 . . . xn-1 X = .. . 
. . .. .. . . . .. .. . . . .. = SX. (4) 1 4 1 2 1 4 0 ... ... 0 111 x n 0 ... ... 0xn 424 The smoothing 
operator S is related to the Laplace operator L by 1 S = I - L. 2 To analyze the behavior of Laplacian 
smoothing, in particular what happens in the limit, we rely on the set of basis vectors formed by the 
eigenvectors of L. This leads to a framework for spectral analysis of geometry. From linear algebra, 
we know that since L is symmetric, it has real eigenvalues and a set of real and orthogonal set of eigenvectors 
which form a basis. Any vector of size n can be expressed as a linear sum of these basis vectors. We 
are particularly interested in such an expression for the coordinate vector X. Denote by e1, e2,...,en 
the normalized eigenvectors of L, corresponding to eigenvalues .1, .2, ..., .n, and let E be the matrix 
whose columns are the eigenvectors. Then we  Figure 4: Plots of .rst 8 eigenvectors of the 1D discrete 
Laplace operator (n = 401) given in equation (3). They are sorted by the eigenvalue .. can express X 
as a linear sum of the eigenvectors, x1 .. . . .. .. E11 E1n E11 ... E1n nX = eix i E21 ... ... x 1 + 
... + ... E2n ... ... x n = ... E21 ... E2n ... ... ... ... x2 = E X. = i=1 xn (5) The above expression 
represents a transform from the original signal X to a new signal X in terms of a new basis, the basis 
given by the eigenvectors of L. We call this a spectral transform, whose coe.cients X can be obtained 
by X = ETX, where ET is the transpose of E, and for each i, the spectral transform coe.cient T x i = 
e  X. (6) i That is, the spectral coe.cient xi is obtained as a projection of the signal X along the 
direction of the i-th eigenvector ei. In Figure 4, we plot the .rst 8 eigenvectors of L, sorted by increasing 
eigenvalues, where n = 401, maching the size of the seahorse shape from Figure 1. The indexing of elements 
in each eigenvector follows the same contour vertex indexing as X, the coordinate vector; it was plotted 
in Figure 3(b) for the seahorse. It is worth noting that aside from an agreement on indexing, the Laplace 
operator L and the eigenbasis vectors do not depend on X, which speci.es the geometry of the contour. 
L, as de.ned in equation (3), is completely determined by n, the size of the input contour, and a vertex 
ordering. As we can see, the eigenvector corresponding to the zero eigenvalue is a constant vector. As 
eigenvalue increases, the eigenvectors start to oscillate as sinusoidal curves at higher and higher frequencies. 
Note that the eigenvalues of En1 Enn En1 ... Enn ... ... ... ... k =3.k =5.k = 10.k = 20.k = 30.k 1 
n. Original. 2 Figure 5: Shape reconstruction via Laplacian-based spectral analysis. L repeat (multiplicity 
2) after the .rst one, hence the corresponding eigenvectors of these repeated eigenvalues are not unique. 
One particular choice of the eigenvectors reveals a connection of our spectral analysis to the classical 
Fourier analysis; this will be discussed in Section 1.5.  1.3 Signal reconstruction and compression 
With the spectral transform of a coordinate signal de.ned as in equation (5), we can now look at compression 
and .ltering of a 2D shape represented by a contour. Analogous to image compression using JPEG, we can 
obtain compact representations of a contour by retaining the leading (low-frequency) spectral transform 
coe.cients and eliminating the rest. Given a signal X as in equation (5), the signal reconstructed by 
using the k leading coe.cients is k X(k) = eix i,k = n. (7) i=1 This represents a compression of the 
contour geometry since only k out of n coe.cients need to be stored to approximate the original shape. 
We can quantify the information loss by measuring the L2 error n n ||X - X(k)|| = || eix i|| = x 2 i 
, i=k+1 i=k+1 which is simply the Euclidean distance between X and X(k). The last equality T is easy 
to obtain if we note the orthogonality of the eigenvectors, i.e., eej =0 i T whenever i Also, since 
the ei s are normalized, ei = j.  ei = 1. In Figure 5, we show some results of this type of shape reconstruction 
(7), with varying k for the seahorse and a bird shape. As more and more highfrequency spectral coe.cients 
are removed, i.e., with decreasing k, we obtain Figure 6: Plot of spectral transform coe.cients for the 
x component of a contour. Left: seahorse. Right: bird. The models are shown in Figure 5.  smoother and 
smoother reconstructed contours. How e.ectively a 2D shape can be compressed this way may be visualized 
by plotting the spectral transform coe.cients, the xi s in (6), as done in Figure 6. In the plot, the 
horizontal axis represents eigenvalue indexes, i =1,...,n, which roughly correspond to frequencies. One 
can view the magnitude of the xi s as the energy of the input signal X at di.erent frequencies. A signal 
whose energies are sharply concentrated in the low-frequency end can be e.ectively compressed at a high 
compression rate, since as a consequence, the total energy at the high-frequency end, representing the 
reconstruction error, is very low. Such signals will exhibit fast decay in its spectral coe.cients. Both 
the seahorse and the bird models contain noisy or sharp features so they are not as highly compressible 
as a shape with smoother boundaries. This can be observed from the plots in Figure 6. Nevertheless, at 
2:1 compression ratio, we can obtain a fairly good approximation, as one can see from Figure 5.  1.4 
Filtering and Laplacian smoothing Compression by truncating the vector X of spectral transform coe.cients 
can be seen as a .ltering process. When a discrete .lter function f is applied to X, we obtain a new 
coe.cient vector X ', where X '(i)= f(i)  X (i), for all i. The .ltered signal X' is then reconstructed 
from X ' by X' = EX ', where E is the matrix of eigenvectors as de.ned in equation (5). We next show 
that Laplacian smoothing is one particular .ltering process. Speci.cally, when we apply the Laplacian 
smoothing operator S to a coordinate vector m times, the resulting coordinate vector becomes, nn 11 1 
X(m) = SmX =(I - L)mX =(I - L)m eix i = ei(1 - .i)m x i. (8) 22 2 i=1 i=1 Equation (8) provides a characterization 
of Laplacian smoothing in the spec tral domain via .ltering and the .lter function is given by f(.) 
= (1 - 1 .)m . 2 A few such .lters with di.erent m are plotted in the .rst row of Figure 7. The Figure 
7: First row: .lter plots, (1 - 1 .)m with m =1, 5, 10, 50. Second row: 2 corresponding results of Laplacian 
smoothing on the seahorse. corresponding Laplacian smoothing leads to attenuation of the high-frequency 
content of the signal and hence achieves denoising or smoothing. To examine the limit behavior of Laplacian 
smoothing, let us look at equation (8). Note that it can be shown (via the Gerschgorin s Theorem [TB97]) 
that the eigenvalues of the Laplace operator are in the interval [0, 2] and the smallest eigenvalue .1 
= 0. Since . . [0, 2], the .lter function f(.) = (1 - 1 .)m 2 is bounded by the unit interval [0, 1] 
and attains the maximum f(0) = 1at . = 0. As m .8, all the terms in the right-hand side of equation (8) 
will vanish except for the .rst, which is given by e1x 1. Since e1, the eigenvector corresponding to 
the zero eigenvalue is a normalized, constant vector, we have 1v v v 11 nn n ]T . e1 = [ Now taking the 
y-component into consideration, we ... 1v n [ x1 y 1]. Finally, noting that get the limit point for Laplacian 
smoothing as TT x 1 = e X and y1 = e Y , we conclude that the limit point of Laplacian 11 smoothing 
is the centroid of the set of original contour vertices. 1.5 Spectral analysis vs. discrete Fourier 
transform The sinosoidal behavior of the eigenvectors of the 1D discrete Laplace operator (see plots 
in Figure 4) leads one to believe that there must be a connection between the discrete Fourier transform 
(DFT) and the spectral transform we have de.ned so far. We now make that connection explicit. Typically, 
one introduces the DFT in the context of Fourier series expansion. Given a discrete signal X =[x1 x2 
... xn]T, its DFT is given by n X (k)= 1 X(k)e -i2p(k-1)(j-1)/n,k =1, . . . , n. n j=1 And the corresponding 
inverse DFT is given by n X(j)= X (k)e i2p(j-1)(k-1)/n,j =1, . . . , n, k=1 or n X = X (k)gk, where gk(j)= 
e i2p(j-1)(k-1)/n,k =1, . . . , n. k=1 We see that in the context of DFT, the signal X is expressed as 
a linear combination of the complex exponential DFT basis functions, the gk s. The coe.cients are given 
by the X(k) s, which form the DFT of X. Fourier analysis, provided by the DFT in the discrete setting, 
is one of the most important topics in mathematics and has wide-ranging applications in many scienti.c 
and engineering disciplines. For a systematic study of the subject, we refer the reader to the classic 
text by Bracewell [Bra99]. The connection we seek, between DFT and spectral analysis with respect to 
the Laplace operator, is that the DFT basis functions, the gk s, form a set of eigenvectors of the 1D 
discrete Laplace operator L, as de.ned in (3). A proof of this fact can be found in Jain s classic text 
on image processing [Jai89], where a stronger claim with respect to circulant matrices was made. Note 
that a matrix is circulant if each row can be obtained as a shift (with circular wrap-around) of the 
previous row. It is clear that L is circulant. Speci.cally, if we sort the eigenvalues of L in ascending 
order, then they are plk/2J .k = 2 sin2 ,k =2, . . . , n. (9) n The .rst eigenvalue .1 is always 0. We 
can see that every eigenvalue of L, except for the .rst, and possibly the last, has a multiplicity of 
2. That is, it corresponds to an eigensubspace spanned by two eigenvectors. If we de.ne the matrix G 
of DFT basis as Gkj = ei2p(j-1)(k-1)/n,1 = k, j = n, then the .rst column of G is an eigenvector corresponding 
to .1 and the k-th and (n +2 - k)-th columns of G are two eigenvectors corresponding to .k, for k =2,...,n. 
The set of eigenvectors of L is not unique. In particular, it has a set of real eigenvectors; some of 
these eigenvectors are plotted in Figure 4.  1.6 Towards spectral mesh transform Based on the above 
observation, one way to extend the notion of Fourier analysis to the manifold or surface setting, where 
our signal will represent the geometry of the surfaces, is to de.ne appropriate discrete Laplace operators 
for meshes and rely on the eigenvectors of the Laplace operators to perform Fourier analysis. This was 
already observed in Taubin s seminal paper [Tau95b]. To extend spectral analysis of 1D signals presented 
in Section 1.2 to surfaces modeled by triangle meshes, we .rst need to extend the signal representation. 
This is quite straightforward: any function de.ned on the mesh vertices can be seen as a discrete mesh 
signal. Typically, we focus on the coordinate signal for (a) Original. (b) k = 300. (c) k = 200. (d) 
k = 100. (e) k = 50. (f) k = 10. (g) k = 5. (h) k = 3. Figure 8: Shape reconstruction based on spectral 
analysis using a typical mesh Laplace operator, where k is the number of eigenvectors or spectral coe.cients 
used. The original model has 7,502 vertices and 15,000 faces. the mesh, which, for a mesh with n vertices, 
is an n  3 matrix whose columns specify the x, y, and z coordinates of the mesh vertices. The main task 
is to de.ne an appropriate Laplace operator for the mesh. Here a crucial di.erence to the classical DFTs 
is that while the DFT basis functions are .xed as long as the length of the signal in question is determined, 
the eigenvectors of a mesh Laplace operator would change with the mesh connectivity and/or geometry. 
Formulations for the construction of appropriate mesh Laplace operators will be the subjects of Sections 
2 and 3. Now with a mesh Laplace operator chosen, de.ning the spectral transform of a mesh signal X 
with respect to the operator is exactly the same as the 1D case for L in Section 1.2. Denote by e1, e2,...,en 
the normalized eigenvectors of the mesh Laplace operator, corresponding to eigenvalues .1 = .2 = ... 
= .n, and let E be the matrix whose columns are the eigenvectors. The vector of spectral transform coe.cients 
X is obtained by X = ETX. And for each i, we T obtain the spectral coe.cient by xi = e X via projection. 
i As a .rst visual example, we show in Figure 8 some results of spectral reconstruction, as de.ne in 
(7), of a mesh model with progressively more spectral coe.cients added. As we can see, higher-frequency 
contents of the geometric mesh signal manifest themselves as rough geometric features over the shape 
s surface; these features can be smoothed out by taking only the low-frequence spectral coe.cients in 
a reconstruction. A tolerance on such loss of geometric features would lead to a JPEG-like compression 
of mesh geometry, as .rst proposed by Karni and Gotsman [KG00a] in 2000. More applications using spectral 
transforms of meshes will be given in Section 5. 2 Fourier analysis for meshes The previous section 
introduced the idea of Fourier analysis applied to shapes, with the example of a closed curve, for which 
the frequencies (sine waves) are naturally obtained as the eigenvectors of the 1D discrete Laplacian. 
We now study how to formalize the idea presented in the last subsection, i.e. porting this setting to 
the case of arbitrary surfaces. Before diving into the heart of the matter, we recall the de.nition of 
the Fourier transform, that is to say the continuous version of the discrete signal processing framework 
presented in Subsections 1.2 and 1.4.  2.1 Fourier analysis As in Taubin s article [Tau95b], we start 
by studying the case of a closed curve, but staying in the continuous setting. Given a square-integrable 
periodic function f : x . [0, 1] . f(x), or a function f de.ned on a closed curve parameterized by 
normalized arclength, it is well known that f can be expanded into an in.nite series of sines and cosines 
of increasing frequencies: . 8 . H0 =1 H2k+1 f(x)= f kHk(x) ; = cos(2kpx) (10). H2k+2 k=0 = sin(2kpx) 
where the coe.cients f k of the decomposition are given by: . 1 f k =< f,Hk >= f(x)Hk(x)dx (11) 0 and 
where < .,. > denotes the inner product (i.e. the dot product for functions de.ned on [0, 1]). See [Arv95] 
or [Lev06] for an introduction to functional analysis. The Circle harmonics basis Hk is orthonormal 
with respect to < .,. >: <Hk,Hk >= 1, <Hk,Hl >=0 if k = l. The set of coe.cients f k (Equation 11) is 
called the Fourier Transform (FT) of the function f. Given the coe.cients f k, the function f can be 
reconstructed by applying the inverse Fourier Transform FT-1 (Equation 10). Our goal is now to explain 
the generalization of these notions to arbitrary manifolds. To do so, we can consider the functions Hk 
of the Fourier basis as the eigenfunctions of -.2/.x2: the eigenfunctions H2k+1 (resp. H2k+2) are associated 
with the eigenvalues (2kp)2: .2H2k+1(x)- = (2kp)2 cos(2kpx) = (2kp)2H2k+1(x) .x2 This construction can 
be extended to arbitrary manifolds by considering the generalization of the second derivative to arbitrary 
manifolds, i.e. the Laplace operator and its variants, introduced below. Before studying the continuous 
theory, we .rst do a step backward into the discrete setting, in which it is easier Figure 9: The Fiedler 
vector gives a natural ordering of the nodes of a graph. The displayed contours show that it naturally 
follows the shape of the dragon. to grasp a geometric meaning of the eigenfunctions. In this setting, 
eigenfunctions can be considered as orthogonal non-distorting 1D parameterizations of the shape, as 
explained further. 2.2 The discrete setting: Graph Laplacians Spectral graph theory was used for instance 
in [IL05] to compute an ordering of vertices in a mesh that facilitates out-of-core processing. Such 
a natural ordering can be derived from the Fiedler eigenvector of the Graph Laplacian. The Graph Laplacian 
L =(ai,j) is a matrix de.ned by: ai,j = wi,j > 0 if(i, j) is an edge , ai,i = - j wi,j ai,j = 0 otherwise 
where the coe.cients wi,j are weights associated to the edges of the graph. One may use the uniform weighting 
wi,j = 1 or more elaborate weightings, computed from the embedding of the graph. There are several variants 
of the Graph Laplacian, the reader is referred to [ZvKDar] for an extensive survey. Among these discrete 
Laplacians, the so-called Tutte Laplacian applies a normalization in each row of L and is given by T 
=(ti,j ), where ti,j = wi,j> 0 if(i, j) is an edge j wi,j ti,i = -1 ti,j = 0 otherwise The Tutte Laplacian 
was employed in the original work of Taubin [Tau95a], among others [GGS03a, Kor03]. The .rst eigenvector 
of the Graph Laplacian is (1, 1 ... 1) and its associated eigenvalue is 0. The second eigenvector is 
called the Fiedler vector and has interesting properties, making it a good permutation vector for numerical 
computations [Fie73, Fie75]. It has many possible applications, such as .nding natural vertices ordering 
for streaming meshes [IL05]. Figure 9 shows what it looks like for a snake-like mesh (it naturally follows 
the shape of the mesh). More insight on the Fiedler vector is given by the following alternative definition. 
The Fiedler vector u =(u1 ...un) is the solution of the following constrained minimization problem: 
, Minimize: F (u)= utLu = i,j wi,j (ui - uj)2 (12) ,, 2 Subject to: i ui = 0 and i ui =1 In other words, 
given a graph embedded in some space, and supposing that the edge weight wi,j corresponds to the lengths 
of the edges in that space, the Fielder vector (u1 ...un) de.nes a (1-dimensional) embedding of the graph 
on a line that tries to respect the edge lengths of the graph. This naturally leads to the question of 
whether embedding in higher-dimensional spaces can be computed (for instance, computing a 2-dimensional 
embedding of a surface corresponds the the classical surface parameterization problem). This general 
problem is well known by the automatic learning research community as a Manifold learning problem, also 
called dimension reduction. One of the problems in manifold learning is extracting from a set of input 
(e.g. a set of images of the same object) some meaningful parameters (e.g. camera orientation and lighting 
conditions), and sort these images with respect to these parameters. From an abstract point of view, 
the images leave in a highdimensional space (the dimension corresponds to the number of pixels of the 
images), and one tries to parameterize this image space. The .rst step constructs a graph, by connecting 
each sample to its nearest neighbors, according to some distance function. Then, di.erent classes of 
methods have been de.ned, we quickly review the most popular ones: Local Linear Embedding [RS00a] tries 
to create an embedding that best approximates the barycentric coordinates of each vertex relative to 
its neighbors. In a certain sense, Floater s Shape Preserving Parameterization (see [FH04]) is a particular 
case of this approach. Isomap [TdSL00] computes the geodesic distances between each pair of ver tex 
in the graph, and then uses MDS (multidimensional scaling) [You85] to com pute an embedding that best 
approximates these distances. Multidimensional scaling simply minimizes an objective function that measures 
the deviation between the geodesic distances in the initial space and the Euclidean distances in the 
embedding space (GDD for Geodesic Distance Deviation), by computing the eigenvectors of the matrix D 
=(di,j ) where di,j denotes the geodesic distance between vertex i and vertex j. This is a multivariate 
version of Equation 12, that characterizes the Fiedler vector (in the univariate setting). Isomaps and 
Multidimensional scaling were used to de.ne parameterization algorithms in [ZKK02], and more recently 
in the ISO-charts method [ZSGS04], used in Mi crosoft s DirectX combined with the packing algorithm 
presented in [LPM02]. At that point, we understand that the eigenvectors play an important role in determining 
meaningful parameters. Just think about the simple linear case: in PCA (principal component analysis), 
the eigenvectors of the covariance matrix characterize the most appropriate hyperplane on which the data 
should be projected. In dimension reduction, we seek for eigenvectors that will .t non-linear features. 
For instance, in MDS, these eigenvectors are computed in a way that makes the embedding space mimic the 
global metric structure of the surface, captured by the matrix D =(di,j) of all geodesic distances between 
all pairs of vertices in the graph. Instead of using the dense matrix D, methods based on the Graph Laplacian 
only use local neighborhoods (one-ring neighborhoods). As a consequence, the used matrix is sparse, and 
extracting its eigenvectors requires lighter computations. Note that since the Graph Laplacian is a 
symmetric matrix, its eigenvectors are orthogonal, and can be used as a vector basis to represent functions. 
This was used in [KG00b] to de.ne a compact encoding of mesh geometry. The basic idea consists in encoding 
the topology of the mesh together with the coef.cients that de.ne the geometry projected onto the basis 
of eigenvectors. The decoder simply recomputes the basis of eigenvectors and multiplies them with the 
coe.cients stored in the .le. A survey of spectral geometry compression and its links with graph partitioning 
is given in [Got03]. Spectral graph theory also enables to exhibit ways of de.ning valid graph embeddings. 
For instance, Colinde-verdi`ere s number [dV90] was used in [GGS03b] to construct valid spherical embeddings 
of genus 0 meshes. Other methods that use spectral graph theory to compute graph embeddings are reviewed 
in [Kor02]. Spectral graph theory can also be used to compute topological invariants (e.g. Betti numbers), 
as explained in [Fei96]. As can be seen from this short review of spectral graph theory, the eigenvectors 
and eigenvalues of the graph Laplacian contain both geometric and topological information. However, as 
explained in [ZSGS04], only using the connectivity of the graph may lead to highly distorted mappings. 
Methods based on MDS solve this issue by considering the matrix D of the geodesic distances between all 
possible pairs of vertices. However, we think that it is also possible to inject more geometry in the 
Graph Laplacian approach, and understand how the global geometry and topology of the shape may emerge 
from the interaction of local neighborhoods. This typically refers to notions from the continuous setting, 
i.e. functional analysis and operators. The next section shows its link with the Laplace-Beltrami operator, 
that appears in the wave equation (Helmholtz s equation). We will also exhibit the link between the so-called 
stationary waves and spectral graph theory. 2.3 The Continuous Setting: Laplace Beltrami The Laplace 
operator (or Laplacian) plays a fundamental role in physics and mathematics. In Rn, it is de.ned as the 
divergence of the gradient: .2 . = div grad = V.V = .x2 i i Intuitively, the Laplacian generalizes the 
second order derivative to higher dimensions, and is a characteristic of the irregularity of a function 
as .f(P ) measures the di.erence between f(P ) and its average in a small neighborhood of P . Generalizing 
the Laplacian to curved surfaces require complex calculations. These calculations can be simpli.ed by 
a mathematical tool named exterior calculus (EC) 1 . EC is a coordinate free geometric calculus where 
functions are considered as abstract mathematical objects on which operators act. To use these functions, 
we cannot avoid instantiating them in some coordinate frames. However, most calculations are simpli.ed 
thanks to higher-level considerations. For instance, the divergence and gradient are known to be coordinate 
free operators, but are usually de.ned through coordinates. EC generalizes the gradient by d and divergence 
by d, which are built independently of any coordinate frame. Using EC, the de.nition of the Laplacian 
can be generalized to functions de.ned over a manifold S with metric g, and is then called the Laplace-Beltrami 
operator: 1.. . = div grad = dd = |g||g| .xi.xi i where |g| denotes the determinant of g. The additional 
term|g| can be interpreted as a local scale factor since the local area element dA on S is given by 
 dA =|g|dx1 . dx2. Finally, for the sake of completeness, we can mention that the Laplacian can be extended 
to k-forms and is then called the Laplace-de Rham operator de.ned by . = dd + dd. Note that for functions 
(i.e. 0-forms), the second term dd vanishes and the .rst term dd corresponds to the previous de.nition. 
Since we aim at constructing a function basis, we need some notions from functional analysis, quickly 
reviewed here. A similar review in the context of light simulation is given in [Arv95]. 2.3.1 Functional 
analysis In a way similar to what is done for vector spaces, we need to introduce a dot product (or inner 
product) to be able to de.ne function bases and project functions onto those bases. This corresponds 
to the notion of Hilbert space, outlined 1 To our knowledge, besides Hodge duality used to compute minimal 
surfaces [PP93], one of the .rst uses of EC in geometry processing [GY02] applied some of the fundamental 
notions involved in the proof of Poincare s conjecture to global conformal parameterization. More recently, 
a Siggraph course was given by Schroeder et al. [SGD05], making these notions usable by a wider community. 
below. Hilbert spaces Given a surface S, let X denote the space of real-valued functions de.ned on S. 
Given a norm I.I, the function space X is said to be complete with respect to the norm if Cauchy sequences 
converge in X, where a Cauchy sequence is a sequence of functions f1,f2,... such that limn,m->8 Ifn -fmI 
= 0. A complete vector space is called a Banach space. The space X is called a Hilbert space in the speci.c 
case of the norm de.ned v by IfI = < f,f >, where < .,. > denotes an inner product. A possible i de.nition 
of an inner product is given by < f,g >= f(x)g(x)dx, which yields S the L2 norm. One of the interesting 
features provided by this additional level of structure is the ability to de.ne function bases and project 
onto these bases using the inner product. Using a function basis (Fi), a function f will be de.ned by 
, f = aiFi. Similarly to the de.nition in geometry, a function basis (Fi) is orthonormal if IFiI = 1 
for all i and < Fi, Fj >= 0 for all i = j. Still following the analogy with geometry, given the function 
f, one can easily retrieve its coordinates ai with respect to an orthonormal basis (Fi) by projecting 
f onto the basis, i.e. ai =< f, Fi >. Operators We now give the basic de.nitions related with operators. 
Simply put, an operator is a function of functions (i.e. from X to X). An operator L applied to a function 
f . X is denoted by Lf, and results in another function in X. An operator L is said to be linear if L(.f)= 
.Lf for all f . X, . . R. An eigenfunction of the operator L is a function f such that Lf = .f. The scalar 
. is an eigenvalue of L. In other words, the e.ect of applying the operator to one of its eigenfunctions 
means simply scaling the function by the scalar .. A linear operator L is said to be Hermitian (or with 
Hermitian symmetry)2 if < Lf,g >=< f,Lg > for each f, g . X. An important property of Hermitian operators 
is that their eigenfunctions associated to di.erent eigenvalues have real eigenvalues and are orthogonal. 
This latter property can be easily proven as follows, by considering two eigenfunctions f, g associated 
with the di.erent eigenvalues .,  respectively: < Lf,g > = < f,Lg > < .f,g > = < f,g > . < f,g > = 
 < f,g > which gives the result (< f,g >= 0) since . = . As a consequence, considering the eigenfunctions 
of an Hermitian operator is a possible way of de.ning an orthonormal function basis associated to a given 
2the general de.nition of Hermitian operators concerns complex-valued functions, we only consider here 
real-valued functions. function space X. The next section shows this method applied to the Laplace-Beltrami 
operator. Before entering the heart of the matter, we will .rst consider the historical perspective. 
 2.3.2 Chladni plates In 1787, the physicist Ernst Chladni published the book entitled Discoveries Concerning 
the Theories of Music . In this book, he reports his observations obtained when putting a thin metal 
plate into vibration using a bow, and spreading sand over it. Sand accumulates in certain zones, forming 
surprisingly complex patterns (see Figure 10). This behavior can be explained by the theory of stationary 
waves. When the metal plate vibrates, some zones remain still, and sand naturally concentrates in these 
zones. This behavior can be modeled as follows, by the spatial component of Helmholtz s wave propagation 
equation: .f = .f (13) In this equation, . denotes the Laplace-Beltrami operator on the considered object. 
In Cartesian 2D space, . = .2/.x2 + .2/.y2 . We are seeking for the eigenfunctions of this operator. 
To better understand the meaning of this equation, let us .rst consider a vibrating circle. This corresponds 
to the univariate case on the interval [0, 2p] with cyclic boundary conditions (i.e. f(0) = f(2p)). In 
this setting, the Laplace-Beltrami operator simply corresponds to the second order derivative. Recalling 
that sin(.x) '' = .2sin(.x), the eigenfunctions are simply sin(Nx), cos(Nx) and the constant function, 
where N is an integer. Note that the so-constructed function basis is the one used in Fourier analysis. 
From the spectrum of the Laplace-Beltrami operator, it is well known that one can extract the area of 
S, the length of its border and its genus. This leads to the question asked by Kac in 1966: Can we hear 
the shape of a drum ? [Kac66]. The answer to this question is no : one can .nd drums that have the same 
spectrum although they do not have the same shape [Cip93] (they are then referred to as isospectral shapes). 
However, the spectrum contains much information, which led to the idea of using it as a signature for 
shape matching and classi.cation, as explained in the shape DNA approach [RWP05a].  We are now going 
now to take a look at the eigenfunctions. Mathematicians mostly studied bounds and convergence of the 
spectrum. However, some results are known about the geometry of the eigenfunctions [JNT]. More precisely, 
we are interested in the so-called nodal sets, de.ned to be the zero-set of an eigenfunction. Intuitively, 
they correspond to the locations that do not move on a Chladni plate, where sand accumulates (see Figure 
10). A nodal set partitions the surface into a set of nodal domains of constant sign. The nodal sets 
and nodal domains are characterized by the following theorems: 1. the n-th eigenfunction has at most 
n nodal domains 2. the nodal sets are curves intersecting at constant angles  Besides their orthogonality, 
these properties make eigenfunction an interesting choice for function bases. Theorem 1 exhibits their 
multi-resolution nature, and from theorem 2, one can suppose that they are strongly linked with the geometry 
of the shape. Note also that these theorems explain the appearance of Chladni plates. This may also explain 
the very interesting re-meshing results obtained by Dong et. al [DBG+05], that use a Morse-smale decomposition 
of one of the eigenfunctions. In the case of simple objects, a closed form of the eigenfunctions can 
be derived. This made it possible to retrieve the patterns observed by Chladni in the case of a square 
and a circle. For curved geometry, Chladni could not make the experiment, since sand would not remain 
in the nodal set. However, one can still study the eigenfunctions. For instance, on a sphere, the eigenfunction 
correspond to spherical harmonics (see e.g. [JNT]), often used in computer graphics to represent functions 
de.ned on the sphere (such as radiance .elds or Bidirectional Re.ectance Distribution Functions). In 
other words, on a sphere, the eigenfunctions of the Laplace-Beltrami operator de.ne an interesting hierarchical 
function basis. One can now wonder whether the same approach could be used to create function bases on 
more complex geometries. In the general case, a closed form cannot be derived, and one needs to use a 
numerical approach, as explained in the next section. Discretizing the Laplace operator The previous 
section mentioned two di.erent settings for spectral analysis : the discrete setting is concerned with 
graphs, matrices and vectors;  the continuous setting is concerned with manifolds, operators and functions. 
 The continuous setting belongs to the realm of di.erential geometry, a powerful formalism for studying 
relations between manifolds and functions de.ned over them. However, computer science can only manipulate 
discrete quantities. Therefore, a practical implementation of the continuous setting requires a discretization. 
This section shows how to convert the continuous setting into a discrete counterpart using the standard 
Finite Element Modeling technique. In Geometry Processing, another approach consists in trying to de.ne 
a discrete setting that mimics the property of the continous theory. As such, we will also show how spectral 
geometry processing can be derived using Discrete Exterior Calculus (DEC). The eigenfunctions and eigenvalues 
of the Laplacian on a (manifold) surface S, are all the pairs (Hk,.k) that satisfy: -.Hk = .kHk (14) 
The - sign is here required for the eigenvalues to be positive. On a closed curve, the eigenfunctions 
of the Laplace operator de.ne the function basis (sines and cosines) of Fourier analysis, as recalled 
in the previous section. On a square, they correspond to the function basis of the DCT (Discrete Cosine 
Transform), used for instance by the JPEG image format. Finally, the eigenfunctions of the Laplace-Beltrami 
operator on a sphere de.ne the Spherical Harmonics basis. In these three simple cases, two reasons make 
the eigenfunctions a function basis suitable for spectral analysis of manifolds: 1. Because the Laplacian 
is symmetric (< .f,g >=< f, .g>), its eigenfunctions are orthogonal, so it is extremely simple to project 
a function onto this basis, i.e. to apply a Fourier-like transform to the function. 2. For physicists, 
the eigenproblem (Equation 14) is called the Helmoltz equa tion, and its solutions Hk are stationary 
waves. This means that the Hk  v are functions of constant wavelength (or spatial frequency) .k = .k. 
Hence, using the eigenfunctions of the Laplacian to construct a function basis on a manifold is a natural 
way to extend the usual spectral analysis to this manifold. In our case, the manifold is a mesh, so we 
need to port this construction to the discrete setting. The .rst idea that may come to the mind is to 
apply spectral analysis to a discrete Laplacian matrix (e.g. the cotan weights). However, the discrete 
Laplacian is not a symmetric matrix (the denominator of the ai,j coe.cient is the area of vertex i ' 
s neighborhood, that does not necessarily correspond to the area of vertex j s neighborhood). Therefore, 
we lose the symmetry of the Laplacian and the orthogonality of its eigenvectors. This makes it di.cult 
to project functions onto the basis. For this reason, we will clarify the relations between the continuous 
setting (with functions and operators), and the discrete one (with vectors and matrices) in the next 
section. In this section, we present two ways of discretizing the Laplace operator on a mesh. The .rst 
approach is based on Finite Element Modeling (FEM) such as done in [WBH+07], and converge to the continuous 
Laplacian under certain conditions as explained in [HPW06] and [AFW06]. Reuter et al. [RWP05b] also use 
FEM to compute the spectrum (i.e. the eigenvalues) of a mesh, which provides a signature for shape classi.cation. 
The cotan weights were also used in [DBG+06b] to compute an eigenfunction to steer a quad-remeshing process. 
 The cotan weights alone are still dependent on the sampling as shown in Figure 18-B, so they are usually 
weighted by the one ring or dual cell area of each vertex, which makes them loose their symmetry. As 
a consequence, they are improper for spectral analysis (18-C). An empirical symmetrization was pro posed 
in [Lev06] (see Figure 18-D). The FEM formulation enables to preserve the symmetry property of the continuous 
counterpart of the Laplace operator. It is also possible to derive a symmetric Laplacian by using the 
DEC formulation (Discrete Exterior Calculus). Then symmetry is recovered by expressing the operator 
in a proper basis. This ensures that its eigenfunctions are both geometry aware and orthogonal (Figure 
18-E). Note that a recent important proof [WMKG07] states that a perfect discrete Laplacian that satis.es 
all the prop erties of the continuous one cannot exist on general meshes. This explains the large number 
of de.nitions for a discrete Laplacian, depending on the desired properties.  3.1 The FEM Laplacian 
In this subsection, we explain the Finite Element Modeling (FEM) formulation of the discrete Laplacian. 
The Discrete Exterior Calculus (DEC)formululation, presented during the course, is described in the next 
subsection. We have chosen to include in these course nodes the full derivations for the FEM Laplacian 
for the sake of completeness. To setup the .nite element formulation, we .rst need to de.ne a set of 
basis functions used to express the solutions, and a set of test functions onto which the eigenproblem 
(Equation 14) will be projected. As it is often done in FEM, we choose for both basis and test functions 
the same set Fi(i =1 ...n). We use the hat functions (also called P1), that are piecewise-linear on the 
triangles, and that are such that Fi(i) = 1andFi(j) = 0if i = j. Geometrically, Fi corresponds to the 
barycentric coordinate associated with vertex i on each triangle containing i. Solving the .nite element 
formulation of Equation 14 , n relative to the Fi s means looking for functions of the form: Hk = HkFi 
i=1 i which satisfy Equation 14 in projection on the Fj s: .j, < -.Hk , Fj >= .k <Hk , Fj > or in matrix 
form: -Qhk = .Bhk (15) where Qi,j =< .Fi , Fj >, Bi,j =< Fi , Fj > and where hk denotes the vector [H1 
k,...Hk]. The matrix Q is called the sti.ness matrix, and B the mass matrix. n This appendix derives 
the expressions for the coe.cients of the sti.ness matrix Q and the mass matrix B. To do so, we start 
by parameterizing a triangle t =(i, j, k) by the barycentric coordinates (or hat functions) Fi and Fj 
of a point P . t relative to vertices i and j. This allows to write P = k +Fiej - Fj ei (Figure 14). 
This yields an area element dA(P )= ei . ej dFidFj =2|t|dFidFj , where |t| is the area of t, so we get 
the integral: 11-Fi FiFj dA =2|t| FiFjdFidFj = P .t Fi=0 Fj =0 1 121 |t| |t| Fi(1 - Fi)2dFi = |t|- 
+= 23412 Fi=0 which we sum up on the 2 triangles sharing (i, j) to get Bi,j =(|t| + |t ' |/12. We get 
the diagonal terms by: 11-Fi F2 i dA =2|t| F2 i dFj = P .t Fi=0 Fj =0 1 11 |t| 2|t| Fi 2(1 - Fi)dFi 
=2|t|- = 346 Fi=0 which are summed up over the set St(i) of triangles containing i to get Bi,i = , (|t|)/6. 
t.St(i) To compute the coe.cients of the sti.ness matrix Q, we use the fact that d and d are adjoint 
to get the more symmetric expression: Qi,j =< .Fi , Fj >=< ddFi , Fj >=<dFi,dFj >= VFi .VFj S In t, the 
gradients of barycentric coordinates are the constants : . -eei.ej i VFi = VFi .VFj = 2|t| 4|t|2 Where 
e. denotes ei rotated by p/2 around t s normal. By integrating on t we i get: ei.ej ||ei||.||ej|| cos(ij 
) cot(ij) VFi .VFjdA == = 4|t| 2||ei||.||ej|| sin(ij )2 t Summing these expressions, the coe.cients 
of the sti.ness matrix Q are given by: 2 e iQi,i = VFi .VFi = 4|t| t.St(i) t.St(i) = VFi .VFj =1 cot(ij 
) + cot( ' t.tl Qi,j ij 2 Note that this expression is equivalent to the numerator of the classical 
cotan weights. cotan(i,j ) + cotan( ' /2 Qi,j =i,j) , Qi,i = - j Qi,j (16) Bi,j =(|t| + |t ' |) /12 
, Bi,i =(t.St(i) |t|)/6 where t, t ' are the two triangles that share the edge (i, j), |t| and |t ' | 
denote their areas, i,j ,  ' denote the two angles opposite to the edge (i, j) in t and i,j t ' , and 
St(i) denotes the set of triangles incident to i. To simplify the computations, a common practice of 
FEM consists in replacing this equation with an approximation: -Qhk = .Dhkor - D-1Qhk = .hk(17) where 
the mass matrix B is replaced with a diagonal matrix D called the lumped mass matrix, and de.ned by: 
Di,i = Bi,j =( |t|)/3. (18) jt.St(i) Note that D is non-degenerate (as long as mesh triangles have non-zero 
areas). FEM researchers [Pra99] explain that besides simplifying the computations this approximation 
fortuitously improves the accuracy of the result, due to a cancellation of errors, as pointed out in 
[Dye06]. The practical solution mechanism to solve Equation 17 will be explained further in the section 
about numerics. Remark: The matrix D-1Q in (Equation 17) exactly corresponds to the usual discrete Laplacian 
(cotan weights). Hence, in addition to direct derivation of triangle energies [PP93] or averaged di.erential 
values [MDSB03], the discrete Laplacian can be derived from a lumped-mass FEM formulation. As will be 
seen further, the FEM formulation and associated inner product will help us understand why the orthogonality 
of the eigenvectors seems to be lost (since D-1Q is not symmetric), and how to retrieve it. Without entering 
the details, we mention some interesting features and degrees of freedom obtained with the FEM formulation. 
The function basis F onto the eigenfunction problem is projected can be chosen. One can use piecewise 
polynomial functions. Figure 16 shows how the eigenfunctions look like with the usual piecewise linear 
basis (also called P1 in FEM parlance) and degree 3 polynomial basis (P3). Degree 3 triangles are de.ned 
by 10 values (1 value per vertex, two values per edge and a central value). As can be seen, more complex 
function shapes can be obtained (here displayed using a fragment shader). It is also worth mentioning 
that the boundaries can be constrained in two di.erent ways (see Figure 17). With Dirichlet boundary 
conditions, the value of the func tion is constant on the boundary (contour lines are parallel to the 
boundary). With Neumann boundary condistions, the gradient of the eigenfunction is parallel to the boundary 
(therefore, contour lines are orthogonal to the boundary). 3.2 The DEC Laplacian For a complete introduction 
to DEC we refer the reader to [DKT05], [Hir03] and to [AFW06] for proofs of convergence. We quickly introduce 
the few notions and notations that we are using to de.ne the inner product < .,. > and generalized second-order 
derivative (i.e. Laplacian operator). A k-simplex sk is the geometric span of k + 1 points. For instance, 
0, 1 and 2-simplices are points, edges and triangles respectively. In our context, a mesh can be de.ned 
as a 2-dimensional simplicial complex S, i.e. a collection of nk k-simplices (k =0, 1, 2), with some 
conditions to make it manifold. A discrete k-form .k on S is given by a real value .k(sk) associated 
with each oriented ksimplex (that corresponds to the integral of a smooth k-form over the simplex). 
The set Ok(S) of k-forms on S is a vector-space of dimension nk. With a proper numbering of the k-simplices, 
.k can be assimilated to a vector of size nk, and linear operators from Ok(S) to Ol(S) can be assimilated 
to (nk,nl) matrices. The exterior derivative dk :Ok(S) . Ok+1(S) is de.ned by the signed adjacency matrix: 
(dk)sk,sk+1 = 1 if sk belongs to the boundary of sk+1, with the sign depending on their respective orientations. 
 DEC provides Ok(S) with a L2 inner product: <.1 k,.2 k >=(.1 k)T *k .k (19) 2 where *k is the so-called 
Hodge star. As a matrix, the Hodge star is diagonal ** with elements |s |/|sk| where s denotes the circumcentric 
dual of simplex sk, kk and |.| is the simplex volume. In particular, for vertices, edges and triangles: 
|e *| (*0)vv = |v * | ;(*1)ee = = cot e + cot  ' ;(*2)tt = |t|-1 e |e| where e and  ' denote the 
two angles opposite to e. e The codi.erential dk :Ok(S) . Ok-1(S) is the adjoint of the exterior derivative 
for the inner product: <dk.1 k,.k-1 >=<.1 k,dk-1.k-1 > 22 which yields dk = - *-1 dT *k. k-1 k-1Finally 
the Laplace de Rham operator on k-forms is given by: .k = dk-1dk + dk+1dk. In this paper, we are only 
interested in the Laplacian . = .0, i.e. the Laplace de Rham operator for 0-forms . Since d0 and d2 are 
unde.ned (zero by convention), . = d1d0 01 *1 d0 and its coe.cients are given by: = - *-1 dT cot(ij) 
+ cot( ' ).ij = -ij ;.ii = - .ij * |v | i j For surfaces with borders, if the edge ij is on the border, 
the term cot( ' ) ij * vanishes and the dual cell v is cropped by the border. This matches the FEM i 
formulation with Neumann boundary conditions (not detailed here). Figure 17: Contour lines of eigenfunctions 
obtained with Dirichlet boundary conditions (left) and Neumann boundary conditions (right) ij Weighted 
cotan (cot(ij)+cot( ' ))/Ai is not symmetric which does not allow for ij correct reconstruction (C). 
Only symmetric weights (cot(ij )+cot( ' AiAj ij ))/ is fully mesh-independent (E). Remark: The matrix 
. corresponds to the standard discrete Laplacian, except for the sign. The sign di.erence comes from 
the distinction between Laplace-Beltrami and Laplace de Rham operators. The so-de.ned Laplacian . apparently 
looses the symmetry of its continuous counterpart (.ij =.ji). This makes the eigenfunction basis no 
longer orthonormal, which is problematic for our spectral processing purposes (Figure 18-C). To recover 
symmetry, consider the canonical basis (fi) of 0-forms: fi = 1 on vertex i and fi = 0 on other vertices. 
This basis is orthogonal but not normal with respect to the inner product de.ned in Equation 19 (<fi,fi 
>=(fi)T * 0 fi = 1). However, since the Hodge star * 0 is a diagonal matrix, one can easily normalize 
(fi) as follows: -1/2  = * fi 0 fi  In this orthonormalized basis ( fi), the Laplacian . is symmetric, 
and its coef.cients are given by: cot ij + cot  ' -1/21/2 ij  .= *.*;.ij = - -(20) 00 ** |v ||v 
| ij Computing eigenfunctions Now that we have seen two equivalent discretizations of the Laplacien, 
namely FEM (Finite Elements Modeling) and DEC (Discrete Exterior Calculus), we will now focus on the 
problem of computing the eigenfunctions in practice. An implementation of the algorithm is available 
from the WIKI of the course (see web reference at the beginning of this document). Computing the eigenfunctions 
means solving for the eigenvalues .k and  eigenvectors Hk for the symmetric positive semi-de.nite matrix 
.: .Hk Hk = .k However, eigenvalues and eigenvectors computations are known to be extremely computationally 
intensive. To reduce the computation time, Karni et al. [KG00c] partition the mesh into smaller charts, 
and [DBG+06b] use mul tiresolution techniques. In our case, we need to compute multiple eigenvectors 
(typically a few thousands). This is known to be currently impossible for meshes with more than a few 
thousand vertices [WK05]. In this section, we show how this limit can be overcome by several orders of 
magnitude. To compute the solutions of a large sparse eigenproblems, several iterative algorithms exist. 
The publicly available library ARPACK (used in [DBG+06b]) provides an e.cient implementation of the Arnoldi 
method. Yet, two characteristics of eigenproblem solvers hinder us from using them directly to compute 
the MHB for surfaces with more than a few thousand vertices: .rst of all, we are interested in the lower 
frequencies, i.e. eigenvectors with associated eigenvalues lower than .2 . Unfortunately, iterative solvers 
m performs much better for the other end of the spectrum. This can be explained in terms of .ltering 
as lower frequencies correspond to higher powers of the smoothing kernel, which may have a poor condition 
number; secondly, we need to compute a large number of eigenvectors (typically a thousand), and it is 
well known that computation time is superlinear in the number of requested eigenpairs. In addition, if 
the surface is large (millions of vertices), the MHB does not .t in system RAM. 4.1 Band-by-band computation 
of the MHB We address both issues by applying spectral transforms to the eigenproblem. To get the eigenvectors 
of a spectral band centered around a value .S , we start  by shifting the spectrum by .S, by replacing 
. with . - .SId. Then, we can swap the spectrum by inverting this matrix in .SI = (.- .SId)-1 . This 
is called the Shift-Invert spectral transform, and the new eigenproblem to solve is given by: .SI Hk 
Hk = k  It is easy to check that . and .SI have the same eigenvectors, and that their eigenvalues 
are related by .k = .S +1/k. Applying an iterative solver to .SI will return the high end of the spectrum 
(largest  s), corresponding to a band  of eigenvalues of . centered around .S . It is then possible 
to split the MHB computation into multiple bands, and obtain a computation time that is linear in the 
number of computed eigenpairs. In addition, if the MHB does not .t in RAM, each band can be streamed 
into a .le. This gives the following algorithm: (1) .S . 0; .last . 0 (2) while(.last <.2 )  m (3) 
compute an inverse .SI of ( .- .S Id) (4) .nd the 50 .rst eigenpairs ( Hk ,k) of .SI (5) for k =1 
to 50 (6) .k . .S +1/k (7) if (.k >.last) write(Hk,.k)  (8) end //for (9) .S . max(.k)+0.4(max(.k) 
- min(.k)) (10) .last . max(.k)  (11) end //while Before calling the eigen solver, we pre-compute .SI 
with a sparse direct  solver (Line 3). Note that . - .SId may be singular. This is not a problem since 
the spectral transform still works with an inde.nite factorization. The factorized .- .SId is used in 
the inner loop of the eigen solver (Line 4). To factorize .-.SId, we used sparse direct solvers (TAUCS, 
SUPERLU). For large models (millions of vertices), we used the sparse OOC (out-of-core) symmetric inde.nite 
factorization [MIT06] implemented in the future release of TAUCS, kindly provided by S. Toledo. We then 
recover the . s from the  s (Line 6) and stream-write the new eigenpairs into a .le (Line 7). Since 
the eigenvalues are centered around the shift .S, the shift for the next band is given by the last computed 
eigenvalue plus slightly less than half the bandwidth to ensure that the bands overlap and that we are 
not missing any eigenvalue (Line 9). If the bands do not overlap, we recompute a larger band until they 
do. Note that this is di.erent from the shift-invert spectral transform implemented by ARPACK, dedicated 
to iterative solvers. Ours makes use of the factorization of the matrix, resulting in much better performances. 
5 Applications The eigendecomposition of a discrete mesh Laplace operator provides a set of eigenvalues 
and eigenvectors, which can be directly used by an application to accomplish di.erent tasks. Moreover, 
the eigenvectors can also be used as a basis onto which a signal de.ned on a triangle mesh is projected. 
The resulting spectral transform coe.cients can be further analyzed or manipulated. Here we discuss a 
subset of the applications which utilize the spectral transform or the eigenvectors of mesh Laplace or 
more general linear mesh operators. 5.1 Use of eigenvectors Eigenvectors are typically used to obtain 
an embedding of the input shape in the spectral domain, which we call a spectral embedding. After obtaining 
the eigendecomposition of a speci.c operator, the coordinates of vertex i in a k-dimensional spectral 
embedding are given by the i-th row of matrix Vk =[v1,..., vk], where v1,..., vk are the .rst k eigenvectors 
from the spectrum (possibly after scaling). Whether the eigenvectors should be in ascending or descending 
order of eigenvalues depends on the operator that is being used. The choice of k also varies between 
applications. For planar mesh parameterization, k is typically 2, while in other applications, a single 
eigenvector possessing desirable properties may be selected for a task. In computer vision and machine 
learning, spectral methods usually employ a di.erent operator, the so-called a.nity matrix [SM00, Wei99]. 
Each entry Wij of an a.nity matrix W represents a numerical relation, the a.nity, between two data points 
i and j, e.g., pixels in an image, vertices in a mesh, or two face models in the context of face recognition. 
Note that the a.nity matrix di.ers from the Laplacian in that a.nities between all data pairs are de.ned. 
Therefore this matrix is not sparse in general. In practice, this non-sparse structure implies more memory 
requirements and more expensive computations. The use of a.nity matrices for spectral mesh processing 
has received wide appeal in computer graphics as well, e.g., for mesh segmentation. 5.1.1 Parameterization 
and remeshing In the context of mesh parameterization, spectral methods have the interesting property 
of connecting local entities in a way that lets a global behavior emerge. This property can be used to 
compute a good base complex [DBG+06a], [HZM+08], or to directly de.ne a parameterization [ZSGS04], [MTAD08], 
as ex plained below. Figure 19 shows that Mullen et.al s spectral parameterization achieves a result 
comparable to ABF++. The interesting point is that it uses a simple and elegant formulation of the problem 
(just compute the Fiedler vector of the LSCM matrix). Besides corresponding to a Fourier basis on meshes 
, eigenfunctions can be used to de.ne parameterizations. For instance, in the context of data analysis, 
LLE (Local Linear Embedding) [RS00b] computes an embedding of a graph that preserves metric relations 
in local neighborhoods. Similarly, the MDS method (multidimensional scaling) [You85] computes an embedding 
that best approximates all the geodesic distances between the vertices of a graph. Multidimensional 
scaling simply minimizes an objective function that measures the deviation between the geodesic distances 
in the initial space and the Euclidean distances in the embedding space (GDD for Geodesic Distance Deviation), 
by computing the eigenvectors of the matrix D =(di,j ) where di,j denotes the geodesic distance between 
vertex i and vertex j. Isomaps and Multidimensional scaling were used to de.ne parameterization algorithms 
in [ZKK02], and more recently in the ISO-charts method [ZSGS04]. Spectral analysis also has the interesting 
property of de.ning an orthogonal basis. This property can be used to avoid the degenerate con.gurations 
encountered by linear conformal parameterization methods [LPRM02], [DMA02]. Those linear methods use 
vertex pinning to avoid the trivial constant solution, whereas the spectral method introduced in [MTAD08] 
computes the .rst solution orthogonal to the trivial one, that is to say, the eigenvector associated 
with the .rst non-zero eigenvalue. However, the spectral parameterization methods listed above still 
need to partition the mesh into charts. More recently, Dong et al. used the Laplacian to decompose a 
mesh into quadrilaterals [DBG+05, DBG+06a], in a way that facil itates constructing a globally smooth 
parameterization. As shown in Figure 20, their method .rst computes one eigenfunction of the Laplacian 
(the 38th in this example), then extract the Morse complex of this function, .lters and smoothes the 
Morse complex, and uses it to partition the mesh into quads. These quads are parameterized, and inter-chart 
smoothness can be further optimized using global relaxation [KLS03, THCM04]. More recently, a generalization 
was proposed [HZM+08], still based on Laplacian eigenfunctions, and steered by a guidance vector .eld. 
 5.1.2 Clustering and segmentation One of the most well-known techniques in this regard is spectral clustering 
[BN03, KVV00, NJW02]. Interested readers should refer to the recent survey by von Luxburg [vL06] and 
the comparative study by Verma and Meil.a [VM03]. Ng et al. [NJW02] presented a clustering method where 
the entries of the .rst k eigenvectors corresponding to the largest eigenvalues of an a.nity matrix 
are used to obtain the transformed coordinates of the input data points. Additionally, the embedded points 
are projected onto the unit k-sphere. Points that possess high a.nities tend to be grouped together in 
the spectral domain, where a simple clustering algorithm, such as k-means, can reveal the .nal clusters. 
Other approaches di.er only slightly from the core solution paradigm, e.g., in terms of the operator 
used and the dimensionality of the embedding. The ubiquity of the clustering problem makes spectral clustering 
an extremely useful technique. Kolluri et al. [KSO04] used spectral clustering to determine the inside 
and outside tetrahedra in the context of a Delaunay-based surface reconstruction algorithm. Liu and Zhang 
[LZ04] performed mesh seg mentation via spectral clustering. Basically, an a.nity matrix is constructed 
where the a.nity measure depends on both geodesic distances and curvature information. Next, the eigenvectors 
given by the eigendecomposition of this matrix guide a clustering method, which provides patches of faces 
that de.ne the di.erent segments of the mesh returned by the segmentation algorithm. It is shown by example 
that it can be advantageous to perform segmentation in the spectral domain, e.g., in terms of higher-quality 
cut boundaries. In a follow-up work, Zhang and Liu [ZL05] presented a mesh segmentation approach based 
on a recursive 2-way spectral clustering method. Only the .rst two largest eigenvectors of the a.nity 
matrix are computed. This provides a one-dimensional embedding of mesh faces given by the quotient of 
the entries of the two eigenvectors. The most salient cut in this embedding is located, given by a part 
salience measure [HS97]. The cut provides a segmentation of the faces into two parts. This process is 
recursively repeated in order to obtain a hierarchical binary partitioning of the input mesh. In yet 
another follow-up work, Liu and Zhang [LZ07] proposed an algorithm for mesh segmentation via recursive 
bisection where at each step, a sub-mesh embedded in 3D is spectrally projected to 2D and then a contour 
is extracted from the planar embedding. Two operators are used in combination to compute the projection: 
the well-known graph Laplacian and a geometric operator designed to emphasize concavity. The two embeddings 
reveal distinctive shape semantics of the 3D model and complement each other in capturing the structural 
or geometrical aspects of a segmentation. Transforming the shape analysis problem to the 2D domain also 
facilitates segmentability analysis and sampling, where the latter is needed to identify two samples 
residing on di.erent parts of the sub-mesh. These two samples are used by the Nystrom method in the 
construction of a 1D face sequence for .nding an optimal cut, as in [ZL05]. Similar to the technique 
presented in Section 4, the Nystrom method is designed to ef.ciently compute eigenvectors of large 
matrices. However, it relies on sampling and extrapolation and only approximates the eigenvectors. Recently, 
de Goes et al. [dGGV08] presented a hierarchical segmentation method for articulated bodies. Their approach 
relies on the di.usion distance, which is a multi-scale metric based on the heat kernel and computed 
from the eigenvectors of the Laplace-Beltrami operator. The di.usion distance is used to compute a bijection 
between medial structures and segments of the model. The medial structures yield a means to further re.ne 
the segmentation in an iterative manner and provide a full hierarchy of segments for the shape. Huang 
et al. [HWAG09] also performed hierarchical shape decomposition via spectral analysis. However, the operator 
they use encapsulates shape geometry beyond the static setting. The idea is to de.ne a certain deformation 
energy and use the eigenvectors of the Hessian of the deformation energy to characterize the space of 
possible deformations of a given shape. The eigenmodes corresponding to the low-end of the spectrum of 
the Hessian capture low-energy or in their formulation, more rigid deformations, called typical deformations. 
The optimal shape decomposition they compute is one whose optimal articulated (piecewise rigid) deformation 
de.ned on the parts of the decomposition best conforms to the basis vectors of the space of typical deformations. 
As a result, their method tends to identify parts of a shape which would likely remain rigid during the 
typical deformations. 5.1.3 Shape correspondence and retrieval Depending on the requirement of the problem 
at hand, the mesh operator we use to compute a spectral embedding can be made to incorporate any intrinsic 
measure on a shape in order to obtain useful invariance properties, e.g., with respect to part articulation 
or bending. In Figure 21, we show 3D spectral embeddings of a few human and hand models obtained from 
an operator derived from geodesic distances over the mesh surfaces. As geodesic distance is bendingtolerant, 
the resulting embeddings are normalized with respect to bending and can facilitate shape retrieval under 
part articulation [EK03, JZ07]. Elad and Kimmel [EK03] used MDS to compute bending-invariant signa 
tures for meshes. Geodesic distances between mesh vertices are computed by fast marching. The resulting 
spectral embedding e.ectively normalizes the mesh shapes with respect to translation, rotation, as well 
as bending transformations. The similarity between two shapes is then given by the Euclidean distance 
between the moments of the .rst few eigenvectors, usually less than 15, and these similarity distances 
can be used for shape classi.cation. Inspired by works in computer vision on spectral point correspondence 
[SB92], Jain and Zhang [JZvK07] relied on higher-dimensional embeddings based on the eigenvectors of 
an a.nity matrix to obtain point correspondence between two mesh shapes. The .rst k eigenvectors of the 
a.nity matrix encoding the geodesic distances between pairs of vertices are used to embed the model in 
a k-dimensional space; typically k = 5 or 6. After this process is performed for Figure 22: Eigenvector 
plots for two similar shapes, both with 252 vertices. The entries in an eigenvector are color-mapped. 
As we can see, there is an eigenvector switching occurring between the .fth and sixth eigenvectors. Such 
switching is di.cult to detect from the magnitude of the eigenvalues. The .rst 8 eigenvalues for the 
two shapes are [205.6, 11.4, 4.7, 3.8, 1.8, 0.4, 0.26, 0.1] and [201.9, 10.9, 6.3, 3.4, 1.8, 1.2, 0.31, 
0.25], respectively.  two models, the two embeddings are non-rigidly aligned via thin-plate splines 
and the correspondence between the two shapes is given by the proximity of the vertices after such alignment. 
Any measure for the cost of a correspondence, e.g., sum of distances between corresponding vertices, 
can be used as a similarity distance for shape retrieval. One of the key observations made in [JZvK07] 
is the presence of eigen vector switching due to non-uniform scaling of the shapes. Speci.cally, the 
eigenmodes of similar shapes do not line up with respect to the magnitude of their corresponding eigenvalues, 
as illustrated in Figure 22. As a result, it is unreliable to sort the eigenvectors according to the 
magnitude of their respective eigenvalues, as has been done in all works on spectral correspondence 
so far. Jain and Zhang [JZvK07] relied on a heuristic to unswitch the eigenmodes and thin-plate splines 
to further align the shapes in the spectral domain [JZvK07]. Recent work of Mateus et al. [MCBH07] addressed 
the issue using an alignment by the Expectation-Maximization (EM) algorithm instead. The method by Leordeanu 
and Hebert [LH05] focuses on the global char acteristics of correspondence computation and aims at .nding 
consistent correspondences between two sets of shape or image features, where the spectral approach 
has also found its utility. They build a graph whose nodes represent possible feature pairings and edge 
weights measure how agreeable the pairings are. The principal eigenvector of an a.nity matrix W , one 
corresponding to the largest eigenvalue, is inspected to detect how strongly the graph nodes belong to 
the main cluster of W . The idea is that correct correspondences are likely to establish links among 
each other and thus form a strongly connected cluster. To de.ne informative shape descriptors, another 
possibility consists in using the heat di.usion equation and the heat kernel. Heat di.usion is governed 
by the following di.erential equation : .f = k.f .t where k is a constant. This equation admits a solution 
that can be written as : f(t, x)= K(t, y, x)dy where K(t, x, y) denotes the heat kernel. Intuitively, 
considering that a Dirac of heat was emitted from point x at time t = 0, the heat kernel K(t, x, y) indicates 
the quantity of heat that can be measured at point y after time t. The heat kernel admits an expression 
as an (in.nite) sum of eigenfunctions : 8 K(t, x, y)= e -.itfi(x)fi(y) i=0 Using this expression, the 
idea of using the auto-di.usion function was simultaneously proposed in [SOG] and [GBAL]. The auto-di.usion 
function corresponds to the amount of heat that remains at point x after time t for a given t, in other 
words K(t, x, x). This de.nes a scalar .eld on the surface that provably has good properties to be used 
as a shape descriptor [SOG]. In particular, protrusions correspond to extrema of the function. The value 
of t acts as a smoothing parameter (smaller values of t preserve most details, and higher values of 
t tend to a constant function). Another application of the auto-di.usion function is to compute a Morse-Smale 
complex for shape segmentation [GBAL].  5.1.4 Global intrinsic symmetry detection Ovsjanikov et al. 
[OSG08] propose an approach to detect the intrinsic symmetries of a shape which are invariant to isometry 
preserving transformations. They show that if the shape is embedded into a signature space de.ned by 
the eigenfunctions of the Laplace-Beltrami operator, then the intrinsic symmetries are transformed into 
extrinsic Euclidean symmetries (rotations or re.ections). However, it is possible to restrict the search 
of symmetries only to re.ections, avoiding the search for rotational symmetries, a task that can be hard 
in highdimensional space. This result allows to obtain the intrinsic symmetries by .rst computing the 
eigenvectors of the operator, then embedding the shape into the signature space, and .nally .nding point-to-point 
correspondences between symmetric points. The signature adopted is derived from the GPS embedding of 
Rustamov [Rus07].  5.2 Use of spectral transforms The spectral mesh transforms are closely related 
to the Fourier transform that is the foundation of signal processing theory. Conceivably, any application 
of the classical Fourier transform in signal or image processing has the potential to be realized in 
the mesh setting. In geometry processing, the mesh signal considered is often the embedding function 
that speci.es the 3D coordinates of each vertex. This signal serves to represent mesh geometry. 5.2.1 
Geometry compression Karni and Gotsman [KG00c] proposed an approach to compress the geometry of triangle 
meshes. Firstly, the set of eigenvectors of the Tutte Laplacian is computed. Next, the mesh vertex coordinates 
are projected into the spectral space spanned by the computed eigenvectors. Part of the coe.cients obtained 
by this transformation is eliminated in order to reduce the storage space required for mesh geometry. 
The coe.cients related to the eigenvectors associated to larger eigenvalues are .rstly removed, which 
would correspond to high frequency detail, when following an analogy with Fourier analysis. The main 
drawback of this method is that many eigenvectors need to be computed. Karni and Gotsman propose to partition 
the mesh into smaller sets of vertices. Although that alleviates the problem of computing the eigenvectors 
for large matrices, it still requires a good partitioning of the mesh for the e.ciency of the compression, 
and artifacts along the partition boundaries are evident when higher compression rates are employed. 
 5.2.2 Watermarking Ohbuchi et al. [OTMM01, OMT02] also employed the spectral transform ap proach, but 
to insert watermarks into triangle meshes. In their method, the eigenvectors of the graph Laplacian are 
used as the basis for the projection. After transforming the geometry of the mesh and obtaining the spectral 
coe.cients, a watermark is inserted into the model by modifying coe.cients at the low-frequency end 
of the spectrum. In this way, modi.cations on the geometry of the mesh are well-spread over the model 
and less noticeable than if they were directly added to the vertex coordinates. This method also requires 
the computation of eigenvectors of the Laplace operator, which is prohibitive in the case of large meshes. 
Mesh partitioning is again used to address this problem. 5.2.3 Fourier descriptors 2D Fourier descriptors 
have been quite successful as a means to characterize 2D shapes. Using eigendecomposition with respect 
to the mesh Laplacians, one can compute analogous Fourier-like descriptors to describe mesh geometry. 
However, there have not seen such mesh Fourier descriptors being proposed for shape analysis so far. 
There have been methods, e.g., [VS01], which rely on 3D Fourier descriptors for 3D shape retrieval. In 
this case, the mesh shapes are .rst voxelized and 3D Fourier descriptors are extracted from the resulting 
volumetric data. We suspect that the main di.culties with the use of mesh Fourier descriptors for shape 
matching include computational costs and the fact that when the eigenmodes vary between the two mesh 
shapes to be matched, it becomes doubtful whether their associated eigenspace projections can serve as 
reliable shape descriptors. Also, even when the shapes are very similar, eigenvector switching, as depicted 
in Figure 22, can occur when the eigenvectors are ordered by the magnitude of their eigenvalues. References 
[AFW06] D. N. Arnold, R. S. Falk, and R. Winther. Finite element exterior calculus, homological techniques, 
and applications. Acta Numerica 15, 2006. [Arv95] James Arvo. The Role of Functional Analysis in Global 
Illumination. In P. M. Hanrahan and W. Purgathofer, editors, Rendering Techniques 95 (Proceedings of 
the Sixth Eurographics Workshop on Rendering), pages 115 126, New York, NY, 1995. Springer-Verlag. [BN03] 
M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. Neural 
Computations, 15(6):1373 1396, 2003. [Bra99] Ronald N. Bracewell. The Fourier Transform And Its Applications. 
McGraw-Hill, 1999. [Cip93] Barri Cipra. You can t always hear the shape of a drum. What s Happening in 
the Mathematical Sciences, 1, 1993. [DBG+05] S. Dong, P.-T. Bremer, M. Garland, V. Pascucci, and J. C. 
Hart. Quadrangulating a mesh using laplacian eigenvectors. Technical report, June 2005. [DBG+06a] S. 
Dong, P.-T. Bremer, M. Garland, V. Pascucci, and J. C. Hart. Spectral mesh quadrangulation. ACM Transactions 
on Graphics (SIGGRAPH 2006 special issue), 2006. [DBG+06b] Shen Dong, Peer-Timo Bremer, Michael Garland, 
Valerio Pascucci, and John C. Hart. Spectral surface quadrangulation. In SIG-GRAPH 06: ACM SIGGRAPH 2006 
Papers, pages 1057 1066, New York, NY, USA, 2006. ACM Press. [dGGV08] Fernando de Goes, Siome Goldenstein, 
and Luiz Velho. A hierarchical segmentation of articulated bodies. Computer Graphics Forum (Symposium 
on Geometry Processing), 27(5):1349 1356, 2008. [DKT05] Mathieu Desbrun, Eva Kanzo, and Yiying Tong. 
Discrete di.erential forms for computational modeling. Siggraph 05 course notes on Discrete Di.erential 
Geometry, Chapter 7, 2005. [DMA02] Mathieu Desbrun, Mark Meyer, and Pierre Alliez. Intrinsic parameterizations 
of surface meshes. In Proceedings of Eurographics, pages 209 218, 2002. [dV90] Y. Colin de Verdiere. 
Sur un nouvel invariant des graphes et un critere de planarite. J. of Combinatorial Theory, 50, 1990. 
[Dye06] Ramsey Dyer. Mass weights and the cot operator (personal communication). Technical report, Simon 
Fraser University, CA, 2006. [EK03] A. Elad and R. Kimmel. On bending invariant signatures for surfaces. 
IEEE Trans. Pattern Anal. Mach. Intell., 25(10):1285 1295, 2003. [Fei96] J. Feidman. Computing betti 
numbers via combinatorial laplacians. In Proc. 28th Sympos. Theory Comput., pages 386 391. ACM, 1996. 
[FH04] M. S. Floater and K. Hormann. Surface parameterization: a tutorial and survey. Springer, 2004. 
[Fie73] Miroslav Fiedler. Algebraic connectivity of graphs. Czech. Math. Journal, 23:298 305, 1973. [Fie75] 
Miroslav Fiedler. A property of eigenvectors of nonnegative symmetric matrices and its application to 
graph theory. Czech. Math. Journal, 25:619 633, 1975. [GBAL] Katarzyna Gebal, Andreas Baerentzen, Henrik 
Aanaes, and Rasmus Larsen. Shape analysis using the auto di.usion function. Computer Graphics Forum 
(Proc. of Symp. on Geom. Proc.). [GGS03a] C. Gotsman, X. Gu, and A. She.er. Fundamentals of spherical 
parameterization for 3d meshes. ACM Trans. Graph., 22(3):358 363, 2003. [GGS03b] C. Gotsman, X. Gu, and 
A. She.er. Fundamentals of spherical parameterization for 3d meshes, 2003. [Got03] Craig Gotsman. On 
graph partitioning, spectral analysis, and digital mesh processing. In Shape Modeling International, 
pages 165 174, 2003. [GY02] X. Gu and S.-T. Yau. Computing conformal structures of surfaces. Communications 
in Information and Systems, 2(2):121 146, 2002. [Hir03] Anil Hirani. Discrete exterior calculus. PhD 
thesis, 2003. [HPW06] Klaus Hildebrandt, Konrad Polthier, and Max Wardetzky. On the convergence of metric 
and geometric properties of polyhedral surfaces. Geom Dedicata, 2006. [HS97] D. D. Ho.man and M. Singh. 
Salience of visual parts. Cognition, 63:29 78, 1997. [HWAG09] Qixing Huang, Martin Wicke, Bart Adams, 
and Leonidas J. Guibas. Shape decomposition using modal analysis. 28(2):to appear, 2009. [HZM+08] Jin 
Huang, Muyang Zhang, Jin Ma, Xinguo Liu, Leif Kobbelt, and Hujun Bao. Spectral quadrangulation with orientation 
and alignment control. ACM Transactions on Graphics (SIGGRAPH Asia conf. proc., 2008. [IL05] Martin Isenburg 
and Peter Lindstrom. Streaming meshes. In IEEE Visualization, page 30, 2005. [Jai89] A. K. Jain. Hall, 
1989. Fundamentals of Digital Image Processing. Prentice [JNT] Dmitry Jakobson, Nikolai Nadirashvili, 
and John Toth. Geometric properties of eigenfunctions. [JZ07] Varun Jain and Hao Zhang. A spectral approach 
to shape-based retrieval of articulated 3D models. Computer Aided Design, 39:398 407, 2007. [JZvK07] 
Varun Jain, Hao Zhang, and Oliver van Kaick. Non-rigid spectral correspondence of triangle meshes. International 
Journal on Shape Modeling, 2007. to appear. [Kac66] Mark Kac. Can you hear the shape of Monthly, 73, 
1966. a drum? Amer. Math. [KG00a] Z. Karni and C. Gotsman. Spectral compression of mesh geometry. In 
Proc. of ACM SIGGRAPH, pages 279 286, 2000. [KG00b] Zachi Karni and Craig Gotsman. Spectral compression 
of mesh geometry. In SIGGRAPH, pages 279 286, 2000. [KG00c] Zachi Karni and Craig Gotsman. Spectral compression 
of mesh geometry. In SIGGRAPH 00: Proceedings of the 27th annual conference on Computer graphics and 
interactive techniques, pages 279 286, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co. 
 [KLS03] A. Khodakovsky, N. Litke, and P. Schroder. Globally smooth parameterizations with low distortion. 
ACM TOG (SIGGRAPH), 2003. [Kor02] Y. Koren. On spectral graph drawing, 2002. [Kor03] Y. Koren. On spectral 
graph drawing. In Proc. of the International Computing and Combinatorics Conference, pages 496 508, 2003. 
[KSO04] Ravikrishna Kolluri, Jonathan Richard Shewchuk, and James F. O Brien. Spectral surface reconstruction 
from noisy point clouds. In Proc. of Eurographics Symposium on Geometry Processing, pages 11 21, 2004. 
[KVV00] R. Kannan, S. Vempala, and A. Vetta. On clustering -good, bad, and spectral. In FOCS, pages 367 
377, 2000. [Lev06] Bruno Levy. Laplace-beltrami eigenfunctions: Towards an algorithm that understands 
geometry. In IEEE International Conference on Shape Modeling and Applications, 2006. [LH05] Marius Leordeanu 
and Martial Hebert. A spectral technique for correspondence problems using pairwise constraints. In International 
Conference of Computer Vision (ICCV), volume 2, pages 1482 1489, October 2005. [LPM02] Bruno Levy, Sylvain 
Petitjean, and Nicolas Ray Nicolas Jerome Maillot. Least squares conformal maps for automatic texture 
atlas generation. In ACM, editor, SIGGRAPH conf. proc, 2002. [LPRM02] B. Levy, S. Petitjean, N. Ray, 
and J. Maillot. Least squares conformal maps for automatic texture atlas generation. In Proc. of ACM 
SIGGRAPH 02, pages 362 371, 2002. [LZ04] R. Liu and H. Zhang. Segmentation of 3D meshes through spectral 
clustering. In Paci.c Graphics, pages 298 305, 2004. [LZ07] Rong Liu and Hao Zhang. Mesh segmentation 
via spectral embedding and contour analysis. Computer Graphics Forum (Special Issue of Eurographics 
2007), 26:385 394, 2007. [MCBH07] Diana Mateus, Fabio Cuzzolin, Edmond Boyer, and Radu Horaud. Articulated 
shape matching by robust alignment of embedded representations. In ICCV 07 Workshop on 3D Representation 
for Recognition (3dRR-07), 2007. [MDSB03] Mark Meyer, Mathieu Desbrun, Peter Schroder, and Alan H. Barr. 
Discrete di.erential-geometry operators for triangulated 2manifolds. In Hans-Christian Hege and Konrad 
Polthier, editors, Visualization and Mathematics III, pages 35 57. Springer-Verlag, Heidelberg, 2003. 
 [MIT06] Omer Meshar, Dror Irony, and Sivan Toledo. An out-of-core sparse symmetric inde.nite factorization 
method. ACM Transactions on Mathematical Software, 32:445 471, 2006. [MTAD08] Patrick Mullen, Yiying 
Tong, Pierre Alliez, and Mathieu Desbrun. Spectral conformal parameterization. In ACM/EG Symposium of 
Geometry Processing, 2008. [NJW02] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: analysis 
and an algorithm. In Neural Information Processing Systems, volume 14, pages 849 856, 2002. [OMT02] R. 
Ohbuchi, A. Mukaiyama, and S. Takahashi. A frequency-domain approach to watermarking 3D shapes. Computer 
Graphics Forum, 21(3):373 382, 2002. [OSG08] Maks Ovsjanikov, Jian Sun, and Leonidas Guibas. Global intrinsic 
symmetries of shapes. Computer Graphics Forum (Symposium on Geometry Processing), 27(5):1341 1348, 2008. 
[OTMM01] R. Ohbuchi, S. Takahashi, T. Miyazawa, and A. Mukaiyama. Watermarking 3D polygonal meshes in 
the mesh spectral domain. In Proc. of Graphics Interface, pages 9 18, 2001.  [PP93] Ulrich Pinkall and 
Konrad Polthier. Computing discrete minimal surfaces and their conjugates. Experimental Mathematics, 
2(1), 1993. [Pra99] G. Prathap. Towards a science of fea: Patterns, predictability and proof through 
some case studies. Current Science, 77:1311 1318, 1999. [RS00a] S. T. Roweis and L. K. Saul. Nonlinear 
dimensionality reduction by locally linear embedding. Science, 290:2323 2326, 2000. [RS00b] Sam Roweis 
and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323 
2326, Dec 2000. [Rus07] R. M. Rustamov. Laplace-beltrami eigenfunctions for deformation invariant shape 
representation. In Proc. of Eurographics Symposium on Geometry Processing, pages 225 233, 2007. [RWP05a] 
M. Reuter, F.-E. Wolter, and N. Peinecke. Laplace-beltrami spectra as shape-dna of surfaces and solids. 
CAD Journal, 2005. [RWP05b] Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke. Laplacespectra as 
.ngerprints for shape matching. In SPM 05: Proceedings of the 2005 ACM symposium on Solid and physical 
modeling, pages 101 106, New York, NY, USA, 2005. ACM Press. [SB92] L. S. Shapiro and J. M. Brady. Feature-based 
correspondence: an eigenvector approach. Image and Vision Computing, 10(5):283 288, 1992. [SGD05] P. 
Schroder, E. Grinspun, and M. Desbrun. Discrete di.erential geometry: an applied introduction. In SIGGRAPH 
Course Notes, 2005. [SM00] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE 
Trans. on Pattern Analysis and Machine Intelligence, 22(8):888 905, 2000. [SOG] Jian Sun, Maks Ovsjanikov, 
and Leonidas Guibas. A concise and provably informative multi-scale signature based on heat di.usion. 
Computer Graphics Forum (Proc. of Symp. on Geom. Proc.). [Tau95a] G. Taubin. A signal processing approach 
to fair surface design. In Proc. of ACM SIGGRAPH, pages 351 358, 1995. [Tau95b] Gabriel Taubin. A signal 
processing approach to fair surface design. In SIGGRAPH 95: Proceedings of the 22nd annual conference 
on Computer graphics and interactive techniques, pages 351 358, New York, NY, USA, 1995. ACM Press. [TB97] 
Lloyd N. Trefethen and David Bau. SIAM, 1997. Numerical Linear Algebra. [TdSL00] J. B. Tenenbaum, V. 
de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 
290:2319 2323, 2000. [THCM04] M. Tarini, K. Hormann, P. Cignoni, and C. Montani. Polycubemaps. ACM TOG 
(SIGGRAPH), 2004. [vL06] Ulrike von Luxburg. A tutorial on spectral clustering. Technical Report TR-149, 
Max Plank Institute for Biological Cybernetics, August 2006. [VM03] D. Verma and M. Meila. A comparison 
of spectral clustering algorithms. Technical Report UW-CSE-03-05-01, University of Washington, 2003. 
[VS01] D. V. Vranic and D. Saupe. 3D shape descriptor based on 3D Fourier transform. In Proc. EURASIP 
Conf. on Digital Signal Processing for Multimedia Communications and Services, 2001. [WBH+07] Max Wardetzky, 
Miklos Bergou, David Harmon, Denis Zorin, and Eitan Grinspun. Discrete quadratic curvature energies. 
Computer Aided Geometric Design (CAGD), 2007. [Wei99] Y. Weiss. Segmentation using eigenvectors: A unifying 
view. In Proc. of the International Conference on Computer Vision, pages 975 983, 1999. [WK05] Jianhua 
Wu and Leif Kobbelt. E.cient spectral watermarking of large meshes with orthogonal basis functions. In 
The Visual Computer, 2005. [WMKG07] Max Wardetzky, Saurabh Mathur, Felix Kalberer, and Eitan Grinspun. 
Discrete laplace operators: No free lunch. Eurographics Symposium on Geometry Processing, 2007. [You85] 
F. W. Young. Multidimensional scaling. Encyclopedia of Statistical Sciences, 5:649 658, 1985. [ZKK02] 
Gil Zigelman, Ron Kimmel, and Nahum Kiryati. Texture mapping using surface .attening via multidimensional 
scaling. IEEE Transactions on Visualization and Computer Graphics, 8(2), 2002. [ZL05] H. Zhang and R. 
Liu. Mesh segmentation via recursive and visually salient spectral cuts. In Proc. of Vision, Modeling, 
and Visualization, 2005. [ZSGS04] Kun Zhou, John Snyder, Baining Guo, and Heung-Yeung Shum. Iso-charts: 
Stretch-driven mesh parameterization using spectral analysis. In Symposium on Geometry Processing, pages 
47 56, 2004. [ZvKDar] Hao Zhang, Oliver van Kaick, and Ramsay Dyer. Spectral mesh processing. Computer 
Graphics Forum, 2009, to appear. http: //www.cs.sfu.ca/~haoz/pubs/zhang cgf09 spect survey.pdf.  SIGGRAPH 
2010  Spectral Mesh Processing Bruno Lvy and Richard Hao Zhang Introduction [Levy]  What is so spectral? 
[Zhang]  Intuition and theory behind spectral methods  Different interpretations and motivating applications 
  Do your own Spectral Mesh processing at home [Levy]  DEC Laplacian, numerics of spectral analysis 
 Tutorial on implementation with open source software   Applications -What can we do with it ?" 1/2 
[Zhang] Segmentation, shape retrieval, non-rigid matching, symmetry detect . . .  Applications -What 
can we do with it ?" 2/2 [Levy] Quadrangulation, parameterization, . . .  Wrapup, conclusion, Q&#38;A 
[Zhang and Levy]  ----------------------break ----------------------------  SIGGRAPH 2010 Spectral 
Mesh Processing What is So Spectral ? Richard Hao Zhang Spectral Mesh Processing Hao (Richard) Zhang 
School of Computing Science Simon Fraser University, Canada  Spectral Mesh Processing Hao (Richard) 
Zhang School of Computing Science Simon Fraser University, Canada     ?! Spectral approach: a solution 
paradigm  Spectral approach: a solution paradigm y x x y   Talking about different views The spectral 
approach itself under different views!  Signal processing perspective  Geometric perspective  Machine 
learning . dimensionality reduction  A first example Correspondence problem   A first example Correspondence 
problem   A first example Correspondence problem  A first example Correspondence problem  Spectral 
mesh processing  Eigenstructures Eigenvalues and eigenvectors:  Diagonalization or eigen-decomposition: 
   Projection into eigensubspace:  A transform into eigenspace:  y = UTy  This lecture Overview 
of spectral methods for mesh processing  What is the spectral approach exactly?  Three perspectives 
or views  Motivations  Sample applications   Difficulties and challenges   Overview  A signal 
processing perspective  A very gentle introduction . boredom alert Signal compression and filtering 
  Relation to discrete Fourier transform (DFT)  Geometric perspective: global and intrinsic  Dimensionality 
reduction  Difficulties and challenges  Overview  A signal processing perspective   A very gentle 
introduction . boredom alert Signal compression and filtering  Relation to discrete Fourier transform 
(DFT)  Geometric perspective: global and intrinsic  Dimensionality reduction  Difficulties and challenges 
  operator A  Classification of applications Eigenstructure(s) used  Eigenvalues: signature for 
shape characterization  Eigenvectors: form spectral embedding (a transform)  Eigenprojection: also 
a transform . DFT-like   Dimensionality of spectral embeddings  1D: mesh sequencing  2D or 3D: 
graph drawing or mesh parameterization  Higher D: clustering, segmentation, correspondence   Mesh 
operator used  Combinatorial vs. geometric, 1st-order vs. higher order  Classification of applications 
 Eigenstructure(s) used  Eigenvalues: signature for shape characterization  Eigenvectors: form spectral 
embedding (a transform)  Eigenprojection: also a transform . DFT-like   Dimensionality of spectral 
embeddings  1D: mesh sequencing  2D or 3D: graph drawing or mesh parameterization  Higher D: clustering, 
segmentation, correspondence   Mesh operator used  Combinatorial vs. geometric, 1st-order vs. higher 
order  Classification of applications Eigenstructure(s) used  Eigenvalues: signature for shape characterization 
 Eigenvectors: form spectral embedding (a transform)  Eigenprojection: also a transform . DFT-like 
   Dimensionality of spectral embeddings  1D: mesh sequencing  2D or 3D: graph drawing or mesh parameterization 
 Higher D: clustering, segmentation, correspondence   Mesh operator used  Combinatorial vs. geometric, 
1st-order vs. higher order  Much more details in survey H. Zhang, O. van Kaick, and R. Dyer, Spectral 
Mesh Processing , Computer Graphics Forum (shorter version as Eurographics 2007 STAR), 2009, accepted. 
 Eigenvector (of a combinatorial operator) plots on a sphere  Overview  A signal processing perspective 
  A very gentle introduction . boredom alert Signal compression and filtering  Relation to discrete 
Fourier transform (DFT)  Geometric perspective: global and intrinsic  Dimensionality reduction  Difficulties 
and challenges  The smoothing problem Smooth out rough features of a contour (2D shape)  Midpoint 
smoothing Successive connection of midpoints  Midpoint . Laplacian smoothing Two steps of midpoint 
smoothing  One step   Laplacian smoothing and Laplacian Local averaging  1D discrete Laplacian  
Laplacian smoothing Obtained by 10 steps of Laplacian smoothing   x-coordinates of the seahorse contour 
 Laplacian smoothing in matrix form Smoothing operator x component only y treated same way 1D discrete 
Laplacian operator  Smoothing and Laplacian operator   Spectral or eigen-decomposition Spectral decomposition 
of the underlying linear space  Use of eigenvectors of the Laplacian operator L  L is real and symmetric 
 Real eigenvalues  Real and orthogonal set of eigenvectors  Eigenvectors span the vector space Rn 
   X  X Projection of X Spatial Spectral along eigenvector domain domain Plot of eigenvectors First 
8 eigenvectors of the 1D Laplacian   Relation to DFT Smallest eigenvalue of L is zero  Each remaining 
eigenvalue (except for the last one when n is even) has multiplicity 2  The plotted real eigenvectors 
are not unique to L  One particular set of eigenvectors are the DFT basis  Oscillatory behaviour of 
the two bases are similar   Signal reconstruction and compression  Reconstruction using k leading 
coefficients  A form of compression with information loss given by L2    Eigenvalue plots Fairly 
fast eigenvalue decay   Filtering and Laplacian smoothing Recall the Laplacian smoothing operator 
 A filter applied to spectral coefficients Examples  m = 1 m = 5 m = 10 m = 50 More by Bruno Filter: 
 Aside: other filters   Computational issues No need to compute spectral coefficients for filtering 
Polynomial filters (Laplacian): matrix-vector multiplication  Spectral compression needs explicit spectral 
transform  Efficient computation by Bruno  Towards spectral mesh transform Signal representation 
 Vectors of x, y, z vertex coordinates  Laplacian operator for meshes  Encodes connectivity and geometry 
 Combinatorial: graph Laplacians and variants  Discretization of the continuous Laplace-Beltrami operator 
. coverage by Bruno   The same kind of spectral transform and analysis  Main references  Overview 
 A signal processing perspective A very gentle introduction . boredom alert  Signal compression and 
filtering  Relation to discrete Fourier transform (DFT)   Geometric perspective: global and intrinsic 
 Dimensionality reduction  Difficulties and challenges   A geometric perspective: classical Classical 
Euclidean geometry Primitives not represented in coordinates  Geometric relationships deduced in a 
pure and self-contained manner  Use of axioms   A geometric perspective: analytic Descartes analytic 
geometry Algebraic analysis tools introduced  Primitives referenced in global frame . extrinsic approach 
   Intrinsic approach Riemann s intrinsic view of geometry Geometry viewed purely from the surface 
perspective  Many spaces can be treated simultaneously: isometry  Metric: distance between   Spectral 
methods: intrinsic view Spectral approach takes the intrinsic view Intrinsic geometric/mesh information 
captured via a linear mesh operator  Eigenstructures of the operator present the intrinsic geometric 
information in an organized manner  Never need all the eigenstructures, the dominant ones often suffice 
 Global, by definition  Courant-Fisher Theorem:  where V is a subspace of .n with the given dimension. 
For only the smallest and largest eigenvalues, Corollary to Courant-Fisher Let S ..n . n be a symmetric 
matrix. Then its eigenvalues ..2 . . ..must satisfy the following, .1 n where v1, v2, , vi 1 are eigenvectors 
of S corresponding to the smallest eigenvalues .1, .2 , , .i 1, respectively. Interpretation  Smallest 
eigenvector minimizes the Rayleigh quotient  k-th smallest eigenvector minimizes Rayleigh quotient, 
among the vectors orthogonal to all previous eigenvectors  Solutions to global optimization problems 
  Capture of global information Eigenvalues  Spectral graph theory: graph eigenvalues closely related 
to almost all major global graph invariants  Often adopted as compact global shape descriptors   Eigenvectors 
 Useful extremal properties, e.g., heuristic for NP-hard problems . normalized cuts and sequencing 
 Spectral embeddings capture global information   Example: clustering problem  Example: clustering 
problem  Spectral clustering Encode information about   Leading eigenvectors      Perform any 
clustering eigenvectors In spectral domain (e.g., k-means) in spectral domain   Local vs. global distances 
 A good distance: Points in same cluster closer in transformed domain  Look at set of shortest paths 
. more global  Commute time distance cij =  Would be nice to cluster expected time for random walk 
to according to cij go from i to j and then back to i Local vs. global distances   Commute time and 
spectral Consider eigen-decompose the graph Laplacian K K = U.UT  Let K be the generalized inverse 
of K,  K = U. UT, = 1/.ii if .ii . 0, otherwise . ii = 0. . ii Note: the Laplacian is singular  
Commute time and spectral Let zi be the i-th row of U. 1/2 . the spectral embedding  Essentially scaling 
each eigenvector by the inverse square root of its eigenvalue  Then  ||zi zj||2 = cij the communite 
time distance [Klein &#38; Randic 93, Fouss et al. 06] Full set of eigenvectors used, but select first 
k in practice  Main references   Overview  A signal processing perspective  A very gentle introduction 
. boredom alert Signal compression and filtering  Relation to discrete Fourier transform (DFT)  
Geometric perspective: global and intrinsic  Dimensionality reduction  Difficulties and challenges 
 Spectral embedding  Spectral decomposition  Full spectral embedding given by scaled eigenvectors 
(each scaled by squared root of eigenvalue) completely captures the operator   Dimensionality reduction 
 Full spectral embedding is high-dimensional  Use few dominant eigenvectors dimensionality reduction 
 Information-preserving  Structure enhancement (Polarization Theorem)  Low-D representation: simplifying 
solutions  .    Eckard &#38; Young: Info-preserving A ..n . n : symmetric and positive semi-definite 
 U(k) ..n . k : leading eigenvectors of A, scaled by square root of eigenvalues  Then U(k)U(k)T is 
the best rank-k approximation of  A in Frobenius norm   Mesh projected into the eigenspace formed 
by the first two eigenvectors of a mesh Laplacian  Reduce 3D analysis to contour analysis [Liu &#38; 
Zhang 07]  Main references   Overview  A signal processing perspective  A very gentle introduction 
. boredom alert Signal compression and filtering  Relation to discrete Fourier transform (DFT)  
Geometric perspective: global and intrinsic  Dimensionality reduction  Difficulties and challenges 
  Not quite DFT Basis for DFT is fixed given n, e.g., regular and easy to compare (Fourier descriptors) 
 Spectral mesh transform operator-dependent  Different behavior of eigenfunctions on the same sphere 
Which operator to use?  No free lunch No mesh Laplacian on general meshes can satisfy a list of all 
desirable properties  Remedy: use nice meshes . Delaunay or non-obtuse    Delaunay but obtuse Non-obtuse 
  No DFT again Computational issues: FFT vs. eigen-decomposition  Regularity of vibration patterns 
lost  Difficult to characterize eigenvectors, eigenvalue not enough  Non-trivial to compare two sets 
of eigenvectors . how to pair up?   More in the afternoon Main references  Conclusion Spectral mesh 
processing Use eigenstructures of appropriately defined linear mesh operators for geometry analysis 
and processing Solve a problem in a different domain via a transform . spectral transform Fourier analysis 
on meshes Captures global and intrinsic shape characteristics Dimensionality reduction: effective and 
simplifying  SIGGRAPH 2010 Spectral Mesh Processing Bruno Lvy and Richard Hao Zhang Bruno Lvy INRIA 
-ALICE Introduction I. Harmonics II. DEC formulation III. Filtering IV. Numerics   Introduction Fourier 
transform =S f sin(kx)  Introduction Filtering x =   Introduction Filtering Filtering Convolution 
Fourier Fourier Transform Geometric space Transform Frequency space  x   Introduction Filtering on 
a mesh Filtering [Taubin 95] Geometric space Frequency space  ? x ?  Introduction Filtering on a mesh 
 Filtering [Taubin 95] Geometric space Frequency space  ? x ?  Introduction Filtering on a mesh Filtering 
[Taubin 95] Geometric space Frequency space [Karni00] mesh compression [Zhang06] shape matching ?? [Reuter03] 
shape signatures [Dong06] quadrangulation I Harmonics Introduction I. Harmonics II. DEC formulation 
III. Filtering IV. Numerics Results and conclusion  I Harmonics Question on ? sin(kx)  I Harmonics 
Harmonics and vibrations   sin(kx) are the stationary vibrating modes = harmonics of a string   I 
Harmonics Manifold Harmonics  Harmonics Harmonics ? I Harmonics Square Harmonics Harmonics  Harmonics 
 ?  I Harmonics Chladni plates   I Harmonics Chladni plates  I Harmonics Chladni plates  I Harmonics 
Chladni plates and jpeg   I Harmonics Spherical Harmonics Harmonics    Harmonics   I Harmonics 
 Manifold Harmonics  Harmonics Harmonics Stationary vibrating modes Harmonics and vibrations Wave 
equation: T .y/.x =  .y/.t T: stiffness : mass Stationary modes: y(x,t) = y(x)sin(.t) .y/.x 
= -./T y eigenfunctions of ./.x    I Harmonics I Harmonics : recap Harmonics are eigenfunctions 
of ./.x  On a mesh, ./.x is the Laplacian .  We need the eigenfunctions of .  Several possible 
discretizations:  Finite Element Modeling [Martin Reuter]  Discrete Exterior Calculus (this talk) 
  II DEC formulation Introduction I. Harmonics II. DEC formulation III. Filtering IV. Numerics Results 
and conclusion   II DEC formulation Discrete Exterior Calculus Discretize equations on a mesh  Simple 
  Rigorous [Harrisson], [Mercat], [Hirani], [Arnold], [Desbrun] Based on k-forms   II DEC formulation 
k-forms mesh dual mesh   II DEC formulation 0-forms  II DEC formulation 1-forms  II DEC formulation 
2-forms  II DEC formulation dual 0-forms  II DEC formulation dual 1-forms  II DEC formulation dual 
2-forms  II DEC formulation Hodge star *0 from to term 0-forms dual 2-forms |*i|  mesh dual mesh 
II DEC formulation Hodge star *1 from to term 1-forms dual 1-forms |*ij|/|ij| = cot()+cot( )  i 
 *ij j mesh dual mesh   II DEC formulation Exterior derivative d dijkl f ij -1 +10 0 fi jk 0 -1 +1 
0 fj ki +10 -1 0 fk il -1 0 0+1 fl lj 0+10 -1 j  from to term 0-forms 1-forms df (ij) = fi -fj 
   II DEC formulation DEC Laplacian *0 -1 *1 dT In DEC the Laplacian is d 0-form (function) f   II 
DEC formulation DEC Laplacian *0 -1 *1 dT In DEC the Laplacian is d 0-form (function) f d (fj-fi)  
II DEC formulation DEC Laplacian *0 -1 *1 dT In DEC the Laplacian is d 0-form (function) f d (cot()+cot( 
))(fj-fi) *1 dual 1-form (cogradient) *df  II DEC formulation DEC Laplacian *0 -1 *1 dT In DEC the 
Laplacian is d 0-form (function) f d S(cot()+cot( ))(fj-fi) *1  II DEC formulation DEC Laplacian 
*0 -1 *1 dT In DEC the Laplacian is d 0-form (function) f 0-form (pointwise laplacian)  d -1d*df * 
S*i(cot()+cot( )) (fj-fi) *0 -1 *1|*i|  II DEC formulation DEC Laplacian    II Two words on FEM 
Boundary conditions   II DEC formulation Manifold Harmonics Basis Eigenfunctions of +1 operator . 
FEM or DEC Eigenvectors of -1 matrix L   II DEC formulation II DEC formulation : recap on = sin(kx) 
 III Filtering Introduction I. Harmonics II. DEC formulation III. Filtering IV. Numerics Results and 
conclusion III Filtering   ?  III Filtering Spectral Filtering  The Manifold Harmonics Hk come with 
an eigenvalue .k  The .k= .k2 is a squared spatial frequency  A filter is a transfer function F(.) 
  Filtering Geometric space f i MHT MHT-1 Frequency space  III Filtering Color Filtering   III Filtering 
Geometry Filtering -DEMO Take f =(x,y,z) IV Numerics Introduction I. Harmonics II. DEC formulation 
 III. Filtering IV. Numerics IV Numerics Eigenvalues Compute the eigenpairs (Hk, .k) of L = *0  Solver 
returns eigenvectors of highest eigenvalue   Problems: (1) We want smallest .k  (2) We want more 
than 50   IV Numerics Eigenvalues Problem #1: Eigensolver returns highest eigenvalues, we want smallest 
Use invert spectral transform  L Hk = .k Hk -1 Hk  L-1 Hk = .k  We do not need to invert L, factorizing 
it is sufficient. Each time the eigensolver queries x = Lv, solve for x in Lx = v instead  IV Numerics 
Eigenvalues Problem #2: We want more than 50 eigenpairs Use shift spectral transform  L Hk = .k Hk 
 (L .s Id) Hk = ( .k .s )Hk  We do not need to invert L, factorizing it is sufficient. Each time the 
eigensolver queries x = Lv, solve for x in Lx = v instead  IV Numerics Combining: Shift Invert L. (L 
.s Id)-1  same Hk  shift .  .k 0 L  IV Numerics Eigen solver Compute a band of eigenpairs (Hk,.k) 
around . Sparse indefinite Cholesky factorization + backward substitution v [Meshar &#38; Toledo] x 
 IV Numerics Band by band algorithm Compute the eigenpairs (hk  0  IV Numerics Band by band algorithm 
Compute the eigenpairs (hk,.k) of L band by band .0=0 s  IV Numerics Band by band algorithm Compute 
the eigenpairs (hk,.k) of L band by band 1 .s0=0 .s  IV Numerics Band by band algorithm Compute the 
eigenpairs (hk,.k) of L band by band .0=0 .1 .2 s ss  IV Numerics Band by band algorithm Compute the 
eigenpairs (hk,.k) of L band by band 12 3 .0=0 ... sss s  IV Numerics Band by band algorithm Compute 
the eigenpairs (hk,.k) of L band by band 12 34 .0=0 .... sss ss s Factorize (L .s Id) using a sparse 
direct solver (TAUCS, SuperLU )  Each time the eigensolver queries a matrix  vector product x = Mv, 
solve Lx -.sx = v instead  IV Numerics Eigen solver Main loop: ARPACK (Arnoldi solver) Matrix factorization: 
SuperLU or TAUCS Full Implementation: http://alice.loria.fr/software/graphite http://alice.loria.fr/WIKI/index.php/Graphite/SpectralMeshProcessing 
 Sparse indefinite Cholesky factorization + backward substitution v [Meshar &#38; Toledo] x  IV Numerics 
  SIGGRAPH 2010 Spectral Mesh Processing Bruno Lvy and Richard Hao Zhang Spectral Mesh Processing 
 Hao Zhang School of Computing Science Simon Fraser University, Canada  Spectral mesh processing  Use 
eigenstructures of appropriately defined linear mesh operators for geometry analysis and processing Solve 
a problem in a different domain via a transform spectral transform Fourier analysis on meshes Captures 
global and intrinsic shape characteristics Dimensionality reduction: effective and simplifying  Mesh 
segmentation  Non-rigid correspondence  Shape retrieval  Global intrinsic symmetry detection  All 
utilize spectral embeddings in some way  Spectral embedding   Mesh segmentation  Non-rigid correspondence 
 Shape retrieval  Global intrinsic symmetry detection  Mesh segmentation Lots of recent work; survey 
by [Shamir 08]  Focus on meaningful segmentation    Why spectral? Captures global information; 
structures better revealed in spectral domain (local info) spectral  domain  Spectral clustering 
 Key issues Distance definition  Fundamental to most shape analysis tasks  Ideal: short distance if 
elements in same part  But not clear what a part is   Key issues  Distance definition  Fundamental 
to most shape analysis tasks  Ideal: short distance if elements in same part  But not clear what a 
part is   Computation of spectral embeddings  Efficiency: Nystrm approximation Key issues  Distance 
definition  Fundamental to most shape analysis tasks  Ideal: short distance if elements in same part 
 But not clear what a part is   Computation of spectral embeddings Efficiency: Nystrm approximation 
  How to cluster, depending on DIM of embeddings  Higher-D: k-means most typical (easier in spectral) 
 1D: linear search with hard-to-optimize energy   Key issues   Computation of spectral embeddings 
 Efficiency: Nystrm approximation  How to cluster, depending on DIM of embeddings  Higher-D: k-means 
most typical (easier in spectral)  1D: linear search with hard-to-optimize energy   Euclidean  Geodesic 
  Isophotic or angular [Pottmann 04, Katz 03] Diffusion distance [deGoes 08, Coifman 06]  Diffusion 
and commute time distances oth model connected-ness between points More global sense than geodesics 
 Consider more paths between two points   Diffusion distance  Defined by a time/scale parameter t 
 Consider only paths of length t or less   Commute time distance: consider paths of all lengths  
Both are still surface-based distances   Most typical: geodesic &#38; angular  Geodesic distance (approximate) 
 Distant faces tend to belong to  Gestalt law of proximity   Angular distance  Faces separated by 
concave parts  The Minima rule   Geodesic distance: insensitive to parts  Geodesic &#38; angular 
   Missing: connect to volumes  Shape Diameter Function Not a metric [Shapira et al. 08]  Volumetric 
view Look at the object from inside   Volumetric view Look at the object from inside and define a 
metric  Use of visibility Intuition: use of visibility information   Reference points  Need a surface 
metric, but visibility from surface unstable  Connect to surface: find reference points  Reference 
points  Need a surface metric, but visibility from surface unstable  Connect to surface: find reference 
points    Volumetric Shape Images (VSI) Sample visible regions from ref. points VSI difference Compute 
VSI difference   Difference is based on the reach of local volume along   Moving along a path on 
the surface  Moving along a path on the surface  Moving along a path on the surface  Moving along 
a path on the surface  Moving along a path on the surface No leakage problem  VSI distance fields 
  A part-aware metric Metric: graph distance on a combined weighted geodesic graph angular graph graph 
 edge weight  edge weight normalization normalization =     combined graph  Placed into an algorithm 
 Early method of [Liu &#38; Zhang 04]  G n*n W n*n Any metric  distance matrix affinity matrix (positive 
sdf) Spectral embedding by eigen-decomposing the matrix  Apply k-means in spectral domain   Segmentation 
results     Computational issues  Spectral embedding requires eigenvectors  Expensive with large 
datasets (dense meshes)  One possible solution: Nystrm approximation  Sub-sampling of dense data 
 Solve small-scale spectral decomposition problem  Extrapolate results to full eigenvectors   Nystrm 
approximation Basic steps (input matrix: W n):  Select k samples, k << n W =  Compute submatrices 
A and B A: affinities between samples B: affinities between samples and non-samples  Eigendecompose 
A = U   Extrapolate eigenvectors of W based on B, U, and  A B BT C   Nystrm approximation continued 
 Complexity: O(n3) to O(mk2+ k3), k << n in practice  Only approximations however, e.g.,   W = not 
orthogonal in general Quality depends on sampling Only limited studies on better approximation to C 
 Dimensionality of embeddings  Typical k-means: a handful of dimensions  Low dimensional embeddings, 
e.g., 1D or 2D  Large distortions, but possibly feature-exaggerating  Greatly simplifying certain 
analysis tasks, e.g., search    1D or 2D embeddings  1D embedding  Linear search possible  Can 
deal with hard-to-optimize energy, e.g., part salience [Zhang &#38; Liu 05]   2D embedding  3D analysis 
turned into contour analysis  Facilitate definition of segmentability measure and sampling for Nystrm 
[Liu &#38; Zhang 07]    Main references R. Liu, H. Zhang, A. Shamir, and D. Cohen-Or, ! Part-Aware 
Surface Metric for Shape Processing , Eurographics 2009 !; Shamir, ! Survey on Mesh Segmentation Techniques 
, Computer Graphics Forum (Eurographics STAR 2006), 2008. F. de Goes, Siome Goldenstein, and Luiz Velho, 
! hierarchical Segmentation of !rticulated odies , SGP 2008. R; Liu and H; Zhang, Spectral lustering 
for Mesh Segmentation , Pacific Graphics 2004. R; Liu and H; Zhang, Mesh Segmentation via Spectral Embedding 
and ontour !nalysis , Eurographics 2007. S; Katz and !; Tal, Hierarchical Mesh Segmentation via Fuzzy 
lustering and uts , SIGGRAPH 2003.  Mesh segmentation  Non-rigid correspondence  Shape retrieval 
 Global intrinsic symmetry detection  The correspondence problem   Another fundamental problem in 
shape analysis  Handle rigid transformations and poses (not isometry)  Much harder is non-uniform 
part scaling  Framework for spectral approach 1. Define appropriate distance between mesh elements 
 2. Distances to affinities 3. Spectrally embed two shapes based on affinity matrix 4. Correspondence 
analysis in spectral domain  Perhaps rigid transforms would suffice  At least good initialization 
  Rigid alignment  Why spectral?  Intrinsic affinities/distance  Whichever transformation, e.g., 
pose, correspondence is to be invariant of, build that invariance into affinity, e.g., geodesic distance 
 Serves as a normalization   Affinities (e.g., Gaussian) give scale normalization   Normalization 
Operator defined by   Spectral embeddings  eigenvectors  Use of 2nd, 3rd, and 4th scaled eigenvectors 
 Switching causes a reflection, as does a sign flip  Eigenvector switching   Un-switching and un-flipping 
by exhaustive search  Search through all orthogonal transformations (rotation  + reflection) in spectral 
domain [Mateus et al., 07] Some results  switching   symmetry Models with articulation and moderate 
stretching Thinking of the cause Stretching of parts  [Gal et al.2007]  Stretch-invariant correspondence? 
 Cannot really allow arbitrary stretching  But stretching that preserves part structure Distance measure 
that captures part structure? Use of part-aware metric Spectral embeddings Robustness against part 
stretching  Homer Stretched  Geodesic only Geodesic only Part-aware metric Part-aware metric Spectral 
embeddings Rigid ICP registration Limitations  Intrinsic geodesic affinities  Symmetric switches: 
combine with extrinsic info  Topology sensitivity: use of Laplacian embeddings   Primitive heuristic 
for resolving eigenvector switching  Shape characterization using heat kernels  Being global, do not 
solve partial matching problem    Mesh segmentation  Non-rigid correspondence  Shape retrieval 
 Global intrinsic symmetry detection   Shape retrieval  Search and retrieval most similar shapes to 
a query from a shape database  Geometry-based similarity, toeralating  Rigid transformations  Similarity 
transformation  Shape poses  Small topological changes  Small local structural variations  Non-uniform 
part stretching   A spectral approach Operator: Matrix of Gaussian-filtered geodesic distances Only 
take 20 samples Nystrm with sampling on dense mesh  Apply conventional shape descriptors spectral embeddings 
  [Jian and Zhang 06]  Use of eigenvalues Shape descriptor (EVD): eigenvalues 20 20 matrix very efficient 
to compute  Retrieval on McGill Articulated Shape Database EVD Add in use of spectral embeddings LFD-S: 
LFD on spectral embedding SHD-S: SHD on spectral embedding LFD-S SHD SHD-S  Retrieval on McGill Articulated 
Shape Database EVD  Geodesics Laplacian-Beltrami Deal with inherent limitation of geodesic distances 
sensitivity to local topology change, e;g;, a short circuit Use Laplacian-Beltrami eigenfunctions to 
construct spectral embeddings Eigenfunctions have global nature  More stability to local changes  
Isometry invariant in sensitive to shape pose [Rustamov, SGP 07]  Global Point Signatures (GPS) Given 
a point p on the surface, define   : value of the eigenfunction i at the point p s are the Laplacian-Beltrami 
eigenvalues  Scaling in light of commute time distances  GPS-based shape retrieval eigenfunctions 
Sign flips and eigenvector still issues   Mesh segmentation  Non-rigid correspondence  Shape retrieval 
 Global intrinsic symmetry detection  Intrinsic vs. extrinsic, partial vs. global  Symmetry-aware 
processing  Segmentation  Editing  Compression, etc.      Application of GPS embeddings Reduce 
to Euclidean symmetry detection in GPS space  Restricted GPS embeddings  Only consider non-repeated 
eigenvalues  Need to consider sign flips (canonical reflections)  Eigenfunctions associated with repeated 
eigenvalues  introduce rotational symmetries in GPS space  unstable under small non-isometric deformations 
  Results and limitation   Limitation: partial intrinsic symmetry detection Self-map discontinuous 
 Global symmetry unnatural  Distortion in embeddings   Main references R; Rustomov, Laplacian-Beltrami 
Eigenfunctions for Deformation Invariant Shape Representation , SGP 2007. V; Jain, H; Zhang, O; van Kaick, 
Non-Rigid Spectral Correspondence of Triangle Meshes, IJSM 2007. M; Ovsjanikov, J; Sun, and L; Guibas, 
Global Intrinsic Symmetries of Shapes , SGP 2008. V; Jain and H; Zhang, ! Spectral !pproach to Shape-Based 
Retrieval of !rticulated 3D Models , CAD 2007. K; Xu, H; Zhang, !; Tagliasacchi, L; Liu, M; Meng, L; 
Guo, Y; Xiong, Partial Intrinsic Reflectional Symmetry of 3D Shapes , SIGGRAPH Asia 2009. SIGGRAPH 2010 
Spectral Mesh Processing Bruno Lvy and Richard Hao Zhang Bruno Lvy INRIA -ALICE Overview I. 1D parameterization 
II. Surface quadrangulation  III. Surface parameterization IV. Surface characterization Green function 
Heat kernel   1D surface parameterization Graph Laplacian ai,j = w > 0 if (i,j) is an edge =  ai,i 
ai,j (1,1 1) is an eigenvector assoc. with 0 The second eigenvector is interresting [Fiedler 73, 75] 
 1D surface parameterization Fiedler vector  FEM matrix, Reorder with Non-zero entries Fiedler vector 
 1D surface parameterization Fiedler vector Streaming meshes [Isenburg &#38; Lindstrom]   1D surface 
parameterization Fiedler vector Streaming meshes [Isenburg &#38; Lindstrom]   1D surface parameterization 
Fiedler vector F(u) =  wij (ui -uj)2 Minimize F(u) =  utAu  1D surface parameterization Fiedler vector 
F(u) =  wij (ui -uj)2 Minimize F(u) =  utAu How to avoid trivial solution ? Constrained vertices ? 
 1D surface parameterization Fiedler vector F(u) =  wij (ui -uj)2 Minimize F(u) =  utAu subject to 
 Global constraints are more elegant !   1D surface parameterization Fiedler vector F(u) =  wij (ui 
-uj)2 Minimize F(u) =  utAu subject to  2= 1 Global constraints are more elegant ! We need also to 
constrain the second mementum  1D surface parameterization Fiedler vector F(u) = (ui -uj)2 Minimize 
F(u) =  utAu subject to  2= 1 L(u) =  utAu  t 1   (utu -1) L = ut 1 L= Au  1   u = eigenvector 
of A = 0 L= (ut u 1) = eigenvalue  1D surface parameterization Fiedler vector Rem: Fiedler vector 
is also a minimizer of the Rayleigh quotient xt A x R(A,x) = xt x The other eigenvectors xi are the solutions 
of : minimize R(A,xi) subject to xit xj = 0 for j < i Overview I. 1D parameterization II. Surface quadrangulation 
 III. Surface parameterization IV. Surface characterization Green function Heat kernel  Surface quadrangulation 
 Surface quadrangulation  [L 2006], [Vallet &#38; L 2006]  Surface quadrangulation  One eigenfunction 
Morse complex Filtered morse complex [Dong and Garland 2006] Surface quadrangulation  Reparameterization 
of the quads  Surface quadrangulation  Improvement in [Huang, Zhang, Ma, Liu, Kobbelt and Bao 2008], 
takes a guidance vector field into account. Overview I. 1D parameterization II. Surface quadrangulation 
III. Surface parameterization IV. Surface characterization Green function Heat kernel  Surface parameterization 
Minimize  Surface parameterization Minimize  Surface parameterization [Muellen, Tong, Alliez, Desbrun 
2008] Use Fiedler vector, i.e. the minimizer of R(A,x) = xt A x / xt x that is orthogonal to the trivial 
constant solution Implementation: (1) assemble the matrix of the discrete conformal parameterization 
 (2) compute its eigenvector associated with the first non-zero eigenvalue  See http://alice.loria.fr/WIKI/ 
Graphite tutorials Manifold Harmonics Overview I. 1D parameterization II. Surface quadrangulation III. 
Surface parameterization IV. Surface characterization Green function Heat kernel  Surface characterization 
Green Function Solving Poisson equation: f = g f = G(x,y)f(y) dy Where G: Green function is defined by: 
G(x,y) = (x-y) : dirac  Surface characterization Green Function Solving Poisson equation: f = g f = 
G(x,y)f(y) dy Where G: Green function is defined by: G(x,y) = (x-y) : dirac Proof: G(x,y)g(y) dy = 
(x-y)g(y)dy = g(x) = f(x)  Surface characterization Green Function Solving Poisson equation: f = g 
f = G(x,y)f(y) dy Where G: Green function is defined by: G(x,y) = (x-y) : dirac Proof: G(x,y)g(y) dy 
= (x-y)g(y)dy = g(x) = f(x)  f(x) = g(x) = G(x,y)g(y)dy = G(x,y)g(y)dy  Surface characterization 
Green Function Solving Poisson equation: f = g f = G(x,y)f(y) dy Where G: Green function is defined by: 
G(x,y) = (x-y) : dirac Proof: G(x,y)g(y) dy = (x-y)g(y)dy = g(x) = f(x)  f(x) = g(x) = G(x,y)g(y)dy 
= G(x,y)g(y)dy f(x) = G(x,y)g(y)dy  Surface characterization Green Function How to compute G ? G is 
defined by: G(x,y) = (x-y) : dirac  (x-y) = (x) (y) (completeness of the eigen decomposition)  Surface 
characterization Green Function How to compute G ? G is defined by: G(x,y) = (x-y) : dirac  (x-y) = 
(x) (y) (completeness of the eigen decomposition)  Works ! Using G(x,y) = ( G(x,y) = (x) = (x-y) ) 
 Note: Convergence of G series needs to be proved (complicated)  Surface characterization Green Function 
 (x) (y) G(x,y) = Summary: Solution Poisson equation: f = G(x,y)f(y) dy  Pose-invariant embedding Connection 
with GPS embedding [Rustamov 2007] GPS(x) = [ (x)/ , (x)/ , , (x)/ , ] G(x,y) = GPS(x) * GPS(y) Overview 
I. 1D parameterization II. Surface quadrangulation III. Surface parameterization IV. Surface characterization 
 Green function Heat kernel   Surface characterization Heat equation The heat equation: f(x) (heat) 
 x t = 0  Surface characterization Heat equation The heat equation: f(x) (heat)  x t = 100  Surface 
characterization Heat equation The heat equation: f(x) (heat)  x t = 1000  Surface characterization 
Heat equation The heat equation: Heat kernel: K(t,x,y) =  (y) Solution of the heat equation: f(t,x) 
= K(t,x,y) f(0,y) dy   Surface characterization Heat equation Heat Kernel Signature [Sun, Ovsjanikov 
and Gubas 09] Auto-diffusion [Gebal, Baerentzen, Anaes and Larsen 09] How much heat remains at x after 
t seconds ?  ADF(t,x) = HKS(t,x) = K(t,x,x) = 2(x) Applications: shape signature, segmentation using 
Morse decomposition,  Summary  Minimizing Rayleigh quotient instead of using  pinning  enforces global 
constraints (moments) that avoid the trivial solution  fieldler vector for streaming meshes [Isenburg 
et.al]  Spectral conformal parameterization [Muellen et.al]   The notion of fundamental solution plays 
a fundamental role.  Strong connections with spectral analysis (and this is what Fourier invented Fourier 
analysis for !) Green function / Poisson equation -GPS coordinates [Rustamov]  Heat kernel signature 
,[Sun et.al] / auto-diffusion function [Gebal et.al]  More to explore: the Variational Principle (see 
Wikipedia)  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
</content>
</proceeding>
